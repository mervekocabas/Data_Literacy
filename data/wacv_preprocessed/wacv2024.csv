title,author,aff,university_affiliation,university_country,company_affiliation,company_country,abstract,site,oa,pdf,project,github,arxiv,track,status,ieee_link,ieee_keywords,ieee_index_terms,ieee_author_keywords,ieee_citations,ieee_abstract
2D Feature Distillation for Weakly- and Semi-Supervised 3D Semantic Segmentation,"Ozan Unal, Dengxin Dai, Lukas Hoyer, Yigit Baran Can, Luc Van Gool","ETH Zurich; ETH Zurich, KU Leuven, INSAIT; Huawei Technologies",66.66666666666666,"Belgium, Switzerland",33.33333333333334,China,"As 3D perception problems grow in popularity and the need for large-scale labeled datasets for LiDAR semantic segmentation increase, new methods arise that aim to reduce the necessity for dense annotations by employing weakly-supervised training. However these methods continue to show weak boundary estimation and high false negative rates for small objects and distant sparse regions. We argue that such weaknesses can be compensated by using RGB images which provide a denser representation of the scene. We propose an image-guidance network (IGNet) which builds upon the idea of distilling high level feature information from a domain adapted synthetically trained 2D semantic segmentation network. We further utilize a one-way contrastive learning scheme alongside a novel mixing strategy called FOVMix, to combat the horizontal field-of-view mismatch between the two sensors and enhance the effects of image guidance. IGNet achieves state-of-the-art results for weakly-supervised LiDAR semantic segmentation on ScribbleKITTI, boasting up to 98% relative performance to fully supervised training with only 8% labeled points, while introducing no additional annotation burden or computational/memory cost during inference. Furthermore, we show that our contributions also prove effective for semi-supervised training, where IGNet claims state-of-the-art results on both ScribbleKITTI and SemanticKITTI.",https://openaccess.thecvf.com/content/WACV2024/html/Unal_2D_Feature_Distillation_for_Weakly-_and_Semi-Supervised_3D_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Unal_2D_Feature_Distillation_for_Weakly-_and_Semi-Supervised_3D_Semantic_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484032/,"['Training', 'Image sensors', 'Laser radar', 'Three-dimensional displays', 'Annotations', 'Semantic segmentation', 'Semantics']","['Semantic Segmentation', '2D Feature', '3D Semantic Segmentation', 'Feature Distillation', 'Relative Performance', 'Distal Regions', 'RGB Images', 'High-level Features', 'Small Objects', 'Domain Adaptation', 'Self-supervised Learning', 'Image Guidance', 'High False Negative Rate', 'Dense Representation', 'Boundary Estimation', 'Sparse Regions', 'Image Features', 'Teacher Model', 'Point Cloud', 'Target Domain', 'Weak Labels', 'LiDAR Point Clouds', 'Pseudo Labels', 'Weak Supervision', 'Contrastive Loss', 'Semantic Segmentation Models', 'LiDAR Sensor', 'Additional Memory', 'Student Model', 'Mixed Operator']","['Applications', 'Autonomous Driving', 'Algorithms', '3D computer vision', 'Applications', 'Remote Sensing']",3,"As 3D perception problems grow in popularity and the need for large-scale labeled datasets for LiDAR semantic segmentation increase, new methods arise that aim to reduce the necessity for dense annotations by employing weakly-supervised training. However these methods continue to show weak boundary estimation and high false negative rates for small objects and distant sparse regions. We argue that such weaknesses can be compensated by using RGB images which provide a denser representation of the scene. We propose an image-guidance network (IGNet) which builds upon the idea of distilling high level feature information from a domain adapted synthetically trained 2D semantic segmentation network. We further utilize a one-way contrastive learning scheme alongside a novel mixing strategy called FOVMix, to combat the horizontal field-of-view mismatch between the two sensors and enhance the effects of image guidance. IGNet achieves state-of-the-art results for weakly-supervised LiDAR semantic segmentation on ScribbleKITTI, boasting up to 98% relative performance to fully supervised training with only 8% labeled points, while introducing no additional annotation burden or computational/memory cost during inference. Furthermore, we show that our contributions also prove effective for semi-supervised training, where IGNet claims state-of-the-art results on both ScribbleKITTI and SemanticKITTI."
360BEV: Panoramic Semantic Mapping for Indoor Bird's-Eye View,"Zhifeng Teng, Jiaming Zhang, Kailun Yang, Kunyu Peng, Hao Shi, Simon Reiß, Ke Cao, Rainer Stiefelhagen",Hunan University; Karlsruhe Institute of Technology; Zhejiang University,100.0,"China, Germany",0.0,,"Seeing only a tiny part of the whole is not knowing the full circumstance. Bird's-eye-view (BEV) perception, a process of obtaining allocentric maps from egocentric views, is restricted when using a narrow Field of View (FoV) alone. In this work, mapping from 360deg panoramas to BEV semantics, the 360BEV task, is established for the first time to achieve holistic representations of indoor scenes in a top-down view. Instead of relying on narrow-FoV image sequences, a panoramic image with depth information is sufficient to generate a holistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets, 360BEV-Matterport and 360BEV-Stanford, both of which include egocentric panoramic images and semantic segmentation labels, as well as allocentric semantic maps. Besides delving deep into different mapping paradigms, we propose a dedicated solution for panoramic semantic mapping, namely 360Mapper. Through extensive experiments, our methods achieve 44.32% and 45.78% mIoU on both datasets respectively, surpassing previous counterparts with gains of +7.60% and +9.70% in mIoU.",https://openaccess.thecvf.com/content/WACV2024/html/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Teng_360BEV_Panoramic_Semantic_Mapping_for_Indoor_Birds-Eye_View_WACV_2024_paper.pdf,360BEV,,,main,Poster,https://ieeexplore.ieee.org/document/10484146/,"['Computer vision', 'Semantic segmentation', 'Semantics', 'Computer architecture', 'Benchmark testing', 'Indoor environment', 'Image sequences']","['Bird’s Eye', 'Semantic Map', 'Benchmark', 'Semantic Segmentation', 'Depth Information', 'Semantic Labels', 'Panoramic Images', 'Top-down View', 'Narrow Field Of View', 'Feature Maps', 'Single Image', 'Projection Matrix', 'Image Distortion', 'Complete Function', '3D Point', 'Projection Method', 'Frontal View', 'Linear Layer', 'Attention Weights', 'Distortion Effects', 'Indoor Navigation', 'Scene Understanding', 'Deformable Objects', 'Pinhole Camera', 'Early Project', 'Adaptive Sampling', 'Adaptive Manner', 'Sequence-based Methods']","['Algorithms', 'Image recognition and understanding']",5,"Seeing only a tiny part of the whole is not knowing the full circumstance. Bird’s-eye-view (BEV) perception, a process of obtaining allocentric maps from egocentric views, is restricted when using a narrow Field of View (FoV) alone. In this work, mapping from 360° panoramas to BEV semantics, the 360BEV task, is established for the first time to achieve holistic representations of indoor scenes in a top-down view. Instead of relying on narrow-FoV image sequences, a panoramic image with depth information is sufficient to generate a holistic BEV semantic map. To benchmark 360BEV, we present two indoor datasets, 360BEV-Matterport and 360BEV-Stanford, both of which include egocentric panoramic images and semantic segmentation labels, as well as allocentric semantic maps. Besides delving deep into different mapping paradigms, we propose a dedicated solution for panoramic semantic mapping, namely 360Mapper. Through extensive experiments, our methods achieve 44.32% and 45.78% mIoU on both datasets respectively, surpassing previous counterparts with gains of +7.60% and +9.70% in mIoU. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
3D Face Style Transfer With a Hybrid Solution of NeRF and Mesh Rasterization,"Jianwei Feng, Prateek Singhal",Amazon,0.0,,100.0,USA,"Style transfer for human face has been widely researched in recent years. Majority of the existing approaches work in 2D image domain and have 3D inconsistency issue when applied on different viewpoints of the same face. In this paper, we tackle the problem of 3D face style transfer which aims at generating stylized novel views of a 3D human face with multi-view consistency. We propose to use a neural radiance field (NeRF) to represent 3D human face and combine it with 2D style transfer to stylize the 3D face. We find that directly training a NeRF on stylized images from 2D style transfer brings in 3D inconsistency issue and causes blurriness. On the other hand, training a NeRF jointly with 2D style transfer objectives shows poor convergence due to the identity and head pose gap between style image and content image. It also poses challenge in training time and memory due to the need of volume rendering for full image to apply style transfer loss functions. We therefore propose a hybrid framework of NeRF and mesh rasterization to combine the benefits of high fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our framework consists of three stages: 1. Training a NeRF model on input face images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF model and optimizing it with style transfer objectives via differentiable rasterization; 3. Training a new color network in NeRF conditioned on a style embedding to enable arbitrary style transfer to the 3D face. Experiment results show that our approach generates high quality face style transfer with great 3D consistency, while also enabling a flexible style control.",https://openaccess.thecvf.com/content/WACV2024/html/Feng_3D_Face_Style_Transfer_With_a_Hybrid_Solution_of_NeRF_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Feng_3D_Face_Style_Transfer_With_a_Hybrid_Solution_of_NeRF_WACV_2024_paper.pdf,,,2311.13168,main,Poster,https://ieeexplore.ieee.org/document/10483754/,"['Training', 'Geometry', 'Solid modeling', 'Three-dimensional displays', 'Image resolution', 'Image color analysis', 'Rendering (computer graphics)']","['Style Transfer', '3D Face', 'Neural Radiance Fields', '3D Style', '3D Style Transfer', 'Training Time', 'Face Images', 'Human Faces', '3D Geometry', 'Style Image', 'Head Pose', 'Neural Field', 'Stage 2', 'Input Image', 'Paired Data', 'Multilayer Perceptron', 'Viewing Angle', 'Network Weights', 'Volume Density', '3D Scene', 'View Synthesis', 'Explicit Representation', '3D Point', 'Signed Distance Function', 'Original Image Resolution', 'Mesh Refinement', 'Original Resolution', '3D Representation', 'Multiple Styles']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"Style transfer for human face has been widely researched in recent years. Majority of the existing approaches work in 2D image domain and have 3D inconsistency issue when applied on different viewpoints of the same face. In this paper, we tackle the problem of 3D face style transfer which aims at generating stylized novel views of a 3D human face with multi-view consistency. We propose to use a neural radiance field (NeRF) to represent 3D human face and combine it with 2D style transfer to stylize the 3D face. We find that directly training a NeRF on stylized images from 2D style transfer brings in 3D inconsistency issue and causes blurriness. On the other hand, training a NeRF jointly with 2D style transfer objectives shows poor convergence due to the identity and head pose gap between style image and content image. It also poses challenge in training time and memory due to the need of volume rendering for full image to apply style transfer loss functions. We therefore propose a hybrid framework of NeRF and mesh rasterization to combine the benefits of high fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our framework consists of three stages: 1. Training a NeRF model on input face images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF model and optimizing it with style transfer objectives via differentiable rasterization; 3. Training a new color network in NeRF conditioned on a style embedding to enable arbitrary style transfer to the 3D face. Experiment results show that our approach generates high quality face style transfer with great 3D consistency, while also enabling a flexible style control."
3D Human Pose Estimation With Two-Step Mixed-Training Strategy,"Yingfeng Wang, Zhengwei Wang, Muyu Li, Hong Yan","Dalian University of Technology, China; Centre for Intelligent Multidimensional Data Analysis, Science Park, Hong Kong; City University of Hong Kong",66.66666666666666,"China, Hong Kong",33.33333333333334,Hong Kong,"In monocular 3D human pose estimation, target motions are generally stable and continuous, which indicates that joint velocity can provide valuable information for better estimation. Therefore, it is critical to learn the joint motion trajectory and spatio-temporal information from velocity. Previous works have shown that Transformers are effective in capturing the relationship between tokens. However, in practice, only 2D position is available and 3D velocity has not been explicitly used as a model input. To address this challenge, we propose TMT (Two-step Mixed-Training strategy), a transformer-based approach that effectively incorporates 3D velocity into the input vector during training, allowing for better learning of relevant features in the shallow layers. Extensive experiments demonstrate that TMT significantly improves the performance of state-of-the-art models, such as MixSTE, MHFormer, and PoseFomer, on two datasets: Human3.6M and MPI-INF-3DHP. TMT out performs the state-of-the-art approach by up to 13.8% on the Human3.6M dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_3D_Human_Pose_Estimation_With_Two-Step_Mixed-Training_Strategy_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_3D_Human_Pose_Estimation_With_Two-Step_Mixed-Training_Strategy_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484123/,"['Training', 'Solid modeling', 'Three-dimensional displays', 'Correlation', 'Pose estimation', 'Transformers', 'Vectors']","['Pose Estimation', 'Human Pose Estimation', '3D Pose', 'Shallow Layers', 'Motion Trajectory', 'Joint Velocity', '2D Position', 'Loss Function', 'Contralateral', 'Graphics Processing Unit', 'Training Strategy', 'Input Sequence', 'Velocity Vector', 'Temporal Correlation', 'Training Step', '3D Position', 'Convergence Time', 'Temporal Domain', 'Zero Vector', '2D Keypoints', 'Temporal Smoothing', '2D Input', 'Minimal Overhead', '3D Prediction', 'Center Of Frame', 'Quality Of Input', 'Position Estimation', 'Space Domain', '2D Pose']","['Algorithms', '3D computer vision']",,"In monocular 3D human pose estimation, target motions are generally stable and continuous, which indicates that joint velocity can provide valuable information for better estimation. Therefore, it is critical to learn the joint motion trajectory and spatio-temporal information from velocity. Previous works have shown that Transformers are effective in capturing the relationship between tokens. However, in practice, only 2D position is available and 3D velocity has not been explicitly used as a model input. To address this challenge, we propose TMT (Two-step Mixed-Training strategy), a transformer-based approach that effectively incorporates 3D velocity into the input vector during training, allowing for better learning of relevant features in the shallow layers. Extensive experiments demonstrate that TMT significantly improves the performance of state-of-the-art models, such as MixSTE, MHFormer, and PoseFomer, on two datasets: Human3.6M and MPI-INF-3DHP. TMT outperforms the state-of-the-art approach by up to 13.8% on the Human3.6M dataset."
3D Reconstruction of Interacting Multi-Person in Clothing From a Single Image,"Junuk Cha, Hansol Lee, Jaewon Kim, Nhat Nguyen Bao Truong, Jaeshin Yoon, Seungryul Baek","Adobe Research; UNIST, KIST; UNIST; UNIST, KAIST",75.0,South Korea,25.0,USA,"This paper introduces a novel pipeline to reconstruct the geometry of interacting multi-person in clothing on a globally coherent scene space from a single image. The main challenge arises from the occlusion: a part of a human body is not visible from a single view due to the occlusion by others or the self, which introduces missing geometry and physical implausibility (e.g., penetration). We overcome this challenge by utilizing two human priors for complete 3D geometry and surface contacts. For the geometry prior, an encoder learns to regress the image of a person with missing body parts to the latent vectors; a decoder decodes these vectors to produce 3D features of the associated geometry; and an implicit network combines these features with a surface normal map to reconstruct a complete and detailed 3D humans. For the contact prior, we develop an image-space contact detector that outputs a probability distribution of surface contacts between people in 3D. We use these priors to globally refine the body poses, enabling the penetration-free and accurate reconstruction of interacting multi-person in clothing on the scene space. The results demonstrate that our method is complete, globally coherent, and physically plausible compared to existing methods.",https://openaccess.thecvf.com/content/WACV2024/html/Cha_3D_Reconstruction_of_Interacting_Multi-Person_in_Clothing_From_a_Single_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cha_3D_Reconstruction_of_Interacting_Multi-Person_in_Clothing_From_a_Single_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484416/,"['Geometry', 'Surface reconstruction', 'Solid modeling', 'Three-dimensional displays', 'Monte Carlo methods', 'Clothing', 'Detectors']","['Single Image', '3D Reconstruction', 'Body Parts', 'Contact Surface', '3D Surface', '3D Geometry', '3D Features', 'Latent Vector', 'Person Image', 'Missing Parts', 'Complete 3D', 'Distribution Of Contacts', 'Normal Map', 'Human 3D', 'Body Pose', 'Global Coherence', 'Shape Parameter', 'Singular Value Decomposition', 'Semantic Segmentation', '3D Scanning', '3D Mesh', 'Human Pose Estimation', 'Signed Distance Function', '3D Body', 'Reconstruction Results', 'Chamfer Distance', 'Texture Map', 'Pose Parameters', '3D Joint', 'Pose Estimation']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",1,"This paper introduces a novel pipeline to reconstruct the geometry of interacting multi-person in clothing on a globally coherent scene space from a single image. The main challenge arises from the occlusion: a part of a human body is not visible from a single view due to the occlusion by others or the self, which introduces missing geometry and physical implausibility (e.g., penetration). We overcome this challenge by utilizing two human priors for complete 3D geometry and surface contacts. For the geometry prior, an encoder learns to regress the image of a person with missing body parts to the latent vectors; a decoder decodes these vectors to produce 3D features of the associated geometry; and an implicit network combines these features with a surface normal map to reconstruct a complete and detailed 3D humans. For the contact prior, we develop an image-space contact detector that outputs a probability distribution of surface contacts between people in 3D. We use these priors to globally refine the body poses, enabling the penetration-free and accurate reconstruction of interacting multi-person in clothing on the scene space. The results demonstrate that our method is complete, globally coherent, and physically plausible compared to existing methods."
3D Super-Resolution Model for Vehicle Flow Field Enrichment,"Thanh Luan Trinh, Fangge Chen, Takuya Nanri, Kei Akasaka","Mobility & AI Laboratory, Nissan Motor Co., Ltd. and Yokohama National University, Japan; Mobility & AI Laboratory, Nissan Motor Co., Ltd.; Integrated CAE and PLM Department, Nissan Motor Co., Ltd.",66.66666666666666,Japan,33.33333333333334,Japan,"In vehicle shape design from aerodynamic performance perspective, deep learning methods enable us to estimate the flow field in a short period. However, the estimated flow fields are generally coarse and of low resolution. Therefore, a super-resolution model is required to enrich them. In this study, we propose a novel super-resolution model to enrich the flow fields around the vehicle to a higher resolution. To deal with the complex flow fields of vehicles, we apply the residual-in-residual dense block (RRDB) as the basic network-building unit in the generator without batch normalization. We then apply the relativistic discriminator to provide better feedback regarding the lack of high-frequency components. In addition, we propose a distance-weighted loss to obtain better estimation in wake regions and regions near the vehicle surface. Physics-informed loss is used to help the model generate data that satisfies the physical governing equations. We also propose a new training strategy to improve the leaning effectiveness and avoid instability during training. Experimental results demonstrate that the proposed method outperforms the previous study in vehicle flow field enrichment tasks by a significant margin.",https://openaccess.thecvf.com/content/WACV2024/html/Trinh_3D_Super-Resolution_Model_for_Vehicle_Flow_Field_Enrichment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Trinh_3D_Super-Resolution_Model_for_Vehicle_Flow_Field_Enrichment_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484254/,"['Training', 'Solid modeling', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Superresolution', 'Generative adversarial networks']","['Flow Field', 'Vehicle Flow', 'Super-resolution Model', 'High Frequency Components', 'Dense Block', 'Wake Region', 'Aerodynamic Performance', 'Loss Function', 'Training Set', 'Convolutional Neural Network', 'Flow Velocity', 'Convolutional Layers', 'Generalization Ability', 'High-resolution Data', 'Generative Adversarial Networks', 'Navier Stokes Equations', 'Loss Of Content', 'Batch Normalization Layer', 'Learning Purposes', 'Pressure Field', 'Physical Loss', 'Learning Schedule', 'High-resolution Field', 'Bicubic Interpolation', '3D Flow', 'Vehicle Design', 'CNN-based Methods', 'Central Cross-section', 'Mean Absolute Error', 'Points In Space']","['Applications', 'Visualization', 'Applications', 'Structural engineering / civil engineering']",,"In vehicle shape design from an aerodynamic performance perspective, deep learning methods enable us to estimate the flow field in a short period. However, the estimated flow fields are generally coarse and of low resolution. Therefore, a super-resolution model is required to enrich them. In this study, we propose a novel super-resolution model to enrich the flow fields around the vehicle to a higher resolution. To deal with the complex flow fields of vehicles, we apply the residual-in-residual dense block (RRDB) as the basic network-building unit in the generator without batch normalization. We then apply the relativistic discriminator to provide better feedback regarding the lack of high-frequency components. In addition, we propose a distance-weighted loss to obtain better estimation in wake regions and regions near the vehicle surface. Physics-informed loss is used to help the model generate data that satisfies the physical governing equations. We also propose a new training strategy to improve learning effectiveness and avoid instability during training for our enrichment task. Experimental results demonstrate that the proposed method out-performs the previous study in vehicle flow field enrichment tasks by a significant margin."
3D-Aware Talking-Head Video Motion Transfer,"Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang","The Ohio State University, Columbus, OH, USA; The Pennsylvania State University, University Park, PA, USA",100.0,USA,0.0,,"Motion transfer of talking-head videos involves generating a new video with the appearance of a subject video and the motion pattern of a driving video. Current methodologies primarily depend on a limited number of subject images and 2D representations, thereby neglecting to fully utilize the multi-view appearance features inherent in the subject video. In this paper, we propose a novel 3D-aware talking-head video motion transfer network, Head3D, which fully exploits the subject appearance information by generating a visually-interpretable 3D canonical head from the 2D subject frames with a recurrent network. A key component of our approach is a self-supervised 3D head geometry learning module, designed to predict head poses and depth maps from 2D subject video frames. This module facilitates the estimation of a 3D head in canonical space, which can then be transformed to align with driving video frames. Additionally, we propose an attention-based fusion network to combine the background and other details from subject frames with the 3D subject head to produce the synthetic target video. Our extensive experiments on two public talking-head video datasets demonstrate that Head3D outperforms both 2D and 3D prior arts in the practical cross-identity setting, with evidence showing it can be readily adapted to the pose-controllable novel view synthesis task.",https://openaccess.thecvf.com/content/WACV2024/html/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ni_3D-Aware_Talking-Head_Video_Motion_Transfer_WACV_2024_paper.pdf,,,2311.02549,main,Poster,https://ieeexplore.ieee.org/document/10483641/,"['Geometry', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Head', 'Art', 'Computational modeling']","['Motion Transfer', 'Talking Head Video', 'Public Datasets', 'Recurrent Network', 'Video Frames', 'Depth Map', 'Appearance Features', 'Subjective Image', 'Self-supervised Learning', 'Video Dataset', 'Detailed Background', '2D Representation', 'Appearance Information', 'Head Pose', 'View Synthesis', '3D Head', 'Reference Frame', 'Facial Expressions', 'Explicit Model', 'Head Region', '3D Geometry', 'Pre-trained Network', 'Rigid Transformation', 'Attention Map', 'Head Imaging', 'Target Frame', '3D Information', 'Flow Map', 'Head Frame', 'Final Video']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Motion transfer of talking-head videos involves generating a new video with the appearance of a subject video and the motion pattern of a driving video. Current methodologies primarily depend on a limited number of subject images and 2D representations, thereby neglecting to fully utilize the multi-view appearance features inherent in the subject video. In this paper, we propose a novel 3D-aware talking-head video motion transfer network, Head3D, which fully exploits the subject appearance information by generating a visually-interpretable 3D canonical head from the 2D subject frames with a recurrent network. A key component of our approach is a self-supervised 3D head geometry learning module, designed to predict head poses and depth maps from 2D subject video frames. This module facilitates the estimation of a 3D head in canonical space, which can then be transformed to align with driving video frames. Additionally, we employ an attention-based fusion network to combine the background and other details from subject frames with the 3D subject head to produce the synthetic target video. Our extensive experiments on two public talking-head video datasets demonstrate that Head3D outperforms both 2D and 3D prior arts in the practical cross-identity setting, with evidence showing it can be readily adapted to the pose-controllable novel view synthesis task."
3SD: Self-Supervised Saliency Detection With No Labels,"Rajeev Yasarla, Renliang Weng, Wongun Choi, Vishal M. Patel, Amir Sadeghian","2AIBEE; 1Johns Hopkins University, 2AIBEE; 1Johns Hopkins University",66.66666666666666,USA,33.33333333333334,USA,"We present a conceptually simple self-supervised method for saliency detection. Our method generates and uses pseudo-ground truth labels for training. The generated pseudo-GT labels don't require any kind of human annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent works show that features extracted from classification tasks provide important saliency cues like structure and semantic information of salient objects in the image. Our method, called 3SD, exploits this idea by adding a branch for a self-supervised classification task in parallel with salient object detection, to obtain class activation maps (CAM maps). These CAM maps along with the edges of the input image are used to generate the pseudo-GT saliency maps to train our 3SD network. Specifically, we propose a contrastive learning-based training on multiple image patches for the classification task. We show the multi-patch classification with contrastive loss improves the quality of the CAM maps compared to naive classification on the entire image. Experiments on six benchmark datasets demonstrate that without any labels, our 3SD method outperforms all existing weakly supervised and unsupervised methods, and its performance is on par with the fully-supervised methods.",https://openaccess.thecvf.com/content/WACV2024/html/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yasarla_3SD_Self-Supervised_Saliency_Detection_With_No_Labels_WACV_2024_paper.pdf,,https://github.com/rajeevyasarla/3SD,2203.04478,main,Poster,https://ieeexplore.ieee.org/document/10484296/,"['Training', 'Annotations', 'Image edge detection', 'Semantics', 'Self-supervised learning', 'Object detection', 'Benchmark testing']","['Saliency Detection', 'Detection Methods', 'Classification Task', 'Image Object', 'Semantic Information', 'Unsupervised Methods', 'Objective Information', 'Image Patches', 'Important Cues', 'Saliency Map', 'Salient Object', 'Class Activation Maps', 'Weak Labels', 'Self-supervised Task', 'Salient Object Detection', 'Convolutional Neural Network', 'Local Features', 'Class Labels', 'Stochastic Gradient Descent', 'Global Context', 'Student Network', 'Image Captioning', 'Positive Patch', 'Noisy Labels', 'Encoder Output', 'Self-supervised Learning', 'Teacher Network', 'Handcrafted Features', 'Foreground Objects', 'Multiple Labels']","['Algorithms', 'Image recognition and understanding']",,"We present a conceptually simple self-supervised method for saliency detection. Our method generates and uses pseudo-ground truth labels for training. The generated pseudo-GT labels don’t require any kind of human annotations (e.g., pixel-wise labels or weak labels like scribbles). Recent works show that features extracted from classification tasks provide important saliency cues like structure and semantic information of salient objects in the image. Our method, called 3SD, exploits this idea by adding a branch for a self-supervised classification task in parallel with salient object detection, to obtain class activation maps (CAM maps). These CAM maps along with the edges of the input image are used to generate the pseudo-GT saliency maps to train our 3SD network. Specifically, we propose a contrastive learning-based training on multiple image patches for the classification task. We show the multi-patch classification with contrastive loss improves the quality of the CAM maps compared to naive classification on the entire image. Experiments on six benchmark datasets demonstrate that without any labels, our 3SD method outperforms all existing weakly supervised and unsupervised methods, and its performance is on par with the fully-supervised methods."
4K-Resolution Photo Exposure Correction at 125 FPS With ~8K Parameters,"Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, Jun Xu","Nankai University, Guangdong Provincial Key Laboratory of Big Data Computing, CUHK (Shenzhen); Tianjin University, Lappeenranta-Lahti University of Technology; Nankai University",100.0,"China, Hong Kong",0.0,,"The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only  8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1x1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet.",https://openaccess.thecvf.com/content/WACV2024/html/Zhou_4K-Resolution_Photo_Exposure_Correction_at_125_FPS_With_8K_Parameters_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhou_4K-Resolution_Photo_Exposure_Correction_at_125_FPS_With_8K_Parameters_WACV_2024_paper.pdf,,https://github.com/Zhou-Yijie/MSLTNet,2311.08759,main,Poster,https://ieeexplore.ieee.org/document/10484334/,"['Convolutional codes', 'Computer vision', 'Laplace equations', 'Lighting', 'Graphics processing units', 'Computer architecture', 'Benchmark testing']","['Photo Exposure', 'Transformer', 'Convolutional Neural Network', 'Deep Convolutional Neural Network', 'Benchmark Datasets', 'Promising Performance', 'Multilayer Perception', 'Laplacian Pyramid', 'Titan RTX GPU', 'Training Set', 'Computational Cost', 'Convolutional Layers', 'Supplementary File', 'Image Classification', 'Learning Framework', 'Convolution Kernel', 'Peak Signal-to-noise Ratio', 'Affine Transformation', 'Global Average Pooling', 'Multi-scale Decomposition', 'Image Correction', 'Mean Channel', 'Inference Speed', 'Hierarchical Decomposition', 'High Dynamic Range', 'Objective Metrics', 'Computational Inference', 'Image Enhancement', '3D Grid']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",2,"The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1 × 1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet."
A Closer Look at Robustness of Vision Transformers to Backdoor Attacks,"Akshayvarun Subramanya, Soroush Abbasi Koohpayegani, Aniruddha Saha, Ajinkya Tejankar, Hamed Pirsiavash","University of California, Davis; University of Maryland, Baltimore County",100.0,USA,0.0,,"Transformer architectures are based on self-attention mechanism that processes images as a sequence of patches. As their design is quite different compared to CNNs, it is important to take a closer look at their vulnerability to backdoor attacks and how different transformer architectures affect robustness. Backdoor attacks happen when an attacker poisons a small part of the training images with a specific trigger or backdoor which will be activated later. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger on an image at test time. In this paper, we compare state-of-the-art architectures through the lens of backdoor attacks, specifically how attention mechanisms affect robustness. We observe that the well known vision transformer architecture (ViT) is the least robust architecture and ResMLP, which belongs to a class called Feed Forward Networks (FFN), is most robust to backdoor attacks among state-of-the-art architectures. We also find an intriguing difference between transformers and CNNs -- interpretation algorithms effectively highlight the trigger on test images for transformers but not for CNNs. Based on this observation, we find that a test-time image blocking defense reduces the attack success rate by a large margin for transformers. We also show that such blocking mechanisms can be incorporated during the training process to improve robustness even further. We believe our experimental findings will encourage the community to understand the building block components in developing novel architectures robust to backdoor attacks. Code is available here:https://github.com/UCDvision/backdoor_transformer.git",https://openaccess.thecvf.com/content/WACV2024/html/Subramanya_A_Closer_Look_at_Robustness_of_Vision_Transformers_to_Backdoor_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Subramanya_A_Closer_Look_at_Robustness_of_Vision_Transformers_to_Backdoor_WACV_2024_paper.pdf,,https://github.com/UCDvision/backdoor_transformer.git,,main,Poster,https://ieeexplore.ieee.org/document/10483723/,"['Training', 'Computer vision', 'Toxicology', 'Codes', 'Architecture', 'Computer architecture', 'Transformers']","['Vision Transformer', 'Backdoor Attacks', 'Convolutional Neural Network', 'Attention Mechanism', 'Feed-forward Network', 'Clear Image', 'Self-attention Mechanism', 'Transformer Architecture', 'Blocking Mechanism', 'Robust Architecture', 'Attack Success Rate', 'Training Data', 'Convolutional Network', 'Validation Set', 'Object Detection', 'Clean Data', 'Image Regions', 'Sequence Elements', 'Target Category', 'Threat Model', 'Accuracy Drop', 'Attention Block', 'Inductive Bias', 'Attention Layer', 'Source Images', 'Augmentation Strategy', 'ImageNet Dataset', 'Small Drop']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Transformer architectures are based on self-attention mechanism that processes images as a sequence of patches. As their design is quite different compared to CNNs, it is important to take a closer look at their vulnerability to back-door attacks and how different transformer architectures affect robustness. Backdoor attacks happen when an attacker poisons a small part of the training images with a specific trigger or backdoor which will be activated later. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger on an image at test time. In this paper, we compare state-of-the-art architectures through the lens of backdoor attacks, specifically how attention mechanisms affect robustness. We observe that the well known vision transformer architecture (ViT) is the least robust architecture and ResMLP, which belongs to a class called Feed Forward Networks (FFN), is most robust to backdoor attacks among state-of-the-art architectures. We also find an intriguing difference between transformers and CNNs - interpretation algorithms effectively highlight the trigger on test images for transformers but not for CNNs. Based on this observation, we find that a test-time image blocking defense reduces the attack success rate by a large margin for transformers. We also show that such blocking mechanisms can be incorporated during the training process to improve robustness even further. We believe our experimental findings will encourage the community to understand the building block components in developing novel architectures robust to back-door attacks. Code is available here: https://github.com/UCDvision/backdoor_transformer.git"
A Coarse-To-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection,"Anas Al-lahham, Nurbek Tastan, Muhammad Zaigham Zaheer, Karthik Nandakumar","Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE",100.0,UAE,0.0,,"Detection of anomalous events in videos is an important problem in applications such as surveillance. Video anomaly detection (VAD) is well-studied in the one-class classification (OCC) and weakly supervised (WS) settings. However, fully unsupervised (US) video anomaly detection methods, which learn a complete system without any annotation or human supervision, have not been explored in depth. This is because the lack of any ground truth annotations significantly increases the magnitude of the VAD challenge. To address this challenge, we propose a simple-but-effective two-stage pseudo-label generation framework that produces segment-level (normal/anomaly) pseudo-labels, which can be further used to train a segment-level anomaly detector in a supervised manner. The proposed coarse-to-fine pseudo-label (C2FPL) generator employs carefully-designed hierarchical divisive clustering and statistical hypothesis testing to identify anomalous video segments from a set of completely unlabeled videos. The trained anomaly detector can be directly applied on segments of an unseen test video to obtain segment-level, and subsequently, frame-level anomaly predictions. Extensive studies on two large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that the proposed unsupervised approach achieves superior performance compared to all existing OCC and US methods, while yielding comparable performance to the state-of-the-art WS methods.",https://openaccess.thecvf.com/content/WACV2024/html/Al-lahham_A_Coarse-To-Fine_Pseudo-Labeling_C2FPL_Framework_for_Unsupervised_Video_Anomaly_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Al-lahham_A_Coarse-To-Fine_Pseudo-Labeling_C2FPL_Framework_for_Unsupervised_Video_Anomaly_Detection_WACV_2024_paper.pdf,,,2310.17650,main,Poster,https://ieeexplore.ieee.org/document/10483854/,"['Training', 'Computer vision', 'Annotations', 'Surveillance', 'Detectors', 'Generators', 'Anomaly detection']","['Anomaly Detection', 'Video Anomaly Detection', 'Hierarchical Clustering', 'Large-scale Datasets', 'Unsupervised Methods', 'Statistical Hypothesis Testing', 'Video Segments', 'Test Videos', 'Human Supervision', 'One-class Classification', 'Video Events', 'Training Set', 'Training Data', 'Null Hypothesis', 'Training Dataset', 'Representation Learning', 'Real-world Scenarios', 'Fully-connected Layer', 'Video Data', 'Gaussian Mixture Model', 'Anomaly Score', 'Number Of Videos', 'Training Videos', 'Multiple Instance Learning', 'Ground Truth Labels', 'Graph Convolutional Network', 'Surveillance Cameras', 'Training Batch', 'Manual Annotation', 'Label Noise']","['Algorithms', 'Video recognition and understanding']",9,"Detection of anomalous events in videos is an important problem in applications such as surveillance. Video anomaly detection (VAD) is well-studied in the one-class classification (OCC) and weakly supervised (WS) settings. However, fully unsupervised (US) video anomaly detection methods, which learn a complete system without any annotation or human supervision, have not been explored in depth. This is because the lack of any ground truth annotations significantly increases the magnitude of the VAD challenge. To address this challenge, we propose a simple-but-effective two-stage pseudo-label generation framework that produces segment-level (normal/anomaly) pseudo-labels, which can be further used to train a segment-level anomaly detector in a supervised manner. The proposed coarse-to-fine pseudo-label (C2FPL) generator employs carefully-designed hierarchical divisive clustering and statistical hypothesis testing to identify anomalous video segments from a set of completely unlabeled videos. The trained anomaly detector can be directly applied on segments of an unseen test video to obtain segment-level, and subsequently, frame-level anomaly predictions. Extensive studies on two large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that the proposed unsupervised approach achieves superior performance compared to all existing OCC and US methods, while yielding comparable performance to the state-of-the-art WS methods."
A Generative Multi-Resolution Pyramid and Normal-Conditioning 3D Cloth Draping,"Hunor Laczkó, Meysam Madadi, Sergio Escalera, Jordi Gonzalez","Universitat de Barcelona, Computer Vision Center; Universitat Autònoma de Barcelona, Computer Vision Center; Computer Vision Center, Universitat Autònoma de Barcelona",100.0,Spain,0.0,,"RGB cloth generation has been deeply studied in the related literature, however, 3D garment generation remains an open problem. In this paper, we build a conditional variational autoencoder for 3D garment generation and draping. We propose a pyramid network to add garment details progressively in a canonical space, i.e. unposing and unshaping the garments w.r.t. the body. We study conditioning the network on surface normal UV maps, as an intermediate representation, which is an easier problem to optimize than 3D coordinates. Our results on two public datasets, CLOTH3D and CAPE, show that our model is robust, controllable in terms of detail generation by the use of multi-resolution pyramids, and achieves state-of-the-art results that can highly generalize to unseen garments, poses, and shapes even when training with small amounts of data. The code can be found at: https://github.com/HunorLaczko/pyramid-drape",https://openaccess.thecvf.com/content/WACV2024/html/Laczko_A_Generative_Multi-Resolution_Pyramid_and_Normal-Conditioning_3D_Cloth_Draping_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Laczko_A_Generative_Multi-Resolution_Pyramid_and_Normal-Conditioning_3D_Cloth_Draping_WACV_2024_paper.pdf,,https://github.com/HunorLaczko/pyramid-drape,,main,Poster,https://ieeexplore.ieee.org/document/10484264/,"['Training', 'Convolutional codes', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Clothing']","['Public Datasets', '3D Coordinates', 'Variational Autoencoder', 'Intermediate Representation', 'Surface Normals', 'Normal Map', 'Conditional Variational Autoencoder', 'Point Cloud', 'Body Shape', '3D Mesh', 'Inference Time', 'Implicit Function', 'Surface Quality', 'Graph Neural Networks', 'Reconstruction Loss', 'Deterministic Approach', 'L1 Loss', 'L2 Loss', 'Regularization Loss', 'Pyramid Level', 'Body Pose', '3D Template', 'Qualitative Metrics']","['Applications', 'Virtual / augmented reality', 'Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"RGB cloth generation has been deeply studied in the related literature, however, 3D garment generation remains an open problem. In this paper, we build a conditional variational autoencoder for 3D garment generation and draping. We propose a pyramid network to add garment details progressively in a canonical space, i.e. unposing and unshaping the garments w.r.t. the body. We study conditioning the network on surface normal UV maps, as an intermediate representation, which is an easier problem to optimize than 3D coordinates. Our results on two public datasets, CLOTH3D and CAPE, show that our model is robust, controllable in terms of detail generation by the use of multi-resolution pyramids, and achieves state-of-the-art results that can highly generalize to unseen garments, poses, and shapes even when training with small amounts of data. The code can be found at: https://github.com/HunorLaczko/pyramid-drape"
A Generic and Flexible Regularization Framework for NeRFs,"Thibaud Ehret, Roger Marí, Gabriele Facciolo","Université Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, 91190, Gif-sur-Yvette, France",100.0,France,0.0,,"Neural radiance fields, or NeRF, represent a breakthrough in the field of novel view synthesis and 3D modeling of complex scenes from multi-view image collections. Numerous recent works have shown the importance of making NeRF models more robust, by means of regularization, in order to train with possibly inconsistent and/or very sparse data. In this work, we explore how differential geometry can provide elegant regularization tools for robustly training NeRF-like models, which are modified so as to represent continuous and infinitely differentiable functions. In particular, we present a generic framework for regularizing different types of NeRFs observations to improve the performance in challenging conditions. We also show how the same formalism can also be used to natively encourage the regularity of surfaces by means of Gaussian or mean curvatures.",https://openaccess.thecvf.com/content/WACV2024/html/Ehret_A_Generic_and_Flexible_Regularization_Framework_for_NeRFs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ehret_A_Generic_and_Flexible_Regularization_Framework_for_NeRFs_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483730/,"['Geometry', 'Training', 'Deep learning', 'Computer vision', 'Three-dimensional displays', 'Buildings', 'Estimation']","['Neural Radiance Fields', 'Differentiable Function', 'Smooth Function', 'Mean Curvature', 'Differential Geometry', 'Gaussian Curvature', 'View Synthesis', 'Training Data', 'Convolutional Neural Network', 'Multilayer Perceptron', 'Regularization Term', 'Depth Map', '3D Point', 'Regularization Method', 'Implicit Function', 'Depth Estimation', 'Image Coordinates', '3D Scene', 'Pre-trained Convolutional Neural Network', 'Signed Distance Function', 'View Of The Scene', 'Multi-view Stereo', 'Set Of Views']","['Algorithms', '3D computer vision']",1,"Neural radiance fields, or NeRF, represent a breakthrough in the field of novel view synthesis and 3D modeling of complex scenes from multi-view image collections. Numerous recent works have shown the importance of making NeRF models more robust, by means of regularization, in order to train with possibly inconsistent and/or very sparse data. In this work, we explore how differential geometry can provide elegant regularization tools for robustly training NeRF-like models, which are modified so as to represent continuous and infinitely differentiable functions. In particular, we present a generic framework for regularizing different types of NeRFs observations to improve the performance in challenging conditions. We also show how the same formalism can also be used to natively encourage the regularity of surfaces by means of Gaussian or mean curvatures."
A Geometry Loss Combination for 3D Human Pose Estimation,"Ai Matsune, Shichen Hu, Guangquan Li, Sihan Wen, Xiantan Zhu, Zhiming Tan","Fujitsu R&D Center Co., Ltd.",0.0,,100.0,Japan,"Root-relative loss has formed the basis of 3D human pose estimation for many years. However, this point-to-point loss treats every keypoint separately and ignores internal connection information of the human body. This leads to illegal pose prediction, which humans cannot form in the real world. It also suffers from differences in estimation difficulty between keypoints. The farther the keypoint is from the torso, the less accurate it is. To address the above problems, this paper proposes geometry loss combination to utilize the geometric relationship between each keypoint fully. This loss combination consists of three loss functions: root-relative pose, bone length, and body part orientation. The previous two have already been used in prior works. Beyond them, we further develop a loss function called body part orientation loss for local body parts. Intuitively, the human body can be divided into three parts: the head, torso, and limbs. Based on this, we select the corresponding keypoints and create virtual planes for each body part. Experiments with different datasets and models demonstrate that our proposed method improves the prediction accuracy. We also achieve MPJPE of 65.0 on the 3DPW test set, which outperforms state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Matsune_A_Geometry_Loss_Combination_for_3D_Human_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Matsune_A_Geometry_Loss_Combination_for_3D_Human_Pose_Estimation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483990/,"['Geometry', 'Torso', 'Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Biological system modeling', 'Pose estimation']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Pose', '3D Human Pose', 'Loss Function', 'Body Parts', 'Geometric Relationship', 'Bone Length', 'Data Augmentation', 'Estimation Performance', 'Normal Vector', 'Area Under Curve', 'Joint Position', 'Manhattan Distance', 'Depth Estimation', 'L1 Loss', 'Conduct Ablation Studies', 'Loss Of Symmetry', 'Adjacent Joints']","['Algorithms', '3D computer vision', 'Algorithms', 'Video recognition and understanding']",,"Root-relative loss has formed the basis of 3D human pose estimation for many years. However, this point-to-point loss treats every keypoint separately and ignores internal connection information of the human body. This leads to illegal pose prediction, which humans cannot form in the real world. It also suffers from differences in estimation difficulty between keypoints. The farther the keypoint is from the torso, the less accurate it is. To address the above problems, this paper proposes geometry loss combination to utilize the geometric relationship between each keypoint fully. This loss combination consists of three loss functions: root-relative pose, bone length, and body part orientation. The previous two have already been used in prior works. Beyond them, we further develop a loss function called body part orientation loss for local body parts. Intuitively, the human body can be divided into three parts: the head, torso, and limbs. Based on this, we select the corresponding keypoints and create virtual planes for each body part. Experiments with different datasets and models demonstrate that our proposed method improves the prediction accuracy. We also achieve MPJPE of 65.0 on the 3DPW test set, which outperforms state-of-the-art methods."
A Hybrid Graph Network for Complex Activity Detection in Video,"Salman Khan, Izzeddin Teeti, Andrew Bradley, Mohamed Elhoseiny, Fabio Cuzzolin",Oxford Brookes University; KAUST,100.0,"Saudi Arabia, UK",0.0,,"Interpretation and understanding of video presents a challenging computer vision task in numerous fields - e.g. autonomous driving and sports analytics. Existing approaches to interpreting the actions taking place within a video clip are based upon Temporal Action Localisation (TAL), which typically identifies short-term actions. The emerging field of Complex Activity Detection (CompAD) extends this analysis to long-term activities, with a deeper understanding obtained by modelling the internal structure of a complex activity taking place within the video. We address the CompAD problem using a hybrid graph neural network which combines attention applied to a graph encoding the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Our approach is as follows: i) Firstly, we propose a novel feature extraction technique which, for each video snippet, generates spatiotemporal 'tubes' for the active elements ('agents') in the (local) scene by detecting individual objects, tracking them and then extracting 3D features from all the agent tubes as well as the overall scene. ii) Next, we construct a local scene graph where each node (representing either an agent tube or the scene) is connected to all other nodes. Attention is then applied to this graph to obtain an overall representation of the local dynamic scene. iii) Finally, all local scene graph representations are interconnected via a temporal graph, to estimate the complex activity class together with its start and end time. The proposed framework outperforms all previous state-of-the-art methods on all three datasets including ActivityNet-1.3, Thumos-14, and ROAD.",https://openaccess.thecvf.com/content/WACV2024/html/Khan_A_Hybrid_Graph_Network_for_Complex_Activity_Detection_in_Video_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Khan_A_Hybrid_Graph_Network_for_Complex_Activity_Detection_in_Video_WACV_2024_paper.pdf,,,2310.17493,main,Poster,https://ieeexplore.ieee.org/document/10484343/,"['Training', 'Computer vision', 'Analytical models', 'Uncertainty', 'Three-dimensional displays', 'Roads', 'Feature extraction']","['Activity Of Complex', 'Detection In Videos', 'Hybrid Graph', 'Structure Activity', 'End Time', 'Start Time', 'Autonomous Vehicles', 'Graph Neural Networks', '3D Features', 'Numerous Fields', 'Temporal Localization', 'Dynamic Scenes', 'Local Graph', 'Local Scene', 'Temporal Graph', 'Scene Graph', 'Feature Representation', 'Intersection Over Union', 'Bounding Box', 'Final Feature', 'Graph Attention Network', 'Graph Convolutional Network', 'Intersection Over Union Threshold', 'One-stage Methods', 'Global Graph', 'Feature Aggregation', 'COCO Dataset', 'Graph-based Methods', 'Star Topology', 'Prior Art']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving']",,"Interpretation and understanding of video presents a challenging computer vision task in numerous fields - e.g. autonomous driving and sports analytics. Existing approaches to interpreting the actions taking place within a video clip are based upon Temporal Action Localisation (TAL), which typically identifies short-term actions. The emerging field of Complex Activity Detection (CompAD) extends this analysis to long-term activities, with a deeper understanding obtained by modelling the internal structure of a complex activity taking place within the video.We address the CompAD problem using a hybrid graph neural network which combines attention applied to a graph encoding the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Our approach is as follows: i) Firstly, we propose a novel feature extraction technique which, for each video snippet, generates spatiotemporal ‘tubes’ for the active elements (‘agents’) in the (local) scene by detecting individual objects, tracking them and then extracting 3D features from all the agent tubes as well as the overall scene. ii) Next, we construct a local scene graph where each node (representing either an agent tube or the scene) is connected to all other nodes. Attention is then applied to this graph to obtain an overall representation of the local dynamic scene. iii) Finally, all local scene graph representations are interconnected via a temporal graph, to estimate the complex activity class together with its start and end time.The proposed framework outperforms all previous state-of-the-art methods on all three datasets including ActivityNet-1.3, Thumos-14, and ROAD."
A Multimodal Benchmark and Improved Architecture for Zero Shot Learning,"Keval Doshi, Amanmeet Garg, Burak Uzkent, Xiaolong Wang, Mohamed Omar",Amazon Prime Video,0.0,,100.0,USA,"In this work, we demonstrate that due to the inadequacies in the existing evaluation protocols and datasets, there is a need to revisit and comprehensively examine the multimodal Zero-Shot Learning (MZSL) problem formulation. Specifically, we address two major challenges faced by current MZSL approaches; (1) Established baselines are frequently incomparable and occasionally even flawed since existing evaluation datasets often have some overlap with the training dataset, thus violating the zero-shot paradigm; (2) Most existing methods are biased towards seen classes, which significantly reduces the performance when evaluated on both seen and unseen classes. To address these challenges, we first introduce a new multimodal dataset for zero-shot evaluation called MZSL-50 with 4462 videos from 50 widely diversified classes and no overlap with the training data. Further, we propose a novel multimodal zeroshot transformer (MZST) architecture that leverages attention bottlenecks for multimodal fusion. Our model directly predicts the semantic representation and is superior at reducing the bias towards seen classes. We conduct extensive ablation studies, and achieve state-of-the-art results on three benchmark datasets and our novel MZSL-50 dataset. Specifically, we improve the conventional MZSL performance by a margin of 2.1%, 9.81% and 8.68% on VGGSound, UCF-101 and ActivityNet, respectively. Finally, we expect the introduction of the MZSL-50 dataset will promote the future in-depth research on multimodal zero-shot learning in the community.",https://openaccess.thecvf.com/content/WACV2024/html/Doshi_A_Multimodal_Benchmark_and_Improved_Architecture_for_Zero_Shot_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Doshi_A_Multimodal_Benchmark_and_Improved_Architecture_for_Zero_Shot_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484232/,"['Training', 'Protocols', 'Zero-shot learning', 'Semantics', 'Training data', 'Computer architecture', 'Benchmark testing']","['Multimodal Benchmark', 'Extensive Studies', 'Benchmark Datasets', 'Evaluation Protocol', 'Evaluation Dataset', 'Semantic Representations', 'Multimodal Learning', 'Transformer Architecture', 'Unseen Classes', 'Training Set', 'Visual Features', 'Recent Approaches', 'Representation Learning', 'Semantic Similarity', 'Harmonic Mean', 'Action Recognition', 'Cross-modal', 'Hierarchical Representation', 'Video Features', 'Conventional Setup', 'Semantic Embedding', 'Masked Language Model', 'Action Recognition Model']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Datasets and evaluations']",,"In this work, we demonstrate that due to the inadequacies in the existing evaluation protocols and datasets, there is a need to revisit and comprehensively examine the multimodal Zero-Shot Learning (MZSL) problem formulation. Specifically, we address two major challenges faced by current MZSL approaches; (1) Established baselines are frequently incomparable and occasionally even flawed since existing evaluation datasets often have some overlap with the training dataset, thus violating the zero-shot paradigm; (2) Most existing methods are biased towards seen classes, which significantly reduces the performance when evaluated on both seen and unseen classes. To address these challenges, we first introduce a new multimodal dataset for zero-shot evaluation called MZSL-50 with 4462 videos from 50 widely diversified classes and no overlap with the training data. Further, we propose a novel multimodal zero-shot transformer (MZST) architecture that leverages attention bottlenecks for multimodal fusion. Our model directly predicts the semantic representation and is superior at reducing the bias towards seen classes. We conduct extensive ablation studies, and achieve state-of-the-art results on three benchmark datasets and our novel MZSL-50 dataset. Specifically, we improve the conventional MZSL performance by a margin of 2.1%, 9.81% and 8.68% on VGG-Sound, UCF-101 and ActivityNet, respectively. Finally, we expect the introduction of the MZSL-50 dataset will promote the future in-depth research on multimodal zero-shot learning in the community. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
A Neural Height-Map Approach for the Binocular Photometric Stereo Problem,"Fotios Logothetis, Ignas Budvytis, Roberto Cipolla","Cambridge Research Laboratory, Toshiba Europe Ltd., Cambridge, UK; University of Cambridge, Cambridge, UK",100.0,UK,0.0,,"In this work we propose a novel, highly practical, binocular photometric stereo (PS) framework, which has same acquisition speed as single view PS, however significantly improves the quality of the estimated geometry. As in recent neural multi-view shape estimation frameworks such as NeRF, SIREN and inverse graphics approach to multi-view photometric stereo (e.g. PS-NeRF) we formulate shape estimation task as learning of a differentiable surface and texture representation by minimising surface normal discrepancy for normals estimated from multiple varying light images for two views as well as discrepancy between rendered surface intensity and observed images. Our method differs from typical multi-view shape estimation approaches in two key ways. First, our surface is represented not as a volume but as a neural heightmap where heights of points on a surface are computed by a deep neural network. Second, instead of predicting an average intensity as PS-NeRF or introducing lambertian material assumptions as Guo et al., we use a learnt BRDF and perform near-field per point intensity rendering. Our method achieves the state-of-the-art performance on the DiLiGenT-MV dataset adapted to binocular stereo setup as well as a new binocular photometric stereo dataset - LUCES-ST.",https://openaccess.thecvf.com/content/WACV2024/html/Logothetis_A_Neural_Height-Map_Approach_for_the_Binocular_Photometric_Stereo_Problem_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Logothetis_A_Neural_Height-Map_Approach_for_the_Binocular_Photometric_Stereo_Problem_WACV_2024_paper.pdf,,,2311.05958,main,Poster,https://ieeexplore.ieee.org/document/10484088/,"['Computer vision', 'Shape', 'Estimation', 'Artificial neural networks', 'Benchmark testing', 'Rendering (computer graphics)', 'Surface texture']","['Photometric Stereo', 'Photometric Stereo Problem', 'Neural Network', 'Deep Neural Network', 'Light Images', 'Normal Approximation', 'Shape Estimation', 'Single View', 'Height Of Point', 'Surface Normals', 'Multi-view Stereo', 'Light Source', 'Coordinate System', 'Self-reflection', 'Data Augmentation', 'Intensive Use', 'Structured Illumination', 'Squirrels', 'Depth Estimation', 'Surface Points', 'Normal Map', 'Height Map', 'Stereo Images', 'Hausdorff Distance']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', '3D computer vision', 'Applications', 'Remote Sensing']",,"In this work we propose a novel, highly practical, binocular photometric stereo (PS) framework, which has same acquisition speed as single view PS, however significantly improves the quality of the estimated geometry.As in recent neural multi-view shape estimation frameworks such as NeRF [29], SIREN [35] and inverse graphics approaches to multi-view photometric stereo (e.g. PS-NeRF [38]) we formulate shape estimation task as learning of a differentiable surface and texture representation by minimising surface normal discrepancy for normals estimated from multiple varying light images for two views as well as discrepancy between rendered surface intensity and observed images. Our method differs from typical multi-view shape estimation approaches in two key ways. First, our surface is represented not as a volume but as a neural heightmap where heights of points on a surface are computed by a deep neural network. Second, instead of predicting an average intensity as PS-NeRF or introducing lambertian material assumptions as Guo et al. [7], we use a learnt BRDF and perform near-field per point intensity rendering.Our method achieves the state-of-the-art performance on the DiLiGenT-MV dataset adapted to binocular stereo setup as well as a new binocular photometric stereo dataset - LUCES-ST."
A One-Shot Learning Approach To Document Layout Segmentation of Ancient Arabic Manuscripts,"Axel De Nardin, Silvia Zottin, Claudio Piciarelli, Emanuela Colombi, Gian Luca Foresti","University of Udine, Udine, Italy",100.0,Italy,0.0,,"Document layout segmentation is a challenging task due to the variability and complexity of document layouts. Ancient manuscripts in particular are often damaged by age, have very irregular layouts, and are characterized by progressive editing from different authors over a large time window. All these factors make the semantic segmentation process of specific areas, such as main text and side text, very difficult. However, the study of these manuscripts turns out to be fundamental for historians and humanists, so much so that in recent years the demand for machine learning approaches aimed at simplifying the extraction of information from these documents has consistently increased, leading document layout analysis to become an increasingly important research area. In order for machine learning techniques to be applied effectively to this task, however, a large amount of correctly and precisely labeled images is required for their training. This is obviously a limitation for this field of research as ground truth must be precisely and manually crafted by expert humanists, making it a very time-consuming process. In this paper, with the aim of overcoming this limitation, we present an efficient document layout segmentation framework, which while being trained on only one labeled page per manuscript still achieves state-of-the-art performance compared to other popular approaches trained on all the available data when tested on a challenging dataset of ancient Arabic manuscripts.",https://openaccess.thecvf.com/content/WACV2024/html/De_Nardin_A_One-Shot_Learning_Approach_To_Document_Layout_Segmentation_of_Ancient_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/De_Nardin_A_One-Shot_Learning_Approach_To_Document_Layout_Segmentation_of_Ancient_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484518/,"['Training', 'Text analysis', 'Semantic segmentation', 'Layout', 'Machine learning', 'Complexity theory', 'Task analysis']","['Ancient Manuscripts', 'Arabic Manuscript', 'Main Text', 'Time-consuming Process', 'Semantic Segmentation', 'Main Side', 'High Performance', 'False Negative', 'Precision And Recall', 'Training Epochs', 'Dynamic Mode', 'Segmentation Accuracy', 'Segmentation Task', 'Competition Model', 'Adaptive Framework', 'Validation Loss', 'Optical Character Recognition', 'Segmentation Problem', 'Popular Datasets', 'Portion Of The Image', 'Dice Loss', 'Siamese Network', 'Segmentation Prediction', 'Segment Classification', 'True Positive Predictions', 'Backbone Segments']","['Applications', 'Education', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Document layout segmentation is a challenging task due to the variability and complexity of document layouts. Ancient manuscripts in particular are often damaged by age, have very irregular layouts, and are characterized by progressive editing from different authors over a large time window. All these factors make the semantic segmentation process of specific areas, such as main text and side text, very difficult. However, the study of these manuscripts turns out to be fundamental for historians and humanists, so much so that in recent years the demand for machine learning approaches aimed at simplifying the extraction of information from these documents has consistently increased, leading document layout analysis to become an increasingly important research area. In order for machine learning techniques to be applied effectively to this task, however, a large amount of correctly and precisely labeled images is required for their training. This is obviously a limitation for this field of research as ground truth must be precisely and manually crafted by expert humanists, making it a very time-consuming process. In this paper, with the aim of overcoming this limitation, we present an efficient document layout segmentation framework, which while being trained on only one labeled page per manuscript still achieves state-of-the-art performance compared to other popular approaches trained on all the available data when tested on a challenging dataset of ancient Arabic manuscripts."
A Robust Diffusion Modeling Framework for Radar Camera 3D Object Detection,"Zizhang Wu, Yunzhe Wu, Xiaoquan Wang, Yuanzhu Gan, Jian Pu",ZongmuTech; Fudan University; ExploAI,33.33333333333333,China,66.66666666666667,China,"Radar-camera 3D object detection aims at interacting radar signals with camera images for identifying objects of interest and localizing their corresponding 3D bounding boxes. To overcome the severe sparsity and ambiguity of radar signals, we propose a robust framework based on probabilistic denoising diffusion modeling. We design our framework to be easily implementable on different multi-view 3D detectors without the requirement of using LiDAR point clouds during either the training or inference. In specific, we first design our framework with a denoised radar-camera encoder via developing a lightweight denoising diffusion model with semantic embedding. Secondly, we develop the query denoising training into 3D space via introducing the reconstruction training at depth measurement for the transformer detection decoder. Our framework achieves new state-of-the-art performance on the nuScenes 3D detection benchmark but with few computational cost increases compared to the baseline detectors.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_A_Robust_Diffusion_Modeling_Framework_for_Radar_Camera_3D_Object_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_A_Robust_Diffusion_Modeling_Framework_for_Radar_Camera_3D_Object_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484259/,"['Training', 'Three-dimensional displays', 'Noise reduction', 'Semantics', 'Radar detection', 'Detectors', 'Radar imaging']","['Object Detection', 'Diffusion Model', '3D Object Detection', 'Sparsity', 'Point Cloud', '3D Space', 'Bounding Box', 'Depth Measurements', 'Radar Signal', '3D Detection', 'LiDAR Point Clouds', '3D Bounding Box', 'Semantic Embedding', 'Image Features', 'Diffusion Process', 'Detection Task', 'Positive Features', 'Velocity Measurements', 'Semantic Features', 'Feature Fusion', 'Transformer Decoder', 'Multi-view Images', 'Radar Measurements', 'Multimodal Model', 'Radar Sensor', 'Attention Operation', 'Object Detection Task', 'Calibration Information', 'Validation Split', 'Visual Perspective']","['Algorithms', '3D computer vision', 'Applications', 'Autonomous Driving']",2,"Radar-camera 3D object detection aims at interacting radar signals with camera images for identifying objects of interest and localizing their corresponding 3D bounding boxes. To overcome the severe sparsity and ambiguity of radar signals, we propose a robust framework based on probabilistic denoising diffusion modeling. We design our framework to be easily implementable on different multi-view 3D detectors without the requirement of using LiDAR point clouds during either the training or inference. In specific, we first design our framework with a denoised radar-camera encoder via developing a lightweight denoising diffusion model with semantic embedding. Secondly, we develop the query denoising training into 3D space via introducing the reconstruction training at depth measurement for the transformer detection decoder. Our framework achieves new state-of-the-art performance on the nuScenes 3D detection benchmark but with few computational cost increases compared to the baseline detectors."
A Sequential Learning-Based Approach for Monocular Human Performance Capture,"Jianchun Chen, Jayakorn Vongkulbhisal, Fernando De la Torre Frade","Robotics Institute, Carnegie Mellon University; Independent Researcher",50.0,USA,50.0,,"Human performance capture from RGB videos in unconstrained environments has become very popular for applications that require generating virtual avatars or digital actors. SOTA methods use neural network (NN) techniques to estimate the shape directly from photos, yielding a simplified model of the human body. While effective, NN techniques frequently fail under challenging poses and do not preserve temporal consistency. On the other hand, optimization-based methods like shape-from-silhouette can produce more precise reconstruction; however, they typically require a good initialization and are computationally more intensive than NN. To address issues of previous methods, this work proposes a learning-based approach for optimizing fine-grained shape representation (e.g., clothes, wrinkles) from a monocular RGB video. Our main idea is to sequentially recover different shape details (e.g., average shape, clothing, wrinkles) using separate neural networks. At each level, our network takes the sparse/noisy gradients of body mesh vertices w.r.t the shape, and predicts dense gradients to update the body shape. Despite being trained on synthetic data, these networks have surprisingly good generalization to real images. Experimental validation shows that our approach outperforms NN approaches in recovering shape details while also being an order of magnitude faster than optimization-based methods and robust across varied poses and novel views.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_A_Sequential_Learning-Based_Approach_for_Monocular_Human_Performance_Capture_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_A_Sequential_Learning-Based_Approach_for_Monocular_Human_Performance_Capture_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484273/,"['Three-dimensional displays', 'Shape', 'Deformation', 'Computational modeling', 'Clothing', 'Artificial neural networks', 'Robustness']","['Human Performance Capture', 'Neural Network', 'Body Shape', 'Optimization-based Methods', 'Average Shape', 'Detailed Shape', 'Training Data', 'Deep Neural Network', 'Training Phase', 'Learning-based Methods', 'Temporal Sequence', '3D Scanning', 'Inference Time', '3D Shape', 'Sequence Of Frames', 'Pose Estimation', 'Implicit Function', 'Shape Estimation', 'Human Pose Estimation', 'Surface Normals', 'Human 3D', 'Normal Map', 'Human Shape', '3D Human Model', 'Body Pose', 'Camera Intrinsics', 'Gradient Descent', 'Global Translation', 'Input Image', 'Frontal View']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"Human performance capture from RGB videos in unconstrained environments has become very popular for applications that require generating virtual avatars or digital actors. SOTA methods use neural network (NN) techniques to estimate the shape directly from photos, yielding a simplified model of the human body. While effective, NN techniques frequently fail under challenging poses and do not preserve temporal consistency. On the other hand, optimization-based methods like shape-from-silhouette can produce more precise reconstruction; however, they typically require a good initialization and are computationally more intensive than NN. To address issues of previous methods, this work proposes a learning-based approach for optimizing fine-grained shape representation from a monocular RGB video. Our main idea is to sequentially recover different shape details (i.e. average shape, clothing, wrinkles) using separate neural networks. At each level, our network takes the sparse/noisy gradients of body mesh vertices w.r.t. the shape, and predicts dense gradients to update the body shape. Despite being trained on synthetic data, these networks have surprisingly good generalization to real images. Experimental validation shows that our approach outperforms NN approaches in recovering shape details while also being an order of magnitude faster than optimization-based methods and robust across varied poses and novel views."
A Visual Active Search Framework for Geospatial Exploration,"Anindya Sarkar, Michael Lanier, Scott Alfeld, Jiarui Feng, Roman Garnett, Nathan Jacobs, Yevgeniy Vorobeychik","Department of Computer Science & Engineering, Washington University in St. Louis; Department of Computer Science, Amherst College",100.0,USA,0.0,,"Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which has three key inputs: (1) an image of the entire search area, which is subdivided into regions, (2) a local search function, which determines whether a previously unseen object class is present in a given region, and (3) a fixed search budget, which limits the number of times the local search function can be evaluated. The goal is to maximize the number of objects found within the search budget. We propose a reinforcement learning approach for VAS that learns a meta-search policy from a collection of fully annotated search tasks. This meta-search policy is then used to dynamically search for a novel target-object class, leveraging the outcome of any previous queries to determine where to query next. Through extensive experiments on several large-scale satellite imagery datasets, we show that the proposed approach significantly outperforms several strong baselines. We also propose novel domain adaptation techniques that improve the policy at decision time when there is a significant domain gap with the training data. Code is publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Sarkar_A_Visual_Active_Search_Framework_for_Geospatial_Exploration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sarkar_A_Visual_Active_Search_Framework_for_Geospatial_Exploration_WACV_2024_paper.pdf,,https://github.com/... (link not provided in the text),2211.15788,main,Poster,https://ieeexplore.ieee.org/document/10484136/,"['Visualization', 'Training data', 'Reinforcement learning', 'Search problems', 'Distance measurement', 'Geospatial analysis', 'Metasearch']","['Visual Search', 'Active Search', 'Training Data', 'Aerial Images', 'Decision Time', 'Search Task', 'Human Trafficking', 'Domain Adaptation Techniques', 'Active Learning', 'Training Time', 'Grid Cells', 'Domain Shift', 'Broad Areas', 'Target Class', 'Target Object', 'Search Queries', 'Advantage Of Information', 'Inference Time', 'Markov Decision Process', 'Policy Network', 'Image X', 'Large Vehicles', 'Low-dimensional Feature', 'Update Frequency', 'Greedy Selection', 'Greedy Policy', 'Search Problem', 'Building Roof', 'Policy Parameters', 'Machine Vision']","['Applications', 'Remote Sensing', 'Applications', 'Social good']",,"Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which has three key inputs: (1) an image of the entire search area, which is subdivided into regions, (2) a local search function, which determines whether a previously unseen object class is present in a given region, and (3) a fixed search budget, which limits the number of times the local search function can be evaluated. The goal is to maximize the number of objects found within the search budget. We propose a reinforcement learning approach for VAS that learns a meta-search policy from a collection of fully annotated search tasks. This meta-search policy is then used to dynamically search for a novel target-object class, leveraging the outcome of any previous queries to determine where to query next. Through extensive experiments on several large-scale satellite imagery datasets, we show that the proposed approach significantly outperforms several strong baselines. We also propose novel domain adaptation techniques that improve the policy at decision time when there is a significant domain gap with the training data. Code is publicly available at this link."
A*: Atrous Spatial Temporal Action Recognition for Real Time Applications,"Myeongjun Kim, Federica Spinola, Philipp Benz, Tae-hoon Kim","Deeping Source Inc., Seoul, Republic of Korea",0.0,,100.0,South Korea,"Deep learning has become a popular tool across various fields and is increasingly being integrated into real-world applications such as autonomous driving cars and surveillance cameras. One area of active research is recognizing human actions, including identifying unsafe or abnormal behaviors. Temporal information is crucial for action recognition tasks. Global context, as well as the target person, are also important for judging human behaviors. However, larger networks that can capture all of these features face difficulties operating in real-time. To address these issues, we propose A*: Atrous Spatial Temporal Action Recognition for Real Time Applications. A* includes four modules aimed at improving action detection networks. First, we introduce a Low-Level Feature Aggregation module. Second, we propose the Atrous Spatio-Temporal Pyramid Pooling module. Third, we suggest to fuse all extracted image and video features in an Image-Video Feature Fusion module. Finally, we integrate the Proxy Anchor Loss for action features into the loss function. We evaluate A* on three common action detection benchmarks, and achieve state-of-the-art performance on JHMDB and UCF101-24, while staying competitive on AVA. Furthermore, we demonstrate that A* can achieve real-time inference speeds of 33 FPS, making it suitable for real-world applications.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_A_Atrous_Spatial_Temporal_Action_Recognition_for_Real_Time_Applications_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_A_Atrous_Spatial_Temporal_Action_Recognition_for_Real_Time_Applications_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
AFTer-SAM: Adapting SAM With Axial Fusion Transformer for Medical Imaging Segmentation,"Xiangyi Yan, Shanlin Sun, Kun Han, Thanh-Tung Le, Haoyu Ma, Chenyu You, Xiaohui Xie","University of California, Irvine; Yale University",100.0,USA,0.0,,"The Segmentation Anything Model (SAM) has demonstrated effectiveness in various segmentation tasks. However, its application to 3D medical data has posed challenges due to its inherent design for both 2D and natural images. While there have been attempts to apply SAM to medical images on a slice-by-slice basis, the outcomes have been less than optimal. In this study, we introduce AFTer-SAM, an adaptation of SAM designed for volumetric medical image segmentation. By incorporating an Axial Fusion Transformer, AFTer-SAM is capable of capturing both intra-slice details and inter-slice contextual information, essential for accurate medical image segmentation. Given the potential computational challenges of training this enhanced model, we utilize Low-Rank Adaptation (LoRA) to efficiently fine-tune the weights of the Axial Fusion Transformer. This ensures a streamlined training process without compromising on performance. Our results indicate that AFTer-SAM offers significant improvements in volumetric medical image segmentation, suggesting a promising direction for the application of large pre-trained models in medical imaging.",https://openaccess.thecvf.com/content/WACV2024/html/Yan_AFTer-SAM_Adapting_SAM_With_Axial_Fusion_Transformer_for_Medical_Imaging_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yan_AFTer-SAM_Adapting_SAM_With_Axial_Fusion_Transformer_for_Medical_Imaging_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483865/,"['Training', 'Image segmentation', 'Adaptation models', 'Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Streaming media']","['Medical Imaging', 'Medical Image Segmentation', 'Segmentation Task', 'Convolutional Neural Network', 'Feature Maps', 'Multilayer Perceptron', 'Trainable Parameters', '3D Scanning', 'Left Lung', 'Dice Similarity Coefficient', 'Self-supervised Learning', 'Single Slice', 'Information Fusion', 'Long-range Dependencies', 'Axial Axis', '3D Segmentation', 'Medical Datasets', 'Transformer Architecture', 'Individual Slices', 'Attention Block', 'Attention Heads', 'Medical Image Datasets', 'In-house Dataset', 'Linear Projection', 'Primary Consideration', 'Visual Representation', 'Segmentation Map', 'Computational Demands', 'Visual Capabilities']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"The Segmentation Anything Model (SAM) has demonstrated effectiveness in various segmentation tasks. However, its application to 3D medical data has posed challenges due to its inherent design for both 2D and natural images. While there have been attempts to apply SAM to medical images on a slice-by-slice basis, the outcomes have been less than optimal. In this study, we introduce AFTer-SAM, an adaptation of SAM designed for volumetric medical image segmentation. By incorporating an Axial Fusion Transformer, AFTer-SAM is capable of capturing both intra-slice details and inter-slice contextual information, essential for accurate medical image segmentation. Given the potential computational challenges of training this enhanced model, we utilize Low-Rank Adaptation (LoRA) to efficiently finetune the weights of the Axial Fusion Transformer. This ensures a streamlined training process without compromising on performance. Our results indicate that AFTer-SAM offers significant improvements in volumetric medical image segmentation, suggesting a promising direction for the application of large pre-trained models in medical imaging."
AMEND: Adaptive Margin and Expanded Neighborhood for Efficient Generalized Category Discovery,"Anwesha Banerjee, Liyana Sahir Kallooriyakath, Soma Biswas","Indian Institute of Science, Bengaluru",100.0,India,0.0,,"Generalized Category Discovery aims to discover and cluster images from previously unseen classes, in addition to classifying images from seen classes correctly. In this work, we propose a simple, yet effective framework for this task, which not only performs on-par or better with the current approaches but is also significantly more efficient in terms of computational requirements. Our first contribution is to use expanded neighborhood information in contrastive learning to generate robust and generalizable features. To generate more discriminative feature representations, especially for fine-grained datasets and confusing classes, we propose a class-wise adaptive margin regularizer that aims at increasing the angular separation among the prototypes of all classes. Extensive experiments on three generic as well as four fine-grained benchmark datasets show the usefulness of the proposed Adaptive Margin and Expanded Neighborhood (AMEND) framework.",https://openaccess.thecvf.com/content/WACV2024/html/Banerjee_AMEND_Adaptive_Margin_and_Expanded_Neighborhood_for_Efficient_Generalized_Category_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Banerjee_AMEND_Adaptive_Margin_and_Expanded_Neighborhood_for_Efficient_Generalized_Category_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483957/,"['Training', 'Computer vision', 'Memory management', 'Prototypes', 'Self-supervised learning', 'Benchmark testing', 'Hardware']","['Adaptive Margin', 'Category Discovery', 'Feature Representation', 'Robust Features', 'Self-supervised Learning', 'Neighborhood Information', 'Angular Distance', 'Class Prototypes', 'Discriminative Feature Representation', 'Unseen Classes', 'Semantic', 'Feature Space', 'Data Augmentation', 'Transfer Learning', 'Recent Approaches', 'Multilayer Perceptron', 'Representation Learning', 'Weight Coefficient', 'Set Of Classes', 'Unlabeled Data', 'Unlabeled Set', 'Vision Transformer', 'Performance In Dataset', 'Contrastive Loss', 'Final Loss', 'Representative Class', 'Affinity Values', 'Classification Head', 'Imbalanced Data']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Generalized Category Discovery aims to discover and cluster images from previously unseen classes, in addition to classifying images from seen classes correctly. In this work, we propose a simple, yet effective framework for this task, which not only performs on-par or better with the current approaches but is also significantly more efficient in terms of computational requirements. Our first contribution is to use expanded neighborhood information in contrastive learning to generate robust and generalizable features. To generate more discriminative feature representations, especially for fine-grained datasets and confusing classes, we propose a class-wise adaptive margin regularizer that aims at increasing the angular separation among the prototypes of all classes. Extensive experiments on three generic as well as four fine-grained benchmark datasets show the usefulness of the proposed Adaptive Margin and Expanded Neighborhood (AMEND) framework."
ARNIQA: Learning Distortion Manifold for Image Quality Assessment,"Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo","University of Florence - Media Integration and Communication Center (MICC), Florence, Italy",100.0,Italy,0.0,,"No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Thus, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/miccunifi/ARNIQA.",https://openaccess.thecvf.com/content/WACV2024/html/Agnolucci_ARNIQA_Learning_Distortion_Manifold_for_Image_Quality_Assessment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Agnolucci_ARNIQA_Learning_Distortion_Manifold_for_Image_Quality_Assessment_WACV_2024_paper.pdf,,https://github.com/miccunifi/ARNIQA,2310.14918,main,Poster,https://ieeexplore.ieee.org/document/10483567/,"['Manifolds', 'Image quality', 'Degradation', 'Training', 'Crops', 'Image representation', 'Distortion']","['Image Quality', 'Reference Image', 'Generalization Capability', 'Image Distortion', 'Image Representation', 'Degradation Pattern', 'Image Degradation', 'Self-supervised Approach', 'Training Set', 'Learning Process', 'Training Dataset', 'Batch Size', 'Intensity Levels', 'Training Strategy', 'Projector', 'Image Pairs', 'Unlabeled Data', 'Training Examples', 'Self-supervised Learning', 'Distortion Types', 'Mean Opinion Score', 'Contrastive Loss', 'Material For More Details', 'Typical Degradation', 'Batch Of Images', 'Image Embedding']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",4,"No-Reference Image Quality Assessment (NR-IQA) aims to develop methods to measure image quality in alignment with human perception without the need for a high-quality reference image. In this work, we propose a self-supervised approach named ARNIQA (leArning distoRtion maNifold for Image Quality Assessment) for modeling the image distortion manifold to obtain quality representations in an intrinsic manner. First, we introduce an image degradation model that randomly composes ordered sequences of consecutively applied distortions. In this way, we can synthetically degrade images with a large variety of degradation patterns. Second, we propose to train our model by maximizing the similarity between the representations of patches of different images distorted equally, despite varying content. Therefore, images degraded in the same manner correspond to neighboring positions within the distortion manifold. Finally, we map the image representations to the quality scores with a simple linear regressor, thus without fine-tuning the encoder weights. The experiments show that our approach achieves state-of-the-art performance on several datasets. In addition, ARNIQA demonstrates improved data efficiency, generalization capabilities, and robustness compared to competing methods. The code and the model are publicly available at https://github.com/miccunifi/ARNIQA."
ATS: Adaptive Temperature Scaling for Enhancing Out-of-Distribution Detection Methods,"Gerhard Krumpl, Henning Avenhaus, Horst Possegger, Horst Bischof","Institute of Computer Graphics and Vision, Graz University of Technology, Austria; KESTRELEYE GmbH, Austria",50.0,Austria,50.0,Austria,"Out-of-distribution (OOD) detection is essential to ensure the reliability and robustness of machine learning models in real-world applications. Post-hoc OOD detection methods have gained significant attention due to the fact that they offer the advantage of not requiring additional re-training, which could degrade model performance and increase training time. However, most existing post-hoc methods rely only on the encoder output (features), logits, or the softmax probability, meaning they have no access to information that might be lost in the feature extraction process. In this work, we address this limitation by introducing Adaptive Temperature Scaling (ATS), a novel approach that dynamically calculates a temperature value based on activations of the intermediate layers. Fusing this sample-specific adjustment with class-dependent logits, our ATS captures additional statistical information before they are lost in the feature extraction process, leading to a more robust and powerful OOD detection method. We conduct extensive experiments to demonstrate the efficacy of our approach. Notably, our method can be seamlessly combined with SOTA post-hoc OOD detection methods that rely on the logits, thereby enhancing their performance and improving their robustness.",https://openaccess.thecvf.com/content/WACV2024/html/Krumpl_ATS_Adaptive_Temperature_Scaling_for_Enhancing_Out-of-Distribution_Detection_Methods_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Krumpl_ATS_Adaptive_Temperature_Scaling_for_Enhancing_Out-of-Distribution_Detection_Methods_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483786/,"['Training', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Machine learning', 'Benchmark testing', 'Feature extraction']","['Version Of Scale', 'Access To Information', 'Intermediate Layer', 'Feature Extraction Process', 'Post-hoc Method', 'Softmax Probability', 'Neural Network', 'Training Set', 'Training Dataset', 'Deep Neural Network', 'False Positive Rate', 'Feature Maps', 'Scoring Function', 'Multiple Layers', 'Network Layer', 'Large-scale Datasets', 'Semantic Similarity', 'Baseline Methods', 'Temperature Parameters', 'Shallow Layers', 'iNaturalist', 'Penultimate Layer', 'Detection Score', 'ML Models', 'Intermediate Activity', 'Important Layer']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding']",,"Out-of-distribution (OOD) detection is essential to ensure the reliability and robustness of machine learning models in real-world applications. Post-hoc OOD detection methods have gained significant attention due to the fact that they offer the advantage of not requiring additional re-training, which could degrade model performance and increase training time. However, most existing post-hoc methods rely only on the encoder output (features), logits, or the softmax probability, meaning they have no access to information that might be lost in the feature extraction process. In this work, we address this limitation by introducing Adaptive Temperature Scaling (ATS), a novel approach that dynamically calculates a temperature value based on activations of the intermediate layers. Fusing this sample-specific adjustment with class-dependent logits, our ATS captures additional statistical information before they are lost in the feature extraction process, leading to a more robust and powerful OOD detection method. We conduct extensive experiments to demonstrate the efficacy of our approach. Notably, our method can be seamlessly combined with SOTA post-hoc OOD detection methods that rely on the logits, thereby enhancing their performance and improving their robustness."
AU-Aware Dynamic 3D Face Reconstruction From Videos With Transformer,"Chenyi Kuang, Jeffrey O. Kephart, Qiang Ji",Rensselaer Polytechnic Institute; IBM Thomas J. Watson Research Ctr.,50.0,USA,50.0,USA,"In spite of the significant progresses in monocular or multi-view image based 3D face reconstruction research, recovering 3D faces from videos, which contains rich dynamic information of facial motions, still remains as a highly challenging problem. First, most prior works fail to generate accurate and stable 3D faces on videos, especially for recovering subtle expression details. Furthermore, existing dynamic reconstruction approaches have not fully considered the temporal dependency of facial expression transitions, which is based on the dynamic muscle activation system under a local region of the skin. To tackle the aforementioned challenges, we present a framework for dynamic 3D face reconstruction from monocular videos, which can accurately recover 3D facial geometrical representations for facial action unit (AU). Specifically, we design a coarse-to-fine framework, where the ""coarse"" 3D face sequences are generated by a pre-trained static reconstruction model; and the ""refinement"" is performed through a Transformer-based network. We design 1) a Temporal Module used for modeling temporal dependency of facial motion dynamics; 2) an Spatial Module for modeling AU spatial correlations from geometry-based AU tokens; 3) feature fusion for simultaneous dynamic facial AU recognition and 3D expression capturing. Experimental results show the superiority of our method in generating AU-aware 3D face reconstruction sequences both quantitatively and qualitatively.",https://openaccess.thecvf.com/content/WACV2024/html/Kuang_AU-Aware_Dynamic_3D_Face_Reconstruction_From_Videos_With_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kuang_AU-Aware_Dynamic_3D_Face_Reconstruction_From_Videos_With_Transformer_WACV_2024_paper.pdf,,https://github.com/kuangcy1998/AU-D3DFace,,main,Poster,https://ieeexplore.ieee.org/document/10484097/,"['Gold', 'Solid modeling', 'Three-dimensional displays', 'Correlation', 'Face recognition', 'Dynamics', 'Transformers']","['Dynamic 3D', '3D Face', 'Dynamic Reconstruction', '3D Face Reconstruction', 'Facial Expressions', '3D Reconstruction', 'Expression Dynamics', 'Face Recognition', 'Spatial Module', 'Static Model', 'Temporal Dependencies', 'Reconstruction Model', 'Temporal Modulation', 'Facial Action', 'Action Units', 'Reconstruction Framework', 'Dynamic Facial Expressions', 'Final Model', 'Dynamic Model', 'F1 Score', '3D Mesh', 'Facial Action Coding System', '3D Scanning', '3D Data', 'Vision Transformer', 'Head Pose', 'Computer Vision Area', 'Point Cloud', '3D Point Cloud', 'Transformation Module']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Video recognition and understanding']",1,"In spite of the significant progresses in monocular or multi-view image based 3D face reconstruction research, recovering 3D faces from videos, which contains rich dynamic information of facial motions, still remains as a highly challenging problem. First, most prior works fail to generate accurate and stable 3D faces on videos, especially for recovering subtle expression details. Furthermore, existing dynamic reconstruction approaches have not fully considered the temporal dependency of facial expression transitions, which is based on the dynamic muscle activation system under a local region of the skin. To tackle the aforementioned challenges, we present a framework for dynamic 3D face reconstruction from monocular videos, which can accurately recover 3D facial geometrical representations for facial action unit (AU). Specifically, we design a coarse-to-fine framework, where the ""coarse"" 3D face sequences are generated by a pre-trained static reconstruction model; and the ""refinement"" is performed through a Transformer-based network. We design 1) a Temporal Module used for modeling temporal dependency of facial motion dynamics; 2) an Spatial Module for modeling AU spatial correlations from geometry-based AU tokens; 3) feature fusion for simultaneous dynamic facial AU recognition and 3D expression capturing. Experimental results show the superiority of our method in generating AU-aware 3D face reconstruction sequences both quantitatively and qualitatively."
Active Batch Sampling for Multi-Label Classification With Binary User Feedback,"Debanjan Goswami, Shayok Chakraborty","Department of Computer Science, Florida State University",100.0,USA,0.0,,"Multi-label classification is a generalization of multi-class classification, where a single data sample can have multiple labels. While deep neural networks have depicted commendable performance for multi-label learning, they require a large amount of manually annotated training data to attain good generalization capability. However, annotating a multi-label data sample requires a human oracle to consider the presence/absence of every single class individually, which is extremely laborious. Active learning algorithms automatically identify the salient and exemplar instances from large amounts of unlabeled data and are effective in reducing human annotation effort in inducing a machine learning model. In this paper, we propose a novel active learning framework for multi-label learning, which queries a batch of (image-label) pairs and for each pair, poses the question whether the queried label is present in the corresponding image; the human annotators merely need to provide a binary feedback (yes / no) in response to each query, which involves much less manual work. We pose the image and label selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of (image-label) pairs, which are maximally informative to the underlying deep neural network. Our extensive empirical studies on three challenging datasets corroborate the potential of our method for real-world multi-label classification applications.",https://openaccess.thecvf.com/content/WACV2024/html/Goswami_Active_Batch_Sampling_for_Multi-Label_Classification_With_Binary_User_Feedback_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Goswami_Active_Batch_Sampling_for_Multi-Label_Classification_With_Binary_User_Feedback_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484126/,"['Computer vision', 'Machine learning algorithms', 'Annotations', 'Training data', 'Artificial neural networks', 'Manuals', 'Machine learning']","['Optimization Problem', 'Learning Algorithms', 'Deep Network', 'Deep Neural Network', 'Active Learning', 'Real-world Applications', 'Linear Programming', 'Constrained Optimization Problem', 'Human Effort', 'Multiple Labels', 'Linear Relaxation', 'Linear Programming Relaxation', 'Multi-label Learning', 'Good Generalization Capability', 'Convolutional Neural Network', 'Variety Of Applications', 'Deep Models', 'Less Than Or Equal', 'F1 Score', 'Labeled Samples', 'Labeled Data Set', 'Deep Network Architecture', 'COCO Dataset', 'Part Of Future Work', 'Non-negative Entries', 'Number Of Labels', 'Random Projection', 'Unlabeled Set', 'Linear Programming Solver']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Multi-label classification is a generalization of multiclass classification, where a single data sample can have multiple labels. While deep neural networks have depicted commendable performance for multi-label learning, they require a large amount of manually annotated training data to attain good generalization capability. However, annotating a multi-label data sample requires a human oracle to consider the presence/absence of every single class individually, which is extremely laborious. Active learning algorithms automatically identify the salient and exemplar instances from large amounts of unlabeled data and are effective in reducing human annotation effort in inducing a machine learning model. In this paper, we propose a novel active learning framework for multi-label learning, which queries a batch of (image-label) pairs and for each pair, poses the question whether the queried label is present in the corresponding image; the human annotators merely need to provide a binary feedback (""yes/no"") in response to each query, which involves much less manual work. We pose the image and label selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of (image-label) pairs, which are maximally informative to the underlying deep neural network. Our extensive empirical studies on three challenging datasets corroborate the potential of our method for real-world multi-label classification applications."
Active Learning With Task Consistency and Diversity in Multi-Task Networks,"Aral Hekimoglu, Michael Schmidt, Alvaro Marcos-Ramiro","BMW Group, Munich, Germany; Technical University Munich, Munich, Germany",50.0,Germany,50.0,Germany,"Multi-task networks demonstrate state-of-the-art performance across various vision tasks. However, their performance relies on large-scale annotated datasets, demanding extensive labeling efforts, especially as the number of tasks to label increases. In this paper, we introduce an active learning framework consisting of a data selection strategy that identifies the most informative unlabeled samples and a training strategy that ensures balanced training across multiple tasks. Our selection strategy leverages the inconsistency between initial and refined task predictions generated by recent two-stage multi-task networks. We further enhance our selection by incorporating task-specific sample diversity through a novel feature extraction mechanism. Our method captures task features for all tasks and distills them into a unified representation, which is used to curate a training set encapsulating diverse task-specific scenarios. In our training strategy, we introduce a sample-specific loss weighting mechanism based on the individual task selection scores. This facilitates the individual prioritization of samples for each task, effectively simulating the sample ordering process inherent in single-task active learning. Extensive experimentation on the PASCAL and NYUD-v2 datasets demonstrates that our approach outperforms existing state-of-the-art methods. Our approach reaches the loss of the network trained with all the available data using only 50% of the data, corresponding to 10% fewer labels compared to the state-of-the-art selection strategy. Our code is available at https://github.com/aralhekimoglu/mtal.",https://openaccess.thecvf.com/content/WACV2024/html/Hekimoglu_Active_Learning_With_Task_Consistency_and_Diversity_in_Multi-Task_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hekimoglu_Active_Learning_With_Task_Consistency_and_Diversity_in_Multi-Task_Networks_WACV_2024_paper.pdf,,https://github.com/aralhekimoglu/mtal,,main,Poster,https://ieeexplore.ieee.org/document/10483921/,"['Training', 'Computer vision', 'Codes', 'Annotations', 'Semisupervised learning', 'Multitasking', 'Feature extraction']","['Active Learning', 'Multi-task Network', 'Training Set', 'Selection Strategy', 'Training Strategy', 'Multiple Tasks', 'Vision Tasks', 'Variety Of Scenarios', 'Individual Tasks', 'Network Loss', 'Sample Task', 'Feature Maps', 'Input Samples', 'Combined Score', 'Semantic Segmentation', 'Edge Detection', 'Labeled Samples', 'Depth Estimation', 'Multi-task Learning', 'Intermediate Features', 'Task-specific Features', 'Concatenated Feature Map', 'Monocular Depth Estimation', 'Diversity Score', 'Interaction Task', 'Surface Normals', 'Active Learning Strategies', 'Labeled Training Set', 'Discriminator Loss', 'Weighting Scheme']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Multi-task networks demonstrate state-of-the-art performance across various vision tasks. However, their performance relies on large-scale annotated datasets, demanding extensive labeling efforts, especially as the number of tasks to label increases. In this paper, we introduce an active learning framework consisting of a data selection strategy that identifies the most informative unlabeled samples and a training strategy that ensures balanced training across multiple tasks. Our selection strategy leverages the inconsistency between initial and refined task predictions generated by recent two-stage multi-task networks. We further enhance our selection by incorporating task-specific sample diversity through a novel feature extraction mechanism. Our method captures task features for all tasks and distills them into a unified representation, which is used to curate a training set encapsulating diverse task-specific scenarios. In our training strategy, we introduce a sample-specific loss weighting mechanism based on the individual task selection scores. This facilitates the individual prioritization of samples for each task, effectively simulating the sample ordering process inherent in single-task active learning. Extensive experimentation on the PASCAL and NYUD-v2 datasets demonstrates that our approach outperforms existing state-of-the-art methods. Our approach reaches the loss of the network trained with all the available data using only 50% of the data, corresponding to 10% fewer labels compared to the state-of-the-art selection strategy. Our code is available at https://github.com/aralhekimoglu/mtal."
Active Learning for Single-Stage Object Detection in UAV Images,"Asma Yamani, Albandari Alyami, Hamzah Luqman, Bernard Ghanem, Silvio Giancola","Information and Computer Science Department, KFUPM; SDAIA-KFUPM Joint Research Center for Artificial Intelligence, KFUPM; Image and Video Understanding Laboratory (IVUL), KAUST; Information and Computer Science Department, KFUPM",100.0,Saudi Arabia,0.0,,"Unmanned aerial vehicles (UAVs) are widely used for image acquisition in various applications, and object detection is a crucial task for UAV imagery analysis. However, training accurate object detectors requires a large amount of annotated data, which can be expensive and time-consuming. To address this issue, we propose an active learning framework for single-stage object detectors in UAV images. First, we introduce Diverse Uncertainty Aggregation (DUA), a novel uncertainty aggregation method that aims to select images with a more diverse variety of object classes with high uncertainties. Second, we address the problem of class imbalance by adjusting the uncertainty calculation based on the performance of each class. Third, we illustrate how reducing the number of images for labeling does not necessarily lead to a lower labeling cost. Evaluation of our approach on a common UAV dataset shows that we can perform similarly (within 0.02 0.5mAP) to using the whole dataset while using only 25% of the images and 32% of the labeled objects. It also outperforms Random Selection and some other aggregation methods. Evaluation on VOC2012 show also consistent results utilizing only 25% of the labeling cost to reach a performance within 0.1 0.5mAP of using the whole dataset. Our results suggest that our proposed active learning framework can effectively reduce the annotation cost while improving the performance of single-stage object detectors in UAV image settings. The code is available on: https://github.com/asmayamani/DUA",https://openaccess.thecvf.com/content/WACV2024/html/Yamani_Active_Learning_for_Single-Stage_Object_Detection_in_UAV_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yamani_Active_Learning_for_Single-Stage_Object_Detection_in_UAV_Images_WACV_2024_paper.pdf,,https://github.com/asmayamani/DUA,,main,Poster,https://ieeexplore.ieee.org/document/10483656/,"['Training', 'Uncertainty', 'Costs', 'Annotations', 'Training data', 'Object detection', 'Detectors']","['Active Learning', 'Object Detection', 'Unmanned Aerial Vehicles', 'Single-stage Object Detection', 'Classification Performance', 'High Uncertainty', 'Object Classification', 'Class Imbalance', 'Aggregation Method', 'Labeling Cost', 'Training Data', 'Convolutional Neural Network', 'Pedestrian', 'Training Images', 'Bounding Box', 'Confidence Score', 'Incremental Learning', 'Unlabeled Data', 'Acquisition Function', 'Training Subsets', 'You Only Look Once', 'Object Detection Model', 'Active Learning Approach', 'Average Uncertainty', 'Validation Subset', 'Query Image', 'Underrepresented Classes', 'Epistemic Uncertainty', 'Two-stage Detectors', 'Weighting Approach']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Remote Sensing']",6,"Unmanned aerial vehicles (UAVs) are widely used for image acquisition in various applications, and object detection is a crucial task for UAV imagery analysis. However, training accurate object detectors requires a large amount of annotated data, which can be expensive and time-consuming. To address this issue, we propose an active learning framework for single-stage object detectors in UAV images. First, we introduce Diverse Uncertainty Aggregation (DUA), a novel uncertainty aggregation method that aims to select images with a more diverse variety of object classes with high uncertainties. Second, we address the problem of class imbalance by adjusting the uncertainty calculation based on the performance of each class. Third, we illustrate how reducing the number of images for labeling does not necessarily lead to a lower labeling cost. Evaluation of our approach on a common UAV dataset shows that we can perform similarly (within 0.02 0.5mAP) to using the whole dataset while using only 25% of the images and 32% of the labeled objects. It also outperforms Random Selection and some other aggregation methods. Evaluation on VOC2012 show also consistent results utilizing only 25% of the labeling cost to reach a performance within 0.1 0.5mAP of using the whole dataset. Our results suggest that our proposed active learning framework can effectively reduce the annotation cost while improving the performance of singlestage object detectors in UAV image settings. The code is available on: https://github.com/asmayamani/DUA"
Active Transfer Learning for Efficient Video-Specific Human Pose Estimation,"Hiromu Taketsugu, Norimichi Ukita","Toyota Technological Institute, Nagoya, Japan",100.0,Japan,0.0,,"Human Pose (HP) estimation is actively researched because of its wide range of applications. However, even estimators pre-trained on large datasets may not perform satisfactorily due to a domain gap between the training and test data. To address this issue, we present our approach combining Active Learning (AL) and Transfer Learning (TL) to adapt HP estimators to individual video domains efficiently. For efficient learning, our approach quantifies (i) the estimation uncertainty based on the temporal changes in the estimated heatmaps and (ii) the unnaturalness in the estimated full-body HPs. These quantified criteria are then effectively combined with the state-of-the-art representativeness criterion to select uncertain and diverse samples for efficient HP estimator learning. Furthermore, we reconsider the existing Active Transfer Learning (ATL) method to introduce novel ideas related to the retraining methods and Stopping Criteria (SC). Experimental results demonstrate that our method enhances learning efficiency and outperforms comparative methods. Our code is publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024",https://openaccess.thecvf.com/content/WACV2024/html/Taketsugu_Active_Transfer_Learning_for_Efficient_Video-Specific_Human_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Taketsugu_Active_Transfer_Learning_for_Efficient_Video-Specific_Human_Pose_Estimation_WACV_2024_paper.pdf,,https://github.com/ImIntheMiddle/VATL4Pose-WACV2024,2311.05041,main,Poster,https://ieeexplore.ieee.org/document/10484280/,"['Training', 'Heating systems', 'Computer vision', 'Uncertainty', 'Codes', 'Transfer learning', 'Pose estimation']","['Active Learning', 'Transfer Learning', 'Pose Estimation', 'Human Pose Estimation', 'Human Pose', 'Training Data', 'Diverse Sample', 'Selection Criteria', 'Training Dataset', 'Hyperparameters', 'Bayesian Model', 'Estimation Results', 'Estimation Performance', 'Manual Annotation', 'Target Domain', 'Anomaly Detection', 'Acquisition Function', 'Source Domain', 'Training Videos', 'Adjacent Frames', 'Dynamic Weight', 'Autoencoder Training', 'Simple Baseline', 'Annotated Samples']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",,"Human Pose (HP) estimation is actively researched because of its wide range of applications. However, even estimators pre-trained on large datasets may not perform satisfactorily due to a domain gap between the training and test data. To address this issue, we present our approach combining Active Learning (AL) and Transfer Learning (TL) to adapt HP estimators to individual video domains efficiently. For efficient learning, our approach quantifies (i) the estimation uncertainty based on the temporal changes in the estimated heatmaps and (ii) the unnaturalness in the estimated full-body HPs. These quantified criteria are then effectively combined with the state-of-the-art representativeness criterion to select uncertain and diverse samples for efficient HP estimator learning. Furthermore, we reconsider the existing Active Transfer Learning (ATL) method to introduce novel ideas related to the retraining methods and Stopping Criteria (SC). Experimental results demonstrate that our method enhances learning efficiency and outperforms comparative methods. Our code is publicly available at: https://github.com/ImIntheMiddle/VATL4Pose-WACV2024"
Activity-Based Early Autism Diagnosis Using a Multi-Dataset Supervised Contrastive Learning Approach,"Asha Rani, Yashaswi Verma","IIT Jodhpur, India",100.0,India,0.0,,"Autism Spectrum Disorder (ASD) is a neurological disorder. Its primary symptoms include difficulty in verbal/non-verbal communication and rigid/repetitive behavior. Traditional methods of autism diagnosis require multiple visits to a human specialist. However, this process is generally time-consuming and may result in a delayed (early) intervention. In this paper, we present a data-driven approach to automate autism diagnosis using video clips of subjects performing simple activities recorded in a weakly constrained environment. This task is particularly challenging since the available training data is small, videos from the two categories (""ASD"" and ""Control"") are generally perceptually indistinguishable, and there is no clear understanding of what features would be beneficial in this task. To address these, we present a novel multi-dataset supervised contrastive learning technique to learn discriminative features simultaneously from multiple video datasets with significantly diverse distributions. Extensive empirical analyses demonstrate the promise of our approach compared to competing techniques on this challenging task.",https://openaccess.thecvf.com/content/WACV2024/html/Rani_Activity-Based_Early_Autism_Diagnosis_Using_a_Multi-Dataset_Supervised_Contrastive_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rani_Activity-Based_Early_Autism_Diagnosis_Using_a_Multi-Dataset_Supervised_Contrastive_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484184/,"['Neurological diseases', 'Autism', 'Computer vision', 'Training data', 'Self-supervised learning', 'Task analysis']","['Self-supervised Learning', 'Autism Diagnosis', 'Contrastive Learning Approach', 'Early Treatment', 'Training Data', 'Multiple Datasets', 'Video Clips', 'Video Dataset', 'Discriminative Feature Learning', 'Pairing', 'Deep Neural Network', 'Binary Classification', 'Cross-entropy Loss', 'Paired Data', 'Confidence Score', 'Correct Predictions', 'Fully-connected Layer', 'Deep Features', 'Relationship Management', 'Autism Spectrum Disorder Diagnosis', 'Encoder Network', 'Feature Extraction Network', 'Challenging Dataset', 'Autism Spectrum Disorder Subjects', 'Hand Gestures', 'Score In Cases', 'Category Information', 'Autism Spectrum Disorder Sample', 'Softmax Activation', 'Short Video Clips']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Video recognition and understanding']",1,"Autism Spectrum Disorder (ASD) is a neurological disorder. Its primary symptoms include difficulty in verbal/non-verbal communication and rigid/repetitive behavior. Traditional methods of autism diagnosis require multiple visits to a human specialist. However, this process is generally time-consuming and may result in a delayed (early) intervention. In this paper, we present a data-driven approach to automate autism diagnosis using video clips of subjects performing simple activities recorded in a weakly constrained environment. This task is particularly challenging since the available training data is small, videos from the two categories (""ASD"" and “Control”) are generally perceptually indistinguishable, and there is no clear understanding of what features would be beneficial in this task. To address these, we present a novel multi-dataset supervised contrastive learning technique to learn discriminative features simultaneously from multiple video datasets with significantly diverse distributions. Extensive empirical analyses demonstrate the promise of our approach compared to competing techniques on this challenging task."
Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-Free Continual Learning,"Filip Szatkowski, Mateusz Pyla, Marcin Przewięźlikowski, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński","Gda´nsk University of Technology; Warsaw University of Technology, IDEAS NCBR; Autonomous University of Barcelona, Computer Vision Center; IDEAS NCBR, Tooploox; Jagiellonian University, Faculty of Mathematics and Computer Science, Jagiellonian University, Doctoral School of Exact and Natural Sciences",80.0,"Poland, Spain",20.0,Poland,"In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL models. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main models during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks. The source code for our method is available at https://github.com/fszatkowski/cl-teacher-adaptation.",https://openaccess.thecvf.com/content/WACV2024/html/Szatkowski_Adapt_Your_Teacher_Improving_Knowledge_Distillation_for_Exemplar-Free_Continual_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Szatkowski_Adapt_Your_Teacher_Improving_Knowledge_Distillation_for_Exemplar-Free_Continual_Learning_WACV_2024_paper.pdf,,https://github.com/fszatkowski/cl-teacher-adaptation,,main,Poster,https://ieeexplore.ieee.org/document/10350907/,"['Training', 'Degradation', 'Adaptation models', 'Computer vision', 'Conferences', 'Training data', 'Benchmark testing']","['Incremental Learning', 'Adaptive Method', 'Forgetting', 'Previous Tasks', 'Change Model', 'Teacher Model', 'ImageNet', 'Kullback-Leibler', 'Batch Normalization', 'Domain Shift', 'Domain Adaptation', 'Batch Normalization Layer', 'Sequential Task', 'Subsequent Task', 'Student Model', 'Domain Adaptation Methods', 'Softmax Probability']","['exemplar free continual learning', 'knowledge distillation', 'class incremental learning']",,"In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks."
Adaptive Deep Neural Network Inference Optimization With EENet,"Fatih Ilhan, Ka-Ho Chow, Sihao Hu, Tiansheng Huang, Selim Tekin, Wenqi Wei, Yanzhao Wu, Myungjin Lee, Ramana Kompella, Hugo Latapie, Gaowen Liu, Ling Liu","CISCO Research, San Jose, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA",50.0,USA,50.0,USA,"Well-trained deep neural networks (DNNs) treat all test samples equally during prediction. Adaptive DNN inference with early exiting leverages the observation that some test examples can be easier to predict than others. This paper presents EENet, a novel early-exiting scheduling framework for multi-exit DNN models. Instead of having every sample go through all DNN layers during prediction, EENet learns an early exit scheduler, which can intelligently terminate the inference earlier for certain predictions, which the model has high confidence of early exit. As opposed to previous early-exiting solutions with heuristics-based methods, our EENet framework optimizes an early-exiting policy to maximize model accuracy while satisfying the given per-sample average inference budget. Extensive experiments are conducted on four computer vision datasets (CIFAR-10, CIFAR-100, ImageNet, Cityscapes) and two NLP datasets (SST-2, AgNews). The results demonstrate that the adaptive inference by EENet can outperform the representative existing early exit techniques. We also perform a detailed visualization analysis of the comparison results to interpret the benefits of EENet.",https://openaccess.thecvf.com/content/WACV2024/html/Ilhan_Adaptive_Deep_Neural_Network_Inference_Optimization_With_EENet_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ilhan_Adaptive_Deep_Neural_Network_Inference_Optimization_With_EENet_WACV_2024_paper.pdf,,,2301.07099,main,Poster,https://ieeexplore.ieee.org/document/10484099/,"['Computer vision', 'Adaptation models', 'Visualization', 'Adaptive systems', 'Computational modeling', 'Optimization methods', 'Artificial neural networks']","['Neural Network', 'Deep Neural Network', 'Computer Vision', 'ImageNet', 'Deep Neural Network Model', 'Deep Neural Network Layers', 'Average Budget', 'Computational Cost', 'Performance Metrics', 'Scoring Function', 'Image Segmentation', 'Training Phase', 'Validation Dataset', 'Prediction Score', 'Butterfly', 'Kullback-Leibler', 'Correct Predictions', 'Performance Gain', 'Optimal Schedule', 'Sentiment Analysis', 'Measure Of Confidence', 'Inference Cost', 'Flexible Deployment', 'Pre-trained Deep Neural Networks', 'Vector Core']","['Algorithms', 'Image recognition and understanding']",2,"Well-trained deep neural networks (DNNs) treat all test samples equally during prediction. Adaptive DNN inference with early exiting leverages the observation that some test examples can be easier to predict than others. This paper presents EENet, a novel early-exiting scheduling framework for multi-exit DNN models. Instead of having every sample go through all DNN layers during prediction, EENet learns an early exit scheduler, which can intelligently terminate the inference earlier for certain predictions, which the model has high confidence of early exit. As opposed to previous early-exiting solutions with heuristics-based methods, our EENet framework optimizes an early-exiting policy to maximize model accuracy while satisfying the given per-sample average inference budget. Extensive experiments are conducted on four computer vision datasets (CIFAR-10, CIFAR-100, ImageNet, Cityscapes) and two NLP datasets (SST-2, AgNews). The results demonstrate that the adaptive inference by EENet can outperform the representative existing early exit techniques. We also perform a detailed visualization analysis of the comparison results to interpret the benefits of EENet."
Adaptive Latent Diffusion Model for 3D Medical Image to Image Translation: Multi-Modal Magnetic Resonance Imaging Study,"Jonghun Kim, Hyunjin Park","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea",100.0,South Korea,0.0,,"Multi-modal images play a crucial role in comprehensive evaluations in medical image analysis providing complementary information for identifying clinically important biomarkers. However, in clinical practice, acquiring multiple modalities can be challenging due to reasons such as scan cost, limited scan time, and safety considerations. In this paper, we propose a model based on the latent diffusion model (LDM) that leverages switchable blocks for image-to-image translation in 3D medical images without patch cropping. The 3D LDM combined with conditioning using the target modality allows generating high-quality target modality in 3D overcoming the shortcoming of the missing out-of-slice information in 2D generation methods. The switchable block, noted as multiple switchable spatially adaptive normalization (MS-SPADE), dynamically transforms source latents to the desired style of the target latents to help with the diffusion process. The MS-SPADE block allows us to have one single model to tackle many translation tasks of one source modality to various targets removing the need for many translation models for different scenarios. Our model exhibited successful image synthesis across different source-target modality scenarios and surpassed other models in quantitative evaluations tested on multi-modal brain magnetic resonance imaging datasets of four different modalities. Our model demonstrated successful image synthesis across various modalities even allowing for one-to-many modality translations. Furthermore, it outperformed other one-to-one translation models in quantitative evaluations.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Adaptive_Latent_Diffusion_Model_for_3D_Medical_Image_to_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Adaptive_Latent_Diffusion_Model_for_3D_Medical_Image_to_Image_WACV_2024_paper.pdf,,https://github.com/jongdory/ALDM/,2311.00265,main,Poster,https://ieeexplore.ieee.org/document/10484171/,"['Adaptation models', 'Solid modeling', 'Three-dimensional displays', 'Image synthesis', 'Computational modeling', 'Magnetic resonance imaging', 'Switches']","['Magnetic Resonance Imaging', 'Medical Imaging', '3D Images', 'Diffusion Model', 'Multimodal Imaging', '3D Medical Image', 'Quantitative Evaluation', 'Diffusion Process', 'Multiple Modalities', 'Need For Models', 'Successful Synthesis', 'Safety Considerations', 'Image Synthesis', 'Medical Image Analysis', 'Translation Task', 'Target Modality', 'Time Step', 'Imaging Techniques', 'Contrast Agent', 'Autoencoder', 'Image Compression', 'Latent Space', 'Magnetic Resonance Imaging Modalities', 'Source Images', 'Peak Signal-to-noise Ratio', 'Normalized Mean Square Error', 'Style Transfer', 'Adjacent Slices', 'Image Generation', 'Inpainting']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",4,"Multi-modal images play a crucial role in comprehensive evaluations in medical image analysis providing complementary information for identifying clinically important biomarkers. However, in clinical practice, acquiring multiple modalities can be challenging due to reasons such as scan cost, limited scan time, and safety considerations. In this paper, we propose a model based on the latent diffusion model (LDM) that leverages switchable blocks for image-to-image translation in 3D medical images without patch cropping. The 3D LDM combined with conditioning using the target modality allows generating high-quality target modality in 3D overcoming the shortcoming of the missing out-of-slice information in 2D generation methods. The switchable block, noted as multiple switchable spatially adaptive normalization (MS-SPADE), dynamically transforms source latents to the desired style of the target latents to help with the diffusion process. The MS-SPADE block allows us to have one single model to tackle many translation tasks of one source modality to various targets removing the need for many translation models for different scenarios. Our model exhibited successful image synthesis across different source-target modality scenarios and surpassed other models in quantitative evaluations tested on multi-modal brain magnetic resonance imaging datasets of four different modalities and an independent IXI dataset. Our model demonstrated successful image synthesis across various modalities even allowing for one-to-many modality translations. Furthermore, it outperformed other one-to-one translation models in quantitative evaluations. Our code is available at https://github.com/jongdory/ALDM/"
Adaptive Manifold for Imbalanced Transductive Few-Shot Learning,"Michalis Lazarou, Yannis Avrithis, Tania Stathaki",Institute of Advanced Research on Artificial Intelligence (IARAI); Imperial College London,100.0,"Austria, UK",0.0,,"Transductive few-shot learning algorithms have showed substantially superior performance over their inductive counterparts by leveraging the unlabeled queries at inference. However, the vast majority of transductive methods are evaluated on perfectly class-balanced benchmarks. It has been shown that they undergo remarkable drop in performance under a more realistic, imbalanced setting. To this end, we propose a novel algorithm to address imbalanced transductive few-shot learning, named Adaptive Manifold. Our algorithm exploits the underlying manifold of the labeled examples and unlabeled queries by using manifold similarity to predict the class probability distribution of every query. It is parameterized by one centroid per class and a set of manifold parameters that determine the manifold. All parameters are optimized by minimizing a loss function that can be tuned towards class-balanced or imbalanced distributions. The manifold similarity shows substantial improvement over Euclidean distance, especially in the 1-shot setting. Our algorithm outperforms all other state of the art methods in three benchmark datasets, namely miniImageNet, tieredImageNet and CUB, and two different backbones, namely ResNet-18 and WideResNet-28-10. In certain cases, our algorithm outperforms the previous state of the art by as much as 4.2%. The publicly available source code can be found in https://github.com/MichalisLazarou/AM.",https://openaccess.thecvf.com/content/WACV2024/html/Lazarou_Adaptive_Manifold_for_Imbalanced_Transductive_Few-Shot_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lazarou_Adaptive_Manifold_for_Imbalanced_Transductive_Few-Shot_Learning_WACV_2024_paper.pdf,,https://github.com/MichalisLazarou/AM,2304.14281,main,Poster,https://ieeexplore.ieee.org/document/10483689/,"['Manifolds', 'Source coding', 'Euclidean distance', 'Benchmark testing', 'Prediction algorithms', 'Inference algorithms', 'Loss measurement']","['Few-shot Learning', 'Transductive Learning', 'Transductive Few-shot Learning', 'Loss Function', 'Source Code', 'Data Augmentation', 'Transfer Learning', 'Generative Adversarial Networks', 'Real Sets', 'Performance Gap', 'Variational Autoencoder', 'Balanced Set', 'Support Set', 'Query Set', 'Fuzzy Clustering', 'Class Balance', 'Column Sums', 'Label Propagation', 'Standard Cross-entropy Loss', 'Label Matrix', 'Query Examples', 'Unlabeled Examples', 'K-nearest Neighbor Graph', 'Data Manifold', 'Predicted Probability Distribution', 'Intra-class Variance', 'Latent Space', 'Base Classes', 'Probability Matrix', 'Inference Time']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Transductive few-shot learning algorithms have showed substantially superior performance over their inductive counterparts by leveraging the unlabeled queries at inference. However, the vast majority of transductive methods are evaluated on perfectly class-balanced benchmarks. It has been shown that they undergo remarkable drop in performance under a more realistic, imbalanced setting.To this end, we propose a novel algorithm to address imbalanced transductive few-shot learning, named Adaptive Manifold. Our algorithm exploits the underlying manifold of the labeled examples and unlabeled queries by using manifold similarity to predict the class probability distribution of every query. It is parameterized by one centroid per class and a set of manifold parameters that determine the manifold. All parameters are optimized by minimizing a loss function that can be tuned towards class-balanced or imbalanced distributions. The manifold similarity shows substantial improvement over Euclidean distance, especially in the 1-shot setting.Our algorithm outperforms all other state of the art methods in three benchmark datasets, namely miniImageNet, tieredImageNet and CUB, and two different backbones, namely ResNet-18 and WideResNet-28-10. In certain cases, our algorithm outperforms the previous state of the art by as much as 4.2%. The publicly available source code can be found in https://github.com/MichalisLazarou/AM"
Adversarial Likelihood Estimation With One-Way Flows,"Omri Ben-Dov, Pravir Singh Gupta, Victoria Abrevaya, Michael J. Black, Partha Ghosh","Perceive Inc.; Max Planck Institute for Intelligent Systems, Tübingen, Germany",50.0,Germany,50.0,USA,"Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; and 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require a tractable inverse function. Our experimental results show that our method converges faster, produces comparable sample quality to GANs with similar architecture, successfully avoids over-fitting to commonly used datasets and produces smooth low-dimensional latent representations of the training data.",https://openaccess.thecvf.com/content/WACV2024/html/Ben-Dov_Adversarial_Likelihood_Estimation_With_One-Way_Flows_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ben-Dov_Adversarial_Likelihood_Estimation_With_One-Way_Flows_WACV_2024_paper.pdf,,,2307.09882,main,Poster,https://ieeexplore.ieee.org/document/10484130/,"['Training', 'Maximum likelihood estimation', 'Monte Carlo methods', 'Computational modeling', 'Training data', 'Stochastic processes', 'Computer architecture']","['High-quality', 'Unbiased', 'Density Estimation', 'Generative Adversarial Networks', 'Inverse Function', 'Similar Architecture', 'Partition Function', 'Network Flow', 'Explicit Calculation', 'Entropy Term', 'Computational Efficiency', 'Maximum Likelihood Estimation', 'Autoregressive Model', 'Diffusion Model', 'Latent Space', 'Random Vector', 'Gaussian Mixture Model', 'Approximate Entropy', 'General Architecture', 'Energy-based Model', 'Fréchet Inception Distance', 'Jacobian Determinant', 'Normal Flow', 'Variational Autoencoder', 'Stochastic Mechanism', 'Substitution Rule', 'Entropy Of Distribution', 'Latent Vector', 'Generative Adversarial Networks Training']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; and 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require a tractable inverse function. Our experimental results show that our method converges faster, produces comparable sample quality to GANs with similar architecture, successfully avoids over-fitting to commonly used datasets and produces smooth low-dimensional latent representations of the training data."
Aligning Non-Causal Factors for Transformer-Based Source-Free Domain Adaptation,"Sunandini Sanyal, Ashish Ramayee Asokan, Suvaansh Bhambri, Pradyumna YM, Akshay Kulkarni, Jogendra Nath Kundu, R. Venkatesh Babu","Vision and AI Lab, Indian Insitute of Science, Bengaluru",100.0,India,0.0,,"Conventional domain adaptation algorithms aim to achieve better generalization by aligning only the task-discriminative causal factors between a source and target domain. However, we find that retaining the spurious correlation between causal and non-causal factors plays a vital role in bridging the domain gap and improving target adaptation. Therefore, we propose to build a framework that disentangles and supports causal factor alignment by aligning the non-causal factors first. We also investigate and find that the strong shape bias of vision transformers, coupled with its multi-head attentions, make it a suitable architecture for realizing our proposed disentanglement. Hence, we propose to build a Causality-enforcing Source Free Transformer framework (C-SFTrans) to achieve dis entanglement via a novel two-stage alignment approach: a) non-causal factor alignment: non-causal factors are aligned using a style classification task which leads to an overall global alignment, b) task-discriminative causal factor alignment: causal factors are aligned via target adaptation. We are the first to investigate the role of vision transformers (ViTs) in a privacy-preserving source-free setting. Our approach achieves state-of-the-art results in several DA benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Sanyal_Aligning_Non-Causal_Factors_for_Transformer-Based_Source-Free_Domain_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sanyal_Aligning_Non-Causal_Factors_for_Transformer-Based_Source-Free_Domain_Adaptation_WACV_2024_paper.pdf,https://val.cds.iisc.ac.in/C-SFTrans/,,2311.16294,main,Poster,https://ieeexplore.ieee.org/document/10484156/,"['Training', 'Computer vision', 'Correlation', 'Shape', 'Cause effect analysis', 'Computer architecture', 'Benchmark testing']","['Domain Adaptation', 'Non-causal Factors', 'Source-free Domain Adaptation', 'Classification Task', 'Attention Mechanism', 'Joint Effect', 'Target Domain', 'Source Domain', 'Global Alignment', 'Domain Gap', 'Vision Transformer', 'Transformation Framework', 'Data Sources', 'Poor Performance', 'Factorization', 'Stochastic Gradient Descent', 'Domain Shift', 'Representation Learning', 'Target Data', 'Source Model', 'Source Domain Data', 'Attention Heads', 'Domain Adaptation Methods', 'Task Goal', 'Adversarial Training', 'Criteria For Causality', 'Unlabeled Target Domain', 'Target Domain Data', 'Labeled Source Domain']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Conventional domain adaptation algorithms aim to achieve better generalization by aligning only the task-discriminative causal factors between a source and target domain. However, we find that retaining the spurious correlation between causal and non-causal factors plays a vital role in bridging the domain gap and improving target adaptation. Therefore, we propose to build a framework that disentangles and supports causal factor alignment by aligning the non-causal factors first. We also investigate and find that the strong shape bias of vision transformers, coupled with its multi-head attentions, make it a suitable architecture for realizing our proposed disentanglement. Hence, we propose to build a Causality-enforcing Source-Free Transformer framework (C-SFTrans
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
) to achieve disentanglement via a novel two-stage alignment approach: a) non-causal factor alignment: non-causal factors are aligned using a style classification task which leads to an overall global alignment, b) task-discriminative causal factor alignment: causal factors are aligned via target adaptation. We are the first to investigate the role of vision transformers (ViTs) in a privacy-preserving source-free setting. Our approach achieves state-of-the-art results in several DA benchmarks."
Alleviating Foreground Sparsity for Semi-Supervised Monocular 3D Object Detection,"Weijia Zhang, Dongnan Liu, Chao Ma, Weidong Cai",University of Sydney; Shanghai Jiao Tong University,100.0,"Australia, China",0.0,,"Monocular 3D object detection (M3OD) is a significant yet inherently challenging task in autonomous driving due to absence of explicit depth cues in a single RGB image. In this paper, we strive to boost currently underperforming monocular 3D object detectors by leveraging an abundance of unlabelled data via semi-supervised learning. Our proposed ODM3D framework entails cross-modal knowledge distillation at various levels to inject LiDAR-domain knowledge into a monocular detector during training. By identifying object sparsity as the main culprit behind existing methods' suboptimal training, we exploit the precise localisation information embedded in LiDAR points to enable more foreground-attentive and efficient distillation via the proposed BEV occupancy guidance mask, leading to notably improved knowledge transfer and M3OD performance. Besides, motivated by insights into why existing cross-modal GT-sampling techniques fail on our task at hand, we further design a novel cross-modal object-wise data augmentation strategy for effective RGB-LiDAR joint learning. Our method ranks 1st in both KITTI validation and test benchmarks, significantly surpassing all existing monocular methods, supervised or semi-supervised, on both BEV and 3D detection metrics.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Alleviating_Foreground_Sparsity_for_Semi-Supervised_Monocular_3D_Object_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Alleviating_Foreground_Sparsity_for_Semi-Supervised_Monocular_3D_Object_Detection_WACV_2024_paper.pdf,,https://github.com/arcaninez/odm3d,2310.18620,main,Poster,https://ieeexplore.ieee.org/document/10483662/,"['Training', 'Knowledge engineering', 'Point cloud compression', 'Three-dimensional displays', 'Object detection', 'Detectors', 'Semisupervised learning']","['3D Object Detection', 'Monocular 3D Object Detection', 'Data Augmentation', 'RGB Images', 'Unlabeled Data', 'Semi-supervised Learning', 'Benchmark Test', 'Augmentation Strategy', 'LiDAR Point', '3D Detection', 'Pedestrian', 'Point Cloud', '3D Space', 'Bounding Box', 'Depth Map', 'Gaussian Blur', 'Depth Values', 'Depth Estimation', 'Intermediate Features', 'Prediction Map', 'Distillation Loss', 'Training Scenes', 'Detection Head', 'Lidar Data', 'Stereo Images', 'Raw Point Cloud', '3D Properties', 'Occupancy Grid', 'Student Network', 'Object Point']","['Applications', 'Autonomous Driving', 'Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",2,"Monocular 3D object detection (M3OD) is a significant yet inherently challenging task in autonomous driving due to absence of explicit depth cues in a single RGB image. In this paper, we strive to boost currently underperforming monocular 3D object detectors by leveraging an abundance of unlabelled data via semi-supervised learning. Our proposed ODM3D framework entails cross-modal knowledge distillation at various levels to inject LiDAR-domain knowledge into a monocular detector during training. By identifying foreground sparsity as a main culprit behind existing methods’ suboptimal training, we exploit the precise localisation information embedded in LiDAR points to enable more foreground-attentive and efficient distillation via the proposed BEV occupancy guidance mask, leading to notably improved knowledge transfer and M3OD performance. Besides, motivated by insights into why existing cross-modal GT-sampling techniques fail on our task at hand, we further design a novel cross-modal object-wise data augmentation strategy for effective RGB-LiDAR joint learning. Our method ranks 1
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">st</sup>
 in both KITTI validation and test benchmarks, significantly surpassing all existing monocular methods, supervised or semi-supervised, on both BEV and 3D detection metrics. Code will be released at https://github.com/arcaninez/odm3d."
Amodal Intra-Class Instance Segmentation: Synthetic Datasets and Benchmark,"Jiayang Ao, Qiuhong Ke, Krista A. Ehinger","The University of Melbourne, Parkville VIC 3010; Monash University, Clayton VIC 3800",100.0,Australia,0.0,,"Images of realistic scenes often contain intra-class objects that are heavily occluded from each other, making the amodal perception task that requires parsing the occluded parts of the objects challenging. Although important for downstream tasks such as robotic grasping systems, the lack of large-scale amodal datasets with detailed annotations makes it difficult to model intra-class occlusions explicitly. This paper introduces two new amodal datasets for image amodal completion tasks, which contain a total of over 267K images of intra-class occlusion scenarios, annotated with multiple masks, amodal bounding boxes, dual order relations and full appearance for instances and background. We also present a point-supervised scheme with layer priors for amodal instance segmentation specifically designed for intra-class occlusion scenarios. Experiments show that our weakly supervised approach outperforms the SOTA fully supervised methods, while our layer priors design exhibits remarkable performance improvements in the case of intra-class occlusion in both synthetic and real images.",https://openaccess.thecvf.com/content/WACV2024/html/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ao_Amodal_Intra-Class_Instance_Segmentation_Synthetic_Datasets_and_Benchmark_WACV_2024_paper.pdf,,https://github.com/saraao/amodal-dataset,2303.06596,main,Poster,https://ieeexplore.ieee.org/document/10484113/,"['Instance segmentation', 'Training', 'Computer vision', 'Annotations', 'Grasping', 'Benchmark testing', 'Task analysis']","['Instance Segmentation', 'Large-scale Datasets', 'Bounding Box', 'Synthetic Images', 'Object Parts', 'Detailed Annotation', 'Lack Of Datasets', 'Training Set', 'Visible Light', 'Ordination', 'Data Augmentation', 'Manual Annotation', 'Bilinear Interpolation', 'Training Instances', 'Region Proposal Network', 'Non-maximum Suppression', 'Foreground Objects', 'Weak Supervision', 'Occluded Objects', 'Instance Segmentation Methods', 'Need For Supervision', 'IoU Threshold', 'Performance Of Layers']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations']",,"Images of realistic scenes often contain intra-class objects that are heavily occluded from each other, making the amodal perception task that requires parsing the occluded parts of the objects challenging. Although important for downstream tasks such as robotic grasping systems, the lack of large-scale amodal datasets with detailed annotations makes it difficult to model intra-class occlusions explicitly. This paper introduces two new amodal datasets for image amodal completion tasks, which contain a total of over 267K images of intra-class occlusion scenarios, annotated with multiple masks, amodal bounding boxes, dual order relations and full appearance for instances and background. We also present a point-supervised scheme with layer priors for amodal instance segmentation specifically designed for intra-class occlusion scenarios
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
. Experiments show that our weakly supervised approach outperforms the SOTA fully supervised methods, while our layer priors design exhibits remarkable performance improvements in the case of intra-class occlusion in both synthetic and real images."
An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning,"Grégoire Petit, Michaël Soumm, Eva Feillet, Adrian Popescu, Bertrand Delezoide, David Picard, Céline Hudelot","Universit´e Paris-Saclay, CentraleSup´elec, MICS, France; Universit´e Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France; LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vall´ee, France; Amanda, 34 Avenue Des Champs Elys´ees, F-75008, Paris, France",75.0,France,25.0,France,"Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of classes in the stream and the number of examples available for learning. We conduct a comprehensive experimental study to assess the roles of these factors. We present a statistical analysis framework that quantifies the relative contribution of each factor to incremental performance. Our main finding is that the initial training strategy is the dominant factor influencing the average incremental accuracy, but that the choice of CIL algorithm is more important in preventing forgetting. Based on this analysis, we propose practical recommendations for choosing the right initial training strategy for a given incremental learning use case. These recommendations are intended to facilitate the practical deployment of incremental learning.",https://openaccess.thecvf.com/content/WACV2024/html/Petit_An_Analysis_of_Initial_Training_Strategies_for_Exemplar-Free_Class-Incremental_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Petit_An_Analysis_of_Initial_Training_Strategies_for_Exemplar-Free_Class-Incremental_Learning_WACV_2024_paper.pdf,,,2308.11677,main,Poster,https://ieeexplore.ieee.org/document/10484041/,"['Training', 'Computer vision', 'Statistical analysis', 'Transfer learning', 'Data models', 'Stability analysis', 'Classification algorithms']","['Training Strategy', 'Class-incremental Learning', 'Use Of Models', 'Process Model', 'Large Amount Of Data', 'Average Accuracy', 'Class Distribution', 'Incremental Learning', 'Target Dataset', 'Choice Of Algorithm', 'Catastrophic Forgetting', 'Auxiliary Dataset', 'Large Datasets', 'Accuracy Of Model', 'Transformer', 'Convolutional Neural Network', 'Weak Correlation', 'Supervised Learning', 'Subsequent Steps', 'Subset Of Data', 'Incremental Process', 'Half Of The Class', 'Model Weights', 'Typical Architecture', 'Self-supervised Learning', 'Fine-tuned Model', 'Transfer Learning', 'Incremental Algorithm', 'Incremental Steps', 'External Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Embedded sensing / real-time techniques']",1,"Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of classes in the stream and the number of examples available for learning. We conduct a comprehensive experimental study to assess the roles of these factors. We present a statistical analysis framework that quantifies the relative contribution of each factor to incremental performance. Our main finding is that the initial training strategy is the dominant factor influencing the average incremental accuracy, but that the choice of CIL algorithm is more important in preventing forgetting. Based on this analysis, we propose practical recommendations for choosing the right initial training strategy for a given incremental learning use case. These recommendations are intended to facilitate the practical deployment of incremental learning."
An Empirical Investigation Into Benchmarking Model Multiplicity for Trustworthy Machine Learning: A Case Study on Image Classification,Prakhar Ganesh,"Mila, Quebec AI Insitute",100.0,Canada,0.0,,"Deep learning models have proven to be highly successful. Yet, their over-parameterization gives rise to model multiplicity, a phenomenon in which multiple models achieve similar performance but exhibit distinct underlying behaviours. This multiplicity presents a significant challenge and necessitates additional specifications in model selection to prevent unexpected failures during deployment. While prior studies have examined these concerns, they focus on individual metrics in isolation, making it difficult to obtain a comprehensive view of multiplicity in trustworthy machine learning. Our work stands out by offering a one-stop empirical benchmark of multiplicity across various dimensions of model design and its impact on a diverse set of trustworthy metrics. In this work, we establish a consistent language for studying model multiplicity by translating several trustworthy metrics into accuracy under appropriate interventions. We also develop a framework, which we call multiplicity sheets, to benchmark multiplicity in various scenarios. We demonstrate the advantages of our setup through a case study in image classification and provide actionable insights into the impact and trends of different hyperparameters on model multiplicity. Finally, we show that multiplicity persists in deep learning models even after enforcing additional specifications during model selection, highlighting the severity of over-parameterization. The concerns of under-specification thus remain, and we seek to promote a more comprehensive discussion of multiplicity in trustworthy machine learning.",https://openaccess.thecvf.com/content/WACV2024/html/Ganesh_An_Empirical_Investigation_Into_Benchmarking_Model_Multiplicity_for_Trustworthy_Machine_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ganesh_An_Empirical_Investigation_Into_Benchmarking_Model_Multiplicity_for_Trustworthy_Machine_WACV_2024_paper.pdf,,,2311.14859,main,Poster,https://ieeexplore.ieee.org/document/10483728/,"['Measurement', 'Deep learning', 'Computer vision', 'Computational modeling', 'Benchmark testing', 'Market research', 'Image classification']","['Machine Learning', 'Image Classification', 'Trustworthy Machine Learning', 'Deep Learning', 'Model Selection', 'Deep Learning Models', 'Set Of Metrics', 'Learning Rate', 'Model Specification', 'Batch Size', 'Rate Parameters', 'Data Augmentation', 'Model Architecture', 'Decision Boundary', 'Model Hyperparameters', 'Groups Of Datasets', 'Choice Architecture', 'Adversarial Attacks', 'Individual Privacy', 'Unforeseen Circumstances', 'Projected Gradient Descent', 'Differential Privacy', 'Training Configurations']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Applications', 'Social good']",,"Deep learning models have proven to be highly successful. Yet, their over-parameterization gives rise to model multiplicity, a phenomenon in which multiple models achieve similar performance but exhibit distinct underlying behaviours. This multiplicity presents a significant challenge and necessitates additional specifications in model selection to prevent unexpected failures during deployment. While prior studies have examined these concerns, they focus on individual metrics in isolation, making it difficult to obtain a comprehensive view of multiplicity in trustworthy machine learning. Our work stands out by offering a one-stop empirical benchmark of multiplicity across various dimensions of model design and its impact on a diverse set of trustworthy metrics.In this work, we establish a consistent language for studying model multiplicity by translating several trustworthy metrics into accuracy under appropriate interventions. We also develop a framework, which we call multiplicity sheets, to benchmark multiplicity in various scenarios. We demonstrate the advantages of our setup through a case study in image classification and provide actionable insights into the impact and trends of different hyperparameters on model multiplicity. Finally, we show that multiplicity persists in deep learning models even after enforcing additional specifications during model selection, highlighting the severity of over-parameterization. The concerns of under-specification thus remain, and we seek to promote a more comprehensive discussion of multiplicity in trustworthy machine learning."
Analyzing the Domain Shift Immunity of Deep Homography Estimation,"Mingzhen Shao, Tolga Tasdizen, Sarang Joshi","Department of Electrical & Computer Engineering, University of Utah; Department of Biomedical Engineering, University of Utah; Kahlert School of Computing, University of Utah",100.0,USA,0.0,,"Homography estimation serves as a fundamental technique for image alignment in a wide array of applications. The advent of convolutional neural networks has introduced learning-based methodologies that have exhibited remarkable efficacy in this realm. Yet, the generalizability of these approaches across distinct domains remains underexplored. Unlike other conventional tasks, CNN-driven homography estimation models show a distinctive immunity to domain shifts, enabling seamless deployment from one dataset to another without the necessity of transfer learning. This study explores the resilience of a variety of deep homography estimation models to domain shifts, revealing that the network architecture itself is not a contributing factor to this remarkable adaptability. By closely examining the models' focal regions and subjecting input images to a variety of modifications, we confirm that the models heavily rely on local textures such as edges and corner points for homography estimation. Moreover, our analysis underscores that the domain shift immunity itself is intricately tied to the utilization of these local textures.",https://openaccess.thecvf.com/content/WACV2024/html/Shao_Analyzing_the_Domain_Shift_Immunity_of_Deep_Homography_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shao_Analyzing_the_Domain_Shift_Immunity_of_Deep_Homography_Estimation_WACV_2024_paper.pdf,,https://github.com/MingzhenShao/Homography estimation.git,2304.09976,main,Poster,https://ieeexplore.ieee.org/document/10483811/,"['Performance evaluation', 'Adaptation models', 'Analytical models', 'Correlation', 'Image edge detection', 'Transfer learning', 'Estimation']","['Domain Shift', 'Homography Estimation', 'Deep Models', 'Input Image', 'Focal Region', 'Corner Points', 'Local Texture', 'Model Performance', 'Deep Learning', 'Deep Network', 'Convolutional Layers', 'Deep Learning Models', 'Comparable Accuracy', 'Diverse Characteristics', 'Image Pairs', 'Transformation Matrix', 'Estimation Process', 'Variety Of Domains', 'Deep Learning-based Methods', 'Geometric Method', 'Global Average Pooling Layer', 'Homography Matrix', 'Deep Learning-based Models', 'Sum Of Squared Differences', 'Deep Learning Tasks', 'Illumination Variations', 'Homogeneous Coordinates', 'Class Activation Maps', 'Feature Maps', 'Corresponding Points']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Datasets and evaluations']",,"Homography estimation serves as a fundamental technique for image alignment in a wide array of applications. The advent of convolutional neural networks has introduced learning-based methodologies that have exhibited remarkable efficacy in this realm. Yet, the generalizability of these approaches across distinct domains remains underexplored. Unlike other conventional tasks, CNN-driven homography estimation models show a distinctive immunity to domain shifts, enabling seamless deployment from one dataset to another without the necessity of transfer learning. This study explores the resilience of a variety of deep homography estimation models to domain shifts, revealing that the network architecture itself is not a contributing factor to this remarkable adaptability. By closely examining the models’ focal regions and subjecting input images to a variety of modifications, we confirm that the models heavily rely on local textures such as edges and corner points for homography estimation. Moreover, our analysis underscores that the domain shift immunity itself is intricately tied to the utilization of these local textures. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Annotation-Free Audio-Visual Segmentation,"Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, Weidi Xie","Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai AI Laboratory; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University",66.66666666666666,China,33.33333333333334,China,"The objective of Audio-Visual Segmentation (AVS) is to localise the sounding objects within visual scenes by accurately predicting pixel-wise segmentation masks. To tackle the task, it involves a comprehensive consideration of both the data and model aspects. In this paper, first, we initiate a novel pipeline for generating artificial data for the AVS task without extra manual annotations. We leverage existing image segmentation and audio datasets and match the image-mask pairs with its corresponding audio samples using category labels in segmentation datasets, that allows us to effortlessly compose (image, audio, mask) triplets for training AVS models. The pipeline is annotation-free and scalable to cover a large number of categories. Additionally, we introduce a lightweight model SAMA-AVS which adapts the pre-trained segment anything model (SAM) to the AVS task. By introducing only a small number of trainable parameters with adapters, the proposed model can effectively achieve adequate audio-visual fusion and interaction in the encoding stage with vast majority of parameters fixed. We conduct extensive experiments, and the results show our proposed model remarkably surpasses other competing methods. Moreover, by using the proposed model pretrained with our synthetic data, the performance on real AVSBench data is further improved, achieving 83.17 mIoU on S4 subset and 66.95 mIoU on MS3 set. The project page is https://jinxiang-liu.github.io/anno-free-AVS/.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Annotation-Free_Audio-Visual_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Annotation-Free_Audio-Visual_Segmentation_WACV_2024_paper.pdf,https://jinxiang-liu.github.io/anno-free-AVS/,,2305.11019,main,Poster,https://ieeexplore.ieee.org/document/10484434/,"['Training', 'Adaptation models', 'Image segmentation', 'Visualization', 'Computational modeling', 'Pipelines', 'Data models']","['Image Segmentation', 'Manual Annotation', 'Audio Data', 'Segmentation Dataset', 'Performance Of Method', 'Validation Set', 'Intersection Over Union', 'Video Frames', 'Large Margin', 'Training Objective', 'Binary Cross-entropy Loss', 'Sound Localization', 'Computer Keyboard', 'Computer Vision Community', 'Open Image', 'Foundation Model', 'Object In Frame', 'Transformer Encoder', 'Audio Clips', 'Number Of Adaptations', 'Dog Barking', 'Image Encoder', 'Salient Object Detection', 'Image Embedding', 'Sound Source', 'Task Model', 'Training Set', 'Semantic Level', 'Test Split', 'Object Segmentation']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",8,"The objective of Audio-Visual Segmentation (AVS) is to localise the sounding objects within visual scenes by accurately predicting pixel-wise segmentation masks. To tackle the task, it involves a comprehensive consideration of both the data and model aspects. In this paper, first, we initiate a novel pipeline for generating artificial data for the AVS task without extra manual annotations. We leverage existing image segmentation and audio datasets and match the image-mask pairs with its corresponding audio samples using category labels in segmentation datasets, that allows us to effortlessly compose (image, audio, mask) triplets for training AVS models. The pipeline is annotation-free and scalable to cover a large number of categories. Additionally, we introduce a lightweight model SAMA-AVS which adapts the pre-trained segment anything model (SAM) to the AVS task. By introducing only a small number of trainable parameters with adapters, the proposed model can effectively achieve adequate audio-visual fusion and interaction in the encoding stage with vast majority of parameters fixed. We conduct extensive experiments, and the results show our proposed model remarkably surpasses other competing methods. Moreover, by using the proposed model pretrained with our synthetic data, the performance on real AVSBench data is further improved, achieving 83.17 mIoU on S4 subset and 66.95 mIoU on MS3 set. The project page is https://jinxiang-liu.github.io/anno-free-AVS/."
AnyStar: Domain Randomized Universal Star-Convex 3D Instance Segmentation,"Neel Dey, Mazdak Abulnaga, Benjamin Billot, Esra Abaci Turk, Ellen Grant, Adrian V. Dalca, Polina Golland","Boston Children’s Hospital, Harvard Medical School; MIT CSAIL, Martinos Center, Massachusetts General Hospital; MIT CSAIL",100.0,USA,0.0,,"Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, finetuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar.",https://openaccess.thecvf.com/content/WACV2024/html/Dey_AnyStar_Domain_Randomized_Universal_Star-Convex_3D_Instance_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dey_AnyStar_Domain_Randomized_Universal_Star-Convex_3D_Instance_Segmentation_WACV_2024_paper.pdf,,https://github.com/neel-dey/AnyStar,2307.07044,main,Poster,https://ieeexplore.ieee.org/document/10483858/,"['Instance segmentation', 'Adaptation models', 'Three-dimensional displays', 'Shape', 'Biological system modeling', 'Magnetic resonance imaging', 'Transfer learning']","['Domain Adaptation', 'Instance Segmentation', '3D Instance', '3D Instance Segmentation', 'Fluorescence Microscopy', 'Training Data', 'Transfer Learning', '3D Segmentation', 'Image Annotation', 'Labeling Density', 'Synthetic Training Data', 'Ablation', 'Medical Imaging', 'Mixture Model', 'Training Images', 'Domain Shift', 'Semantic Segmentation', 'Gaussian Mixture Model', 'Training Loss', '2D Model', 'Source Domain', 'Shape Priors', 'Target Dataset', 'Annotated Dataset', 'IoU Threshold', 'Imaging Configuration', 'Foreground Objects', 'Training Labels', 'Gaussian Blur', 'Semantic Segmentation Methods']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",4,"Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from un-seen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in μCT, zebrafish brain nuclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, finetuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar."
Appearance-Based Curriculum for Semi-Supervised Learning With Multi-Angle Unlabeled Data,"Yuki Tanaka, Shuhei M. Yoshida, Takashi Shibata, Makoto Terao, Takayuki Okatani, Masashi Sugiyama","RIKEN AIP, Tohoku University; NEC Corporation; RIKEN AIP, The University of Tokyo",66.66666666666666,Japan,33.33333333333334,Japan,"We propose an appearance-based curriculum (ABC) for a semi-supervised learning scenario where labeled images taken from limited angles and unlabeled ones taken from various angles are available for training. A common approach to semi-supervised learning relies on pseudo-labeling and data augmentation, but it struggles with large visual variations that cannot be covered by data augmentation. To solve this problem, ABC incrementally expands the pool of unlabeled images fed to a base semi-supervised learner so that newly added data are the ones most similar to those already in the pool. This way, the learner can assign pseudo-labels to the new data with high accuracy, keeping the quality of pseudo-labels higher than that when all the unlabeled data are processed at once, as customarily done in existing semi-supervised learning methods. We conducted extensive experiments and confirmed that our method outperforms the state-of-the-art semi-supervised learning methods in our scenario.",https://openaccess.thecvf.com/content/WACV2024/html/Tanaka_Appearance-Based_Curriculum_for_Semi-Supervised_Learning_With_Multi-Angle_Unlabeled_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tanaka_Appearance-Based_Curriculum_for_Semi-Supervised_Learning_With_Multi-Angle_Unlabeled_Data_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483825/,"['Training', 'Visualization', 'Computer vision', 'Semisupervised learning', 'Data augmentation']","['Unlabeled Data', 'Semi-supervised Learning', 'Data Augmentation', 'Semi-supervised Methods', 'Learning Scenarios', 'Unlabeled Images', 'Training Data', 'Deep Learning', 'Supervised Learning', 'Local Features', 'Distance Function', 'Global Features', 'Image Object', 'ImageNet', 'Azimuth Angle', 'Stopping Criterion', 'Elevation Angle', 'Image Classification Tasks', 'Curriculum Learning', 'Camera Angle', 'Semi-supervised Learning Algorithm', 'Scale-invariant Feature Transform', 'Ranking Algorithm', 'Speeded Up Robust Features', 'Proportion Of Images', 'Image Annotation', 'Number Of Descriptors']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"We propose an appearance-based curriculum (ABC) for a semi-supervised learning scenario where labeled images taken from limited angles and unlabeled ones taken from various angles are available for training. A common approach to semi-supervised learning relies on pseudo-labeling and data augmentation, but it struggles with large visual variations that cannot be covered by data augmentation. To solve this problem, ABC incrementally expands the pool of unlabeled images fed to a base semi-supervised learner so that newly added data are the ones most similar to those already in the pool. This way, the learner can assign pseudo-labels to the new data with high accuracy, keeping the quality of pseudo-labels higher than that when all the unlabeled data are processed at once, as customarily done in existing semi-supervised learning methods. We conducted extensive experiments and confirmed that our method outperforms the state-of-the-art semi-supervised learning methods in our scenario."
Approximating Intersections and Differences Between Linear Statistical Shape Models Using Markov Chain Monte Carlo,"Maximilian Weiherer, Finn Klein, Bernhard Egger","Department of Computer Science, Friedrich-Alexander-Universtität Erlangen-Nürnberg",100.0,Germany,0.0,,"To date, the comparison of Statistical Shape Models (SSMs) is often solely performance-based, carried out by means of simplistic metrics such as compactness, generalization, or specificity. Any similarities or differences between the actual shape spaces can neither be visualized nor quantified. In this paper, we present a new method to qualitatively compare two linear SSMs in dense correspondence by computing approximate intersection spaces and set-theoretic differences between the (hyper-ellipsoidal) allowable shape domains spanned by the models. To this end, we approximate the distribution of shapes lying in the intersection space using Markov chain Monte Carlo and subsequently apply Principal Component Analysis (PCA) to the posterior samples, eventually yielding a new SSM of the intersection space. We estimate differences between linear SSMs in a similar manner; here, however, the resulting spaces are no longer convex and we do not apply PCA but instead use the posterior samples for visualization. We showcase the proposed algorithm qualitatively by computing and analyzing intersection spaces and differences between publicly available face models, focusing on gender-specific male and female as well as identity and expression models. Our quantitative evaluation based on SSMs built from synthetic and real-world data sets provides detailed evidence that the introduced method is able to recover ground-truth intersection spaces and differences accurately.",https://openaccess.thecvf.com/content/WACV2024/html/Weiherer_Approximating_Intersections_and_Differences_Between_Linear_Statistical_Shape_Models_Using_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Weiherer_Approximating_Intersections_and_Differences_Between_Linear_Statistical_Shape_Models_Using_WACV_2024_paper.pdf,,,2211.16314,main,Poster,https://ieeexplore.ieee.org/document/10483617/,"['Measurement', 'Analytical models', 'Adaptation models', 'Monte Carlo methods', 'Shape', 'Computational modeling', 'Focusing']","['Linear Model', 'Markov Chain', 'Shape Model', 'Statistical Shape Model', 'Quantitative Evaluation', 'Model Identification', 'Posterior Samples', 'Shape Space', 'Face Model', 'Dense Correspondence', 'Training Data', 'Random Sampling', 'Posterior Probability', 'Markov Chain Monte Carlo', 'Individual Models', 'Vector Space', 'Vector-based', 'Orthogonal Matrix', 'Orthogonal Projection', 'Range Of Points', 'Linear Subspace', 'Face Identity', 'Bhattacharyya Distance', 'Star Model', 'Linear Algebra', 'Real-world Model', 'Differences In Dimensions', 'Extreme Differences', 'Projection Operator', 'Downstream Applications']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision', 'Applications', 'Visualization']",,"To date, the comparison of Statistical Shape Models (SSMs) is often solely performance-based, carried out by means of simplistic metrics such as compactness, generalization, or specificity. Any similarities or differences between the actual shape spaces can neither be visualized nor quantified. In this paper, we present a new method to qualitatively compare two linear SSMs in dense correspondence by computing approximate intersection spaces and set-theoretic differences between the (hyper-ellipsoidal) allowable shape domains spanned by the models. To this end, we approximate the distribution of shapes lying in the intersection space using Markov chain Monte Carlo and subsequently apply Principal Component Analysis (PCA) to the posterior samples, eventually yielding a new SSM of the intersection space. We estimate differences between linear SSMs in a similar manner; here, however, the resulting spaces are no longer convex and we do not apply PCA but instead use the posterior samples for visualization. We showcase the proposed algorithm qualitatively by computing and analyzing intersection spaces and differences between publicly available face models, focusing on gender-specific male and female as well as identity and expression models. Our quantitative evaluation based on SSMs built from synthetic and real-world data sets provides detailed evidence that the introduced method is able to recover ground-truth intersection spaces and differences accurately."
Arbitrary-Resolution and Arbitrary-Scale Face Super-Resolution With Implicit Representation Networks,"Yi Ting Tsai, Yu Wei Chen, Hong-Han Shuai, Ching-Chun Huang",National Yang Ming Chiao Tung University,100.0,Taiwan,0.0,,"Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior knowledge of facial structure effectively and achieve resolution adaptation. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales.",https://openaccess.thecvf.com/content/WACV2024/html/Tsai_Arbitrary-Resolution_and_Arbitrary-Scale_Face_Super-Resolution_With_Implicit_Representation_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tsai_Arbitrary-Resolution_and_Arbitrary-Scale_Face_Super-Resolution_With_Implicit_Representation_Networks_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483821/,"['Computer vision', 'Sensitivity', 'Superresolution', 'Modulation', 'Robustness', 'Frequency estimation', 'Task analysis']","['Local Coordinate', 'Local Estimates', 'Face Images', 'Input Size', 'Low-resolution Images', 'Local Module', 'Ratio Scale', 'Global Module', 'RGB Values', 'Facial Structure', 'Global Coordinates', 'Local Features', 'Attention Mechanism', 'Multilayer Perceptron', 'Visual Comparison', 'Local Image', 'Skip Connections', 'Deep Learning-based Methods', 'Implicit Function', 'Oral And Maxillofacial Surgery', 'Latent Code', 'Single Image Super-resolution', 'Input Resolution', 'Surveillance Cameras', 'JPEG Compression', 'Facial Details', 'Compression Artifacts', 'Positional Encoding', 'Bicubic', 'Target Coordinates']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"Face super-resolution (FSR) is a critical technique for enhancing low-resolution facial images and has significant implications for face-related tasks. However, existing FSR methods are limited by fixed up-sampling scales and sensitivity to input size variations. To address these limitations, this paper introduces an Arbitrary-Resolution and Arbitrary-Scale FSR method with implicit representation networks (ARASFSR), featuring three novel designs. First, ARASFSR employs 2D deep features, local relative coordinates, and up-sampling scale ratios to predict RGB values for each target pixel, allowing super-resolution at any up-sampling scale. Second, a local frequency estimation module captures high-frequency facial texture information to reduce the spectral bias effect. Lastly, a global coordinate modulation module guides FSR to leverage prior facial structure knowledge and achieve resolution adaptation effectively. Quantitative and qualitative evaluations demonstrate the robustness of ARASFSR over existing state-of-the-art methods while super-resolving facial images across various input sizes and up-sampling scales."
ArcAid: Analysis of Archaeological Artifacts Using Drawings,"Offry Hayon, Stefan Münger, Ilan Shimshoni, Ayellet Tal",Technion; U. of Bern; U. of Haifa,100.0,"Israel, Switzerland",0.0,,"Archaeology is an intriguing domain for computer vision. It suffers not only from shortage in (labeled) data, but also from highly-challenging data, which is often extremely abraded and damaged. This paper proposes a novel semi-supervised model for classification and retrieval of images of archaeological artifacts. This model utilizes unique data that exists in the domain--manual drawings made by special artists. These are used during training to implicitly transfer the domain knowledge from the drawings to their corresponding images, improving their classification results. We show that while learning how to classify, our model also learns how to generate drawings of the artifacts, an important documentation task, which is currently performed manually. Last but not least, we collected a new dataset of stamp-seals of the Southern Levant. Our code and dataset are publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Hayon_ArcAid_Analysis_of_Archaeological_Artifacts_Using_Drawings_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hayon_ArcAid_Analysis_of_Archaeological_Artifacts_Using_Drawings_WACV_2024_paper.pdf,https://cgm.technion.ac.il/arcaid/,https://github.com/offry/Arc-Aid,,main,Poster,https://ieeexplore.ieee.org/document/10484148/,"['Training', 'Computer vision', 'Archeology', 'Computational modeling', 'Documentation', 'Data models', 'Task analysis']","['Archaeological Finds', 'Computer Vision', 'Small Datasets', 'Image Dataset', 'Image Pairs', 'Edge Detection', 'Unlabeled Data', 'Encoder Output', 'Iron Age', 'Multimodal Learning', 'Image Encoder', 'Image Embedding', 'Similar Embeddings', 'Original Drawings']","['Applications', 'Arts / games / social media', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Archaeology is an intriguing domain for computer vision. It suffers not only from shortage in (labeled) data, but also from highly-challenging data, which is often extremely abraded and damaged. This paper proposes a novel semi-supervised model for classification and retrieval of images of archaeological artifacts. This model utilizes unique data that exists in the domain—manual drawings made by special artists. These are used during training to implicitly transfer the domain knowledge from the drawings to their corresponding images, improving their classification results. We show that while learning how to classify, our model also learns how to generate drawings of the artifacts, an important documentation task, which is currently performed manually. Last but not least, we collected a new dataset of stamp-seals of the Southern Levant. Our code
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
 and dataset
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 are publicly available."
ArcGeo: Localizing Limited Field-of-View Images Using Cross-View Matching,"Maxim Shugaev, Ilya Semenov, Kyle Ashley, Michael Klaczynski, Naresh Cuntoor, Mun Wai Lee, Nathan Jacobs",Washington University; BlueHalo,50.0,USA,50.0,USA,"Cross-view matching techniques for image geolocalization attempt to match features in ground level imagery against a collection of satellite images to determine the position of given query image. We present a novel cross-view image matching approach called ArcGeo which introduces a batch-all angular margin loss and several train-time strategies including large-scale pretraining and FoV-based data augmentation. This allows our model to perform well even in challenging cases with limited field-of-view (FoV). Further, we evaluate multiple model architectures, data augmentation approaches and optimization strategies to train a deep cross-view matching network, specifically optimized for limited FoV cases. In low FoV experiments (FoV = 90deg) our method improves top-1 image recall rate on the CVUSA dataset from 30.12% to 43.08%. We also demonstrate improved performance over the state-of-the-art techniques for panoramic cross-view retrieval, improving top-1 recall from 95.43% to 96.06% on the CVUSA dataset and from 64.52% to 79.88% on the CVACT test dataset. Lastly, we evaluate the role of large-scale pretraining for improved robustness. With appropriate pretraining on external data, our model improves top-1 recall dramatically to 66.83% for FoV = 90deg test case on CVUSA, an increase of over twice what is reported by existing approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Shugaev_ArcGeo_Localizing_Limited_Field-of-View_Images_Using_Cross-View_Matching_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shugaev_ArcGeo_Localizing_Limited_Field-of-View_Images_Using_Cross-View_Matching_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484470/,"['Training', 'Computational modeling', 'Image matching', 'Image retrieval', 'Symbols', 'Data augmentation', 'Robustness']","['Cross-view Matching', 'Image Features', 'Data Augmentation', 'Model Architecture', 'Limited Cases', 'Feature Matching', 'Query Image', 'Limited Field Of View', 'Data Augmentation Approach', 'Transformer', 'Image Size', 'Attention Mechanism', 'Face Recognition', 'Image Pairs', 'Aerial Images', 'Latent Space', 'Structure Of Space', 'Performance Gap', 'Image Retrieval', 'Polar Transformation', 'Image Ground', 'View Direction', 'Triplet Loss', 'Training Image Pairs', 'Domain Gap', 'Unknown Directions', 'Input Tokens', 'Ground Features', 'Softmax']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Remote Sensing']",3,"Cross-view matching techniques for image geo-localization attempt to match features in ground-level query images against a collection of satellite images to determine their positions of origin. We present ArcGeo, a novel cross-view image matching approach which introduces a batch-all angular margin loss and several train-time strategies including large-scale pretraining and FoV-based data augmentation. This allows our model to perform well even in challenging cases with limited field-of-view (FoV). Further, we evaluate multiple model architectures, data augmentation approaches and optimization strategies to train a deep cross-view matching network, specifically optimized for limited FoV cases. In low FoV experiments (FoV = 90°) our method improves top-1 image recall rate on the CVUSA dataset from 30.12% to 43.08%. We also demonstrate improved performance over the state-of-the-art techniques for panoramic cross-view retrieval, improving top-1 recall from 95.43% to 96.06% on the CVUSA dataset and from 64.52% to 79.88% on the CVACT test dataset. Lastly, we evaluate the role of large-scale pretraining for improved robustness. With appropriate pretraining on external data, our model improves top-1 recall dramatically to 66.83% for the FoV = 90° test case on CVUSA, an increase of over twice what is reported by existing approaches."
Are Natural Domain Foundation Models Useful for Medical Image Classification?,"Joana Palés Huix, Adithya Raju Ganeshan, Johan Fredin Haslum, Magnus Söderberg, Christos Matsoukas, Kevin Smith","AstraZeneca, Gothenburg, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Science for Life Laboratory, Stockholm, Sweden; AstraZeneca, Gothenburg, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Science for Life Laboratory, Stockholm, Sweden",33.33333333333333,Sweden,66.66666666666667,Sweden,"The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely SAM, SEEM, DINOv2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. DINOv2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Huix_Are_Natural_Domain_Foundation_Models_Useful_for_Medical_Image_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huix_Are_Natural_Domain_Foundation_Models_Useful_for_Medical_Image_Classification_WACV_2024_paper.pdf,,,2310.19522,main,Poster,https://ieeexplore.ieee.org/document/10483777/,"['Training', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Transfer learning', 'Medical services', 'Task analysis']","['Medical Imaging', 'Foundation Model', 'Medical Classification', 'Medical Image Classification', 'Paradigm Shift', 'Computer Vision', 'Classification Task', 'Medical Tasks', 'Medical Datasets', 'ImageNet Pretraining', 'Complex Models', 'Learning Rate', 'Model Evaluation', 'Supervised Learning', 'Training Time', 'Appended', 'Medical Applications', 'Transfer Learning', 'Large Model', 'Domain Shift', 'Wide Range Of Tasks', 'Field Of Computer Vision', 'Linear Classifier', 'Inference Time', 'High Degree Of Adaptation', 'Early Layers']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",4,"The deep learning field is converging towards the use of general foundation models that can be easily adapted for diverse tasks. While this paradigm shift has become common practice within the field of natural language processing, progress has been slower in computer vision. In this paper we attempt to address this issue by investigating the transferability of various state-of-the-art foundation models to medical image classification tasks. Specifically, we evaluate the performance of five foundation models, namely Sam, Seem, Dinov2, BLIP, and OpenCLIP across four well-established medical imaging datasets. We explore different training settings to fully harness the potential of these models. Our study shows mixed results. Dinov2 consistently outperforms the standard practice of ImageNet pretraining. However, other foundation models failed to consistently beat this established baseline indicating limitations in their transferability to medical image classification tasks."
Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble Based Sample Selection,"Akshit Jindal, Vikram Goyal, Saket Anand, Chetan Arora",IIIT-Delhi; IIT Delhi,100.0,India,0.0,,"Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model's capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd's wisdom. Based on the ensemble's collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Jindal_Army_of_Thieves_Enhancing_Black-Box_Model_Extraction_via_Ensemble_Based_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jindal_Army_of_Thieves_Enhancing_Black-Box_Model_Extraction_via_Ensemble_Based_WACV_2024_paper.pdf,,https://github.com/akshitjindal1/AOT_WACV,2311.04588,main,Poster,https://ieeexplore.ieee.org/document/10484430/,"['MIMICs', 'Closed box', 'Training data', 'Semisupervised learning', 'Streaming media', 'Data models', 'Noise measurement']","['Training Data', 'Learning Models', 'Active Learning', 'Machine Learning Models', 'Ensemble Model', 'Semi-supervised Learning', 'Query Efficiency', 'Train Multiple Models', 'Accuracy Of Model', 'Learning Rate', 'Image Classification', 'Individual Models', 'Stochastic Gradient Descent', 'Application Programming Interface', 'Class Imbalance', 'Decision Boundary', 'AlexNet', 'Learning Capacity', 'Probability Vector', 'Ensemble Approach', 'Ensemble Members', 'Output Label', 'Vision Transformer', 'ImageNet Dataset', 'Accuracy Drop', 'Unlabeled Set', 'Projected Gradient Descent', 'Extraction Accuracy']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-Supervised learning to minimize costs. However, in the black-box setting, these approaches may select sub-optimal samples as they train only one thief model. Depending on the thief model’s capacity and the data it was pretrained on, the model might even select noisy samples that harm the learning process. In this work, we explore the usage of an ensemble of deep learning models as our thief model. We call our attack Army of Thieves(AOT) as we train multiple models with varying complexities to leverage the crowd’s wisdom. Based on the ensemble’s collective decision, uncertain samples are selected for querying, while the most confident samples are directly included in the training data. Our approach is the first one to utilize an ensemble of thief models to perform model extraction. We outperform the base approaches of existing state-of-the-art methods by at least 3% and achieve a 21% higher adversarial sample transferability than previous work for models trained on the CIFAR-10 dataset. Code is available at: https://github.com/akshitjindal1/AOT_WACV."
ArtQuest: Countering Hidden Language Biases in ArtVQA,"Tibor Bleidt, Sedigheh Eslami, Gerard de Melo",Hasso Plattner Institute,100.0,Germany,0.0,,"The task of Visual Question Answering (VQA) has been studied extensively on general-domain real-world images. Transferring insights from general domain VQA to the art domain (ArtVQA) is non-trivial, as the latter requires models to identify abstract concepts, details of brushstrokes and styles of paintings in the visual data as well as possess background knowledge about art. This is exacerbated by the lack of high-quality datasets. In this work, we shed light on hidden linguistic biases in the AQUA dataset, which is the only publicly available benchmark dataset for ArtVQA. As a result, the majority of questions can be answered without consulting the visual information, making the ""V"" in ArtVQA rather insignificant. In order to counter this problem, we create a simple, yet practical dataset, ArtQuest, using structured information from the SemArt collection. Our dataset and the pipeline to reproduce our results are publicly available at https://github.com/bletib/artquest.",https://openaccess.thecvf.com/content/WACV2024/html/Bleidt_ArtQuest_Countering_Hidden_Language_Biases_in_ArtVQA_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bleidt_ArtQuest_Countering_Hidden_Language_Biases_in_ArtVQA_WACV_2024_paper.pdf,,https://github.com/bletib/artquest,,main,Poster,https://ieeexplore.ieee.org/document/10484162/,"['Visualization', 'Computer vision', 'Art', 'Pipelines', 'Benchmark testing', 'Linguistics', 'Question answering (information retrieval)']","['Language Bias', 'Hidden Bias', 'Data Visualization', 'Visual Information', 'Language Barriers', 'Benchmark Datasets', 'Question Answering', 'Domain Generalization', 'Majority Of Questions', 'Visual Question', 'Visual Question Answering', 'Artistic Domains', 'Validation Set', 'Challenging Task', 'General Approach', 'Natural Language', 'Accuracy Scores', 'Types Of Questions', 'Encoder-decoder', 'Language Model', 'Image Presentation', 'Qualitative Examples']","['Applications', 'Arts / games / social media', 'Algorithms', 'Datasets and evaluations']",,"The task of Visual Question Answering (VQA) has been studied extensively on general-domain real-world images. Transferring insights from general domain VQA to the art domain (ArtVQA) is non-trivial, as the latter requires models to identify abstract concepts, details of brushstrokes and styles of paintings in the visual data as well as possess background knowledge about art. This is exacerbated by the lack of high-quality datasets. In this work, we shed light on hidden linguistic biases in the AQUA dataset, which is the only publicly available benchmark dataset for ArtVQA. As a result, the majority of questions can be answered without consulting the visual information, making the “V” in ArtVQA rather insignificant. In order to counter this problem, we create a simple, yet practical dataset, ArtQuest, using structured information from the SemArt collection. Our dataset and the pipeline to reproduce our results are publicly available at https://github.com/bletib/artquest."
AssemblyNet: A Point Cloud Dataset and Benchmark for Predicting Part Directions in an Exploded Layout,"Jesper Gaarsdal, Joakim Bruslund Haurum, Sune Wolff, Claus Brøndgaard Madsen","Computer Graphics Group, Aalborg University, Denmark; SynergyXR ApS, Aarhus, Denmark; Visual Analysis and Perception (V AP) Laboratory, Aalborg University & Pioneer Centre for AI, Denmark",66.66666666666666,Denmark,33.33333333333334,Denmark,"Exploded views are powerful tools for visualizing the assembly and disassembly of complex objects, widely used in technical illustrations, assembly instructions, and product presentations. Previous methods for automating the creation of exploded views are either slow and computationally costly or compromise on accuracy. Therefore, the construction of exploded views is typically a manual process. In this paper, we propose a novel approach for automatically predicting the direction of parts in an exploded view using deep learning. To achieve this, we introduce a new dataset, AssemblyNet, which contains point cloud data sampled from 3D models of real-world assemblies, including water pumps, mixed industrial assemblies, and LEGO models. The AssemblyNet dataset includes a total of 44 assemblies, separated into 495 subassemblies with a total of 5420 parts. We provide ground truth labels for regression and classification, representing the directions in which the parts are moved in the exploded views. We also provide performance benchmarks using various state-of-the-art models for shape classification on point clouds and propose a novel two-path network architecture. Project page available at https://github.com/jgaarsdal/AssemblyNet",https://openaccess.thecvf.com/content/WACV2024/html/Gaarsdal_AssemblyNet_A_Point_Cloud_Dataset_and_Benchmark_for_Predicting_Part_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gaarsdal_AssemblyNet_A_Point_Cloud_Dataset_and_Benchmark_for_Predicting_Part_WACV_2024_paper.pdf,,https://github.com/jgaarsdal/AssemblyNet,,main,Poster,https://ieeexplore.ieee.org/document/10484418/,"['Point cloud compression', 'Solid modeling', 'Three-dimensional displays', 'Manuals', 'Benchmark testing', 'Predictive models', 'Network architecture']","['Point Cloud', 'Point Cloud Dataset', 'Deep Learning', 'Classification Model', 'Ground Truth Labels', 'Point Cloud Data', 'Training Set', 'Classification Task', 'Ordinal Regression', 'Multilayer Perceptron', 'Bounding Box', 'Computer-aided Design', 'Segmentation Task', 'Graph Convolutional Network', 'Manufacturing Companies', '3D Point Cloud', 'Set Of Classifiers', 'Encoder Layer', 'Point Cloud Classification']","['Applications', 'Visualization', 'Algorithms', '3D computer vision', 'Algorithms', 'Datasets and evaluations']",,"Exploded views are powerful tools for visualizing the assembly and disassembly of complex objects, widely used in technical illustrations, assembly instructions, and product presentations. Previous methods for automating the creation of exploded views are either slow and computationally costly or compromise on accuracy. Therefore, the construction of exploded views is typically a manual process. In this paper, we propose a novel approach for automatically predicting the direction of parts in an exploded view using deep learning. To achieve this, we introduce a new dataset, AssemblyNet, which contains point cloud data sampled from 3D models of real-world assemblies, including water pumps, mixed industrial assemblies, and LEGO models. The AssemblyNet dataset includes a total of 44 assemblies, separated into 495 subassemblies with a total of 5420 parts. We provide ground truth labels for regression and classification, representing the directions in which the parts are moved in the exploded views. We also provide performance benchmarks using various state-of-the-art models for shape classification on point clouds and propose a novel two-path network architecture. Project page available at https://github.com/jgaarsdal/AssemblyNet"
Assessing Neural Network Robustness via Adversarial Pivotal Tuning,"Peter Ebert Christensen, Vésteinn Snæbjarnarson, Andrea Dittadi, Serge Belongie, Sagie Benaim",Hebrew University of Jerusalem; University of Copenhagen; Helmholtz AI,66.66666666666666,"Denmark, Israel",33.33333333333334,Germany,"The robustness of image classifiers is essential to their deployment in the real world. The ability to assess this resilience to manipulations or deviations from the training data is thus crucial. These modifications have traditionally consisted of minimal changes that still manage to fool classifiers, and modern approaches are increasingly robust to them. Semantic manipulations that modify elements of an image in meaningful ways have thus gained traction for this purpose. However, they have primarily been limited to style, color, or attribute changes. While expressive, these manipulations do not make use of the full capabilities of a pretrained generative model. In this work, we aim to bridge this gap. We show how a pretrained image generator can be used to semantically manipulate images in a detailed, diverse, and photorealistic way while still preserving the class of the original image. Inspired by recent GAN-based image inversion methods, we propose a method called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a pivot latent space input that reconstructs the image using a pretrained generator. It then adjusts the generator's weights to create small yet semantic manipulations in order to fool a pretrained classifier. APT preserves the full expressive editing capabilities of the generative model. We demonstrate that APT is capable of a wide range of class-preserving semantic image manipulations that fool a variety of pretrained classifiers. Finally, we show that classifiers that are robust to other benchmarks are not robust to APT manipulations and suggest a method to improve them.",https://openaccess.thecvf.com/content/WACV2024/html/Christensen_Assessing_Neural_Network_Robustness_via_Adversarial_Pivotal_Tuning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Christensen_Assessing_Neural_Network_Robustness_via_Adversarial_Pivotal_Tuning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484437/,"['Training', 'Semantics', 'Neural networks', 'Training data', 'Benchmark testing', 'Robustness', 'Generators']","['Robustness Of Neural Networks', 'Use Of Imaging', 'Latent Space', 'Image Generation', 'Inverse Method', 'Robust Classification', 'Validation Set', 'Latent Variables', 'Input Image', 'Maximum Distance', 'Diffusion Model', 'Target Class', 'Types Of Attacks', 'Group Of Images', 'L1-norm', 'Noise Vector', 'Reconstruction Loss', 'Latent Vector', 'Accuracy Drop', 'Adversarial Attacks', 'Image X', 'Style Changes', 'Distance Perception', 'Attack Success Rate', 'Adversarial Robustness', 'Latent Code']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",,"The robustness of image classifiers is essential to their deployment in the real world. The ability to assess this resilience to manipulations or deviations from the training data is thus crucial. These modifications have traditionally consisted of minimal changes that still manage to fool classifiers, and modern approaches are increasingly robust to them. Semantic manipulations that modify elements of an image in meaningful ways have thus gained traction for this purpose. However, they have primarily been limited to style, color, or attribute changes. While expressive, these manipulations do not make use of the full capabilities of a pretrained generative model. In this work, we aim to bridge this gap. We show how a pretrained image generator can be used to semantically manipulate images in a detailed, diverse, and photorealistic way while still preserving the class of the original image. Inspired by recent GAN-based image inversion methods, we propose a method called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a pivot latent space input that reconstructs the image using a pretrained generator. It then adjusts the generator’s weights to create small yet semantic manipulations in order to fool a pretrained classifier. APT preserves the full expressive editing capabilities of the generative model. We demonstrate that APT is capable of a wide range of class-preserving semantic image manipulations that fool a variety of pretrained classifiers. Finally, we show that classifiers that are robust to other benchmarks are not robust to APT manipulations and suggest a method to improve them."
Assist Is Just As Important as the Goal: Image Resurfacing To Aid Model's Robust Prediction,"Abhijith Sharma, Phil Munz, Apurva Narayan","TrojAI Inc., NB, Canada; University of British Columbia, BC, Canada; Western University, ON, Canada",66.66666666666666,Canada,33.33333333333334,Canada,"Adversarial patches threaten visual AI models in the real world. The number of patches in a patch attack is variable and determines the attack's potency in a specific environment. Most existing defenses assume a single patch in the scene, and the multiple patch scenario are shown to overcome them. This paper presents a model-agnostic defense against patch attacks based on total variation for image resurfacing (TVR). The TVR is an image-cleansing method that processes images to remove probable adversarial regions. TVR can be utilized solely or augmented with a defended model, providing multi-level security for robust prediction. TVR nullifies the influence of patches in a single image scan with no prior assumption on the number of patches in the scene. We validate TVR on the ImageNet-Patch benchmark dataset and with real-world physical objects, demonstrating its ability to mitigate patch attack.",https://openaccess.thecvf.com/content/WACV2024/html/Sharma_Assist_Is_Just_As_Important_as_the_Goal_Image_Resurfacing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sharma_Assist_Is_Just_As_Important_as_the_Goal_Image_Resurfacing_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Asymmetric Image Retrieval With Cross Model Compatible Ensembles,"Alon Shoshan, Ori Linial, Nadav Bhonker, Elad Hirsch, Lior Zamir, Igor Kviatkovsky, Gérard Medioni",Technion - Israel Institute of Technology; Amazon,50.0,Israel,50.0,USA,"The asymmetrical retrieval setting is a well suited solution for resource constrained applications such as face recognition and image retrieval. In this setting, a large model is used for indexing the gallery while a lightweight model is used for querying. The key principle in such systems is ensuring that both models share the same embedding space. Most methods in this domain are based on knowledge distillation. While useful, they suffer from several drawbacks: they are upper-bounded by the performance of the single best model found and cannot be extended to use an ensemble of models in a straightforward manner. In this paper we present an approach that does not rely on knowledge distillation, rather it utilizes embedding transformation models. This allows the use of N independently trained and diverse gallery models (e.g., trained on different datasets or having a different architecture) and a single query model. As a result, we improve the overall accuracy beyond that of any single model while maintaining a low computational budget for querying. Additionally, we propose a gallery image rejection method that utilizes the diversity between multiple transformed embeddings to estimate the uncertainty of gallery images.",https://openaccess.thecvf.com/content/WACV2024/html/Shoshan_Asymmetric_Image_Retrieval_With_Cross_Model_Compatible_Ensembles_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shoshan_Asymmetric_Image_Retrieval_With_Cross_Model_Compatible_Ensembles_WACV_2024_paper.pdf,,,2303.17531,main,Poster,https://ieeexplore.ieee.org/document/10484081/,"['Training', 'Computer vision', 'Uncertainty', 'Computational modeling', 'Face recognition', 'Image retrieval', 'Diversity reception']","['Image Retrieval', 'Face Recognition', 'Ensemble Model', 'Latent Space', 'Face Images', 'Transformer Model', 'Domain Method', 'Lightweight Model', 'Gallery Images', 'Training Data', 'Single Image', 'Computational Resources', 'Training Strategy', 'Ensemble Approach', 'Embedding Vectors', 'Retrieval Accuracy', 'Multiple Spaces', 'Query Image', 'Ensemble Size', 'Symmetric Set', 'Gallery Set', 'False Acceptance Rate', 'Ensemble Diversity']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"The asymmetrical retrieval setting is a well suited solution for resource constrained applications such as face recognition and image retrieval. In this setting, a large model is used for indexing the gallery while a lightweight model is used for querying. The key principle in such systems is ensuring that both models share the same embedding space. Most methods in this domain are based on knowledge distillation. While useful, they suffer from several drawbacks: they are upper-bounded by the performance of the single best model found and cannot be extended to use an ensemble of models in a straightforward manner. In this paper we present an approach that does not rely on knowledge distillation, rather it utilizes embedding transformation models. This allows the use of N independently trained and diverse gallery models (e.g., trained on different datasets or having a different architecture) and a single query model. As a result, we improve the overall accuracy beyond that of any single model while maintaining a low computational budget for querying. Additionally, we propose a gallery image rejection method that utilizes the diversity between multiple transformed embeddings to estimate the uncertainty of gallery images."
Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study,"André Luiz Vieira e Silva, Francisco Simões, Danny Kowerko, Tobias Schlosser, Felipe Battisti, Veronica Teichrieb","Junior Professorship of Media Computing, Chemnitz University of Technology, Germany; Visual Computing Lab, DC, Universidade Federal Rural de Pernambuco, Brazil; Voxar Labs, Centro de Informática, Universidade Federal de Pernambuco, Brazil",100.0,"Brazil, Germany",0.0,,"Within (semi-)automated visual industrial inspection, learning-based approaches for assessing visual defects, including deep neural networks, enable the processing of otherwise small defect patterns in pixel size on high-resolution imagery. The emergence of these often rarely occurring defect patterns explains the general need for labeled data corpora. To alleviate this issue and advance the current state of the art in unsupervised visual inspection, this work proposes a DifferNet-based solution enhanced with attention modules: AttentDifferNet. It improves image-level detection and classification capabilities on three visual anomaly detection datasets for industrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. In comparison to the state of the art, AttentDifferNet achieves improved results, which are, in turn, highlighted throughout our quali-quantitative study. Our quantitative evaluation shows an average improvement - compared to DifferNet - of 1.77 +- 0.25 percentage points in overall AUROC considering all three datasets, reaching SOTA results in InsPLAD-fault, an industrial inspection in-the-wild dataset. As our variants to AttentDifferNet show great prospects in the context of currently investigated approaches, a baseline is formulated, emphasizing the importance of attention for industrial anomaly detection both in the wild and in controlled environments.",https://openaccess.thecvf.com/content/WACV2024/html/Vieira_e_Silva_Attention_Modules_Improve_Image-Level_Anomaly_Detection_for_Industrial_Inspection_A_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Vieira_e_Silva_Attention_Modules_Improve_Image-Level_Anomaly_Detection_for_Industrial_Inspection_A_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483952/,"['Visualization', 'Computer vision', 'Artificial neural networks', 'Inspection', 'Anomaly detection']","['Attention Module', 'Anomaly Detection', 'Industrial Inspection', 'Image-level Anomaly Detection', 'Neural Network', 'Visual Inspection', 'Deep Neural Network', 'Percentage Points', 'Environmental Control', 'Patterning Defects', 'Classification Capability', 'Convolutional Neural Network', 'Feature Maps', 'Attention Mechanism', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Independent Component Analysis', 'Unmanned Aerial Vehicles', 'Latent Space', 'Spatial Attention', 'Anomaly Detection Methods', 'Attention Block', 'Normal Flow', 'Power Line', 'Vision Transformer', 'Channel Attention', 'Pixel Level', 'Memory Bank', 'AlexNet', 'Foreground Objects']","['Applications', 'Remote Sensing', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Within (semi-)automated visual industrial inspection, learning-based approaches for assessing visual defects, including deep neural networks, enable the processing of otherwise small defect patterns in pixel size on high-resolution imagery. The emergence of these often rarely occurring defect patterns explains the general need for labeled data corpora. To alleviate this issue and advance the current state of the art in unsupervised visual inspection, this work proposes a DifferNet-based solution enhanced with attention modules: AttentDifferNet. It improves image-level detection and classification capabilities on three visual anomaly detection datasets for industrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. In comparison to the state of the art, AttentDifferNet achieves improved results, which are, in turn, highlighted throughout our quali-quantitative study. Our quantitative evaluation shows an average improvement - compared to DifferNet -of 1.77 ± 0.25 percentage points in overall AUROC considering all three datasets, reaching SOTA results in InsPLAD-fault, an industrial inspection in-the-wild dataset. As our variants to AttentDifferNet show great prospects in the context of currently investigated approaches, a baseline is formulated, emphasizing the importance of attention for industrial anomaly detection both in the wild and in controlled environments."
Attention-Guided Prototype Mixing: Diversifying Minority Context on Imbalanced Whole Slide Images Classification Learning,"Farchan Hakim Raswa, Chun-Shien Lu, Jia-Ching Wang","IIS, Academia Sinica, Taiwan, ROC; National Central University, Taiwan, ROC",100.0,Taiwan,0.0,,"Real-world medical datasets often suffer from class imbalance, which can lead to degraded performance due to limited samples of the minority class. In another line of research, Transformer-based multiple instance learning (Transformer-MIL) has shown promise in addressing the pairwise correlation between instances in medical whole slide images (WSIs) with gigapixel resolution and non-uniform sizes. However, these characteristics pose challenges for state-of-the-art (SOTA) oversampling methods aiming at diversifying the minority context in imbalanced WSIs. In this paper, we propose an Attention-Guided Prototype Mixing scheme at the WSI level. We leverage Transformer-MIL training to determine the distribution of semantic instances and identify relevant instances for cutting and pasting across different WSI (bag of instances). To our knowledge, applying Transformer is often limited by memory requirements and time complexity, particularly when dealing with gigabyte-sized WSIs. We introduce the concept of prototype instances that have smaller representations while preserving the uniform size and intrinsic features of the WSI. We demonstrate that our proposed method can boost performance compared to competitive SOTA oversampling and augmentation methods at an imbalanced WSI level.",https://openaccess.thecvf.com/content/WACV2024/html/Raswa_Attention-Guided_Prototype_Mixing_Diversifying_Minority_Context_on_Imbalanced_Whole_Slide_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Raswa_Attention-Guided_Prototype_Mixing_Diversifying_Minority_Context_on_Imbalanced_Whole_Slide_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484414/,"['Training', 'Pairwise error probability', 'Image resolution', 'Semantics', 'Memory management', 'Prototypes', 'Transformers']","['Prototype', 'Image Classification', 'Slide Images', 'Minority Context', 'Medical Imaging', 'Time Complexity', 'Uniform Size', 'Real-world Datasets', 'Class Imbalance', 'Minority Class', 'Digital Pathology', 'Cut-and-paste', 'Medical Datasets', 'Small Representation', 'Non-uniform Size', 'Multiple Instance Learning', 'Relevant Instances', 'Oversampling Methods', 'Major Classes', 'Classification Of Tumors', 'Minority Samples', 'Normal Images', 'Imbalanced Datasets', 'Imbalanced Class Distribution', 'Transformation Module', 'Attention Scores', 'Tumor Slides', 'Classifier Construction', 'Oversampling Technique', 'State Of The Art Methods']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Real-world medical datasets often suffer from class imbalance, which can lead to degraded performance due to limited samples of the minority class. In another line of research, Transformer-based multiple instance learning (Transformer-MIL) has shown promise in addressing the pairwise correlation between instances in medical whole slide images (WSIs) with gigapixel resolution and non-uniform sizes. However, these characteristics pose challenges for state-of-the-art (SOTA) oversampling methods aiming at diversifying the minority context in imbalanced WSIs.In this paper, we propose an Attention-Guided Prototype Mixing scheme at the WSI level. We leverage Transformer-MIL training to determine the distribution of semantic instances and identify relevant instances for cutting and pasting across different WSI (bag of instances). To our knowledge, applying Transformer is often limited by memory requirements and time complexity, particularly when dealing with gigabyte-sized WSIs. We introduce the concept of prototype instances that have smaller representations while preserving the uniform size and intrinsic features of the WSI.We demonstrate that our proposed method can boost performance compared to competitive SOTA oversampling and augmentation methods at an imbalanced WSI level."
Attentive Prototypes for Source-Free Unsupervised Domain Adaptive 3D Object Detection,"Deepti Hegde, Vishal M. Patel",Johns Hopkins University,100.0,USA,0.0,,"3D object detection networks tend to be biased towards the data they are trained on. Evaluation on datasets captured in different locations, conditions or sensors than that of the training (source) data results in a drop in model performance due to the gap in distribution with the test (or target) data. Current methods for domain adaptation either assume access to source data during training, which may not be available due to privacy or memory concerns, or require a sequence of lidar frames as an input. We propose a single-frame approach for source-free, unsupervised domain adaptation of lidar-based 3D object detectors that uses class prototypes to mitigate the effect pseudo-label noise. Addressing the limitations of traditional feature aggregation methods for prototype computation in the presence of noisy labels, we utilize a transformer module to identify outlier ROI's that correspond to incorrect, over-confident annotations, and compute an attentive class prototype. Under an iterative training strategy, the losses associated with noisy pseudo labels are down-weighed and thus refined in the process of self-training. To validate the effectiveness of our proposed approach, we examine the domain shift associated with networks trained on large, label-rich datasets (such as the Waymo Open Dataset and nuScenes) and evaluate on smaller, label-poor datasets (such as KITTI) and vice-versa. We demonstrate our approach on two recent object detectors and achieve results that out-perform the other domain adaptation works.",https://openaccess.thecvf.com/content/WACV2024/html/Hegde_Attentive_Prototypes_for_Source-Free_Unsupervised_Domain_Adaptive_3D_Object_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hegde_Attentive_Prototypes_for_Source-Free_Unsupervised_Domain_Adaptive_3D_Object_Detection_WACV_2024_paper.pdf,,,2111.15656,main,Poster,https://ieeexplore.ieee.org/document/10484491/,"['Training', 'Three-dimensional displays', 'Noise', 'Memory management', 'Prototypes', 'Object detection', 'Detectors']","['Object Detection', 'Domain Adaptation', '3D Object Detection', 'Data Sources', 'Test Data', 'Target Domain', 'Target Data', 'Object Detection Network', 'Noisy Labels', 'Recent Adaptation', 'Class Prototypes', 'Transformation Module', 'Feature Representation', 'Regional Characteristics', 'Pedestrian', 'Attention Mechanism', 'Point Cloud', 'Multilayer Perceptron', 'Bounding Box', 'Global Context', 'Source Domain', 'Presence Of Noise', 'Unsupervised Domain Adaptation Methods', 'Final Prototype', 'Entropy Weight', 'Classification Branch', 'Domain Shift', 'Region Proposal Network', '3D Scene', 'Target Dataset']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",5,"3D object detection networks tend to be biased towards the data they are trained on. It has been demonstrated that the evaluation on datasets captured in different locations, conditions or with sensors of different specifications than that of the training (source) data results in a drop in model performance due to the domain gap with the test (or target) data. Current methods for adapting to the target domain data either assume access to source data during training, which may not be available due to privacy or memory concerns, or require a sequence of LiDAR frames as an input. We propose a single-frame approach for source-free, un-supervised domain adaptation of LiDAR-based 3D object detectors that uses class prototypes to mitigate the effect of pseudo-label noise. Addressing the limitations of traditional feature aggregation methods for prototype computation in the presence of noisy labels, we utilize a transformer module to identify outlier regions that correspond to incorrect, over-confident annotations, and compute an attentive class prototype. The losses associated with noisy pseudo-labels are down-weighed in the process of self-training. We demonstrate our approach on two recent object detectors and show that our method outperforms recent source-free domain adaptation works as well as those that leverage source information during training. The code will be made available."
Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models,"Jingru Yi, Burak Uzkent, Oana Ignat, Zili Li, Amanmeet Garg, Xiang Yu, Linda Liu",Amazon Prime Video,0.0,,100.0,USA,"Grounding-based vision and language models have been successfully applied to low-level vision tasks, aiming to precisely locate objects referred in captions. The effectiveness of grounding representation learning heavily relies on the scale of the training dataset. Despite being a useful data enrichment strategy, data augmentation has received minimal attention in existing vision and language tasks as augmentation for image-caption pairs is non-trivial. In this study, we propose a robust phrase grounding model trained with text-conditioned and text-unconditioned data augmentations. Specifically, we apply text-conditioned color jittering and horizontal flipping to ensure semantic consistency between images and captions. To guarantee image-caption correspondence in the training samples, we modify the captions according to pre-defined keywords when applying horizontal flipping. Additionally, inspired by recent masked signal reconstruction, we propose to use pixel-level masking as a novel form of data augmentation. While we demonstrate our data augmentation method with MDETR framework, the proposed approach is applicable to common grounding-based vision and language tasks with other frameworks. Finally, we show that larger capacity image encoder such as CLIP can further improve the results. Through extensive experiments on three commonly applied datasets: Flickr30k, referring expressions, and GQA, our method demonstrates advanced performance over the state-of-the-arts with various metrics. Code can be found in https://github.com/amzn/augment-the-pairs-wacv2024.",https://openaccess.thecvf.com/content/WACV2024/html/Yi_Augment_the_Pairs_Semantics-Preserving_Image-Caption_Pair_Augmentation_for_Grounding-Based_Vision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yi_Augment_the_Pairs_Semantics-Preserving_Image-Caption_Pair_Augmentation_for_Grounding-Based_Vision_WACV_2024_paper.pdf,,https://github.com/amzn/augment-the-pairs-wacv2024,2311.02536,main,Poster,https://ieeexplore.ieee.org/document/10484174/,"['Training', 'Representation learning', 'Measurement', 'Grounding', 'Image color analysis', 'Semantics', 'Data augmentation']","['Language Model', 'Visual Model', 'Image-caption Pairs', 'Semantic', 'Training Dataset', 'Data Augmentation', 'Vision Tasks', 'Horizontal Flip', 'Language Tasks', 'Image Encoder', 'Color Jittering', 'Ablation', 'Image Features', 'Input Image', 'Additive Noise', 'Object Detection', 'Multilayer Perceptron', 'Bounding Box', 'Color Space', 'Gaussian Blur', 'Average Precision Values', 'Bounding Box Regression', 'Pre-training Dataset', 'Pre-training Stage', 'Box Coordinates', 'Contrastive Loss', 'List Of Keywords', 'Language Representation', 'Object Regions', 'Augmentation Strategy']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Robotics']",,"Grounding-based vision and language models have been successfully applied to low-level vision tasks, aiming to precisely locate objects referred in captions. The effectiveness of grounding representation learning heavily relies on the scale of the training dataset. Despite being a useful data enrichment strategy, data augmentation has received minimal attention in existing vision and language tasks as augmentation for image-caption pairs is non-trivial. In this study, we propose a robust phrase grounding model trained with text-conditioned and text-unconditioned data augmentations. Specifically, we apply text-conditioned color jittering and horizontal flipping to ensure semantic consistency between images and captions. To guarantee image-caption correspondence in the training samples, we modify the captions according to pre-defined keywords when applying horizontal flipping. Additionally, inspired by recent masked signal reconstruction, we propose to use pixel-level masking as a novel form of data augmentation. While we demonstrate our data augmentation method with MDETR framework, the proposed approach is applicable to common grounding-based vision and language tasks with other frameworks. Finally, we show that image encoder pretrained on large-scale image and language datasets (such as CLIP) can further improve the results. Through extensive experiments on three commonly applied datasets: Flickr30k, referring expressions and GQA, our method demonstrates advanced performance over the state-of-the-arts with various metrics. Code can be found in https://github.com/amzn/augment-the-pairs-wacv2024."
Auto-BPA: An Enhanced Ball-Pivoting Algorithm With Adaptive Radius Using Contextual Bandits,"Houda Saffi, Naima Otberdout, Youssef Hmamouche, Amal El Fallah Seghrouchni","Ai movement - International Artificial Intelligence Center of Morocco, University Mohammed VI Polytechnic, Rabat, Morocco; Ai movement - International Artificial Intelligence Center of Morocco, University Mohammed VI Polytechnic, Rabat, Morocco; Sorbonne Universit ´e, LIP6 - UMR 7606 CNRS, France",100.0,"France, Morocco",0.0,,"The Ball-Pivoting Algorithm (BPA) is a notable technique for 3D surface reconstruction from point clouds, heavily reliant on the ball radius. In practical application, determining the optimal radius for BPA often necessitates iterative experimentation to achieve better reconstruction quality. BPA entails geometric computations like iterative pivoting, inherently lacking differentiability. In this paper, we tackle the dual challenges of radius selection and non-differentiability in BPA. Inspired by contextual bandits, we propose an innovative approach that learns the optimal radius based on local geometric features within point clouds. We validate our method on the ModelNet10 and ShapeNet datasets, showcasing superior surface reconstruction compared to manual tuning and other classic methods both for low and high point cloud densities. Our code is available at https://github.com/houda-pixel/Auto-BPA.",https://openaccess.thecvf.com/content/WACV2024/html/Saffi_Auto-BPA_An_Enhanced_Ball-Pivoting_Algorithm_With_Adaptive_Radius_Using_Contextual_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Saffi_Auto-BPA_An_Enhanced_Ball-Pivoting_Algorithm_With_Adaptive_Radius_Using_Contextual_WACV_2024_paper.pdf,,https://github.com/houda-pixel/Auto-BPA,,main,Poster,https://ieeexplore.ieee.org/document/10484203/,"['Point cloud compression', 'Surface reconstruction', 'Three-dimensional displays', 'Merging', 'Manuals', 'Prediction algorithms', 'User experience']","['Contextual Bandit', 'Adaptive Radius', 'Ball-Pivoting Algorithm', 'Point Cloud', 'Reconstruction Quality', 'Surface Reconstruction', 'Computational Geometry', 'Dense Point Cloud', 'Optimal Radius', 'Neural Network', 'Deep Learning', 'K-means', 'Continuous Action', 'Non-uniform Distribution', 'Set Of Data Points', 'Policy Learning', 'Value Of Radius', 'Reconstruction Loss', 'Computer Graphics', 'Problem Context', 'Multi-armed Bandit', 'Chamfer Distance', 'Radius Parameter', 'Delaunay Triangulation', 'Continuous Action Space', 'Input Point Cloud', 'Surface Normals', 'Fast Feature', 'Hole Surface', 'Effective Radius']","['Algorithms', '3D computer vision']",,"The Ball-Pivoting Algorithm (BPA) is a notable technique for 3D surface reconstruction from point clouds, heavily reliant on the ball radius. In practical application, determining the optimal radius for BPA often necessitates iterative experimentation to achieve better reconstruction quality. BPA entails geometric computations like iterative pivoting, inherently lacking differentiability. In this paper, we tackle the dual challenges of radius selection and non-differentiability in BPA. Inspired by contextual bandits, we propose an innovative approach that learns the optimal radius based on local geometric features within point clouds. We validate our method on the ModelNet10 and ShapeNet datasets, showcasing superior surface reconstruction compared to manual tuning and other classic methods both for low and high point cloud densities. Our code is available at https://github.com/houda-pixel/Auto-BPA."
Automated Camera Calibration via Homography Estimation With GNNs,"Giacomo D'Amicantonio, Egor Bondarev, Peter H.N. de With",Eindhoven University of Technology,100.0,Netherlands,0.0,,"Over the past few decades, a significant rise of camera-based applications for traffic monitoring has occurred. Governments and local administrations are increasingly relying on the data collected from these cameras to enhance road safety and optimize traffic conditions. However, for effective data utilization, it is imperative to ensure accurate and automated calibration of the involved cameras. This paper proposes a novel approach to address this challenge by leveraging the topological structure of intersections. We propose a framework involving the generation of a set of synthetic intersection viewpoint images from a bird's-eye-view image, framed as a graph of virtual cameras to model these images. Using the capabilities of Graph Neural Networks, we effectively learn the relationships within this graph, thereby facilitating the estimation of a homography matrix. This estimation leverages the neighbourhood representation for any real-world camera and is enhanced by exploiting multiple images instead of a single match. In turn, the homography matrix allows the retrieval of extrinsic calibration parameters. As a result, the proposed framework demonstrates superior performance on both synthetic datasets and real-world cameras, setting a new state-of-the-art benchmark.",https://openaccess.thecvf.com/content/WACV2024/html/DAmicantonio_Automated_Camera_Calibration_via_Homography_Estimation_With_GNNs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/DAmicantonio_Automated_Camera_Calibration_via_Homography_Estimation_With_GNNs_WACV_2024_paper.pdf,,,2311.02598,main,Poster,https://ieeexplore.ieee.org/document/10483620/,"['Training', 'Image segmentation', 'Refining', 'Estimation', 'Cameras', 'Graph neural networks', 'Road safety']","['Graph Neural Networks', 'Camera Calibration', 'Homography Estimation', 'Synthetic Images', 'Extrinsic Parameters', 'Single Match', 'Virtual Camera', 'Homography Matrix', 'Differences In Performance', 'Input Image', 'Neighboring Nodes', 'Matching Process', 'Graph Convolutional Network', 'Spatial Network', 'Ground Truth Image', 'Cycle Path', 'Link Prediction', 'Graph Attention Network', 'Graph Neural Network Model', 'Spatial Transformer Network']","['Applications', 'Visualization', 'Applications', 'Autonomous Driving', 'Applications', 'Structural engineering / civil engineering']",3,"Over the past few decades, a significant rise of camera-based applications for traffic monitoring has occurred. Governments and local administrations are increasingly relying on the data collected from these cameras to enhance road safety and optimize traffic conditions. However, for effective data utilization, it is imperative to ensure accurate and automated calibration of the involved cameras. This paper proposes a novel approach to address this challenge by leveraging the topological structure of intersections.We propose a framework involving the generation of a set of synthetic intersection viewpoint images from a bird’seye-view image, framed as a graph of virtual cameras to model these images. Using the capabilities of Graph Neural Networks, we effectively learn the relationships within this graph, thereby facilitating the estimation of a homography matrix. This estimation leverages the neighbourhood representation for any real-world camera and is enhanced by exploiting multiple images instead of a single match. In turn, the homography matrix allows the retrieval of extrinsic calibration parameters. As a result, the proposed framework demonstrates superior performance on both synthetic datasets and real-world cameras, setting a new state-of-the-art benchmark."
Automated Monitoring of Ear Biting in Pigs by Tracking Individuals and Events,"Anicetus Odo, Niall McLaughlin, Ilias Kyriazakis","Queen’s University Belfast, School of EEECS, United Kingdom; Queen’s University Belfast, School of Biological Sciences, United Kingdom",100.0,Canada,0.0,,"We propose a system for automated monitoring of ear-biting in pigs. Ear-biting presents a welfare challenge to commercial pig farming, leading to injuries and infections that affect animal welfare. We use a computer vision system to detect and track all pigs and ear-biting events. Our goal is to provide early warning of ear-biting to allow quick intervention to improve the health and welfare of commercial farm animals. We compare several different object detection methods for the detection of individual pigs, including an oriented bounding box detector, which is better suited to the accurate detection of pigs from overhead cameras. We track all pigs and all ear-biting events using a specialised two-stage multi-object tracking system. The tracking system is adapted to match the characteristics of each entity being tracked. The tracking system allows the individual pigs involved in an ear-biting incident to be identified, allowing for targeted welfare interventions. We evaluate our complete system on real farm videos and demonstrate that our complete system improves compared to existing ear-biting detection methods.",https://openaccess.thecvf.com/content/WACV2024/html/Odo_Automated_Monitoring_of_Ear_Biting_in_Pigs_by_Tracking_Individuals_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Odo_Automated_Monitoring_of_Ear_Biting_in_Pigs_by_Tracking_Individuals_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484109/,"['Computer vision', 'Target tracking', 'Animals', 'Ear', 'Object detection', 'Cameras', 'Robustness']","['Ear Biting', 'Biting In Pigs', 'Computer Vision', 'Object Detection', 'Tracking System', 'Bounding Box', 'Commercial Farms', 'Individual Pigs', 'Computer Vision System', 'Overhead Camera', 'Multi-object Tracking', 'False Positive', 'Training Set', 'Validation Set', 'Intersection Over Union', 'Real Effect', 'Stochastic Gradient Descent', 'Detection Model', 'Video Frames', 'Methods In The Literature', 'Minimum Bounding Box', 'Set Of Tracks', 'Tracking Of Events', 'Current Window', 'Tracking Accuracy', 'Contact Behavior', 'Consecutive Frames', 'Use Of Tags', 'Radio Frequency Identification', 'Appearance Information']","['Applications', 'Agriculture', 'Applications', 'Animals / Insects']",,"We propose a system for automated monitoring of ear biting in pigs. Ear-biting presents a welfare challenge to commercial pig farming, leading to injuries and infections that affect animal welfare. We use a computer vision system to detect and track all pigs and ear-biting events. Our goal is to provide early warning of ear-biting to allow quick intervention to improve the health and welfare of commercial farm animals. We compare several different object detection methods for the detection of individual pigs, including an oriented bounding box detector, which is better suited to the accurate detection of pigs from overhead cameras. We track all pigs and all ear-biting events using a specialised two-stage multi-object tracking system. The tracking system is adapted to match the characteristics of each entity being tracked. The tracking system allows the individual pigs involved in an ear-biting incident to be identified, allowing for targeted welfare interventions. We evaluate our complete system on real farm videos and demonstrate that our complete system improves compared to existing ear-biting detection methods."
Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition,"Takuro Fujii, Hayato Nakagawa, Teppei Takeshima, Yasushi Yumura, Tomoki Hamagami",Yokohama National University; Yokohama City University Medical Center,100.0,Japan,0.0,,"Infertility is a global health problem, and an increasing number of couples are seeking medical assistance to achieve reproduction, at least half of which are caused by men. The success rate of assisted reproductive technologies depends on sperm assessment, in which experts determine whether sperm can be used for reproduction based on morphology and motility of sperm. Previous sperm assessment studies with deep learning have used datasets comprising images that include only sperm heads, which cannot consider motility and other morphologies of sperm. Furthermore, the labels of the dataset are one-hot, which provides insufficient support for experts, because assessment results are inconsistent between experts, and they have no absolute answer. Therefore, we constructed the video dataset for sperm assessment whose videos include sperm head as well as neck and tail, and its labels were annotated with soft-label. Furthermore, we proposed the sperm assessment framework and the neural network, RoSTFine, for sperm video recognition. Experimental results showed that RoSTFine could improve the sperm assessment performances compared to existing video recognition models and focus strongly on important sperm parts (i.e., head and neck).",https://openaccess.thecvf.com/content/WACV2024/html/Fujii_Automated_Sperm_Assessment_Framework_and_Neural_Network_Specialized_for_Sperm_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fujii_Automated_Sperm_Assessment_Framework_and_Neural_Network_Specialized_for_Sperm_WACV_2024_paper.pdf,,https://github.com/FTKR12/RoSTFine,2311.05927,main,Poster,https://ieeexplore.ieee.org/document/10484335/,"['Deep learning', 'Computer vision', 'Head', 'Filters', 'Neural networks', 'Morphology', 'Tail']","['Neural Network', 'Video Recognition', 'Motility', 'Deep Learning', 'Assisted Reproductive Technology', 'Recognition Model', 'Acrosome', 'Sperm Morphology', 'Labeled Data Set', 'Video Dataset', 'Assessment Dataset', 'Spatial Features', 'Global Features', 'Temporal Features', 'Loss Of Diversity', 'Diverse Characteristics', 'Deoxyribonucleic Acid', 'Spatial Attention', 'Intracytoplasmic Sperm Injection', 'Training Objective', 'Fine-grained Features', 'Grade Distribution', 'Attention Map', 'Attention Block', 'Sperm Characteristics', 'Sperm Tail', 'Range Of Performance', 'Attention Scores', 'Temporal Attention', 'Pre-training Data']","['Applications', 'Biomedical / healthcare / medicine', 'Applications', 'Animals / Insects']",1,"Infertility is a global health problem, and an increasing number of couples are seeking medical assistance to achieve reproduction, at least half of which are caused by men. The success rate of assisted reproductive technologies depends on sperm assessment, in which experts determine whether sperm can be used for reproduction based on morphology and motility of sperm. Previous sperm assessment studies with deep learning have used datasets comprising images that include only sperm heads, which cannot consider motility and other morphologies of sperm. Furthermore, the labels of the dataset are one-hot, which provides insufficient support for experts, because assessment results are inconsistent between experts, and they have no absolute answer. Therefore, we constructed the video dataset for sperm assessment whose videos include sperm head as well as neck and tail, and its labels were annotated with soft-label. Furthermore, we proposed the sperm assessment framework and the neural network, RoSTFine, for sperm video recognition. Experimental results showed that RoS-TFine could improve the sperm assessment performances compared to existing video recognition models and focus strongly on important sperm parts (i.e., head and neck). Our code is publickly available at https://github.com/FTKR12/RoSTFine."
AvatarOne: Monocular 3D Human Animation,"Akash Karthikeyan, Robert Ren, Yash Kant, Igor Gilitschenski","University of Toronto, Vector Institute for AI; University of Toronto",100.0,Canada,0.0,,"Reconstructing realistic human avatars from monocular videos is a challenge that demands intricate modeling of 3D surface and articulation. In this paper, we introduce a comprehensive approach that synergizes three pivotal components: (1) a Signed Distance Field (SDF) representation with volume rendering and grid-based ray sampling to prune empty raysets, enabling efficient 3D reconstruction; (2) faster 3D surface reconstruction through a warmup stage for human surfaces, which ensures detailed modeling of body limbs; and (3) temporally consistent subjectspecific forward canonical skinning, which helps in retaining correspondences across frames, all of which can be trained in an end-to-end fashion under 30 mins. Leveraging warmup and grid-based ray marching, along with a faster voxel-based correspondence search, our model streamlines the computational demands of the problem. We further experiment with different sampling representations to improve ray radiance approximations and obtain a floater free surface. Through rigorous evaluation, we demonstrate that our method is on par with current techniques while offering novel insights and avenues for future research in 3D avatar modeling. This work showcases a fast and robust solution for both surface modeling and novel view animation.",https://openaccess.thecvf.com/content/WACV2024/html/Karthikeyan_AvatarOne_Monocular_3D_Human_Animation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Karthikeyan_AvatarOne_Monocular_3D_Human_Animation_WACV_2024_paper.pdf,https://aku02.github.io/projects/avatarone,,,main,Poster,https://ieeexplore.ieee.org/document/10484025/,"['Training', 'Solid modeling', 'Surface reconstruction', 'Three-dimensional displays', 'Computational modeling', 'Avatars', 'Rendering (computer graphics)']","['Temporal Consistency', 'Signed Distance Function', 'Volume Rendering', 'Parametrized', 'Multilayer Perceptron', 'Neural Representations', 'Depth Camera', 'Joint Optimization', 'Human Motion', 'Reconstruction Loss', 'Deformation Field', 'Observation Space', 'Identity Transformation', 'Latent Code', 'Root-finding', 'Occupancy Grid', 'Body Pose']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",2,"Reconstructing realistic human avatars from monocular videos is a challenge that demands intricate modeling of 3D surface and articulation. In this paper, we introduce a comprehensive approach that synergizes three pivotal components: (1) a Signed Distance Field (SDF) representation with volume rendering and grid-based ray sampling to prune empty raysets, enabling efficient 3D reconstruction; (2) faster 3D surface reconstruction through a warmup stage for human surfaces, which ensures detailed modeling of body limbs; and (3) temporally consistent subject-specific forward canonical skinning, which helps in retaining correspondences across frames, all of which can be trained in an end-to-end fashion under 15 minutes.Leveraging warmup and grid-based ray marching, along with a faster voxel-based correspondence search, our model streamlines the computational demands of the problem. We further experiment with different sampling representations to improve ray radiance approximations and obtain a floater free surface. Through rigorous evaluation, we demonstrate that our method is on par with current techniques while offering novel insights and avenues for future research in 3D avatar modeling. This work showcases a fast and robust solution for both surface modeling and novel-view animation. Project website: https://aku02.github.io/projects/avatarone"
BALF: Simple and Efficient Blur Aware Local Feature Detector,Zhenjun Zhao,The Chinese University of Hong Kong,100.0,Hong Kong,0.0,,"Local feature detection is a key ingredient of many image processing and computer vision applications, such as visual odometry and localization. Most existing algorithms focus on feature detection from a sharp image. They would thus have degraded performance once the image is blurred, which could happen easily under low-lighting conditions. To address this issue, we propose a simple yet both efficient and effective keypoint detection method that is able to accurately localize the salient keypoints in a blurred image. Our method takes advantages of a novel multi-layer perceptron (MLP) based architecture that significantly improve the detection repeatability for a blurred image. The network is also light-weight and able to run in real-time, which enables its deployment for time-constrained applications. Extensive experimental results demonstrate that our detector is able to improve the detection repeatability with blurred images, while keeping comparable performance as existing state-of-the-art detectors for sharp images. The code and trained weights are publicly available at github.com/ericzzj1989/BALF.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_BALF_Simple_and_Efficient_Blur_Aware_Local_Feature_Detector_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_BALF_Simple_and_Efficient_Blur_Aware_Local_Feature_Detector_WACV_2024_paper.pdf,,github.com/ericzzj1989/BALF,2211.14731,main,Poster,https://ieeexplore.ieee.org/document/10483749/,"['Location awareness', 'Codes', 'Feature detection', 'Detectors', 'Computer architecture', 'Feature extraction', 'Real-time systems']","['Local Features', 'Feature Detection', 'Local Feature Detectors', 'Detection Methods', 'Computer Vision', 'Multilayer Perceptron', 'Low Light Conditions', 'Computer Vision Applications', 'Image Sharpness', 'Blurred Images', 'Keypoint Detection', 'Visual Odometry', 'Convolutional Neural Network', 'Deep Network', 'Feature Representation', 'Learning-based Methods', 'Residual Block', 'Visual Recognition', 'Detection Module', 'Illumination Changes', 'Image Deblurring', 'Motion Blur', 'Robot Navigation', 'Simultaneous Localization And Mapping', 'Blur Kernel', 'Attention Block', 'Vision Transformer', 'Viewpoint Changes', 'Visual Localization', 'Atrous Convolution']","['Algorithms', '3D computer vision', 'Algorithms', 'Low-level and physics-based vision']",2,"Local feature detection is a key ingredient of many image processing and computer vision applications, such as visual odometry and localization. Most existing algorithms focus on feature detection from a sharp image. They would thus have degraded performance once the image is blurred, which could happen easily under low-lighting conditions. To address this issue, we propose a simple yet both efficient and effective keypoint detection method that is able to accurately localize the salient keypoints in a blurred image. Our method takes advantages of a novel multi-layer perceptron (MLP) based architecture that significantly improve the detection repeatability for a blurred image. The network is also light-weight and able to run in real-time, which enables its deployment for time-constrained applications. Extensive experimental results demonstrate that our detector is able to improve the detection repeatability with blurred images, while keeping comparable performance as existing state-of-the-art detectors for sharp images. The code and trained weights are publicly available at github.com/ericzzj1989/BALF."
BEVMap: Map-Aware BEV Modeling for 3D Perception,"Mincheol Chang, Seokha Moon, Reza Mahjourian, Jinkyu Kim","Waymo Research, Mountain View, CA 94043, USA; Department of Computer Science and Engineering, Korea University, Seoul 02841, Korea",50.0,South Korea,50.0,USA,"In autonomous driving applications, there is a strong preference for modeling the world in Bird's-Eye View (BEV), as it leads to improved accuracy and performance. BEV features are widely used in perception tasks since they allow fusing information from multiple views in an efficient manner. However, BEV features generated from camera images are prone to be imprecise due to the difficulty of estimating depth in the perspective view. Improper placement of BEV features limits the accuracy of downstream tasks. We introduce a method for incorporating map information to improve perspective depth estimation from 2D camera images and thereby producing geometrically- and semantically-robust BEV features. We show that augmenting the camera images with the BEV map and map-to-camera projections can compensate for the depth uncertainty. Experiments on the nuScenes dataset demonstrate that our method outperforms previous approaches using only camera images in segmentation and detection tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Chang_BEVMap_Map-Aware_BEV_Modeling_for_3D_Perception_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chang_BEVMap_Map-Aware_BEV_Modeling_for_3D_Perception_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483874/,"['Image segmentation', 'Solid modeling', 'Three-dimensional displays', 'Uncertainty', 'Roads', 'Estimation', 'Object detection']","['3D Perception', 'Detection Task', 'Camera Images', 'Segmentation Task', 'Visual Perspective', 'Depth Estimation', 'Bird’s Eye', 'Feature Maps', 'Point Cloud', 'Semantic Information', 'Bounding Box', 'Image Sensor', 'Multiple Sensors', 'Geometric Information', 'Segmentation Map', 'Depth Distribution', 'Depth Perception', 'LiDAR Sensor', '3D Detection', 'LiDAR Point Clouds', '3D Object Detection', '3D Bounding Box', 'Map Elements', 'Camera Features', 'Ground-truth Bounding Box', 'Improve Detection Performance', 'Top-down System', 'Object Detection', 'Spatial Information']","['Applications', 'Autonomous Driving', 'Algorithms', 'Image recognition and understanding']",2,"In autonomous driving applications, there is a strong preference for modeling the world in Bird’s-Eye View (BEV), as it leads to improved accuracy and performance. BEV features are widely used in perception tasks since they allow fusing information from multiple views in an efficient manner. However, BEV features generated from camera images are prone to be imprecise due to the difficulty of estimating depth in the perspective view. Improper placement of BEV features limits the accuracy of downstream tasks. We introduce a method for incorporating map information to improve perspective depth estimation from 2D camera images and thereby producing geometrically- and semantically-robust BEV features. We show that augmenting the camera images with the BEV map and map-to-camera projections can compensate for the depth uncertainty and enrich camera-only BEV features with road contexts. Experiments on the nuScenes dataset demonstrate that our method outperforms previous approaches using only camera images in segmentation and detection tasks."
BPKD: Boundary Privileged Knowledge Distillation for Semantic Segmentation,"Liyang Liu, Zihan Wang, Minh Hieu Phan, Bowen Zhang, Jinchao Ge, Yifan Liu","The University of Queensland, Australia; The University of Adelaide, Australia",100.0,Australia,0.0,,"Current knowledge distillation approaches in semantic segmentation tend to adopt a holistic approach that treats all spatial locations equally. However, for dense prediction, students' predictions on edge regions are highly uncertain due to contextual information leakage, requiring higher spatial sensitivity knowledge than the body regions. To address this challenge, this paper proposes a novel approach called boundary-privileged knowledge distillation (BPKD). it distils the knowledge from the teacher model's body and edges separately to the compact student model. Specifically, we employ two distinct loss functions: (i) edge loss, which aims to distinguish between ambiguous classes at the pixel level in edge regions; (ii) body loss, which utilizes shape constraints and selectively attends to the inner-semantic regions. Our experiments demonstrate that the proposed BPKD method provides extensive refinements and aggregation for edge and body regions. Additionally, the method achieves state-of-the-art distillation performance for semantic segmentation on three popular benchmark datasets, highlighting its effectiveness and generalization ability. BPKD shows consistent improvements across a diverse array of lightweight segmentation structures, including both CNNs and transformers, underscoring its architecture-agnostic adaptability.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_BPKD_Boundary_Privileged_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_BPKD_Boundary_Privileged_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/AkideLiu/BPKD,2306.08075,main,Poster,https://ieeexplore.ieee.org/document/10484195/,"['Body regions', 'Adaptation models', 'Computer vision', 'Sensitivity', 'Shape', 'Semantic segmentation', 'Knowledge representation']","['Semantic Segmentation', 'Contextual Information', 'Teacher Model', 'Body Regions', 'Benchmark Datasets', 'Pixel Level', 'Edge Region', 'Student Model', 'Shape Constraints', 'Edge Loss', 'Validation Set', 'Specific Design', 'Edge Detection', 'Segmentation Performance', 'Knowledge Representation', 'Teacher Network', 'Edge Area', 'Fully Convolutional Network', 'Student Network', 'Compact Network', 'Distillation Method', 'Distillation Loss', 'Edge Width', 'Distillation Process', 'Atrous Spatial Pyramid Pooling', 'Edge Extraction', 'Edge Filter', 'Semantic Segmentation Methods', 'Pyramid Pooling Module']","['Algorithms', 'Image recognition and understanding']",3,"Current knowledge distillation approaches in semantic segmentation tend to adopt a holistic approach that treats all spatial locations equally. However, for dense prediction, students’ predictions on edge regions are highly uncertain due to contextual information leakage, requiring higher spatial sensitivity knowledge than the body regions. To address this challenge, this paper proposes a novel approach called boundary-privileged knowledge distillation (BPKD). BPKD distills the knowledge of the teacher model’s body and edges separately to the compact student model. Specifically, we employ two distinct loss functions: (i) edge loss, which aims to distinguish between ambiguous classes at the pixel level in edge regions; (ii) body loss, which utilizes shape constraints and selectively attends to the inner-semantic regions. Our experiments demonstrate that the proposed BPKD method provides extensive refinements and aggregation for edge and body regions. Additionally, the method achieves state-of-the-art distillation performance for semantic segmentation on three popular benchmark datasets, highlighting its effectiveness and generalization ability. BPKD shows consistent improvements across a diverse array of lightweight segmentation structures, including both CNNs and transformers, underscoring its architecture-agnostic adaptability. The code is available at https://github.com/AkideLiu/BPKD."
BSRAW: Improving Blind RAW Image Super-Resolution,"Marcos V. Conde, Florin Vasluianu, Radu Timofte","Computer Vision Lab, CAIDAS & IFI, University of Würzburg",100.0,Germany,0.0,,"In smartphones and compact cameras, the Image Signal Processor (ISP) transforms the RAW sensor image into a human-readable sRGB image. Most popular super-resolution methods depart from a sRGB image and upscale it further, improving its quality. However, modeling the degradations in the sRGB domain is complicated because of the non-linear ISP transformations. Despite this known issue, only a few methods work directly with RAW images and tackle real-world sensor degradations. We tackle blind image super-resolution in the RAW domain. We design a realistic degradation pipeline tailored specifically for training models with raw sensor data. Our approach considers sensor noise, defocus, exposure, and other common issues. Our BSRAW models trained with our pipeline can upscale real-scene RAW images and improve their quality. As part of this effort, we also present a new DSLM dataset and benchmark for this task.",https://openaccess.thecvf.com/content/WACV2024/html/Conde_BSRAW_Improving_Blind_RAW_Image_Super-Resolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Conde_BSRAW_Improving_Blind_RAW_Image_Super-Resolution_WACV_2024_paper.pdf,,https://github.com/mv-lab/AISP,,main,Poster,https://ieeexplore.ieee.org/document/10484432/,"['Degradation', 'Training', 'Superresolution', 'Pipelines', 'Transforms', 'Cameras', 'Optics']","['Raw Images', 'Blind Image', 'Raw Data', 'Part Of Effort', 'Image Sensor', 'Defocus', 'Image Signal', 'Sensor Noise', 'Raw Sensor Data', 'Deep Learning', 'Image Processing', 'Correction Method', 'Real-world Scenarios', 'RGB Images', 'Complete Degradation', 'Level Of Degradation', 'Real Scenes', 'Motion Blur', 'Bicubic', 'Single Image Super-resolution', 'RGB Color Space', 'Noise Injection', 'Sensor Size', 'Blur Kernel', 'Complete Pipeline', 'Method For Tasks', 'Super-resolution Model']","['Applications', 'Smartphones / end user devices', 'Applications', 'Embedded sensing / real-time techniques']",3,"In smartphones and compact cameras, the Image Signal Processor (ISP) transforms the RAW sensor image into a human-readable sRGB image. Most popular super-resolution methods depart from a sRGB image and upscale it further, improving its quality. However, modeling the degradations in the sRGB domain is complicated because of the non-linear ISP transformations. Despite this known issue, only a few methods work directly with RAW images and tackle real-world sensor degradations.We tackle blind image super-resolution in the RAW domain. We design a realistic degradation pipeline tailored specifically for training models with raw sensor data. Our approach considers sensor noise, defocus, exposure, and other common issues. Our BSRAW models trained with our pipeline can upscale real-scene RAW images and improve their quality. As part of this effort, we also present a new DSLM dataset and benchmark for this task."
Back to Optimization: Diffusion-Based Zero-Shot 3D Human Pose Estimation,"Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang",University of Washington; University of Copenhagen,100.0,"Denmark, USA",0.0,,"Learning-based methods have dominated the 3D human pose estimation (HPE) tasks with significantly better performance in most benchmarks than traditional optimization-based methods. Nonetheless, 3D HPE in the wild is still the biggest challenge of learning-based models, whether with 2D-3D lifting, image-to-3D, or diffusion-based methods, since the trained networks implicitly learn camera intrinsic parameters and domain-based 3D human pose distributions and estimate poses by statistical average. On the other hand, the optimization-based methods estimate results case-by-case, which can predict more diverse and sophisticated human poses in the wild. By combining the advantages of optimization-based and learning-based methods, we propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE 51.4mm without training with any 2D-3D or image-3D pairs. Moreover, our single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE 42.6mm on cross-dataset evaluation, which even outperforms learning-based methods trained on 3DPW.",https://openaccess.thecvf.com/content/WACV2024/html/Jiang_Back_to_Optimization_Diffusion-Based_Zero-Shot_3D_Human_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jiang_Back_to_Optimization_Diffusion-Based_Zero-Shot_3D_Human_Pose_Estimation_WACV_2024_paper.pdf,,https://github.com/ipl-uw/ZeDO-Release,2307.03833,main,Poster,https://ieeexplore.ieee.org/document/10484332/,"['Learning systems', 'Training', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Pose estimation', 'Pipelines']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Learning-based Methods', '3D Distribution', 'Optimization-based Methods', '3D Pose', 'Camera Intrinsic Parameters', 'Pipeline Optimization', 'Training Set', 'Training Dataset', 'Denoising', 'Data Augmentation', 'Benchmark Datasets', 'Diffusion Model', 'Iterative Optimization', 'Human Motion', 'Domain Gap', '2D Keypoints', 'Reprojection Error', 'Kinematic Constraints', '2D Pose', 'Optimal Translation', '3D Human Model', 'Multiple Poses']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision']",9,"Learning-based methods have dominated the 3D human pose estimation (HPE) tasks with significantly better performance in most benchmarks than traditional optimization-based methods. Nonetheless, 3D HPE in the wild is still the biggest challenge for learning-based models, whether with 2D-3D lifting, image-to-3D, or diffusion-based methods, since the trained networks implicitly learn camera intrinsic parameters and domain-based 3D human pose distributions and estimate poses by statistical average. On the other hand, the optimization-based methods estimate results case-by-case, which can predict more diverse and sophisticated human poses in the wild. By combining the advantages of optimization-based and learning-based methods, we propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO achieves state-of-the-art (SOTA) performance on Human3.6M, with minMPJPE 51.4mm, without training with any 2D-3D or image-3D pairs. Moreover, our single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE 40.3mm on cross-dataset evaluation, which even outperforms learning-based methods trained on 3DPW. Our code is available here: https://github.com/ipl-uw/ZeDO-Release."
Bag of Tricks for Fully Test-Time Adaptation,"Saypraseuth Mounsaveng, Florent Chiaroni, Malik Boudiaf, Marco Pedersoli, Ismail Ben Ayed","ETS Montr´eal, Canada",100.0,Canada,0.0,,"Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community's knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results.",https://openaccess.thecvf.com/content/WACV2024/html/Mounsaveng_Bag_of_Tricks_for_Fully_Test-Time_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mounsaveng_Bag_of_Tricks_for_Fully_Test-Time_Adaptation_WACV_2024_paper.pdf,,https://github.com/smounsav/tta_bot,2310.02416,main,Poster,https://ieeexplore.ieee.org/document/10483890/,"['Knowledge engineering', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Computer network reliability', 'Data models', 'Reliability']","['Test-time Adaptation', 'Batch Normalization', 'Concept Drift', 'Corruption', 'Performance Of Method', 'Classification Accuracy', 'Sampling Weights', 'Batch Size', 'Domain Shift', 'Normalization Layer', 'Classification Datasets', 'Class Imbalance', 'Inference Time', 'Strong Gradient', 'High Entropy', 'Performance In Cases', 'Batch Normalization Layer', 'Source Domain', 'Low Entropy', 'Continuous Stream', 'Small Batch Size', 'Main Takeaway', 'Class Imbalance Problem', 'Fewer Samples', 'Combination Of Statistics']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community’s knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results."
Benchmark Generation Framework With Customizable Distortions for Image Classifier Robustness,"Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Zachariah Carmichael, Vineet Gundecha, Sahand Ghorbanpour, Ricardo Luna Gutierrez, Antonio Guillen, Avisek Naug","Hewlett Packard Enterprise, USA",0.0,,100.0,USA,"We present a novel framework for generating adversarial benchmarks to evaluate the robustness of image classification models. The RLAB framework allows users to customize the types of distortions to be optimally applied to images, which helps address the specific distortions relevant to their deployment. The benchmark can generate datasets at various distortion levels to assess the robustness of different image classifiers. Our results show that the adversarial samples generated by our framework with any of the image classification models, like ResNet-50, Inception-V3, and VGG-16, are effective and transferable to other models causing them to fail. These failures happen even when these models are adversarially retrained using state-of-the-art techniques, demonstrating the generalizability of our adversarial samples. Our framework also allows the creation of adversarial samples for non-ground truth classes at different levels of intensity, enabling tunable benchmarks for the evaluation of false positives. We achieve competitive performance in terms of net L_2 distortion compared to state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we demonstrate our framework achieves such results with simple distortions like Gaussian noise without introducing unnatural artifacts or color bleeds. This is made possible by a model-based reinforcement learning (RL) agent and a technique that reduces a deep tree search of the image for model sensitivity to perturbations, to a one-level analysis and action. The flexibility of choosing distortions and setting classification probability thresholds for multiple classes makes our framework suitable for algorithmic audits.",https://openaccess.thecvf.com/content/WACV2024/html/Sarkar_Benchmark_Generation_Framework_With_Customizable_Distortions_for_Image_Classifier_Robustness_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sarkar_Benchmark_Generation_Framework_With_Customizable_Distortions_for_Image_Classifier_Robustness_WACV_2024_paper.pdf,,,2310.18626,main,Poster,https://ieeexplore.ieee.org/document/10483918/,"['Analytical models', 'Sensitivity', 'Image color analysis', 'Perturbation methods', 'Gaussian noise', 'Reinforcement learning', 'Benchmark testing']","['Image Classification', 'Benchmark Generation', 'Gaussian Noise', 'Class Probabilities', 'Tree Search', 'Reinforcement Learning Agent', 'Distortion Types', 'Model-based Reinforcement Learning', 'Neural Network', 'Convolutional Neural Network', 'Deep Network', 'Changes In Distribution', 'Robust Method', 'Performance Degradation', 'Data Augmentation', 'Diffusion Model', 'Gaussian Blur', 'Markov Decision Process', 'L1-norm', 'ImageNet Dataset', 'Adversarial Examples', 'Adversarial Perturbations', 'Adversarial Robustness', 'Adversarial Training', 'Benchmark Evaluation', 'Data Augmentation Techniques', 'Board Games', 'Robustness Of Neural Networks']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",,"We present a novel framework for generating adversarial benchmarks to evaluate the robustness of image classification models. Our framework allows users to customize the types of distortions to be optimally applied to images, which helps address the specific distortions relevant to their deployment. The benchmark can generate datasets at various distortion levels to assess the robustness of different image classifiers. Our results show that the adversarial samples generated by our framework with any of the image classification models, such as ResNet-50, Inception-V3 and VGG16, are effective and transferable to other models causing them to fail. These failures happen even when these models are adversarially retrained using state-of-the-art techniques, demonstrating the generalizability of our adversarial samples. We achieve competitive performance in terms of net L
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf>
 distortion compared to state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we demonstrate that our framework achieves such results with simple distortions like Gaussian noise without introducing unnatural artifacts or color bleeds. This is made possible by a model-based reinforcement learning (RL) agent and a technique that reduces a deep tree search of the image for model sensitivity to perturbations, to a one-level analysis and action. The flexibility of choosing distortions and setting classification probability thresholds for multiple classes makes our framework suitable for algorithmic audits."
Benchmarking Out-of-Distribution Detection in Visual Question Answering,"Xiangxi Shi, Stefan Lee",Oregon State University,100.0,USA,0.0,,"When faced with an out-of-distribution (OOD) question or image, visual question answering (VQA) systems may provide unreliable answers. If relied on by real users or secondary systems, these failures may range from annoying to potentially endangering. Detecting OOD samples in single-modality settings is well-studied; however, limited attention has been paid to vision-and-language settings. In this work, we examine the question of OOD detection in the multimodal VQA task and benchmark a suite of approaches to identify OOD image-question pairs. In our experiments, we leverage popular VQA datasets to benchmark detection performance across a range of difficulties. We also produce composite datasets to examine impacts of individual modalities and of image-question agreement. Our results show that answer confidence alone is often a poor signal and that methods based on image-based question generation or examining model attention can lead to significantly better results. We find detecting ungrounded image-question pairs and small shifts in image distribution remain challenging.",https://openaccess.thecvf.com/content/WACV2024/html/Shi_Benchmarking_Out-of-Distribution_Detection_in_Visual_Question_Answering_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shi_Benchmarking_Out-of-Distribution_Detection_in_Visual_Question_Answering_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483677/,"['Visualization', 'Computer vision', 'Computational modeling', 'Estimation', 'Benchmark testing', 'Predictive models', 'Feature extraction']","['Benchmark', 'Visual Question Answering', 'Domain Shift', 'Popular Datasets', 'Multimodal Tasks', 'Training Set', 'High Scores', 'Image Classification', 'Scoring Function', 'Attention Mechanism', 'Language Model', 'Mahalanobis Distance', 'Feature Encoder', 'Distribution Of Attention', 'Binary Questions', 'Image Inpainting', 'BERT Model']","['Algorithms', 'Vision + language and/or other modalities']",1,"When faced with an out-of-distribution (OOD) question or image, visual question answering (VQA) systems may provide unreliable answers. If relied on by real users or secondary systems, these failures may range from annoying to potentially endangering. Detecting OOD samples in single-modality settings is well-studied; however, limited attention has been paid to vision-and-language settings. In this work, we examine the question of OOD detection in the multimodal VQA task and benchmark a suite of approaches to identify OOD image-question pairs. In our experiments, we leverage popular VQA datasets to benchmark detection performance across a range of difficulties. We also produce composite datasets to examine impacts of individual modalities and of image-question agreement. Our results show that answer confidence alone is often a poor signal and that methods based on image-based question generation or examining model attention can lead to significantly better results. We find detecting ungrounded image-question pairs and small shifts in image distribution remain challenging."
Best of Both Worlds: Learning Arbitrary-Scale Blind Super-Resolution via Dual Degradation Representations and Cycle-Consistency,"Shao-Yu Weng, Hsuan Yuan, Yu-Syuan Xu, Ching-Chun Huang, Wei-Chen Chiu",MediaTek Inc.; National Yang Ming Chiao Tung University,50.0,Taiwan,50.0,Taiwan,"Single image super-resolution (SISR) for reconstructing from a low-resolution (LR) input image its corresponding high-resolution (HR) output is a widely-studied research problem in the field of multimedia applications and computer vision. Despite the magic leap brought by recent development of deep neural networks for SISR, such problem is still considered to be quite challenging and non-scalable for the real-world data due to its ill-posed nature, where the degradations happened to the input LR images are usually complex and even unknown (in which the degradations in the test data could be unseen or different from the ones shown in the training dataset). To this end, two branches of SISR methods have emerged: blind super-resolution (blind-SR) and arbitrary-scale super-resolution (ASSR), where the former aims to reconstruct SR images under the unknown degradations, while the latter improves the scalability via learning to handle arbitrary up-sampling ratios. In this paper, we propose a holistic framework to take both blind-SR and ASSR tasks (accordingly named as arbitrary-scale blind-SR) into consideration with two main designs: 1) learning dual degradation representations where the implicit and explicit representations of degradation are sequentially extracted from the input LR image, and 2) modeling both upsampling (i.e. LR to HR) and downsampling (i.e. HR to LR) processes at the same time, where they utilize the implicit and explicit degradation representations respectively, in order to enable the cycle-consistency objective and further improve the training. We conduct extensive experiments on various datasets where the results well verify the effectiveness of our proposed framework in handling complex degradations as well as its superiority with respect to several state-of-the-art baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Weng_Best_of_Both_Worlds_Learning_Arbitrary-Scale_Blind_Super-Resolution_via_Dual_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Weng_Best_of_Both_Worlds_Learning_Arbitrary-Scale_Blind_Super-Resolution_via_Dual_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484031/,"['Degradation', 'Training', 'Computer vision', 'Scalability', 'Superresolution', 'Artificial neural networks', 'Task analysis']","['Dual Representation', 'Blind Super-resolution', 'Degradation Representations', 'Training Dataset', 'Deep Network', 'Input Image', 'Low-resolution Images', 'Problem In The Field', 'Explicit Representation', 'Holistic Framework', 'Implicit Representation', 'Low-resolution Input', 'Single Image Super-resolution', 'High-resolution Images', 'Image Features', 'Scaling Factor', 'Degradation Process', 'Model Design', 'Multilayer Perceptron', 'Explicit Estimates', 'Self-supervised Learning', 'Cycle Consistency Loss', 'Kernel Estimation', 'Super-resolution Results', 'Arbitrary Scale', 'Image Feature Extraction', 'Image X', 'Image Coordinates', 'Continuous Scale']","['Algorithms', 'Low-level and physics-based vision', 'Applications', 'Visualization']",2,"Single image super-resolution (SISR) for reconstructing from a low-resolution (LR) input image its corresponding high-resolution (HR) output is a widely-studied research problem in the field of multimedia applications and computer vision. Despite the magic leap brought by recent development of deep neural networks for SISR, such problem is still considered to be quite challenging and non-scalable for the real-world data due to its ill-posed nature, where the degradations happened to the input LR images are usually complex and even unknown (in which the degradations in the test data could be unseen or different from the ones shown in the training dataset). To this end, two branches of SISR methods have emerged: blind super-resolution (blindSR) and arbitrary-scale super-resolution (ASSR), where the former aims to reconstruct SR images under the unknown degradations, while the latter improves the scalability via learning to handle arbitrary up-sampling ratios. In this paper, we propose a holistic framework to take both blind-SR and ASSR tasks (accordingly named as arbitrary-scale blind-SR) into consideration with two main designs: 1) learning dual degradation representations where the implicit and explicit representations of degradation are sequentially extracted from the input LR image, and 2) modeling both upsampling (i.e. LR→HR) and downsampling (i.e. HR→LR) processes at the same time, where they utilize the implicit and explicit degradation representations respectively, in order to enable the cycle-consistency objective and further improve the training. We conduct extensive experiments on various datasets where the results well verify the effectiveness of our proposed framework in handling complex degradations as well as its superiority with respect to several state-of-the-art baselines."
"Beyond Active Learning: Leveraging the Full Potential of Human Interaction via Auto-Labeling, Human Correction, and Human Verification","Nathan Beck, Krishnateja Killamsetty, Suraj Kothawade, Rishabh Iyer","University of Texas, Dallas",100.0,USA,0.0,,"Active Learning (AL) is a human-in-the-loop framework to interactively and adaptively label data instances, thereby enabling significant gains in model performance compared to random sampling. AL approaches function by selecting the hardest instances to label, often relying on notions of diversity and uncertainty. However, we believe that these current paradigms of AL do not leverage the full potential of human interaction granted by automated label suggestions. Indeed, we show that for many classification tasks and datasets, most people verifying if an automatically suggested label is correct take 3x to 4x less time than they do changing an incorrect suggestion to the correct label (or labeling from scratch without any suggestion). Utilizing this result, we propose CLARIFIER (aCtive LeARnIng From tIEred haRdness), an Interactive Learning framework that admits more effective use of human interaction by leveraging the reduced cost of verification. By targeting the hard (uncertain) instances with existing AL methods, the intermediate instances with a novel label suggestion scheme using submodular mutual information functions on a per-class basis, and the easy (confident) instances with highest-confidence auto-labeling, CLARIFIER can improve over the performance of existing AL approaches on multiple datasets -- particularly on those that have a large number of classes -- by almost 1.5x to 2x in terms of relative labeling cost.",https://openaccess.thecvf.com/content/WACV2024/html/Beck_Beyond_Active_Learning_Leveraging_the_Full_Potential_of_Human_Interaction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Beck_Beyond_Active_Learning_Leveraging_the_Full_Potential_of_Human_Interaction_WACV_2024_paper.pdf,,,2306.01277,main,Poster,https://ieeexplore.ieee.org/document/10483705/,"['Computer vision', 'Adaptation models', 'Costs', 'Uncertainty', 'Human in the loop', 'Data models', 'Labeling']","['Active Learning', 'Model Performance', 'Hardness', 'Correct Label', 'Interactive Framework', 'Active Learning Methods', 'Active Learning Approach', 'Labeling Cost', 'Notion Of Diversity', 'Land Use', 'Deep Neural Network', 'Test Accuracy', 'Improvement In Accuracy', 'Performance Degradation', 'Performance Gain', 'Labeling Experiments', 'Labeling Efficiency', 'Semi-supervised Learning', 'Improve Model Performance', 'Human Effort', 'Unlabeled Instances', 'Unlabeled Set', 'CV Values', 'Labeling Effort', 'Label Prediction', 'Fisher Information Matrix', 'Human Labeling', 'Random Horizontal Flipping', 'Values Of Ca', 'Object Detection']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Active Learning (AL) is a human-in-the-loop framework to interactively and adaptively label data instances, thereby enabling significant gains in model performance compared to random sampling. AL approaches function by selecting the hardest instances to label, often relying on notions of diversity and uncertainty. However, we believe that these current paradigms of AL do not leverage the full potential of human interaction granted by automated label suggestions. Indeed, we show that for many classification tasks and datasets, most people verifying if an automatically suggested label is correct take 3× to 4× less time than they do changing an incorrect suggestion to the correct label (or labeling from scratch without any suggestion). Utilizing this result, we propose Clarifier (aCtive LeARnIng From tIEred haRdness), an Interactive Learning framework that admits more effective use of human interaction by leveraging the reduced cost of verification. By targeting the hard (uncertain) instances with existing AL methods, the intermediate instances with a novel label suggestion scheme using submodular mutual information functions on a per-class basis, and the easy (confident) instances with highest-confidence auto-labeling, Clarifier can improve over the performance of existing AL approaches on multiple datasets – particularly on those that have a large number of classes – by almost 1.5× to 2× in terms of relative labeling cost."
Beyond Classification: Definition and Density-Based Estimation of Calibration in Object Detection,"Teodora Popordanoska, Aleksei Tiulpin, Matthew B. Blaschko","HST Research Unit, University of Oulu; ESAT-PSI, KU Leuven",100.0,"Belgium, Finland",0.0,,"Despite their impressive predictive performance in various computer vision tasks, deep neural networks (DNNs) tend to make overly confident predictions, which hinders their widespread use in safety-critical applications. While there have been recent attempts to calibrate DNNs, most of these efforts have primarily been focused on classification tasks, thus neglecting DNN-based object detectors. Although several recent works addressed calibration for object detection and proposed differentiable penalties, none of them are consistent estimators of established concepts in calibration. In this work, we tackle the challenge of defining and estimating calibration error specifically for this task. In particular, we adapt the definition of classification calibration error to handle the nuances associated with object detection, and predictions in structured output spaces more generally. Furthermore, we propose a consistent and differentiable estimator of the detection calibration error, utilizing kernel density estimation. Our experiments demonstrate the effectiveness of our estimator against competing train-time and post-hoc calibration methods, while maintaining similar detection performance.",https://openaccess.thecvf.com/content/WACV2024/html/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Popordanoska_Beyond_Classification_Definition_and_Density-Based_Estimation_of_Calibration_in_Object_WACV_2024_paper.pdf,,,2312.06645,main,Poster,https://ieeexplore.ieee.org/document/10484015/,"['Computer vision', 'Estimation', 'Object detection', 'Detectors', 'Artificial neural networks', 'Calibration', 'Task analysis']","['Object Detection', 'Task Performance', 'Deep Neural Network', 'Computer Vision', 'Classification Task', 'Density Estimation', 'Consistent Estimates', 'Calibration Method', 'Safety-critical Applications', 'Definition Of Error', 'Post-hoc Method', 'Random Variables', 'Validation Set', 'Similarity Measure', 'Link Function', 'Parametrized', 'Intersection Over Union', 'Bounding Box', 'Confidence Score', 'Continuous Distribution', 'Auxiliary Loss', 'Binary Detection', 'Threshold Function', 'Ground-truth Box', 'Functional Identification', 'Predicted Bounding Box', 'PASCAL VOC', 'Error Detection', 'Synthetic Experiments', 'Conditional Expectation']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving', 'Applications', 'Biomedical / healthcare / medicine']",2,"Despite their impressive predictive performance in various computer vision tasks, deep neural networks (DNNs) tend to make overly confident predictions, which hinders their widespread use in safety-critical applications. While there have been recent attempts to calibrate DNNs, most of these efforts have primarily been focused on classification tasks, thus neglecting DNN-based object detectors. Although several recent works addressed calibration for object detection and proposed differentiable penalties, none of them are consistent estimators of established concepts in calibration. In this work, we tackle the challenge of defining and estimating calibration error specifically for this task. In particular, we adapt the definition of classification calibration error to handle the nuances associated with object detection, and predictions in structured output spaces more generally. Furthermore, we propose a consistent and differentiable estimator of the detection calibration error, utilizing kernel density estimation. Our experiments demonstrate the effectiveness of our estimator against competing train-time and post-hoc calibration methods, while maintaining similar detection performance."
"Beyond Document Page Classification: Design, Datasets, and Challenges","Jordy Van Landeghem, Sanket Biswas, Matthew Blaschko, Marie-Francine Moens","KU Leuven, Contract.fit; Computer Vision Center, Universitat Aut `onoma de Barcelona; KU Leuven",100.0,"Belgium, Spain",0.0,,"This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested (X: multi-channel, multi-paged, multi-industry; Y: class distributions and label set variety) and in classification tasks considered (f: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements.",https://openaccess.thecvf.com/content/WACV2024/html/Van_Landeghem_Beyond_Document_Page_Classification_Design_Datasets_and_Challenges_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Van_Landeghem_Beyond_Document_Page_Classification_Design_Datasets_and_Challenges_WACV_2024_paper.pdf,,,2308.12896,main,Poster,https://ieeexplore.ieee.org/document/10484381/,"['Computer vision', 'Noise', 'Benchmark testing', 'Complexity theory', 'Calibration', 'Task analysis']","['Experimental Studies', 'Classification Task', 'Real-world Applications', 'Evaluation Methodology', 'Text Classification', 'Current Benchmark', 'Low Resolution', 'Object Detection', 'National Registry', 'Multi-label', 'Current Dataset', 'Concrete Examples', 'Evaluation Protocol', 'Practical Scenarios', 'Input Dimension', 'Collection Of Papers', 'Local Cues', 'Optical Character Recognition', 'Dataset Construction', 'Few-shot Learning', 'Label Consistency', 'Single Page', 'Synthetic Generation', 'Inference Scheme', 'Number Of Height', 'Label Noise']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",1,"This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested (X: multi-channel, multi-paged, multi-industry; Y : class distributions and label set variety) and in classification tasks considered (f: multi-page document, page stream, and document bundle classification, …). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements."
Beyond Fusion: Modality Hallucination-Based Multispectral Fusion for Pedestrian Detection,"Qian Xie, Ta-Ying Cheng, Jia-Xing Zhong, Kaichen Zhou, Andrew Markham, Niki Trigoni","Department of Computer Science, University of Oxford, Oxford, UK",100.0,UK,0.0,,"Pedestrian detection is a fundamental task for many downstream applications. Visible and thermal images, as the two most important data types, are usually used to detect pedestrians under various environmental conditions. Many state-of-the-art works have been proposed to use two-stream (i.e., two-branch) architectures to combine visible and thermal information to improve detection performance. However, conventional visible-thermal fusion-based methods have no ability to obtain useful information from the visible branch under poor visibility conditions. The visible branch could even sometimes bring noise into the combined features. In this paper, we present a novel thermal and visible fusion architecture for pedestrian detection. Instead of simply using two branches to separately extract thermal and visible features and then fusing them, we introduce a hallucination branch to learn the mapping from thermal to visible domain, forming a three-branch feature extraction module. We then adaptively fuse feature maps from all the three branches (i.e., thermal, visible, and hallucination). With this new integrated hallucination branch, our network can still get relatively good visible feature maps under challenging low visibility conditions, thus boosting the overall detection performance. Finally, we experimentally demonstrate the superiority of the proposed architecture over conventional fusion methods.",https://openaccess.thecvf.com/content/WACV2024/html/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xie_Beyond_Fusion_Modality_Hallucination-Based_Multispectral_Fusion_for_Pedestrian_Detection_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483978/,"['Computer vision', 'Pedestrians', 'Fuses', 'Noise', 'Computer architecture', 'Feature extraction', 'Boosting']","['Pedestrian Detection', 'Infrared Imaging', 'Feature Maps', 'Combination Of Features', 'Detection Performance', 'Visual Features', 'Good Characteristics', 'Thermal Characteristics', 'Fusion Method', 'Visible Images', 'Architecture For Detection', 'Fused Feature Map', 'Fusion Architecture', 'Object Detection', 'Detection Results', 'Bounding Box', 'Weight Vector', 'Image Pairs', 'Training Stage', 'Feature Fusion', 'Feature Fusion Module', 'Bad Conditions', 'Modal Features', 'Color Information', 'Fusion Weights', 'Fusion Strategy', 'Depth Information', 'Illumination Conditions', 'Detection Head', 'Feature Extraction Backbone']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Pedestrian detection is a fundamental task for many downstream applications. Visible and thermal images, as the two most important data types, are usually used to detect pedestrians under various environmental conditions. Many state-of-the-art works have been proposed to use two-stream (i.e., two-branch) architectures to combine visible and thermal information to improve detection performance. However, conventional visible-thermal fusion-based methods have no ability to obtain useful information from the visible branch under poor visibility conditions. The visible branch could even sometimes bring noise into the combined features. In this paper, we present a novel thermal and visible fusion architecture for pedestrian detection. Instead of simply using two branches to separately extract thermal and visible features and then fusing them, we introduce a hallucination branch to learn the mapping from the thermal to the visible domain, forming a novel three-branch feature extraction module. We then adaptively fuse feature maps from all three branches (i.e., thermal, visible, and hallucination). With this new integrated hallucination branch, our network can still get relatively good visible feature maps under challenging low-visibility conditions, thus boosting the overall detection performance. Finally, we experimentally demonstrate the superiority of the proposed architecture over conventional fusion methods."
Beyond RGB: A Real World Dataset for Multispectral Imaging in Mobile Devices,"Ortal Glatt, Yotam Ater, Woo-Shik Kim, Shira Werman, Oded Berby, Yael Zini, Shay Zelinger, Sangyoon Lee, Heejin Choi, Evgeny Soloveichik","Samsung Israel R&D Center, Israel; Samsung Advanced Institute of Technology, Korea",50.0,South Korea,50.0,South Korea,"Multispectral (MS) imaging systems have a wide range of applications for computer vision and computational photography tasks, but do not yet enjoy widespread adoption due to their prohibitive costs. Recently, advances in the design and fabrication of photonic metamaterials have enabled the development of MS sensors suitable for integration into consumer grade mobile devices. Augmenting existing RGB cameras and their processing algorithms with richer spectral information has the potential to be utilized in many steps of the image processing pipeline, but diverse real world datasets suitable for conducting such research are not freely available. We introduce Beyond RGB, a real-world dataset comprising thousands of multispectral and RGB images in diverse real world and lab conditions that is suitable for the development and evaluation of algorithms utilizing multispectral and RGB data. All the scenes in our dataset include a colorimetric reference and a measurement of the spectrum of the scene illuminant. Additionally, we demonstrate the practical use of our dataset through the introduction of a novel illuminant spectral estimation (ISE) algorithm. Our algorithm surpasses the current state-of-the-art (SoTA) by up to 58.6% on established benchmarks and sets a baseline performance on our own dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Glatt_Beyond_RGB_A_Real_World_Dataset_for_Multispectral_Imaging_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Glatt_Beyond_RGB_A_Real_World_Dataset_for_Multispectral_Imaging_in_WACV_2024_paper.pdf,,https://github.com/shirawerman/Beyond-RGB,,main,Poster,https://ieeexplore.ieee.org/document/10483818/,"['Photography', 'Performance evaluation', 'Computer vision', 'Pipelines', 'Benchmark testing', 'Sensor fusion', 'Cameras']","['Mobile Devices', 'Real-world Datasets', 'Multispectral Images', 'RGB Images', 'Advanced Design', 'Multispectral Data', 'Prohibitive Cost', 'RGB Camera', 'Multispectral Sensors', 'RGB Data', 'Rich Spectral Information', 'Multispectral Imaging System', 'Deep Learning', 'Convolution', 'Wavelength Of Light', 'Raw Images', 'Image Sensor', 'Pixel Resolution', 'Spectral Response', 'Data Fusion', 'Multispectral Camera', 'Color Chart', 'Score Map', 'HDF5 File', 'Channel Index', 'Angular Error', 'Spectral Reflectance', 'Spectral Reflectance Data']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Computational photography', 'image and video synthesis']",2,"Multispectral (MS) imaging systems have a wide range of applications for computer vision and computational photography tasks, but do not yet enjoy widespread adoption due to their prohibitive costs. Recently, advances in the design and fabrication of photonic metamaterials have enabled the development of MS sensors suitable for integration into consumer grade mobile devices. Augmenting existing RGB cameras and their processing algorithms with richer spectral information has the potential to be utilized in many steps of the image processing pipeline, but diverse real world datasets suitable for conducting such research are not freely available. We introduce Beyond RGB
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
, a real-world dataset comprising thousands of multispectral and RGB images in diverse real world and lab conditions that is suitable for the development and evaluation of algorithms utilizing multispectral and RGB data. All the scenes in our dataset include a colorimetric reference and a measurement of the spectrum of the scene illuminant. Additionally, we demonstrate the practical use of our dataset through the introduction of a novel illuminant spectral estimation (ISE) algorithm. Our algorithm surpasses the current state-of-the-art (SoTA) by up to 58.6% on established benchmarks and sets a baseline performance on our own dataset."
Beyond SOT: Tracking Multiple Generic Objects at Once,"Christoph Mayer, Martin Danelljan, Ming-Hsuan Yang, Vittorio Ferrari, Luc Van Gool, Alina Kuznetsova",ETH Zürich; Google Research,50.0,Switzerland,50.0,USA,"Generic Object Tracking (GOT) is the problem of tracking target objects, specified by bounding boxes in the first frame of a video. While the task has received much attention in the last decades, researchers have almost exclusively focused on the single object setting. However multi-object GOT poses its own challenges and is more attractive in real-world applications. We attribute the lack of research interest into this problem to the absence of suitable benchmarks. In this work, we introduce a new large-scale GOT benchmark, LaGOT, containing multiple annotated target objects per sequence. Our benchmark allows users to tackle key remaining challenges in GOT, aiming to increase robustness and reduce computation through joint tracking of multiple objects simultaneously. In addition, we propose a transformer-based GOT tracker baseline capable of joint processing of multiple objects through shared computation.  Our approach achieves a 4x faster run-time in case of 10 concurrent objects compared to tracking each object independently and outperforms existing single object trackers on our new benchmark. In addition, our approach achieves highly competitive results on single-object GOT datasets, setting a new state of the art on TrackingNet with a success rate AUC of 84.4%. Our benchmark, code, and trained models will be made publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Mayer_Beyond_SOT_Tracking_Multiple_Generic_Objects_at_Once_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mayer_Beyond_SOT_Tracking_Multiple_Generic_Objects_at_Once_WACV_2024_paper.pdf,,https://github.com/visionml/pytracking,2212.11920,main,Poster,https://ieeexplore.ieee.org/document/10483760/,"['Computer vision', 'Target tracking', 'Codes', 'Benchmark testing', 'Transformers', 'Robustness', 'Encoding']","['Multiple Objects', 'General Objective', 'Single Object Tracking', 'Track Multiple Objects', 'Bounding Box', 'Video Frames', 'Single Object', 'Target Object', 'Object Tracking', 'Wooden Frame', 'Multiple Object Tracking', 'Transformer', 'Single Class', 'Number Of Objects', 'Video Sequences', 'Target Model', 'Training Characteristics', 'Search Area', 'Score Map', 'Entire Frame', 'Video Object', 'Sequence Of Objects', 'Multi Objective', 'Bounding Box Regression', 'Training Frames', 'Test Frame', 'Multiple Benchmarks', 'Correlation Filter', 'Object Annotations', 'List Of Classes']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Datasets and evaluations']",3,"Generic Object Tracking (GOT) is the problem of tracking target objects, specified by bounding boxes in the first frame of a video. While the task has received much attention in the last decades, researchers have almost exclusively focused on the single object setting. However multiobject GOT poses its own challenges and is more attractive in real-world applications. We attribute the lack of research interest into this problem to the absence of suitable benchmarks. In this work, we introduce a new largescale GOT benchmark, LaGOT, containing multiple annotated target objects per sequence. Our benchmark allows users to tackle key remaining challenges in GOT, aiming to increase robustness and reduce computation through joint tracking of multiple objects simultaneously. In addition, we propose a transformer-based GOT tracker baseline capable of joint processing of multiple objects through shared computation. Our approach achieves a 4× faster run-time in case of 10 concurrent objects compared to tracking each object independently and outperforms existing single object trackers on our new benchmark. In addition, our approach achieves highly competitive results on single-object GOT datasets, setting a new state of the art on TrackingNet with a success rate AUC of 84.4%. Our benchmark, code, results and trained models are available at https://github.com/visionml/pytracking."
Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation,"Reza Azad, Leon Niggemeier, Michael Hüttemann, Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Yury Velichko, Ulas Bagci, Dorit Merhof",Northwestern University; University of Regensburg; Iran University of Science and Technology; Shahid Beheshti University; RWTH Aachen University,100.0,"Germany, Iran, USA",0.0,,"Medical image segmentation has seen significant improvements with transformer models, which excel in grasping far-reaching contexts and global contextual information. However, the increasing computational demands of these models, proportional to the squared token count, limit their depth and resolution capabilities. Most current methods process D volumetric image data slice-by-slice (called pseudo 3D), missing crucial inter-slice information and thus reducing the model's overall performance. To address these challenges, we introduce the concept of Deformable Large Kernel Attention (D-LKA Attention), a streamlined attention mechanism employing large convolution kernels to fully appreciate volumetric context. This mechanism operates within a receptive field akin to self-attention while sidestepping the computational overhead. Additionally, our proposed attention mechanism benefits from deformable convolutions to flexibly warp the sampling grid, enabling the model to adapt appropriately to diverse data patterns. We designed both 2D and 3D adaptations of the D-LKA Attention, with the latter excelling in cross-depth data understanding. Together, these components shape our novel hierarchical Vision Transformer architecture, the D-LKA Net. Evaluations of our model against leading methods on popular medical segmentation datasets (Synapse, NIH Pancreas, and Skin lesion) demonstrate its superior performance.",https://openaccess.thecvf.com/content/WACV2024/html/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Azad_Beyond_Self-Attention_Deformable_Large_Kernel_Attention_for_Medical_Image_Segmentation_WACV_2024_paper.pdf,,https://github.com/,,main,Poster,https://ieeexplore.ieee.org/document/10484516/,"['Image segmentation', 'Adaptation models', 'Three-dimensional displays', 'Convolution', 'Computational modeling', 'Transformers', 'Data models']","['Medical Imaging', 'Medical Image Segmentation', 'Large Kernel', 'Synapse', 'Attention Mechanism', 'Receptive Field', 'Global Information', 'Convolution Kernel', 'Transformer Model', 'Segmentation Dataset', 'Global Context Information', 'Vision Transformer', 'Deformable Convolution', 'Convolutional Neural Network', 'Convolutional Layers', 'Stochastic Gradient Descent', 'Gallbladder', 'Increase In Performance', 'Convolutional Neural Network Model', 'Semantic Segmentation', 'Deformable Layer', 'Dice Similarity Coefficient', 'Left Kidney', 'Task Success', '2D Version', 'Floating-point Operations', 'Skip Connections', 'Depthwise Convolution', 'Segmentation Results', 'Abdominal Computed Tomography']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Biomedical / healthcare / medicine']",19,"Medical image segmentation has seen significant improvements with transformer models, which excel in grasping far-reaching contexts and global contextual information. However, the increasing computational demands of these models, proportional to the squared token count, limit their depth and resolution capabilities. Most current methods process D volumetric image data slice-by-slice (called pseudo 3D), missing crucial inter-slice information and thus reducing the model’s overall performance. To address these challenges, we introduce the concept of Deformable Large Kernel Attention (D-LKA Attention), a streamlined attention mechanism employing large convolution kernels to fully appreciate volumetric context. This mechanism operates within a receptive field akin to self-attention while sidestepping the computational overhead. Additionally, our proposed attention mechanism benefits from deformable convolutions to flexibly warp the sampling grid, enabling the model to adapt appropriately to diverse data patterns. We designed both 2D and 3D adaptations of the D-LKA Attention, with the latter excelling in cross-depth data understanding. Together, these components shape our novel hierarchical Vision Transformer architecture, the D-LKA Net. Evaluations of our model against leading methods on popular medical segmentation datasets (Synapse, NIH Pancreas, and Skin lesion) demonstrate its superior performance. Our code is publicly available at GitHub."
Bi-Directional Training for Composed Image Retrieval via Text Prompt Learning,"Zheyuan Liu, Weixuan Sun, Yicong Hong, Damien Teney, Stephen Gould","Australian National University; Idiap Research Institute, Australian Institute for Machine Learning, University of Adelaide",100.0,Australia,0.0,,"Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as described by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures with minimum changes, which improves the performance of the model. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datasets show that our novel approach achieves improved performance over a baseline BLIP-based model that itself already achieves competitive performance. Our code is released at https://github.com/Cuberick-Orion/Bi-Blip4CIR.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Bi-Directional_Training_for_Composed_Image_Retrieval_via_Text_Prompt_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Bi-Directional_Training_for_Composed_Image_Retrieval_via_Text_Prompt_Learning_WACV_2024_paper.pdf,,https://github.com/Cuberick-Orion/Bi-Blip4CIR,2303.16604,main,Poster,https://ieeexplore.ieee.org/document/10484044/,"['Training', 'Computer vision', 'Costs', 'Computational modeling', 'Image retrieval', 'Semantics', 'Bidirectional control']","['Image Retrieval', 'Bi-directional Training', 'Opposite Direction', 'Fine-tuned', 'Target Image', 'Reference Image', 'Training Data', 'False Negative', 'Negative Samples', 'Image Generation', 'Secondary Loss', 'Loss Term', 'Input Modalities', 'Tokenized', 'Contrastive Loss', 'Input Text', 'Joint Training', 'Inference Scheme', 'Text Complexity', 'Text Encoder', 'Special Token', 'Image Encoder', 'Training Pipeline', 'Qualitative Examples', 'Joint Embedding', 'Validation Set', 'Balanced Approach']","['Algorithms', 'Vision + language and/or other modalities']",3,"Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as described by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures with minimum changes, which improves the performance of the model. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datasets show that our novel approach achieves improved performance over a baseline BLIP-based model that itself already achieves competitive performance. Our code is released at https://github.com/Cuberick-Orion/Bi-Blip4CIR"
Bias and Diversity in Synthetic-Based Face Recognition,"Marco Huber, Anh Thi Luu, Fadi Boutros, Arjan Kuijper, Naser Damer","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Darmstadt, Germany",100.0,Germany,0.0,,"Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias.",https://openaccess.thecvf.com/content/WACV2024/html/Huber_Bias_and_Diversity_in_Synthetic-Based_Face_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huber_Bias_and_Diversity_in_Synthetic-Based_Face_Recognition_WACV_2024_paper.pdf,,,2311.03970,main,Poster,https://ieeexplore.ieee.org/document/10483704/,"['Training', 'Ethics', 'Head', 'Law', 'Face recognition', 'Computational modeling', 'Training data']","['Face Recognition', 'Algorithmic Bias', 'Training Data', 'Data Distribution', 'Gender Distribution', 'Face Images', 'Message Authentication', 'Similar Bias', 'Ethnic Distribution', 'Demographic Attributes', 'Distribution Of Training Data', 'Face Recognition Model', 'Training Dataset', 'Support Vector Machine', 'Diversity Analysis', 'Mean Accuracy', 'Distribution Properties', 'Male Individuals', 'Model Bias', 'Head Pose', 'Synthetic Images', 'Synthetic Training Data', 'Face Dataset', 'Synthetic Generation', 'Yaw Angle', 'Accuracy Verification', 'Ethnic Bias', 'Performance Verification', 'Self-supervised Learning']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",5,"Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias."
BigSmall: Efficient Multi-Task Learning for Disparate Spatial and Temporal Physiological Measurements,"Girish Narayanswamy, Yujia Liu, Yuzhe Yang, Chengqian Ma, Xin Liu, Daniel McDuff, Shwetak Patel",Massachusetts Institute of Technology; University of Washington,100.0,USA,0.0,,"Understanding of human visual perception has historically inspired the design of computer vision architectures. As an example, perception occurs at different scales both spatially and temporally, suggesting that the extraction of salient visual information may be made more effective by attending to specific features at varying scales. Visual changes in the body due to physiological processes also occur at varying scales and with modality-specific characteristic properties. Inspired by this, we present BigSmall, an efficient architecture for physiological and behavioral measurement. We present the first joint camera-based facial action, cardiac, and pulmonary measurement model. We propose a multi-branch network with wrapping temporal shift modules that yields both accuracy and efficiency gains. We observe that fusing low-level features leads to suboptimal performance, but that fusing high level features enables efficiency gains with negligible losses in accuracy. Experimental results demonstrate that BigSmall significantly reduces the computational costs. Furthermore, compared to existing task-specific models, BigSmall achieves comparable or better results on multiple physiological measurement tasks simultaneously with a unified model.",https://openaccess.thecvf.com/content/WACV2024/html/Narayanswamy_BigSmall_Efficient_Multi-Task_Learning_for_Disparate_Spatial_and_Temporal_Physiological_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Narayanswamy_BigSmall_Efficient_Multi-Task_Learning_for_Disparate_Spatial_and_Temporal_Physiological_WACV_2024_paper.pdf,,,2303.11573,main,Poster,https://ieeexplore.ieee.org/document/10483769/,"['Visualization', 'Computer vision', 'Pulse measurements', 'Computational modeling', 'Computer architecture', 'Licenses', 'Multitasking']","['Unified Model', 'Low-level Features', 'Human Vision', 'Temporal Modulation', 'Facial Action', 'Low Resolution', 'Spatial Scales', 'Spatial Features', 'Temporal Dynamics', 'Temporal Scales', 'Temporal Information', 'Spatial Task', 'Consecutive Frames', 'Temporal Model', 'Pulse Signal', 'Small Branches', 'Respiratory Sinus Arrhythmia', 'Floating-point Operations', 'Input Frames', 'Action Units', 'Temporal Representation', 'High-resolution Input', 'Facial Action Coding System', 'Multi-task Model', 'Raw Frames', 'Respiration Measurements', 'Subtle Changes', 'High-resolution', 'Convolutional Layers', 'Long Short-term Memory']","['Applications', 'Biomedical / healthcare / medicine', 'Applications', 'Remote Sensing']",5,"Understanding of human visual perception has historically inspired the design of computer vision architectures. As an example, perception occurs at different scales both spatially and temporally, suggesting that the extraction of salient visual information may be made more effective by attending to specific features at varying scales. Visual changes in the body, due to physiological processes, also occur at varying scales and with modality-specific characteristic properties. Inspired by this, we present BigSmall, an efficient architecture for physiological and behavioral measurement. We present the first joint camera-based facial action, cardiac, and pulmonary measurement model. We propose a multi-branch network with wrapping temporal shift modules that yields efficiency gains and accuracy on par with task-optimized methods. We observe that fusing low-level features leads to suboptimal performance, but that fusing high level features enables efficiency gains with negligible losses in accuracy. We experimentally validate that BigSmall significantly reduces computational cost while achieving comparable results on multiple physiological measurement tasks simultaneously with a unified model."
Bipartite Graph Diffusion Model for Human Interaction Generation,"Baptiste Chopin, Hao Tang, Mohamed Daoudi","Carnegie Mellon University, Pittsburgh, USA; Inria, Universit ´e Cˆote d’Azur, France; Univ. Lille, CNRS, Centrale Lille, Institut Mines-T ´el´ecom, UMR 9189 CRIStAL, Lille, F-59000, France; IMT Nord Europe, Institut Mines-T ´el´ecom, Univ. Lille, Centre for Digital Systems, Lille, F-59000, France",100.0,"France, USA",0.0,,"The generation of natural human motion interactions is a hot topic in computer vision and computer animation. It is a challenging task due to the diversity of possible human motion interactions. Diffusion models, which have already shown remarkable generative capabilities in other domains, are a good candidate for this task. In this paper, we introduce a novel bipartite graph diffusion method (BiGraphDiff) to generate human motion interactions between two persons. Specifically, bipartite node sets are constructed to model the inherent geometric constraints between skeleton nodes during interactions. The interaction graph diffusion model is transformer-based, combining some state-of-the-art motion methods. We show that the proposed achieves new state-of-the-art results on leading benchmarks for the human interaction generation task.",https://openaccess.thecvf.com/content/WACV2024/html/Chopin_Bipartite_Graph_Diffusion_Model_for_Human_Interaction_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chopin_Bipartite_Graph_Diffusion_Model_for_Human_Interaction_Generation_WACV_2024_paper.pdf,,https://github.com/CRISTAL-3DSAM/BiGraphDiff,2301.10134,main,Poster,https://ieeexplore.ieee.org/document/10484215/,"['Computer vision', 'Codes', 'Computational modeling', 'Diversity reception', 'Benchmark testing', 'Transformers', 'Animation']","['Human Interaction', 'Diffusion Model', 'Computer Vision', 'Human Motion', 'Training Set', 'Transformer', 'Denoising', 'Qualitative Results', 'Diffusion Process', 'Reversible Process', 'Noisy Data', 'Variational Autoencoder', 'Residual Connection', 'Linear Projection', 'Number Of Joints', 'Current Time Step', 'Forward Process', 'Bipartite Network', 'Motion Sequences', 'Arbitrary Conditions', 'Motion Generation', 'Motion Classification', 'Fréchet Inception Distance', 'Text Encoder', 'Method Suffers', '3D Motion', 'Motion-based', 'Computer Graphics', 'Classification Accuracy', 'Deep Features']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",4,"The generation of natural human motion interactions is a hot topic in computer vision and computer animation. It is a challenging task due to the diversity of possible human motion interactions. Diffusion models, which have already shown remarkable generative capabilities in other domains, are a good candidate for this task. In this paper, we introduce a novel bipartite graph diffusion method (BiGraphDiff) to generate human motion interactions between two persons. Specifically, bipartite node sets are constructed to model the inherent geometric constraints between skeleton nodes during interactions. The interaction graph diffusion model is transformer-based, combining some state-of-theart motion methods. We show that the proposed achieves new state-of-the-art results on leading benchmarks for the human interaction generation task. Code, pre-trained models and additional results are available at https://github.com/CRISTAL-3DSAM/BiGraphDiff."
BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping,"Srikumar Sastry, Subash Khanal, Aayush Dhakal, Di Huang, Nathan Jacobs",Washington University in St. Louis,100.0,USA,0.0,,"We propose a metadata-aware self-supervised learning (SSL) framework useful for fine-grained classification and ecological mapping of bird species around the world. Our framework unifies two SSL strategies: Contrastive Learning (CL) and Masked Image Modeling (MIM), while also enriching the embedding space with meta-information available with ground-level imagery of birds. We separately train uni-modal and cross-modal ViT on a novel cross-view global birds species dataset containing ground-level imagery, metadata (location, time), and corresponding satellite imagery. We demonstrate that our models learn fine-grained and geographically conditioned features of birds, by evaluating on two downstream tasks: fine-grained visual classification (FGVC) and cross-modal retrieval. Pre-trained models learned using our framework achieve SotA performance on FGVC of iNAT-2021 birds as well as in transfer learning setting for CUB-200-2011 and NABirds datasets. Moreover, the impressive cross-modal retrieval performance of our model enables the creation of species distribution maps across any geographic region. The dataset and source code will be released at https://github.com/TBD.",https://openaccess.thecvf.com/content/WACV2024/html/Sastry_BirdSAT_Cross-View_Contrastive_Masked_Autoencoders_for_Bird_Species_Classification_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sastry_BirdSAT_Cross-View_Contrastive_Masked_Autoencoders_for_Bird_Species_Classification_and_WACV_2024_paper.pdf,,https://github.com/mvrl/BirdSAT,2310.19168,main,Poster,https://ieeexplore.ieee.org/document/10483847/,"['Visualization', 'Computer vision', 'Biological system modeling', 'Source coding', 'Transfer learning', 'Self-supervised learning', 'Metadata']","['Bird Species', 'Geographic Regions', 'Species Distribution', 'Learning Framework', 'Transfer Learning', 'Latent Space', 'Satellite Imagery', 'Self-supervised Learning', 'Species Distribution Maps', 'Computer Vision', 'Satellite Images', 'Digital Elevation Model', 'Visual Task', 'Ground Level', 'Representation Learning', 'Visual Recognition', 'Image Retrieval', 'Linear Probe', 'Forward Pass', 'L2 Loss', 'Visual Recognition Tasks', 'Retrieval Approach', 'Object Reconstruction', 'Contrast Objective', 'AdamW Optimizer', 'Feed-forward Layer', 'Masked Language Model']","['Applications', 'Animals / Insects', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Remote Sensing']",2,"We propose a metadata-aware self-supervised learning (SSL) framework useful for fine-grained classification and ecological mapping of bird species around the world. Our framework unifies two SSL strategies: Contrastive Learning (CL) and Masked Image Modeling (MIM), while also enriching the embedding space with metadata available with ground-level imagery of birds. We separately train uni-modal and cross-modal ViT on a novel cross-view global bird species dataset containing ground-level imagery, metadata (location, time), and corresponding satellite imagery. We demonstrate that our models learn fine-grained and geographically conditioned features of birds, by evaluating on two downstream tasks: fine-grained visual classification (FGVC) and cross-modal retrieval. Pre-trained models learned using our framework achieve SotA performance on FGVC of iNAT-2021 birds and in transfer learning settings for CUB-200-2011 and NABirds datasets. Moreover, the impressive cross-modal retrieval performance of our model enables the creation of species distribution maps across any geographic region. The dataset and source code will be released at https://github.com/mvrl/BirdSAT."
Blurry Video Compression: A Trade-Off Between Visual Enhancement and Data Compression,"Dawit Mureja Argaw, Junsik Kim, In So Kweon",KAIST; Harvard University,100.0,"South Korea, USA",0.0,,"Existing video compression (VC) methods primarily aim to reduce the spatial and temporal redundancies between consecutive frames in a video while preserving its quality. In this regard, previous works have achieved remarkable results on videos acquired under specific settings such as instant (known) exposure time and shutter speed which often result in sharp videos. However, when these methods are evaluated on videos captured under different temporal priors, which lead to degradations like motion blur and low frame rate, they fail to maintain the quality of the contents. In this work, we tackle the VC problem in a general scenario where a given video can be blurry due to predefined camera settings or dynamics in the scene. By exploiting the natural trade-off between visual enhancement and data compression, we formulate VC as a min-max optimization problem and propose an effective framework and training strategy to tackle the problem. Extensive experimental results on several benchmark datasets confirm the effectiveness of our method compared to several state-of-the-art VC approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Argaw_Blurry_Video_Compression_A_Trade-Off_Between_Visual_Enhancement_and_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Argaw_Blurry_Video_Compression_A_Trade-Off_Between_Visual_Enhancement_and_Data_WACV_2024_paper.pdf,,,2311.04430,main,Poster,https://ieeexplore.ieee.org/document/10484074,"['Training', 'Degradation', 'Visualization', 'Computer vision', 'Redundancy', 'Dynamics', 'Data compression']","['Source Code', 'Video Compression', 'Visual Enhancement', 'Blurry Videos', 'Training Strategy', 'Consecutive Frames', 'Video Capture', 'Compression Method', 'Motion Blur', 'Shutter Speed', 'Lower Frame', 'Mean Square Error', 'Decoding', 'Convolutional Layers', 'Network Flow', 'Residual Information', 'Motion Estimation', 'Error Map', 'Image Compression', 'Cascade Model', 'Network Compression', 'Optimal Compression', 'Temporal Smoothing', 'Joint Training', 'Motion Compensation', 'Video Coding', 'Optical Networks', 'Motion Vector', 'Input Frames']","['Algorithms', 'Computational photography', 'image and video synthesis']",,"Existing video compression (VC) methods primarily aim to reduce the spatial and temporal redundancies between consecutive frames in a video while preserving its quality. In this regard, previous works have achieved remarkable results on videos acquired under specific settings such as instant (known) exposure time and shutter speed which often result in sharp videos. However, when these methods are evaluated on videos captured under different temporal priors, which lead to degradations like motion blur and low frame rate, they fail to maintain the quality of the contents. In this work, we tackle the VC problem in a general scenario where a given video can be blurry due to predefined camera settings or dynamics in the scene. By exploiting the natural trade-off between visual enhancement and data compression, we formulate VC as a min-max optimization problem and propose an effective framework and training strategy to tackle the problem. Extensive experimental results on several benchmark datasets confirm the effectiveness of our method compared to several state-of-the-art VC approaches."
BoostRad: Enhancing Object Detection by Boosting Radar Reflections,"Yuval Haitman, Oded Bialer","General Motors, Technical Center Israel; General Motors, Technical Center Israel; School of Electrical and Computer Engineering in Ben Gurion University of the Negev",33.33333333333333,Israel,66.66666666666667,Israel,"Automotive radars have an important role in autonomous driving systems. The main challenge in automotive radar detection is the radar's wide point spread function (PSF) in the angular domain that causes blurriness and clutter in the radar image. Numerous studies suggest employing an 'end-to-end' learning strategy using a Deep Neural Network (DNN) to directly detect objects from radar images. This approach implicitly addresses the PSF's impact on objects of interest. In this paper, we propose an alternative approach, which we term ""Boosting Radar Reflections"" (BoostRad). In BoostRad, a first DNN is trained to narrow the PSF for all the reflection points in the scene. The output of the first DNN is a boosted reflection image with higher resolution and reduced clutter, resulting in a sharper and cleaner image. Subsequently, a second DNN is employed to detect objects within the boosted reflection image. We develop a novel method for training the boosting DNN that incorporates domain knowledge of radar's PSF characteristics. BoostRad's performance is evaluated using the RADDet and CARRADA datasets, revealing its superiority over reference methods.",https://openaccess.thecvf.com/content/WACV2024/html/Haitman_BoostRad_Enhancing_Object_Detection_by_Boosting_Radar_Reflections_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Haitman_BoostRad_Enhancing_Object_Detection_by_Boosting_Radar_Reflections_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484424/,"['Training', 'Radar detection', 'Radar', 'Artificial neural networks', 'Object detection', 'Radar imaging', 'Boosting']","['Object Detection', 'Radar Reflectivity', 'Deep Neural Network', 'Reference Method', 'Object Of Interest', 'Point Spread Function', 'Radar Images', 'Reflectance Images', 'Reflection Point', 'Radar Detection', 'Scene Point', 'Angular Domain', 'Widespread Functions', 'Loss Function', 'Cross-entropy', 'Cross-entropy Loss', 'Point Cloud', 'Bounding Box', 'Object Classification', 'Ground Truth Reference', 'Constant False Alarm Rate', 'Angular Resolution', 'Reflection Intensity', 'Range Of Dimensions', 'Radar Data', 'Low Sidelobe', 'Object Detection Network', 'Reference Image', 'Doppler Frequency']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving']",1,"Automotive radars have an important role in autonomous driving systems. The main challenge in automotive radar detection is the radar’s wide point spread function (PSF) in the angular domain that causes blurriness and clutter in the radar image. Numerous studies suggest employing an ’end-to-end’ learning strategy using a Deep Neural Network (DNN) to directly detect objects from radar images. This approach implicitly addresses the PSF’s impact on objects of interest. In this paper, we propose an alternative approach, which we term ’’Boosting Radar Reflections"" (BoostRad). In BoostRad, a first DNN is trained to narrow the PSF for all the reflection points in the scene. The output of the first DNN is a boosted reflection image with higher resolution and reduced clutter, resulting in a sharper and cleaner image. Subsequently, a second DNN is employed to detect objects within the boosted reflection image. We develop a novel method for training the boosting DNN that incorporates domain knowledge of radar’s PSF characteristics. BoostRad’s performance is evaluated using the RADDet and CARRADA datasets, revealing its superiority over reference methods."
Booster-SHOT: Boosting Stacked Homography Transformations for Multiview Pedestrian Detection With Attention,"Jinwoo Hwang, Philipp Benz, Pete Kim",Deeping Source Inc.; Seoul National University,50.0,South Korea,50.0,South Korea,"Improving multi-view aggregation is integral for multi-view pedestrian detection, which aims to obtain a bird's-eye-view pedestrian occupancy map from images captured through a set of calibrated cameras. Inspired by the success of attention modules for deep neural networks, we first propose a Homography Attention Module (HAM) which is shown to boost the performance of existing end-to-end multiview detection approaches by utilizing a novel channel gate and spatial gate. Additionally, we propose Booster-SHOT, an end-to-end convolutional approach to multiview pedestrian detection incorporating our proposed HAM as well as elements from previous approaches such as view-coherent augmentation or stacked homography transformations. Booster-SHOT achieves 92.9% and 94.2% for MODA on Wildtrack and MultiviewX respectively, outperforming the state-of-the-art by 1.4% on Wildtrack and 0.5% on MultiviewX, achieving state-of-the-art performance overall for standard evaluation metrics used in multi-view pedestrian detection.",https://openaccess.thecvf.com/content/WACV2024/html/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hwang_Booster-SHOT_Boosting_Stacked_Homography_Transformations_for_Multiview_Pedestrian_Detection_With_WACV_2024_paper.pdf,,https://github.com/luorix1/Booster-SHOT,,main,Poster,https://ieeexplore.ieee.org/document/10483937/,"['Measurement', 'Computer vision', 'Pedestrians', 'Computational modeling', 'Logic gates', 'Benchmark testing', 'Cameras']","['Homography Transformation', 'Neural Network', 'Deep Neural Network', 'Attention Module', 'Channel Gating', 'Standard Evaluation Metrics', 'Convolutional Neural Network', 'Image Features', 'Computer Vision', 'Feature Maps', 'Spatial Features', 'Object Detection', 'Recurrent Neural Network', 'Attention Mechanism', 'Selective Modulators', 'Ground Plane', 'Spatial Attention', 'Backbone Network', 'Camera View', 'Attention Map', 'Spatial Attention Mechanism', 'Perspective Transformation', 'Gate Modulation', 'Auxiliary Loss', 'Channel Attention', 'Backbone Architecture', 'Design Choices', 'Input Image', 'Spatial Dimensions', 'Focal Loss']","['Algorithms', 'Image recognition and understanding']",,"Improving multi-view aggregation is integral for multi-view pedestrian detection, which aims to obtain a bird’s-eye-view pedestrian occupancy map from images captured through a set of calibrated cameras. Inspired by the success of attention modules for deep neural networks, we first propose a Homography Attention Module (HAM) which is shown to boost the performance of existing end-to-end multiview detection approaches by utilizing a novel channel gate and spatial gate. Additionally, we propose Booster-SHOT, an end-to-end convolutional approach to multiview pedestrian detection incorporating our proposed HAM as well as elements from previous approaches such as view-coherent augmentation or stacked homography transformations. Booster-SHOT achieves 92.9% and 94.2% for MODA on Wildtrack and MultiviewX respectively, outperforming the state-of-the-art by 1.4% on Wildtrack and 0.5% on MultiviewX, achieving state-of-the-art performance overall for standard evaluation metrics used in multi-view pedestrian detection. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Boosting Weakly Supervised Object Detection Using Fusion and Priors From Hallucinated Depth,"Cagri Gungor, Adriana Kovashka","Intelligent Systems Program, University of Pittsburgh; Department of Computer Science, University of Pittsburgh",100.0,USA,0.0,,"Despite recent attention and exploration of depth for various tasks, it is still an unexplored modality for weakly-supervised object detection (WSOD). We propose an amplifier method for enhancing the performance of WSOD by integrating depth information. Our approach can be applied to any WSOD method based on multiple instance learning, without necessitating additional annotations or inducing large computational expenses. Our proposed method employs a monocular depth estimation technique to obtain hallucinated depth information, which is then incorporated into a Siamese WSOD network using contrastive loss and fusion. By analyzing the relationship between language context and depth, we calculate depth priors to identify the bounding box proposals that may contain an object of interest. These depth priors are then utilized to update the list of pseudo ground-truth boxes, or adjust the confidence of perbox predictions. Our proposed method is evaluated on six datasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and Comic2k) by implementing it on top of two state-of-the-art WSOD methods, and we demonstrate a substantial enhancement in performance.",https://openaccess.thecvf.com/content/WACV2024/html/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gungor_Boosting_Weakly_Supervised_Object_Detection_Using_Fusion_and_Priors_From_WACV_2024_paper.pdf,https://cagrigungor.github.io/WSOD-AMPLIFIER/,https://github.com/cagrigungor/WSOD-AMPLIFIER,2303.10937,main,Poster,https://ieeexplore.ieee.org/document/10483695/,"['Computer vision', 'Annotations', 'Estimation', 'Object detection', 'Boosting', 'Computational efficiency', 'Proposals']","['Object Detection', 'Hallucinations', 'Weakly Supervised Object Detection', 'Ergogenic', 'Bounding Box', 'Object Of Interest', 'Depth Information', 'Depth Estimation', 'Language Context', 'Additional Annotations', 'Multiple Instance Learning', 'Monocular Depth Estimation', 'RGB Images', 'Classification Score', 'Depth Images', 'Depth Range', 'Ground Truth Labels', 'Depth Values', 'Self-supervised Learning', 'Relative Gain', 'Label Noise', 'Late Fusion', 'Number Of Proposals', 'Object C', 'Siamese Network', 'Early Fusion', 'Predicted Bounding Box', 'Prediction Box', 'RGB Features', 'Image-level Labels']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Despite recent attention to depth for various tasks, it is still an unexplored modality for weakly-supervised object detection (WSOD). We propose an amplifier method for enhancing the performance of WSOD by integrating depth information. Our approach can be applied to different WSOD methods based on multiple-instance learning, without necessitating additional annotations or inducing large computational cost. Our proposed method employs monocular depth estimation to obtain hallucinated depth information, which is then incorporated into a Siamese WSOD network using contrastive loss and fusion. By analyzing the relationship between language context and depth, we calculate depth priors to identify the bounding box proposals that may contain an object of interest. These depth priors are then utilized to update the list of pseudo ground-truth boxes, or adjust the confidence of per-box predictions. We evaluate our proposed method on three datasets (COCO, PASCAL VOC, and Conceptual Captions) by implementing it on top of two state-of-the-art WSOD methods, and we demonstrate a substantial enhancement in performance."
Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-Weighted Brain MR Images,"Md Mahfuzur Rahman Siddiquee, Jay Shah, Teresa Wu, Catherine Chong, Todd J. Schwedt, Gina Dumkrieger, Simona Nikolova, Baoxin Li",Arizona State University; ASU-Mayo Center for Innovative Imaging; ASU-Mayo Center for Innovative Imaging; Mayo Clinic; Mayo Clinic,60.0,USA,40.0,USA,"Harnessing the power of deep neural networks in the medical imaging domain is challenging due to the difficulties in acquiring large annotated datasets, especially for rare diseases, which involve high costs, time, and effort for annotation. Unsupervised disease detection methods, such as anomaly detection, can significantly reduce human effort in these scenarios. While anomaly detection typically focuses on learning from images of healthy subjects only, real-world situations often present unannotated datasets with a mixture of healthy and diseased subjects. Recent studies have demonstrated that utilizing such unannotated images can improve unsupervised disease and anomaly detection. However, these methods do not utilize knowledge specific to registered neuroimages, resulting in a subpar performance in neurologic disease detection. To address this limitation, we propose Brainomaly, a GAN-based image-to-image translation method specifically designed for neurologic disease detection. Brainomaly not only offers tailored image-to-image translation suitable for neuroimages but also leverages unannotated mixed images to achieve superior neurologic disease detection. Additionally, we address the issue of model selection for inference without annotated samples by proposing a pseudo-AUC metric, further enhancing Brainomaly's detection performance. Extensive experiments and ablation studies demonstrate that Brainomaly outperforms existing state-of-the-art unsupervised disease and anomaly detection methods by significant margins in Alzheimer's disease detection using a publicly available dataset and headache detection using an institutional dataset. The code is available from https://github.com/mahfuzmohammad/Brainomaly.",https://openaccess.thecvf.com/content/WACV2024/html/Siddiquee_Brainomaly_Unsupervised_Neurologic_Disease_Detection_Utilizing_Unannotated_T1-Weighted_Brain_MR_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Siddiquee_Brainomaly_Unsupervised_Neurologic_Disease_Detection_Utilizing_Unannotated_T1-Weighted_Brain_MR_WACV_2024_paper.pdf,,https://github.com/mahfuzmohammad/Brainomaly,2302.09200,main,Poster,https://ieeexplore.ieee.org/document/10484019/,"['Measurement', 'Computer vision', 'Costs', 'Codes', 'Magnetic resonance imaging', 'Computational modeling', 'Brain modeling']","['Brain Magnetic Resonance Imaging', 'Disease Detection', 'T1-weighted Brain', 'Detection Methods', 'Healthy Subjects', 'Model Selection', 'Deep Neural Network', 'Detection Performance', 'Anomaly Detection', 'Annotated Dataset', 'Healthy Controls', 'Receiver Operating Characteristic Curve', 'Input Image', 'Migraine', 'Latent Space', 'Large Margin', 'Healthy Brain', 'Difference Map', 'General Learning', 'Fréchet Inception Distance', 'Discriminator Network', 'Anomaly Score', '3-month Time Point', 'Latent Vector', 'Image Annotation', 'Cycle Consistency', 'Loss Of Identity', 'Selection Of Metrics']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Image recognition and understanding']",3,"Harnessing the power of deep neural networks in the medical imaging domain is challenging due to the difficulties in acquiring large annotated datasets, especially for rare diseases, which involve high costs, time, and effort for annotation. Unsupervised disease detection methods, such as anomaly detection, can significantly reduce human effort in these scenarios. While anomaly detection typically focuses on learning from images of healthy subjects only, real-world situations often present unannotated datasets with a mixture of healthy and diseased subjects. Recent studies have demonstrated that utilizing such unannotated images can improve unsupervised disease and anomaly detection. However, these methods do not utilize knowledge specific to registered neuroimages, resulting in a subpar performance in neurologic disease detection. To address this limitation, we propose Brainomaly, a GAN-based image-to-image translation method specifically designed for neurologic disease detection. Brainomaly not only offers tailored image-to-image translation suitable for neuroimages but also leverages unannotated mixed images to achieve superior neurologic disease detection. Additionally, we address the issue of model selection for inference without annotated samples by proposing a pseudo-AUC metric, further enhancing Brainomaly’s detection performance. Extensive experiments and ablation studies demonstrate that Brainomaly outperforms existing state-of-the-art unsupervised disease and anomaly detection methods by significant margins in Alzheimer’s disease detection using a publicly available dataset and headache detection using an institutional dataset. The code is available from https://github.com/mahfuzmohammad/Brainomaly."
Bridging Generalization Gaps in High Content Imaging Through Online Self-Supervised Domain Adaptation,"Johan Fredin Haslum, Christos Matsoukas, Karl-Johan Leuchowius, Kevin Smith","AstraZeneca, Gothenburg, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Science for Life Laboratory, Stockholm, Sweden; AstraZeneca, Gothenburg, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Science for Life Laboratory, Stockholm, Sweden",33.33333333333333,Sweden,66.66666666666667,Sweden,"High Content Imaging (HCI) plays a vital role in modern drug discovery and development pipelines, facilitating various stages from hit identification to candidate drug characterization. Applying machine learning models to these datasets can prove challenging as they typically consist of multiple batches, affected by experimental variation, especially if different imaging equipment have been used. Moreover, as new data arrive, it is preferable that they are analyzed in an online fashion. To overcome this, we propose CODA, an online self-supervised domain adaptation approach. CODA divides the classifier's role into a generic feature extractor and a task-specific model. We adapt the feature extractor's weights to the new domain using cross-batch self-supervision while keeping the task-specific model unchanged. Our results demonstrate that this strategy significantly reduces the generalization gap, achieving up to a 300% improvement when applied to data from different labs utilizing different microscopes. CODA can be applied to new, unlabeled out-of-domain data sources of different sizes, from a single plate to multiple experimental batches.",https://openaccess.thecvf.com/content/WACV2024/html/Haslum_Bridging_Generalization_Gaps_in_High_Content_Imaging_Through_Online_Self-Supervised_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Haslum_Bridging_Generalization_Gaps_in_High_Content_Imaging_Through_Online_Self-Supervised_WACV_2024_paper.pdf,,,2311.12623,main,Poster,https://ieeexplore.ieee.org/document/10483768/,"['Drugs', 'Adaptation models', 'Soft sensors', 'Microscopy', 'Pipelines', 'Machine learning', 'Feature extraction']","['Domain Adaptation', 'High-content Imaging', 'Generalization Gap', 'Self-supervised Domain Adaptation', 'Data Sources', 'Drug Discovery', 'Batch Experiments', 'Single Plate', 'Multiple Batches', 'Role In Drug Discovery', 'Modern Drug Discovery', 'Medical Imaging', 'Learning Rate', 'Biological Significance', 'Feature Space', 'Subset Of Data', 'Biological Variation', 'Batch Effects', 'Domain Shift', 'High-level Features', 'Stand-alone Model', 'Dual Model', 'Generalization Capability', 'Vision Transformer', 'Domain Adaptation Methods', 'Variety Of Experimental Conditions', 'General Representation', 'Low-level Features', 'Biological Noise', 'Generalization Capability Of The Model']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"High Content Imaging (HCI) plays a vital role in modern drug discovery and development pipelines, facilitating various stages from hit identification to candidate drug characterization. Applying machine learning models to these datasets can prove challenging as they typically consist of multiple batches, affected by experimental variation, especially if different imaging equipment have been used. Moreover, as new data arrive, it is preferable that they are analyzed in an online fashion. To overcome this, we propose CODA, an online self-supervised domain adaptation approach. CODA divides the classifier’s role into a generic feature extractor and a task-specific model. We adapt the feature extractor’s weights to the new domain using crossbatch self-supervision while keeping the task-specific model unchanged. Our results demonstrate that this strategy significantly reduces the generalization gap, achieving up to a 300% improvement when applied to data from different labs utilizing different microscopes. CODA can be applied to new, unlabeled out-of-domain data sources of different sizes, from a single plate to multiple experimental batches."
Bridging the Gap Between Multi-Focus and Multi-Modal: A Focused Integration Framework for Multi-Modal Image Fusion,"Xilai Li, Xiaosong Li, Tao Ye, Xiaoqi Cheng, Wuyang Liu, Haishu Tan","China University of Mining and Technology, Beijing 100083, China; Foshan University, Foshan 528225, China",100.0,China,0.0,,"Multi-modal image fusion (MMIF) integrates valuable information from different modality images into a fused one. However, the fusion of multiple visible images with different focal regions and infrared images is a unprecedented challenge in real MMIF applications. This is because of the limited depth of the focus of visible optical lenses, which impedes the simultaneous capture of the focal information within the same scene. To address this issue, in this paper, we propose a MMIF framework for joint focused integration and modalities information extraction. Specifically, a semi-sparsity-based smoothing filter is introduced to decompose the images into structure and texture components. Subsequently, a novel multi-scale operator is proposed to fuse the texture components, capable of detecting significant information by considering the pixel focus attributes and relevant data from various modal images. Additionally, to achieve an effective capture of scene luminance and reasonable contrast maintenance, we consider the distribution of energy information in the structural components in terms of multi-directional frequency variance and information entropy. Extensive experiments on existing MMIF datasets, as well as the object detection and depth estimation tasks, consistently demonstrate that the proposed algorithm can surpass the state-of-the-art methods in visual perception and quantitative evaluation.The code is available at https://github.com/ixilai/MFIF-MMIF.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Bridging_the_Gap_Between_Multi-Focus_and_Multi-Modal_A_Focused_Integration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Bridging_the_Gap_Between_Multi-Focus_and_Multi-Modal_A_Focused_Integration_WACV_2024_paper.pdf,,https://github.com/ixilai/MFIF-MMIF,,main,Poster,https://ieeexplore.ieee.org/document/10484519/,"['Optical filters', 'Smoothing methods', 'Fuses', 'Estimation', 'Object detection', 'Optical imaging', 'Maintenance']","['Structural Components', 'Imaging Modalities', 'Object Detection', 'Distribution Information', 'Depth Estimation', 'Energy Information', 'Visible Images', 'Optical Lens', 'Object Depth', 'Texture Components', 'Input Image', 'Layered Structure', 'Image Information', 'Fusion Method', 'Vision Tasks', 'Source Images', 'Target Information', 'Salient Information', 'Fusion Results', 'Saliency Map', 'Image Entropy', 'Pixel Information', 'Fusion Rule', 'Scene Information', 'Complementary Region', 'Discrete Cosine Transform', 'Fusion Algorithm', 'Laplacian Pyramid', 'Global Gradient', 'Fusion Performance']","['Algorithms', 'Low-level and physics-based vision', 'Applications', 'Autonomous Driving']",5,"Multi-modal image fusion (MMIF) integrates valuable information from different modality images into a fused one. However, the fusion of multiple visible images with different focal regions and infrared images is a unprecedented challenge in real MMIF applications. This is because of the limited depth of the focus of visible optical lenses, which impedes the simultaneous capture of the focal information within the same scene. To address this issue, in this paper, we propose a MMIF framework for joint focused integration and modalities information extraction. Specifically, a semi-sparsity-based smoothing filter is introduced to decompose the images into structure and texture components. Subsequently, a novel multi-scale operator is proposed to fuse the texture components, capable of detecting significant information by considering the pixel focus attributes and relevant data from various modal images. Additionally, to achieve an effective capture of scene luminance and reasonable contrast maintenance, we consider the distribution of energy information in the structural components in terms of multi-directional frequency variance and information entropy. Extensive experiments on existing MMIF datasets, as well as the object detection and depth estimation tasks, consistently demonstrate that the proposed algorithm can surpass the state-of-the-art methods in visual perception and quantitative evaluation. The code is available at https://github.com/ixilai/MFIF-MMIF."
C-CLIP: Contrastive Image-Text Encoders To Close the Descriptive-Commentative Gap,"William Theisen, Walter J. Scheirer",University of Notre Dame,100.0,USA,0.0,,"The interplay between the image and comment on a social media post is one of high importance for understanding its overall message. Recent strides in multimodal embedding models, namely CLIP, have provided an avenue forward in relating image and text. However the current training regime for CLIP models is insufficient for matching content found on social media, regardless of site or language. Current CLIP training data is based on what we call ""descriptive"" text: text in which an image is merely described. This is something rarely seen on social media, where the vast majority of text content is ""commentative"" in nature. The captions provide commentary and broader context related to the image, rather than describing what is in it. Current CLIP models perform poorly on retrieval tasks where image-caption pairs display a commentative relationship. Closing this gap would be beneficial for several important application areas related to social media. For instance, it would allow groups focused on Open-Source Intelligence Operations (OSINT) to further aid efforts during disaster events, such as the ongoing Russian invasion of Ukraine, by easily exposing data to non-technical users for discovery and analysis. In order to close this gap we demonstrate that training contrastive image-text encoders on explicitly commentative pairs results in large improvements in retrieval results, with the results extending across a variety of non-English languages.",https://openaccess.thecvf.com/content/WACV2024/html/Theisen_C-CLIP_Contrastive_Image-Text_Encoders_To_Close_the_Descriptive-Commentative_Gap_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Theisen_C-CLIP_Contrastive_Image-Text_Encoders_To_Close_the_Descriptive-Commentative_Gap_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483804/,"['Training', 'Computer vision', 'Social networking (online)', 'Disasters', 'Computational modeling', 'Sociology', 'Training data']","['Social Media', 'Training Data', 'Language Diversity', 'Training Regimen', 'Current Training', 'Multimodal Model', 'Accuracy Of Model', 'Pairing', 'Descriptive Data', 'Bilingual', 'Percentage Points', 'Set Of Results', 'Image Object', 'Deviation Increase', 'Latent Space', 'Social Media Sites', 'Social Media Data', 'Twitter Data', 'Compound Annual Growth Rate', 'Telegram', 'Brazilian Data', 'MS COCO Dataset', 'Retrieval Accuracy', 'Text Encoder', 'Image Captioning', 'Projection Layer', 'Image Retrieval', 'Description Task', 'Subcategories']","['Applications', 'Arts / games / social media', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"The interplay between the image and comment on a social media post is one of high importance for understanding its overall message. Recent strides in multimodal embedding models, namely CLIP, have provided an avenue forward in relating image and text. However the current training regime for CLIP models is insufficient for matching content found on social media, regardless of site or language. Current CLIP training data is based on what we call ""descriptive"" text: text in which an image is merely described. This is something rarely seen on social media, where the vast majority of text content is ""commentative"" in nature. The captions provide commentary and broader context related to the image, rather than describing what is in it. Current CLIP models perform poorly on retrieval tasks where image-caption pairs display a commentative relationship. Closing this gap would be beneficial for several important application areas related to social media. For instance, it would allow groups focused on Open-Source Intelligence Operations (OSINT) to further aid efforts during disaster events, such as the ongoing Russian invasion of Ukraine, by easily exposing data to non-technical users for discovery and analysis. In order to close this gap we demonstrate that training contrastive image-text encoders on explicitly commentative pairs results in large improvements in retrieval results, with the results extending across a variety of nonEnglish languages."
C2AIR: Consolidated Compact Aerial Image Haze Removal,"Ashutosh Kulkarni, Shruti S. Phutke, Santosh Kumar Vipparthi, Subrahmanyam Murala","Institute for Integrated and Intelligent Systems, Griffith University, Australia; CVPR Lab, School of Computer Science and Statistics, Trinity College Dublin, Ireland; CVPR Lab, Indian Institute of Technology Ropar, India",100.0,"Australia, India, Ireland",0.0,,"Aerial image haze removal deals with improving the visibility and quality of images captured from aerial platforms, such as drones and satellites. Aerial images are commonly used in various applications such as environmental monitoring, and disaster response. These applications usually require cleaner data for accurate functioning. However, atmospheric conditions such as haze or fog can significantly degrade the quality of these images, reducing their contrast, color saturation, and sharpness, making it difficult to extract meaningful information from them. Existing methods rely on computationally heavy and haze density (light, moderate, dense) specific architectures for aerial image dehazing. In light of these limitations, we propose a novel lightweight and consolidated approach for aerial image dehazing. In this approach, we propose Density Aware Query Modulated Block for learning weather degradations in input features and guiding the restoration process. Further, we propose Cross Collaborative Feed-Forward Block for learning to restore varying sizes of the structures in the input images. Finally, we propose Gated Adaptive Feature Fusion block to achieve inter-scale and intra-feature attentive fusion, effective for aerial image restoration. Extensive analysis on benchmark aerial image dehazing datasets and real-world images, along with detailed ablation studies validate the effectiveness of the proposed approach. Further, we have analysed our method for other restoration task such as underwater image enhancement to experiment its wide applicability. The code is available at https: //github.com/AshutoshKulkarni4998/C2AIR.",https://openaccess.thecvf.com/content/WACV2024/html/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kulkarni_C2AIR_Consolidated_Compact_Aerial_Image_Haze_Removal_WACV_2024_paper.pdf,,https://github.com/AshutoshKulkarni4998/C2AIR,,main,Poster,https://ieeexplore.ieee.org/document/10484204/,"['Satellites', 'Image color analysis', 'Collaboration', 'Logic gates', 'Feature extraction', 'Image restoration', 'Task analysis']","['Aerial Images', 'Haze Removal', 'Input Image', 'Feature Fusion', 'Real-world Images', 'Aerial Platforms', 'Restoration Tasks', 'Underwater Image', 'Convolutional Neural Network', 'Performance Of Method', 'Computational Complexity', 'Quantitative Results', 'Convolutional Layers', 'Qualitative Results', 'Data Augmentation', 'Receptive Field', 'Image Pairs', 'Generative Adversarial Networks', 'Trainable Parameters', 'Synthetic Aperture Radar', 'Vision Transformer', 'Transmission Map', 'Peak Signal-to-noise Ratio', 'Structural Similarity Index Measure', 'Word Embedding', 'Perceptual Loss', 'Random Flipping', 'Practical Scenarios']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",2,"Aerial image haze removal deals with improving the visibility and quality of images captured from aerial platforms, such as drones and satellites. Aerial images are commonly used in various applications such as environmental monitoring, and disaster response. These applications usually require cleaner data for accurate functioning. However, atmospheric conditions such as haze or fog can significantly degrade the quality of these images, reducing their contrast, color saturation, and sharpness, making it difficult to extract meaningful information from them. Existing methods rely on computationally heavy and haze density (light, moderate, dense) specific architectures for aerial image dehazing. In light of these limitations, we propose a novel lightweight and consolidated approach for aerial image dehazing. In this approach, we propose Density Aware Query Modulated Block for learning weather degradations in input features and guiding the restoration process. Further, we propose Cross Collaborative Feed-Forward Block for learning to restore varying sizes of the structures in the input images. Finally, we propose Gated Adaptive Feature Fusion block to achieve inter-scale and intra-feature attentive fusion, effective for aerial image restoration. Extensive analysis on benchmark aerial image dehazing datasets and real-world images, along with detailed ablation studies validate the effectiveness of the proposed approach. Further, we have analysed our method for other restoration task such as underwater image enhancement to experiment its wide applicability. The code is available at https://github.com/AshutoshKulkarni4998/C2AIR."
CAD - Contextual Multi-Modal Alignment for Dynamic AVQA,"Asmar Nadeem, Adrian Hilton, Robert Dawes, Graham Thomas, Armin Mustafa","Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, United Kingdom.; BBC Research and Development, United Kingdom.",50.0,UK,50.0,UK,"In the context of Audio Visual Question Answering (AVQA) tasks, the audio and visual modalities could be learnt on three levels: 1) Spatial, 2) Temporal, and 3) Semantic. Existing AVQA methods suffer from two major shortcomings; the audio-visual (AV) information passing through the network isn't aligned on Spatial and Temporal levels; and, inter-modal (audio and visual) Semantic information is often not balanced within a context; this results in poor performance. In this paper, we propose a novel end-to-end Contextual Multi-modal Alignment (CAD) network that addresses the challenges in AVQA methods by i) introducing a parameter-free stochastic Contextual block that ensures robust audio and visual alignment on the Spatial level; ii) proposing a pre-training technique for dynamic audio and visual alignment on Temporal level in a self-supervised setting, and iii) introducing a cross-attention mechanism to balance audio and visual information on Semantic level. The proposed novel CAD network improves the overall performance over the state-of-the-art methods on average by 9.4% on the MUSIC-AVQA dataset. We also demonstrate that our proposed contributions to AVQA can be added to the existing methods to improve their performance without additional complexity requirements.",https://openaccess.thecvf.com/content/WACV2024/html/Nadeem_CAD_-_Contextual_Multi-Modal_Alignment_for_Dynamic_AVQA_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nadeem_CAD_-_Contextual_Multi-Modal_Alignment_for_Dynamic_AVQA_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484229/,"['Visualization', 'Computer vision', 'Semantics', 'Decision making', 'Robustness', 'Question answering (information retrieval)', 'Complexity theory']","['Visual Information', 'Semantic Information', 'Question Answering', 'Visual Modality', 'Spatial Level', 'Semantic Level', 'Visual Question', 'Audio Information', 'Visual Question Answering', 'Audiovisual Information', 'Feature Space', 'Visual Features', 'Large-scale Datasets', 'Latent Space', 'Types Of Questions', 'Visual Input', 'Action Recognition', 'Temporal Aspects', 'Self-supervised Learning', 'Output Of Module', 'Audio Stream', 'Visual Stream', 'Audio Input', 'Output Of Block', 'Audio Data', 'Dynamic Scenarios', 'Pre-training Tasks', 'Self-supervised Manner', 'Temporal Alignment', 'Pre-training Phase']","['Applications', 'Arts / games / social media', 'Algorithms', 'Video recognition and understanding', 'Applications', 'Smartphones / end user devices']",4,"In the context of Audio Visual Question Answering (AVQA) tasks, the audio and visual modalities could be learnt on three levels: 1) Spatial, 2) Temporal, and 3) Semantic. Existing AVQA methods suffer from two major shortcomings; the audio-visual (AV) information passing through the network isn’t aligned on Spatial and Temporal levels; and, inter-modal (audio and visual) Semantic information is often not balanced within a context; this results in poor performance. In this paper, we propose a novel end-to-end Contextual Multi-modal Alignment (CAD) network that addresses the challenges in AVQA methods by i) introducing a parameter-free stochastic Contextual block that ensures robust audio and visual alignment on the Spatial level; ii) proposing a pre-training technique for dynamic audio and visual alignment on Temporal level in a self-supervised setting, and iii) introducing a cross-attention mechanism to balance audio and visual information on Semantic level. The proposed novel CAD network improves the overall performance over the state-of-the-art methods on average by 9.4% on the MUSIC-AVQA dataset. We also demonstrate that our proposed contributions to AVQA can be added to the existing methods to improve their performance without additional complexity requirements."
CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning,"Zhaoheng Zheng, Haidong Zhu, Ram Nevatia","Viterbi School of Engineering, University of Southern California",100.0,USA,0.0,,"In this paper, we study the problem of Compositional Zero-Shot Learning (CZSL), which is to recognize novel attribute-object combinations with pre-existing concepts. Recent researchers focus on applying large-scale Vision-Language Pre-trained (VLP) models like CLIP with strong generalization ability. However, these methods treat the pre-trained model as a black box and focus on pre- and post-CLIP operations, which do not inherently mine the semantic concept between the layers inside CLIP. We propose to dive deep into the architecture and insert adapters, a parameter-efficient technique proven to be effective among large language models, into each CLIP encoder layer. We further equip adapters with concept awareness so that concept-specific features of ""object"", ""attribute"", and ""composition"" can be extracted. We assess our method on four popular CZSL datasets, MIT-States, C-GQA, UT-Zappos, and VAW-CZSL, which shows state-of-the-art performance compared to existing methods on all of them.",https://openaccess.thecvf.com/content/WACV2024/html/Zheng_CAILA_Concept-Aware_Intra-Layer_Adapters_for_Compositional_Zero-Shot_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zheng_CAILA_Concept-Aware_Intra-Layer_Adapters_for_Compositional_Zero-Shot_Learning_WACV_2024_paper.pdf,,github.com/zhaohengz/CAILA,2305.16681,main,Poster,https://ieeexplore.ieee.org/document/10483626/,"['Adaptation models', 'Computer vision', 'Zero-shot learning', 'Semantics', 'Computer architecture', 'Benchmark testing', 'Feature extraction']","['Zero-shot', 'Black Box', 'Popular Datasets', 'Strong Generalization Ability', 'Knowledge Transfer', 'Search Space', 'Latent Space', 'Harmonic Mean', 'Hidden State', 'Word Embedding', 'Graph Convolutional Network', 'Latent Features', 'Image X', 'Image Encoder', 'Open World', 'Text Encoder']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",1,"In this paper, we study the problem of Compositional Zero-Shot Learning (CZSL), which is to recognize novel attribute-object combinations with pre-existing concepts. Recent researchers focus on applying large-scale Vision-Language Pre-trained (VLP) models like CLIP with strong generalization ability. However, these methods treat the pre-trained model as a black box and focus on pre- and post-CLIP operations, which do not inherently mine the semantic concept between the layers inside CLIP. We propose to dive deep into the architecture and insert adapters, a parameter-efficient technique proven to be effective among large language models, into each CLIP encoder layer. We further equip adapters with concept awareness so that concept-specific features of ""object"", ""attribute"", and ""composition"" can be extracted. We assess our method on four popular CZSL datasets, MIT-States, C-GQA, UT-Zappos, and VAW-CZSL, which shows state-of-the-art performance compared to existing methods on all of them."
CAMOT: Camera Angle-Aware Multi-Object Tracking,"Felix Limanta, Kuniaki Uto, Koichi Shinoda","Tokyo Institute of Technology, Meguro, Tokyo, Japan",100.0,Japan,0.0,,"This paper proposes CAMOT, a simple camera angle estimator for multi-object tracking to tackle two problems: 1) occlusion and 2) inaccurate distance estimation in the depth direction. Under the assumption that multiple objects are located on a flat plane in each video frame, CAMOT estimates the camera angle using object detection. In addition, it gives the depth of each object, enabling pseudo-3D MOT. We evaluated its performance by adding it to various 2D MOT methods on the MOT17 and MOT20 datasets and confirmed its effectiveness. Applying CAMOT to ByteTrack, we obtained 63.8% HOTA, 80.6% MOTA, and 78.5% IDF1 in MOT17, which are state-of-the-art results. Its computational cost is significantly lower than the existing deep-learning-based depth estimators for tracking.",https://openaccess.thecvf.com/content/WACV2024/html/Limanta_CAMOT_Camera_Angle-Aware_Multi-Object_Tracking_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Limanta_CAMOT_Camera_Angle-Aware_Multi-Object_Tracking_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483868/,"['Geometry', 'Computer vision', 'Tracking', 'Estimation', 'Object detection', 'Performance gain', 'Cameras']","['Multi-object Tracking', 'Object Detection', 'Video Frames', 'Depth Estimation', 'Camera Angle', 'Angle Estimation', 'Depth Direction', 'Flat Plane', 'Aspect Ratio', 'Single Image', '3D Space', 'Bounding Box', 'Kalman Filter', 'High Angle', 'Tracking Performance', '3D Coordinates', '3D Point', 'Elevation Angle', 'Re-identification', 'Single GPU', 'Monocular Depth Estimation', 'Object In Frame', '3D Tracking', 'Object Depth', 'Pinhole Camera', 'Bottom Point', 'Perspective Distortion', '3D Object Detection', 'Occluded Objects', 'Top Point']","['Algorithms', 'Video recognition and understanding']",2,"This paper proposes CAMOT, a simple camera angle estimator for multi-object tracking to tackle two problems: 1) occlusion and 2) inaccurate distance estimation in the depth direction. Under the assumption that multiple objects are located on a flat plane in each video frame, CAMOT estimates the camera angle using object detection. In addition, it gives the depth of each object, enabling pseudo-3D MOT. We evaluated its performance by adding it to various 2D MOT methods on the MOT17 and MOT20 datasets and confirmed its effectiveness. Applying CAMOT to ByteTrack, we obtained 63.8% HOTA, 80.6% MOTA, and 78.5% IDF1 in MOT17, which are state-of-the-art results. Its computational cost is significantly lower than the existing deep-learning-based depth estimators for tracking."
CARE: Counterfactual-Based Algorithmic Recourse for Explainable Pose Correction,"Bhat Dittakavi, Bharathi Callepalli, Aleti Vardhan, Sai Vikas Desai, Vineeth N. Balasubramanian",Indian Institute of Technology Hyderabad; Variance.ai; Manipal Institute of Technology,66.66666666666666,India,33.33333333333334,USA,"With increasing popularity of home-based fitness regimen post-pandemic, there has been a growing interest in fitness monitoring solutions. Owing to this, human pose monitoring has gained significant commercial importance in the field of computer vision. Most efforts in the past focused on the task of human pose classification for various applications. In this work, we instead focus on a critical aspect of human pose monitoring that naturally follows from basic pose classification i.e., pose analysis and correction. Specifically, we study human pose correction through the lens of algorithmic recourse. Algorithmic recourse involves a model providing explanations on a how a model arrived at a decision, along with possible actions to drive the model to output a favorable decision. To this end, we develop CARE (Counterfactuals based Algorithmic Recourse for Explainable pose correction), a novel method that uses counterfactual explanations to provide recourse for incorrect human poses, thereby helping a user correct their pose. Experiments on three diverse datasets, including two fitness datasets and one hand gestures dataset, demonstrate the effectiveness and applicability of CARE.",https://openaccess.thecvf.com/content/WACV2024/html/Dittakavi_CARE_Counterfactual-Based_Algorithmic_Recourse_for_Explainable_Pose_Correction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dittakavi_CARE_Counterfactual-Based_Algorithmic_Recourse_for_Explainable_Pose_Correction_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483851/,"['Measurement', 'Training', 'Computer vision', 'Machine learning algorithms', 'Machine learning', 'Vectors', 'Robustness']","['Pose Correction', 'Computer Vision', 'Activity Tracker', 'Field Of Computer Vision', 'Human Pose', 'Machine Learning Models', 'Corrective Actions', 'Motion Capture', 'Mean Absolute Deviation', 'Joint Angles', 'Action Recognition', 'Pose Estimation', 'Graph Convolutional Network', 'Active Vectors', 'Gesture Recognition', 'Vector Angle', 'Human Pose Estimation', 'Set Of Angles', 'Angle Correction', 'Hinge Loss', 'American Sign Language', 'Correct Pose', 'Ground Truth Pose', 'Yoga Postures', 'Pilates', 'Range Of Perturbations']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Applications', 'Biomedical / healthcare / medicine', 'Applications', 'Smartphones / end user devices']",,"With increasing popularity of home-based fitness regimen post-pandemic, there has been a growing interest in fitness monitoring solutions. Owing to this, human pose monitoring has gained significant commercial importance in the field of computer vision. Most efforts in the past focused on the task of human pose classification for various applications. In this work, we instead focus on a critical aspect of human pose monitoring that naturally follows from basic pose classification i.e., pose analysis and correction. Specifically, we study human pose correction through the lens of algorithmic recourse. Algorithmic recourse involves a model providing explanations on a how a model arrived at a decision, along with possible actions to drive the model to output a favorable decision. To this end, we develop CARE (Counterfactuals based Algorithmic Recourse for Explainable pose correction), a novel framework that uses counterfactual explanations to provide recourse for incorrect human poses, thereby helping a user correct their pose. Experiments on three diverse datasets, including two fitness datasets and one hand gestures dataset, demonstrate the effectiveness and applicability of CARE."
CATS: Combined Activation and Temporal Suppression for Efficient Network Inference,"Zeqi Zhu, Arash Pourtaherian, Luc Waeijen, Ibrahim Batuhan Akkaya, Egor Bondarev, Orlando Moreira",GrAI Matter Labs B.V.; Eindhoven University of Technology; GrAI Matter Labs B.V. and Eindhoven University of Technology,66.66666666666666,Netherlands,33.33333333333334,Netherlands,"Brain-inspired event-driven processors execute deep neural networks (DNNs) in an event sparsity-aware manner, leading to superior performance compared to conventional platforms. In the pursuit of higher event sparsity, prior studies suppress non-zero events by either eliminating the intra-frame activations (spatially) or leveraging the redundancy in the inter-frame differences for a video (temporally). However, we have empirically observed that simultaneously enhancing activation and temporal sparsity can lead to a synergistic suppression outcome. To this end, we propose an end-to-end event suppression training approach CATS -- Combined Activation and Temporal Suppression for efficient network inference. It utilizes a gradient-based method to search for the optimal temporal thresholds for the network while penalizing the presence of non-zero events in spatial and temporal domains simultaneously. We demonstrate that CATS achieves 2 6 times more event suppression compared to the inherent ReLU suppression, consistently outperforming the SOTA by a significant margin at various accuracy levels. Extensive experimental results show that CATS also generalizes to multiple tasks -- object detection, object tracking, pose estimation, and semantic segmentation. Furthermore, a case study for the commercial event-driven processor GrAI-VIP highlights that the induced event sparsity in SSD on EgoHands datasets efficiently translates into significant improvements of 2.5 x in FPS, 2.1 x in latency, and 3.8 x in energy consumption, while maintaining the model accuracy.",https://openaccess.thecvf.com/content/WACV2024/html/Zhu_CATS_Combined_Activation_and_Temporal_Suppression_for_Efficient_Network_Inference_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_CATS_Combined_Activation_and_Temporal_Suppression_for_Efficient_Network_Inference_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483976/,"['Training', 'Energy consumption', 'Computer vision', 'Program processors', 'Computational modeling', 'Redundancy', 'Energy conservation']","['Suppression Of Activity', 'Efficient Inference', 'Energy Consumption', 'Deep Neural Network', 'Spatial Domain', 'Optimal Threshold', 'Training Approach', 'Temporal Domain', 'Convolution', 'Energy Conservation', 'Model Quality', 'Object Detection', 'Effect Of Density', 'Probability Sampling', 'Stochastic Gradient Descent', 'Video Frames', 'Semantic Segmentation', 'Standard Training', 'Pose Estimation', 'Object Tracking', 'Sparsity Penalty', 'Lth Layer', 'Bayesian Optimization', 'Discrete Samples', 'Quantization Bits', 'Spatial Redundancy', 'Task Loss', 'Threshold Distribution', 'Event Counts', 'Time Step']","['Applications', 'Embedded sensing / real-time techniques', 'Applications', 'Smartphones / end user devices']",1,"Brain-inspired event-driven processors execute deep neural networks (DNNs) in a sparsity-aware manner, leading to superior performance compared to conventional platforms. In the pursuit of higher event sparsity, prior studies suppress non-zero events by either eliminating the intra-frame activations (spatially) or leveraging the redundancy in the inter-frame differences for a video (temporally). However, we have empirically observed that simultaneously enhancing activation and temporal sparsity can lead to a synergistic suppression outcome. To this end, we propose an end-to-end event suppression training approach CATS −− Combined Activation and Temporal Suppression for efficient network inference. It utilizes a gradient-based method to search for the optimal temporal thresholds per layer while penalizing the presence of events in both spatial and temporal domains. Our experimental results show that CATS achieves 2 ∼ 6× higher event suppression compared to the inherent ReLU suppression across a wide range of vision applications, consistently outperforming the state-of-the-art (SOTA) methods by a significant margin at all accuracy levels. Furthermore, a case study on the commercial event-driven processor GrAI-VIP highlights that the induced event sparsity in SSD on the EgoHands dataset can be efficiently translated into a performance enhancement of 2.5× in FPS, 2.1× in latency, and 3.8× in energy consumption, while maintaining the model accuracy."
CCMR: High Resolution Optical Flow Estimation via Coarse-To-Fine Context-Guided Motion Reasoning,"Azin Jahedi, Maximilian Luz, Marc Rivinius, Andrés Bruhn","VIS, University of Stuttgart; RLL, University of Freiburg; SEC, University of Stuttgart",100.0,Germany,0.0,,"Attention-based motion aggregation concepts have recently shown their usefulness in optical flow estimation, in particular when it comes to handling occluded regions.  However, due to their complexity, such concepts have been mainly restricted to coarse-resolution single-scale approaches that fail to provide the detailed outcome of high-resolution multi-scale networks. In this paper, we hence propose CCMR: a high-resolution coarse-to-fine approach that leverages attention-based motion grouping concepts to multi-scale optical flow estimation. CCMR relies on a hierarchical two-step attention-based context-motion grouping strategy that first computes global multi-scale context features and then uses them to guide the actual motion grouping. As we iterate both steps over all coarse-to-fine scales, we adapt cross covariance image transformers to allow for an efficient realization while maintaining scale-dependent properties. Experiments and ablations demonstrate that our efforts of combining multi-scale and attention-based concepts pay off. By providing highly detailed flow fields  with strong improvements in both occluded and non-occluded regions, our CCMR approach not only outperforms both the corresponding single-scale attention-based and multi-scale attention-free baselines by up to 23.0% and 21.6%, respectively, it also achieves state-of-the-art results, ranking first on KITTI 2015 and second on MPI Sintel Clean and Final. Code and trained models are available at https://github.com/cv-stuttgart/CCMR.",https://openaccess.thecvf.com/content/WACV2024/html/Jahedi_CCMR_High_Resolution_Optical_Flow_Estimation_via_Coarse-To-Fine_Context-Guided_Motion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jahedi_CCMR_High_Resolution_Optical_Flow_Estimation_via_Coarse-To-Fine_Context-Guided_Motion_WACV_2024_paper.pdf,,https://github.com/cv-stuttgart/CCMR,2311.02661,main,Poster,https://ieeexplore.ieee.org/document/10483845/,"['Computer vision', 'Codes', 'Computational modeling', 'Neural networks', 'Estimation', 'Feature extraction', 'Transformers']","['Optical Flow', 'Flow Estimation', 'Optical Flow Estimation', 'Ablation', 'Global Features', 'Global Context', 'Multi-scale Features', 'Detailed Field', 'Occluded Regions', 'Neural Network', 'Convolutional Neural Network', 'Image Features', 'Fine Scale', 'Feed-forward Network', 'Motion Features', 'Recurrent Unit', 'Intermediate Features', 'Gated Recurrent Unit', 'Choice Architecture', 'Global Motion', 'Query Features', 'Cost Volume', 'Matching Regions', 'Residual Unit', 'Finest Scale', 'Matching Cost', 'Coarse Scale', 'Global Interaction']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Attention-based motion aggregation concepts have recently shown their usefulness in optical flow estimation, in particular when it comes to handling occluded regions. However, due to their complexity, such concepts have been mainly restricted to coarse-resolution single-scale approaches that fail to provide the detailed outcome of high-resolution multi-scale networks. In this paper, we hence propose CCMR: a high-resolution coarse-to-fine approach that leverages attention-based motion grouping concepts to multi-scale optical flow estimation. CCMR relies on a hierarchical two-step attention-based context-motion grouping strategy that first computes global multi-scale context features and then uses them to guide the actual motion grouping. As we iterate both steps over all coarse-to-fine scales, we adapt cross covariance image transformers to allow for an efficient realization while maintaining scale-dependent properties. Experiments and ablations demonstrate that our efforts of combining multi-scale and attention-based concepts pay off. By providing highly detailed flow fields with strong improvements in both occluded and non-occluded regions, our CCMR approach not only outperforms both the corresponding single-scale attention-based and multi-scale attention-free baselines by up to 23.0% and 21.6%, respectively, it also achieves state-of-the-art results, ranking first on KITTI 2015 and second on MPI Sintel Clean and Final. Code and trained models are available at https://github.com/cv-stuttgart/CCMR."
CGAPoseNet+GCAN: A Geometric Clifford Algebra Network for Geometry-Aware Camera Pose Regression,"Alberto Pepe, Joan Lasenby, Sven Buchholz","Signal Processing and Communications Lab, University of Cambridge; Department of Computer Science and Media, Technical University Brandenburg",100.0,"Germany, UK",0.0,,"We introduce CGAPoseNet+GCAN, which enhances CGAPoseNet, an architecture for camera pose regression, with a Geometric Clifford Algebra Network (GCAN). With the addition of the GCAN we obtain a geometry-aware pipeline for camera pose regression from RGB images only. CGAPoseNet employs Clifford Geometric Algebra to unify quaternions and translation vectors into a single mathematical object, the motor, which can be used to uniquely describe camera poses. CGAPoseNet solves the issue of balancing rotation and translation components in the loss function, and can obtain comparable results to other approaches without the need of expensive tuning of the loss function or additional information about the scene, such as 3D point clouds, which might not always be available. CGAPoseNet, however, like several approaches in the literature, only learns to predict motor coefficients, and it is unaware of the mathematical space in which predictions sit in and of their geometrical meaning. By leveraging recent advances in Geometric Deep Learning, we modify CGAPoseNet with a GCAN: proposals of possible motor coefficients associated with a camera frame are obtained from the InceptionV3 backbone, and the GCAN downsamples them to a single motor through a sequence of layers that work in G_ 4,0 . The network is hence geometry-aware, has multivector-valued inputs, weights and biases and preserves the grade of the objects that it receives in input. CGAPoseNet+GCAN has almost 4 million fewer trainable parameters, it reduces the average rotation error by 41% and the average translation error by 8.8% compared to CGAPoseNet. Similarly, it reduces rotation and translation errors by 32.6% and 19.9%, respectively, compared to the best performing PoseNet strategy. CGAPoseNet+GCAN reaches the state-of-the-art results on 13 commonly employed datasets. To the best of our knowledge, it is the first experiment in GCANs applied to the problem of camera pose regression.",https://openaccess.thecvf.com/content/WACV2024/html/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484115/,"['Deep learning', 'Computer vision', 'Three-dimensional displays', 'Algebra', 'Quaternions', 'Cameras', 'Motors']","['Camera Pose', 'Clifford Algebra', 'Pose Regression', 'Camera Pose Regression', 'Loss Function', 'Geometric Mean', 'Trainable Parameters', 'RGB Images', 'Single Object', 'Fewer Parameters', '3D Point', 'Approaches In The Literature', 'Translation Error', 'Translation Vector', 'Rotation Error', 'Neural Network', 'Preschool', 'Convolutional Neural Network', 'Parametrized', 'Vector-based', 'Rotated Component', 'Geometric Transformation', 'Scene Geometry', 'Extra Dimension', 'Jupyter Notebook', 'Euclidean Space', 'Bundle Adjustment', 'Multivector', 'Reprojection Error', 'Spherical Space']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"We introduce CGAPoseNet+GCAN, which enhances CGAPoseNet, an architecture for camera pose regression, with a Geometric Clifford Algebra Network (GCAN). With the addition of the GCAN we obtain a geometry-aware pipeline for camera pose regression from RGB images only. CGAPoseNet employs Clifford Geometric Algebra to unify quaternions and translation vectors into a single mathematical object, the motor, which can be used to uniquely describe camera poses. CGAPoseNet can obtain comparable results to other approaches without the need of expensive tuning of the loss function or additional information about the scene, such as 3D point clouds, which might not always be available. CGAPoseNet, however, like several approaches in the literature, only learns to predict motor coefficients, and it is unaware of the mathematical space in which predictions sit in and of their geometrical meaning. By leveraging recent advances in Geometric Deep Learning, we modify CGAPoseNet with a GCAN: proposals of possible motor coefficients associated with a camera frame are obtained from the InceptionV3 backbone, and the GCAN downsamples them to a single motor through a sequence of layers that work in ${{\mathcal{G}}_{4,0}}$
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</inf>
. The network is hence geometry-aware, has multivector-valued inputs, weights and biases and preserves the grade of the objects that it receives in input. CGAPoseNet+GCAN has almost 4 million fewer trainable parameters, it reduces the average rotation error by 41% and the average translation error by 8.8% compared to CGAPoseNet. Similarly, it reduces rotation and translation errors by 32.6% and 19.9%, respectively, compared to the best performing PoseNet strategy. CGAPoseNet+GCAN reaches the state-of-the-art results on 13 commonly employed datasets. To the best of our knowledge, it is the first experiment in GCANs applied to the problem of camera pose regression."
CHAI: Craters in Historical Aerial Images,"Marvin Burges, Sebastian Zambanini, Philipp Pirker","Luftbilddatenbank Dr. Carls GmbH, Austria; Computer Vision Lab, TU Wien, Austria",50.0,Austria,50.0,Austria,"In this paper we highlight the importance of historical aerial images in better understanding past events and their impact on their surroundings. More specifically, we are interested in studying bomb craters from World War II in Central Europe. We note the scarcity of publicly accessible datasets that provide labeled bomb craters and subsequently introduce a novel, domain-expert-annotated dataset comprised of 99 historical aerial images of Austria and Germany. We divide said data into training, validation, and test sets, and conduct training and evaluation using different object detectors - both general purpose and specifically designed for remote sensing applications. This dataset thus serves as a benchmark for developing and evaluating (several) algorithms dedicated to the automated detection and analysis of bomb craters in historical aerial images. We underscore the uniqueness of this dataset as the first publicly available resource containing annotated bomb craters, thereby offering researchers a valueable and novel opportunity for future exploration. Lastly, we investigate possibilities for extending and enriching our data to enhance future studies, particularly within the context of preliminary risk estimation for unexploded bombs.",https://openaccess.thecvf.com/content/WACV2024/html/Burges_CHAI_Craters_in_Historical_Aerial_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Burges_CHAI_Craters_in_Historical_Aerial_Images_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483872/,"['Training', 'Computer vision', 'Weapons', 'Europe', 'Estimation', 'Detectors', 'Benchmark testing']","['Aerial Images', 'Historical Aerial Images', 'Validation Set', 'Central Europe', 'Object Detection', 'Urban Areas', 'Large Datasets', 'Training Data', 'Deep Learning', 'Convolutional Neural Network', 'Image Dataset', 'Number Of Images', 'Georeferenced', 'Bounding Box', 'Multiple Images', 'Small Objects', 'Domain Adaptation', 'Industry Partners', 'Annotation Process', 'Historical Dataset', 'Ground Sampling Distance', 'Real-world Images', 'National Archives', 'Domain Adaptation Methods']","['Applications', 'Remote Sensing', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Environmental monitoring / climate change / ecology']",,"In this paper we highlight the importance of historical aerial images in better understanding past events and their impact on their surroundings. More specifically, we are interested in studying bomb craters from World War II in Central Europe. We note the scarcity of publicly accessible datasets that provide labeled bomb craters and subsequently introduce a novel, domain-expert-annotated dataset comprised of 99 historical aerial images of Austria and Germany. We divide said data into training, validation, and test sets, and conduct training and evaluation using different object detectors - both general-purpose and specifically designed for remote sensing applications. This dataset thus serves as a benchmark for developing and evaluating (several) algorithms dedicated to the automated detection and analysis of bomb craters in historical aerial images. We underscore the uniqueness of this dataset as the first publicly available resource containing annotated bomb craters, thereby offering researchers a valueable and novel opportunity for future exploration. Lastly, we investigate possibilities for extending and enriching our data to enhance future studies, particularly within the context of preliminary risk estimation for unexploded bombs."
CL-MAE: Curriculum-Learned Masked Autoencoders,"Neelu Madan, Nicolae-Cătălin Ristea, Kamal Nasrollahi, Thomas B. Moeslund, Radu Tudor Ionescu","University of Bucharest, Romania; Aalborg University, Denmark; University Politehnica of Bucharest, Romania",100.0,"Denmark, Romania",0.0,,"Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same reconstruction loss) to an adversary (optimizing the opposite loss), while passing through a neutral state. The transition between these behaviors is smooth, being regulated by a factor that is multiplied with the reconstruction loss of the masking module. The resulting training procedure generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior representation learning capabilities compared to MAE. The empirical results on five downstream tasks confirm our conjecture, demonstrating that curriculum learning can be successfully used to self-supervise masked autoencoders. We release our code at https://github.com/ristea/cl-mae.",https://openaccess.thecvf.com/content/WACV2024/html/Madan_CL-MAE_Curriculum-Learned_Masked_Autoencoders_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Madan_CL-MAE_Curriculum-Learned_Masked_Autoencoders_WACV_2024_paper.pdf,,https://github.com/ristea/cl-mae,,main,Poster,https://ieeexplore.ieee.org/document/10484320/,"['Training', 'Representation learning', 'Computer vision', 'Codes', 'Transfer learning', 'Self-supervised learning', 'Complexity theory']","['Input Image', 'ImageNet', 'Representation Learning', 'Reconstruction Loss', 'Masked Images', 'Robust Representation', 'Curriculum Learning', 'Joint Training', 'Reconstruction Task', 'Pretext Task', 'Self-supervised Task', 'Masking Strategy', 'Loss Function', 'Hyperparameters', 'Loss Of Diversity', 'Multilayer Perceptron', 'Binary Vector', 'Generative Adversarial Networks', 'McNemar Test', 'Aerial Images', 'Thresholding Operator', 'Tokenized', 'Input Tokens', 'Vision Transformer', 'Linear Probe', 'Adversarial Training', 'Self-supervised Learning', 'Sigmoid Activation', 'Wind Turbine', 'Joint Loss']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding']",,"Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same reconstruction loss) to an adversary (optimizing the opposite loss), while passing through a neutral state. The transition between these behaviors is smooth, being regulated by a factor that is multiplied with the reconstruction loss of the masking module. The resulting training procedure generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior representation learning capabilities compared to MAE. The empirical results on five downstream tasks confirm our conjecture, demonstrating that curriculum learning can be successfully used to self-supervise masked autoencoders. We release our code at https://github.com/ristea/cl-mae."
CLID: Controlled-Length Image Descriptions With Limited Data,"Elad Hirsch, Ayellet Tal",Technion – Israel Institute of Technology,100.0,Israel,0.0,,"Controllable image captioning models generate human-like image descriptions, enabling some kind of control over the generated captions. This paper focuses on controlling the caption length, i.e. a short and concise description or a long and detailed one. Since existing image captioning datasets contain mostly short captions, generating long captions is challenging. To address the shortage of long training examples, we propose to enrich the dataset with varying-length self-generated captions. These, however, might be of varying quality and are thus unsuitable for conventional training. We introduce a novel training strategy that selects the data points to be used at different times during the training. Our method dramatically improves the length-control abilities, while exhibiting SoTA performance in terms of caption quality. Our approach is general and is shown to be applicable also to paragraph generation.",https://openaccess.thecvf.com/content/WACV2024/html/Hirsch_CLID_Controlled-Length_Image_Descriptions_With_Limited_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hirsch_CLID_Controlled-Length_Image_Descriptions_With_Limited_Data_WACV_2024_paper.pdf,,https://github.com/eladhi/CLIDCOCO,2211.14835,main,Poster,https://ieeexplore.ieee.org/document/10483930/,"['Training', 'Computer vision', 'Codes', 'Data models']","['Image Descriptors', 'Short Description', 'Image Captioning', 'Training Dataset', 'Small Datasets', 'Visual Features', 'Step Function', 'Training Procedure', 'Bounding Box', 'Language Model', 'Word Embedding', 'Extensive Dataset', 'Image Retrieval', 'Control Ability', 'Low-quality Data', 'Saliency Map', 'Depth-first', 'Consecutive Iterations', 'Neural Machine Translation', 'Scene Graph']","['Algorithms', 'Vision + language and/or other modalities']",1,"Controllable image captioning models generate human-like image descriptions, enabling some kind of control over the generated captions. This paper focuses on controlling the caption length, i.e. a short and concise description or a long and detailed one. Since existing image captioning datasets contain mostly short captions, generating long captions is challenging. To address the shortage of long training examples, we propose to enrich the dataset with varying-length self-generated captions. These, however, might be of varying quality and are thus unsuitable for conventional training. We introduce a novel training strategy that selects the data points to be used at different times during the training. Our method dramatically improves the length-control abilities, while exhibiting SoTA performance in terms of caption quality. Our approach is general and is shown to be applicable also to paragraph generation. Our code is publicly available 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free,"Monika Wysoczańska, Michaël Ramamonjisoa, Tomasz Trzciński, Oriane Siméoni","Valeo.ai; Meta AI; Warsaw University of Technology; Warsaw University of Technology, Tooploox, IDEAS NCBR",50.0,Poland,50.0,France,"The emergence of CLIP has opened the way for open-world image perception. The zero-shot classification capabilities of the model are impressive but are harder to use for dense tasks such as image segmentation. Several methods have proposed different modifications and learning schemes to produce dense output. Instead, we propose in this work an open-vocabulary semantic segmentation method, dubbed CLIP-DIY, which does not require any additional training or annotations, but instead leverages existing unsupervised object localization approaches. In particular, CLIP-DIY is a multi-scale approach that directly exploits CLIP classification abilities on patches of different sizes and aggregates the decision in a single map. We further guide the segmentation using foreground/background scores obtained using unsupervised object localization methods. With our method, we obtain state-of-the-art zero-shot semantic segmentation results on PASCAL VOC and perform on par with the best methods on COCO.",https://openaccess.thecvf.com/content/WACV2024/html/Wysoczanska_CLIP-DIY_CLIP_Dense_Inference_Yields_Open-Vocabulary_Semantic_Segmentation_For-Free_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wysoczanska_CLIP-DIY_CLIP_Dense_Inference_Yields_Open-Vocabulary_Semantic_Segmentation_For-Free_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484481/,"['Location awareness', 'Training', 'Computer vision', 'Annotations', 'Semantic segmentation', 'Aggregates', 'Semantics']","['Semantic Segmentation', 'Segmentation Method', 'Object Location', 'Patch Size', 'Classification Ability', 'Single Map', 'Multiscale Approach', 'Semantic Segmentation Methods', 'Input Image', 'Image Pixels', 'Failure Cases', 'Self-supervised Learning', 'Left Image', 'Image X', 'COCO Dataset', 'Foreground Objects', 'Background Class', 'Pixel-level Annotations', 'Text Query', 'PASCAL VOC Dataset']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",7,"The emergence of CLIP has opened the way for open-world image perception. The zero-shot classification capabilities of the model are impressive but are harder to use for dense tasks such as image segmentation. Several methods have proposed different modifications and learning schemes to produce dense output. Instead, we propose in this work an open-vocabulary semantic segmentation method, dubbed CLIP-DIY, which does not require any additional training or annotations, but instead leverages existing unsupervised object localization approaches. In particular, CLIP-DIY is a multi-scale approach that directly exploits CLIP classification abilities on patches of different sizes and aggregates the decision in a single map. We further guide the segmentation using foreground/background scores obtained using unsupervised object localization methods. With our method, we obtain state-of-the-art zero-shot semantic segmentation results on PASCAL VOC and perform on par with the best methods on COCO."
CLIPAG: Towards Generator-Free Text-to-Image Generation,"Roy Ganz, Michael Elad","Department of Computer Science, Technion, Haifa, Israel; Department of ECE, Technion, Haifa, Israel",100.0,Israel,0.0,,"Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a ""plug-n-play"" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation without any generative model, which typically requires huge generators.",https://openaccess.thecvf.com/content/WACV2024/html/Ganz_CLIPAG_Towards_Generator-Free_Text-to-Image_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ganz_CLIPAG_Towards_Generator-Free_Text-to-Image_Generation_WACV_2024_paper.pdf,,,2306.16805,main,Poster,https://ieeexplore.ieee.org/document/10484349,"['Computer vision', 'Computational modeling', 'Semantics', 'Computer architecture', 'Generators', 'Task analysis', 'Image classification']","['Feature Space', 'Image Generation', 'Design Choices', 'Textual Descriptions', 'Adversarial Training', 'Adversarial Attacks', 'Threat Model', 'Style Transfer', 'Image Editing', 'Adversarial Examples', 'Framework Synthesis', 'Vision Transformer', 'Image Encoder', 'Fréchet Inception Distance', 'Bezier Curve', 'Projected Gradient Descent', 'Adversarial Robustness', 'Text Encoder']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Vision + language and/or other modalities']",1,"Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a ""plug-n-play"" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation without any generative model, which typically requires huge generators."
CLRerNet: Improving Confidence of Lane Detection With LaneIoU,"Hiroto Honda, Yusuke Uchida",GO Inc.,0.0,,100.0,USA,"Lane marker detection is a crucial component of the autonomous driving and driver assistance systems. Modern deep lane detection methods with anchor-based lane representation exhibit excellent performance on lane detection benchmarks. Through preliminary oracle experiments, we firstly disentangle the lane representation components to determine the direction of our approach. We show that correct lane positions are already among the predictions of an existing anchor-based detector, and the confidence scores that accurately represent intersection-over-union (IoU) with ground truths are the most beneficial. Based on the finding, we propose LaneIoU that better correlates with the metric, by taking the local lane angles into consideration. We develop a novel detector coined CLRerNet featuring LaneIoU for the target assignment cost and loss functions aiming at the improved quality of confidence scores. Through careful and fair benchmark including cross validation, we demonstrate that CLRerNet outperforms the state-of-the-art by a large margin - enjoying F1 score of 81.43% compared with 80.47% of the existing method on CULane, and 86.47% compared with 86.10% on CurveLanes.",https://openaccess.thecvf.com/content/WACV2024/html/Honda_CLRerNet_Improving_Confidence_of_Lane_Detection_With_LaneIoU_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Honda_CLRerNet_Improving_Confidence_of_Lane_Detection_With_LaneIoU_WACV_2024_paper.pdf,,https://github.com/hirotomusiker/CLRerNet,2305.08366,main,Poster,https://ieeexplore.ieee.org/document/10484275/,"['Measurement', 'Training', 'Computer vision', 'Protocols', 'Costs', 'Codes', 'Lane detection']","['Lane Detection', 'Loss Function', 'F1 Score', 'Confidence Score', 'Large Margin', 'Advanced Driver Assistance Systems', 'Lane Markings', 'Convolutional Neural Network', 'Horizontal Plane', 'Cost Function', 'Feature Maps', 'Object Detection', 'Parametric Methods', 'Bounding Box', 'Fully-connected Layer', 'Line Of Work', 'Five-fold Cross-validation', '5-fold Cross-validation', 'Imbalanced Data', 'Confidence Threshold', 'Anchor-based Methods', 'Dynamic Assignment', 'Auxiliary Loss', 'Segmentation-based Methods', 'Validation Split', 'Cost Matrix', 'Test Split']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving']",11,"Lane marker detection is a crucial component of the autonomous driving and driver assistance systems. Modern deep lane detection methods with anchor-based lane representation exhibit excellent performance on lane detection benchmarks. Through preliminary oracle experiments, we firstly disentangle the lane representation components to determine the direction of our approach. We show that correct lane positions are already among the predictions of an existing anchor-based detector, and the confidence scores that accurately represent intersection-over-union (IoU) with ground truths are the most beneficial. Based on the finding, we propose LaneIoU that better correlates with the metric, by taking the local lane angles into consideration. We develop a novel detector coined CLRerNet featuring LaneIoU for the target assignment cost and loss functions aiming at the improved quality of confidence scores. Through careful and fair benchmark including cross validation, we demonstrate that CLRerNet outperforms the state-of-the-art by a large margin - enjoying F1 score of 81.43% compared with 80.47% of the existing method on CULane, and 86.47% compared with 86.10% on CurveLanes. Code and models are available at https://github.com/hirotomusiker/CLRerNet."
CPSeg: Finer-Grained Image Semantic Segmentation via Chain-of-Thought Language Prompting,Lei Li,University of Copenhagen,100.0,Denmark,0.0,,"Natural scene analysis and remote sensing imagery offer immense potential for advancements in large-scale language-guided context-aware data utilization. This potential is particularly significant for enhancing performance in downstream tasks such as object detection and segmentation with designed language prompting. In light of this, we introduce the CPSeg (Chain-of-Thought Language Prompting for Finer-grained Semantic Segmentation), an innovative framework designed to augment image segmentation performance by integrating a novel ""Chain-of-Thought"" process that harnesses textual information associated with images. This groundbreaking approach has been applied to a flood disaster scenario. CPSeg encodes prompt texts derived from various sentences to formulate a coherent chain-of-thought. We use a new vision-language dataset, FloodPrompt, which includes images, semantic masks, and corresponding text information. This not only strengthens the semantic understanding of the scenario but also aids in the key task of semantic segmentation through an interplay of pixel and text matching maps. Our qualitative and quantitative analyses validate the effectiveness of CPSeg.",https://openaccess.thecvf.com/content/WACV2024/html/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_CPSeg_Finer-Grained_Image_Semantic_Segmentation_via_Chain-of-Thought_Language_Prompting_WACV_2024_paper.pdf,,,2310.16069,main,Poster,https://ieeexplore.ieee.org/document/10483771/,"['Computer vision', 'Statistical analysis', 'Semantic segmentation', 'Disasters', 'Semantics', 'Object detection', 'Floods']","['Semantic Segmentation', 'Image Segmentation', 'Object Detection', 'Segmentation Method', 'Human Cognition', 'Language Model', 'Textual Information', 'Segmentation Accuracy', 'Segmentation Task', 'Segmentation Approach', 'Segmentation Performance', 'Semantic Segmentation Task', 'Objects In Context', 'Contribution Of Different Components', 'Disaster Scenarios', 'Text Encoder', 'Computer Vision', 'Image Regions', 'Tokenized', 'Substring', 'Sequential Decision', 'Vision Transformer', 'Improve Segmentation Performance', 'Score Map']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Remote Sensing']",4,"Natural scene analysis and remote sensing imagery offer immense potential for advancements in large-scale language-guided context-aware data utilization. This potential is particularly significant for enhancing performance in downstream tasks such as object detection and segmentation with designed language prompting. In light of this, we introduce the CPSeg (Chain-of-Thought Language Prompting for Finer-grained Semantic Segmentation), an innovative framework designed to augment image segmentation performance by integrating a novel ""Chain-of-Thought"" process that harnesses textual information associated with images. This groundbreaking approach has been applied to a flood disaster scenario. CPSeg encodes prompt texts derived from various sentences to formulate a coherent chain-of-thought. We use a new vision-language dataset, FloodPrompt, which includes images, semantic masks, and corresponding text information. This not only strengthens the semantic understanding of the scenario but also aids in the key task of semantic segmentation through an interplay of pixel and text matching maps. Our qualitative and quantitative analyses validate the effectiveness of CPSeg."
CSAM: A 2.5D Cross-Slice Attention Module for Anisotropic Volumetric Medical Image Segmentation,"Alex Ling Yu Hung, Haoxin Zheng, Kai Zhao, Xiaoxi Du, Kaifeng Pang, Qi Miao, Steven S. Raman, Demetri Terzopoulos, Kyunghyun Sung","University of California, Los Angeles",100.0,USA,0.0,,"A large portion of volumetric medical data, especially magnetic resonance imaging (MRI) data, is anisotropic, as the through-plane resolution is typically much lower than the in-plane resolution. Both 3D and purely 2D deep learning-based segmentation methods are deficient in dealing with such volumetric data since the performance of 3D methods suffers when confronting anisotropic data, and 2D methods disregard crucial volumetric information. Insufficient work has been done on 2.5D methods, in which 2D convolution is mainly used in concert with volumetric information. These models focus on learning the relationship across slices, but typically have many parameters to train. We offer a Cross-Slice Attention Module (CSAM) with minimal trainable parameters, which captures information across all the slices in the volume by applying semantic, positional, and slice attention on deep feature maps at different scales. Our extensive experiments using different network architectures and tasks demonstrate the usefulness and generalizability of CSAM. Associated code is available at https://github.com/aL3x-O-o-Hung/CSAM.",https://openaccess.thecvf.com/content/WACV2024/html/Hung_CSAM_A_2.5D_Cross-Slice_Attention_Module_for_Anisotropic_Volumetric_Medical_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hung_CSAM_A_2.5D_Cross-Slice_Attention_Module_for_Anisotropic_Volumetric_Medical_WACV_2024_paper.pdf,,https://github.com/aL3x-O-o-Hung/CSAM,,main,Poster,https://ieeexplore.ieee.org/document/10484030/,"['Image segmentation', 'Solid modeling', 'Three-dimensional displays', 'Image resolution', 'Uncertainty', 'Magnetic resonance imaging', 'Anisotropic']","['Medical Imaging', 'Image Segmentation', 'Attention Module', 'Medical Image Segmentation', 'Magnetic Resonance Imaging', 'Imaging Data', 'Feature Maps', 'Segmentation Method', 'Trainable Parameters', 'In-plane Resolution', '3D Method', 'Volumetric Data', '2D Methods', 'Slice Of Volume', 'Convolutional Neural Network', 'Placenta', 'Diffusion Tensor Imaging', 'Attention Mechanism', 'Semantic Information', 'Multilayer Perceptron', 'Dice Similarity Coefficient', 'Volumetric Imaging', 'Attention Map', '2D Model', 'Cardiovascular Magnetic Resonance', 'Convolutional Block', 'Average Pooling', 'Peripheral Zone', 'Transformer Block', 'Skip Connections']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",,"A large portion of volumetric medical data, especially magnetic resonance imaging (MRI) data, is anisotropic, as the through-plane resolution is typically much lower than the in-plane resolution. Both 3D and purely 2D deep learning-based segmentation methods are deficient in dealing with such volumetric data since the performance of 3D methods suffers when confronting anisotropic data, and 2D methods disregard crucial volumetric information. Insufficient work has been done on 2.5D methods, in which 2D convolution is mainly used in concert with volumetric information. These models focus on learning the relationship across slices, but typically have many parameters to train. We offer a Cross-Slice Attention Module (CSAM) with minimal trainable parameters, which captures information across all the slices in the volume by applying semantic, positional, and slice attention on deep feature maps at different scales. Our extensive experiments using different network architectures and tasks demonstrate the usefulness and generalizability of CSAM. Associated code is available at https://github.com/aL3x-O-o-Hung/CSAM."
CVTHead: One-Shot Controllable Head Avatar With Vertex-Feature Transformer,"Haoyu Ma, Tong Zhang, Shanlin Sun, Xiangyi Yan, Kun Han, Xiaohui Xie","University of California, Irvine",100.0,USA,0.0,,"Reconstructing personalized animatable head avatars has significant implications in the fields of AR/VR. Existing methods for achieving explicit face control of 3D Morphable Models (3DMM) typically rely on multi-view images or videos of a single subject, making the reconstruction process complex. Additionally, the traditional rendering pipeline is time-consuming, limiting real-time animation possibilities. In this paper, we introduce CVTHead, a novel approach that generates controllable neural head avatars from a single reference image using point-based neural rendering. CVTHead considers the sparse vertices of mesh as the point set and employs the proposed Vertex-feature Transformer to learn local feature descriptors for each vertex. This enables the modeling of long-range dependencies among all the vertices. Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves comparable performance to state-of-the-art graphics-based methods. Moreover, it enables efficient rendering of novel human heads with various expressions, head poses, and camera views. These attributes can be explicitly controlled using the coefficients of 3DMMs, facilitating versatile and realistic animation in real-time scenarios.",https://openaccess.thecvf.com/content/WACV2024/html/Ma_CVTHead_One-Shot_Controllable_Head_Avatar_With_Vertex-Feature_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ma_CVTHead_One-Shot_Controllable_Head_Avatar_With_Vertex-Feature_Transformer_WACV_2024_paper.pdf,,https://github.com/HowieMa/CVTHead,2311.06443,main,Poster,https://ieeexplore.ieee.org/document/10483850/,"['Solid modeling', 'Head', 'Three-dimensional displays', 'Avatars', 'Rendering (computer graphics)', 'Transformers', 'Animation']","['Single Image', 'Descriptive Characteristics', 'Reference Image', 'Humeral Head', 'Camera View', 'Local Descriptors', 'Long-range Dependencies', 'Implications For The Field', 'Multi-view Images', 'Explicit Control', 'Head Pose', 'Model Parameters', 'Feature Learning', 'Attention Mechanism', 'Point Cloud', 'Multilayer Perceptron', 'Image Space', 'Corresponding Points', '3D Scanning', 'Source Images', 'Facial Shape', 'Peak Signal-to-noise Ratio', '3D Point', 'Frames Per Second', '2D Projection', 'Mesh Vertices', 'Multi-head Self-attention', 'Motion Field', 'Shoulder Region', 'Head Rotation']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision']",1,"Reconstructing personalized animatable head avatars has significant implications in the fields of AR/VR. Existing methods for achieving explicit face control of 3D Morphable Models (3DMM) typically rely on multi-view images or videos of a single subject, making the reconstruction process complex. Additionally, the traditional rendering pipeline is time-consuming, limiting real-time animation possibilities. In this paper, we introduce CVTHead, a novel approach that generates controllable neural head avatars from a single reference image using point-based neural rendering. CVT-Head considers the sparse vertices of mesh as the point set and employs the proposed Vertex-feature Transformer to learn local feature descriptors for each vertex. This enables the modeling of long-range dependencies among all the vertices. Experimental results on the VoxCeleb dataset demonstrate that CVTHead achieves comparable performance to state-of-the-art graphics-based methods. Moreover, it enables efficient rendering of novel human heads with various expressions, head poses, and camera views. These attributes can be explicitly controlled using the coefficients of 3DMMs, facilitating versatile and realistic animation in real-time scenarios. Codes and pre-trained model can be found at https://github.com/HowieMa/CVTHead."
CXR-IRGen: An Integrated Vision and Language Model for the Generation of Clinically Accurate Chest X-Ray Image-Report Pairs,"Junjie Shentu, Noura Al Moubayed",Durham University,100.0,UK,0.0,,"Chest X-Ray (CXR) images play a crucial role in clinical practice, providing vital support for diagnosis and treatment. Augmenting the CXR dataset with synthetically generated CXR images annotated with radiology reports can enhance the performance of deep learning models for various tasks. However, existing studies have primarily focused on generating unimodal data of either images or reports. In this study, we propose an integrated model, CXR-IRGen, designed specifically for generating CXR image-report pairs. Our model follows a modularized structure consisting of a vision module and a language module. Notably, we present a novel prompt design for the vision module by combining both text embedding and image embedding of a reference image. Additionally, we propose a new CXR report generation model as the language module, which effectively leverages a large language model and self-supervised learning strategy. Experimental results demonstrate that our new prompt is capable of improving the general quality (FID) and clinical efficacy (AUROC) of the generated images, with average improvements of 15.84% and 1.84%, respectively. Moreover, the proposed CXR report generation model outperforms baseline models in terms of clinical efficacy (F1 score) and exhibits a high-level alignment of image and text, as the best F1 score of our model is 6.93% higher than the state-of-the-art CXR report generation model. Our code is available at https://github.com/junjie-shentu/CXR-IRGen.",https://openaccess.thecvf.com/content/WACV2024/html/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shentu_CXR-IRGen_An_Integrated_Vision_and_Language_Model_for_the_Generation_WACV_2024_paper.pdf,,https://github.com/junjie-shentu/CXR-IRGen,,main,Poster,https://ieeexplore.ieee.org/document/10483744/,"['Deep learning', 'Computer vision', 'Codes', 'Computational modeling', 'Self-supervised learning', 'Radiology', 'Task analysis']","['Chest X-ray', 'Integrated Model', 'Language Model', 'F1 Score', 'General Quality', 'Reference Image', 'Self-supervised Learning', 'Radiology Reports', 'Chest X-ray Images', 'Fréchet Inception Distance', 'Role In Clinical Practice', 'Image Embedding', 'Language Faculty', 'Mean Square Error', 'Natural Language', 'Generative Adversarial Networks', 'Diffusion Model', 'High-quality Images', 'Peak Signal-to-noise Ratio', 'General Metrics', 'Domain Adaptation', 'Image Encoder', 'Vision Transformer', 'Image Quality Metrics', 'Feature Recognition', 'Deep Generative Models', 'Variational Autoencoder', 'Image Captioning', 'Report Generation']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Image recognition and understanding']",3,"Chest X-Ray (CXR) images play a crucial role in clinical practice, providing vital support for diagnosis and treatment. Augmenting the CXR dataset with synthetically generated CXR images annotated with radiology reports can enhance the performance of deep learning models for various tasks. However, existing studies have primarily focused on generating unimodal data of either images or reports. In this study, we propose an integrated model, CXR-IRGen, designed specifically for generating CXR image-report pairs. Our model follows a modularized structure consisting of a vision module and a language module. Notably, we present a novel prompt design for the vision module by combining both text embedding and image embedding of a reference image. Additionally, we propose a new CXR report generation model as the language module, which effectively leverages a large language model and self-supervised learning strategy. Experimental results demonstrate that our new prompt is capable of improving the general quality (FID) and clinical efficacy (AUROC) of the generated images, with average improvements of 15.84% and 1.84%, respectively. Moreover, the proposed CXR report generation model outperforms baseline models in terms of clinical efficacy (F
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf>
 score) and exhibits a high-level alignment of image and text, as the best F
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf>
 score of our model is 6.93% higher than the state-of-the-art CXR report generation model. Our code is available at https://github.com/junjie-shentu/CXR-IRGen."
Camera-Independent Single Image Depth Estimation From Defocus Blur,"Lahiru Wijayasingha, Homa Alemzadeh, John A. Stankovic","Electrical and Computer Engineering Department, University of Virginia, USA; Computer Science Department, University of Virginia, USA",100.0,USA,0.0,,Monocular depth estimation is an important step in many downstream tasks in machine vision. We address the topic of estimating monocular depth from defocus blur which can yield more accurate results than the semantic based depth estimation methods. The existing monocular depth from defocus techniques are sensitive to the particular camera that the images are taken from. We show how several camera-related parameters affect the defocus blur using optical physics equations and how they make the defocus blur depend on these parameters. The simple correction procedure we propose can alleviate this problem which does not require any retraining of the original model. We created a synthetic dataset which can be used to test the camera independent performance of depth from defocus blur models. We evaluate our model on both synthetic and real datasets (DDFF12 and NYU depth V2) obtained with different cameras and show that our methods are significantly more robust to the changes of cameras.,https://openaccess.thecvf.com/content/WACV2024/html/Wijayasingha_Camera-Independent_Single_Image_Depth_Estimation_From_Defocus_Blur_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wijayasingha_Camera-Independent_Single_Image_Depth_Estimation_From_Defocus_Blur_WACV_2024_paper.pdf,,https://github.com/sleekEagle/defocus_camind.git,2311.13045,main,Poster,https://ieeexplore.ieee.org/document/10484050/,"['Privacy', 'Sensitivity', 'Semantics', 'Estimation', 'Predictive models', 'Cameras', 'Mathematical models']","['Single Image', 'Defocus', 'Depth Estimation', 'Machine Vision', 'Monocular Depth Estimation', 'Deep Learning', 'Field Of View', 'Image Pixels', 'Image Pairs', 'Focal Length', 'Analog-to-digital Converter', 'RGB Images', 'Final Image', 'Image Sensor', 'Object Size', 'Depth Map', 'Point Spread Function', 'Structured Illumination', 'Objective Structured', 'Structure From Motion', 'Blurred Images', 'Depth Prediction', 'Scene Depth', 'Camera Array', 'Stereo Matching', 'Mean Square Error', 'Cognitive Domains']","['Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Datasets and evaluations']",,Monocular depth estimation is an important step in many downstream tasks in machine vision. We address the topic of estimating monocular depth from defocus blur which can yield more accurate results than the semantic based depth estimation methods. The existing monocular depth from defocus techniques are sensitive to the particular camera that the images are taken from. We show how several camera-related parameters affect the defocus blur using optical physics equations and how they make the defocus blur depend on these parameters. The simple correction procedure we propose can alleviate this problem which does not require any retraining of the original model. We created a synthetic dataset which can be used to test the camera independent performance of depth from defocus blur models. We evaluate our model on both synthetic and real datasets (DDFF12 and NYU depth V2) obtained with different cameras and show that our methods are significantly more robust to the changes of cameras. Code: https://github.com/sleekEagle/defocus_camind.git
CamoFocus: Enhancing Camouflage Object Detection With Split-Feature Focal Modulation and Context Refinement,"Abbas Khan, Mustaqeem Khan, Wail Gueaieb, Abdulmotaleb El Saddik, Giulia De Masi, Fakhri Karray","University of Waterloo, Canada; Technology Innovation Institute, UAE; University of Ottawa, Canada; MBZUAI, UAE",100.0,"Canada, UAE",0.0,,"Camouflage Object Detection (COD) involves the challenge of isolating a target object from a visually similar background, presenting a formidable challenge for learning algorithms. Drawing inspiration from state-of-the-art (SOTA) Focal Modulation Networks, our objective is to proficiently modulate the foreground and background components, thereby capturing the distinct features of each. We introduce a Feature Split and Modulation (FSM) module to attain this goal. This module efficiently separates the object from the background by utilizing foreground and background modulators guided by a supervisory mask. For enhanced feature refinement, we propose a Context Refinement Module (CRM), which considers features acquired from FSM across various spatial scales, leading to comprehensive enrichment and highly accurate prediction maps. Through extensive experimentation, we showcase the superiority of CamoFocus over recent SOTA COD methods. Our evaluations encompass diverse benchmark datasets, including CAMO, COD10K, CHAMELEON, and NC4K. The findings underscore the potential and significance of the proposed CamoFocus model and establish its efficacy in addressing the critical challenges of camouflage object detection.",https://openaccess.thecvf.com/content/WACV2024/html/Khan_CamoFocus_Enhancing_Camouflage_Object_Detection_With_Split-Feature_Focal_Modulation_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Khan_CamoFocus_Enhancing_Camouflage_Object_Detection_With_Split-Feature_Focal_Modulation_and_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483928/,"['Computer vision', 'Modulation', 'Customer relationship management', 'Collaboration', 'Object detection', 'Benchmark testing']","['Camouflaged Object', 'Focal Modulation', 'Benchmark Datasets', 'Prediction Map', 'Feature Module', 'Drawing Inspiration', 'Background Components', 'Convolutional Layers', 'Feature Maps', 'Attention Mechanism', 'Receptive Field', 'Background Characteristics', 'Filter Size', 'ReLU Activation', 'Fewer Parameters', 'Element-wise Multiplication', 'Concatenated Feature Map', 'State Of The Art Techniques']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",6,"Camouflage Object Detection (COD) involves the challenge of isolating a target object from a visually similar background, presenting a formidable challenge for learning algorithms. Drawing inspiration from state-of-the-art (SOTA) Focal Modulation Networks, our objective is to proficiently modulate the foreground and background components, thereby capturing the distinct features of each. We introduce a Feature Split and Modulation (FSM) module to attain this goal. This module efficiently separates the object from the background by utilizing foreground and background modulators guided by a supervisory mask. For enhanced feature refinement, we propose a Context Refinement Module (CRM), which considers features acquired from FSM across various spatial scales, leading to comprehensive enrichment and highly accurate prediction maps. Through extensive experimentation, we showcase the superiority of CamoFocus over recent SOTA COD methods. Our evaluations encompass diverse benchmark datasets, including CAMO, COD10K, CHAMELEON, and NC4K. The findings underscore the potential and significance of the proposed CamoFocus model and establish its efficacy in addressing the critical challenges of camouflage object detection."
Can CLIP Help Sound Source Localization?,"Sooyoung Park, Arda Senocak, Joon Son Chung","Korea Advanced Institute of Science and Technology, South Korea",100.0,South Korea,0.0,,"Large-scale pre-trained image-text models demonstrate remarkable versatility across diverse tasks, benefiting from their robust representational capabilities and effective multimodal alignment. We extend the application of these models, specifically CLIP, to the domain of sound source localization. Unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP's text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Can_CLIP_Help_Sound_Source_Localization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Can_CLIP_Help_Sound_Source_Localization_WACV_2024_paper.pdf,,,2311.04066,main,Poster,https://ieeexplore.ieee.org/document/10484023/,"['Location awareness', 'Image segmentation', 'Computer vision', 'Grounding', 'Computational modeling', 'Self-supervised learning', 'Feature extraction']","['Sound Source', 'Sound Localization', 'Contrastive Language-Image Pre-training', 'Image Features', 'Input Text', 'Text Encoder', 'Spatial Features', 'Visual Features', 'Image Area', 'Image Regions', 'Large-scale Data', 'Paired Data', 'Bounding Box', 'Local Method', 'Representation Learning', 'Open Set', 'Localization Task', 'Substantial Gap', 'Self-supervised Learning', 'Contrastive Loss', 'Image Encoder', 'Strong Alignment', 'Self-supervised Manner', 'Sound Methods', 'Class Label Of Sample', 'Network Project', 'Standard Benchmark', 'Background Regions']","['Algorithms', 'Vision + language and/or other modalities']",2,"Large-scale pre-trained image-text models demonstrate remarkable versatility across diverse tasks, benefiting from their robust representational capabilities and effective multimodal alignment. We extend the application of these models, specifically CLIP, to the domain of sound source localization. Unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP’s text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin."
Can Vision-Language Models Be a Good Guesser? Exploring VLMs for Times and Location Reasoning,"Gengyuan Zhang, Yurui Zhang, Kerui Zhang, Volker Tresp","LMU Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany; Technical University of Munich; LMU Munich, Munich, Germany",75.0,Germany,25.0,Germany,"Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even surpass human capability in reasoning times and location. To address this question, we propose a two-stage Recognition & Reasoning probing task applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the studies, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In extensive evaluation experiments, we find that although VLMs can effectively retain times and location-relevant features in visual encoders, they still fail to make perfect reasoning with context-conditioned visual features. The dataset is available at https://github.com/gengyuanmax/WikiTiLo.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Can_Vision-Language_Models_Be_a_Good_Guesser_Exploring_VLMs_for_WACV_2024_paper.pdf,,https://github.com/gengyuanmax/WikiTiLo,2307.06166,main,Poster,https://ieeexplore.ieee.org/document/10484512/,"['Visualization', 'Computer vision', 'Computational modeling', 'Feature extraction', 'Cognition', 'Task analysis', 'Commonsense reasoning']","['Vision-language Models', 'Visual Cues', 'Visual Features', 'Probe Task', 'Visual Encoding', 'Commonsense Knowledge', 'Model Performance', 'Recognition Task', 'Question Answering', 'Model Discrimination', 'Language Model', 'Failure Cases', 'Linear Probe', 'Reasoning Ability', 'Flamingo', 'Recognition Stage', 'Visual Question Answering', 'Pre-trained ResNet-50', 'Cloze Test']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",1,"Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even surpass human capability in reasoning times and location. To address this question, we propose a two-stage Recognition & Reasoning probing task applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the studies, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In extensive evaluation experiments, we find that although VLMs can effectively retain times and location-relevant features in visual encoders, they still fail to make perfect reasoning with context-conditioned visual features. The dataset is available at https://github.com/gengyuanmax/WikiTiLo."
Can You Even Tell Left From Right? Presenting a New Challenge for VQA,"Sai Raam Venkataraman, Rishi Sridhar Rao, S. Balasubramanian, R. Raghunatha Sarma, Chandra Sekhar Vorugunti","Sri Sathya Sai Institute of Higher Learning, Prasanthi Nilayam, Andhra Pradesh; Samsung Research India",50.0,India,50.0,South Korea,"Visual Question Answering (VQA) needs a means of evaluating the strengths and weaknesses of models. One aspect of such an evaluation is the measurement of compositional generalisation. This relates to the ability of a model to answer well on scenes whose compositions are different from those of scenes in the training dataset. In this work, we present several quantitative measures of compositional separation and find that popular datasets for VQA are not good compositional evaluators. To solve this, we present Uncommon Objects in Unseen Configurations (UOUC), a synthetic dataset for VQA. UOUC is at once fairly complex while also being compositionally well-separated. The object-class of UOUC consists of 380 clasess taken from 528 characters from the Dungeons and Dragons game. The training dataset of UOUC consists of 200,000 scenes; whereas the test set consists of 30,000 scenes. In order to study compositional generalisation, simple reasoning and memorisation, each scene of UOUC is annotated with up to 10 novel questions. These deal with spatial relationships, hypothetical changes to scenes, counting, comparison, memorisation and memory-based reasoning. In total, UOUC presents over 2 million questions. Our evaluation of recent high-performing models for VQA shows that they exhibit poor compositional generalisation, and comparatively lower ability towards simple reasoning. These results suggest that UOUC could lead to advances in research by being a strong benchmark for VQA, especially in the study of compositional generalisation.",https://openaccess.thecvf.com/content/WACV2024/html/Venkataraman_Can_You_Even_Tell_Left_From_Right_Presenting_a_New_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Venkataraman_Can_You_Even_Tell_Left_From_Right_Presenting_a_New_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484035/,"['Training', 'Visualization', 'Computer vision', 'Games', 'Benchmark testing', 'Cognition', 'Question answering (information retrieval)']","['Training Dataset', 'Simple Reason', 'Visual Question Answering', 'Model Performance', 'Changes In Distribution', 'Test Dataset', 'Recent Models', 'Text Analysis', 'Bounding Box', 'Number Of Objects', 'Object Properties', 'Objects In The Scene', 'Objective Description', 'Object Pairs', 'COCO Dataset', 'Sanity Check']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Visual Question Answering (VQA) needs a means of evaluating the strengths and weaknesses of models. One aspect of such an evaluation is the measurement of compositional generalisation. This relates to the ability of a model to answer well on scenes whose compositions are different from those of scenes in the training dataset. In this work, we present several quantitative measures of compositional separation and find that popular datasets for VQA are not good compositional evaluators. To solve this, we present Uncommon Objects in Unseen Configurations (UOUC), a synthetic dataset for VQA. UOUC is at once fairly complex while also being compositionally well-separated. The object-class of UOUC consists of 380 clasess taken from 528 characters from the Dungeons and Dragons game. The training dataset of UOUC consists of 200,000 scenes; whereas the test set consists of 30,000 scenes. In order to study compositional generalisation, simple reasoning and memorisation, each scene of UOUC is annotated with up to 10 novel questions. These deal with spatial relationships, hypothetical changes to scenes, counting, comparison, memorisation and memory-based reasoning. In total, UOUC presents over 2 million questions. Our evaluation of recent state-of-the-art models for VQA shows that they exhibit poor compositional generalisation, and comparatively lower ability towards simple reasoning. These results suggest that UOUC could lead to advances in research by being a strong benchmark for VQA, especially in the study of compositional generalisation."
Causal Analysis for Robust Interpretability of Neural Networks,"Ola Ahmad, Nicolas Béreux, Loïc Baret, Vahid Hashemi, Freddy Lecue","Paris-Saclay University, Paris, France; AUDI AG, Ingolstadt, Germany; Thales Digital Solutions, CortAIx, Montreal, Canada; Inria, Sophia Antipolis, France",50.0,France,50.0,Germany,"Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model's behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specific causal explanatory graphs that can audit model behavior and express the actual causes underlying its performance. We apply our method to vision models trained on classification tasks. On image classification tasks, we provide extensive quantitative experiments to show that our approach can capture more stable and faithful explanations than standard attribution-based methods. Furthermore, the underlying causal graphs express the neural interactions in the model, making it a valuable tool in other applications (e.g., model repair).",https://openaccess.thecvf.com/content/WACV2024/html/Ahmad_Causal_Analysis_for_Robust_Interpretability_of_Neural_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ahmad_Causal_Analysis_for_Robust_Interpretability_of_Neural_Networks_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484102/,"['Training', 'Phase measurement', 'Computational modeling', 'Neural networks', 'Noise', 'Predictive models', 'Maintenance engineering']","['Neural Network', 'Causal Analysis', 'Causal Effect', 'Hidden Layer', 'Visual Model', 'Pre-trained Neural Network', 'Input Bias', 'Causal Graph', 'Effective Interventions', 'Convolutional Neural Network', 'Deep Neural Network', 'Validation Set', 'Convolutional Layers', 'Causal Inference', 'Social Dimensions', 'Neurons In Layer', 'Root Node', 'Complex Architecture', 'Language Model', 'Exploratory Methods', 'Attribution Methods', 'Nodes In Layer', 'Gradient-based Methods', 'Critical Nodes', 'ImageNet Data', 'Path Selection', 'Red Ones', 'Causal Approach', 'Causal Explanations', 'Node Connectivity']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Interpreting the inner function of neural networks is crucial for the trustworthy development and deployment of these black-box models. Prior interpretability methods focus on correlation-based measures to attribute model decisions to individual examples. However, these measures are susceptible to noise and spurious correlations encoded in the model during the training phase (e.g., biased inputs, model overfitting, or misspecification). Moreover, this process has proven to result in noisy and unstable attributions that prevent any transparent understanding of the model’s behavior. In this paper, we develop a robust interventional-based method grounded by causal analysis to capture cause-effect mechanisms in pre-trained neural networks and their relation to the prediction. Our novel approach relies on path interventions to infer the causal mechanisms within hidden layers and isolate relevant and necessary information (to model prediction), avoiding noisy ones. The result is task-specific causal explanatory graphs that can audit model behavior and express the actual causes underlying its performance. We apply our method to vision models trained on classification tasks. On image classification tasks, we provide extensive quantitative experiments to show that our approach can capture more stable and faithful explanations than standard attribution-based methods. Furthermore, the underlying causal graphs express the neural interactions in the model, making it a valuable tool in other applications (e.g., model repair)."
Causal Feature Alignment: Learning To Ignore Spurious Background Features,"Rahul Venkataramani, Parag Dutta, Vikram Melapudi, Ambedkar Dukkipati","GE HealthCare, Bangalore; Indian Institute of Science, Bangalore",50.0,India,50.0,India,"Deep neural networks are susceptible to spurious features strongly correlating with the target. This phenomenon leads to sub-optimal performance during real-world deployment where the spurious correlations do not exist, leading to deployment challenges in safety-critical environments like healthcare, autonomous navigation etc. While spurious features can correlate with causal features in myriad ways, we propose a solution for a common manifestation in computer vision where the background corresponds to a spurious feature. In contrast to previous works, we do not require apriori knowledge of different sub-groups in the data induced by the presence/absence of spurious features and the corresponding access to samples from these sub-groups. Our proposed method, Causal Feature Alignment (CFA), utilizes segmentation of foreground (a proxy for the causal component) on a small subset of training examples to align the representations of the original images to match words from only causal elements. We first demonstrate the validity of the proposed method on semi-synthetic data. Subsequently, we obtain state-of-the-art results on worst-group accuracy (93%) on the benchmark dataset of Waterbirds using CFA. Furthermore, we demonstrate significant gains of 6% on the Backgrounds Challenge. Finally, we show that utilizing the recently released foundational methods can alleviate the requirement of dense segmentation and can be substituted with weaker modes of human input like bounding boxes, clicks etc., without any performance loss compared to the original CFA.",https://openaccess.thecvf.com/content/WACV2024/html/Venkataramani_Causal_Feature_Alignment_Learning_To_Ignore_Spurious_Background_Features_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Venkataramani_Causal_Feature_Alignment_Learning_To_Ignore_Spurious_Background_Features_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483772/,"['Location awareness', 'Computer vision', 'Image segmentation', 'Correlation', 'Annotations', 'Image color analysis', 'Reviews']","['Causal Features', 'Spurious Features', 'Neural Network', 'Training Data', 'Deep Neural Network', 'Subset Of Data', 'Joint Effect', 'ImageNet', 'Cognitive Group', 'Waterbirds', 'Foundation Model', 'Subset Of The Training Data', 'Small Subset Of Data', 'Wide Range Of Datasets', 'Fine-tuned', 'Training Dataset', 'Ethnic Minority', 'Validation Set', 'Entire Dataset', 'Deep Learning Models', 'Empirical Risk Minimization', 'Group Labels', 'Saliency Map', 'Challenging Dataset', 'Bounding Box', 'Underrepresented Groups', 'Two-stage Algorithm', 'Chest Tube', 'Hyperparameter Tuning', 'Validation Dataset']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Deep neural networks are susceptible to spurious features strongly correlating with the target. This phenomenon leads to sub-optimal performance during real-world deployment where spurious correlations do not exist, leading to deployment challenges in safety-critical environments like health-care. While spurious features can correlate with causal features in myriad ways, we propose a solution for a common manifestation in computer vision where the background corresponds to a spurious feature. In contrast to previous works, we do not require apriori knowledge of different groups in the data induced by the presence/absence of spurious features and corresponding access to samples. We propose a method, Causal Feature Alignment (CFA), to ignore the spurious background features by utilizing segmentations on a small subset of training data. To reduce the annotation burden, we reduce the pixel-wise annotation task of segmentation to a review task of selecting the best mask by utilizing the recently released foundation model and a feature attribution method. We demonstrate our method on a wide range of datasets, including the semi-synthetic ColoredMNIST, WaterBirds, and ImageNet Backgrounds Challenge, and obtain significant gains over state-of-the-art methods."
Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation,"Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj","Faculty of Computer and Information Science, University of Ljubljana",100.0,Slovenia,0.0,,"RGB-based surface anomaly detection methods have advanced significantly. However, certain surface anomalies remain practically invisible in RGB alone, necessitating the incorporation of 3D information. Existing approaches that employ point-cloud backbones suffer from suboptimal representations and reduced applicability due to slow processing. Re-training RGB backbones, designed for faster dense input processing, on industrial depth datasets is hindered by the limited availability of sufficiently large datasets. We make several contributions to address these challenges. (i) We propose a novel Depth-Aware Discrete Autoencoder (DADA) architecture, that enables learning a general discrete latent space that jointly models RGB and 3D data for 3D surface anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets by introducing a simulation process for learning informative depth features in the depth encoder. (iii) We propose a new surface anomaly detection method 3DSR, which outperforms all existing state-of-the-art on the challenging MVTec3D anomaly detection benchmark, both in terms of accuracy and processing speed. The experimental results validate the effectiveness and efficiency of our approach, highlighting the potential of utilizing depth information for improved surface anomaly detection.",https://openaccess.thecvf.com/content/WACV2024/html/Zavrtanik_Cheating_Depth_Enhancing_3D_Surface_Anomaly_Detection_via_Depth_Simulation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zavrtanik_Cheating_Depth_Enhancing_3D_Surface_Anomaly_Detection_via_Depth_Simulation_WACV_2024_paper.pdf,,https://github.com/VitjanZ/3DSR,,main,Poster,https://ieeexplore.ieee.org/document/10483670/,"['Training', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Information retrieval', 'Feature extraction']","['Anomaly Detection', 'Surface Anomalies', 'Point Cloud', 'Simulation Process', '3D Data', 'Depth Information', '3D Information', 'Depth Features', 'Autoencoder Architecture', 'RGB Data', 'Anomaly Detection Methods', 'Discretion', 'Simulated Data', 'Percentage Points', 'Feature Space', 'Subtle Changes', 'Grayscale Images', 'Object Classification', 'RGB Images', 'Depth Images', 'Depth Data', 'Problem Setup', 'Industrial Data', 'RGB Features', 'Normal Appearance', 'Efficient Design', 'Simulated Images', 'Vector Quantization', 'Simulated Depth', 'Changes In Depth']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"RGB-based surface anomaly detection methods have advanced significantly. However, certain surface anomalies remain practically invisible in RGB alone, necessitating the incorporation of 3D information. Existing approaches that employ point-cloud backbones suffer from suboptimal representations and reduced applicability due to slow processing. Re-training RGB backbones, designed for faster dense input processing, on industrial depth datasets is hindered by the limited availability of sufficiently large datasets. We make several contributions to address these challenges. (i) We propose a novel Depth-Aware Discrete Autoencoder (DADA) architecture, that enables learning a general discrete latent space that jointly models RGB and 3D data for 3D surface anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets by introducing a simulation process for learning informative depth features in the depth encoder. (iii) We propose a new surface anomaly detection method 3DSR, which outperforms all existing state-of-theart on the challenging MVTec3D anomaly detection benchmark, both in terms of accuracy and processing speed. The experimental results validate the effectiveness and efficiency of our approach, highlighting the potential of utilizing depth information for improved surface anomaly detection. Code is available at: https://github.com/VitjanZ/3DSR"
Classifying Cable Tendency With Semantic Segmentation by Utilizing Real and Simulated RGB Data,"Pei-Chun Chien, Powei Liao, Eiji Fukuzawa, Jun Ohya",Waseda University; Yazaki Corporation,50.0,Japan,50.0,Japan,"Cable tendency is the potential shape or characteristic that a cable may possess while being manipulated, of which some are considered erroneous and should be identified as a part of anomaly detection during an automatic manipulation. This research explores the ability of deep-learning models in learning the cable tendencies that, contrary to typical classification tasks of multi-object scenarios, is to differentiate the multiple states displayable by the same object -- in this case, cables. By training multiple models with different combinations of self-collected real-world data and self-generated simulation data, a comparative study is carried out to compare the performance of each approach. In conclusion, the effectiveness of detecting three abnormal states and shapes of cables, and using simulation data is certificated in experiments.",https://openaccess.thecvf.com/content/WACV2024/html/Chien_Classifying_Cable_Tendency_With_Semantic_Segmentation_by_Utilizing_Real_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chien_Classifying_Cable_Tendency_With_Semantic_Segmentation_by_Utilizing_Real_and_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483770/,"['Training', 'Computer vision', 'Shape', 'Semantic segmentation', 'Transfer learning', 'Data models', 'Communication cables']","['Simulated Data', 'Semantic Segmentation', 'Combined Data', 'Ability Of The Model', 'Deep Learning Models', 'Real-world Data', 'Anomaly Detection', 'Train Multiple Models', 'Training Set', 'Image Classification', 'Transfer Learning', 'Types Of Errors', 'Simulation Environment', 'Domain Adaptation', 'Self-driving', 'Simulated Images', '3D Software', 'Simulation Domain', 'Physical Simulation', 'Scene Understanding', 'Reality Gap', 'RGB Information', 'Abnormal Cases', 'Lower Learning Rate']","['Applications', 'Robotics', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Image recognition and understanding']",,"Cable tendency is the potential shape or characteristic that a cable may possess while being manipulated, of which some are considered erroneous and should be identified as a part of anomaly detection during an automatic manipulation. This research explores the ability of deep-learning models in learning the cable tendencies that, contrary to typical classification tasks of multi-object scenarios, is to differentiate the multiple states displayable by the same object – in this case, cables. By training multiple models with different combinations of self-collected real-world data and self-generated simulation data, a comparative study is carried out to compare the performance of each approach. In conclusion, the effectiveness of detecting three abnormal states and shapes of cables, and using simulation data is certificated in experiments."
ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition,"Debaditya Roy, Dhruv Verma, Basura Fernando","Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore; Institute of High-Performance Computing, Agency for Science, Technology and Research, Singapore",50.0,Singapore,50.0,Singapore,"Situation Recognition is the task of generating a structured summary of what is happening in an image using an activity verb and the semantic roles played by actors and objects. In this task, the same activity verb can describe a diverse set of situations as well as the same actor or object category can play a diverse set of semantic roles depending on the situation depicted in the image. Hence a situation recognition model needs to understand the context of the image and the visual-linguistic meaning of semantic roles. Therefore, we leverage the CLIP foundational model that has learned the context of images via language descriptions. We show that deeper-and-wider multi-layer perceptron (MLP) blocks obtain noteworthy results for the situation recognition task by using CLIP image and text embedding features and it even outperforms the state-of-the-art CoFormer, a Transformer-based model, thanks to the external implicit visual-linguistic knowledge encapsulated by CLIP and the expressive power of modern MLP block designs. Motivated by this, we design a cross-attention-based Transformer using CLIP visual tokens that model the relation between textual roles and visual entities. Our cross-attention-based Transformer known as ClipSitu XTF outperforms existing state-of-the-art by a large margin of 14.1% on semantic role labelling (value) for top-1 accuracy using imSitu dataset. Similarly, our ClipSitu XTF obtains state-of-the-art situation localization performance. We will make the code publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Roy_ClipSitu_Effectively_Leveraging_CLIP_for_Conditional_Predictions_in_Situation_Recognition_WACV_2024_paper.pdf,,https://github.com/LUNAProject22/CLIPSitustanding,2307.00586,main,Poster,,,,,,
ClusterFix: A Cluster-Based Debiasing Approach Without Protected-Group Supervision,"Giacomo Capitani, Federico Bolelli, Angelo Porrello, Simone Calderara, Elisa Ficarra","Università degli Studi di Modena e Reggio Emilia, Italy",100.0,Italy,0.0,,"The failures of Deep Networks can sometimes be ascribed to biases in the data or algorithmic choices. Existing debiasing approaches exploit prior knowledge to avoid unintended solutions; we acknowledge that, in real-world settings, it could be unfeasible to gather enough prior information to characterize the bias, or it could even raise ethical considerations. We hence propose a novel debiasing approach, termed ClusterFix, which does not require any external hint about the nature of biases. Such an approach alters the standard empirical risk minimization and introduces a per-example weight, encoding how critical and far from the majority an example is. Notably, the weights consider how difficult it is for the model to infer the correct pseudo-label, which is obtained in a self-supervised manner by dividing examples into multiple clusters. Extensive experiments show that the misclassification error incurred in identifying the correct cluster allows for identifying examples prone to bias-related issues. As a result, our approach outperforms existing methods on standard benchmarks for bias removal and fairness.",https://openaccess.thecvf.com/content/WACV2024/html/Capitani_ClusterFix_A_Cluster-Based_Debiasing_Approach_Without_Protected-Group_Supervision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Capitani_ClusterFix_A_Cluster-Based_Debiasing_Approach_Without_Protected-Group_Supervision_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484188/,"['Ethics', 'Computer vision', 'Risk minimization', 'Clustering algorithms', 'Benchmark testing', 'Encoding', 'Object recognition']","['Debiasing Approach', 'Deep Network', 'Standard Benchmark', 'Choice Of Algorithm', 'Empirical Risk Minimization', 'Bias Removal', 'Training Set', 'Objective Function', 'Ethnic Minority', 'Deep Neural Network', 'Feature Space', 'Average Accuracy', 'Worst-case Scenario', 'Latent Space', 'Target Object', 'Learning Phase', 'Cluster Assignment', 'Classification Loss', 'Domain Adaptation', 'Training Objective', 'Worst-case Performance', 'Protective Group', 'Waterbirds', 'Inductive Bias', 'Group Labels', 'Classical Clustering', 'ImageNet', 'Target Model']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",,"The failures of Deep Networks can sometimes be ascribed to biases in the data or algorithmic choices. Existing debiasing approaches exploit prior knowledge to avoid unintended solutions; we acknowledge that, in real-world settings, it could be unfeasible to gather enough prior information to characterize the bias, or it could even raise ethical considerations. We hence propose a novel debiasing approach, termed ClusterFix, which does not require any external hint about the nature of biases. Such an approach alters the standard empirical risk minimization and introduces a per-example weight, encoding how critical and far from the majority an example is. Notably, the weights consider how difficult it is for the model to infer the correct pseudo-label, which is obtained in a self-supervised manner by dividing examples into multiple clusters. Extensive experiments show that the misclassification error incurred in identifying the correct cluster allows for identifying examples prone to bias-related issues. As a result, our approach outperforms existing methods on standard benchmarks for bias removal and fairness."
Co-Speech Gesture Detection Through Multi-Phase Sequence Labeling,"Esam Ghaleb, Ilya Burenko, Marlou Rasenberg, Wim Pouw, Peter Uhrig, Judith Holler, Ivan Toni, Aslı Özyürek, Raquel Fernández","ScaDS.AI Dresden/Leipzig, TU Dresden; University of Amsterdam; Meertens Institute, Radboud University; Radboud University; Radboud University, Max Planck Institute for Psycholinguistics",100.0,"Germany, Netherlands",0.0,,"Gestures are integral components of face-to-face communication. They unfold over time, often following predictable movement phases of preparation, stroke, and retraction. Yet, the prevalent approach to automatic gesture detection treats the problem as binary classification, classifying a segment as either containing a gesture or not, thus failing to capture its inherently sequential and contextual nature. To address this, we introduce a novel framework that reframes the task as a multi-phase sequence labeling problem rather than binary classification. Our model processes sequences of skeletal movements over time windows, uses Transformer encoders to learn contextual embeddings, and leverages Conditional Random Fields to perform sequence labeling. We evaluate our proposal on a large dataset of diverse co-speech gestures in task-oriented face-to-face dialogues. The results consistently demonstrate that our method significantly outperforms strong baseline models in detecting gesture strokes. Furthermore, applying Transformer encoders to learn contextual embeddings from movement sequences substantially improves gesture unit detection. These results highlight our framework's capacity to capture the fine-grained dynamics of co-speech gesture phases, paving the way for more nuanced and accurate gesture detection and analysis.",https://openaccess.thecvf.com/content/WACV2024/html/Ghaleb_Co-Speech_Gesture_Detection_Through_Multi-Phase_Sequence_Labeling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ghaleb_Co-Speech_Gesture_Detection_Through_Multi-Phase_Sequence_Labeling_WACV_2024_paper.pdf,,,2308.10680,main,Poster,https://ieeexplore.ieee.org/document/10483651/,"['Computer vision', 'Focusing', 'Predictive models', 'Transformers', 'Conditional random fields', 'Labeling', 'Proposals']","['Sequence Labeling', 'Co-speech Gestures', 'Gesture Detection', 'Time Window', 'Binary Classification', 'Face-to-face Interactions', 'Native Sequence', 'Preparation Phase', 'Conditional Random Field', 'Strong Baseline', 'Transformer Encoder', 'Way For Detection', 'Contextual Embedding', 'Neural Network', 'F1 Score', 'Intersection Over Union', 'Sequential Model', 'Latent Space', 'Depth Data', 'Pose Estimation', 'Phase Of Stroke', 'Gesture Recognition', 'Multiphase Model', 'Intersection Over Union Score', 'Graph Convolutional Network', 'Sign Language Recognition', 'RGB Data', 'Video Segments', 'Body Joints', 'Rest Position']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Datasets and evaluations']",1,"Gestures are integral components of face-to-face communication. They unfold over time, often following predictable movement phases of preparation, stroke, and retraction. Yet, the prevalent approach to automatic gesture detection treats the problem as binary classification, classifying a segment as either containing a gesture or not, thus failing to capture its inherently sequential and contextual nature. To address this, we introduce a novel framework that reframes the task as a multi-phase sequence labeling problem rather than binary classification. Our model processes sequences of skeletal movements over time windows, uses Transformer encoders to learn contextual embeddings, and leverages Conditional Random Fields to perform sequence labeling. We evaluate our proposal on a large dataset of diverse co-speech gestures in task-oriented face-to-face dialogues. The results consistently demonstrate that our method significantly outperforms strong baseline models in detecting gesture strokes. Furthermore, applying Transformer encoders to learn contextual embeddings from movement sequences substantially improves gesture unit detection. These results highlight our framework’s capacity to capture the fine-grained dynamics of co-speech gesture phases, paving the way for more nuanced and accurate gesture detection and analysis."
CoD: Coherent Detection of Entities From Images With Multiple Modalities,"Vinay Verma, Dween Sanny, Abhishek Singh, Deepak Gupta",International Machine Learning (IML) Amazon India,100.0,India,0.0,,"However, in real-world scenarios, multiple sources of data in different modalities are often present, making it difficult to accurately define object boundaries for various products or information. For instance, while extracting information from a document, it may be necessary to utilize both visual information (e.g., image/object) and textual information from OCR to detect and classify information associated with objects, such as text blocks, tables, and figures. If visual and textual information pertain to the same object, the model should detect the bounding box around all multi-modal information. The problem of object detection in computer vision has traditionally been viewed as a unimodal problem in the literature, which poses a significant challenge. This work presents a novel approach to automating object boundary identification in multi-modal scenarios. The study proposes an end-to-end method that employs transformers for detecting object boundaries in a multi-modal environment. The proposed model takes multi-scale image features, OCR-based text extraction, and 2D position embedding of words as input, which interact through self- and cross-attention mechanisms. Additionally, the study proposes a domain adaptation model to address the often significant domain gap between training and test samples in such scenarios. The proposed approach shows a significant improvement of 27.2%, 5.0% and 1.7% using hard negative samples, multi-modal and domain shift scenarios, respectively. The ablation studies confirm the effectiveness of the proposed components.",https://openaccess.thecvf.com/content/WACV2024/html/Verma_CoD_Coherent_Detection_of_Entities_From_Images_With_Multiple_Modalities_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Verma_CoD_Coherent_Detection_of_Entities_From_Images_With_Multiple_Modalities_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484189/,"['Training', 'Adaptation models', 'Visualization', 'Computer vision', 'Annotations', 'Computational modeling', 'Soft sensors']","['Named Entity Recognition', 'Image Features', 'Negative Samples', 'Object Detection', 'Unimodal', 'Bounding Box', 'Domain Shift', 'Online Shopping', 'Textual Descriptions', 'Multi-scale Features', 'Domain Adaptation', 'Optical Character Recognition', 'Computer Vision Problems', 'Multimodal Dataset', 'Multimodal Input', 'Single Box', 'Convolutional Neural Network', 'Feature Maps', 'Intersection Over Union', 'Minimal Loss', 'Multimodal Information', 'Encoder Layer', 'Decoder Layer', 'Domain Discriminator', 'Source Domain', 'Multimodal Detection', 'Multimodal Model', 'Transformer Architecture', 'Ground-truth Bounding Box', 'Gradient Reversal']","['Applications', 'Commercial / retail', 'Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Robotics']",,"Object detection is a fundamental problem in computer vision, whose research has primarily focused on unimodal models, solely operating on visual data. However, in many real-world applications, data from multiple modalities may be available, such as text accompanying the visual data. Leveraging traditional models on these multi-modal data sources may lead to difficulties in accurately delineating object boundaries. For example, in a document containing a combination of text and images, the model must encompass the images and texts pertaining to the same object in a single bounding box. To address this, we propose a model that takes in multi-scale image features, text extracted through OCR, and 2D positional embeddings of words as inputs, and returns bounding boxes that incorporate the image and associated description as single entities. Furthermore, to address the challenge posed by the irregular arrangement of images and their corresponding textual descriptions, we propose the concept of a “Negative Product Bounding Box” (PBB). This box encapsulates instances where the model faces confusion and tends to predict incorrect bounding boxes. To enhance the model’s performance, we incorporate these negative boxes into the loss function governing matching and classification. Additionally, a domain adaptation model is proposed to handle scenarios involving a domain gap between training and test samples. In order to assess the effectiveness of our model, we construct a multimodal dataset comprising product descriptions from online retailers’ catalogs. On this dataset, our proposed model demonstrates significant improvements of 27.2%, 4.3%, and 1.7% in handling hard negative samples, multi-modal input, and domain shift scenarios, respectively."
Collage Diffusion,"Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Ré, Kayvon Fatahalian",Stanford University,100.0,USA,0.0,,"We seek to give users precise control over diffusion-based image generation by modeling complex scenes as sequences of layers, which define the desired spatial arrangement and visual attributes of objects in the scene. Collage Diffusion harmonizes the input layers to make objects fit together---the key challenge involves minimizing changes in the positions and key visual attributes of the input layers while allowing other attributes to change in the harmonization process. We ensure that objects are generated in the correct locations by modifying text-image cross-attention with the layers' alpha masks. We preserve key visual attributes of input layers by learning specialized text representations per layer and by extending prior diffusion-based control mechanisms to operate on layers. Layer input allows users to control the extent of image harmonization on a per-object basis, and users can even iteratively edit individual objects in generated images while keeping other objects fixed. By leveraging the rich information present in layer input, Collage Diffusion generates globally harmonized images that maintain desired object characteristics better than prior approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Sarukkai_Collage_Diffusion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sarukkai_Collage_Diffusion_WACV_2024_paper.pdf,,,2303.00262,main,Poster,https://ieeexplore.ieee.org/document/10484308/,"['Visualization', 'Computer vision', 'Image synthesis', 'Process control']","['Precise Control', 'Input Layer', 'Image Generation', 'Correct Location', 'Individual Objects', 'Objects In The Scene', 'Sequence Of Layers', 'Harmonization Process', 'Input Image', 'Single Image', 'Diffusion Process', 'Diffusion Model', 'Image Noise', 'Secondary Loss', 'Output Image', 'Individual Layers', 'Segmentation Map', 'Binary String', 'Pixel Coordinates', 'Usage Intention', 'Image Editing', 'Spatial Composition', 'Object Appearance', 'Benefits Of Control', 'Video Editing', 'User Control', 'Global Coherence']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",6,"We seek to give users precise control over diffusion-based image generation by modeling complex scenes as sequences of layers, which define the desired spatial arrangement and visual attributes of objects in the scene. Collage Diffusion harmonizes the input layers to make objects fit together—the key challenge involves minimizing changes in the positions and key visual attributes of the input layers while allowing other attributes to change in the harmonization process. We ensure that objects are generated in the correct locations by modifying text-image cross-attention with the layers’ alpha masks. We preserve key visual attributes of input layers by learning specialized text representations per layer and by extending prior diffusion-based control mechanisms to operate on layers. Layer input allows users to control the extent of image harmonization on a per-object basis, and users can even iteratively edit individual objects in generated images while keeping other objects fixed. By leveraging the rich information present in layer input, Collage Diffusion generates globally harmonized images that maintain desired object characteristics better than prior approaches."
Common Diffusion Noise Schedules and Sample Steps Are Flawed,"Shanchuan Lin, Bingchen Liu, Jiashi Li, Xiao Yang",ByteDance Inc.,0.0,,100.0,China,"We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the model to only generate images with medium brightness and prevents it from generating very bright and dark samples. We propose a few simple fixes: (1) rescale the noise schedule to enforce zero terminal SNR; (2) train the model with v prediction; (3) change the sampler to always start from the last timestep; (4) rescale classifier-free guidance to prevent over-exposure. These simple changes ensure the diffusion process is congruent between training and inference and allow the model to generate samples more faithful to the original data distribution.",https://openaccess.thecvf.com/content/WACV2024/html/Lin_Common_Diffusion_Noise_Schedules_and_Sample_Steps_Are_Flawed_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lin_Common_Diffusion_Noise_Schedules_and_Sample_Steps_Are_Flawed_WACV_2024_paper.pdf,,,2305.08891,main,Poster,https://ieeexplore.ieee.org/document/10484327/,"['Training', 'Schedules', 'Computer vision', 'Gaussian noise', 'Computational modeling', 'Brightness', 'Diffusion processes']","['Sampling Step', 'Common Noise', 'Signal-to-noise', 'Gaussian Noise', 'Simple Change', 'Design Flaws', 'Pure Noise', 'Diffusion Model', 'White Background', 'Input Noise', 'Forward Process', 'Mean Channel', 'Fréchet Inception Distance']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",36,"We discover that common diffusion noise schedules do not enforce the last timestep to have zero signal-to-noise ratio (SNR), and some implementations of diffusion samplers do not start from the last timestep. Such designs are flawed and do not reflect the fact that the model is given pure Gaussian noise at inference, creating a discrepancy between training and inference. We show that the flawed design causes real problems in existing implementations. In Stable Diffusion, it severely limits the model to only generate images with medium brightness and prevents it from generating very bright and dark samples. We propose a few simple fixes: (1) rescale the noise schedule to enforce zero terminal SNR; (2) train the model with v prediction; (3) change the sampler to always start from the last timestep; (4) rescale classifier-free guidance to prevent over-exposure. These simple changes ensure the diffusion process is congruent between training and inference and allow the model to generate samples more faithful to the original data distribution."
Complementary-Contradictory Feature Regularization Against Multimodal Overfitting,Antonio Tejero-de-Pablos,"CyberAgent, Shibuya, Tokyo, Japan",100.0,Japan,0.0,,"Understanding multimodal learning is essential to design intelligent systems that can effectively combine various data types (visual, audio, etc.). Multimodal learning is not trivial, as adding new modalities does not always result in a significant improvement in performance, i.e., multimodal overfitting. To tackle this, several works proposed regularizing each modality's learning speed and feature distribution. However, in these methods, characterizing quantitatively and qualitatively multimodal overfitting is not intuitive. We hypothesize that, rather than regularizing abstract hyperparameters, regularizing the features learned is a more straightforward methodology against multimodal overfitting. For the given input modalities and task, we constrain ""complementary"" (useful) and ""contradictory"" (obstacle) features via a masking operation on the multimodal latent space. In addition, we leverage latent discretization so the size of the complementary-contradictory spaces becomes learnable, allowing the estimation of a modal complementarity measure. Our method successfully improves the performance of datasets with modality overfitting in different tasks, providing insight into ""what"" and ""how much"" is learned from each modality. Furthermore, it facilitates transfer learning to new datasets. Our code and a detailed manual are available at https://github.com/CyberAgentAILab/CM-VQVAE.",https://openaccess.thecvf.com/content/WACV2024/html/Tejero-de-Pablos_Complementary-Contradictory_Feature_Regularization_Against_Multimodal_Overfitting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tejero-de-Pablos_Complementary-Contradictory_Feature_Regularization_Against_Multimodal_Overfitting_WACV_2024_paper.pdf,,https://github.com/CyberAgentAILab/CM-VQVAE,,main,Poster,https://ieeexplore.ieee.org/document/10484420/,"['Visualization', 'Computer vision', 'Codes', 'Transfer learning', 'Semantics', 'Estimation', 'Manuals']","['Hyperparameters', 'Transfer Learning', 'Latent Space', 'Size Of Space', 'Multimodal Learning', 'Semantic', 'Feature Space', 'Skin Color', 'Emotion Recognition', 'Regularization Term', 'Model Discrimination', 'Combination Of Modalities', 'Part Features', 'Separate Features', 'Amount Of Features', 'Complementary Features', 'Basic Shape', 'Multimodal Features', 'Multimodal Model', 'Multimodal Dataset', 'Multimodal Tasks', 'Contradictory Features', 'Modal Features', 'Pose Information', 'Feature Compression', 'Attention Weights', 'Classification Task']","['Algorithms', 'Vision + language and/or other modalities']",,"Understanding multimodal learning is essential to design intelligent systems that can effectively combine various data types (visual, audio, etc.). Multimodal learning is not trivial, as adding new modalities does not always result in a significant improvement in performance, i.e., multimodal overfitting. To tackle this, several works proposed regularizing each modality’s learning speed and feature distribution. However, in these methods, characterizing quantitatively and qualitatively multimodal overfitting is not intuitive. We hypothesize that, rather than regularizing abstract hyperparameters, regularizing the features learned is a more straightforward methodology against multimodal overfitting. For the given input modalities and task, we constrain ""complementary"" (useful) and ""contradictory"" (obstacle) features via a masking operation on the multimodal latent space. In addition, we leverage latent discretization so the size of the complementary-contradictory spaces becomes learnable, allowing the estimation of a modal complementarity measure. Our method successfully improves the performance of datasets with modality overfitting in different tasks, providing insight into ""what""and ""how much"" is learned from each modality. Furthermore, it facilitates transfer learning to new datasets. Our code and a detailed manual are available at https://github.com/CyberAgentAILab/CM-VQVAE."
Complex Organ Mask Guided Radiology Report Generation,"Tiancheng Gu, Dongnan Liu, Zhiyuan Li, Weidong Cai","University of Sydney, Sydney, Australia",100.0,Australia,0.0,,"The goal of automatic report generation is to generate a clinically accurate and coherent phrase from a single given X-ray image, which could alleviate the workload of traditional radiology reporting.However, in a real-world scenario, radiologists frequently face the challenge of producing extensive reports derived from numerous medical images, thereby medical report generation from multi-image perspective is needed.In this paper, we propose the Complex Organ Mask Guided (termed as COMG) report generation model, which incorporates masks from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide more detailed information and guide the model's attention to these crucial body regions. Specifically, we leverage prior knowledge of the disease corresponding to each organ in the fusion process to enhance the disease identification phase during the report generation process. Additionally, cosine similarity loss is introduced as target function to ensure the convergence of cross-modal consistency and facilitate model optimization.Experimental results on two public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.",https://openaccess.thecvf.com/content/WACV2024/html/Gu_Complex_Organ_Mask_Guided_Radiology_Report_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gu_Complex_Organ_Mask_Guided_Radiology_Report_Generation_WACV_2024_paper.pdf,,https://github.com/GaryGuTC/COMG_model,2311.02329,main,Poster,https://ieeexplore.ieee.org/document/10484342/,"['Representation learning', 'Semantics', 'MIMICs', 'Lung', 'Radiology', 'Streaming media', 'X-ray imaging']","['Radiology Reports', 'Medical Imaging', 'Similar Loss', 'Semantic', 'Convolutional Neural Network', 'Image Features', 'Input Image', 'Long Short-term Memory', 'First Pass', 'Transformer Model', 'External Information', 'Similar Learning', 'Image Captioning', 'Public Benchmark', 'Text Generation', 'Chest X-ray Images']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Vision + language and/or other modalities']",6,"The goal of automatic report generation is to generate a clinically accurate and coherent phrase from a single given X-ray image, which could alleviate the workload of traditional radiology reporting. However, in a real-world scenario, radiologists frequently face the challenge of producing extensive reports derived from numerous medical images, thereby medical report generation from multi-image perspective is needed. In this paper, we propose the Complex Organ Mask Guided (termed as COMG) report generation model, which incorporates masks from multiple organs (e.g., bones, lungs, heart, and mediastinum), to pro-vide more detailed information and guide the model’s attention to these crucial body regions. Specifically, we leverage prior knowledge of the disease corresponding to each organ in the fusion process to enhance the disease identification phase during the report generation process. Additionally, cosine similarity loss is introduced as target function to ensure the convergence of cross-modal consistency and facilitate model optimization. Experimental results on two public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively. The code is publicly available at https://github.com/GaryGuTC/COMG_model."
Composite Diffusion: whole >= Sparts,"Vikram Jamwal, Ramaneswaran S.","TCS Research, India; NVIDIA, India",0.0,,100.0,India,"For artists or graphic designers, the spatial arrangement of a scene is a critical design choice. However, existing text-to-image diffusion models provide limited support for incorporating spatial information. This paper introduces Composite Diffusion as a means for artists to generate high-quality images by composing from sub-scenes. The artists can specify the arrangement of the sub-scenes through a free-form segment layout and can describe the content of each sub-scene using natural text and additional control inputs. We provide a comprehensive and modular framework for Composite Diffusion that enables alternative ways of generating, composing, and harmonizing sub-scenes. We further argue that existing image quality metrics lack a holistic evaluation of image composites. To address this, we propose novel quality criteria especially relevant to composite generation. We believe that our approach provides an intuitive method of art creation. Through extensive user surveys and quantitative and qualitative analysis, we show how it achieves greater spatial, semantic, and creative control over image generation. In addition, our methods do not need to retrain or modify the architecture of the base diffusion models and can work in a plug-and-play manner with the fine-tuned models.",https://openaccess.thecvf.com/content/WACV2024/html/Jamwal_Composite_Diffusion_whole__Sparts_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jamwal_Composite_Diffusion_whole__Sparts_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Computer Vision on the Edge: Individual Cattle Identification in Real-Time With ReadMyCow System,"Moniek Smink, Haotian Liu, Dörte Döpfer, Yong Jae Lee","School of Veterinary Medicine, University of Wisconsin - Madison, United States; Department of Computer Sciences, University of Wisconsin - Madison, United States",100.0,USA,0.0,,"In precision livestock farming, the individual identification of cattle is crucial to inform the decisions made to enhance animal welfare, health, and productivity. In literature, models exist that can read ear tags; however, they are not easily portable to real-world cattle production environments and make predictions mainly on still images. We propose a video-based cattle ear tag reading system, called ReadMyCow, which takes advantage of the temporal characteristics in videos to accurately detect, track, and read cattle ear tags at 25 FPS on edge devices. For each frame in a video, ReadMyCow functions in two steps. 1) Tag detection: a YOLOv5s Object Detection model and NVIDIA Deepstream Tracking Layer detect and track the tags present. 2) Tag reading: the novel WhenToRead module decides whether to read each tag, using a TRBA Scene Text Recognition model, or to use the reading from a previous frame. The system is implemented on an edge device, namely the NVIDIA Jetson AGX Orin or Xavier, making it portable to cattle production environments without external computational resources. To attain real-time speeds, ReadMyCow only reads the detected tag in the current frame if it thinks it will get a better reading when a decision metric is significantly improved in the current frame. Ideally, this means the best reading of a tag is found and stored throughout a tag's presence in the video, even when the tag becomes occluded or blurry. While testing the system at a real Midwestern dairy farm housing 9,000 cows, 96.1% of printed ear tags were accurately read by the ReadMyCow system, demonstrating its real-world commercial potential. ReadMyCow opens opportunities for informed data-driven decision-making processes on commercial cattle farms.",https://openaccess.thecvf.com/content/WACV2024/html/Smink_Computer_Vision_on_the_Edge_Individual_Cattle_Identification_in_Real-Time_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Smink_Computer_Vision_on_the_Edge_Individual_Cattle_Identification_in_Real-Time_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484127/,"['Productivity', 'Computer vision', 'Image edge detection', 'Computational modeling', 'Decision making', 'Ear', 'Cows']","['Individual Identity', 'Animal Health', 'Computational Resources', 'Object Detection', 'Detection Model', 'Video Frames', 'Commercial Farms', 'Midwestern', 'Livestock Farming', 'Recognition Model', 'Real-world Environments', 'Still Images', 'Current Frame', 'Cattle Production', 'Cattle Farms', 'Optical Character Recognition', 'Previous Frame', 'Edge Devices', 'Ear Tags', 'Tag Detection', 'Correct Reading', 'Commercial Dairy Farms', 'Speed Of System', 'Bounding Box', 'You Only Look Once', 'Radio Frequency Identification Tags', 'False Positive Detection', 'AI Models', 'Security Cameras', 'Disease Detection']","['Applications', 'Agriculture', 'Applications', 'Animals / Insects']",2,"In precision livestock farming, the individual identification of cattle is crucial to inform the decisions made to enhance animal welfare, health, and productivity. In literature, models exist that can read ear tags; however, they are not easily portable to real-world cattle production environments and make predictions mainly on still images. We propose a video-based cattle ear tag reading system, called ReadMyCow, which takes advantage of the temporal characteristics in videos to accurately detect, track, and read cattle ear tags at 25 FPS on edge devices. For each frame in a video, ReadMyCow functions in two steps. 1) Tag detection: a YOLOv5s Object Detection model and NVIDIA Deepstream Tracking Layer detect and track the tags present. 2) Tag reading: the novel WhenToRead module decides whether to read each tag, using a TRBA Scene Text Recognition model, or to use the reading from a previous frame. The system is implemented on an edge device, namely the NVIDIA Jetson AGX Orin or Xavier, making it portable to cattle production environments without external computational resources. To attain real-time speeds, ReadMyCow only reads the detected tag in the current frame if it thinks it will get a better reading when a decision metric is significantly improved in the current frame. Ideally, this means the best reading of a tag is found and stored throughout a tag’s presence in the video, even when the tag becomes occluded or blurry. While testing the system at a real Midwestern dairy farm housing 9,000 cows, 96.1% of printed ear tags were accurately read by the ReadMyCow system, demonstrating its real-world commercial potential. ReadMyCow opens opportunities for informed data-driven decision-making processes on commercial cattle farms."
Concept-Centric Transformers: Enhancing Model Interpretability Through Object-Centric Concept Learning Within a Shared Global Workspace,"Jinyung Hong, Keun Hee Park, Theodore P. Pavlic","School of Computing and Augmented Intelligence, Arizona State University; School of Computing and Augmented Intelligence, School of Life Sciences, Arizona State University",100.0,USA,0.0,,"Many interpretable AI approaches have been proposed to provide plausible explanations for a model's decision-making. However, configuring an explainable model that effectively communicates among computational modules has received less attention. A recently proposed shared global workspace theory showed that networks of distributed modules can benefit from sharing information with a bottlenecked memory because the communication constraints encourage specialization, compositionality, and synchronization among the modules. Inspired by this, we propose Concept-Centric Transformers, a simple yet effective configuration of the shared global workspace for interpretability, consisting of: i) an object-centric-based memory module for extracting semantic concepts from input features, ii) a cross-attention mechanism between the learned concept and input embeddings, and iii) standard classification and explanation losses to allow human analysts to directly assess an explanation for the model's classification reasoning. We test our approach against other existing concept-based methods on classification tasks for various datasets, including CIFAR100, CUB-200-2011, and ImageNet, and we show that our model achieves better classification accuracy than all baselines across all problems but also generates more consistent concept-based explanations of classification output.",https://openaccess.thecvf.com/content/WACV2024/html/Hong_Concept-Centric_Transformers_Enhancing_Model_Interpretability_Through_Object-Centric_Concept_Learning_Within_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hong_Concept-Centric_Transformers_Enhancing_Model_Interpretability_Through_Object-Centric_Concept_Learning_Within_WACV_2024_paper.pdf,,,2305.15775,main,Poster,https://ieeexplore.ieee.org/document/10484356/,"['Computational modeling', 'Semantics', 'Decision making', 'Memory modules', 'Transformers', 'Feature extraction', 'Synchronization']","['Model Interpretation', 'Concept Of Learning', 'Global Workspace', 'Classification Task', 'Input Features', 'ImageNet', 'Classification Output', 'Memory Module', 'Calculation Module', 'Working Memory', 'Attention Mechanism', 'Modularity', 'Domain Experts', 'Image Patches', 'Modular Structure', 'Gated Recurrent Unit', 'Modular Architecture', 'Number Of Concepts', 'Attention Weights', 'Attention Scores', 'Coworking Spaces', 'Latent Concept', 'Global Concept', 'Vision Transformer', 'Input Feature Vector', 'Linear Projection', 'Logical Constraints', 'Broadcast Information', 'Cormorants', 'Part Of The Input']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Many interpretable AI approaches have been proposed to provide plausible explanations for a model’s decision-making. However, configuring an explainable model that effectively communicates among computational modules has received less attention. A recently proposed shared global workspace theory showed that networks of distributed modules can benefit from sharing information with a bottle-necked memory because the communication constraints encourage specialization, compositionality, and synchronization among the modules. Inspired by this, we propose Concept-Centric Transformers, a simple yet effective configuration of the shared global workspace for interpretability, consisting of: i) an object-centric-based memory module for extracting semantic concepts from input features, ii) a cross-attention mechanism between the learned concept and input embeddings, and iii) standard classification and explanation losses to allow human analysts to directly assess an explanation for the model’s classification reasoning. We test our approach against other existing concept-based methods on classification tasks for various datasets, including CIFAR100, CUB-200-2011, and ImageNet, and we show that our model achieves better classification accuracy than all baselines across all problems but also generates more consistent concept-based explanations of classification output."
Concurrent Band Selection and Traversability Estimation From Long-Wave Hyperspectral Imagery in Off-Road Settings,"Florence Yellin, Scott McCloskey, Cole Hill, Eric Smith, Brian Clipp",University of South Florida; Kitware,100.0,USA,0.0,,"Autonomous navigation has become increasingly popular in recent years; However, most existing methods focus on on-road navigation and utilize active sensors, such as LiDAR. This paper instead focuses on autonomous off-road navigation using traversability estimation from passive sensors, specifically long-wave (LW) hyperspectral imagery (HSI). We present a method for selecting a subset of hyperspectral bands that are most useful for traversability estimation by designing a band selection module that designs a minimal sensor that measures sparsely-sampled spectral bands while jointly training a semantic segmentation network for traversability estimation. The effectiveness of our method is demonstrated using our dataset of LW HSI from diverse off-road scenes including forest, desert, snow, ponds, and open fields. Our dataset includes imagery collected both during the daytime and nighttime during various weather conditions, including challenging scenes with a wide range of obstacles. Using our method, we learn a small subset (2%) of all the HSI bands that can achieve competitive or better traversability estimation accuracy to that achieved when utilizing all hyperspectral bands. Using only 5 bands, our method is able to achieve a mean class accuracy that is only 1.3% less than that achieved using full 256-band HSI and only 0.1% less than that achieved using 250-band HSI, demonstrating the success of our method.",https://openaccess.thecvf.com/content/WACV2024/html/Yellin_Concurrent_Band_Selection_and_Traversability_Estimation_From_Long-Wave_Hyperspectral_Imagery_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yellin_Concurrent_Band_Selection_and_Traversability_Estimation_From_Long-Wave_Hyperspectral_Imagery_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483729/,"['Training', 'Navigation', 'Snow', 'Semantic segmentation', 'Estimation', 'Sensors', 'Task analysis']","['Hyperspectral Imagery', 'Band Selection', 'Traversability Estimation', 'Nighttime', 'Semantic Segmentation', 'Active Sensors', 'Estimation Network', 'Passive Sensors', 'Autonomous Navigation', 'Machine Learning', 'Learning Rate', 'Adam Optimizer', 'Increase In Accuracy', 'Local Image', 'Point Spread Function', 'Training Details', 'Model Hyperparameters', 'Network Reconstruction', 'Sensor Design', 'Test Split', 'Hyperspectral Image Classification', 'Diffractive Optical Elements', 'Hyperspectral Sensors', 'Sensor Layer', 'Band Center', 'Hyperspectral Image Pixels', 'High Dynamic Range Image', 'Downstream Network']","['Applications', 'Autonomous Driving', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Image recognition and understanding']",,"Autonomous navigation has become increasingly popular in recent years; However, most existing methods focus on on-road navigation and utilize active sensors, such as LiDAR. This paper instead focuses on autonomous off-road navigation using traversability estimation from passive sensors, specifically long-wave (LW) hyperspectral imagery (HSI). We present a method for selecting a subset of hyperspectral bands that are most useful for traversability estimation by designing a band selection module that designs a minimal sensor that measures sparsely-sampled spectral bands while jointly training a semantic segmentation network for traversability estimation. The effectiveness of our method is demonstrated using our dataset of LW HSI from diverse off-road scenes including forest, desert, snow, ponds, and open fields. Our dataset includes imagery collected both during the daytime and nighttime during various weather conditions, including challenging scenes with a wide range of obstacles. Using our method, we learn a small subset (2%) of all the HSI bands that can achieve competitive or better traversability estimation accuracy to that achieved when utilizing all hyperspectral bands. Using only 5 bands, our method is able to achieve a mean class accuracy that is only 1.3% less than that achieved using full 256-band HSI and only 0.1% less than that achieved using 250-band HSI, demonstrating the success of our method."
Conditional Velocity Score Estimation for Image Restoration,"Ziqiang Shi, Rujie Liu","Fujitsu R&D Center, Beijing, China",0.0,,100.0,China,"This paper proposes a new image restoration method by introducing a velocity variable on top of the data position during recovery. Under the guidance of the degraded image, it can effectively and dynamically control the direction of the diffusion path in the reverse-time stochastic differential equation (SDE). So the crucial factor is how to combine the degraded signal as a guide in this second-order reverse process with velocity, especially in the moving direction as a diffusion path. To this end, we propose a conditional velocity score approximation (CVSA) method based on the Bayesian principle to approximate the true posterior conditional velocity score by the sum of a priori conditional velocity score and an observation velocity score of the degraded measurement at the current moment. Our method is versatile from two perspectives. It can be used for both non-blind restoration and blind restoration. At the same time, there is almost no requirement for the degradation operator, and both linear and nonlinear tasks are acceptable. In non-blind restoration, including deblurring, inpainting, super-resolution, phase retrieval, and blind restoration, such as deblurring experiments, CVSA is better than other methods and achieves a new state-of-the-art.",https://openaccess.thecvf.com/content/WACV2024/html/Shi_Conditional_Velocity_Score_Estimation_for_Image_Restoration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shi_Conditional_Velocity_Score_Estimation_for_Image_Restoration_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484390/,"['Training', 'Degradation', 'Monte Carlo methods', 'Current measurement', 'Superresolution', 'Estimation', 'Differential equations']","['Reversible Process', 'Stochastic Differential Equations', 'Diffusion Path', 'Direction Of Path', 'Phase Retrieval', 'Diffusion Process', 'Diffusion Model', 'Convex Function', 'Peak Signal-to-noise Ratio', 'Gaussian Blur', 'Ground Truth Image', 'Jensen’s Inequality', 'Proof Sketch', 'Fréchet Inception Distance', 'Restoration Tasks']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Image recognition and understanding']",,"This paper proposes a new image restoration method by introducing a velocity variable on top of the data position during recovery. Under the guidance of the degraded image, it can effectively and dynamically control the direction of the diffusion path in the reverse-time stochastic differential equation (SDE). So the crucial factor is how to combine the degraded signal as a guide in this second-order reverse process with velocity, especially in the moving direction as a diffusion path. To this end, we propose a conditional velocity score approximation (CVSA) method based on the Bayesian principle to approximate the true posterior conditional velocity score by the sum of a priori conditional velocity score and an observation velocity score of the degraded measurement at the current moment. Our method is versatile from two perspectives. It can be used for both nonblind restoration and blind restoration. At the same time, there is almost no requirement for the degradation operator, and both linear and nonlinear tasks are acceptable. In nonblind restoration, including deblurring, inpainting, superresolution, phase retrieval, and blind restoration, such as deblurring experiments, CVSA is better than other methods and achieves a new state-of-the-art."
ConeQuest: A Benchmark for Cone Segmentation on Mars,"Mirali Purohit, Jacob Adler, Hannah Kerner",Arizona State University,100.0,USA,0.0,,"Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.",https://openaccess.thecvf.com/content/WACV2024/html/Purohit_ConeQuest_A_Benchmark_for_Cone_Segmentation_on_Mars_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Purohit_ConeQuest_A_Benchmark_for_Cone_Segmentation_on_Mars_WACV_2024_paper.pdf,,https://github.com/kerner-lab/ConeQuest,2311.08657,main,Poster,https://ieeexplore.ieee.org/document/10483575/,"['Training', 'Mars', 'Image segmentation', 'Computer vision', 'Computational modeling', 'Geology', 'Benchmark testing']","['Segments Of Cones', 'Computer Vision', 'Segmentation Model', 'Global Map', 'Benchmark Tasks', 'Mud Volcanoes', 'Positive Samples', 'Negative Samples', 'Geographic Information System', 'Bounding Box', 'Annotation Data', 'Overview Of Data', 'Size Categories', 'Annotation Process', 'Larger Categories', 'Masked Images', 'Small Category', 'Medium Categories', 'Planetary Science', 'Number Of Cones']","['Applications', 'Remote Sensing', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest."
ConfTrack: Kalman Filter-Based Multi-Person Tracking by Utilizing Confidence Score of Detection Box,"Hyeonchul Jung, Seokjun Kang, Takgen Kim, HyeongKi Kim","HL Klemove, Republic of Korea",0.0,,100.0,South Korea,"Kalman filter-based tracking-by-detection (KFTBD) trackers are effective methods for solving multi-person tracking tasks. However, in crowd circumstances, noisy detection results (bounding boxes with low-confidence scores) can cause ID switch and tracking failure of trackers since these trackers utilize the detector's output directly. In this paper, to solve the problem, we suggest a novel tracker called ConfTrack based on a KFTBD tracker. Compared with conventional KFTBD trackers, ConfTrack consists of novel algorithms, including low-confidence object penalization and cascading algorithms for effectively dealing with noisy detector outputs. ConfTrack is tested on diverse domains of datasets such as the MOT17, MOT20, DanceTrack, and HiEve datasets. ConfTrack has proved its robustness in crowd circumstances by achieving the highest score at HOTA and IDF1 metrics in the MOT20 dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Jung_ConfTrack_Kalman_Filter-Based_Multi-Person_Tracking_by_Utilizing_Confidence_Score_of_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jung_ConfTrack_Kalman_Filter-Based_Multi-Person_Tracking_by_Utilizing_Confidence_Score_of_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483909/,"['Matched filters', 'Computer vision', 'Noise', 'Detectors', 'Switches', 'Filtering algorithms', 'Robustness']","['Confidence Score', 'Detection Boxes', 'Multi-person Tracking', 'Kalman Filter-based', 'Bounding Box', 'Low Confidence Scores', 'Tracking Failure', 'Validation Set', 'Tracking System', 'Intersection Over Union', 'Low Confidence', 'Kalman Filter', 'General Situation', 'Object Tracking', 'Non-maximum Suppression', 'Crowded Environment', 'COCO Dataset', 'Lot Of Noise', 'Cost Matrix', 'Kalman Gain', 'Prediction Box', 'Matching Stage', 'Candidate Matches', 'Nonlinear Motion', 'Matching Step', 'Intersection Over Union Threshold', 'Intersection Over Union Value', 'Tracking Performance', 'Tracking Accuracy', 'Deep Features']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding']",4,"Kalman filter-based tracking-by-detection (KFTBD) trackers are effective methods for solving multi-person tracking tasks. However, in crowd circumstances, noisy detection results (bounding boxes with low-confidence scores) can cause ID switch and tracking failure of trackers since these trackers utilize the detector’s output directly. In this paper, to solve the problem, we suggest a novel tracker called ConfTrack based on a KFTBD tracker. Compared with conventional KFTBD trackers, ConfTrack consists of novel algorithms, including low-confidence object penalization and cascading algorithms for effectively dealing with noisy detector outputs. ConfTrack is tested on diverse domains of datasets such as the MOT17, MOT20, DanceTrack, and HiEve datasets. ConfTrack has proved its robustness in crowd circumstances by achieving the highest score at HOTA and IDF1 metrics in the MOT20 dataset."
Consistent Multimodal Generation via a Unified GAN Framework,"Zhen Zhu, Yijun Li, Weijie Lyu, Krishna Kumar Singh, Zhixin Shu, Sören Pirk, Derek Hoiem",UIUC; Adobe Inc.,50.0,USA,50.0,USA,"We investigate how to generate multimodal image outputs, such as RGB, depth, and surface normals, with a single generative model. The challenge is to produce outputs that are realistic, and also consistent with each other. Our solution builds on the StyleGAN3 architecture, with a shared backbone and modality-specific branches in the last layers of the synthesis network, and we propose per-modality fidelity discriminators and a cross-modality consistency discriminator. In experiments on the Stanford2D3D dataset, we demonstrate realistic and consistent generation of RGB, depth, and normal images. We also show a training recipe to easily extend our pretrained model on a new domain, even with a few pairwise data. We further evaluate the use of synthetically generated RGB and depth pairs for training or fine-tuning depth estimators. Code will be available at https://github.com/jessemelpolio/MultimodalGAN.",https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_Consistent_Multimodal_Generation_via_a_Unified_GAN_Framework_WACV_2024_paper.pdf,Not provided,Not provided,2307.01425,main,Poster,https://ieeexplore.ieee.org/document/10484423/,"['Training', 'Computer vision', 'Codes', 'Computational modeling', 'Computer architecture', 'Generative adversarial networks', 'Data models']","['GAN Framework', 'RGB Images', 'Depth Images', 'Depth Estimation', 'Pairwise Data', 'Surface Normals', 'Synthesis Network', 'Training Set', 'Data Augmentation', 'Multiple Modalities', 'Paired Data', 'Latent Space', 'Data Modalities', 'Depth Map', 'Target Domain', 'Depth Data', 'Multi-task Learning', 'Source Domain', 'Augmentation Strategy', 'Types Of Discrimination', 'Shared Representation', 'Latent Code', 'Augmentation Operations', 'RGB Depth']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"We investigate how to generate multimodal image outputs, such as RGB, depth, and surface normals, with a single generative model. The challenge is to produce outputs that are realistic, and also consistent with each other. Our solution builds on the StyleGAN3 architecture, with a shared backbone and modality-specific branches in the last layers of the synthesis network, and we propose per-modality fidelity discriminators and a cross-modality consistency discriminator. In experiments on the Stanford2D3D dataset, we demonstrate realistic and consistent generation of RGB, depth, and normal images. We also show a training recipe to easily extend our pretrained model on a new domain, even with a few pairwise data. We further evaluate the use of synthetically generated RGB and depth pairs for training or fine-tuning depth estimators. Code will be available at here."
Constrained Probabilistic Mask Learning for Task-Specific Undersampled MRI Reconstruction,"Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer","LMU Munich, Munich Center for Machine Learning (MCML)",100.0,Germany,0.0,,"Undersampling is a common method in Magnetic Resonance Imaging (MRI) to subsample the number of data points in k-space, reducing acquisition times at the cost of decreased image quality. A popular approach is to employ undersampling patterns following various strategies, e.g., variable density sampling or radial trajectories. In this work, we propose a method that directly learns the undersampling masks from data points, thereby also providing task- and domain-specific patterns. To solve the resulting discrete optimization problem, we propose a general optimization routine called ProM: A fully probabilistic, differentiable, versatile, and model-free framework for mask optimization that enforces acceleration factors through a convex constraint. Analyzing knee, brain, and cardiac MRI datasets with our method, we discover that different anatomic regions reveal distinct optimal undersampling masks, demonstrating the benefits of using custom masks, tailored for a downstream task. For example, ProM can create undersampling masks that maximize performance in downstream tasks like segmentation with networks trained on fully-sampled MRIs. Even with extreme acceleration factors, ProM yields reasonable performance while being more versatile than existing methods, paving the way for data-driven all-purpose mask generation.",https://openaccess.thecvf.com/content/WACV2024/html/Weber_Constrained_Probabilistic_Mask_Learning_for_Task-Specific_Undersampled_MRI_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Weber_Constrained_Probabilistic_Mask_Learning_for_Task-Specific_Undersampled_MRI_Reconstruction_WACV_2024_paper.pdf,,https://github.com/saiboxx/bernoulli-mribia,2305.16376,main,Poster,https://ieeexplore.ieee.org/document/10483718/,"['Image segmentation', 'Protocols', 'Magnetic resonance imaging', 'PROM', 'Probabilistic logic', 'Trajectory', 'Task analysis']","['Magnetic Resonance Imaging', 'Undersampled Magnetic Resonance Imaging', 'Image Quality', 'Acquisition Time', 'Brain Magnetic Resonance Imaging', 'Anatomical Regions', 'Cardiovascular Magnetic Resonance', 'Acceleration Factor', 'Magnetic Resonance Imaging Datasets', 'Reduce Acquisition Time', 'Knee Magnetic Resonance Imaging', 'Mean Square Error', 'Posterior Probability', 'High Spatial Resolution', 'Spatial Coverage', 'Peak Signal-to-noise Ratio', 'Posterior Mode', 'Image Enhancement', 'Reconstruction Quality', 'Network Reconstruction', 'Structural Similarity Index Measure', 'Projected Gradient Descent', 'K-space Data', 'Dice Score', 'Normalized Mean Square Error', 'Forward Pass', 'Enhance Image Quality']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Undersampling is a common method in Magnetic Resonance Imaging (MRI) to subsample the number of data points in k-space, reducing acquisition times at the cost of decreased image quality. A popular approach is to employ undersampling patterns following various strategies, e.g., variable density sampling or radial trajectories. In this work, we propose a method that directly learns the under-sampling masks from data points, thereby also providing task- and domain-specific patterns. To solve the resulting discrete optimization problem, we propose a general optimization routine called ProM: A fully probabilistic, differentiable, versatile, and model-free framework for mask optimization that enforces acceleration factors through a convex constraint. Analyzing knee, brain, and cardiac MRI datasets with our method, we discover that different anatomic regions reveal distinct optimal undersampling masks, demonstrating the benefits of using custom masks, tailored for a downstream task. For example, ProM can create undersampling masks that maximize performance in downstream tasks like segmentation with networks trained on fully-sampled MRIs. Even with extreme acceleration factors, ProM yields reasonable performance while being more versatile than existing methods, paving the way for data-driven all-purpose mask generation.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Content-Aware Image Color Editing With Auxiliary Color Restoration Tasks,"Yixuan Ren, Jing Shi, Zhifei Zhang, Yifei Fan, Zhe Lin, Bo He, Abhinav Shrivastava","Adobe Research; University of Maryland, College Park",50.0,USA,50.0,USA,"Diversified image color editing is typically modeled as a multimodal image-to-image translation (MMI2IT) problem with an impact on multiple applications such as photo enhancement and retouching. Although previous GAN-based algorithms successfully generate diverse edits with clear control, we observe two issues remaining: Firstly, they tend to apply the same color style to all kinds of input images when sampling with the same style latent, regardless of the input content and scenes. Secondly, they usually edit the color style globally in an image and fail to keep each semantic region and instance in harmonic colors individually. We attribute these issues to the strong independence between the style latent and the condition image in most current MMI2IT methods. To edit the raw image into a more harmonic direction with awareness of its global content and local semantics, we introduce auxiliary color restoration tasks by reducing the input color information and training jointly. We also increase the model's capacity and enrich the noise's locality with diffusion models. Furthermore, we propose a new set of metrics to measure the content-awareness of MMI2IT models, that is, how the generated style is adaptive to the input image's content. Our model is also extensible to several downstream applications including exemplar-based color editing and language-guided color editing, without imposing extra demands on the already trained model.",https://openaccess.thecvf.com/content/WACV2024/html/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ren_Content-Aware_Image_Color_Editing_With_Auxiliary_Color_Restoration_Tasks_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483697/,"['Training', 'Measurement', 'Adaptation models', 'Image color analysis', 'Computational modeling', 'Semantics', 'Harmonic analysis']","['Color Images', 'Auxiliary Task', 'Auxiliary Color', 'Color Editing', 'Input Image', 'Raw Images', 'Diffusion Model', 'Downstream Applications', 'Translation Problems', 'Denoising', 'Random Noise', 'Data Augmentation', 'Target Image', 'Latent Space', 'Color Space', 'Multimodal Imaging', 'Output Image', 'Manhattan Distance', 'Joint Model', 'Image Editing', 'Joint Training', 'L2 Loss', 'Semantic Understanding', 'Spatial Alignment', 'Reconstruction Loss', 'Original Content']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Diversified image color editing is typically modeled as a multimodal image-to-image translation (MMI2IT) problem with an impact on multiple applications such as photo enhancement and retouching. Although previous GAN-based algorithms successfully generate diverse edits with clear control, we observe two issues remaining: Firstly, they tend to apply the same color style to all kinds of input images when sampling with the same style latent, regardless of the input content and scenes. Secondly, they usually edit the color style globally in an image and fail to keep each semantic region and instance in harmonic colors individually. We attribute these issues to the strong independence between the style latent and the condition image in most current MMI2IT methods.To edit the raw image into a more harmonic direction with awareness of its global content and local semantics, we introduce auxiliary color restoration tasks by reducing the input color information and training jointly. We also increase the model’s capacity and enrich the noise’s locality with diffusion models. Furthermore, we propose a new set of metrics to measure the content-awareness of MMI2IT models, that is, how the generated style is adaptive to the input image’s content. Our model is also extensible to several downstream applications including exemplar-based color editing and language-guided color editing, without imposing extra demands on the already trained model."
Context in Human Action Through Motion Complementarity,"Eadom Dessalene, Michael Maynord, Cornelia Fermüller, Yiannis Aloimonos","University of Maryland, College Park",100.0,USA,0.0,,"Motivated by Goldman's Theory of Human Action - a framework in which action decomposes into 1) base physical movements, and 2) the context in which they occur - we propose a novel learning formulation for motion and context, where context is derived as the complement to motion. More specifically, we model physical movement through the adoption of Therbligs, a set of elemental physical motions centered around object manipulation. Context is modeled through the use of a contrastive mutual information loss that formulates context information as the action information not contained within movement information. We empirically prove the utility brought by this separation of representation, showing sizable improvements in action recognition and action anticipation accuracies for a variety of models. We present results over two object manipulation datasets: EPIC Kitchens 100, and 50 Salads.",https://openaccess.thecvf.com/content/WACV2024/html/Dessalene_Context_in_Human_Action_Through_Motion_Complementarity_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dessalene_Context_in_Human_Action_Through_Motion_Complementarity_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483612/,"['Computer vision', 'Codes', 'Computational modeling', 'Mutual information', 'Context modeling']","['Contextual Information', 'Mutual Information', 'Action Recognition', 'Activity Prediction', 'Activity Theory', 'Physical Movement', 'Movement Information', 'Computer Vision', 'Body Movements', 'Representation Learning', 'Actual Context', 'Understanding Of Activity', 'Optical Flow', 'Motion Features', 'Loss Term', 'Contextual Cues', 'Assembly Line', 'Motion Information', '3D Convolution', 'Categorical Cross-entropy Loss', 'Level Of Movement', 'Context Encoder', 'Action Recognition Task', 'Final Classification Layer', 'Mutual Information Estimation', 'Representation Of Movement']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Motivated by Goldman’s Theory of Human Action - a framework in which action decomposes into 1) base physical movements, and 2) the context in which they occur - we propose a novel learning formulation for motion and context, where context is derived as the complement to motion. More specifically, we model physical movement through the adoption of Therbligs, a set of elemental physical motions centered around object manipulation. Context is modeled through the use of a contrastive mutual information loss that formulates context information as the action information not contained within movement information. We empirically prove the utility brought by this separation of representation, showing sizable improvements in action recognition and action anticipation accuracies for a variety of models. We present results over two object manipulation datasets: EPIC Kitchens 100, and 50 Salads."
Context-Based Interpretable Spatio-Temporal Graph Convolutional Network for Human Motion Forecasting,"Edgar Medina, Leyong Loh, Namrata Gurung, Kyung Hun Oh, Niels Heller",QualityMinds GmbH,0.0,,100.0,Germany,"Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms previous methods in human motion prediction and robustness. Since the idea of enhancing interpretability for motion prediction has its merits, we showcase experiments towards it and provide preliminary evaluations of such insights here.",https://openaccess.thecvf.com/content/WACV2024/html/Medina_Context-Based_Interpretable_Spatio-Temporal_Graph_Convolutional_Network_for_Human_Motion_Forecasting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Medina_Context-Based_Interpretable_Spatio-Temporal_Graph_Convolutional_Network_for_Human_Motion_Forecasting_WACV_2024_paper.pdf,Not provided,Not provided,,main,Poster,https://ieeexplore.ieee.org/document/10484409/,"['Solid modeling', 'Analytical models', 'Three-dimensional displays', 'Predictive models', 'Robustness', 'Spatiotemporal phenomena', 'Safety']","['Graph Convolutional Network', 'Human Motion', 'Graph Convolution', 'Spatiotemporal Network', 'Spatio-Temporal Graph Convolutional Network', 'Motion Forecasting', 'Safe Application', 'Motion Prediction', '3D Pose', 'Walking', 'Classification Task', 'Hidden Layer', 'Feature Maps', 'Data Augmentation', 'Weight Vector', 'Spatial Domain', 'Input Sequence', 'Trainable Parameters', 'Temporal Domain', 'Linear Layer', 'Saliency Map', 'Temporal Convolutional Network', 'Motion Classification', 'State-of-the-art Models', 'Long-term Prediction', 'Gated Recurrent Unit', 'Left Foot', 'Temporal Convolution', '3D Transformation', 'Movement Cycle']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Autonomous Driving']",,"Human motion prediction is still an open problem extremely important for autonomous driving and safety applications. Due to the complex spatiotemporal relation of motion sequences, this remains a challenging problem not only for movement prediction but also to perform a preliminary interpretation of the joint connections. In this work, we present a Context-based Interpretable Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D human pose forecasting model based on GCNs that encompasses specific layers, aiding model interpretability and providing information that might be useful when analyzing motion distribution and body behavior. Our architecture extracts meaningful information from pose sequences, aggregates displacements and accelerations into the input model, and finally predicts the output displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI datasets demonstrate that CIST-GCN outperforms previous methods in human motion prediction and robustness. Since the idea of enhancing interpretability for motion prediction has its merits, we showcase experiments towards it and provide preliminary evaluations of such insights here.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Contextual Affinity Distillation for Image Anomaly Detection,"Jie Zhang, Masanori Suganuma, Takayuki Okatani","Graduate School of Information Sciences, Tohoku University; Graduate School of Information Sciences, Tohoku University; RIKEN Center for AIP",100.0,Japan,0.0,,"Previous studies on unsupervised industrial anomaly detection mainly focus on 'structural' types of anomalies such as cracks and color contamination by matching or learning local feature representations. While achieving significantly high detection performance on this kind of anomaly, they are faced with 'logical' types of anomalies that violate the long-range dependencies such as a normal object placed in the wrong position. Noting the reverse distillation approaches that are under the encoder-decoder paradigm could learn from the high abstract level knowledge, we propose to use two students (local and global) to better mimic the teacher's local and global behavior in reverse distillation. The local student, which is used in previous studies mainly focuses on accurate local feature learning while the global student pays attention to learning global correlations. To further encourage the global student's learning to capture long-range dependencies, we design the global context condensing block (GCCB) and propose a contextual affinity loss for the student training and anomaly scoring. Experimental results show that the proposed method sets a new state-of-the-art performance on the MVTec LOCO AD dataset without using complex training techniques.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Contextual_Affinity_Distillation_for_Image_Anomaly_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Contextual_Affinity_Distillation_for_Image_Anomaly_Detection_WACV_2024_paper.pdf,,,2307.03101,main,Poster,https://ieeexplore.ieee.org/document/10483687/,"['Training', 'Representation learning', 'Computer vision', 'Correlation', 'Image color analysis', 'Feature extraction', 'Vectors']","['Anomaly Detection', 'Image Anomaly', 'Local Features', 'Feature Representation', 'Global Context', 'Training Students', 'Accurate Characterization', 'Training Loss', 'Long-range Dependencies', 'Local Students', 'Wrong Location', 'Types Of Anomalies', 'Anomaly Score', 'Deep Models', 'Feature Space', 'Feature Maps', 'Spatial Relationship', 'Autoencoder', 'Kullback-Leibler', 'Global Information', 'Structural Anomalies', 'Reconstruction Accuracy', 'Multilayer Feature', 'Global Context Information', 'Student Model', 'Multi-level Features', 'Score Map', 'Feature Reconstruction', 'Pre-trained Network', 'Low-level Features']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Previous studies on unsupervised industrial anomaly detection mainly focus on ‘structural’ types of anomalies such as cracks and color contamination by matching or learning local feature representations. While achieving significantly high detection performance on this kind of anomaly, they are faced with ‘logical’ types of anomalies that violate the long-range dependencies such as a normal object placed in the wrong position. Noting the reverse distillation approaches that are under the encoder-decoder paradigm could learn from the high abstract level knowledge, we propose to use two students (local and global) to better mimic the teacher’s local and global behavior in reverse distillation. The local student, which is used in previous studies mainly focuses on accurate local feature learning while the global student pays attention to learning global correlations. To further encourage the global student’s learning to capture long-range dependencies, we design the global context condensing block (GCCB) and propose a contextual affinity loss for the student training and anomaly scoring. Experimental results show that the proposed method sets a new state-of-the-art performance on the MVTec LOCO AD dataset without using complex training techniques."
Continual Atlas-Based Segmentation of Prostate MRI,"Amin Ranem, Camila González, Daniel Pinto dos Santos, Andreas M. Bucher, Ahmed E. Othman, Anirban Mukhopadhyay",Stanford University; Technical University of Darmstadt; University of Frankfurt; University Medical Center Mainz; University of Cologne,100.0,"Germany, USA",0.0,,"Continual learning (CL) methods designed for natural image classification often fail to reach basic quality standards for medical image segmentation. Atlas-based segmentation, a well-established approach in medical imaging, incorporates domain knowledge on the region of interest, leading to semantically coherent predictions. This is especially promising for CL, as it allows us to leverage structural information and strike an optimal balance between model rigidity and plasticity over time. When combined with privacy-preserving prototypes, this process offers the advantages of rehearsal-based CL without compromising patient privacy. We propose Atlas Replay, an atlas-based segmentation approach that uses prototypes to generate high-quality segmentation masks through image registration that maintain consistency even as the training distribution changes. We explore how our proposed method performs compared to state-of-the-art CL methods in terms of knowledge transferability across seven publicly available prostate segmentation datasets. Prostate segmentation plays a vital role in diagnosing prostate cancer, however, it poses challenges due to substantial anatomical variations, benign structural differences in older age groups, and fluctuating acquisition parameters. Our results show that Atlas Replay is both robust and generalizes well to yet-unseen domains while being able to maintain knowledge, unlike end-to-end segmentation methods. Our code base is available under https://github.com/MECLabTUDA/Atlas-Replay.",https://openaccess.thecvf.com/content/WACV2024/html/Ranem_Continual_Atlas-Based_Segmentation_of_Prostate_MRI_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ranem_Continual_Atlas-Based_Segmentation_of_Prostate_MRI_WACV_2024_paper.pdf,,https://github.com/MECLabTUDA/Atlas-Replay,,main,Poster,https://ieeexplore.ieee.org/document/10484007/,"['Training', 'Image segmentation', 'Privacy', 'Codes', 'Magnetic resonance imaging', 'Prototypes', 'Rigidity']","['Magnetic Resonance Imaging', 'Atlas-based Segmentation', 'Prostate Cancer', 'Medical Imaging', 'Structural Information', 'Cognitive Domains', 'Segmentation Method', 'Coded Based', 'Incremental Learning', 'Segmentation Approach', 'Patient Privacy', 'Medical Image Segmentation', 'Semantic Coherence', 'Training Dataset', 'True Positive', 'Computer Science', 'User Study', 'Bias Correction', 'Continuous Training', 'Matthews Correlation Coefficient', 'Catastrophic Forgetting', 'Hyperparameter Search', 'Privacy Preservation', 'Coil Type', 'Senior Radiologist', 'Concept Drift', 'Dice Similarity Coefficient', 'Training Sequences', 'Deformation Field', 'Segmentation Performance']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Continual learning (CL) methods designed for natural image classification often fail to reach basic quality standards for medical image segmentation. Atlas-based segmentation, a well-established approach in medical imaging, incorporates domain knowledge on the region of interest, leading to semantically coherent predictions. This is especially promising for CL, as it allows us to leverage structural information and strike an optimal balance between model rigidity and plasticity over time. When combined with privacy-preserving prototypes, this process offers the advantages of rehearsal-based CL without compromising patient privacy. We propose Atlas Replay, an atlas-based segmentation approach that uses prototypes to generate high-quality segmentation masks through image registration that maintain consistency even as the training distribution changes. We explore how our proposed method performs compared to state-of-the-art CL methods in terms of knowledge transferability across seven publicly available prostate segmentation datasets. Prostate segmentation plays a vital role in diagnosing prostate cancer, however, it poses challenges due to substantial anatomical variations, benign structural differences in older age groups, and fluctuating acquisition parameters. Our results show that Atlas Replay is both robust and generalizes well to yet-unseen domains while being able to maintain knowledge, unlike end-to-end segmentation methods. Our code base is available under https://github.com/MECLabTUDA/Atlas-Replay."
Continual Learning of Unsupervised Monocular Depth From Videos,"Hemang Chawla, Arnav Varma, Elahe Arani, Bahram Zonooz",Wayve; Eindhoven University of Technology (TU/e) and TomTom; TomTom; Eindhoven University of Technology (TU/e),50.0,Netherlands,50.0,UK,"Spatial scene understanding, including monocular depth estimation, is an important problem with various applications such as robotics and autonomous driving. While improvements in unsupervised monocular depth estimation have potentially allowed models to be trained on diverse crowdsourced videos, this remains under-explored as most methods utilize the standard training protocol wherein the models are trained from scratch on all data after new data is collected. Instead, continual training of models on sequentially collected data would significantly reduce computational and memory costs. Nevertheless, naive continual training leads to catastrophic forgetting, where the model performance deteriorates on older domains as it learns on newer domains, highlighting the trade-off between model stability and plasticity. While several techniques have been proposed to address this issue in image classification, the high-dimensional and spatiotemporally correlated outputs of depth estimation make it a distinct challenge. To the best of our knowledge, no framework or method currently exists focusing on the problem of continual learning in depth estimation. Thus, we introduce a framework that captures the challenges of continual unsupervised depth estimation (CUDE), and define the necessary metrics for evaluating model performance. We propose a rehearsal-based dual-memory method MonoDepthCL, which utilizes spatiotemporal consistency for continual learning in depth estimation, even when the camera intrinsics are unknown.",https://openaccess.thecvf.com/content/WACV2024/html/Chawla_Continual_Learning_of_Unsupervised_Monocular_Depth_From_Videos_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chawla_Continual_Learning_of_Unsupervised_Monocular_Depth_From_Videos_WACV_2024_paper.pdf,,https://github.com/NeurAI-Lab/CUDE-MonoDepthCL.git,2311.02393,main,Poster,https://ieeexplore.ieee.org/document/10484504/,"['Training', 'Measurement', 'Computational modeling', 'Estimation', 'Benchmark testing', 'Cameras', 'Data models']","['Incremental Learning', 'Unsupervised Monocular Depth', 'Standard Protocol', 'Image Classification', 'Training Protocol', 'Continuous Training', 'Depth Estimation', 'Spatial Understanding', 'Catastrophic Forgetting', 'Monocular Depth Estimation', 'Camera Intrinsics', 'Estimation Method', 'Single Image', 'Working Model', 'Fast System', 'Average Performance', 'Domain Shift', 'Target Image', 'Final Performance', 'Source Images', 'View Synthesis', 'Slow System', 'Ground Truth Depth', 'Buffer Size', 'Depth Range', 'Depth Prediction', 'Outdoor Scenes', 'Joint Training', 'Learning Trajectories', 'Random Cropping']","['Applications', 'Robotics', 'Algorithms', '3D computer vision', 'Applications', 'Autonomous Driving']",3,"Spatial scene understanding, including monocular depth estimation, is an important problem in various applications such as robotics and autonomous driving. While improvements in unsupervised monocular depth estimation have potentially allowed models to be trained on diverse crowd-sourced videos, this remains underexplored as most methods utilize the standard training protocol wherein the models are trained from scratch on all data after new data is collected. Instead, continual training of models on sequentially collected data would significantly reduce computational and memory costs. Nevertheless, naive continual training leads to catastrophic forgetting, where the model performance deteriorates on older domains as it learns on newer domains, highlighting the trade-off between model stability and plasticity. While several techniques have been proposed to address this issue in image classification, the high-dimensional and spatiotemporally correlated outputs of depth estimation make it a distinct challenge. To the best of our knowledge, no framework or method currently exists focusing on the problem of continual learning in depth estimation. Thus, we introduce a framework that captures the challenges of continual unsupervised depth estimation (CUDE), and define the necessary metrics to evaluate model performance. We propose a rehearsal-based dual-memory method MonoDepthCL, which utilizes spatiotemporal consistency for continual learning in depth estimation, even when the camera intrinsics are unknown.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">§</sup>"
Continual Test-Time Domain Adaptation via Dynamic Sample Selection,"Yanshuo Wang, Jie Hong, Ali Cheraghian, Shafin Rahman, David Ahmedt-Aristizabal, Lars Petersson, Mehrtash Harandi","North South University, Bangladesh; Data61-CSIRO, Australia; Data61-CSIRO, Australia, Monash University, Australia; Australian National University, Data61-CSIRO, Australia",100.0,"Australia, Bangladesh",0.0,,"The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually adapt a pre-trained model to a sequence of target domains without accessing the source data. This paper proposes a Dynamic Sample Selection (DSS) method for CTDA. DSS consists of dynamic thresholding, positive learning, and negative learning processes. Traditionally, models learn from unlabeled unknown environment data and equally rely on all samples' pseudo-labels to update their parameters through self-training. However, noisy predictions exist in these pseudo-labels, so all samples are not equally trustworthy. Therefore, in our method, a dynamic thresholding module is first designed to select suspected low-quality from high-quality samples. The selected low-quality samples are more likely to be wrongly predicted. Therefore, we apply joint positive and negative learning on both high- and low-quality samples to reduce the risk of using wrong information. We conduct extensive experiments that demonstrate the effectiveness of our proposed method for CTDA in the image domain, outperforming the state-of-the-art results. Furthermore, our approach is also evaluated in the 3D point cloud domain, showcasing its versatility and potential for broader applicability.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Continual_Test-Time_Domain_Adaptation_via_Dynamic_Sample_Selection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Continual_Test-Time_Domain_Adaptation_via_Dynamic_Sample_Selection_WACV_2024_paper.pdf,,,2310.03335,main,Poster,https://ieeexplore.ieee.org/document/10484131/,"['Point cloud compression', 'Training', 'Adaptation models', 'Computer vision', 'Three-dimensional displays', 'Data models', 'Noise measurement']","['Domain Adaptation', 'Continuous Adaptation', 'Dynamic Selection', 'Data Sources', 'Point Cloud', 'Domain Sequences', 'Target Domain', 'Unlabeled Data', '3D Point Cloud', 'Dynamic Threshold', 'Low-quality Samples', 'Error Rate', 'Corruption', 'Teacher Model', 'Low Confidence', '3D Data', 'Source Model', 'Semi-supervised Learning', 'Learning Progress', 'Adaptive Threshold', 'Pseudo Labels', 'Error Accumulation', 'Average Confidence', 'Average Error Rate', 'Label Prediction', 'Online Manner', 'Noisy Labels', 'Lowest Error Rate', 'Student Model', 'Self-supervised Learning']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",3,"The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually adapt a pre-trained model to a sequence of target domains without accessing the source data. This paper proposes a Dynamic Sample Selection (DSS) method for CTDA. DSS consists of dynamic thresholding, positive learning, and negative learning processes. Traditionally, models learn from unlabeled unknown environment data and equally rely on all samples’ pseudo-labels to update their parameters through self-training. However, noisy predictions exist in these pseudo-labels, so all samples are not equally trustworthy. Therefore, in our method, a dynamic thresholding module is first designed to select suspected low-quality from high-quality samples. The selected low-quality samples are more likely to be wrongly predicted. Therefore, we apply joint positive and negative learning on both high- and low-quality samples to reduce the risk of using wrong information. We conduct extensive experiments that demonstrate the effectiveness of our proposed method for CTDA in the image domain, outperforming the state-of-the-art results. Furthermore, our approach is also evaluated in the 3D point cloud domain, showcasing its versatility and potential for broader applicability."
Continuous Adaptation for Interactive Segmentation Using Teacher-Student Architecture,"Barsegh Atanyan, Levon Khachatryan, Shant Navasardyan, Yunchao Wei, Humphrey Shi","Picsart AI Research (PAIR), Georgia Tech; Picsart AI Research (PAIR); BJTU",66.66666666666666,"China, USA",33.33333333333334,Ukraine,"Interactive segmentation is the task of segmenting objects or regions of interest from images based on user annotations. While most current methods perform effectively on images from the same distribution as the training dataset, they suffer to generalize on unseen domains. To address this issue some approaches incorporate test-time adaptation techniques which, on the other hand, may lead to catastrophic forgetting (i.e. degrading the performance on the previously seen domains) when applied on datasets from various domains sequentially.In this paper, we propose a novel domain adaptation approach leveraging a teacher-student learning framework to tackle the catastrophic forgetting issue. Continuously updating the student and teacher models based on user clicks results in improved segmentation accuracy on unseen domains, while preserving comparable performance on previous domains.Our approach is evaluated on a sequence of datasets from unseen domains (i.e. medical, aerial images, etc.), and, after adaptation, on the source domain demonstrating a significant decline of catastrophic forgetting (e.g. from 55% to 4% on Berkeley dataset).",https://openaccess.thecvf.com/content/WACV2024/html/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Atanyan_Continuous_Adaptation_for_Interactive_Segmentation_Using_Teacher-Student_Architecture_WACV_2024_paper.pdf,,https://github.com/Picsart-AI-Research/Interactive-Segmentation-with-Continuous-Adapation,,main,Poster,https://ieeexplore.ieee.org/document/10483571/,"['Training', 'Image segmentation', 'Adaptation models', 'Computer vision', 'Annotations', 'Computer architecture', 'Benchmark testing']","['Continuous Adaptation', 'Interactive Segmentation', 'Teacher-student Architecture', 'Training Dataset', 'Teacher Model', 'Aerial Images', 'Adaptive Approach', 'Domain Adaptation', 'Student Model', 'Catastrophic Forgetting', 'Improve Segmentation Accuracy', 'Unseen Domains', 'Model Parameters', 'Deep Learning', 'Medical Imaging', 'Interaction Model', 'Benchmark Datasets', 'Bounding Box', 'Semantic Segmentation', 'Target Object', 'Domain Dataset', 'Target Domain', 'Adaptation Scenarios', 'Interaction Task', 'Fully Convolutional Network', 'Gradient Descent Step']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Interactive segmentation is the task of segmenting objects or regions of interest from images based on user annotations. While most current methods perform effectively on images from the same distribution as the training dataset, they suffer to generalize on unseen domains. To address this issue some approaches incorporate test-time adaptation techniques which, on the other hand, may lead to catastrophic forgetting (i.e. degrading the performance on the previously seen domains) when applied on datasets from various domains sequentially. In this paper, we propose a novel domain adaptation approach leveraging a teacher-student learning framework to tackle the catastrophic forgetting issue. Continuously updating the student and teacher models based on user clicks results in improved segmentation accuracy on unseen domains, while preserving comparable performance on previous domains. Our approach is evaluated on a sequence of datasets from unseen domains (i.e. medical, aerial images, etc.), and, after adaptation, on the source domain demonstrating a significant decline of catastrophic forgetting (e.g. from 55% to 4% on Berkeley dataset)."
Contrastive Learning for Multi-Object Tracking With Transformers,"Pierre-François De Plaen, Nicola Marinello, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool","ESAT-PSI, KU Leuven, Belgium; CVL, ETH Zürich, Switzerland; TRACE vzw; ESAT-PSI, KU Leuven, Belgium",75.0,"Belgium, Switzerland",25.0,Belgium,"The DEtection TRansformer (DETR) opened new possibilities for object detection by modeling it as a translation task: converting image features into object-level representations. Previous works typically add expensive modules to DETR to perform Multi-Object Tracking (MOT), resulting in more complicated architectures. We instead show how DETR can be turned into a MOT model by employing an instance-level contrastive loss, a revised sampling strategy and a lightweight assignment method. Our training scheme learns object appearances while preserving detection capabilities and with little overhead. Its performance surpasses the previous state-of-the-art by +2.6 mMOTA on the challenging BDD100K dataset and is comparable to existing transformer-based methods on the MOT17 dataset.",https://openaccess.thecvf.com/content/WACV2024/html/De_Plaen_Contrastive_Learning_for_Multi-Object_Tracking_With_Transformers_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/De_Plaen_Contrastive_Learning_for_Multi-Object_Tracking_With_Transformers_WACV_2024_paper.pdf,,,2311.08043,main,Poster,https://ieeexplore.ieee.org/document/10484234/,"['Training', 'Training data', 'Object detection', 'Self-supervised learning', 'Detectors', 'Transformers', 'Feature extraction']","['Self-supervised Learning', 'Multi-object Tracking', 'Image Features', 'Object Detection', 'Contrastive Loss', 'Object Appearance', 'Translation Task', 'Training Set', 'Training Data', 'Adverse Conditions', 'Image Object', 'Detection Model', 'Bounding Box', 'Feed-forward Network', 'Discriminative Features', 'Latent Space', 'Attention Module', 'Loss Term', 'Local Loss', 'Track Model', 'Ground Truth Object', 'Transformer Decoder', 'Batch Of Images', 'Object Detection Dataset', 'Bipartite Matching', 'Joint Detection', 'Main Metrics', 'Tracking Accuracy', 'Computer Vision', 'Inference Time']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving']",,"The DEtection TRansformer (DETR) opened new possibilities for object detection by modeling it as a translation task: converting image features into object-level representations. Previous works typically add expensive modules to DETR to perform Multi-Object Tracking (MOT), resulting in more complicated architectures. We instead show how DETR can be turned into a MOT model by employing an instance-level contrastive loss, a revised sampling strategy and a lightweight assignment method. Our training scheme learns object appearances while preserving detection capabilities and with little overhead. Its performance surpasses the previous state-of-the-art by +2.6 mMOTA on the challenging BDD100K dataset and is comparable to existing transformer-based methods on the MOT17 dataset."
Contrastive Viewpoint-Aware Shape Learning for Long-Term Person Re-Identification,"Vuong D. Nguyen, Khadija Khaldi, Dung Nguyen, Pranav Mantini, Shishir Shah",University of Houston; Hanoi University of Science and Technology,100.0,"USA, Vietnam",0.0,,"Traditional approaches for Person Re-identification (Re-ID) rely heavily on modeling the appearance of persons. This measure is unreliable over longer durations due to the possibility for changes in clothing or biometric information. Furthermore, viewpoint changes significantly degrade the matching ability of these methods. In this paper, we propose ""Contrastive Viewpoint-aware Shape Learning for Long-term Person Re-Identification"" (CVSL) to address these challenges. Our method robustly extracts local and global texture-invariant human body shape cues from 2D pose using the Relational Shape Embedding branch, which consists of a pose estimator and a shape encoder built on a Graph Attention Network. To enhance the discriminability of the shape and appearance of identities under viewpoint variations, we propose Contrastive Viewpoint-aware Losses (CVL). CVL leverages contrastive learning to simultaneously minimize the intra-class gap under different viewpoints and maximize the inter-class gap under the same viewpoint. Extensive experiments demonstrate that our proposed framework outperforms state-of-the-art methods on long-term person Re-ID benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Contrastive_Viewpoint-Aware_Shape_Learning_for_Long-Term_Person_Re-Identification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_Contrastive_Viewpoint-Aware_Shape_Learning_for_Long-Term_Person_Re-Identification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483285/,"['Adaptation models', 'Computer vision', 'Shape', 'Biometrics (access control)', 'Clothing', 'Self-supervised learning', 'Benchmark testing']","['Self-supervised Learning', 'Body Shape', 'Pose Estimation', 'Contrastive Loss', 'Local Shape', 'Global Shape', 'Graph Attention', 'Viewpoint Changes', 'Graph Attention Network', 'Personal Appearance', 'Changing Clothes', 'Viewpoint Variations', '2D Pose', 'Human Body Shape', 'Positive Samples', 'Negative Samples', 'Attention Mechanism', 'Shape Features', 'Latent Space', 'Real-world Scenarios', 'Appearance Features', 'Graph Convolutional Network', 'Shape Representation', 'Global Representation', 'Triplet Loss', 'Refinement Network', 'Metric Learning', 'Image X', 'Final Representation', 'CNN Backbone']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",7,"Traditional approaches for Person Re-identification (ReID) rely heavily on modeling the appearance of persons. This measure is unreliable over longer durations due to the possibility for changes in clothing or biometric information. Furthermore, viewpoint changes significantly degrade the matching ability of these methods. In this paper, we propose ""Contrastive Viewpoint-aware Shape Learning for Long-term Person Re-Identification"" (CVSL) to address these challenges. Our method robustly extracts local and global texture-invariant human body shape cues from 2D pose using the Relational Shape Embedding branch, which consists of a pose estimator and a shape encoder built on a Graph Attention Network. To enhance the discriminability of the shape and appearance of identities under viewpoint variations, we propose Contrastive Viewpoint-aware Losses (CVL). CVL leverages contrastive learning to simultaneously minimize the intra-class gap under different viewpoints and maximize the inter-class gap under the same viewpoint. Extensive experiments demonstrate that our proposed framework outperforms state-of-the-art methods on long-term person Re-ID benchmarks."
Controllable Image Synthesis of Industrial Data Using Stable Diffusion,"Gabriele Valvano, Antonino Agostino, Giovanni De Magistris, Antonino Graziano, Giacomo Veneri","Baker Hughes, Florence, Italy",0.0,,100.0,Italy,"Training supervised deep neural networks that perform defect detection and segmentation requires large-scale fully-annotated datasets, which can be hard or even impossible to obtain in industrial environments. Generative AI offers opportunities to enlarge small industrial datasets artificially, thus enabling the usage of state-of-the-art supervised approaches in the industry. Unfortunately, also good generative models need a lot of data to train, while industrial datasets are often tiny. Here, we propose a new approach for reusing general-purpose pre-trained generative models on industrial data, ultimately allowing the generation of self-labelled defective images. First, we let the model learn the new concept, entailing the novel data distribution. Then, we force it to learn to condition the generative process, producing industrial images that satisfy well-defined topological characteristics and show defects with a given geometry and location. To highlight the advantage of our approach, we use the synthetic dataset to optimise a crack segmentor for a real industrial use case. When the available data is small, we observe considerable performance increase under several metrics, showing the method's potential in production environments.",https://openaccess.thecvf.com/content/WACV2024/html/Valvano_Controllable_Image_Synthesis_of_Industrial_Data_Using_Stable_Diffusion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Valvano_Controllable_Image_Synthesis_of_Industrial_Data_Using_Stable_Diffusion_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483699/,"['Training', 'Measurement', 'Instance segmentation', 'Industries', 'Adaptation models', 'Image synthesis', 'Production']","['Image Synthesis', 'Industrial Data', 'Data Distribution', 'Small Datasets', 'Large-scale Datasets', 'Industrial Environment', 'Industrial Approach', 'Denoising', 'Gaussian Noise', 'Diffusion Process', 'Data Augmentation', 'Number Of Images', 'Diffusion Model', 'Information Leakage', 'Fault Location', 'Visual Model', 'Synthetic Images', 'Training Objective', 'Realistic Images', 'Subset Of Images', 'Fréchet Inception Distance', 'Shape Defects', 'Foundation Model', 'Unlabeled Images', 'Prior Imaging', 'Image Appearance', 'Few-shot Learning', 'Forward Process', 'Input State', 'Crack Detection']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Commercial / retail']",2,"Training supervised deep neural networks that perform defect detection and segmentation requires large-scale fully-annotated datasets, which can be hard or even impossible to obtain in industrial environments. Generative AI offers opportunities to enlarge small industrial datasets artificially, thus enabling the usage of state-of-the-art supervised approaches in the industry. Unfortunately, also good generative models need a lot of data to train, while industrial datasets are often tiny. Here, we propose a new approach for reusing general-purpose pre-trained generative models on industrial data, ultimately allowing the generation of self-labelled defective images. First, we let the model learn the new concept, entailing the novel data distribution. Then, we force it to learn to condition the generative process, producing industrial images that satisfy well-defined topological characteristics and show defects with a given geometry and location. To highlight the advantage of our approach, we use the synthetic dataset to optimise a crack segmentor for a real industrial use case. When the available data is small, we observe considerable performance increase under several metrics, showing the method’s potential in production environments."
Controllable Text-to-Image Synthesis for Multi-Modality MR Images,"Kyuri Kim, Yoonho Na, Sung-Joon Ye, Jimin Lee, Sung Soo Ahn, Ji Eun Park, Hwiyoung Kim",Ulsan National Institute of Science & Technology; Seoul National University; Yonsei University College of Medicine; University of Ulsan College of Medicine,100.0,South Korea,0.0,,"Generative modeling has seen significant advancements in recent years, especially in the realm of text-to-image synthesis. Despite this progress, the medical field has yet to fully leverage the capabilities of large-scale foundational models for synthetic data generation. This paper introduces a framework for text-conditional magnetic resonance (MR) imaging generation, addressing the complexities associated with multi-modality considerations. The framework comprises a pre-trained large language model, a diffusion-based prompt-conditional image generation architecture, and an additional denoising network for input structural binary masks. Experimental results demonstrate that the proposed framework is capable of generating realistic, high-resolution, and high-fidelity multi-modal MR images that align with medical language text prompts. Further, the study interprets the cross-attention maps of the generated results based on text-conditional statements. The contributions of this research lay a robust foundation for future studies in text-conditional medical image generation and hold significant promise for accelerating advancements in medical imaging research.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Controllable_Text-to-Image_Synthesis_for_Multi-Modality_MR_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Controllable_Text-to-Image_Synthesis_for_Multi-Modality_MR_Images_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Controlling Character Motions Without Observable Driving Source,"Weiyuan Li, Bin Dai, Ziyi Zhou, Qi Yao, Baoyuan Wang",Xiaobing.AI,0.0,,100.0,China,"How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Controlling_Character_Motions_Without_Observable_Driving_Source_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Controlling_Character_Motions_Without_Observable_Driving_Source_WACV_2024_paper.pdf,,,2308.06025,main,Poster,https://ieeexplore.ieee.org/document/10484468/,"['Computer vision', 'Systematics', 'Semantics', 'Pipelines', 'Buildings', 'Reinforcement learning', 'Task analysis']","['Autoregressive Model', 'Reward Function', 'Strong Baseline', 'Quantum', 'Supervised Learning', 'Feature Space', 'Challenging Problem', 'Latent Space', 'Body Motion', 'Markov Decision Process', 'Discrete Space', 'Human Motion', 'Live Streaming', 'Policy Network', 'Motion Prediction', 'Reinforcement Learning Framework', 'L2 Loss', 'Inference Phase', 'High-level Policy', 'Sequence Of Tokens', 'Proximal Policy Optimization', 'Head Pose', 'Input Tokens']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is nontrivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly."
"Controlling Rate, Distortion, and Realism: Towards a Single Comprehensive Neural Image Compression Model","Shoma Iwai, Tomo Miyazaki, Shinichiro Omachi","Graduate School of Engineering, Tohoku University, Japan",100.0,Japan,0.0,,"In recent years, neural network-driven image compression (NIC) has gained significant attention. Some works adopt deep generative models such as GANs and diffusion models to enhance perceptual quality (realism). A critical obstacle of these generative NIC methods is that each model is optimized for a single bit rate. Consequently, multiple models are required to compress images to different bit rates, which is impractical for real-world applications. To tackle this issue, we propose a variable-rate generative NIC model. Specifically, we explore several discriminator designs tailored for the variable-rate approach and introduce a novel adversarial loss. Moreover, by incorporating the newly proposed multi-realism technique, our method allows the users to adjust the bit rate, distortion, and realism with a single model, achieving ultra-controllability. Unlike existing variable-rate generative NIC models, our method matches or surpasses the performance of state-of-the-art single-rate generative NIC models while covering a wide range of bit rates using just one model.",https://openaccess.thecvf.com/content/WACV2024/html/Iwai_Controlling_Rate_Distortion_and_Realism_Towards_a_Single_Comprehensive_Neural_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Iwai_Controlling_Rate_Distortion_and_Realism_Towards_a_Single_Comprehensive_Neural_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483997/,"['Computer vision', 'Image coding', 'Bit rate', 'Rate-distortion', 'Rate distortion theory', 'Distortion', 'Controllability']","['Image Compression', 'Neural Compression', 'Neural Image Compression', 'Range Of Rates', 'Generative Adversarial Networks', 'Diffusion Model', 'Perception Of Quality', 'Bitrate', 'Wide Range Of Rates', 'Loss Function', 'Level Of Quality', 'Training Step', 'Bitstream', 'Balance Training', 'Least Significant Bit', 'Most Significant Bit', 'Attention Layer', 'Entropy Model', 'Lower Mean Square Error', 'Fake Images', 'Fréchet Inception Distance']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",4,"In recent years, neural network-driven image compression (NIC) has gained significant attention. Some works adopt deep generative models such as GANs and diffusion models to enhance perceptual quality (realism). A critical obstacle of these generative NIC methods is that each model is optimized for a single bit rate. Consequently, multiple models are required to compress images to different bit rates, which is impractical for real-world applications. To tackle this issue, we propose a variable-rate generative NIC model. Specifically, we explore several discriminator designs tailored for the variable-rate approach and introduce a novel adversarial loss. Moreover, by incorporating the newly proposed multi-realism technique, our method allows the users to adjust the bit rate, distortion, and realism with a single model, achieving ultra-controllability. Unlike existing variable-rate generative NIC models, our method matches or surpasses the performance of state-of-the-art single-rate generative NIC models while covering a wide range of bit rates using just one model."
Controlling Virtual Try-On Pipeline Through Rendering Policies,"Kedan Li, Jeffrey Zhang, Shao-Yu Chang, David Forsyth",University of Illinois at Urbana-Champaign; revery.ai,50.0,USA,50.0,USA,"This paper shows how to impose rendering policies on a virtual try-on (VTON) pipeline. Our rendering policies are lightweight procedural descriptions of how the pipeline should render outfits or render particular types of garments. Our policies are procedural expressions describing offsets to the control points for each set of garment types. The policies are easily authored and are generalizable to any outfit composed of garments of similar types. We describe a VTON pipeline that accepts our policies to modify garment drapes and produce high-quality try-on images with garment attributes preserved. Layered outfits are a particular challenge to VTON systems because learning to coordinate warps between multiple garments so that nothing sticks out is difficult. Our rendering policies offer a lightweight and effective procedure to achieve this coordination, while also allowing precise manipulation of drape. Drape describes the way in which a garment is worn (for example, a shirt could be tucked or untucked). Quantitative and qualitative evaluations demonstrate that our method allows effective manipulation of drape and produces significant measurable improvements in rendering quality for complicated layering interactions.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Controlling_Virtual_Try-On_Pipeline_Through_Rendering_Policies_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Controlling_Virtual_Try-On_Pipeline_Through_Rendering_Policies_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483943/,"['Computer vision', 'Clothing', 'Pipelines', 'Rendering (computer graphics)']","['Virtual Try-on', 'Quantitative Evaluation', 'Control Points', 'Layering', 'Types Of Errors', 'Target Image', 'Image Generation', 'Baseline Methods', 'Correction Procedure', 'Appendix For Details', 'L1 Loss', 'Person Image', 'Variety Of Styles', 'Left Shoulder', 'Prior Art', 'Qualitative Examples', 'Type Of Editing', 'Body Pose']","['Applications', 'Visualization', 'Applications', 'Arts / games / social media', 'Applications', 'Commercial / retail']",,"This paper shows how to impose rendering policies on a virtual try-on (VTON) pipeline. Our rendering policies are lightweight procedural descriptions of how the pipeline should render outfits or render particular types of garments. Our policies are procedural expressions describing offsets to the control points for each set of garment types. The policies are easily authored and are generalizable to any outfit composed of garments of similar types. We describe a VTON pipeline that accepts our policies to modify garment drapes and produce high-quality try-on images with garment attributes preserved.Layered outfits are a particular challenge to VTON systems because learning to coordinate warps between multiple garments so that nothing sticks out is difficult. Our rendering policies offer a lightweight and effective procedure to achieve this coordination, while also allowing precise manipulation of drape. Drape describes the way in which a garment is worn (for example, a shirt could be tucked or untucked).Quantitative and qualitative evaluations demonstrate that our method allows effective manipulation of drape and produces significant measurable improvements in rendering quality for complicated layering interactions."
Convolutional Masked Image Modeling for Dense Prediction Tasks on Pathology Images,"Yan Yang, Liyuan Pan, Liu Liu, Eric A. Stone","BDSI, ANU; BITSZ & School of CSAT, BIT; Cyberverse Lab",66.66666666666666,"Australia, India",33.33333333333334,USA,"This paper studies a convolutional masked image modeling approach for boosting downstream dense prediction tasks on pathology images. Our method is self-supervised, and entails two strategies in sequence. Considering features contained in the pathology images usually have a large spatial span, e.g., glands, we insert [MASK] tokens to the masked regions after the stem layer of the convolutional network for encoding unmasked pixels, which facilitates information propagation through masked regions for reconstructing unmasked pixels. Furthermore, the pathology images contain features that are represented in diverse affine shapes and color spaces. We, therefore, enforce the network to learn the affine and color invariant embedding by imposing transformation constraints between the unmasked image-encoded embedding and reconstruction targets. Our approach is simple but effective. With extensive experiments on standard benchmark datasets, we demonstrate superior transfer learning performance on downstream tasks over past state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_Convolutional_Masked_Image_Modeling_for_Dense_Prediction_Tasks_on_Pathology_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Convolutional_Masked_Image_Modeling_for_Dense_Prediction_Tasks_on_Pathology_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484144/,"['Pathology', 'Image coding', 'Shape', 'Image color analysis', 'Transfer learning', 'Glands', 'Predictive models']","['Masked Images', 'Pathological Images', 'Dense Prediction', 'Dense Prediction Tasks', 'Convolutional Network', 'Network Layer', 'Color Variation', 'Paper Studies', 'Color Space', 'Information Propagation', 'Large Span', 'Convolutional Approach', 'Standard Benchmark Datasets', 'Object Detection', 'Bounding Box', 'Backbone Network', 'Affine Transformation', 'Slide Images', 'Digital Pathology', 'Self-supervised Learning', 'Vision Transformer', 'Image X', 'Mask R-CNN', 'Foundation Model', 'Masking Strategy', 'Color Transformation', 'Instance Segmentation', 'ResNet-50 Backbone', 'Architecture For Detection', 'Ground-truth Bounding Box']","['Applications', 'Biomedical / healthcare / medicine']",1,"This paper studies a convolutional masked image modeling approach for boosting downstream dense prediction tasks on pathology images. Our method is self-supervised, and entails two strategies in sequence. Considering features contained in the pathology images usually have a large spatial span, e.g., glands, we insert [MASK] tokens to the masked regions after the stem layer of the convolutional network for encoding unmasked pixels, which facilitates information propagation through masked regions for reconstructing unmasked pixels. Furthermore, the pathology images contain features that are represented in diverse affine shapes and color spaces. We, therefore, enforce the network to learn the affine and color invariant embedding by imposing transformation constraints between the unmasked image-encoded embedding and reconstruction targets. Our approach is simple but effective. With extensive experiments on standard benchmark datasets, we demonstrate superior transfer learning performance on downstream tasks over past state-of-the-art approaches."
Correlation-Aware Active Learning for Surgery Video Segmentation,"Fei Wu, Pablo Márquez-Neila, Mingyi Zheng, Hedyeh Rafii-Tari, Raphael Sznitman","Auris Health. Inc, JNJ; AIMI, University of Bern",50.0,Switzerland,50.0,USA,"Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. How- ever, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, COWAL, COrrelation aWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_Correlation-Aware_Active_Learning_for_Surgery_Video_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_Correlation-Aware_Active_Learning_for_Surgery_Video_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484247/,"['Computer vision', 'Uncertainty', 'Annotations', 'Semantic segmentation', 'Instruments', 'Surgery', 'Self-supervised learning']","['Active Learning', 'Video Segments', 'Surgery Videos', 'Model Uncertainty', 'Latent Space', 'Video Frames', 'Semantic Segmentation', 'Native Sequence', 'Surgical Instruments', 'Self-supervised Learning', 'Video Dataset', 'Training Set', 'Centroid', 'Validation Set', 'Images Of Samples', 'Segmentation Method', 'Segmentation Model', 'Segmentation Task', 'Video Sequences', 'Training Videos', 'Active Learning Methods', 'Test Videos', 'Set Of Frames', 'Skateboarding', 'Segmentation Annotations', 'Unlabeled Images', 'Highest Entropy', 'Binary Segmentation', 'Semantic Segmentation Methods', 'Segmentation Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Biomedical / healthcare / medicine']",,"Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. However, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, COWAL, COrrelationaWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals."
CrashCar101: Procedural Generation for Damage Assessment,"Jens Parslov, Erik Riise, Dim P. Papadopoulos","Technical University of Denmark, Pioneer Center for AI; Technical University of Denmark",100.0,Denmark,0.0,,"In this paper, we are interested in addressing the problem of damage assessment for vehicles, such as cars. This task requires not only detecting the location and the extent of the damage but also identifying the damaged part. To train a computer vision system for the semantic part and damage segmentation in images, we need to manually annotate images with costly pixel annotations for both part categories and damage types. To overcome this need, we propose to use synthetic data to train these models. Synthetic data can provide samples with high variability, pixel-accurate annotations, and arbitrarily large training sets without any human intervention. We propose a procedural generation pipeline that damages 3D car models and we obtain synthetic 2D images of damaged cars paired with pixel-accurate annotations for part and damage categories. To validate our idea, we execute our pipeline and render our CrashCar101 dataset. We run experiments on three real datasets for the tasks of part and damage segmentation. For part segmentation, we show that the segmentation models trained on a combination of real data and our synthetic data outperform all models trained only on real data. For damage segmentation, we show the sim2real transfer ability of CrashCar101.",https://openaccess.thecvf.com/content/WACV2024/html/Parslov_CrashCar101_Procedural_Generation_for_Damage_Assessment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Parslov_CrashCar101_Procedural_Generation_for_Damage_Assessment_WACV_2024_paper.pdf,https://crashcar.compute.dtu.dk,,2311.06536,main,Poster,https://ieeexplore.ieee.org/document/10483618/,"['Training', 'Image segmentation', 'Three-dimensional displays', 'Annotations', 'Pipelines', 'Data models', 'Automobiles']","['Damage Assessment', 'Training Set', 'Computer Vision', 'Human Intervention', '2D Images', 'Segmentation Model', 'Types Of Damage', 'Manual Annotation', 'Segmentation Task', 'Synthetic Images', 'Part Segmentation', 'Car Model', 'Generation Pipeline', 'Damage Categories', 'Validation Set', 'Background Noise', 'ImageNet', 'Noise Sources', 'Rate Set', 'Semantic Segmentation', 'ResNet-101 Backbone', 'Overview Of The Dataset', 'Domain Adaptation', '3D Mesh', 'Synthetic Data Generation']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"In this paper, we are interested in addressing the problem of damage assessment for vehicles, such as cars. This task requires not only detecting the location and the extent of the damage but also identifying the damaged part. To train a computer vision system for the semantic part and damage segmentation in images, we need to manually annotate images with costly pixel annotations for both part categories and damage types. To overcome this need, we propose to use synthetic data to train these models. Synthetic data can provide samples with high variability, pixel-accurate annotations, and arbitrarily large training sets without any human intervention. We propose a procedural generation pipeline that damages 3D car models and we obtain synthetic 2D images of damaged cars paired with pixel-accurate annotations for part and damage categories. To validate our idea, we execute our pipeline and render our CrashCar101 dataset. We run experiments on three real datasets for the tasks of part and damage segmentation. For part segmentation, we show that the segmentation models trained on a combination of real data and our synthetic data outperform all models trained only on real data. For damage segmentation, we show the sim2real transfer ability of CrashCar101."
Critical Gap Between Generalization Error and Empirical Error in Active Learning,Yusuke Kanebako,"Ricoh Company, Ltd.",0.0,,100.0,Japan,"Conventional research papers on Active Learning (AL) have conducted evaluations based on the assumption that a large amount of annotated data is available for evaluating model performance apart from the data selected by AL. This evaluation method is not realistic for the setting where AL learns models with few annotation costs. If a large amount of annotated data is available, it should be used for both evaluation and training, not only for evaluation. Therefore, in a realistic model construction using AL, generalization error in the actual production environment should be estimated by cross-validation only using the data selected by AL. However, the data selected by AL tend to be a biased dataset because the data are selected based on some criteria. Therefore, there is a gap between the actual generalization error and the empirical error when conducting cross-validation on the AL-selected data. In addition, if validation is performed using only the selected dataset by AL, it is possible to fail to realize this fatal gap. In this paper, we show that cross-validation using selected data in conventional AL methods either overestimate or underestimate model performance. As a result, we show a significant difference between generalization error and empirical error from cross-validation.",https://openaccess.thecvf.com/content/WACV2024/html/Kanebako_Critical_Gap_Between_Generalization_Error_and_Empirical_Error_in_Active_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kanebako_Critical_Gap_Between_Generalization_Error_and_Empirical_Error_in_Active_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484141/,"['Training', 'Computer vision', 'Uncertainty', 'Costs', 'Annotations', 'Production', 'Data models']","['Active Learning', 'Generalization Error', 'Model Performance', 'Evaluation Method', 'Large Amount Of Data', 'Annotation Data', 'Dataset Bias', 'Actual Error', 'Active Learning Methods', 'Amount Of Annotated Data', 'Data Distribution', 'Test Data', 'Random Sampling', 'Validation Data', 'Random Selection', 'Model Uncertainty', 'Generalization Performance', 'Variety Of Tasks', 'Data Bias', 'Cross-validation Accuracy', 'Cross-validation Results', 'Problem Setup', 'Actual Operation']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Conventional research papers on Active Learning (AL) have conducted evaluations based on the assumption that a large amount of annotated data is available for evaluating model performance apart from the data selected by AL. This evaluation method is not realistic for the setting where AL learns models with few annotation costs. If a large amount of annotated data is available, it should be used for both evaluation and training, not only for evaluation. Therefore, in a realistic model construction using AL, generalization error in the actual production environment should be estimated by cross-validation only using the data selected by AL. However, the data selected by AL tend to be a biased dataset because the data are selected based on some criteria. Therefore, there is a gap between the actual generalization error and the empirical error when conducting cross-validation on the AL-selected data. In addition, if validation is performed using only the selected dataset by AL, it is possible to fail to realize this fatal gap. In this paper, we show that cross-validation using selected data in conventional AL methods either overestimate or underestimate model performance. As a result, we show a significant difference between generalization error and empirical error from cross-validation."
Cross-Attention Between Satellite and Ground Views for Enhanced Fine-Grained Robot Geo-Localization,"Dong Yuan, Frederic Maire, Feras Dayoub","QUT Centre for Robotics, Queensland University of Technology, Australia; Australian Institute for Machine Learning (AIML), University of Adelaide, Australia",100.0,Australia,0.0,,"Cross-view image geo-localization aims to determine the locations of outdoor robots by mapping current street-view images with GPS-tagged satellite image patches. Recent works have attained a remarkable level of accuracy in identifying which satellite patches the robot is in, where the location of the central pixel within the matched satellite patch is used as the robot coarse location estimation. This work focuses on robot fine-grained localization within a known satellite patch. Existing fine-grain localization work utilizes correlation operation to obtain similarity between satellite image local descriptors and street-view global descriptors. The correlation operation based on liner matching simplifies the interaction process between two views, leading to a large distance error and affecting model generalization. To address this issue, we devise a cross-view feature fusion network with self-attention and cross-attention layers to replace correlation operation. Additionally, we combine classification and regression prediction to further decrease location distance error. Experiments show that our novel network architecture outperforms the state-of-the-art, exhibiting better generalization capabilities in unseen areas. Specifically, our method reduces the median localization distance error by 43% and 50% respectively in the same area and unseen areas on the VIGOR benchmark.",https://openaccess.thecvf.com/content/WACV2024/html/Yuan_Cross-Attention_Between_Satellite_and_Ground_Views_for_Enhanced_Fine-Grained_Robot_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_Cross-Attention_Between_Satellite_and_Ground_Views_for_Enhanced_Fine-Grained_Robot_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483962/,"['Location awareness', 'Computer vision', 'Satellites', 'Correlation', 'Estimation', 'Network architecture', 'Robustness']","['Ground View', 'Benchmark', 'Satellite Images', 'Feature Fusion', 'Regression Prediction', 'Local Descriptors', 'Median Error', 'Global Descriptors', 'Robot Localization', 'Street View Images', 'Error Of The Mean', 'Transformer', 'Convolutional Neural Network', 'Convolutional Layers', 'Local Features', 'Feature Maps', 'Feature Representation', 'Multi-label', 'Semantic Information', 'Image Pairs', 'Positional Encoding', 'Image Retrieval', 'Aerial Images', 'Prediction Head', 'Local Feature Extraction', 'Attention Operation', 'ReLU Activation Function', 'Hidden Dimension', 'Linear Projection', 'Feature Fusion Module']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving', 'Applications', 'Robotics']",2,"Cross-view image geo-localization aims to determine the locations of outdoor robots by mapping current street-view images with GPS-tagged satellite image patches. Recent works have attained a remarkable level of accuracy in identifying which satellite patches the robot is in, where the location of the central pixel within the matched satellite patch is used as the robot coarse location estimation. This work focuses on robot fine-grained localization within a known satellite patch. Existing fine-grain localization work utilizes correlation operation to obtain similarity between satellite image local descriptors and street-view global descriptors. The correlation operation based on liner matching simplifies the interaction process between two views, leading to a large distance error and affecting model generalization. To address this issue, we devise a cross-view feature fusion network with self-attention and cross-attention layers to replace correlation operation. Additionally, we combine classification and regression prediction to further decrease location distance error. Experiments show that our novel network architecture outperforms the state-of-the-art, exhibiting better generalization capabilities in unseen areas. Specifically, our method reduces the median localization distance error by 43% and 50% respectively in the same area and unseen areas on the VIGOR benchmark."
Cross-Domain Few-Shot Incremental Learning for Point-Cloud Recognition,"Yuwen Tan, Xiang Xiang","Key Lab of Image Processing and Intelligent Control, Ministry of Education, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China",100.0,China,0.0,,"Sensing 3D objects is critical when 2D object recognition is not accessible. A robot pre-trained on a large point-cloud dataset will encounter unseen classes of 3D objects after deploying it. Therefore, the robot should be able to learn continuously in real-world scenarios. Few-shot class-incremental learning (FSCIL) requires the model to learn from few-shot new examples continually and not forget past classes. However, there is an implicit but strong assumption in the FSCIL that the distribution of the base and incremental classes is the same. In this paper, we focus on cross-domain FSCIL for point-cloud recognition. We decompose the catastrophic forgetting into base class forgetting and incremental class forgetting and alleviate them separately. We utilize the base model to discriminate base samples and new samples by treating base samples as in-distribution samples, and new objects as out-of-distribution samples. We retain the base model to avoid catastrophic forgetting of base classes and train an extra domain-specific module for all new samples to adapt to new classes. At inference, we first discriminate whether the sample belongs to the base class or the new class. Once classified at the model level, test samples are then passed to the corresponding model for class-level classification. To better mitigate the forgetting of new classes, we adopt the soft label and hard label replay together. Extensive experiments on synthetic-to-real incremental 3D datasets show that our proposed method can balance the performance between the base and new objects and outperforms the previous state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Tan_Cross-Domain_Few-Shot_Incremental_Learning_for_Point-Cloud_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tan_Cross-Domain_Few-Shot_Incremental_Learning_for_Point-Cloud_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484150/,"['Adaptation models', 'Computer vision', 'Three-dimensional displays', 'Robot sensing systems', 'Power capacitors', 'Sensors', 'Object recognition']","['Incremental Learning', 'Point Cloud Recognition', 'Test Samples', 'Object Recognition', 'Base Classes', '3D Datasets', 'Few-shot Learning', 'Soft Labels', 'Catastrophic Forgetting', 'Point Cloud Dataset', 'Classification Accuracy', 'Classification Performance', 'Feature Space', 'Generalization Ability', 'Deeper Layers', 'Average Accuracy', 'Domain Shift', 'Learning-based Methods', 'Set Of Classes', 'Linear Classifier', 'Paste Samples', 'Incremental Model', 'Local Feature Extraction', 'Classification Layer', 'Projection Layer', 'Joint Training', '3D Point', 'Output Logits', '2D Domain', 'Binary Classification Problem']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",2,"Sensing 3D objects is critical when 2D object recognition is not accessible. A robot pre-trained on a large point-cloud dataset will encounter unseen classes of 3D objects after deploying it. Therefore, the robot should be able to learn continuously in real-world scenarios. Few-shot class-incremental learning (FSCIL) requires the model to learn from few-shot new examples continually and not forget past classes. However, there is an implicit but strong assumption in the FSCIL that the distribution of the base and incremental classes is the same. In this paper, we focus on cross-domain FSCIL for point-cloud recognition. We decompose the catastrophic forgetting into base class forgetting and incremental class forgetting and alleviate them separately. We utilize the base model to discriminate base samples and new samples by treating base samples as in-distribution samples, and new objects as out-of-distribution samples. We retain the base model to avoid catastrophic forgetting of base classes and train an extra domain-specific module for all new samples to adapt to new classes. At inference, we first discriminate whether the sample belongs to the base class or the new class. Once classified at the model level, test samples are then passed to the corresponding model for class-level classification. To better mitigate the forgetting of new classes, we adopt the soft label and hard label replay together. Extensive experiments on synthetic-to-real incremental 3D datasets show that our proposed method can balance the performance between the base and new objects and outperforms the previous state-of-the-art methods."
Cross-Feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data,"Sai Aparna Aketi, Kaushik Roy","Purdue University, USA",100.0,USA,0.0,,"The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, ImageNette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (0.2-4% improvement in test accuracy) compared to other existing techniques for decentralized learning on heterogeneous data.",https://openaccess.thecvf.com/content/WACV2024/html/Aketi_Cross-Feature_Contrastive_Loss_for_Decentralized_Deep_Learning_on_Heterogeneous_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Aketi_Cross-Feature_Contrastive_Loss_for_Decentralized_Deep_Learning_on_Heterogeneous_Data_WACV_2024_paper.pdf,,,2310.15890,main,Poster,https://ieeexplore.ieee.org/document/10484037/,"['Training', 'Deep learning', 'Computer vision', 'Network topology', 'Computational modeling', 'Distributed databases', 'Computer architecture']","['Heterogeneous Data', 'Contrastive Loss', 'Model Parameters', 'Data Distribution', 'Model Architecture', 'Visualization Of Datasets', 'Fashion-MNIST', 'Loss Function', 'Hyperparameters', 'Cross-entropy Loss', 'Stochastic Gradient Descent', 'Degree Of Heterogeneity', 'Computational Overhead', 'Representative Class', 'Loss Term', 'Communication Overhead', 'Local Dataset', 'Central Server', 'L1 Loss', 'Graph Topology', 'Mean Square Error Loss', 'Ring Topology', 'Federated Learning', 'Graph Size', 'Mixing Matrix', 'Communication Rounds']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (0.2 – 4% improvement in test accuracy) compared to other existing techniques for decentralized learning on heterogeneous data."
CryoRL: Reinforcement Learning Enables Efficient Cryo-EM Data Collection,"Quanfu Fan, Yilai Li, Yuguang Yao, John Cohn, Sijia Liu, Ziping Xu, Seychelle Vos, Michael Cianfrocco",,,,,,"Single-particle cryo-electron microscopy (cryo-EM) has become one of the mainstream structural biology techniques because of its ability to determine high-resolution structures of dynamic bio-molecules. However, cryo-EM data acquisition remains expensive and labor-intensive, requiring substantial expertise. Structural biologists need a more efficient and objective method to collect the best data in a limited time frame. We formulate the cryo-EM data collection task as an optimization problem in this work. The goal is to maximize the total number of good images taken within a specified period. We show that reinforcement learning offers an effective way to plan cryo-EM data collection, successfully navigating heterogenous cryo-EM grids. The approach we developed, cryoRL, demonstrates better performance than average users for data collection under similar settings.",https://openaccess.thecvf.com/content/WACV2024/html/Fan_CryoRL_Reinforcement_Learning_Enables_Efficient_Cryo-EM_Data_Collection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fan_CryoRL_Reinforcement_Learning_Enables_Efficient_Cryo-EM_Data_Collection_WACV_2024_paper.pdf,,,2204.07543,main,Poster,https://ieeexplore.ieee.org/document/10483995/,"['Navigation', 'Microscopy', 'Data acquisition', 'Reinforcement learning', 'Data collection', 'Biology', 'Trajectory']","['Cryo-EM Data', 'Cryo-EM Data Collection', 'Efficient Data Collection', 'Optimization Problem', 'Structural Biology', 'Average User', 'Work Problems', 'cryo-EM Grids', 'Single-particle Cryo-electron Microscopy', 'Structure Of Biomolecules', 'Cryo-EM Data Acquisition', 'Data Quality', 'Amount Of Time', 'Image Quality', 'Validation Set', 'Spike Protein', 'Simulated Annealing', 'Path Planning', 'Image Patches', 'Reward Function', 'Deep Q-network', 'Number Of Holes', 'Contrast Transfer Function', 'Reinforcement Learning Model', 'Deep Reinforcement Learning', 'Typical Grid', 'Deep Q-learning', 'Scanning Probe Microscopy', 'Patch Level', 'Reinforcement Learning Policy']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Single-particle cryo-electron microscopy (cryo-EM) has become one of the mainstream structural biology techniques because of its ability to determine high-resolution structures of dynamic bio-molecules. However, cryo-EM data acquisition remains expensive and labor-intensive, requiring substantial expertise. Structural biologists need a more efficient and objective method to collect the best data in a limited time frame. We formulate the cryo-EM data collection task as an optimization problem in this work. The goal is to maximize the total number of good images taken within a specified period. We show that reinforcement learning offers an effective way to plan cryo-EM data collection, successfully navigating heterogenous cryo-EM grids. The approach we developed, cryoRL, demonstrates better performance than average users for data collection under similar settings."
Customizing 360-Degree Panoramas Through Text-to-Image Diffusion Models,"Hai Wang, Xiaoyu Xiang, Yuchen Fan, Jing-Hao Xue",Meta Reality Labs; University College London,50.0,UK,50.0,USA,"Personalized text-to-image (T2I) synthesis based on diffusion models has attracted significant attention in recent research. However, existing methods primarily concentrate on customizing subjects or styles, neglecting the exploration of global geometry. In this study, we propose an approach that focuses on the customization of 360-degree panoramas, which inherently possess global geometric properties, using a T2I diffusion model. To achieve this, we curate a paired image-text dataset specifically designed for the task and subsequently employ it to fine-tune a pre-trained T2I diffusion model with LoRA. Nevertheless, the fine-tuned model alone does not ensure the continuity between the leftmost and rightmost sides of the synthesized images, a crucial characteristic of 360-degree panoramas. To address this issue, we propose a method called StitchDiffusion. Specifically, we perform pre-denoising operations twice at each time step of the denoising process on the stitch block consisting of the leftmost and rightmost image regions. Furthermore, a global cropping is adopted to synthesize seamless 360-degree panoramas. Experimental results demonstrate the effectiveness of our customized model combined with the proposed StitchDiffusion in generating high-quality 360-degree panoramic images. Moreover, our customized model exhibits exceptional generalization ability in producing scenes unseen in the fine-tuning dataset. Code is available at https://github.com/littlewhitesea/StitchDiffusion.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Customizing_360-Degree_Panoramas_Through_Text-to-Image_Diffusion_Models_WACV_2024_paper.pdf,,https://github.com/littlewhitesea/StitchDiffusion,2310.18840,main,Poster,,,,,,
CycleCL: Self-Supervised Learning for Periodic Videos,"Matteo Destro, Michael Gygli","Cerrion, Inc.",0.0,,100.0,USA,"Analyzing periodic video sequences is a key topic in applications such as automatic production systems, remote sensing, medical applications, or physical training. An example is counting repetitions of a physical exercise. Due to the distinct characteristics of periodic data, self-supervised methods designed for standard image datasets do not capture changes relevant to the progression of the cycle and fail to ignore unrelated noise. They thus do not work well on periodic data. In this paper, we propose CycleCL, a self-supervised learning method specifically designed to work with periodic data. We start from the insight that a good visual representation for periodic data should be sensitive to the phase of a cycle, but be invariant to the exact repetition, i.e. it should generate identical representations for a specific phase throughout all repetitions. We exploit the repetitions in videos to design a novel contrastive learning method based on a triplet loss that optimizes for these desired properties. Our method uses pre-trained features to sample pairs of frames from approximately the same phase and negative pairs of frames from different phases. Then, we iterate between optimizing a feature encoder and resampling triplets, until convergence. By optimizing a model this way, we are able to learn features that have the mentioned desired properties. We evaluate CycleCL on an industrial and multiple human actions datasets, where it significantly outperforms previous video-based self-supervised learning methods on all tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Destro_CycleCL_Self-Supervised_Learning_for_Periodic_Videos_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Destro_CycleCL_Self-Supervised_Learning_for_Periodic_Videos_WACV_2024_paper.pdf,,,2311.03402,main,Poster,https://ieeexplore.ieee.org/document/10484305/,"['Training', 'Visualization', 'Production systems', 'Video sequences', 'Noise', 'Self-supervised learning', 'Medical services']","['Self-supervised Learning', 'Phase Of Cycle', 'Feature Learning', 'Video Sequences', 'Standard Datasets', 'Triplet Loss', 'Pair Of Frames', 'Self-supervised Learning Methods', 'Positive Samples', 'Negative Samples', 'Feature Representation', 'K-nearest Neighbor', 'Autoencoder', 'ImageNet', 'Representation Learning', 'Anomaly Detection', 'Mean Threshold', 'Metric Learning', 'Temporal Consistency', 'Repeat Count', 'Vision Transformer', 'ResNet-18 Model', 'Number Of Triplets']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Analyzing periodic video sequences is a key topic in applications such as automatic production systems, remote sensing, medical applications, or physical training. An example is counting repetitions of a physical exercise. Due to the distinct characteristics of periodic data, self-supervised methods designed for standard image datasets do not capture changes relevant to the progression of the cycle and fail to ignore unrelated noise. They thus do not work well on periodic data.In this paper, we propose CycleCL, a self-supervised learning method specifically designed to work with periodic data. We start from the insight that a good visual representation for periodic data should be sensitive to the phase of a cycle, but be invariant to the exact repetition, i.e. it should generate identical representations for a specific phase throughout all repetitions. We exploit the repetitions in videos to design a novel contrastive learning method based on a triplet loss that optimizes for these desired properties. Our method uses pre-trained features to sample pairs of frames from approximately the same phase and negative pairs of frames from different phases. Then, we iterate between optimizing a feature encoder and resampling triplets, until convergence.By optimizing a model this way, we are able to learn features that have the mentioned desired properties. We evaluate CycleCL on an industrial and multiple human actions datasets, where it significantly outperforms previous video-based self-supervised learning methods on all tasks."
D3GU: Multi-Target Active Domain Adaptation via Enhancing Domain Alignment,"Lin Zhang, Linghan Xu, Saman Motamed, Shayok Chakraborty, Fernando De la Torre","Robotics Institute, Carnegie Mellon University; Department of Computer Science, Florida State University",100.0,USA,0.0,,"Unsupervised domain adaptation (UDA) for image classification has made remarkable progress in transferring classification knowledge from a labeled source domain to an unlabeled target domain, thanks to effective domain alignment techniques. Recently, in order to further improve performance on a target domain, many Single-Target Active Domain Adaptation (ST-ADA) methods have been proposed to identify and annotate the salient and exemplar target samples. However, it requires one model to be trained and deployed for each target domain and the domain label associated with each test sample. This largely restricts its application in the ubiquitous scenarios with multiple target domains. Therefore, we propose a Multi-Target Active Domain Adaptation (MT-ADA) framework for image classification, named D3GU, to simultaneously align different domains and actively select samples from them for annotation. This is the first research effort in this field to our best knowledge. D3GU applies Decomposed Domain Discrimination (D3) during training to achieve both source-target and target-target domain alignments. Then during active sampling, a Gradient Utility (GU) score is designed to weight every unlabeled target image by its contribution towards classification and domain alignment tasks, and is further combined with KMeans clustering to form GU-KMeans for diverse image sampling. Extensive experiments on three benchmark datasets, Office31, OfficeHome, and DomainNet, have been conducted to validate consistently superior performance of D3GU for MT-ADA.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_D3GU_Multi-Target_Active_Domain_Adaptation_via_Enhancing_Domain_Alignment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_D3GU_Multi-Target_Active_Domain_Adaptation_via_Enhancing_Domain_Alignment_WACV_2024_paper.pdf,,https://github.com/lzhangbj/D3GU,,main,Poster,,,,,,
D4: Detection of Adversarial Diffusion Deepfakes Using Disjoint Ensembles,"Ashish Hooda, Neal Mangaokar, Ryan Feng, Kassem Fawaz, Somesh Jha, Atul Prakash",University of Wisconsin–Madison; University of Michigan,100.0,USA,0.0,,"Detecting diffusion-generated deepfake images remains an open problem. Current detection methods fail against an adversary who adds imperceptible adversarial perturbations to the deepfake to evade detection. In this work, we propose Disjoint Diffusion Deepfake Detection (D4), a deepfake detector designed to improve black-box adversarial robustness beyond de facto solutions such as adversarial training. D4 uses an ensemble of models over disjoint subsets of the frequency spectrum to significantly improve adversarial robustness. Our key insight is to leverage a redundancy in the frequency domain and apply a saliency partitioning technique to disjointly distribute frequency components across multiple models. We formally prove that these disjoint ensembles lead to a reduction in the dimensionality of the input subspace where adversarial deepfakes lie, thereby making adversarial deepfakes harder to find for black-box attacks. We then empirically validate the D4 method against several black-box attacks and find that D4 significantly outperforms existing state-of-the-art defenses applied to diffusion-generated deepfake detection. We also demonstrate that D4 provides robustness against adversarial deepfakes from unseen data distributions as well as unseen generative techniques.",https://openaccess.thecvf.com/content/WACV2024/html/Hooda_D4_Detection_of_Adversarial_Diffusion_Deepfakes_Using_Disjoint_Ensembles_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hooda_D4_Detection_of_Adversarial_Diffusion_Deepfakes_Using_Disjoint_Ensembles_WACV_2024_paper.pdf,,,2202.05687,main,Poster,https://ieeexplore.ieee.org/document/10483793/,"['Training', 'Deepfakes', 'Frequency-domain analysis', 'Perturbation methods', 'Redundancy', 'Closed box', 'Detectors']","['Deepfake Detection', 'Dimensionality Reduction', 'Frequency Spectrum', 'Frequency Components', 'Ensemble Model', 'Adversarial Training', 'Adversarial Perturbations', 'Adversarial Robustness', 'Black-box Attacks', 'Deep Neural Network', 'Signal Detection', 'Feature Space', 'Training Time', 'Individual Models', 'Challenging Problem', 'Generative Adversarial Networks', 'Diffusion Model', 'Majority Voting', 'Feature Subset', 'Partitioning Scheme', 'Adversarial Examples', 'Discrete Cosine Transform', 'Attack Success Rate', 'Average Precision Score', 'Set Of Frequencies', 'ResNet-50 Architecture', 'Frequency Space', 'Generative Adversarial Networks Model', 'Cosine Transform', 'Input Features']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"Detecting diffusion-generated deepfake images remains an open problem. Current detection methods fail against an adversary who adds imperceptible adversarial perturbations to the deepfake to evade detection. In this work, we propose Disjoint Diffusion Deepfake Detection (D4), a deepfake detector designed to improve black-box adversarial robustness beyond de facto solutions such as adversarial training. D4 uses an ensemble of models over disjoint subsets of the frequency spectrum to significantly improve adversarial robustness. Our key insight is to leverage a redundancy in the frequency domain and apply a saliency partitioning technique to disjointly distribute frequency components across multiple models. We formally prove that these disjoint ensembles lead to a reduction in the dimensionality of the input subspace where adversarial deepfakes lie, thereby making adversarial deepfakes harder to find for black-box attacks. We then empirically validate the D4 method against several black-box attacks and find that D4 significantly outperforms existing state-of-the-art defenses applied to diffusion-generated deepfake detection. We also demonstrate that D4 provides robustness against adversarial deepfakes from unseen data distributions as well as unseen generative techniques."
DDAM-PS: Diligent Domain Adaptive Mixer for Person Search,"Mohammed Khaleed Almansoori, Mustansar Fiaz, Hisham Cholakkal","Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; IBM (Work done at Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE)",100.0,"UAE, USA",0.0,,"Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-identification (ReID). Although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for person search (DDAP-PS) framework that aims to bridge a gap to improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the norm-aware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our code is publicly available at https://github.com/mustansarfiaz/DDAM.",https://openaccess.thecvf.com/content/WACV2024/html/Almansoori_DDAM-PS_Diligent_Domain_Adaptive_Mixer_for_Person_Search_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Almansoori_DDAM-PS_Diligent_Domain_Adaptive_Mixer_for_Person_Search_WACV_2024_paper.pdf,,https://github.com/mustansarfiaz/DDAM-PS,,main,Poster,https://ieeexplore.ieee.org/document/10484454/,"['Bridges', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Source coding', 'Supervised learning', 'Search problems']","['Domain Adaptation', 'Supervised Learning', 'Knowledge Transfer', 'Pedestrian', 'Target Domain', 'Joint Optimization', 'Field Performance', 'Source Domain', 'Domain Representation', 'Pedestrian Detection', 'Unlabeled Target Domain', 'Labeled Source Domain', 'Re-identification Task', 'Data Sources', 'Shortest Path', 'Bounding Box', 'Domain Shift', 'Lookup Table', 'Target Data', 'Two-stage Method', 'Pseudo Labels', 'Domain-invariant Representations', 'Unsupervised Domain Adaptation Methods', 'Mixed Mechanism', 'Faster R-CNN', 'Source Distribution', 'One-stage Methods', 'Bridging Mechanism', 'Domain Gap', 'Joint Training']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding']",1,"Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-identification (ReID). Although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for person search (DDAP-PS) framework that aims to bridge a gap to improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the normaware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our source code is publicly available at https://github.com/mustansarfiaz/DDAM-PS."
DECDM: Document Enhancement Using Cycle-Consistent Diffusion Models,"Jiaxin Zhang, Joy Rimchala, Lalla Mouatadid, Kamalika Das, Sricharan Kumar",Intuit AI Research,100.0,USA,0.0,,"The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and shadow removal, and demonstrate the superiority of performance quantitatively and qualitatively.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_DECDM_Document_Enhancement_Using_Cycle-Consistent_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_DECDM_Document_Enhancement_Using_Cycle-Consistent_Diffusion_Models_WACV_2024_paper.pdf,,,2311.09625,main,Poster,https://ieeexplore.ieee.org/document/10483958/,"['Training', 'Data privacy', 'Adaptation models', 'Optical character recognition', 'Pipelines', 'Noise reduction', 'Character recognition']","['Diffusion Model', 'Denoising', 'Data Privacy', 'Data Augmentation', 'Benchmark Datasets', 'Privacy Protection', 'Target Domain', 'Source Model', 'Target Model', 'Source Domain', 'Paired Box', 'Optical Character Recognition', 'Data Privacy Protection', 'Domain-specific Models', 'Recent Advances In Models', 'Data Sources', 'Convolutional Neural Network', 'Characteristic Features', 'Ordinary Differential Equations', 'Image Pairs', 'Cycle Consistency', 'Source Dataset', 'Structural Similarity Index Measure', 'Peak Signal-to-noise Ratio', 'Generative Adversarial Networks', 'Stochastic Differential Equations', 'Competitive Methods', 'Image X', 'Noisy Data', 'Target Dataset']","['Applications', 'Commercial / retail']",2,"The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and shadow removal, and demonstrate the superiority of performance quantitatively and qualitatively."
DISCO: Distributed Inference With Sparse Communications,"Minghai Qin, Chao Sun, Jaco Hofmann, Dejan Vucinic",Western Digital,0.0,,100.0,USA,"Deep neural networks (DNNs) have great potential to solve many real-world problems, but they usually require an extensive amount of computation and memory. It is of great difficulty to deploy a large DNN model to a single resource-limited device with small memory capacity. Distributed computing is a common approach to reduce single-node memory consumption and to accelerate the inference of DNN models. In this paper, we explore the ""within-layer model parallelism"", which distributes the inference of each layer into multiple nodes. In this way, the memory requirement can be distributed to many nodes, making it possible to use several edge devices to infer a large DNN model. Due to the dependency within each layer, data communications between nodes during this parallel inference can be a bottleneck when the communication bandwidth is limited. We propose a framework to train DNN models for Distributed Inference with Sparse Communications (DISCO). We convert the problem of selecting which subset of data to transmit between nodes into a model optimization problem, and derive models with both computation and communication reduction when each layer is inferred on multiple nodes. We show the benefit of the DISCO framework on a variety of CV tasks such as image classification, object detection, semantic segmentation, and image super resolution. The corresponding models include important DNN building blocks such as convolutions and transformers. For example, each layer of a ResNet-50 model can be distributively inferred across two nodes with 5x less data communications, almost half overall computations and less than half memory requirement for a single node, and achieve comparable accuracy to the original ResNet-50 model.",https://openaccess.thecvf.com/content/WACV2024/html/Qin_DISCO_Distributed_Inference_With_Sparse_Communications_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Qin_DISCO_Distributed_Inference_With_Sparse_Communications_WACV_2024_paper.pdf,,,2302.11180,main,Poster,https://ieeexplore.ieee.org/document/10484122/,"['Computational modeling', 'Semantic segmentation', 'Memory management', 'Superresolution', 'Artificial neural networks', 'Parallel processing', 'Transformers']","['Neural Network', 'Deep Neural Network', 'Image Classification', 'Object Detection', 'Super-resolution', 'Digital Communication', 'Semantic Segmentation', 'Memory Capacity', 'Model Inference', 'Distributed Computing', 'Deep Neural Network Model', 'Image Object Detection', 'Communication Bandwidth', 'ResNet-50 Model', 'Convolutional Neural Network', 'Convolutional Layers', 'Sparsity', 'Feature Maps', 'Input Features', 'Row Vector', 'Communication Latency', 'Number Of Input Features', 'Independent Branch', 'Feature Subset', 'Convolution Kernel', 'Computation Latency', 'Baseline Methods', 'Accuracy Loss', 'Output Feature', 'Machine Learning Tasks']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Deep neural networks (DNNs) have great potential to solve many real-world problems, but they usually require an extensive amount of computation and memory. It is of great difficulty to deploy a large DNN model to a single resource-limited device with small memory capacity. Distributed computing is a common approach to reduce single-node memory consumption and to accelerate the inference of DNN models. In this paper, we explore the ""within-layer model parallelism"", which distributes the inference of each layer into multiple nodes. In this way, the memory requirement can be distributed to many nodes, making it possible to use several edge devices to infer a large DNN model. Due to the dependency within each layer, data communications between nodes during this parallel inference can be a bottleneck when the communication bandwidth is limited. We propose a framework to train DNN models for Distributed Inference with Sparse Communications (DISCO). We convert the problem of selecting which subset of data to transmit between nodes into a model optimization problem, and derive models with both computation and communication reduction when each layer is inferred on multiple nodes. We show the benefit of the DISCO framework on a variety of CV tasks such as image classification, object detection, semantic segmentation, and image super resolution. The corresponding models include important DNN building blocks such as convolutions and transformers. For example, each layer of a ResNet-50 model can be distributively inferred across two nodes with 5x less data communications, almost half overall computations and less than half memory requirement for a single node, and achieve comparable accuracy to the original ResNet-50 model."
DPPMask: Masked Image Modeling With Determinantal Point Processes,"Junde Xu, Zikai Lin, Donghao Zhou, Yaodong Yang, Xiangyun Liao, Qiong Wang, Bian Wu, Guangyong Chen, Pheng-Ann Heng","Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences; The Chinese University of Hong Kong; Zhejiang Lab",100.0,"China, Hong Kong",0.0,,"Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MAE and iBOT. We show that DPPMask surpassed random sampling under both lower and higher masking ratios, indicating that DPPMask makes the reconstruction task more reasonable. We further test our method on the background challenge and multi-class classification tasks, showing that our method is more robust at various tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_DPPMask_Masked_Image_Modeling_With_Determinantal_Point_Processes_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_DPPMask_Masked_Image_Modeling_With_Determinantal_Point_Processes_WACV_2024_paper.pdf,,,2303.12736,main,Poster,https://ieeexplore.ieee.org/document/10484508/,"['Training', 'Computer vision', 'Semantics', 'Force', 'Random processes', 'Task analysis', 'Image reconstruction']","['Point Process', 'Masked Images', 'Random Sampling', 'Multi-label', 'Semantic Information', 'Original Information', 'Reconstruction Task', 'Semantic Change', 'Multi-class Classification Task', 'Misalignment Problem', 'Masking Strategy', 'F1 Score', 'Feature Learning', 'Image Information', 'Random Strategy', 'Reconstruction Results', 'Maximum A Posteriori', 'Self-supervised Learning', 'Markov Random Field', 'Pretext Task', 'Tokenized', 'Denoising Autoencoder', 'Linear Probe', 'Semantic Space', 'Greedy Selection', 'Masked Language Model']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MAE and iBOT. We show that DPPMask surpassed random sampling under both lower and higher masking ratios, indicating that DPP-Mask makes the reconstruction task more reasonable. We further test our method on the background challenge and multi-class classification tasks, showing that our method is more robust at various tasks."
DR10K: Transfer Learning Using Weak Labels for Grading Diabetic Retinopathy on DR10K Dataset,"Mohamed ElHabebe, Shereen ElKordi, Ahmed Gamal ElDin, Noha Adly, Marwan Torki, Ahmed Elmassry, Islam SH Ahmed","Alexandria University, Alexandria, Egypt; Applied Innovation Center, MCIT, Cairo, Egypt; Alexandria University, Alexandria, Egypt; Applied Innovation Center, MCIT, Cairo, Egypt",50.0,Egypt,50.0,Egypt,"In this paper, we contrast the usage of two deep-learning approaches for the automatic grading of diabetic retinopathy (DR) and diabetic macular edema (DME) in retinal fundus photographs using a relatively small novel dataset. We developed a telemedicine system to collect and humanly grade 11,109 diabetic patients. The certified graders annotated the level of DR as well as the existence of a referable DME in the macula-centered fundus images only. We use EfficientNet to build an AI-based model for both problems. To examine the transfer learning validity, the model was trained on an external dataset (EyePacs) and then finetuned on the egyptian data for the DR and DME grading problems. Firstly, we use the macula-centered images only in fine-tuning. Secondly, we use optic-disc-centered images in addition to macula-centered images. We obtained the labels for the optic-disc-centered images directly from the corresponding macula-centered labels as weak labels. Then, both types of images are used in fine-tuning. We found an increase in the DR performance using the second approach in both accuracy and quadratic weighted kappa(QWK). Notably, QWK increased from 90.23% to 91.3% using additional weakly labeled optic-disc-centered fundus images.",https://openaccess.thecvf.com/content/WACV2024/html/ElHabebe_DR10K_Transfer_Learning_Using_Weak_Labels_for_Grading_Diabetic_Retinopathy_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/ElHabebe_DR10K_Transfer_Learning_Using_Weak_Labels_for_Grading_Diabetic_Retinopathy_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483954/,"['Training', 'Diabetic retinopathy', 'Telemedicine', 'Roads', 'Transfer learning', 'Pipelines', 'Developing countries']","['Transfer Learning', 'Weak Labels', 'Diabetic Retinopathy Grading', 'Deep Learning', 'Fundus Photography', 'Macular Edema', 'External Dataset', 'Fundus Images', 'Diabetic Macular Edema', 'Large Datasets', 'Convolutional Neural Network', 'Ophthalmology', 'Glaucoma', 'Screening Program', 'ImageNet', 'Ensemble Model', 'Binary Problem', 'Linear Layer', 'Test Split', 'Stand-alone Model', 'Fine-tuned Model', 'Score Regression', 'Complete Pipeline', 'Step Of The Pipeline', 'Diabetic Retinopathy Screening', 'Diabetic Retinopathy Severity']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In this paper, we contrast the usage of two deep-learning approaches for the automatic grading of diabetic retinopathy (DR) and diabetic macular edema (DME) in retinal fundus photographs using a relatively small novel dataset. We developed a telemedicine system to collect and humanly grade 11,109 diabetic patients. The certified graders annotated the level of DR as well as the existence of a referable DME in the macula-centered fundus images only. We use EfficientNet to build an AI-based model for both problems. To examine the transfer learning validity, the model was trained on an external dataset (EyePacs) and then finetuned on the egyptian data for the DR and DME grading problems. Firstly, we use the macula-centered images only in fine-tuning. Secondly, we use optic-disc-centered images in addition to macula-centered images. We obtained the labels for the optic-disc-centered images directly from the corresponding macula-centered labels as weak labels. Then, both types of images are used in fine-tuning. We found an increase in the DR performance using the second approach in both accuracy and quadratic weighted kappa(QWK). Notably, QWK increased from 90.23% to 91.3% using additional weakly labeled optic-disc-centered fundus images."
DR2: Disentangled Recurrent Representation Learning for Data-Efficient Speech Video Synthesis,"Chenxu Zhang, Chao Wang, Yifan Zhao, Shuo Cheng, Linjie Luo, Xiaohu Guo",Peking University; The University of Texas at Dallas; Georgia Institute of Technology; ByteDance Inc,75.0,"China, USA",25.0,China,"Although substantial progress has been made in audio-driven talking video synthesis, there still remain two major difficulties: existing works 1) need a long sequence of training dataset (>1h) to synthesize co-speech gestures, which causes a significant limitation on their applicability; 2) usually fail to generate long sequences, or can only generate long sequences without enough diversity. To solve these challenges, we propose a Disentangled Recurrent Representation Learning framework to synthesize long diversified gesture sequences with a short training video of around 2 minutes. In our framework, we first make a disentangled latent space assumption to encourage unpaired audio and pose combinations, which results in diverse ""one-to-many"" mappings in pose generation. Next, we apply a recurrent inference module to feed back the last generation as initial guidance to the next phase, enhancing the long-term video generation of full continuity and diversity. Comprehensive experimental results verify that our model can generate realistic synchronized full-body talking videos with training data efficiency.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_DR2_Disentangled_Recurrent_Representation_Learning_for_Data-Efficient_Speech_Video_Synthesis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_DR2_Disentangled_Recurrent_Representation_Learning_for_Data-Efficient_Speech_Video_Synthesis_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483627,"['Training', 'Representation learning', 'Computer vision', 'Computational modeling', 'Training data', 'Data models', 'Synchronization']","['Representation Learning', 'Recurrent Learning', 'Video Synthesis', 'Speech Videos', 'Training Data', 'Training Dataset', 'Learning Framework', 'Latent Space', 'Short Training', 'Decoding', 'Image Quality', 'Facial Expressions', 'Random Noise', 'Video Clips', 'Learning Objectives', 'General Appearance', 'Hand Gestures', 'Human Speech', 'Motor Loss', 'Sequence Reconstruction', 'Audio Input', 'Lip-sync', 'State Bank', 'Disentangled Representation']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",1,"Although substantial progress has been made in audiodriven talking video synthesis, there still remain two major difficulties: existing works 1) need a long sequence of training dataset (>1h) to synthesize co-speech gestures, which causes a significant limitation on their applicability; 2) usually fail to generate long sequences, or can only generate long sequences without enough diversity. To solve these challenges, we propose a Disentangled Recurrent Representation Learning framework to synthesize long diversified gesture sequences with a short training video of around 2 minutes. In our framework, we first make a disentangled latent space assumption to encourage unpaired audio and pose combinations, which results in diverse ""one-to-many"" mappings in pose generation. Next, we apply a recurrent inference module to feed back the last generation as initial guidance to the next phase, enhancing the long-term video generation of full continuity and diversity. Comprehensive experimental results verify that our model can generate realistic synchronized full-body talking videos with training data efficiency."
DREAM: Visual Decoding From Reversing Human Visual System,"Weihao Xia, Raoul de Charette, Cengiz Oztireli, Jing-Hao Xue",Inria; University of Cambridge; University College London,100.0,"France, UK",0.0,,"In this work we present DREAM, an fMRI-to-image method for reconstructing viewed images from brain activities, grounded on fundamental knowledge of the human visual system. We craft reverse pathways that emulate the hierarchical and parallel nature of how humans perceive the visual world. These tailored pathways are specialized to decipher semantics, color, and depth cues from fMRI data, mirroring the forward pathways from visual stimuli to fMRI recordings. To do so, two components mimic the inverse processes within the human visual system: the Reverse Visual Association Cortex (R-VAC) which reverses pathways of this brain region, extracting semantics from fMRI data; the Reverse Parallel PKM (R-PKM) component simultaneously predicting color and depth from fMRI signals. The experiments indicate that our method outperforms the current state-of-the-art models in terms of the consistency of appearance, structure, and semantics. Code will be available at https://github.com/weihaox/DREAM.",https://openaccess.thecvf.com/content/WACV2024/html/Xia_DREAM_Visual_Decoding_From_Reversing_Human_Visual_System_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xia_DREAM_Visual_Decoding_From_Reversing_Human_Visual_System_WACV_2024_paper.pdf,,https://github.com/weihaox/DREAM,2310.02265,main,Poster,https://ieeexplore.ieee.org/document/10483647/,"['Visualization', 'Image color analysis', 'Semantics', 'Functional magnetic resonance imaging', 'Visual systems', 'Recording', 'Decoding']","['Human Visual System', 'Visual Decoding', 'Brain Activity', 'Human System', 'Visual Stimuli', 'Visual Cortex', 'fMRI Data', 'fMRI Signal', 'Depth Perception', 'Reverse Pathway', 'Color Cues', 'Visual Association Cortex', 'Image Reconstruction', 'Data Augmentation', 'Generative Adversarial Networks', 'Diffusion Model', 'Visual Methods', 'Field Of Practice', 'Ground Truth Depth', 'Forward Process', 'Text Encoder', 'Depth Map', 'Color Palette', 'High-level Semantics', 'Lateral Geniculate Nucleus', 'Scene Structure', 'Self-supervised Learning', 'RGB-D Data']","['Applications', 'Psychology and cognitive science', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",3,"In this work we present DREAM, an fMRI-to-image method for reconstructing viewed images from brain activities, grounded on fundamental knowledge of the human visual system. We craft reverse pathways that emulate the hierarchical and parallel nature of how humans perceive the visual world. These tailored pathways are specialized to decipher semantics, color, and depth cues from fMRI data, mirroring the forward pathways from visual stimuli to fMRI recordings. To do so, two components mimic the inverse processes within the human visual system: the Reverse Visual Association Cortex (R-VAC) which reverses pathways of this brain region, extracting semantics from fMRI data; the Reverse Parallel PKM (R-PKM) component simultaneously predicting color and depth from fMRI signals. The experiments indicate that our method outperforms the current state-of-the-art models in terms of the consistency of appearance, structure, and semantics. Code will be available at https://github.com/weihaox/DREAM."
DTrOCR: Decoder-Only Transformer for Optical Character Recognition,Masato Fujitake,"FA Research, Fast Accounting Co., Ltd. Japan",0.0,,100.0,Japan,"Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese.",https://openaccess.thecvf.com/content/WACV2024/html/Fujitake_DTrOCR_Decoder-Only_Transformer_for_Optical_Character_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fujitake_DTrOCR_Decoder-Only_Transformer_for_Optical_Character_Recognition_WACV_2024_paper.pdf,,,2308.15996,main,Poster,https://ieeexplore.ieee.org/document/10483819/,"['Computer vision', 'Image recognition', 'Text recognition', 'Computational modeling', 'Optical character recognition', 'Linguistics', 'Transformers']","['Optical Character Recognition', 'Language Model', 'Large Corpus', 'English Text', 'Encoder-decoder Structure', 'Benchmark', 'Convolutional Neural Network', 'Input Image', 'Simple Structure', 'Recurrent Neural Network', 'Attention Mechanism', 'Image Information', 'Real-world Datasets', 'Transformer Model', 'Language Knowledge', 'Image Texture', 'Linguistic Information', 'Natural Language Processing Tasks', 'Vision Transformer', 'Word Tokens', 'Masked Language Model', 'Text Sequence', 'Natural Scene Images', 'Sequence Of Tokens', 'Special Token', 'Standard Text', 'Input Text', 'Street View', 'Pre-trained Language Models']","['Applications', 'Commercial / retail', 'Applications', 'Autonomous Driving']",12,"Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese."
Data Augmentation for Object Detection via Controllable Diffusion Models,"Haoyang Fang, Boran Han, Shuai Zhang, Su Zhou, Cuixiong Hu, Wen-Ming Ye","AWS AI, Bellevue, US",0.0,,100.0,USA,"Data augmentation is vital for object detection tasks that require expensive bounding box annotations. Recent successes in diffusion models have inspired the use of diffusion-based synthetic images for data augmentation. However, existing works have primarily focused on image classification, and their applicability to boost object detection's performance remains unclear. To address this gap, we propose a data augmentation pipeline based on controllable diffusion models and CLIP. Our approach involves generating appropriate visual priors to control the generation of synthetic data and implementing post-filtering techniques using category-calibrated CLIP scores. The evaluation of our approach is conducted under few-shot settings in MSCOCO, full PASCAL VOC dataset, and selected downstream datasets. We observe the performance increase using our augmentation pipeline. Specifically, the mAP improvement is +18.0%/+15.6%/+15.9% for COCO 5/10/30-shot, +2.9% on full PASCAL VOC dataset, and +12.4% on average for selected downstream datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Fang_Data_Augmentation_for_Object_Detection_via_Controllable_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fang_Data_Augmentation_for_Object_Detection_via_Controllable_Diffusion_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484172/,"['Visualization', 'Computer vision', 'Annotations', 'Pipelines', 'Object detection', 'Data augmentation', 'Data models']","['Object Detection', 'Data Augmentation', 'Diffusion Model', 'Image Classification', 'Use Of Imaging', 'Bounding Box', 'Synthetic Images', 'Object Detection Task', 'Bounding Box Annotations', 'PASCAL VOC Dataset', 'Segmentation Model', 'Large Margin', 'Scribble', 'Segmentation Dataset', 'Data Augmentation Methods', 'Category Names', 'Object Appearance', 'COCO Dataset', 'Object Detection Model', 'Canny Edge', 'Object Detection Dataset']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",4,"Data augmentation is vital for object detection tasks that require expensive bounding box annotations. Recent successes in diffusion models have inspired the use of diffusion-based synthetic images for data augmentation. However, existing works have primarily focused on image classification, and their applicability to boost object detection’s performance remains unclear. To address this gap, we propose a data augmentation pipeline based on controllable diffusion models and CLIP. Our approach involves generating appropriate visual priors to control the generation of synthetic data and implementing post-filtering techniques using category-calibrated CLIP scores. The evaluation of our approach is conducted under few-shot settings in MSCOCO, full PASCAL VOC dataset, and selected downstream datasets. We observe the performance increase using our augmentation pipeline. Specifically, the mAP improvement is +18.0%/+15.6%/+15.9% for COCO 5/10/30-shot, +2.9% on full PASCAL VOC dataset, and +12.4% on average for selected downstream datasets."
Data-Centric Debugging: Mitigating Model Failures via Targeted Image Retrieval,"Sahil Singla, Atoosa Malemir Chegini, Mazda Moayeri, Soheil Feizi",University of Maryland; Google Research,50.0,USA,50.0,USA,"Deep neural networks can be unreliable in the real world when the training set does not adequately cover all the settings where they are deployed. Focusing on image classification, we consider the setting where we have an error distribution E representing a deployment scenario where the model fails. We have access to a small set of samples E_sample from E and it can be expensive to obtain additional samples. In the traditional model development framework, mitigating failures of the model in E can be challenging and is often done in an ad hoc manner. In this paper, we propose a general methodology for model debugging that can systemically improve model performance on E while maintaining its performance on the original test set. Our key assumption is that we have access to a large pool of weakly (noisily) labeled data F. However, naively adding F to the training would hurt model performance due to the large extent of label noise. Our Data-Centric Debugging (DCD) framework carefully creates a debug-train set by selecting images from F that are perceptually similar to the images in E_sample. To do this, we use the l_2 distance in the feature space (penultimate layer activations) of various models including ResNet, Robust ResNet and DINO where we observe DINO ViTs are significantly better at discovering similar images compared to Resnets. Compared to the baselines that maintain model performance on the test set, we achieve significantly (+9.45%) improved results on the debug-heldout sets.",https://openaccess.thecvf.com/content/WACV2024/html/Singla_Data-Centric_Debugging_Mitigating_Model_Failures_via_Targeted_Image_Retrieval_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Singla_Data-Centric_Debugging_Mitigating_Model_Failures_via_Targeted_Image_Retrieval_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484014/,"['Training', 'Computer vision', 'Computational modeling', 'Noise', 'Image retrieval', 'Focusing', 'Debugging']","['Debugging', 'Image Retrieval', 'Model Performance', 'Training Set', 'Deep Neural Network', 'Feature Space', 'Error Distribution', 'Similar Images', 'Improve Model Performance', 'Visual Similarity', 'Penultimate Layer', 'Original Test Set', 'Training Data', 'Classification Accuracy', 'Validation Set', 'Similarity Measure', 'Active Learning', 'Similar Procedure', 'Noisy Data', 'Failure Modes', 'Semi-supervised Learning', 'Weak Labels', 'Similarity Matching', 'Image X', 'Visual Matching', 'Image Classification Problems', 'Visual Distance', 'Disjoint Sets', 'Noisy Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Datasets and evaluations']",,"Deep neural networks can be unreliable in the real world when the training set does not adequately cover all the settings where they are deployed. Focusing on image classification, we consider the setting where we have an error distribution $\mathcal{E}$ representing a deployment scenario where the model fails. We have access to a small set of samples ${\mathcal{E}_{{\text{sample}}}}$ from $\mathcal{E}$ and it can be expensive to obtain additional samples. In the traditional model development framework, mitigating failures of the model in $\mathcal{E}$ can be challenging and is often done in an ad hoc manner. In this paper, we propose a general methodology for model debugging that can systemically improve model performance on $\mathcal{E}$ while maintaining its performance on the original test set. Our key assumption is that we have access to a large pool of weakly (noisily) labeled data $\mathcal{F}$. However, naively adding $\mathcal{F}$ to the training would hurt model performance due to the large extent of label noise. Our Data-Centric Debugging (DCD) framework carefully creates a debug-train set by selecting images from $\mathcal{F}$ that are visually similar to the images in ${\mathcal{E}_{{\text{sample}}}}$. To do this, we use the ℓ
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf>
 distance in the feature space (penultimate layer activations) of various models including ResNet, Robust ResNet and DINO where we observe DINO ViTs are significantly better at discovering similar images compared to Resnets. Compared to the baselines that maintain model performance on the test set, we achieve significantly (+9.45%) improved results on the debug-heldout sets."
DeVos: Flow-Guided Deformable Transformer for Video Object Segmentation,"Volodymyr Fedynyak, Yaroslav Romanus, Bohdan Hlovatskyi, Bohdan Sydor, Oles Dobosevych, Igor Babin, Roman Riazantsev","ADV A Soft; Ukrainian Catholic University; Ukrainian Catholic University, ADV A Soft",66.66666666666666,Ukraine,33.33333333333334,USA,"The recent works on Video Object Segmentation achieved remarkable results by matching dense semantic and instance-level features between the current and previous frames for long-time propagation. Nevertheless, global feature matching ignores scene motion context, failing to satisfy temporal consistency. Even though some methods introduce local matching branch to achieve smooth propagation, they fail to model complex appearance changes due to the constraints of the local window. In this paper, we present DeVOS (Deformable VOS), an architecture for Video Object Segmentation that combines memory-based matching with motion-guided propagation resulting in stable long-term modeling and strong temporal consistency. For short-term local propagation, we propose a novel attention mechanism ADVA (Adaptive Deformable Video Attention), allowing the adaption of similarity search region to query-specific semantic features, which ensures robust tracking of complex shape and scale changes. DeVOS employs an optical flow to obtain scene motion features which are further injected to deformable attention as strong priors to learnable offsets. Our method achieves top-rank performance on DAVIS 2017 val and test-dev (88.1%, 83.0%), YouTube-VOS 2019 val (86.6%) while featuring consistent run-time speed and stable memory consumption.",https://openaccess.thecvf.com/content/WACV2024/html/Fedynyak_DeVos_Flow-Guided_Deformable_Transformer_for_Video_Object_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fedynyak_DeVos_Flow-Guided_Deformable_Transformer_for_Video_Object_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484285/,"['Deformable models', 'Computer vision', 'Adaptation models', 'Tracking', 'Shape', 'Semantics', 'Memory management']","['Video Object Segmentation', 'Deformation Transformation', 'Shape Changes', 'Attention Mechanism', 'Scale Changes', 'Semantic Features', 'Changes In Appearance', 'Optical Flow', 'Motion Features', 'Strong Consistency', 'Current Frame', 'Previous Frame', 'Temporal Consistency', 'Search Region', 'Global Matching', 'Benchmark', 'Long-term Memory', 'Feature Maps', 'Latent Space', 'Video Sequences', 'Motion Information', 'Optical Flow Estimation', 'Query Features', 'Memory Bank', 'Past Frames', 'Multi-scale Feature Maps', 'Semantic Matching', 'Global Motion', 'Linear Projection', 'Flow Estimation']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Remote Sensing']",1,"The recent works on Video Object Segmentation achieved remarkable results by matching dense semantic and instance-level features between the current and previous frames for long-time propagation. Nevertheless, global feature matching ignores scene motion context, failing to satisfy temporal consistency. Even though some methods introduce local matching branch to achieve smooth propagation, they fail to model complex appearance changes due to the constraints of the local window. In this paper, we present DeVOS (Deformable VOS), an architecture for Video Object Segmentation that combines memory-based matching with motion-guided propagation resulting in stable long-term modeling and strong temporal consistency. For short-term local propagation, we propose a novel attention mechanism ADVA (Adaptive Deformable Video Attention), allowing the adaption of similarity search region to query-specific semantic features, which ensures robust tracking of complex shape and scale changes. DeVOS employs an optical flow to obtain scene motion features which are further injected to deformable attention as strong priors to learnable offsets. Our method achieves top-rank performance on DAVIS 2017 val and test-dev (88.1%, 83.0%), YouTube-VOS 2019 val (86.6%) while featuring consistent run-time speed and stable memory consumption."
"Debiasing, Calibrating, and Improving Semi-Supervised Learning Performance via Simple Ensemble Projector",Khanh-Binh Nguyen,"Sungkyunkwan University, South Korea",100.0,South Korea,0.0,,"Recent studies on semi-supervised learning (SSL) have achieved great success. Despite their promising performance, current state-of-the-art methods tend toward increasingly complex designs at the cost of introducing more network components and additional training procedures. In this paper, we propose a simple method named Ensemble Projectors Aided for Semi-supervised Learning (EPASS), which focuses mainly on improving the learned embeddings to boost the performance of the existing contrastive joint-training semi-supervised learning frameworks. Unlike standard methods, where the learned embeddings from one projector are stored in memory banks to be used with contrastive learning, EPASS stores the ensemble embeddings from multiple projectors in memory banks. As a result, EPASS improves generalization, strengthens feature representation, and boosts performance. For instance, EPASS improves strong baselines for semi-supervised learning by 39.47%/31.39%/24.70% top-1 error rate, while using only 100k/1%/10% of labeled data for SimMatch, and achieves 40.24%/32.64%/25.90% top-1 error rate for CoMatch on the ImageNet dataset. These improvements are consistent across methods, network architectures, and datasets, proving the general effectiveness of the proposed methods.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Debiasing_Calibrating_and_Improving_Semi-Supervised_Learning_Performance_via_Simple_Ensemble_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_Debiasing_Calibrating_and_Improving_Semi-Supervised_Learning_Performance_via_Simple_Ensemble_WACV_2024_paper.pdf,,https://github.com/beandkay/EPASS,2310.15764,main,Poster,https://ieeexplore.ieee.org/document/10484268/,"['Training', 'Computer vision', 'Costs', 'Error analysis', 'Pipelines', 'Self-supervised learning', 'Semisupervised learning']","['Semi-supervised Learning', 'Debiasing', 'Error Rate', 'Self-supervised Learning', 'ImageNet Dataset', 'Embedding Learning', 'Memory Bank', 'Learning Rate', 'Hyperparameters', 'Performance Of Method', 'Room For Improvement', 'Large-scale Datasets', 'Speech Recognition', 'Multilayer Perceptron', 'Representation Learning', 'Semantic Similarity', 'Unlabeled Data', 'Confirmation Bias', 'Semantic Level', 'Little Room For Improvement', 'Vision Transformer', 'Classification Head', 'Imbalance Ratio']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding']",,"Recent studies on semi-supervised learning (SSL) have achieved great success. Despite their promising performance, current state-of-the-art methods tend toward increasingly complex designs at the cost of introducing more network components and additional training procedures. In this paper, we propose a simple method named Ensemble Projectors Aided for Semi-supervised Learning (EPASS), which focuses mainly on improving the learned embeddings to boost the performance of the existing contrastive joint-training semi-supervised learning frameworks. Unlike standard methods, where the learned embeddings from one projector are stored in memory banks to be used with contrastive learning, EPASS stores the ensemble embeddings from multiple projectors in memory banks. As a result, EPASS improves generalization, strengthens feature representation, and boosts performance. For instance, EPASS improves strong baselines for semi-supervised learning by 39.47%/31.39%/24.70% top-1 error rate, while using only 100k/1%/10% of labeled data for SimMatch, and achieves 40.24%/32.64%/25.90% top-1 error rate for CoMatch on the ImageNet dataset. These improvements are consistent across methods, network architectures, and datasets, proving the general effectiveness of the proposed methods. Code is available at https://github.com/beandkay/EPASS."
Deblur-NSFF: Neural Scene Flow Fields for Blurry Dynamic Scenes,"Achleshwar Luthra, Shiva Souhith Gantha, Xiyun Song, Heather Yu, Zongfang Lin, Liang Peng",Carnegie Mellon University; Georgia Institute of Technology; Futurewei Technologies,100.0,USA,0.0,,"In this work, we present a method to address the problem of novel view and time synthesis of complex dynamic scenes considering the input video is subject to blurriness caused due to camera or object motion or out-of-focus blur. Neural Scene Flow Field (NSFF) has shown remarkable results by training a dynamic NeRF to capture motion in the scene, but this method is not robust to unstable camera handling which can lead to blurred renderings. We propose Deblur-NSFF, a method that learns spatially-varying blur kernels to simulate the blurring process and gradually learns a sharp time-conditioned NeRF representation. We describe how to optimize our representation for sharp space-time view synthesis. Given blurry input frames, we perform both quantitative and qualitative comparison with state-of-the-art methods on modified NVIDIA Dynamic Scene dataset. We also compare our method with Deblur-NeRF, a method that has been designed to handle blur in static scenes. The demonstrated results show that our method outperforms prior work.",https://openaccess.thecvf.com/content/WACV2024/html/Luthra_Deblur-NSFF_Neural_Scene_Flow_Fields_for_Blurry_Dynamic_Scenes_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Luthra_Deblur-NSFF_Neural_Scene_Flow_Fields_for_Blurry_Dynamic_Scenes_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483942/,"['Training', 'Interpolation', 'Computer vision', 'Dynamics', 'Cameras', 'Rendering (computer graphics)', 'Proposals']","['Dynamic Scenes', 'Neural Field', 'Scene Flow', 'Camera Motion', 'Synthesis Problem', 'Static Scenes', 'Dynamic Datasets', 'View Synthesis', 'Blur Kernel', 'Input Image', '3D Space', 'Multilayer Perceptron', 'Regional Dynamics', 'Ablation Experiments', 'Color Values', 'Blue Box', 'Kernel Estimation', 'Volumetric Density', '3D Scene', 'Camera Pose', 'Temporal Consistency', 'Blurry Images', 'PSNR Values', 'Great Job', 'Scene Regions', 'Temporal Loss']","['Algorithms', '3D computer vision', 'Applications', 'Virtual / augmented reality']",,"In this work, we present a method to address the problem of novel view and time synthesis of complex dynamic scenes considering the input video is subject to blurriness caused due to camera or object motion or out-of-focus blur. Neural Scene Flow Field (NSFF) has shown remarkable results by training a dynamic NeRF to capture motion in the scene, but this method is not robust to unstable camera handling which can lead to blurred renderings. We propose Deblur-NSFF, a method that learns spatially-varying blur kernels to simulate the blurring process and gradually learns a sharp time-conditioned NeRF representation. We describe how to optimize our representation for sharp space-time view synthesis. Given blurry input frames, we perform both quantitative and qualitative comparison with state-of-the-art methods on modified NVIDIA Dynamic Scene dataset. We also compare our method with Deblur-NeRF, a method that has been designed to handle blur in static scenes. The demonstrated results show that our method outperforms prior work."
Deep Image Fingerprint: Towards Low Budget Synthetic Image Detection and Model Lineage Analysis,"Sergey Sinitsa, Ohad Fried",Reichman University,100.0,Israel,0.0,,"The generation of high-quality images has become widely accessible and is a rapidly evolving process. As a result, anyone can generate images that are indistinguishable from real ones. This leads to a wide range of applications, including malicious usage with deceptive intentions. Despite advances in detection techniques for generated images, a robust detection method still eludes us. Furthermore, model personalization techniques might affect the detection capabilities of existing methods. In this work, we utilize the architectural properties of convolutional neural networks (CNNs) to develop a new detection method. Our method can detect images from a known generative model and enable us to establish relationships between fine-tuned generative models. We tested the method on images produced by both Generative Adversarial Networks (GANs) and recent large text-to-image models (LTIMs) that rely on Diffusion Models. Our approach outperforms others trained under identical conditions and achieves comparable performance to state-of-the-art pre-trained detection methods on images generated by Stable Diffusion and MidJourney, with significantly fewer required train samples.",https://openaccess.thecvf.com/content/WACV2024/html/Sinitsa_Deep_Image_Fingerprint_Towards_Low_Budget_Synthetic_Image_Detection_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sinitsa_Deep_Image_Fingerprint_Towards_Low_Budget_Synthetic_Image_Detection_and_WACV_2024_paper.pdf,https://sergo2020.github.io/DIF/,,2303.10762,main,Poster,https://ieeexplore.ieee.org/document/10483843/,"['Computer vision', 'Analytical models', 'Computational modeling', 'Fingerprint recognition', 'Generative adversarial networks', 'Convolutional neural networks']","['Deep Image', 'Convolutional Neural Network', 'Generative Adversarial Networks', 'Diffusion Model', 'Image Generation', 'High-quality Images', 'Fine-tuned Model', 'Pre-training Method', 'Robust Detection Method', 'Training Set', 'Deep Neural Network', 'Detection Accuracy', 'Convolutional Layers', 'Fast Fourier Transform', 'Detection In Images', 'Data-driven Methods', 'Source Model', 'Image Domain', 'Lowest Accuracy', 'Generative Adversarial Networks Model', 'Rule-based Methods', 'Spectrum Domain', 'Blurred Images', 'Image Compression', 'Image Resampling', 'Weight Initialization']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",4,"The generation of high-quality images has become widely accessible and is a rapidly evolving process. As a result, anyone can generate images that are indistinguishable from real ones. This leads to a wide range of applications, including malicious usage with deceptive intentions. Despite advances in detection techniques for generated images, a robust detection method still eludes us. Furthermore, model personalization techniques might affect the detection capabilities of existing methods. In this work, we utilize the architectural properties of convolutional neural networks (CNNs) to develop a new detection method. Our method can detect images from a known generative model and enable us to establish relationships between fine-tuned generative models. We tested the method on images produced by both Generative Adversarial Networks (GANs) and recent large text-to-image models (LTIMs) that rely on Diffusion Models. Our approach outperforms others trained under identical conditions and achieves comparable performance to state-of-the-art pre-trained detection methods on images generated by Stable Diffusion and MidJourney, with significantly fewer required train samples."
Deep Metric Learning With Chance Constraints,"Yeti Z. Gürbüz, Oğul Can, Aydin Alatan",1MetaDialog2Cerebrate AI3OGAM and METU; 1MetaDialog; 3OGAM and METU,66.66666666666666,Turkey,33.33333333333334,USA,"Deep metric learning (DML) aims to minimize empirical expected loss of the pairwise intra-/inter- class proximity violations in the embedding space. We relate DML to feasibility problem of finite chance constraints. We show that minimizer of proxy-based DML satisfies certain chance constraints, and that the worst case generalization performance of the proxy-based methods can be characterized by the radius of the smallest ball around a class proxy to cover the entire domain of the corresponding class samples, suggesting multiple proxies per class helps performance. To provide a scalable algorithm as well as exploiting more proxies, we consider the chance constraints implied by the minimizers of proxy-based DML instances and reformulate DML as finding a feasible point in intersection of such constraints, resulting in a problem to be approximately solved by iterative projections. Simply put, we repeatedly train a regularized proxy-based loss and re-initialize the proxies with the embeddings of the deliberately selected new samples. We applied our method with 4 well-accepted DML losses and show the effectiveness with extensive evaluations on 4 popular DML benchmarks. Code is available at: https://github.com/yetigurbuz/ccp-dml",https://openaccess.thecvf.com/content/WACV2024/html/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gurbuz_Deep_Metric_Learning_With_Chance_Constraints_WACV_2024_paper.pdf,,https://github.com/yetigurbuz/ccp-dml,,main,Poster,https://ieeexplore.ieee.org/document/10484253/,"['Measurement', 'Greedy algorithms', 'Computer vision', 'Codes', 'Benchmark testing', 'Approximation algorithms', 'Iterative algorithms']","['Metric Learning', 'Deep Metric Learning', 'Latent Space', 'Feasible Point', 'Feasibility Problem', 'Random Sampling', 'Convolutional Neural Network', 'Pairwise Distances', 'Finite Set', 'Surgical Margins', 'Intersection Point', 'Representation Of Space', 'Lipschitz Continuous', 'Large-scale Problems', 'Loss Term', 'Training Error', 'Generalization Error', 'Contrastive Loss', 'Intersection Set', 'Proxy For The Number', 'Empirical Loss', 'Subset Of Pairs', 'Hinge Loss', 'Unseen Classes']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Deep metric learning (DML) aims to minimize empirical expected loss of the pairwise intra-/inter- class proximity violations in the embedding space. We relate DML to feasibility problem of finite chance constraints. We show that minimizer of proxy-based DML satisfies certain chance constraints, and that the worst case generalization performance of the proxy-based methods can be characterized by the radius of the smallest ball around a class proxy to cover the entire domain of the corresponding class samples, suggesting multiple proxies per class helps performance. To provide a scalable algorithm as well as exploiting more proxies, we consider the chance constraints implied by the minimizers of proxy-based DML instances and reformulate DML as finding a feasible point in intersection of such constraints, resulting in a problem to be approximately solved by iterative projections. Simply put, we repeatedly train a regularized proxy-based loss and re-initialize the proxies with the embeddings of the deliberately selected new samples. We applied our method with 4 well-accepted DML losses and show the effectiveness with extensive evaluations on 4 popular DML benchmarks. Code is available at: https://github.com/yetigurbuz/ccp-dml"
Deep Optics for Optomechanical Control Policy Design,Justin Fletcher,University of Hawai ‘i at Mānoa,100.0,USA,0.0,,"An emerging class of Fizeau optical telescopes have the potential to upend prior cost scaling models, substantially improving the angular resolution and contrast attainable by ground-based astronomical instruments. However, this design introduces a challenging visual control problem that must be solved to compensate for wavefront aberrations induced by the flexible substructure it employs. We subvert this problem with a deep optics approach to policy design and image recovery that exploits, rather than corrects, aberrations to obtain domain-specific object recovery performance exceeding that of more costly filled aperture designs.",https://openaccess.thecvf.com/content/WACV2024/html/Fletcher_Deep_Optics_for_Optomechanical_Control_Policy_Design_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fletcher_Deep_Optics_for_Optomechanical_Control_Policy_Design_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484489/,"['Optical losses', 'Visualization', 'Biomedical optical imaging', 'Image resolution', 'Information processing', 'Apertures', 'Telescopes']","['Optomechanical Control', 'Control Problem', 'Angular Resolution', 'Image Recovery', 'Actuator', 'Focal Plane', 'Single Object', 'Image Formation', 'Peak Signal-to-noise Ratio', 'Point Spread Function', 'Recovery Model', 'Planning Model', 'Optical Model', 'Low Earth Orbit', 'Objective Observation', 'Modulation Transfer Function', 'Object Plane', 'Adaptive Optics', 'Atmospheric Turbulence', 'Task Loss', 'iNaturalist', 'Image Formation Process', 'Deformable Mirror', 'Filter Scale', 'Exoplanets', 'Optical System', 'Shot Noise', 'Key Insights', 'Learning Control', 'Screening Phase']","['Applications', 'Remote Sensing', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"An emerging class of Fizeau optical telescopes have the potential to upend prior cost scaling models, substantially improving the angular resolution and contrast attainable by ground-based astronomical instruments. However, this design introduces a challenging visual control problem that must be solved to compensate for wavefront aberrations induced by the flexible substructure it employs. We subvert this problem with a deep optics approach to policy design and image recovery that exploits, rather than corrects, aberrations to obtain domain-specific object recovery performance exceeding that of more costly filled aperture designs."
Deep Plug-and-Play Nighttime Non-Blind Deblurring With Saturated Pixel Handling Schemes,"Hung-Yu Shu, Yi-Hsien Lin, Yi-Chang Lu",National Taiwan University,100.0,Taiwan,0.0,,"Due to the setting of shutter speeds, over-exposed blurry images can often be seen in nighttime photography. Although image deblurring is a classic problem in image restoration, state-of-the-art methods often fail in nighttime cases with saturated pixels. The primary reason is that those pixels are out of the sensor range and thus violate the assumption of the linear blur model. To address this issue, we propose a new nighttime non-blind deblurring algorithm with saturated pixel handling schemes, including a pixel stretching mask, an image segment mask, and a saturation awareness mechanism (SAM). Our algorithm achieves superior results by strategically adjusting mask configurations, making our method robust to various saturation levels. We formulate our task into two new optimization problems and introduce a unified framework based on the plug-and-play alternating direction method of multipliers (PnP-ADMM). We also evaluate our approach qualitatively and quantitatively to demonstrate its effectiveness. The results show that the proposed algorithm recovers sharp latent images with finer details and fewer artifacts than other state-of-the-art deblurring methods.",https://openaccess.thecvf.com/content/WACV2024/html/Shu_Deep_Plug-and-Play_Nighttime_Non-Blind_Deblurring_With_Saturated_Pixel_Handling_Schemes_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shu_Deep_Plug-and-Play_Nighttime_Non-Blind_Deblurring_With_Saturated_Pixel_Handling_Schemes_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484520/,"['Photography', 'Image segmentation', 'Computer vision', 'Benchmark testing', 'Image restoration', 'Task analysis', 'Optimization']","['Saturated Pixels', 'Optimization Problem', 'Fine Details', 'Image Sharpness', 'Blurry Images', 'Fewer Artifacts', 'Image Deblurring', 'Latent Image', 'Supplemental Material', 'Deconvolution', 'Scaling Factor', 'Learning-based Methods', 'Regularization Term', 'Projection Operator', 'Image Texture', 'Hadamard Product', 'Degree Of Saturation', 'Non-negativity Constraints', 'Blurred Images', 'Severe Artifacts', 'Saturation Region', 'Blur Kernel', 'Proximal Operator', 'Ring Artifact', 'Optimization-based Methods', 'Night-time Images', 'Maximum Pixel Value', 'Guided Filter', 'Textual Patterns', 'Hard Threshold']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Due to the setting of shutter speeds, over-exposed blurry images can often be seen in nighttime photography. Although image deblurring is a classic problem in image restoration, state-of-the-art methods often fail in nighttime cases with saturated pixels. The primary reason is that those pixels are out of the sensor range and thus violate the assumption of the linear blur model. To address this issue, we propose a new nighttime non-blind deblurring algorithm with saturated pixel handling schemes, including a pixel stretching mask, an image segment mask, and a saturation awareness mechanism (SAM). Our algorithm achieves superior results by strategically adjusting mask configurations, making our method robust to various saturation levels. We formulate our task into two new optimization problems and introduce a unified framework based on the plug-and-play alternating direction method of multipliers (PnP-ADMM). We also evaluate our approach qualitatively and quantitatively to demonstrate its effectiveness. The results show that the proposed algorithm recovers sharp latent images with finer details and fewer artifacts than other state-of-the-art deblurring methods."
Deep Subdomain Alignment for Cross-Domain Image Classification,"Yewei Zhao, Hu Han, Shiguang Shan, Xilin Chen","Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China",100.0,China,0.0,,"Unsupervised domain adaptation (UDA), which aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain, is useful for various cross-domain image classification scenarios. A commonly used approach for UDA is to minimize the distribution differences between two domains, and subdomain alignment is found to be an effective method. However, most of the existing subdomain alignment methods are based on adversarial learning and focus on subdomain alignment procedures without considering the discriminability among individual subdomains, resulting in slow convergence and unsatisfactory adaptation results. To address these issues, we propose a novel deep subdomain alignment method for UDA in image classification, which consists of a Union Subdomain Contrastive Learning (USCL) module and a Multi-view Subdomain Alignment (MvSA) strategy. USCL can create discriminative and dispersed subdomains by bringing samples from the same subdomain closer while pushing away samples from different subdomains. MvSA makes use of labeled source domain data and easy target domain data to perform target-to-source and target-to-target alignment. Experimental results on three image classification datasets (Office-31, Office-Home, Visda-17) demonstrate that our proposed method is effective for UDA and achieves promising results in several cross-domain image classification tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Deep_Subdomain_Alignment_for_Cross-Domain_Image_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_Deep_Subdomain_Alignment_for_Cross-Domain_Image_Classification_WACV_2024_paper.pdf,,https://github.com/zhaoyewei/DSACDIC,,main,Poster,https://ieeexplore.ieee.org/document/10484059/,"['Computer vision', 'Codes', 'Self-supervised learning', 'Hilbert space', 'Adversarial machine learning', 'Task analysis', 'Kernel']","['Image Classification', 'Cross-domain Image Classification', 'Generative Adversarial Networks', 'Learning Module', 'Target Domain', 'Domain Adaptation', 'Source Domain', 'Self-supervised Learning', 'Easy Target', 'Alignment Strategy', 'Target Domain Data', 'Unlabeled Target Domain', 'Labeled Source Domain', 'Deep Learning', 'Deep Network', 'Target Sample', 'Distinct Domains', 'Labeled Samples', 'Decision Boundary', 'Source Characteristics', 'Easy Samples', 'Source Domain Samples', 'Maximum Mean Discrepancy', 'Reproducing Kernel Hilbert Space', 'Target Domain Samples', 'Memory Bank', 'Source Distribution', 'Alignment Loss', 'Early Stage Of Training', 'Pseudo Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Unsupervised domain adaptation (UDA), which aims to transfer knowledge learned from a labeled source domain to an unlabeled target domain, is useful for various cross-domain image classification scenarios. A commonly used approach for UDA is to minimize the distribution differences between two domains, and subdomain alignment is found to be an effective method. However, most of the existing subdomain alignment methods are based on adversarial learning and focus on subdomain alignment procedures without considering the discriminability among individual subdomains, resulting in slow convergence and unsatisfactory adaptation results. To address these issues, we propose a novel deep subdomain alignment method for UDA in image classification, which consists of a Union Subdo-main Contrastive Learning (USCL) module and a Multi-view Subdomain Alignment (MvSA) strategy. USCL can create discriminative and dispersed subdomains by bringing samples from the same subdomain closer while pushing away samples from different subdomains. MvSA makes use of labeled source domain data and easy target domain data to perform target-to-source and target-to-target alignment. Experimental results on three image classifi-cation datasets (Office-31, Office-Home, Visda-17) demonstrate that our proposed method is effective for UDA and achieves promising results in several cross-domain image classification tasks. Our code will be available: https://github.com/zhaoyewei/DSACDIC."
Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species,"Tayfun Karaderi, Tilo Burghardt, Raphaël Morard, Daniela N. Schmidt","MARUM, University of Bremen; School of Earth Sciences, University of Bristol; Dept of Computer Science, University of Bristol",100.0,"Germany, UK",0.0,,"Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic learning as a new application domain with the aim to implicitly encode cross-modality associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR), particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest species. Technically, we pre-train a visual ResNet50 deep learning model using triplet loss formulations to create an initial embedding space. We re-structure this space based on genetic anchors embedded via a Sequence Graph Transform (SGT) and linked to visual data by cross-domain cosine alignment. We show that an LTR approach improves the state-of-the-art across all benchmarks and that adding our visual-genetic alignment improves per-class and particularly rare tail class benchmarks significantly further. We conclude that visual-genetic alignment can be a highly effective tool for complementing visual biological data containing rare classes. The concept proposed may serve as an important future tool for integrating genetics and imageomics towards a more complete scientific representation of taxonomic spaces and life itself. Code, weights, and data splits are published for full reproducibility.",https://openaccess.thecvf.com/content/WACV2024/html/Karaderi_Deep_Visual-Genetic_Biometrics_for_Taxonomic_Classification_of_Rare_Species_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Karaderi_Deep_Visual-Genetic_Biometrics_for_Taxonomic_Classification_of_Rare_Species_WACV_2024_paper.pdf,,,2305.06695,main,Poster,https://ieeexplore.ieee.org/document/10483640/,"['Training', 'Visualization', 'Biometrics (access control)', 'Transforms', 'Tail', 'Benchmark testing', 'Genetics']","['Biometric', 'Rare Species', 'Deep Learning', 'Data Visualization', 'Genetic Data', 'Latent Space', 'Visual Model', 'Species Recognition', 'Visual Classification', 'Triplet Loss', 'Rare Classes', 'Deep Embedding', 'Genetic Information', 'Softmax', 'Transfer Learning', 'Information Transfer', 'Shared Space', 'Embedding Vectors', 'Image Descriptors', 'Metric Learning', 'Genetic Sources', 'Planktonic Foraminifera', 'Microfossils', 'Projection Layer', 'ResNet-50 Model', 'Weight Balance']","['Applications', 'Animals / Insects', 'Algorithms', 'Vision + language and/or other modalities']",1,"Visual as well as genetic biometrics are routinely employed to identify species and individuals in biological applications. However, no attempts have been made in this domain to computationally enhance visual classification of rare classes with little image data via genetics. In this paper, we thus propose aligned visual-genetic learning as a new application domain with the aim to implicitly encode cross-modality associations for improved performance. We demonstrate for the first time that such alignment can be achieved via deep embedding models and that the approach is directly applicable to boosting long-tailed recognition (LTR), particularly for rare species. We experimentally demonstrate the efficacy of the concept via application to microscopic imagery of 30k+ planktic foraminifer shells across 32 species when used together with independent genetic data samples. Most importantly for practitioners, we show that visual-genetic alignment can significantly benefit visual-only recognition of the rarest species. Technically, we pre-train a visual ResNet50 deep learning model using triplet loss formulations to create an initial embedding space. We re-structure this space based on genetic anchors embedded via a Sequence Graph Transform (SGT) and linked to visual data by cross-domain cosine alignment. We show that an LTR approach improves the state-of-the-art across all benchmarks and that adding our visual-genetic alignment improves per-class and particularly rare tail class benchmarks significantly further. Overall, visual-genetic LTR training raises rare per-class accuracy from 37.4% to benchmark-beating 59.7%. We conclude that visual-genetic alignment can be a highly effective tool for complementing visual biological data containing rare classes. The concept proposed may serve as an important future tool for integrating genetics and imageomics towards a more complete scientific representation of taxonomic spaces and life itself. Code, weights, and data splits are published for full reproducibility."
Defending Object Detection Models Against Image Distortions,"Mark Ofori-Oduro, Maria Amer","Department of Electrical and Computer Engineering, Concordia University, Montr´eal, Canada",100.0,Canada,0.0,,"Image distortions pose a significant challenge to object detection. To address this issue, our paper introduces a novel data augmentation method that generates new samples resembling the original training images. The new sample exhibits randomly altered pixels based on a pixel distribution obtained from multiple image distortions using kernel density estimation (KDE). The main steps of our method, GSES, are generating distorted versions of each pixel of an original training image, selecting a set of pixels in each version, and then, for each selected pixel, estimating its distribution using KDE and then sampling one pixel from this distribution. By employing this approach, the new samples possess distorted pixels while maintaining a certain degree of similarity to the original image. This degree of similarity is essential to balance the accuracy of object detection models under distorted and clean images. Our approach improves the accuracy of different object detection models under 15 image distortions, such as motion blur, fog, and noise. For example, the average accuracy of YOLOv4 improves by 9.19% and 9.54% across all 15 distortions added to the COCO and PASCAL datasets, respectively. Our method surpasses other defence methods to combat image distortions. Our ablation and stability studies show why our method performs well. Moreover, we also show that our method can be well used to improve the accuracy of image classification under 15 distortions and cross-domains. Our code is available at https://github.com/moforio/GSES/.",https://openaccess.thecvf.com/content/WACV2024/html/Ofori-Oduro_Defending_Object_Detection_Models_Against_Image_Distortions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ofori-Oduro_Defending_Object_Detection_Models_Against_Image_Distortions_WACV_2024_paper.pdf,,https://github.com/moforio/GSES,,main,Poster,https://ieeexplore.ieee.org/document/10484446/,"['Training', 'Computational modeling', 'Estimation', 'Object detection', 'Transforms', 'Distortion', 'Data augmentation']","['Object Detection', 'Detection Model', 'Image Distortion', 'Object Detection Model', 'Image Classification', 'Data Augmentation', 'Training Images', 'Clear Image', 'Data Augmentation Methods', 'Motion Blur', 'Distribution Of Pixels', 'COCO Dataset', 'Training Set', 'Training Data', 'Classification Model', 'Corruption', 'Gaussian Noise', 'Random Selection', 'Intersection Over Union', 'Defocus', 'JPEG Compression', 'Presence Of Distortions', 'Gaussian Blur', 'Distortion Types', 'Impulsive Noise', 'Faster R-CNN', 'Shot Noise', 'Clean Samples', 'Image Processing Tasks', 'Image Noise']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"Image distortions pose a significant challenge to object detection. To address this issue, our paper introduces a novel data augmentation method that generates new samples resembling the original training images. The new sample exhibits randomly altered pixels based on a pixel distribution obtained from multiple image distortions using kernel density estimation (KDE). The main steps of our method, GSES, are generating distorted versions of each pixel of an original training image, selecting a set of pixels in each version, and then, for each selected pixel, estimating its distribution using KDE and then sampling one pixel from this distribution. By employing this approach, the new samples possess distorted pixels while maintaining a certain degree of similarity to the original image. This degree of similarity is essential to balance the accuracy of object detection models under distorted and clean images. Our approach improves the accuracy of different object detection models under 15 image distortions, such as motion blur, fog, and noise. For example, the average accuracy of YOLOv4 improves by 9.19% and 9.54 % across all 15 distortions added to the COCO and PASCAL datasets, respectively. Our method surpasses other defence methods to combat image distortions. Our ablation and stability studies show why our method performs well. Moreover, we also show that our method can be well used to improve the accuracy of image classification under 15 distortions and cross-domains. Our code is available at https://github.com/moforio/GSES/."
Defense Against Adversarial Cloud Attack on Remote Sensing Salient Object Detection,"Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang, Yuewei Lin, Hongkai Yu","Brookhaven National Laboratory; OPPO US Research Center; Cleveland State University; IHPC and CFAR, Agency for Science, Technology and Research (A*STAR)",75.0,"Singapore, USA",25.0,USA,"Detecting the salient objects in a remote sensing image has wide applications. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images with remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original image, could result in a collapse for the well-trained deep learning model. Different with existing methods adding perturbation to original images, we propose to jointly tune adversarial exposure and additive perturbation for attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is natural and common in remote sensing images, however, camouflaging cloud based adversarial attack and defense for remote sensing images are not well studied before. Furthermore, we design DefenseNet as a learnable pre-processing to the adversarial cloudy images to preserve the performance of the deep learning based remote sensing SOD model, without tuning the already deployed deep SOD model. By considering both regular and generalized adversarial examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in white-box setting and other attack methods in black-box setting. Experimental results on a synthesized benchmark from the public remote sensing dataset (EORSSD) show the promising defense against adversarial cloud attacks.",https://openaccess.thecvf.com/content/WACV2024/html/Sun_Defense_Against_Adversarial_Cloud_Attack_on_Remote_Sensing_Salient_Object_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Defense_Against_Adversarial_Cloud_Attack_on_Remote_Sensing_Salient_Object_WACV_2024_paper.pdf,,,2306.17431,main,Poster,https://ieeexplore.ieee.org/document/10483622/,"['Deep learning', 'Computer vision', 'Clouds', 'Perturbation methods', 'Computational modeling', 'Object detection', 'Benchmark testing']","['Object Detection', 'Remote Sensing', 'Adversarial Attacks', 'Salient Object', 'Salient Object Detection', 'Deep Learning', 'Deep Models', 'Cloud Computing', 'Adversarial Examples', 'Attack Methods', 'Additional Perturbations', 'Loss Function', 'Gaussian Noise', 'Random Noise', 'Image Object', 'Clear Image', 'Normal Images', 'Adversarial Training', 'Attack Strategy', 'Exposure Matrix', 'Fast Gradient Sign Method', 'White-box Attack', 'Defense Methods', 'Projected Gradient Descent']","['Applications', 'Remote Sensing', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",6,"Detecting the salient objects in a remote sensing image has wide applications. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images with remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original image, could result in a collapse for the well-trained deep learning model. Different with existing methods adding perturbation to original images, we propose to jointly tune adversarial exposure and additive perturbation for attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is natural and common in remote sensing images, however, camouflaging cloud based adversarial attack and defense for remote sensing images are not well studied before. Furthermore, we design DefenseNet as a learnable pre-processing to the adversarial cloudy images to preserve the performance of the deep learning based remote sensing SOD model, without tuning the already deployed deep SOD model. By considering both regular and generalized adversarial examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in white-box setting and other attack methods in black-box setting. Experimental results on a synthesized benchmark from the public remote sensing dataset (EORSSD) show the promising defense against adversarial cloud attacks."
Denoising and Selecting Pseudo-Heatmaps for Semi-Supervised Human Pose Estimation,"Zhuoran Yu, Manchen Wang, Yanbei Chen, Paolo Favaro, Davide Modolo",AWS AI Labs; Currently at The University of Wisconsin–Madison; AWS AI Labs; AWS AI Labs; Corresponding author,20.0,USA,80.0,USA,"We propose a new semi-supervised learning design for human pose estimation that revisits the popular dual-student framework and enhances it two ways. First, we introduce a denoising scheme to generate reliable pseudo-heatmaps as targets for learning from unlabeled data. This uses multi-view augmentations and a threshold-and-refine procedure to produce a pool of pseudo-heatmaps. Second, we select the learning targets from these pseudo-heatmaps guided by the estimated cross-student uncertainty. We evaluate our proposed method on multiple evaluation setups on the COCO benchmark. Our results show that our model outperforms previous state-of-the-art semi-supervised pose estimators, especially in extreme low-data regime. For example with only 0.5K labeled images our method is capable of surpassing the best competitor by 7.22 mAP (+25% absolute improvement). We also demonstrate that our model can learn effectively from unlabeled data in the wild to further boost its generalization and performance.",https://openaccess.thecvf.com/content/WACV2024/html/Yu_Denoising_and_Selecting_Pseudo-Heatmaps_for_Semi-Supervised_Human_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yu_Denoising_and_Selecting_Pseudo-Heatmaps_for_Semi-Supervised_Human_Pose_Estimation_WACV_2024_paper.pdf,,,2310.00099,main,Poster,https://ieeexplore.ieee.org/document/10483917/,"['Heating systems', 'Computer vision', 'Uncertainty', 'Computational modeling', 'Pose estimation', 'Noise reduction', 'Semisupervised learning']","['Denoising', 'Pose Estimation', 'Human Pose Estimation', 'Unlabeled Data', 'Semi-supervised Learning', 'Learning Targets', 'Absolute Improvement', 'Input Image', 'Image Classification', 'Object Detection', 'Data Augmentation', 'Uncertainty Estimation', 'Maximum Response', 'Bounding Box', 'Backbone Network', 'Affine Transformation', 'Semi-supervised Methods', 'Single View', 'Strong Views', 'Learning Rate Decay', 'Student Network', 'Unlabeled Images', 'Semi-supervised Classification', 'Consistency Regularization', '2D Gaussian', 'Target Reliability', 'Unlabeled Set', 'Input Space', 'Ensemble Model']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"We propose a new semi-supervised learning design for human pose estimation that revisits the popular dual-student framework and enhances it two ways. First, we introduce a denoising scheme to generate reliable pseudo-heatmaps as targets for learning from unlabeled data. This uses multi-view augmentations and a threshold-and-refine procedure to produce a pool of pseudo-heatmaps. Second, we select the learning targets from these pseudo-heatmaps guided by the estimated cross-student uncertainty. We evaluate our proposed method on multiple evaluation setups on the COCO benchmark. Our results show that our model outperforms previous state-of-the-art semi-supervised pose estimators, especially in extreme low-data regime. For example with only 0.5K labeled images our method is capable of surpassing the best competitor by 7.22 mAP (+25% absolute improvement). We also demonstrate that our model can learn effectively from unlabeled data in the wild to further boost its generalization and performance."
Density-Based Flow Mask Integration via Deformable Convolution for Video People Flux Estimation,"Chang-Lin Wan, Feng-Kai Huang, Hong-Han Shuai",National Yang Ming Chiao Tung University,100.0,Taiwan,0.0,,"Crowd counting is currently applied in many areas, such as transportation hubs and streets. However, most of the research still focuses on counting the number of people in a single image, and there is little research on solving the problem of calculating the number of non-repeated people in a video segment. Currently, multiple object tracking is mainly relied upon for video counting, but this method is not suitable for situations where the crowd density is too high. Therefore, we propose a Flow Mask Integration Deformable Convolution network (FMDC) combined with Intra-Frame Head Contrastive Learning (IFHC) to predict the situation of people entering and exiting the screen in a density-based manner. We verify that our proposed method is highly effective in densely populated situations and diverse scenes, and the experimental results show that our proposed method surpasses existing methods.",https://openaccess.thecvf.com/content/WACV2024/html/Wan_Density-Based_Flow_Mask_Integration_via_Deformable_Convolution_for_Video_People_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wan_Density-Based_Flow_Mask_Integration_via_Deformable_Convolution_for_Video_People_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483743/,"['Image segmentation', 'Computer vision', 'Head', 'Convolution', 'Computational modeling', 'Transportation', 'Estimation']","['Deformable Convolution', 'Number Of People', 'Single Image', 'Object Tracking', 'Self-supervised Learning', 'Video Segments', 'Crowd Density', 'Multiple Object Tracking', 'Density Estimation', 'Object Detection', 'Pedestrian', 'Density Map', 'Bounding Box', 'Kalman Filter', 'Image Pairs', 'Target Domain', 'Tracking Algorithm', 'Interval Training', 'Optimal Transport', 'Multiple Frames', 'Characteristics Of Head', 'Spatial Alignment', 'Deformable Layer', 'Counting Technique', 'Tracking Technique', 'Subsequent Frames', 'Test Interval', 'Feature Pyramid Network', 'Blue Box', 'Training Details']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Crowd counting is currently applied in many areas, such as transportation hubs and streets. However, most of the research still focuses on counting the number of people in a single image, and there is little research on solving the problem of calculating the number of non-repeated people in a video segment. Currently, multiple object tracking is mainly relied upon for video counting, but this method is not suitable for situations where the crowd density is too high. Therefore, we propose a Flow Mask Integration Deformable Convolution network (FMDC) combined with Inter-Frame Head Contrastive Learning (IFHC) to predict the situation of people entering and exiting the screen in a density-based manner. We verify that our proposed method is highly effective in densely populated situations and diverse scenes, and the experimental results show that our proposed method surpasses existing methods."
Depth From Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach,"Xihao Chen, Wenming Weng, Yueyi Zhang, Zhiwei Xiong",University of Science and Technology of China,100.0,China,0.0,,"Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483860/,"['Computer vision', 'Costs', 'Brightness', 'Estimation', 'Cameras', 'Motion measurement']","['Divide-and-conquer Approach', 'Input Sequence', 'Depth Estimation', 'Camera Frame', 'Motion Blur', 'Dual Strategy', 'Brightness Changes', 'Scene Changes', 'Stereo Matching', 'Spatial Depth', 'Temporal Scheme', 'Dynamic Vision Sensor', 'Cost Volume', 'Root Mean Square Error', 'Effectiveness Of Strategies', 'Mean Absolute Error', 'Image Frames', 'Abundant Information', 'Standard Metrics', 'Blue Triangles', 'Depth Planes', 'Left Camera', 'Event Stream', 'Symmetric System', 'Current Time Step', 'High Dynamic Range', 'Camera Pose', 'Yellow Star', 'Challenging Scenarios', 'High-quality Signals']","['Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions."
Describe Images in a Boring Way: Towards Cross-Modal Sarcasm Generation,"Jie Ruan, Yue Wu, Xiaojun Wan, Yuesheng Zhu",Peking University,100.0,China,0.0,,"Sarcasm generation has been investigated in previous studies by considering it as a text-to-text generation problem, i.e., generating a sarcastic sentence for an input sentence. In this paper, we study a new problem of cross-modal sarcasm generation (CMSG), i.e., generating a sarcastic description for a given image. CMSG is challenging as models need to satisfy the characteristics of sarcasm, as well as the correlation between different modalities. In addition, there should be some inconsistency between the two modalities, which requires imagination. Moreover, high-quality training data is insufficient. To address these problems, we take a step toward generating sarcastic descriptions from images without paired training data and propose an Extraction-Generation-Ranking based Modular method (EGRM) for CMSG. Specifically, EGRM first extracts diverse information from an image at different levels and uses the obtained image tags, sentimental descriptive caption, and commonsense-based consequence to generate candidate sarcastic texts. Then, a comprehensive ranking algorithm, which considers image-text relation, sarcasticness, and grammaticality, is proposed to select a final text from the candidate texts. Human evaluation at five criteria on a total of 2100 generated image-text pairs and auxiliary automatic evaluation show the superiority of our method. Code and data will be publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Ruan_Describe_Images_in_a_Boring_Way_Towards_Cross-Modal_Sarcasm_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ruan_Describe_Images_in_a_Boring_Way_Towards_Cross-Modal_Sarcasm_Generation_WACV_2024_paper.pdf,,https://github.com/EnablerRx/CMSG-EGRM,,main,Poster,https://ieeexplore.ieee.org/document/10483791/,"['Computer vision', 'Correlation', 'Codes', 'Training data', 'Data mining']","['Imagination', 'Diverse Information', 'Generation Problem', 'Grammaticality', 'Comprehensive Ranking', 'Image Tags', 'Comparative Method', 'Creativity', 'Deep Neural Network', 'Detailed Results', 'Input Image', 'Image Information', 'Multiple Perspectives', 'Traffic Congestion', 'Content Creation', 'Generation Module', 'Rainy Days', 'Positive Sentiment', 'Sunburn', 'Image X', 'Image Captioning', 'Text Generation', 'Ablation Method', 'Dialogue System', 'Target Side', 'Detailed Experimental Analysis', 'Attention Mechanism']","['Algorithms', 'Vision + language and/or other modalities']",1,"Sarcasm generation has been investigated in previous studies by considering it as a text-to-text generation problem, i.e., generating a sarcastic sentence for an input sentence. In this paper, we study a new problem of cross-modal sarcasm generation (CMSG), i.e., generating a sarcastic description for a given image. CMSG is challenging as models need to satisfy the characteristics of sarcasm, as well as the correlation between different modalities. In addition, there should be some inconsistency between the two modalities, which requires imagination. Moreover, high-quality training data is insufficient. To address these problems, we take a step toward generating sarcastic descriptions from images without paired training data and propose an Extraction-Generation-Ranking based Modular method (EGRM) for CMSG. Specifically, EGRM first extracts diverse information from an image at different levels and uses the obtained image tags, sentimental descriptive caption, and commonsense-based consequence to generate candidate sarcastic texts. Then, a comprehensive ranking algorithm, which considers image-text relation, sarcasticness, and grammaticality, is proposed to select a final text from the candidate texts. Human evaluation at five criteria on a total of 2100 generated image-text pairs and auxiliary automatic evaluation show the superiority of our method. Code and data are publicly available
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Design Choices for Enhancing Noisy Student Self-Training,"Aswathnarayan Radhakrishnan, Jim Davis, Zachary Rabin, Benjamin Lewis, Matthew Scherreik, Roman Ilin","Ohio State University; AFRL, Wright-Patterson AFB",50.0,USA,50.0,USA,"Semi-supervised learning approaches train on small sets of labeled data in addition to large sets of unlabeled data. Self-training is a semi-supervised teacher-student approach that often suffers from ""confirmation bias"" that occurs when the student model repeatedly overfits to incorrect pseudo-labels given by the teacher model for the unlabeled data. This bias impedes improvements in pseudo-label accuracy across self-training iterations, leading to unwanted saturation in model performance after just a few iterations. In this work, we study multiple design choices to improve the Noisy Student self-training pipeline and reduce confirmation bias. We showed that our proposed Weighted SplitBatch Sampler and Dataset-Adaptive Techniques for Model Calibration and Entropy-Based Pseudo-Label Selection provided performance gains over existing design choices across multiple datasets. Finally, we also study the extendability of our enhanced approach to Open Set unlabeled data (containing classes not seen in labeled data). The source code can be licensed for use via email.",https://openaccess.thecvf.com/content/WACV2024/html/Radhakrishnan_Design_Choices_for_Enhancing_Noisy_Student_Self-Training_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Radhakrishnan_Design_Choices_for_Enhancing_Noisy_Student_Self-Training_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484467/,"['Filtering', 'Source coding', 'Pipelines', 'Training data', 'Self-supervised learning', 'Semisupervised learning', 'Performance gain']","['Design Choices', 'Large Datasets', 'Teacher Model', 'Multiple Datasets', 'Open Set', 'Unlabeled Data', 'Semi-supervised Learning', 'Confirmation Bias', 'Student Model', 'Semi-supervised Learning Approach', 'Training Data', 'Deep Learning', 'Validation Data', 'False Positive Rate', 'Data Augmentation', 'Model Size', 'Large Model', 'ImageNet', 'Hyperparameter Tuning', 'Target Class', 'Hard Loss', 'Consistency Regularization', 'Entropy Threshold', 'Ground Truth Labels', 'Majority Voting', 'Self-supervised Learning', 'Subset Size', 'Unlabeled Set', 'Closed Set', 'Previous Iteration']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Semi-supervised learning approaches train on small sets of labeled data in addition to large sets of unlabeled data. Self-training is a semi-supervised teacher-student approach that often suffers from ""confirmation bias"" that occurs when the student model repeatedly overfits to incorrect pseudo-labels given by the teacher model for the unlabeled data. This bias impedes improvements in pseudo-label accuracy across self-training iterations, leading to unwanted saturation in model performance after just a few iterations. In this work, we study multiple design choices to improve the Noisy Student self-training pipeline and reduce confirmation bias. We showed that our proposed Weighted SplitBatch Sampler and Dataset-Adaptive Techniques for Model Calibration and Entropy-Based Pseudo-Label Selection provided performance gains over existing design choices across multiple datasets. Finally, we also study the extendability of our enhanced approach to Open Set unlabeled data (containing classes not seen in labeled data). The source code can be licensed for use via email."
Designing a Hybrid Neural System To Learn Real-World Crack Segmentation From Fractal-Based Simulation,"Achref Jaziri, Martin Mundt, Andres Fernandez, Visvanathan Ramesh","TU Darmstadt and hessian.AI, Darmstadt, Germany; University of Tübingen, Tübingen, Germany; Goethe University, Frankfurt am Main, Germany; Goethe University and hessian.AI, Frankfurt am Main, Germany",100.0,Germany,0.0,,"Identification of cracks is essential to assess the structural integrity of concrete infrastructure. However, robust crack segmentation remains a challenging task for computer vision systems due to the diverse appearance of concrete surfaces, variable lighting and weather conditions, and the overlapping of different defects. In particular recent data-driven methods struggle with the limited availability of data, the fine-grained and time-consuming nature of crack annotation, and face subsequent difficulty in generalizing to out-of-distribution samples. In this work, we move past these challenges in a two-fold way. We introduce a high-fidelity crack graphics simulator based on fractals and a corresponding fully-annotated crack dataset. We then complement the latter with a system that learns generalizable representations from simulation, by leveraging both a pointwise mutual information estimate along with adaptive instance normalization as inductive biases. Finally, we empirically highlight how different design choices are symbiotic in bridging the simulation to real gap, and ultimately demonstrate that our introduced system can effectively handle real-world crack segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Jaziri_Designing_a_Hybrid_Neural_System_To_Learn_Real-World_Crack_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jaziri_Designing_a_Hybrid_Neural_System_To_Learn_Real-World_Crack_Segmentation_WACV_2024_paper.pdf,,,2309.09637,main,Poster,https://ieeexplore.ieee.org/document/10483900/,"['Symbiosis', 'Computer vision', 'Adaptation models', 'Fractals', 'Data models', 'Surface cracks', 'Task analysis']","['Design Choices', 'Inductive Bias', 'Concrete Surface', 'Computer Vision System', 'Pointwise Mutual Information', 'Real-world Data', 'Intersection Over Union', 'Radial Basis Function', 'Semantic Segmentation', 'Synthetic Images', 'Domain Adaptation', 'Color Temperature', 'Multi-source Data', 'Consistency Loss', 'Hausdorff Distance', 'Style Transfer', 'Multiset', 'Ground Truth Map', 'Auxiliary Task', 'Affinity Score', 'Physics-based Simulation', 'Concrete Cracking', 'Snowflake', 'Crack Detection', 'Surface Normals', 'Synthetic Training Data', 'Drawing Inspiration', 'Distancing Measures', 'Training Data', 'Crack Propagation']","['Applications', 'Structural engineering / civil engineering', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"Identification of cracks is essential to assess the structural integrity of concrete infrastructure. However, robust crack segmentation remains a challenging task for computer vision systems due to the diverse appearance of concrete surfaces, variable lighting and weather conditions, and the overlapping of different defects. In particular recent data-driven methods struggle with the limited availability of data, the fine-grained and time-consuming nature of crack annotation, and face subsequent difficulty in generalizing to out-of-distribution samples. In this work, we move past these challenges in a two-fold way. We introduce a high-fidelity crack graphics simulator based on fractals and a corresponding fully-annotated crack dataset. We then complement the latter with a system that learns generalizable representations from simulation, by leveraging both a pointwise mutual information estimate along with adaptive instance normalization as inductive biases. Finally, we empirically highlight how different design choices are symbiotic in bridging the simulation to real gap, and ultimately demonstrate that our introduced system can effectively handle real-world crack segmentation."
Detecting Content Segments From Online Sports Streaming Events: Challenges and Solutions,"Zongyi Liu, Yarong Feng, Shunyan Luo, Yuan Ling, Shujing Dong, Shuyi Wang","Customer Experience and Business Trends, Amazon.com",0.0,,100.0,USA,"Developing a client-side segmentation algorithm for online sports streaming holds significant importance. For instance, in order to assess the video quality from an end-user perspective such as artifact detection, it is important to initially segment the content within the streaming playback. The challenge lies in localizing the content due to the intricate scene changes between content and non-content sections in popular sports like football, tennis, baseball, and more. Client-side content detection can be implemented in two ways: intrusively, involving the interception of network traffic and parsing service provider data and logs, or non-intrusively, which entails capturing streamed videos from content providers and subjecting them to analysis using computer vision technologies. In this paper, we introduce a non-intrusive framework that leverages a combination of traditional machine learning algorithms and deep neural networks (DNN) to distinguish content sections from non-content sections across various online sports streaming services. Our algorithm has demonstrated a remarkable level of accuracy and effectiveness in sports broadcasting events, effectively overcoming the complexities introduced by intricate non-content insertion methods during the games.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Detecting_Content_Segments_From_Online_Sports_Streaming_Events_Challenges_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Detecting_Content_Segments_From_Online_Sports_Streaming_Events_Challenges_and_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483789/,"['Computer vision', 'Machine learning algorithms', 'Artificial neural networks', 'Telecommunication traffic', 'Games', 'Streaming media', 'Quality assessment']","['Streaming Events', 'Deep Neural Network', 'Computer Vision', 'Traditional Machine Learning', 'Video Quality', 'Artifact Detection', 'Computer Vision Technology', 'End-user Perspective', 'Training Data', 'Training Dataset', 'Convolutional Neural Network', 'Support Vector Machine', 'Changes In Frequency', 'Temporal Dimension', 'Basketball', 'Video Clips', 'Type Of Content', 'Space Complexity', 'Sporting Events', 'Recall Rate', 'Video Playback', 'Video Features', '1D Convolution', 'Number Of Clips', 'Audio Data', 'Sampling Window', 'Type Of Sport', 'Live Streaming', 'Post-processing Step', 'Running Time']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Smartphones / end user devices']",2,"Developing a client-side segmentation algorithm for on-line sports streaming holds significant importance. For instance, in order to assess the video quality from an end-user perspective such as artifact detection, it is important to initially segment the content within the streaming playback. The challenge lies in localizing the content due to the intricate scene changes between content and non-content sections in popular sports like football, tennis, baseball, and more. Client-side content detection can be implemented in two ways: intrusively, involving the interception of network traffic and parsing service provider data and logs, or non-intrusively, which entails capturing streamed videos from content providers and subjecting them to analysis using computer vision technologies. In this paper, we introduce a non-intrusive framework that leverages a combination of traditional machine learning algorithms and deep neural networks (DNN) to distinguish content sections from noncontent sections across various online sports streaming services. Our algorithm has demonstrated a remarkable level of accuracy and effectiveness in sports broadcasting events, effectively overcoming the complexities introduced by intricate non-content insertion methods during the games."
Detection Defenses: An Empty Promise Against Adversarial Patch Attacks on Optical Flow,"Erik Scheurer, Jenny Schmalfuss, Alexander Lis, Andrés Bruhn","Institute for Visualization and Interactive Systems, University of Stuttgart",100.0,Germany,0.0,,"Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patch attacks for all tested optical flow methods except FlowNetC. As currently employed detect-and-remove defenses fail to deliver the promised adversarial robustness for optical flow, they evoke a false sense of security. The code is available at https://github.com/cv-stuttgart/DetectionDefenses.",https://openaccess.thecvf.com/content/WACV2024/html/Scheurer_Detection_Defenses_An_Empty_Promise_Against_Adversarial_Patch_Attacks_on_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Scheurer_Detection_Defenses_An_Empty_Promise_Against_Adversarial_Patch_Attacks_on_WACV_2024_paper.pdf,,https://github.com/cv-stuttgart/DetectionDefenses,2310.17403,main,Poster,https://ieeexplore.ieee.org/document/10484398/,"['Image quality', 'Degradation', 'Computer vision', 'Image motion analysis', 'Motion estimation', 'Robustness', 'Motion detection']","['Optical Flow', 'Adversarial Attacks', 'Adversarial Patches', 'Patch Attacks', 'Defense Mechanisms', 'Flow Prediction', 'Optical Flow Method', 'Smoothing', 'Changes In Variables', 'Image Quality', 'Second Derivative', 'Stochastic Gradient Descent', 'Vanilla', 'Action Recognition', 'Loss Term', 'Adversarial Training', 'Gradient Magnitude', 'Image Gradient', 'Patch Area', 'Inpainting', 'Original Prediction', 'Endpoint Error', 'Box Constraints', 'Robust Score', 'Input Frames', 'Backpropagation']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",1,"Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patch attacks for all tested optical flow methods except FlowNetC. As currently employed detect-and-remove defenses fail to deliver the promised adversarial robustness for optical flow, they evoke a false sense of security. The code is available at https://github.com/cvstuttgart/DetectionDefenses."
Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization,"Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde, Abhinav Shrivastava","New York University; University of Maryland, College Park",100.0,"Canada, USA",0.0,,"The task of lip synchronization (lip-sync) seeks to match the lips of human faces with different audio. It has various applications in the film industry as well as for creating virtual avatars and for video conferencing. This is a challenging problem as one needs to simultaneously introduce detailed, realistic lip movements while preserving the identity, pose, emotions, and image quality. Many of the previous methods trying to solve this problem suffer from image quality degradation due to a lack of complete contextual information. In this paper, we present Diff2Lip, an audio-conditioned diffusion-based model which is able to do lip synchronization in-the-wild while preserving these qualities. We train our model on Voxceleb2, a video dataset containing in-the-wild talking face videos. Extensive studies show that our method outperforms popular methods like Wav2Lip and PC-AVS in Frechet inception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We show results on both reconstruction (same audio-video inputs) as well as cross (different audio-video inputs) settings on Voxceleb2 and LRWdatasets. Video results are available at https://soumik-kanad.github.io/diff2lip.",https://openaccess.thecvf.com/content/WACV2024/html/Mukhopadhyay_Diff2Lip_Audio_Conditioned_Diffusion_Models_for_Lip-Synchronization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mukhopadhyay_Diff2Lip_Audio_Conditioned_Diffusion_Models_for_Lip-Synchronization_WACV_2024_paper.pdf,https://soumik-kanad.github.io/diff2lip,https://github.com/soumik-kanad/diff2lip,2308.09716,main,Poster,https://ieeexplore.ieee.org/document/10484521/,"['Image quality', 'Measurement', 'Degradation', 'Lips', 'Entertainment industry', 'Synchronization', 'Task analysis']","['Diffusion Model', 'Synchronization', 'Image Quality', 'Contextual Information', 'Human Faces', 'Film Industry', 'Virtual Avatar', 'Fréchet Inception Distance', 'Mean Opinion Score', 'High-quality', 'Neural Network', 'Reference Frame', 'Diffusion Process', 'Video Frames', 'Visual Quality', 'Sparse Representation', 'Loss Of Identity', 'Masked Images', 'Temporal Consistency', 'Input Frames', 'Sequential Loss', 'Audio Input', 'Landmark Detection']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",3,"The task of lip synchronization (lip-sync) seeks to match the lips of human faces with different audio. It has various applications in the film industry as well as for creating virtual avatars and for video conferencing. This is a challenging problem as one needs to simultaneously introduce detailed, realistic lip movements while preserving the identity, pose, emotions, and image quality. Many of the previous methods trying to solve this problem suffer from image quality degradation due to a lack of complete contextual information. In this paper, we present Diff2Lip, an audio-conditioned diffusion-based model which is able to do lip synchronization in-the-wild while preserving these qualities. We train our model on Voxceleb2, a video dataset containing in-the-wild talking face videos. Extensive studies show that our method outperforms popular methods like Wav2Lip and PC-AVS in Fréchet inception distance (FID) metric and Mean Opinion Scores (MOS) of the users. We show results on both reconstruction (same audio-video inputs) as well as cross (different audio-video inputs) settings on Voxceleb2 and LRW datasets. Video results are available at https://soumik-kanad.github.io/diff2lip."
DiffBody: Diffusion-Based Pose and Shape Editing of Human Images,"Yuta Okuyama, Yuki Endo, Yoshihiro Kanamori",University of Tsukuba,100.0,Japan,0.0,,"Pose and body shape editing in a human image has received increasing attention. However, current methods often struggle with dataset biases and deteriorate realism and the person's identity when users make large edits. We propose a one-shot approach that enables large edits with identity preservation. To enable large edits, we fit a 3D body model, project the input image onto the 3D model, and change the body's pose and shape. Because this initial textured body model has artifacts due to occlusion and the inaccurate body shape, the rendered image undergoes a diffusion-based refinement, in which strong noise destroys body structure and identity whereas insufficient noise does not help. We thus propose an iterative refinement with weak noise, applied first for the whole body and then for the face. We further enhance the realism by fine-tuning text embeddings via self-supervised learning. Our quantitative and qualitative evaluations demonstrate that our method outperforms other existing methods across various datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Okuyama_DiffBody_Diffusion-Based_Pose_and_Shape_Editing_of_Human_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Okuyama_DiffBody_Diffusion-Based_Pose_and_Shape_Editing_of_Human_Images_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484054/,"['Solid modeling', 'Three-dimensional displays', 'Shape', 'Noise', 'Pipelines', 'Self-supervised learning', 'Iterative methods']","['Shape Editing', 'Input Image', 'Body Shape', 'Self-supervised Learning', 'Iterative Refinement', 'Body Model', '3D Body', 'Identity Preservation', 'Texture Model', 'Weak Noise', 'Body Weight', 'Fine-tuned', 'Reversible Process', 'Flow Field', 'Generative Adversarial Networks', 'Diffusion Model', 'Reference Image', 'Face Images', 'Output Image', '3D Human Model', 'Face Identity', 'Warped Image', 'Forward Process', 'Variational Autoencoder', 'Noise Strength', 'Coarse Structure', 'Noise Map', 'Translation Technique', 'Refinement Step']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Pose and body shape editing in a human image has received increasing attention. However, current methods often struggle with dataset biases and deteriorate realism and the person’s identity when users make large edits. We propose a one-shot approach that enables large edits with identity preservation. To enable large edits, we fit a 3D body model, project the input image onto the 3D model, and change the body’s pose and shape. Because this initial textured body model has artifacts due to occlusion and the inaccurate body shape, the rendered image undergoes a diffusion-based refinement, in which strong noise destroys body structure and identity whereas insufficient noise does not help. We thus propose an iterative refinement with weak noise, applied first for the whole body and then for the face. We further enhance the realism by fine-tuning text embeddings via self-supervised learning. Our quantitative and qualitative evaluations demonstrate that our method outperforms other existing methods across various datasets."
DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification,"Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu","Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Jiaotong University, Beijing, China; Massachusetts Institute of Technology, Cambridge, USA",100.0,"China, USA",0.0,,"Large pre-trained models have revolutionized the field of computer vision by facilitating multi-modal learning. Notably, the CLIP model has exhibited remarkable proficiency in tasks such as image classification, object detection, and semantic segmentation. Nevertheless, its efficacy in processing 3D point clouds is restricted by the domain gap between the depth maps derived from 3D projection and the training images of CLIP. This paper introduces DiffCLIP, a novel pre-training framework that seamlessly integrates stable diffusion with ControlNet. The primary objective of DiffCLIP is to bridge the domain gap inherent in the visual branch. Furthermore, to address few-shot tasks in the textual branch, we incorporate a style-prompt generation module. Extensive experiments on the ModelNet10, ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities for 3D understanding. By using stable diffusion and style-prompt generation, DiffCLIP achieves an accuracy of 43.2% for zero-shot classification on OBJ_BG of ScanObjectNN, which is state-of-the-art performance, and an accuracy of 82.4% for zero-shot classification on ModelNet10, which is also state-of-the-art performance.",https://openaccess.thecvf.com/content/WACV2024/html/Shen_DiffCLIP_Leveraging_Stable_Diffusion_for_Language_Grounded_3D_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shen_DiffCLIP_Leveraging_Stable_Diffusion_for_Language_Grounded_3D_Classification_WACV_2024_paper.pdf,,,2305.15957,main,Poster,https://ieeexplore.ieee.org/document/10483742/,"['Point cloud compression', 'Training', 'Solid modeling', 'Visualization', 'Computer vision', 'Three-dimensional displays', 'Computational modeling']","['3D Classification', 'Image Classification', 'Object Detection', 'Point Cloud', 'Semantic Segmentation', 'Depth Map', '3D Point Cloud', 'Generation Module', 'Multimodal Learning', 'Domain Gap', '3D Projection', 'Training Data', 'Local Information', 'Local Features', 'Diffusion Process', 'Visual Features', 'Diffusion Model', 'Textual Features', 'Realistic Images', 'Few-shot Classification', 'Style Transfer', '3D Tasks', 'Style Features', 'Visual Encoding', 'Projection Views', 'Few-shot Learning', 'Point Cloud Processing', 'Text Encoder', 'Projection Point']","['Algorithms', '3D computer vision', 'Algorithms', 'Vision + language and/or other modalities']",2,"Large pre-trained models have revolutionized the field of computer vision by facilitating multi-modal learning. Notably, the CLIP model has exhibited remarkable proficiency in tasks such as image classification, object detection, and semantic segmentation. Nevertheless, its efficacy in processing 3D point clouds is restricted by the domain gap between the depth maps derived from 3D projection and the training images of CLIP.This paper introduces DiffCLIP, a novel pre-training framework that seamlessly integrates stable diffusion with ControlNet. The primary objective of DiffCLIP is to bridge the domain gap inherent in the visual branch. Furthermore, to address few-shot tasks in the textual branch, we incorporate a style-prompt generation module.Extensive experiments on the ModelNet10, ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities for 3D understanding. By using stable diffusion and style-prompt generation, DiffCLIP achieves an accuracy of 43.2% for zero-shot classification on OBJ_BG of ScanObjectNN, which is state-of-the-art performance, and an accuracy of 82.4% for zero-shot classification on Model-Net10, which is also state-of-the-art performance."
Differentiable JPEG: The Devil Is in the Details,"Christoph Reich, Biplob Debnath, Deep Patel, Srimat Chakradhar","NEC Laboratories America, Inc. and Technische Universit Èat Darmstadt; NEC Laboratories America, Inc.",50.0,Germany,50.0,USA,"JPEG remains one of the most widespread lossy image coding methods. However, the non-differentiable nature of JPEG restricts the application in deep learning pipelines. Several differentiable approximations of JPEG have recently been proposed to address this issue. This paper conducts a comprehensive review of existing diff. JPEG approaches and identifies critical details that have been missed by previous methods. To this end, we propose a novel diff. JPEG approach, overcoming previous limitations. Our approach is differentiable w.r.t. the input image, the JPEG quality, the quantization tables, and the color conversion parameters. We evaluate the forward and backward performance of our diff. JPEG approach against existing methods. Additionally, extensive ablations are performed to evaluate crucial design choices. Our proposed diff. JPEG resembles the (non-diff.) reference implementation best, significantly surpassing the recent-best diff. approach by 3.47dB (PSNR) on average. For strong compression rates, we can even improve PSNR by 9.51dB. Strong adversarial attack results are yielded by our diff. JPEG, demonstrating the effective gradient approximation. Our code is available at https://github.com/necla-ml/Diff-JPEG.",https://openaccess.thecvf.com/content/WACV2024/html/Reich_Differentiable_JPEG_The_Devil_Is_in_the_Details_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Reich_Differentiable_JPEG_The_Devil_Is_in_the_Details_WACV_2024_paper.pdf,https://christophreich1996.github.io/differentiable_jpeg/,https://github.com/necla-ml/Diff-JPEG,2309.06978,main,Poster,https://ieeexplore.ieee.org/document/10484348/,"['Deep learning', 'Quantization (signal)', 'Image coding', 'Reviews', 'Image color analysis', 'Pipelines', 'Transform coding']","['Joint Photographic Experts Group', 'Design Choices', 'Adversarial Attacks', 'Strong Compression', 'Color Conversion', 'Deep Neural Network', 'Compressive Strength', 'Global Minimum', 'RGB Images', 'Color Space', 'Lossless', 'Color Of Samples', 'File Size', 'Forward Pass', 'True Function', 'Discrete Cosine Transform', 'Polynomial Approximation', 'Backward Pass', 'Adversarial Examples', 'Floor Function', 'Round Function', 'Fast Gradient Sign Method', 'RGB Color Space', 'Gradient Norm', 'Official Code', 'Scaling Factor', 'Discretion', 'Decoding']","['Algorithms', 'Computational photography', 'image and video synthesis']",1,"JPEG remains one of the most widespread lossy image coding methods. However, the non-differentiable nature of JPEG restricts the application in deep learning pipelines. Several differentiable approximations of JPEG have recently been proposed to address this issue. This paper conducts a comprehensive review of existing diff. JPEG approaches and identifies critical details that have been missed by previous methods. To this end, we propose a novel diff. JPEG approach, overcoming previous limitations. Our approach is differentiable w.r.t. the input image, the JPEG quality, the quantization tables, and the color conversion parameters. We evaluate the forward and backward performance of our diff. JPEG approach against existing methods. Additionally, extensive ablations are performed to evaluate crucial design choices. Our proposed diff. JPEG resembles the (non-diff.) reference implementation best, significantly surpassing the recent-best diff. approach by 3.47dB (PSNR) on average. For strong compression rates, we can even improve PSNR by 9.51dB. Strong adversarial attack results are yielded by our diff. JPEG, demonstrating the effective gradient approximation. Our code is available at https://github.com/necla-ml/Diff-JPEG."
Differentially Private Video Activity Recognition,"Zelun Luo, Yuliang Zou, Yijin Yang, Zane Durante, De-An Huang, Zhiding Yu, Chaowei Xiao, Li Fei-Fei, Animashree Anandkumar",Stanford University; Caltech; Arizona State University; University of Wisconsin–Madison; Virginia Tech; NVIDIA,83.33333333333334,USA,16.666666666666657,USA,"In recent years, differential privacy has seen significant advancements in image classification; however, its application to video activity recognition remains under-explored. This paper addresses the challenges of applying differential privacy to video activity recognition, which primarily stem from: (1) a discrepancy between the desired privacy level for entire videos and the nature of input data processed by contemporary video architectures, which are typically short, segmented clips; and (2) the complexity and sheer size of video datasets relative to those in image classification, which render traditional differential privacy methods inadequate. To tackle these issues, we propose Multi-Clip DP-SGD, a novel framework for enforcing video-level differential privacy through clip-based classification models. This method samples multiple clips from each video, averages their gradients, and applies gradient clipping in DP-SGD without incurring additional privacy loss. Moreover, we incorporate a parameter-efficient transfer learning strategy to make the model scalable for large-scale video datasets. Through extensive evaluations on the UCF-101 and HMDB-51 datasets, our approach exhibits impressive performance, achieving 81% accuracy with a privacy budget of epsilon=5 on UCF-101, marking a 76% improvement compared to a direct application of DP-SGD. Furthermore, we demonstrate that our transfer learning strategy is versatile and can enhance differentially private image classification across an array of datasets including CheXpert, ImageNet, CIFAR-10, and CIFAR-100.",https://openaccess.thecvf.com/content/WACV2024/html/Luo_Differentially_Private_Video_Activity_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Luo_Differentially_Private_Video_Activity_Recognition_WACV_2024_paper.pdf,,,2306.15742,main,Poster,https://ieeexplore.ieee.org/document/10484452/,"['Differential privacy', 'Privacy', 'Image segmentation', 'Computer vision', 'Transfer learning', 'Computer architecture', 'Activity recognition']","['Video Action Recognition', 'Image Classification', 'Transfer Learning', 'Large-scale Datasets', 'ImageNet', 'Video Dataset', 'Differential Privacy', 'Entire Video', 'Loss Of Privacy', 'Transfer Learning Strategy', 'Convolutional Neural Network', 'Gaussian Noise', 'Data Entry', 'Trainable Parameters', 'Normalization Layer', 'Video Analysis', 'Classification Datasets', 'Training Algorithm', 'Privacy Protection', 'Language Model', 'Linear Probe', 'Number Of Clips', 'Linear Layer', 'Vision Transformer', 'Privacy Guarantee', 'Short Clips', 'Pre-training Dataset', 'Video Modeling']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"In recent years, differential privacy has seen significant advancements in image classification; however, its application to video activity recognition remains under-explored. This paper addresses the challenges of applying differential privacy to video activity recognition, which primarily stem from: (1) a discrepancy between the desired privacy level for entire videos and the nature of input data processed by contemporary video architectures, which are typically short, segmented clips; and (2) the complexity and sheer size of video datasets relative to those in image classification, which render traditional differential privacy methods inadequate. To tackle these issues, we propose Multi-Clip DP-SGD, a novel framework for enforcing video-level differential privacy through clip-based classification models. This method samples multiple clips from each video, averages their gradients, and applies gradient clipping in DP-SGD without incurring additional privacy loss. Moreover, we incorporate a parameter-efficient transfer learning strategy to make the model scalable for large-scale video datasets. Through extensive evaluations on the UCF-101 and HMDB-51 datasets, our approach exhibits impressive performance, achieving 81% accuracy with a privacy budget of ϵ = 5 on UCF-101, marking a 76% improvement compared to a direct application of DP-SGD. Furthermore, we demonstrate that our transfer learning strategy is versatile and can enhance differentially private image classification across an array of datasets including CheXpert, ImageNet, CIFAR-10, and CIFAR-100."
Diffuse and Restore: A Region-Adaptive Diffusion Model for Identity-Preserving Blind Face Restoration,"Maitreya Suin, Nithin Gopalakrishnan Nair, Chun Pong Lau, Vishal M. Patel, Rama Chellappa",City University of Hong Kong; Johns Hopkins University,100.0,"Hong Kong, USA",0.0,,"Blind face restoration (BFR) from severely degraded face images in the wild is a highly ill-posed problem. Due to the complex unknown degradation, existing generative works typically struggle to restore realistic details when the input is of poor quality. Recently, diffusion-based approaches were successfully used for high-quality image synthesis. But, for BFR, maintaining a balance between the fidelity of the restored image and the reconstructed identity information is important. Minor changes in certain facial regions may alter the identity or degrade the perceptual quality. With this observation, we present a conditional diffusion-based framework for BFR. We alleviate the drawbacks of existing diffusion-based approaches and design an region-adaptive strategy. Specifically, we use a identity preserving conditioner network to recover the identity information from the input image as much as possible and use that to guide the reverse diffusion process, specifically for important facial locations that contribute the most to the identity. This leads to a significant improvement in perceptual quality as well as face-recognition scores over existing GAN and diffusion-based restoration models. Our approach achieves superior results to prior art on a range of real and synthetic datasets, particularly for severely degraded face images.",https://openaccess.thecvf.com/content/WACV2024/html/Suin_Diffuse_and_Restore_A_Region-Adaptive_Diffusion_Model_for_Identity-Preserving_Blind_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Suin_Diffuse_and_Restore_A_Region-Adaptive_Diffusion_Model_for_Identity-Preserving_Blind_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483864/,"['Degradation', 'Adaptation models', 'Image synthesis', 'Face recognition', 'Diffusion processes', 'Generative adversarial networks', 'Image restoration']","['Diffusion Model', 'Input Image', 'Diffusion Process', 'Identity Information', 'Reversible Process', 'Face Recognition', 'Perception Of Quality', 'Face Images', 'Prior Art', 'Identity Preservation', 'Time Step', 'Denoising', 'Gaussian Noise', 'Latent Space', 'Facial Features', 'Fine Details', 'Visual Quality', 'Clear Image', 'Recognition Model', 'Pixel Location', 'Fréchet Inception Distance', 'Pure Noise', 'Image X', 'Facial Details', 'Face Recognition Model', 'Forward Process', 'Unconditional Model', 'Iterative Refinement', 'Restoration Quality', 'Face Recognition Performance']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Low-level and physics-based vision']",3,"Blind face restoration (BFR) from severely degraded face images in the wild is a highly ill-posed problem. Due to the complex unknown degradation, existing generative works typically struggle to restore realistic details when the input is of poor quality. Recently, diffusion-based approaches were successfully used for high-quality image synthesis. But, for BFR, maintaining a balance between the fidelity of the restored image and the reconstructed identity information is important. Minor changes in certain facial regions may alter the identity or degrade the perceptual quality. With this observation, we present a conditional diffusion-based framework for BFR. We alleviate the drawbacks of existing diffusion-based approaches and design a region-adaptive strategy. Specifically, we use an identity preserving conditioner network to recover the identity information from the input image as much as possible and use that to guide the reverse diffusion process, specifically for important facial locations that contribute the most to the identity. This leads to a significant improvement in perceptual quality as well as face-recognition scores over existing GAN and diffusion-based restoration models. Our approach achieves superior results to prior art on a range of real and synthetic datasets, particularly for severely degraded face images."
Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation,"Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zięba, Stavros Petridis, Maja Pantic",; University of Wrocław; Imperial College London; Wrocław University of Science and Technology,100.0,"Poland, UK",0.0,,"Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results in expressiveness and smoothness on both of them.",https://openaccess.thecvf.com/content/WACV2024/html/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Stypulkowski_Diffused_Heads_Diffusion_Models_Beat_GANs_on_Talking-Face_Generation_WACV_2024_paper.pdf,https://mstypulkowski.github.io/diffusedheads/,,,main,Poster,https://ieeexplore.ieee.org/document/10484496/,"['Computer vision', 'Data models', 'Complexity theory', 'Faces', 'Videos']","['Generative Adversarial Networks', 'Diffusion Model', 'Facial Expressions', 'Head Movements', 'Deep Learning', 'Denoising', 'Audio Recordings', 'Hidden Markov Model', 'Kullback-Leibler', 'Head Motion', 'Optical Flow', 'Real Ones', 'Attention Layer', 'Forward Process', 'Generative Adversarial Networks Training', 'Fréchet Inception Distance', 'Real Videos', 'Lip-sync', 'Motion Frames', 'Identity Frames']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Vision + language and/or other modalities']",18,"Talking face generation has historically struggled to produce head movements and natural facial expressions without guidance from additional reference videos. Recent developments in diffusion-based generative models allow for more realistic and stable data synthesis and their performance on image and video generation has surpassed that of other generative models. In this work, we present an autoregressive diffusion model that requires only one identity image and audio sequence to generate a video of a realistic talking head. Our solution is capable of hallucinating head movements, facial expressions, such as blinks, and preserving a given background. We evaluate our model on two different datasets, achieving state-of-the-art results in expressiveness and smoothness on both of them.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Diffusion Models Meet Image Counter-Forensics,"Matías Tailanián, Marina Gardella, Alvaro Pardo, Pablo Musé","IIE, Facultad de Ingenier ´ıa, Universidad de la Rep ´ublica; Centre Borelli, ´Ecole Normale Sup ´erieure Paris-Saclay, Universit ´e Paris-Saclay; Digital Sense",66.66666666666666,"France, Uruguay",33.33333333333334,USA,"From its acquisition in the camera sensors to its storage, different operations are performed to generate the final image. This pipeline imprints specific traces into the image to form a natural watermark. Tampering with an image disturbs these traces; these disruptions are clues that are used by most methods to detect and locate forgeries. In this article, we assess the capabilities of diffusion models to erase the traces left by forgers and, therefore, deceive forensics methods. Such an approach has been recently introduced for adversarial purification, achieving significant performance. We show that diffusion purification methods are well suited for counter-forensics tasks. Such approaches outperform already existing counter-forensics techniques both in deceiving forensics methods, and in preserving the natural look of the purified images. The source code will be provided upon acceptance.",https://openaccess.thecvf.com/content/WACV2024/html/Tailanian_Diffusion_Models_Meet_Image_Counter-Forensics_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tailanian_Diffusion_Models_Meet_Image_Counter-Forensics_WACV_2024_paper.pdf,,https://github.com/mtailanian/diff-cf,,main,Poster,https://ieeexplore.ieee.org/document/10483646/,"['Image sensors', 'Computer vision', 'Purification', 'Forensics', 'Source coding', 'Pipelines', 'Watermarking']","['Diffusion Model', 'Purification Methods', 'Forensic Methods', 'Time Step', 'Image Quality', 'Denoising', 'Input Image', 'Diffusion Process', 'Clean Data', 'Intersection Over Union', 'Reversible Process', 'Generative Adversarial Networks', 'Matthews Correlation Coefficient', 'Semantic Content', 'Version Of Image', 'Assessment Metrics', 'Forward Process', 'Adversarial Attacks', 'Adversarial Examples', 'Quality Assessment Metrics', 'Yellow Patches', 'Main Article', 'Median Filter', 'Adversarial Perturbations', 'Clear Image', 'Markov Chain', 'Neural Network']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",2,"From its acquisition in the camera sensors to its storage, different operations are performed to generate the final image. This pipeline imprints specific traces into the image to form a natural watermark. Tampering with an image disturbs these traces; these disruptions are clues that are used by most methods to detect and locate forgeries. In this article, we assess the capabilities of diffusion models to erase the traces left by forgers and, therefore, deceive forensics methods. Such an approach has been recently introduced for adversarial purification, achieving significant performance. We show that diffusion purification methods are well suited for counter-forensics tasks. Such approaches outperform already existing counter-forensics techniques both in deceiving forensics methods and in preserving the natural look of the purified images. The source code is publicly available at https://github.com/mtailanian/diff-cf."
Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition,"Cindy M. Nguyen, Eric R. Chan, Alexander W. Bergman, Gordon Wetzstein",Stanford University,100.0,USA,0.0,,"Capturing images is a key part of automation for high-level tasks such as scene text recognition. Low-light conditions pose a challenge for high-level perception stacks, which are often optimized on well-lit, artifact-free images. Reconstruction methods for low-light images can produce well-lit counterparts, but typically at the cost of high-frequency details critical for downstream tasks. We propose Diffusion in the Dark (DiD), a diffusion model for low-light image reconstruction for text recognition. DiD provides qualitatively competitive reconstructions with that of state-of-the-art (SOTA), while preserving high-frequency details even in extremely noisy, dark conditions. We demonstrate that DiD, without any task-specific optimization, can outperform SOTA low-light methods in low-light text recognition on real images, bolstering the potential of diffusion models to solve ill-posed inverse problems.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Diffusion_in_the_Dark_A_Diffusion_Model_for_Low-Light_Text_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_Diffusion_in_the_Dark_A_Diffusion_Model_for_Low-Light_Text_WACV_2024_paper.pdf,https://ccnguyen.github.io/diffusion-in-the-dark/,,2303.04291,main,Poster,https://ieeexplore.ieee.org/document/10484330/,"['Computer vision', 'Costs', 'Text recognition', 'Inverse problems', 'Computational modeling', 'Reconstruction algorithms', 'Noise measurement']","['Diffusion Model', 'Optical Character Recognition', 'Dark Conditions', 'Reconstruction Method', 'Inverse Problem', 'Low Light Conditions', 'High-level Tasks', 'Low-light Image', 'Convolutional Neural Network', 'Denoising', 'Exposure Levels', 'Probabilistic Model', 'Image Segmentation', 'Low Light', 'Reversible Process', 'Generative Adversarial Networks', 'Latent Space', 'Number Of Scales', 'Variational Autoencoder', 'Supplement For Details', 'Noisy Images', 'Noisy Conditions', 'Forward Process', 'White Balance', 'Corner Cases', 'Text Dataset', 'Inference Phase', 'Reconstruction Quality', 'Ablation']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",8,"Capturing images is a key part of automation for high-level tasks such as scene text recognition. Low-light conditions pose a challenge for high-level perception stacks, which are often optimized on well-lit, artifact-free images. Reconstruction methods for low-light images can produce well-lit counterparts, but typically at the cost of high-frequency details critical for downstream tasks. We propose Diffusion in the Dark (DiD), a diffusion model for low-light image reconstruction for text recognition. DiD provides qualitatively competitive reconstructions with that of state-of-the-art (SOTA), while preserving high-frequency details even in extremely noisy, dark conditions. We demonstrate that DiD, without any task-specific optimization, can outperform SOTA low-light methods in low-light text recognition on real images, bolstering the potential of diffusion models to solve ill-posed inverse problems. Our code and pretrained models can be found on https://ccnguyen.github.io/diffusion-in-the-dark/."
Diffusion-Based Generation of Histopathological Whole Slide Images at a Gigapixel Scale,"Robert Harb, Thomas Pock, Heimo Müller","Institute of Computer Graphics and Vision, Graz University of Technology, Austria; Diagnostic and Research Institute of Pathology, Medical University of Graz, Austria",100.0,Austria,0.0,,"We present a novel diffusion-based approach to generate synthetic histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel scale. Synthetic WSIs have many potential applications: They can augment training datasets to enhance the performance of many computational pathology applications. They allow the creation of synthesized copies of datasets that can be shared without violating privacy regulations. Or they can facilitate learning representations of WSIs without requiring data annotations. Despite this variety of applications, no existing deep-learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA- BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs.",https://openaccess.thecvf.com/content/WACV2024/html/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Harb_Diffusion-Based_Generation_of_Histopathological_Whole_Slide_Images_at_a_Gigapixel_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484337/,"['Training', 'Pathology', 'Data privacy', 'Computer vision', 'Image resolution', 'Image synthesis', 'Annotations']","['Image Generation', 'Slide Images', 'Histopathological Images', 'Training Dataset', 'Quantitative Evaluation', 'User Study', 'Diffusion Model', 'Low-resolution Images', 'Synthetic Images', 'Digital Pathology', 'Spatial Resolution', 'Methods For Details', 'High-resolution Images', 'Image Quality', 'Images Of Samples', 'Feature Space', 'Diffusion Process', 'Ordinary Differential Equations', 'Latent Space', 'Good Trade-off', 'Noisy Images', 'Stochastic Differential Equations', 'Relaxation Parameter', 'Coarse Structure', 'Ordinary Differential Equation Solver', 'Pixel Spacing', 'Real Ones']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Biomedical / healthcare / medicine']",5,"We present a novel diffusion-based approach to generate synthetic histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel scale. Synthetic WSIs have many potential applications: They can augment training datasets to enhance the performance of many computational pathology applications. They allow the creation of synthesized copies of datasets that can be shared without violating privacy regulations. Or they can facilitate learning representations of WSIs without requiring data annotations. Despite this variety of applications, no existing deep-learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs."
Discovering and Mitigating Biases in CLIP-Based Image Editing,"Md Mehrab Tanjim, Krishna Kumar Singh, Kushal Kafle, Ritwik Sinha, Garrison W. Cottrell",Adobe Research; UC San Diego,50.0,USA,50.0,USA,"In recent years, the use of CLIP (Contrastive Language-Image Pre-Training) has become increasingly popular in a wide range of downstream applications, including zero-shot image classification and text-to-image synthesis. Despite being trained on a vast dataset, the CLIP model has been found to exhibit biases against certain protected attributes, such as gender and race. While previous research has focused on the impact of such biases on image classification, there has been little investigation into their effects on CLIP-based generative tasks. In this paper, we aim to address this gap in the literature by uncovering the queries for which the CLIP model introduces biases in the text-based image editing task. Through a series of experiments, we demonstrate that these biases can have a significant impact on the quality and content of the generated images. To mitigate these biases, we propose a debiasing technique that does not require retraining either the CLIP model or the underlying generative model. Our results show that our proposed framework can effectively reduce the impact of biases in CLIP-based image editing models. Overall, this paper highlights the importance of addressing biases in CLIP-based generative tasks and provides practical solutions that can be readily adopted by researchers and practitioners working in this area.",https://openaccess.thecvf.com/content/WACV2024/html/Tanjim_Discovering_and_Mitigating_Biases_in_CLIP-Based_Image_Editing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tanjim_Discovering_and_Mitigating_Biases_in_CLIP-Based_Image_Editing_WACV_2024_paper.pdf,mehrab-tanjim.github.io/Debiasing-CLIP-based-Editing,,,main,Poster,https://ieeexplore.ieee.org/document/10483690/,"['Computer vision', 'Computational modeling', 'Computer architecture', 'Task analysis', 'Image classification']","['Image Editing', 'Protective Properties', 'Impact Of Bias', 'Profession', 'Imaging Modalities', 'Classification Task', 'Input Image', 'ImageNet', 'Image Pairs', 'Latent Space', 'Language Model', 'Gradient-based Optimization', 'Female Faces', 'Latent Code', 'Specific Queries', 'Target Text', 'Image Encoder', 'Personal Photos', 'Text Encoder', 'Text Modality', 'Text Query', 'Root Mean Square Error', 'Weak Predictor', 'Use Of Imaging', 'Feature Maps', 'Image Collection']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",2,"In recent years, the use of CLIP (Contrastive Language-Image Pre-Training) has become increasingly popular in a wide range of downstream applications, including zero-shot image classification and text-to-image synthesis. Despite being trained on a vast dataset, the CLIP model has been found to exhibit biases against certain protected attributes, such as gender and race. While previous research has focused on the impact of such biases on image classification, there has been little investigation into their effects on CLIP-based generative tasks. In this paper, we aim to address this gap in the literature by uncovering the queries for which the CLIP model introduces biases in the text-based image editing task. Through a series of experiments, we demonstrate that these biases can have a significant impact on the quality and content of the generated images. To mitigate these biases, we propose a debiasing technique that does not require retraining either the CLIP model or the underlying generative model. Our results show that our proposed framework can effectively reduce the impact of biases in CLIP-based image editing models. Overall, this paper highlights the importance of addressing biases in CLIP-based generative tasks and provides practical solutions that can be readily adopted by researchers and practitioners working in this area. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Discriminator-Free Unsupervised Domain Adaptation for Multi-Label Image Classification,"Inder Pal Singh, Enjie Ghorbel, Anis Kacem, Arunkumar Rathinam, Djamila Aouada","Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; High Institute of Multimedia Arts (ISAMM), University of Manouba, Tunisia",100.0,"Luxembourg, Tunisia",0.0,,"In this paper, a discriminator-free adversarial-based Unsupervised Domain Adaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as DDA-MLIC is proposed. Recently, some attempts have been made for introducing adversarial-based UDA methods in the context of MLIC. However, these methods which rely on an additional discriminator subnet present one major shortcoming. The learning of domain-invariant features may harm their task-specific discriminative power, since the classification and discrimination tasks are decoupled. Herein, we propose to overcome this issue by introducing a novel adversarial critic that is directly deduced from the task-specific classifier. Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on the source and target predictions in order to distinguish between two clusters. This allows extracting a Gaussian distribution for each component. The resulting Gaussian distributions are then used for formulating an adversarial loss based on a Frechet distance. The proposed method is evaluated on several multi-label image datasets covering three different types of domain shift. The obtained results demonstrate that DDA-MLIC outperforms existing state-of-the-art methods in terms of precision while requiring a lower number of parameters. The code is publicly available at github.com/cvi2snt/DDA-MLIC.",https://openaccess.thecvf.com/content/WACV2024/html/Singh_Discriminator-Free_Unsupervised_Domain_Adaptation_for_Multi-Label_Image_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Singh_Discriminator-Free_Unsupervised_Domain_Adaptation_for_Multi-Label_Image_Classification_WACV_2024_paper.pdf,,github.com/cvi2snt/DDA-MLIC,2301.10611,main,Poster,https://ieeexplore.ieee.org/document/10484046/,"['Training', 'Computer vision', 'Codes', 'Fitting', 'Gaussian distribution', 'Task analysis', 'Gaussian mixture model']","['Image Classification', 'Domain Adaptation', 'Multi-label Image', 'Multi-label Image Classification', 'Domain Shift', 'Discrimination Task', 'Gaussian Mixture Model', 'Order Prediction', 'Domain-invariant Features', 'Unsupervised Domain Adaptation Methods', 'Data Sources', 'Semantic Segmentation', 'Average Precision', 'Prediction Probability', 'Predictive Distribution', 'Target Domain', 'Target Data', 'Source Domain', 'Mean Average Precision', 'Target Dataset', 'Source Dataset', 'Asymmetric Loss', 'Domain Discriminator', 'Adversarial Approach', 'Domain Gap', 'Mixture Weights', 'Object Labels', 'Discrepancy Measure', 'Adversarial Domain Adaptation', 'Negative Labels']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding']",4,"In this paper, a discriminator-free adversarial-based Unsupervised Domain Adaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as DDA-MLIC is proposed. Recently, some attempts have been made for introducing adversarial-based UDA methods in the context of MLIC. However, these methods, which rely on an additional discriminator subnet present one major shortcoming. The learning of domain-invariant features may harm their task-specific discriminative power, since the classification and discrimination tasks are decoupled. Herein, we propose to overcome this issue by introducing a novel adversarial critic that is directly deduced from the task-specific classifier. Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on the source and target predictions in order to distinguish between two clusters. This allows extracting a Gaussian distribution for each component. The resulting Gaussian distributions are then used for formulating an adversarial loss based on a Fréchet distance. The proposed method is evaluated on several multi-label image datasets covering three different types of domain shift. The obtained results demonstrate that DDA-MLIC outperforms existing state-of-the-art methods in terms of precision while requiring a lower number of parameters. The code is publicly available at github.com/cvi2snt/DDA-MLIC.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Disentangled Pre-Training for Image Matting,"Yanda Li, Zilong Huang, Gang Yu, Ling Chen, Yunchao Wei, Jianbo Jiao","Beijing Jiaotong University, China; Tencent; University of Birmingham, UK; University of Technology Sydney, Australia",75.0,"Australia, China, UK",25.0,China,"Image matting requires high-quality pixel-level human annotations to support the training of a deep model in recent literature. Whereas such annotation is costly and hard to scale, significantly holding back the development of the research. In this work, we make the first attempt towards addressing this problem, by proposing a self-supervised pre-training approach that can leverage infinite numbers of data to boost the matting performance. The pre-training task is designed in a similar manner as image matting, where random trimap and alpha matte are generated to achieve an image disentanglement objective. The pre-trained model is then used as an initialisation of the downstream matting task for fine-tuning. Extensive experimental evaluations show that the proposed approach outperforms both the state-of-the-art matting methods and other alternative self-supervised initialisation approaches by a large margin. We also show the robustness of the proposed approach over different backbone architectures. Our project page is available at https://crystraldo.github.io/dpt_mat/.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Disentangled_Pre-Training_for_Image_Matting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Disentangled_Pre-Training_for_Image_Matting_WACV_2024_paper.pdf,https://crystraldo.github.io/dpt_mat/,https://github.com/crystraldo/dpt_mat,2304.00784,main,Poster,https://ieeexplore.ieee.org/document/10484476/,"['Training', 'Computer vision', 'Annotations', 'Computer architecture', 'Robustness', 'Task analysis']","['Image Matting', 'Large Margin', 'Alpha Matte', 'Pre-training Tasks', 'Input Image', 'Data Augmentation', 'Natural Images', 'Representation Learning', 'Language Model', 'Background Image', 'Unlabeled Data', 'Background Regions', 'Synthetic Images', 'Self-supervised Learning', 'Contrastive Loss', 'Pre-trained Weights', 'Pseudo Labels', 'Foreground Objects', 'Sum Of Absolute Differences', 'Unknown Regions', 'Foreground Regions', 'Self-supervised Learning Methods', 'Fine-tuning Stage', 'Masked Language Model', 'Pretext Task', 'Objective Function', 'Deep Learning', 'Affine Transformation']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Image recognition and understanding']",,"Image matting requires high-quality pixel-level human annotations to support the training of a deep model in recent literature. Whereas such annotation is costly and hard to scale, significantly holding back the development of the research. In this work, we make the first attempt towards addressing this problem, by proposing a self-supervised pretraining approach that can leverage infinite numbers of data to boost the matting performance. The pre-training task is designed in a similar manner as image matting, where random trimap and alpha matte are generated to achieve an image disentanglement objective. The pre-trained model is then used as an initialisation of the downstream matting task for fine-tuning. Extensive experimental evaluations show that the proposed approach outperforms both the state-of-the-art matting methods and other alternative self-supervised initialisation approaches by a large margin. We also show the robustness of the proposed approach over different backbone architectures. Our project page is available at https://crystraldo.github.io/dpt_mat/."
Distortion-Disentangled Contrastive Learning,"Jinfeng Wang, Sifan Song, Jionglong Su, S. Kevin Zhou","School of BME & Suzhou Institute for Advanced Research, Center for Medical Imaging, Robotics, Analytic Computing & Learning (MIRACLE), University of Science and Technology of China, Suzhou, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China; School of AIAC, Xi’an Jiaotong-liverpool University, Suzhou, China; School of AIAC, Xi’an Jiaotong-liverpool University, Suzhou, China; School of BME & Suzhou Institute for Advanced Research, Center for Medical Imaging, Robotics, Analytic Computing & Learning (MIRACLE), University of Science and Technology of China, Suzhou, China",100.0,China,0.0,,"Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single objective function to extract the distortion invariant representation (DIR), which describes the proximity of positive-pair representations affected by different distortions. This objective function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, some recent studies have shown that proper use of DVR in contrastive can optimize the performance of models in some downstream domain-specific tasks. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a novel POCL framework named Distortion-Disentangled Contrastive Learning (DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to explicitly and adaptively disentangle and exploit the DVR inside the model and feature stream to improve the overall representation utilization efficiency, robustness, and representation ability. Experiments demonstrate our framework's superiority to Barlow Twins and Simsiam in terms of convergence, representation quality (Including transferability and generality), and robustness on several benchmark datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Distortion-Disentangled_Contrastive_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Distortion-Disentangled_Contrastive_Learning_WACV_2024_paper.pdf,,,2303.05066,main,Poster,https://ieeexplore.ieee.org/document/10484358/,"['Training', 'Representation learning', 'Computer vision', 'Adaptation models', 'Memory management', 'Self-supervised learning', 'Distortion']","['Self-supervised Learning', 'Model Performance', 'Objective Function', 'Batch Size', 'Representation Learning', 'Representation Ability', 'Augmentation Strategy', 'Quality Of Representations', 'Loss Function', 'Hyperparameters', 'Positive Samples', 'Feature Space', 'Negative Samples', 'Weight Decay', 'Stochastic Gradient Descent', 'Generalization Capability', 'Gaussian Blur', 'Content Features', 'Convergence Performance', 'Encoder Layer', 'Pretext Task', 'Cross-correlation Matrix', 'Disentangled Representation', 'Large Batch Size', 'Information Distortion', 'Memory Bank']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single objective function to extract the distortion invariant representation (DIR) which describes the proximity of positive-pair representations affected by different distortions. This objective function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, some recent studies have shown that proper use of DVR in contrastive can optimize the performance of models in some downstream domain-specific tasks. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a novel POCL framework named Distortion-Disentangled Contrastive Learning (DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to explicitly and adaptively disentangle and exploit the DVR inside the model and feature stream to improve the representation utilization efficiency, robustness and representation ability. Experiments demonstrate our framework’s superiority to Barlow Twins and Simsiam in terms of convergence, representation quality (including transferability and generalization), and robustness on several datasets."
Diverse Imagenet Models Transfer Better,"Niv Nayman, Avram Golbert, Asaf Noy, Lihi Zelnik-Manor",Technion - Israel Institute of Technology; Mobileye; Google Research,33.33333333333333,Israel,66.66666666666667,Israel,"A commonly accepted hypothesis is that models with higher accuracy on Imagenet perform better on other downstream tasks, leading to much research dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been challenged by evidence showing that self-supervised models transfer better than their supervised counterparts, despite their inferior Imagenet accuracy. This calls for identifying the additional factors, on top of Imagenet accuracy, that make models transferable. In this work we show that high diversity of the filters learnt by the model promotes transferability jointly with Imagenet accuracy. Encouraged by the recent transferability results of self-supervised models, we use a simple procedure to combine self-supervised and supervised pretraining and generate models with both high diversity and high accuracy, and as a result high transferability. We experiment with several architectures and multiple downstream tasks, including both single-label and multi-label classification.",https://openaccess.thecvf.com/content/WACV2024/html/Nayman_Diverse_Imagenet_Models_Transfer_Better_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nayman_Diverse_Imagenet_Models_Transfer_Better_WACV_2024_paper.pdf,,,2204.09134,main,Poster,https://ieeexplore.ieee.org/document/10483759/,"['Analytical models', 'Computer vision', 'Filters', 'Computer architecture', 'Task analysis']","['High Diversity', 'Multi-label', 'Deep Neural Network', 'Image Classification', 'Unsupervised Learning', 'Controlled Manner', 'Transfer Learning', 'Properties Of Model', 'Object Classification', 'Transfer Model', 'Cycle Control', 'Class Separation', 'Label Information', 'Self-supervised Learning', 'Linear Probe', 'Target Dataset', 'Principal Directions', 'Vision Transformer', 'Self-supervised Learning Methods', 'Different Levels Of Diversity', 'Notion Of Diversity']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"A commonly accepted hypothesis is that models with higher accuracy on Imagenet perform better on other downstream tasks, leading to much research dedicated to optimizing Imagenet accuracy. Recently this hypothesis has been challenged by evidence showing that self-supervised models transfer better than their supervised counterparts, despite their inferior Imagenet accuracy. This calls for identifying the additional factors, on top of Imagenet accuracy, that make models transferable. In this work we show that high diversity of the filters learnt by the model promotes transferability jointly with Imagenet accuracy. Encouraged by the recent transferability results of self-supervised models, we use a simple procedure to combine self-supervised and supervised pretraining and generate models with both high diversity and high accuracy, and as a result high transferability. We experiment with several architectures and multiple downstream tasks, including both single-label and multi-label classification."
Do VSR Models Generalize Beyond LRS3?,"Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Eustache LeBihan, Haithem Boussaid, Ebtesam Almazrouei, Merouane Debbah","Dublin City University, Ireland; Technology Innovation Institute, UAE",100.0,"Ireland, UAE",0.0,,"The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of intense research in visual speech recognition (VSR) during the last few years. As a result, there is an increased risk of overfitting to its excessively used test set, which is only one hour duration. To alleviate this issue, we build a new VSR test set by closely following the LRS3 dataset creation processes. We then evaluate and analyse the extent to which the current VSR models generalize to the new test data. We evaluate a broad range of publicly available VSR models and find significant drops in performance on our test set, compared to their corresponding LRS3 results. Our results suggest that the increase in word error rates is caused by the models' inability to generalize to slightly ""harder"" and more realistic lip sequences than those found in the LRS3 test set. Our new test benchmark will be made public in order to enable future research towards more robust VSR models.",https://openaccess.thecvf.com/content/WACV2024/html/Djilali_Do_VSR_Models_Generalize_Beyond_LRS3_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Djilali_Do_VSR_Models_Generalize_Beyond_LRS3_WACV_2024_paper.pdf,,https://github.com/YasserdahouML/VSRL_test_set,,main,Poster,https://ieeexplore.ieee.org/document/10483898/,"['Visualization', 'Computer vision', 'Analytical models', 'Error analysis', 'Lips', 'Computational modeling', 'Speech recognition']","['Visual Speech Recognition', 'Visual Recognition', 'Benchmark Test', 'Focus Of Intense Research', 'Word Error Rate', 'Model Performance', 'Computational Efficiency', 'Utterances', 'Matrix Factorization', 'Short Video', 'Whispering', 'Original Test', 'Self-supervised Learning', 'Vocabulary Size', 'Distribution Gap', 'Clear Drop', 'Head Pose', 'Tensor Representation', 'Tucker Decomposition', 'Original Test Set', 'YouTube']","['Algorithms', 'Video recognition and understanding']",,"The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of intense research in visual speech recognition (VSR) during the last few years. As a result, there is an increased risk of overfitting to its excessively used test set, which is only one hour duration. To alleviate this issue, we build a new VSR test set named WildVSR, by closely following the LRS3 dataset creation processes. We then evaluate and analyse the extent to which the current VSR models generalize to the new test data. We evaluate a broad range of publicly available VSR models and find significant drops in performance on our test set, compared to their corresponding LRS3 results. Our results suggest that the increase in word error rates is caused by the models’ inability to generalize to slightly ""harder"" and in the wild lip sequences than those found in the LRS3 test set. Our new test benchmark is made public in order to enable future research towards more robust VSR models."
Do We Still Need Non-Maximum Suppression? Accurate Confidence Estimates and Implicit Duplication Modeling With IoU-Aware Calibration,"Johannes Gilg, Torben Teepe, Fabian Herzog, Philipp Wolters, Gerhard Rigoll",Technical University Munich,100.0,Germany,0.0,,"Object detectors are at the heart of many semi- and fully autonomous decision systems and are poised to become even more indispensable. They are, however, still lacking in accessibility and can sometimes produce unreliable predictions. Especially concerning in this regard are the (essentially hand-crafted) non-maximum suppression algorithms that lead to an obfuscated prediction process and biased confidence estimates. We show that we can eliminate classic NMS-style post-processing by using IoU-aware calibration. IoU-aware calibration is a conditional Beta calibration; this makes it parallelizable with no hyper-parameters. Instead of arbitrary cutoffs or discounts, it implicitly accounts for the likelihood of each detection being a duplicate and adjusts the confidence score accordingly, resulting in empirically based precision estimates for each detection. Our extensive experiments on diverse detection architectures show that the proposed IoU-aware calibration can successfully model duplicate detections and improve calibration. Compared to the standard sequential NMS and calibration approach, our joint modeling can deliver performance gains over the best NMS-based alternative while producing consistently better-calibrated confidence predictions with less complexity.",https://openaccess.thecvf.com/content/WACV2024/html/Gilg_Do_We_Still_Need_Non-Maximum_Suppression_Accurate_Confidence_Estimates_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gilg_Do_We_Still_Need_Non-Maximum_Suppression_Accurate_Confidence_Estimates_and_WACV_2024_paper.pdf,,https://github.com/Blueblue4/IoU-AwareCalibration,2309.03110,main,Poster,https://ieeexplore.ieee.org/document/10484186/,"['Heart', 'Computer vision', 'Detectors', 'Computer architecture', 'Performance gain', 'Predictive models', 'Prediction algorithms']","['Non-maximum Suppression', 'Precise Estimates', 'Autonomic System', 'Object Detection', 'Confidence Score', 'Prediction Confidence', 'Confidence Estimation', 'Architecture For Detection', 'Duplicate Detection', 'Ablation', 'Performance Metrics', 'Parametrized', 'Intersection Over Union', 'Logistic Function', 'Bounding Box', 'Detection Probability', 'Calibration Method', 'Confidence Value', 'Decision-making Algorithm', 'Conditional Dependence', 'Multivariate Calibration', 'True Positive Detection', 'Negative Log-likelihood', 'Scoring Rules', 'Histogram Bins', 'Independent Calibration', 'Ground Truth Object']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding']",,"Object detectors are at the heart of many semi- and fully autonomous decision systems and are poised to become even more indispensable. They are, however, still lacking in accessibility and can sometimes produce unreliable predictions. Especially concerning in this regard are the—essentially hand-crafted—non-maximum suppression algorithms that lead to an obfuscated prediction process and biased confidence estimates. We show that we can eliminate classic NMS-style post-processing by using IoU-aware calibration. IoU-aware calibration is a conditional Beta calibration; this makes it parallelizable with no hyper-parameters. Instead of arbitrary cutoffs or discounts, it implicitly accounts for the likelihood of each detection being a duplicate and adjusts the confidence score accordingly, resulting in empirically based precision estimates for each detection. Our extensive experiments on diverse detection architectures show that the proposed IoU-aware calibration can successfully model duplicate detections and improve calibration. Compared to the standard sequential NMS and calibration approach, our joint modeling can deliver performance gains over the best NMS-based alternative while producing consistently better-calibrated confidence predictions with less complexity. The code for all our experiments is publicly available 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
DocReal: Robust Document Dewarping of Real-Life Images via Attention-Enhanced Control Point Prediction,"Fangchen Yu, Yina Xie, Lei Wu, Yafei Wen, Guozhi Wang, Shuai Ren, Xiaoxin Chen, Jianfeng Mao, Wenye Li","The Chinese University of Hong Kong, Shenzhen, China; The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Research Institute of Big Data, Shenzhen, China; vivo AI Lab, Shenzhen, China",75.0,"China, Hong Kong",25.0,China,"Document image dewarping is a crucial task in computer vision with numerous practical applications. The control point method, as a popular image dewarping approach, has attracted attention due to its simplicity and efficiency. However, inaccurate control point prediction due to varying background noises and deformation types can result in unsatisfactory performance. To address these issues, we propose a robust document dewarping approach for real-life images, namely DocReal, which utilizes Enet to effectively remove background noise and an attention-enhanced control point (AECP) module to better capture local deformations. Moreover, we augment the training data by synthesizing 2D images with 3D deformations and additional deformation types. Our proposed method achieves state-of-the-art performance on the DocUNet benchmark and a newly proposed benchmark of 200 Chinese distorted images, exhibiting superior dewarping accuracy, OCR performance, and robustness to various types of image distortion.",https://openaccess.thecvf.com/content/WACV2024/html/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yu_DocReal_Robust_Document_Dewarping_of_Real-Life_Images_via_Attention-Enhanced_Control_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484027/,"['Computer vision', 'Three-dimensional displays', 'Deformation', 'Optical character recognition', 'Training data', 'Benchmark testing', 'Distortion']","['Control Points', 'Real-life Images', 'Distortion', 'Training Data', '2D Images', 'Image Distortion', 'Local Deformation', 'Optical Character Recognition', 'Unsatisfactory Performance', 'Distortion Types', 'Remove Background Noise', '3D Deformation', 'Training Dataset', '3D Structure', 'Convolutional Layers', 'Readability', 'Real-world Data', 'Benchmark Datasets', 'Attention Module', 'Residual Block', 'Deformation Range', 'Accurate Capture', 'Training Data Augmentation', 'Text Types', '3D Mesh', '3D Coordinates', 'Residual Background', 'Amount Of Training Data']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Low-level and physics-based vision']",,"Document image dewarping is a crucial task in computer vision with numerous practical applications. The control point method, as a popular image dewarping approach, has attracted attention due to its simplicity and efficiency. However, inaccurate control point prediction due to varying background noises and deformation types can result in unsatisfactory performance. To address these issues, we propose a robust document dewarping approach for real-life images, namely DocReal, which utilizes Enet to effectively remove background noise and an attention-enhanced control point (AECP) module to better capture local deformations. Moreover, we augment the training data by synthesizing 2D images with 3D deformations and additional deformation types. Our proposed method achieves state-of-the-art performance on the DocUNet benchmark and a newly proposed benchmark of 200 Chinese distorted images, exhibiting superior dewarping accuracy, OCR performance, and robustness to various types of image distortion."
Domain Adaptive 3D Shape Retrieval From Monocular Images,"Harsh Pal, Ritwik Khandelwal, Shivam Pande, Biplab Banerjee, Srikrishna Karanam","IIT Bombay, Mumbai, India; Adobe Research, Bengaluru, India",50.0,India,50.0,USA,"In this work, we address the novel and challenging problem of domain adaptive 3D shape retrieval from single 2D images (DA-IBSR). While the existing image-based 3D shape retrieval (IBSR) problem focuses on modality alignment for retrieving a matchable 3D shape from a shape repository given a 2D image query, it does not consider any distribution shift between the training and testing image-shape pairs, making the performance of off-the-shelves IBSR methods subpar. In contrast, the proposed DA-IBSR addresses the non-trivial problem of modality shift as well distribution shift across training and test sets. To address these issues, we propose an end-to-end trainable model called DAIS-NET. Our objective is to align the images and shapes separately from both domains while simultaneously learn a shared embedding space for the 2D and 3D modalities. The former problem is addressed by separately employing maximum mean discrepancy loss across the 2D images and 3D shapes of the two domains. To address the modality alignment, we incorporate the notion of negative sample mining and employ triplet loss to bridge the gap between positive 2D-3D pairs (of same class) and increase the separation between negative 2D-3D pairs (of different class). Additionally, we employ an entropy minimization strategy to align the unlabeled target domain data in the semantic space. To evaluate our proposed approach, we define the experimental setting of DA-IBSR on the following benchmarks: SHREC'14 <-> Pix3D and ShapeNet <-> SHREC'14. Considering the novelty of the problem statement, we have demonstrated that the issue of domain gap is prevalent by comparing our method with the existing literature. Additionally, through extensive evaluations, we demonstrate the capability of DAIS-NET to successfully mitigate this domain gap in image based 3D shape retrieval.",https://openaccess.thecvf.com/content/WACV2024/html/Pal_Domain_Adaptive_3D_Shape_Retrieval_From_Monocular_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pal_Domain_Adaptive_3D_Shape_Retrieval_From_Monocular_Images_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484426/,"['Training', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Semantics', 'Benchmark testing', 'Minimization']","['3D Shape', 'Domain Adaptation', 'Shape Retrieval', '3D Shape Retrieval', 'Single Image', 'Negative Samples', '2D Images', 'Target Domain', 'Shared Space', 'Minimum Entropy', 'Triplet Loss', 'Pair Of Classes', 'Semantic Space', 'Query Image', '3D Mode', 'Maximum Mean Discrepancy', 'Target Domain Data', 'Computer Vision', 'Feature Space', 'Classification Of Samples', 'Source Domain', 'Absence Of Labels', 'Domain Alignment', 'Samples Of The Same Class', '3D Domain', 'Metric Learning', 'Generative Adversarial Networks', 'Paired Datasets', 'Color Information', 'Reproducing Kernel Hilbert Space']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In this work, we address the novel and challenging problem of domain adaptive 3D shape retrieval from single 2D images (DA-IBSR). While the existing image-based 3D shape retrieval (IBSR) problem focuses on modality alignment for retrieving a matchable 3D shape from a shape repository given a 2D image query, it does not consider any distribution shift between the training and testing image-shape pairs, making the performance of off-the-shelves IBSR methods subpar. In contrast, the proposed DA-IBSR addresses the non-trivial problem of modality shift as well distribution shift across training and test sets. To address these issues, we propose an end-to-end trainable model called DAIS-NET. Our objective is to align the images and shapes separately from both domains while simultaneously learn a shared embedding space for the 2D and 3D modalities. The former problem is addressed by separately employing maximum mean discrepancy loss across the 2D images and 3D shapes of the two domains. To address the modality alignment, we incorporate the notion of negative sample mining and employ triplet loss to bridge the gap between positive 2D-3D pairs (of same class) and increase the separation between negative 2D-3D pairs (of different class). Additionally, we employ an entropy minimization strategy to align the unlabeled target domain data in the semantic space. To evaluate our proposed approach, we define the experimental setting of DA-IBSR on the following benchmarks: SHREC’14 ↔ Pix3D and ShapeNet ↔ SHREC’14. Considering the novelty of the problem statement, we have demonstrated that the issue of domain gap is prevalent by comparing our method with the existing literature. Additionally, through extensive evaluations, we demonstrate the capability of DAIS-NET to successfully mitigate this domain gap in image based 3D shape retrieval."
Domain Aligned CLIP for Few-Shot Classification,"Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl",Amazon,0.0,,100.0,USA,"Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve inter-modal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Gondal_Domain_Aligned_CLIP_for_Few-Shot_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gondal_Domain_Aligned_CLIP_for_Few-Shot_Classification_WACV_2024_paper.pdf,,,2311.09191,main,Poster,https://ieeexplore.ieee.org/document/10483755/,"['Representation learning', 'Computer vision', 'Adaptation models', 'Costs', 'Computational modeling', 'Benchmark testing', 'Robustness']","['Few-shot Classification', 'Classification Task', 'Domain Shift', 'Domain Adaptation', 'Target Distribution', 'Image Classification Tasks', 'Contrast Objective', 'Validation Set', 'Classification Performance', 'Visual Representation', 'Visual Features', 'Class Labels', 'ImageNet', 'Latent Space', 'Language Model', 'Target Domain', 'Image Representation', 'Class Assignment', 'Linear Layer', 'Self-supervised Learning', 'Visual Adaptation', 'Adaptive Layer', 'Image Embedding', 'Feature Alignment', 'Text Representation', 'Text Modality', 'Visual Domain', 'Transfer Learning', 'Vocabulary']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding']",5,"Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve intermodal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP’s parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks."
Domain Generalisation via Risk Distribution Matching,"Toan Nguyen, Kien Do, Bao Duong, Thin Nguyen","Applied Artificial Intelligence Institute, Deakin University, Australia",100.0,Australia,0.0,,"We propose a novel approach for domain generalisation (DG) leveraging risk distributions to characterise domains, thereby achieving domain invariance. In our findings, risk distributions effectively highlight differences between training domains and reveal their inherent complexities. In testing, we may observe similar, or potentially intensifying in magnitude, divergences between risk distributions. Hence, we propose a compelling proposition: Minimising the divergences between risk distributions across training domains leads to robust invariance for DG. The key rationale behind this concept is that a model, trained on domain-invariant or stable features, may consistently produce similar risk distributions across various domains. Building upon this idea, we propose Risk Distribution Matching (RDM). Using the maximum mean discrepancy (MMD) distance, RDM aims to minimise the variance of risk distributions across training domains. However, when the number of domains increases, the direct optimisation of variance leads to linear growth in MMD computations, resulting in inefficiency. Instead, we propose an approximation that requires only one MMD computation, by aligning just two distributions: that of the worst-case domain and the aggregated distribution from all domains. Notably, this method empirically outperforms optimising distributional variance while being computationally more efficient. Unlike conventional DG matching algorithms, RDM stands out for its enhanced efficacy by concentrating on scalar risk distributions, sidestepping the pitfalls of high-dimensional challenges seen in feature or gradient matching. Our extensive experiments on standard benchmark datasets demonstrate that RDM shows superior generalisation capability over state-of-the-art DG methods.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Domain_Generalisation_via_Risk_Distribution_Matching_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_Domain_Generalisation_via_Risk_Distribution_Matching_WACV_2024_paper.pdf,,,2310.18598,main,Poster,https://ieeexplore.ieee.org/document/10484089/,"['Training', 'Computer vision', 'Aggregates', 'Buildings', 'Benchmark testing', 'Approximation algorithms', 'Computational efficiency']","['Domain Generalization', 'Distribution Matching', 'Benchmark', 'Divergence', 'Extensive Experiments', 'Stable Characteristics', 'Linear Growth', 'Number Of Domains', 'Distribution Of Aggregates', 'Training Domain', 'Maximum Mean Discrepancy', 'Standard Benchmark Datasets', 'Loss Function', 'Batch Size', 'Kernel Function', 'Types Of Algorithms', 'Radial Basis Function Kernel', 'Training Iterations', 'Robust Optimization', 'Empirical Risk Minimization', 'Unseen Domains', 'Reproducing Kernel Hilbert Space', 'High Computational Demand', 'Domain-invariant Features', 'Matching Coefficient', 'Approximate Version', 'Domain Alignment']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"We propose a novel approach for domain generalisation (DG) leveraging risk distributions to characterise domains, thereby achieving domain invariance. In our findings, risk distributions effectively highlight differences between training domains and reveal their inherent complexities. In testing, we may observe similar, or potentially intensifying in magnitude, divergences between risk distributions. Hence, we propose a compelling proposition: Minimising the divergences between risk distributions across training domains leads to robust invariance for DG. The key rationale behind this concept is that a model, trained on domain-invariant or stable features, may consistently produce similar risk distributions across various domains. Building upon this idea, we propose Risk Distribution Matching (RDM). Using the maximum mean discrepancy (MMD) distance, RDM aims to minimise the variance of risk distributions across training domains. However, when the number of domains increases, the direct optimisation of variance leads to linear growth in MMD computations, resulting in inefficiency. Instead, we propose an approximation that requires only one MMD computation, by aligning just two distributions: that of the worst-case domain and the aggregated distribution from all domains. Notably, this method empirically outperforms optimising distributional variance while being computationally more efficient. Unlike conventional DG matching algorithms, RDM stands out for its enhanced efficacy by concentrating on scalar risk distributions, sidestepping the pitfalls of high-dimensional challenges seen in feature or gradient matching. Our extensive experiments on standard benchmark datasets demonstrate that RDM shows superior generalisation capability over state-of-the-art DG methods."
Domain Generalization With Correlated Style Uncertainty,"Zheyuan Zhang, Bin Wang, Debesh Jha, Ugur Demir, Ulas Bagci","Machine & Hybrid Intelligence Lab, Northwestern University, USA",100.0,USA,0.0,,"Domain generalization (DG) approaches intend to extract domain invariant features that can lead to a more robust deep learning model. In this regard, style augmentation is a strong DG method taking advantage of instance-specific feature statistics containing informative style characteristics to synthetic novel domains. While it is one of the state-of-the-art methods, prior works on style augmentation have either disregarded the interdependence amongst distinct feature channels or have solely constrained style augmentation to linear interpolation. To address these research gaps, in this work, we introduce a novel augmentation approach, named Correlated Style Uncertainty (CSU), surpassing the limitations of linear interpolation in style statistic space and simultaneously preserving vital correlation information. Our method's efficacy is established through extensive experimentation on diverse cross-domain computer vision and medical imaging classification tasks: PACS, Office-Home, and Camelyon17 datasets, and the Duke-Market1501 instance retrieval task. The results showcase a remarkable improvement margin over existing state-of-the-art techniques. The source code is available: https://github.com/freshman97/CSU.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Domain_Generalization_With_Correlated_Style_Uncertainty_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Domain_Generalization_With_Correlated_Style_Uncertainty_WACV_2024_paper.pdf,,https://github.com/freshman97/CSU,2212.09950,main,Poster,https://ieeexplore.ieee.org/document/10484431/,"['Deep learning', 'Interpolation', 'Computer vision', 'Uncertainty', 'Correlation', 'Computational modeling', 'Source coding']","['Domain Generalization', 'Deep Learning', 'Linear Interpolation', 'Domain-invariant Features', 'Eigenvalues', 'Covariance Matrix', 'Correlation Matrix', 'Test Dataset', 'Eigenvectors', 'Batch Size', 'Feature Space', 'Data Augmentation', 'Domain Shift', 'Regularization Term', 'Target Domain', 'Beta Distribution', 'Ablation Experiments', 'Domain Adaptation', 'Source Domain', 'Feature Alignment', 'Insertion Position', 'SOTA Methods', 'Training Domain', 'Target Domain Data']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Domain generalization (DG) approaches intend to extract domain invariant features that can lead to a more robust deep learning model. In this regard, style augmentation is a strong DG method taking advantage of instance-specific feature statistics containing informative style characteristics to synthetic novel domains. While it is one of the state-of-the-art methods, prior works on style augmentation have either disregarded the interdependence amongst distinct feature channels or have solely constrained style augmentation to linear interpolation. To address these research gaps, in this work, we introduce a novel augmentation approach, named Correlated Style Uncertainty (CSU), surpassing the limitations of linear interpolation in style statistic space and simultaneously preserving vital correlation information. Our method’s efficacy is established through extensive experimentation on diverse cross-domain computer vision and medical imaging classification tasks: PACS, Office-Home, and Camelyon17 datasets, and the Duke-Market1501 instance retrieval task. The results showcase a remarkable improvement margin over existing state-of-the-art techniques. The source code is available https://github.com/freshman97/CSU."
Domain Generalization by Rejecting Extreme Augmentations,"Masih Aminbeidokhti, Fidel A. Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Eric Granger, Marco Pedersoli","LIVIA, Dept. of Systems Engineering, ETS Montreal, Canada",100.0,Canada,0.0,,"Data augmentation is one of the most powerful techniques for regularizing deep learning models and improving their recognition performance in a variety of tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-domain, in which the test data follows a different and unknown distribution, the best recipe for data augmentation is not clear. In this paper, we show that also for out-domain or domain generalization settings, data augmentation can bring a conspicuous and robust improvement in performance. For doing that, we propose a simple procedure: i) use uniform sampling on standard data augmentation transformations ii) increase transformations strength to adapt to the higher data variance expected when working out of domain iii) devise a new reward function to reject extreme transformations that can harm the training. With this simple formula, our data augmentation scheme achieves comparable or better results to state-of-the-art performance on most domain generalization datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Aminbeidokhti_Domain_Generalization_by_Rejecting_Extreme_Augmentations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Aminbeidokhti_Domain_Generalization_by_Rejecting_Extreme_Augmentations_WACV_2024_paper.pdf,,https://github.com/Masseeh/DCAug,,main,Poster,https://ieeexplore.ieee.org/document/10483736/,"['Training', 'Deep learning', 'Computer vision', 'Codes', 'Computational modeling', 'Data augmentation', 'Picture archiving and communication systems']","['Domain Generalization', 'Training Data', 'Data Augmentation', 'Reward Function', 'Unknown Distribution', 'Standard Transformation', 'Improve Recognition Performance', 'Performance In A Variety Of Tasks', 'Classification Task', 'Search Space', 'Parametrized', 'Class Labels', 'ImageNet', 'Latent Space', 'Evaluation Protocol', 'Variety Of Domains', 'Domain Classifier', 'Source Domain', 'Original Meaning', 'Training Domain', 'Empirical Risk Minimization', 'Maximum Mean Discrepancy', 'Domain-invariant Representations', 'Semantic Image', 'Object Classification Tasks', 'Unseen Domains']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Data augmentation is one of the most effective techniques for regularizing deep learning models and improving recognition performance in various tasks and domains. However, this holds for standard in-domain settings, in which the training and test data follow the same distribution. For the out-of-domain case, where the test data follow a different and unknown distribution, the best recipe for data augmentation is unclear. In this paper, we show that for out-of-domain and domain generalization settings, data augmentation can provide a conspicuous and robust improvement in performance. To do that, we propose a simple training procedure: (i) use uniform sampling on standard data augmentation transformations; (ii) increase the strength transformations to account for the higher data variance expected when working out-of-domain, and (iii) devise a new reward function to reject extreme transformations that can harm the training. With this procedure, our data augmentation scheme achieves a level of accuracy comparable to or better than state-of-the-art methods on benchmark domain generalization datasets. Code: https://github.com/Masseeh/DCAug"
Driving Through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving,"Jessica Echterhoff, An Yan, Kyungtae Han, Amr Abdelraouf, Rohit Gupta, Julian McAuley","University of California, San Diego; InfoTech Labs, Toyota Motor North America R&D",50.0,USA,50.0,USA,"Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup.",https://openaccess.thecvf.com/content/WACV2024/html/Echterhoff_Driving_Through_the_Concept_Gridlock_Unraveling_Explainability_Bottlenecks_in_Automated_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Echterhoff_Driving_Through_the_Concept_Gridlock_Unraveling_Explainability_Bottlenecks_in_Automated_WACV_2024_paper.pdf,,https://github.com/jessicamecht/concept_gridlock,2310.16639,main,Poster,https://ieeexplore.ieee.org/document/10483611/,"['Visualization', 'Uncertainty', 'Process control', 'Machine learning', 'Linguistics', 'Predictive models', 'Transformers']","['Automated Vehicles', 'Visual Features', 'Autonomous Vehicles', 'Changes In Preferences', 'Vehicle Behavior', 'Neural Network', 'Close Proximity', 'Decision-making Process', 'Deep Neural Network', 'Image Features', 'Natural Language', 'Recurrent Neural Network', 'Attention Mechanism', 'Multilayer Perceptron', 'Self-driving', 'Conceptual Space', 'Content Words', 'Steering Angle', 'Temporal Coding', 'Vision Transformer', 'Angle Prediction', 'Visual Explanation', 'Scene Description', 'Correct Concept', 'Backbone Feature', 'Prediction Procedure', 'Highway', 'Root Mean Square Error', 'Language Model', 'Traffic Light']","['Applications', 'Autonomous Driving', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Dual Domain Diffusion Guidance for 3D CBCT Metal Artifact Reduction,"Yongjin Choi, Doeyoung Kwon, Seung Jun Baek","Korea University, Seoul, South Korea",100.0,South Korea,0.0,,"Previous methods to solve the problem of metal artifact reduction (MAR) have mostly focused on 2D MAR, making it challenging to apply to problems with 3-dimensional CT such as CBCT. In this paper, we propose a novel approach for 3D MAR which utilizes two diffusion models to model the metal-free CBCT prior and metal artifact prior. Through dual-domain guidance in the image and projection domains, the 3D connectivity is enhanced in the restored images. Moreover, we propose a memory-efficient technique for an efficient sampling of 3-dimensional data, which reduces the memory usage by orders of magnitude. Experiments show that our method achieves the state-of-the-art performance not only with synthetic data but also with real-world clinical and out-of-distribution data.",https://openaccess.thecvf.com/content/WACV2024/html/Choi_Dual_Domain_Diffusion_Guidance_for_3D_CBCT_Metal_Artifact_Reduction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Choi_Dual_Domain_Diffusion_Guidance_for_3D_CBCT_Metal_Artifact_Reduction_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484068/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Computed tomography', 'Memory management', 'Metals', 'Training data']","['Metal Artifacts', 'Dual Domain', 'Metal Artifact Reduction', 'Diffusion Model', 'Image Domain', 'Projection Domain', 'Convolutional Neural Network', 'Gaussian Noise', 'Scoring Function', 'Sagittal Plane', 'Transverse Plane', 'Reversible Process', 'Coronal Plane', 'Inverse Problem', 'Posterior Mean', 'Dental Implants', 'Electrical Noise', 'Cone Beam', 'Forward Process', 'Metal Insertion', 'Types Of Artifacts', '3D Projection', 'CT Slices', 'Parameters A1', 'Imaging Equipment', '3D Domain', '3D Problem', 'Posterior Probability', 'Tube Voltage']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",2,"Previous methods to solve the problem of metal artifact reduction (MAR) have mostly focused on 2D MAR, making it challenging to apply to problems with 3-dimensional CT such as CBCT. In this paper, we propose a novel approach for 3D MAR which utilizes two diffusion models to model the metal-free CBCT prior and metal artifact prior. Through dual-domain guidance in the image and projection domains, the 3D connectivity is enhanced in the restored images. Moreover, we propose a memory-efficient technique for an efficient sampling of 3-dimensional data, which reduces the memory usage by orders of magnitude. Experiments show that our method achieves the state-of-the-art performance not only with synthetic data but also with real-world clinical and out-of-distribution data."
Dynamic Multimodal Information Bottleneck for Multimodality Classification,"Yingying Fang, Shuang Wu, Sheng Zhang, Chaoyan Huang, Tieyong Zeng, Xiaodan Xing, Simon Walsh, Guang Yang","The Chinese University of Hong Kong, Shatin, Hong Kong; National Heart and Lung Institute, Imperial College London, London, SW7 2AZ, UK; Black Sesame Technologies, Fusionopolis, Singapore; National Heart and Lung Institute, Imperial College London, London, SW7 2AZ, UK; Bioengineering Department and Imperial-X, Imperial College London, London, W12 7SL, UK; Bioengineering Department and Imperial-X, Imperial College London, London, W12 7SL, UK",83.33333333333334,"Hong Kong, UK",16.666666666666657,Singapore,"Effectively leveraging multimodal data such as various images, laboratory tests and clinical information is gaining traction in a variety of AI-based medical diagnosis and prognosis tasks. Most existing multi-modal techniques only focus on enhancing their performance by leveraging the differences or shared features from various modalities and fusing feature across different modalities. These approaches are generally not optimal for clinical settings, which pose the additional challenges of limited training data, as well as being rife with redundant data or noisy modality channels, leading to subpar performance. To address this gap, we study the robustness of existing methods to data redundancy and noise and propose a generalized dynamic multimodal information bottleneck framework for attaining a robust fused feature representation. Specifically, our information bottleneck module serves to filter out the task-irrelevant information and noises in the fused feature, and we further introduce a sufficiency loss to prevent dropping of task-relevant information, thus explicitly preserving the sufficiency of prediction information in the distilled feature. We validate our model on an in-house and a public COVID-19 dataset for mortality prediction as well as two public biomedical datasets for diagnostic tasks. Extensive experiments show that our method surpasses the state-of-the-art and is significantly more robust, being the only method to remain performant when large-scale noisy channels exist. Our code is publicly available at https://github.com/Anonymous-PaperSubmission/DMIB.",https://openaccess.thecvf.com/content/WACV2024/html/Fang_Dynamic_Multimodal_Information_Bottleneck_for_Multimodality_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fang_Dynamic_Multimodal_Information_Bottleneck_for_Multimodality_Classification_WACV_2024_paper.pdf,,https://github.com/ayanglab/DMIB,2311.01066,main,Poster,https://ieeexplore.ieee.org/document/10484170/,"['Noise', 'Redundancy', 'Training data', 'Predictive models', 'Robustness', 'Noise measurement', 'Medical diagnosis']","['Information Bottleneck', 'Multimodal Classification', 'Training Data', 'Feature Representation', 'Redundant Data', 'Noisy Channels', 'Task-relevant Information', 'Limited Training Data', 'Magnetic Resonance Imaging', 'Deep Learning', 'Electronic Health Records', 'Clinical Variables', 'Validation Set', 'Mutual Information', 'Noisy Data', 'Fusion Method', '3D Scanning', 'COVID-19 Diagnosis', 'Redundant Features', 'Fusion Strategy', 'Multimodal Learning', 'Multimodal Tasks', 'Fusion Performance', 'Mutual Information Estimation', 'Pure Noise', 'Multimodal Features', 'Input Noise', 'Subjective Bias', 'Neural Network', 'Genetic Information']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Vision + language and/or other modalities']",1,"Effectively leveraging multimodal data such as various images, laboratory tests and clinical information is becoming increasingly attractive in a variety of AI-based medical diagnosis and prognosis tasks. Most existing multi-modal techniques only focus on enhancing their performance by leveraging the differences or shared features from various modalities and fusing feature across different modalities. These approaches are generally not optimal for clinical settings, which pose the additional challenges of limited training data, as well as being rife with redundant data or noisy modality channels, leading to subpar performance. To address this gap, we study the robustness of existing methods to data redundancy and noise and propose a generalized dynamic multimodal information bottleneck framework for attaining a robust fused feature representation. Specifically, our information bottleneck module serves to filter out the task-irrelevant information and noises in the fused feature, and we further introduce a sufficiency loss to prevent dropping of task-relevant information, thus explicitly preserving the sufficiency of prediction information in the distilled feature. We validate our model on an in-house and a public COVID19 dataset for mortality prediction as well as two public biomedical datasets for diagnostic tasks. Extensive experiments show that our method surpasses the state-of-the-art and is significantly more robust, being the only method to remain performance when large-scale noisy channels exist. Our code is publicly available at https://github.com/ayanglab/DMIB."
Dynamic Token-Pass Transformers for Semantic Segmentation,"Yuang Liu, Qiang Zhou, Jing Wang, Zhibin Wang, Fan Wang, Jun Wang, Wei Zhang","DAMO Academy, Alibaba Group; East China Normal University",100.0,China,0.0,,"Vision transformers (ViT) usually extract features via forwarding all the tokens in the self-attention layers from top to toe. In this paper, we introduce dynamic token-pass vision transformers (DoViT) for semantic segmentation, which can adaptively reduce the inference cost for images with different complexity. DoViT gradually stops partial easy tokens from self-attention calculation and keeps the hard tokens forwarding until meeting the stopping criteria. We employ lightweight auxiliary heads to make the token-pass decision and divide the tokens into keeping/stopping parts. With a token separate calculation, the self-attention layers are speeded up with sparse tokens and still work friendly with hardware. A token reconstruction module is built to collect and reset the grouped tokens to their original position in the sequence, which is necessary to predict correct semantic masks. We conduct extensive experiments on two common semantic segmentation tasks, and demonstrate that our method greatly reduces about 40%   60% FLOPs and the drop of mIoU is within 0.8% for various segmentation transformers. The throughput and inference speed of ViT-L/B are increased to more than 2x on Cityscapes.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Dynamic_Token-Pass_Transformers_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Dynamic_Token-Pass_Transformers_for_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/FLHonker/DoViT-code,2308.01944,main,Poster,https://ieeexplore.ieee.org/document/10483763/,"['Costs', 'Semantic segmentation', 'Semantics', 'Predictive models', 'Transformers', 'Feature extraction', 'Throughput']","['Semantic Segmentation', 'Throughput', 'Sequence Position', 'Floating-point Operations', 'Self-attention Layer', 'Vision Transformer', 'Inference Cost', 'Validation Set', 'Feature Maps', 'Image Patches', 'Confidence Threshold', 'Embedding Dimension', 'Prediction Confidence', 'Extra Parameters', 'Inference Phase', 'Architecture For Segmentation', 'Multi-head Self-attention', 'Impact Of Threshold', 'Sequence Of Tokens', 'Acceleration Method', 'Dynamic Inference']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Vision transformers (ViT) usually extract features via forwarding all the tokens in the self-attention layers from top to toe. In this paper, we introduce dynamic token-pass vision transformers (DoViT) for semantic segmentation, which can adaptively reduce the inference cost for images with different complexity. DoViT gradually stops partial easy tokens from self-attention calculation and keeps the hard tokens forwarding until meeting the stopping criteria. We employ lightweight auxiliary heads to make the token-pass decision and divide the tokens into keeping/stopping parts. With a token separate calculation, the self-attention layers are speeded up with sparse tokens and still work friendly with hardware. A token reconstruction module is built to collect and reset the grouped tokens to their original position in the sequence, which is necessary to predict correct semantic masks. We conduct extensive experiments on two common semantic segmentation tasks, and demonstrate that our method greatly reduces about 40% ∼ 60% FLOPs and the drop of mIoU is within 0.8% for various segmentation transformers. The throughput and inference speed of ViT-L/B are increased to more than 2× on Cityscapes. Code is available at https://github.com/FLHonker/DoViT-code."
EASUM: Enhancing Affective State Understanding Through Joint Sentiment and Emotion Modeling for Multimodal Tasks,"Yewon Hwang, Jong-Hwan Kim","School of Electrical Engineering, KAIST",100.0,South Korea,0.0,,"Multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER) tasks have gained a surge of attention in recent years. Although both tasks share common ground in many ways, they are often treated as a separate task. In this work, we propose, EASUM, a new training scheme for bridging the MSA and MER tasks. EASUM aims to bring mutual benefits to both tasks based on the premise that the sentiment and emotion are closely related; hence each information should provide deeper insight into one's affective state to complement the other. We exploit this premise to further improve the performance of each task by 1) first training a domain general model using four benchmark datasets from the MSA and MER tasks: CMU-MOSI, CMU-MOSEI, MELD, and IEMOCAP. Depending on the dataset, the domain general model learns to predict sentiment or emotion values based on the domain invariant features. 2) Then these values are later used as auxiliary pseudo labels when training a domain specific model for each task. Our premise as well as new training scheme are validated through extensive experiments on the four benchmark datasets. The results also demonstrate that the proposed method outperforms the state-of-the-art on the CMU-MOSI, CMU-MOSEI, and MELD datasets, and performs comparable to the state-of-the-art on the IEMOCAP dataset while using approximately 40% fewer parameters.",https://openaccess.thecvf.com/content/WACV2024/html/Hwang_EASUM_Enhancing_Affective_State_Understanding_Through_Joint_Sentiment_and_Emotion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hwang_EASUM_Enhancing_Affective_State_Understanding_Through_Joint_Sentiment_and_Emotion_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484028/,"['Training', 'Measurement', 'Sentiment analysis', 'Emotion recognition', 'Benchmark testing', 'Predictive models', 'Data models']","['Affective States', 'Sentiment Model', 'Task Performance', 'Training Strategy', 'Benchmark Datasets', 'Emotion Recognition', 'Task Model', 'Sentiment Analysis', 'Domain Generalization', 'Emotional Value', 'Pseudo Labels', 'Emotion Recognition Task', 'Domain-invariant Features', 'Sentiment Values', 'Domain-specific Models', 'Visual Information', 'Visual Features', 'Data Augmentation', 'Disgust', 'Attention Mechanism', 'Soft Labels', 'Multimodal Representation', 'Training Domain', 'Moment Matching', 'Multimodal Learning', 'Acoustic Data', 'Emotion Categories', 'Text Data', 'Sentiment Polarity', 'Domain Alignment']","['Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Psychology and cognitive science']",,"Multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER) tasks have gained a surge of attention in recent years. Although both tasks share common ground in many ways, they are often treated as a separate task. In this work, we propose, EASUM, a new training scheme for bridging the MSA and MER tasks. EASUM aims to bring mutual benefits to both tasks based on the premise that the sentiment and emotion are closely related; hence each information should provide deeper insight into one’s affective state to complement the other. We exploit this premise to further improve the performance of each task by 1) first training a domain general model using four benchmark datasets from the MSA and MER tasks: CMU-MOSI, CMU-MOSEI, MELD, and IEMOCAP. Depending on the dataset, the domain general model learns to predict sentiment or emotion values based on the domain invariant features. 2) Then these values are later used as auxiliary pseudo labels when training a domain specific model for each task. Our premise as well as new training scheme are validated through extensive experiments on the four benchmark datasets. The results also demonstrate that the proposed method outperforms the state-of-the-art on the CMU-MOSI, CMU-MOSEI, and MELD datasets, and performs comparable to the state-of-the-art on the IEMOCAP dataset while using approximately 40% fewer parameters."
ECSIC: Epipolar Cross Attention for Stereo Image Compression,"Matthias Wödlinger, Jan Kotera, Manuel Keglevic, Jan Xu, Robert Sablatnig","Deep Render; UTIA CAS, Czechia; TU Wien",66.66666666666666,"Austria, Czech Republic",33.33333333333334,USA,"In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance in stereo image compression on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding.",https://openaccess.thecvf.com/content/WACV2024/html/Wodlinger_ECSIC_Epipolar_Cross_Attention_for_Stereo_Image_Compression_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wodlinger_ECSIC_Epipolar_Cross_Attention_for_Stereo_Image_Compression_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484133/,"['Computer vision', 'Image coding', 'Computational modeling', 'Estimation', 'Benchmark testing', 'Entropy', 'Decoding']","['Image Compression', 'Stereo Images', 'Mutual Information', 'Image Pairs', 'Attention Module', 'Left Image', 'Popular Datasets', 'Popular Image', 'Contextual Modulation', 'Stereo Image Pairs', 'Decoding', 'Convolutional Layers', 'Single Image', 'Baseline Methods', 'Latent Representation', 'Bitstream', 'Color Of Samples', 'Least Significant Bit', 'Most Significant Bit', 'Compression Method', 'Entropy Model', 'Stereo Pairs', 'Entropy Parameter', 'Hyperprior', 'Quadratic Complexity', 'Encoder Module', 'Decoder Layer', 'Entropy Coding', 'Laplace Distribution', 'Image Stream']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance in stereo image compression on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding."
ENIGMA-51: Towards a Fine-Grained Understanding of Human Behavior in Industrial Scenarios,"Francesco Ragusa, Rosario Leonardi, Michele Mazzamuto, Claudia Bonanno, Rosario Scavo, Antonino Furnari, Giovanni Maria Farinella","FPV@IPLab - University of Catania, Italy; Next Vision s.r.l. - Spinoff of the University of Catania, Italy; FPV@IPLab - University of Catania, Italy",100.0,Italy,0.0,,"ENIGMA-51 is a new egocentric dataset acquired in an industrial scenario by 19 subjects who followed instructions to complete the repair of electrical boards using industrial tools (e.g., electric screwdriver) and equipments (e.g., oscilloscope). The 51 egocentric video sequences are densely annotated with a rich set of labels that enable the systematic study of human behavior in the industrial domain. We provide benchmarks on four tasks related to human behavior: 1) untrimmed temporal detection of human-object interactions, 2) egocentric human-object interaction detection, 3) short-term object interaction anticipation and 4) natural language understanding of intents and entities. Baseline results show that the ENIGMA-51 dataset poses a challenging benchmark to study human behavior in industrial scenarios. We publicly release the dataset at https://iplab.dmi.unict.it/ENIGMA-51.",https://openaccess.thecvf.com/content/WACV2024/html/Ragusa_ENIGMA-51_Towards_a_Fine-Grained_Understanding_of_Human_Behavior_in_Industrial_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ragusa_ENIGMA-51_Towards_a_Fine-Grained_Understanding_of_Human_Behavior_in_Industrial_WACV_2024_paper.pdf,https://iplab.dmi.unict.it/ENIGMA-51,,,main,Poster,https://ieeexplore.ieee.org/document/10484165/,"['Computer vision', 'Systematics', 'Video sequences', 'Oscilloscopes', 'Benchmark testing', 'Maintenance engineering', 'Natural language processing']","['Human Behavior', 'Behavior In Scenarios', 'Oscilloscope', 'Detection Of Interactions', 'Object Interaction', 'Industrial Tools', 'Industrial Domains', 'Natural Language Understanding', 'Short-term Interactions', 'Rate Measurements', 'Utterances', 'Aspects Of Behavior', 'Object Detection', 'Hand Side', 'Bounding Box', 'Object Motion', 'Industrial Environment', 'Industrial Settings', 'Contact Conditions', 'Future Interactions', 'Hours Of Video', 'Aspects Of Human Behavior', 'Past Frames', 'Action Detection', 'Egocentric Perspective', 'Different Aspects Of Behavior', 'Industrial Objects', 'Industrial Laboratories', 'Foundation Model', 'Video Capture']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Video recognition and understanding']",3,"ENIGMA-51 is a new egocentric dataset acquired in an industrial scenario by 19 subjects who followed instructions to complete the repair of electrical boards using industrial tools (e.g., electric screwdriver) and equipments (e.g., oscilloscope). The 51 egocentric video sequences are densely annotated with a rich set of labels that enable the systematic study of human behavior in the industrial domain. We provide benchmarks on four tasks related to human behavior: 1) untrimmed temporal detection of human-object interactions, 2) egocentric human-object interaction detection, 3) short-term object interaction anticipation and 4) natural language understanding of intents and entities. Baseline results show that the ENIGMA-51 dataset poses a challenging benchmark to study human behavior in industrial scenarios. We publicly release the dataset at https://iplab.dmi.unict.it/ENIGMA-51."
ENTED: Enhanced Neural Texture Extraction and Distribution for Reference-Based Blind Face Restoration,"Yuen-Fui Lau, Tianjia Zhang, Zhefan Rao, Qifeng Chen",HKUST,100.0,Hong Kong,0.0,,"We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module.",https://openaccess.thecvf.com/content/WACV2024/html/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lau_ENTED_Enhanced_Neural_Texture_Extraction_and_Distribution_for_Reference-Based_Blind_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483633/,"['Manifolds', 'Codes', 'Vector quantization', 'Semantics', 'Network architecture', 'Feature extraction', 'Image restoration']","['Extract Texture', 'Texture Distribution', 'Neural Texture', 'Face Restoration', 'Input Image', 'Latent Space', 'Reference Image', 'Real-world Datasets', 'Semantic Features', 'Latent Code', 'High-quality Features', 'Extraction Process', 'Feature Maps', 'Image Dataset', 'Attention Mechanism', 'Distribution Process', 'Peak Signal-to-noise Ratio', 'Channel Dimension', 'Latent Representation', 'Residual Connection', 'Low-quality Images', 'Fréchet Inception Distance', 'Facial Details', 'Information Fidelity', 'Structural Similarity Index Measure', 'StyleGAN', 'Single Image Super-resolution', 'Photo-realistic Images', 'Erroneous Information', 'Perceptual Loss']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Arts / games / social media']",1,"We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image’s manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module."
EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection,"Joonhyun Jeong, Beomyoung Kim, Joonsang Yu, YoungJoon Yoo","NAVER AI Lab; NAVER Cloud, ImageVision; NAVER Cloud",0.0,,100.0,South Korea,"This paper analyzes the design choices of face detection architecture that improve efficiency of computation cost and accuracy. Specifically, we re-examine the effectiveness of the standard convolutional block as a lightweight backbone architecture for face detection. Unlike the current tendency of lightweight architecture design, which heavily utilizes depthwise separable convolution layers, we show that heavily channel-pruned standard convolution layers can achieve better accuracy and inference speed when using a similar parameter size. This observation is supported by the analyses concerning the characteristics of the target data domain, faces. Based on our observation, we propose to employ ResNet with a highly reduced channel, which surprisingly allows high efficiency compared to other mobile-friendly networks (e.g., MobileNetV1, V2, V3). From the extensive experiments, we show that the proposed backbone can replace that of the state-of-the-art face detector with a faster inference speed. Also, we further propose a new feature aggregation method to maximize the detection performance. Our proposed detector EResFD obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA image inference on CPU. Code is available at https://github.com/clovaai/EResFD.",https://openaccess.thecvf.com/content/WACV2024/html/Jeong_EResFD_Rediscovery_of_the_Effectiveness_of_Standard_Convolution_for_Lightweight_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jeong_EResFD_Rediscovery_of_the_Effectiveness_of_Standard_Convolution_for_Lightweight_WACV_2024_paper.pdf,,https://github.com/clovaai/EResFD,2204.01209,main,Poster,https://ieeexplore.ieee.org/document/10483628/,"['Convolutional codes', 'Convolution', 'Computer architecture', 'Detectors', 'Feature extraction', 'Market research', 'Real-time systems']","['Face Detection', 'Standard Convolution', 'Lightweight Face', 'Convolutional Layers', 'Detection Performance', 'Fast Speed', 'Separate Layers', 'Feature Aggregation', 'Fast Inference', 'Architecture For Detection', 'Backbone Architecture', 'Depthwise Separable Convolution', 'Object Detection', 'Receptive Field', 'High-level Features', 'Low-level Features', 'Input Size', 'Residual Block', 'Backbone Network', 'Channel Dimension', 'Floating-point Operations', 'High Mapping', 'Feature Pyramid Network', 'Landmark Detection', 'Enhancement Module', 'Small Face', 'ResNet Architecture', 'Expansion Ratio', 'Depthwise Convolution', 'Two-stage Detectors']","['Algorithms', 'Image recognition and understanding']",1,"This paper analyzes the design choices of face detection architecture that improve efficiency of computation cost and accuracy. Specifically, we re-examine the effectiveness of the standard convolutional block as a lightweight backbone architecture for face detection. Unlike the current tendency of lightweight architecture design, which heavily utilizes depthwise separable convolution layers, we show that heavily channel-pruned standard convolution layers can achieve better accuracy and inference speed when using a similar parameter size. This observation is supported by the analyses concerning the characteristics of the target data domain, faces. Based on our observation, we propose to employ ResNet with a highly reduced channel, which surprisingly allows high efficiency compared to other mobile-friendly networks (e.g., MobileNetV1, V2, V3). From the extensive experiments, we show that the proposed backbone can replace that of the state-of-the-art face detector with a faster inference speed. Also, we further propose a new feature aggregation method to maximize the detection performance. Our proposed detector EResFD obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA image inference on CPU. Code is available at https://github.com/clovaai/EResFD."
Edge Inference With Fully Differentiable Quantized Mixed Precision Neural Networks,"Clemens JS Schaefer, Siddharth Joshi, Shan Li, Raul Blazquez","University of Notre Dame, Notre Dame, IN, USA; Google LLC, Mountain View, CA, USA",50.0,USA,50.0,USA,"The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of pre-trained quantized models, delivering best-in-class accuracy below 4.3 MB of weights and activations without modifying the model architecture. Our main contributions are: (i) a method for tensor-sliced learned precision with a hardware-aware cost function for heterogeneous differentiable quantization, (ii) targeted gradient modification for weights and activations to mitigate quantization errors, and (iii) a multi-phase learning schedule to address instability in learning arising from updates to the learned quantizer and model parameters. We demonstrate the effectiveness of our techniques on the ImageNet dataset across a range of models including EfficientNet-Lite0 (e.g., 4.14MB of weights and activations at 67.66% accuracy) and MobileNetV2 (e.g., 3.51MB weights and activations at 65.39% accuracy).",https://openaccess.thecvf.com/content/WACV2024/html/Schaefer_Edge_Inference_With_Fully_Differentiable_Quantized_Mixed_Precision_Neural_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Schaefer_Edge_Inference_With_Fully_Differentiable_Quantized_Mixed_Precision_Neural_Networks_WACV_2024_paper.pdf,,,2206.07741,main,Poster,https://ieeexplore.ieee.org/document/10484249/,"['Training', 'Performance evaluation', 'Adaptation models', 'Schedules', 'Quantization (signal)', 'Costs', 'Image edge detection']","['Neural Network', 'Mixed-precision', 'Edge Inference', 'Model Parameters', 'Convolutional Neural Network', 'Deep Neural Network', 'Model Architecture', 'ImageNet Dataset', 'Pareto Front', 'Quantization Parameter', 'Quantification Approach', 'Memory Footprint', 'Quantification Model', 'Gradient Scale', 'Learning Rate', 'High Precision', 'Accuracy Rate', 'Dynamic Range', 'Graphics Processing Unit', 'Model Size', 'Black-box Optimization', 'Backward Pass', 'Variation In Accuracy', 'Deep Neural Network Model', 'Quantization Scheme', 'Uniform Quantization', 'Sum Of Activities', 'Hyperbolic Tangent', 'Load Data']","['Applications', 'Smartphones / end user devices', 'Applications', 'Embedded sensing / real-time techniques']",3,"The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques en-compassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new Pareto frontier in model accuracy and memory footprint demonstrating a range of pre-trained quantized models, delivering best-in-class accuracy below 4.3 MB of weights and activations without modifying the model architecture. Our main contributions are: (i) a method for tensor-sliced learned precision with a hardware-aware cost function for heterogeneous differentiable quantization, (ii) targeted gradient modification for weights and activations to mitigate quantization errors, and (iii) a multi-phase learning schedule to address instability in learning arising from updates to the learned quantizer and model parameters. We demonstrate the effectiveness of our techniques on the ImageNet dataset across a range of models including EfficientNet-Lite0 (e.g., 4.14 MB of weights and activations at 67.66% accuracy) and MobileNetV2 (e.g., 3.51 MB weights and activations at 65.39% accuracy)."
Effective Restoration of Source Knowledge in Continual Test Time Adaptation,"Fahim Faisal Niloy, Sk Miraj Ahmed, Dripta S. Raychaudhuri, Samet Oymak, Amit K. Roy-Chowdhury","AWS AI Labs; University of Michigan, Ann Arbor; University of California, Riverside",66.66666666666666,USA,33.33333333333334,USA,"Traditional test-time adaptation (TTA) methods face significant challenges in adapting to dynamic environments characterized by continuously changing long-term target distributions. These challenges primarily stem from two factors: catastrophic forgetting of previously learned valuable source knowledge and gradual error accumulation caused by miscalibrated pseudo labels. To address these issues, this paper introduces an unsupervised domain change detection method that is capable of identifying domain shifts in dynamic environments and subsequently resets the model parameters to the original source pre-trained values. By restoring the knowledge from the source, it effectively corrects the negative consequences arising from the gradual deterioration of model parameters caused by ongoing shifts in the domain. Our method involves progressive estimation of global batch-norm statistics specific to each domain, while keeping track of changes in the statistics triggered by domain shifts. Importantly, our method is agnostic to the specific adaptation technique employed and thus, can be incorporated to existing TTA methods to enhance their performance in dynamic environments. We perform extensive experiments on benchmark datasets to demonstrate the superior performance of our method compared to state-of-the-art adaptation methods.",https://openaccess.thecvf.com/content/WACV2024/html/Niloy_Effective_Restoration_of_Source_Knowledge_in_Continual_Test_Time_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Niloy_Effective_Restoration_of_Source_Knowledge_in_Continual_Test_Time_Adaptation_WACV_2024_paper.pdf,,,2311.04991,main,Poster,https://ieeexplore.ieee.org/document/10483747/,"['Adaptation models', 'Computer vision', 'Estimation', 'Benchmark testing', 'Robustness', 'Faces']","['Continuous Adaptation', 'Test-time Adaptation', 'Model Parameters', 'Adaptive Method', 'Dynamic Environment', 'Domain Shift', 'Target Distribution', 'Global Statistics', 'Error Accumulation', 'Statistical Changes', 'Pseudo Labels', 'Catastrophic Forgetting', 'Data Sources', 'Average Time', 'Test Data', 'Deep Neural Network', 'ImageNet', 'Kullback-Leibler', 'Real-world Scenarios', 'Semantic Segmentation', 'Unseen Domains', 'Target Domain', 'Batch Tests', 'Current Batch', 'Source Domain', 'Dynamic Environmental Changes', 'Source Model', 'Domain Adaptation', 'Value Of Momentum', 'Peak Detection Algorithm']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Traditional test-time adaptation (TTA) methods face significant challenges in adapting to dynamic environments characterized by continuously changing long-term target distributions. These challenges primarily stem from two factors: catastrophic forgetting of previously learned valuable source knowledge and gradual error accumulation caused by miscalibrated pseudo labels. To address these issues, this paper introduces an unsupervised domain change detection method that is capable of identifying domain shifts in dynamic environments and subsequently resets the model parameters to the original source pre-trained values. By restoring the knowledge from the source, it effectively corrects the negative consequences arising from the gradual deterioration of model parameters caused by ongoing shifts in the domain. Our method involves progressive estimation of global batch-norm statistics specific to each domain, while keeping track of changes in the statistics triggered by domain shifts. Importantly, our method is agnostic to the specific adaptation technique employed and thus, can be incorporated to existing TTA methods to enhance their performance in dynamic environments. We perform extensive experiments on benchmark datasets to demonstrate the superior performance of our method compared to state-of-the-art adaptation methods."
Effects of Markers in Training Datasets on the Accuracy of 6D Pose Estimation,"Janis Rosskamp, Rene Weller, Gabriel Zachmann","Computer Graphics and Virtual Reality, University of Bremen",100.0,Germany,0.0,,"Collecting training data for pose estimation methods on images is a time-consuming task and usually involves some kind of manual labeling of the 6D pose of objects. This time could be reduced considerably by using marker-based tracking that would allow for automatic labeling of training images. However, images containing markers may reduce the accuracy of pose estimation due to a bias introduced by the markers. In this paper, we analyze the influence of markers in training images on pose estimation accuracy. We investigate the accuracy of estimated poses for three different cases: i) training on images with markers, ii) removing markers by inpainting, and iii) augmenting the dataset with randomly generated markers to reduce spatial learning of marker features. Our results demonstrate that utilizing marker-based techniques is an effective strategy for collecting large amounts of ground truth data for pose prediction. Moreover, our findings suggest that the usage of inpainting techniques do not reduce prediction accuracy. Additionally, we investigate the effect of inaccuracies of labeling in training data on prediction accuracy. We show that the precise ground truth data obtained through marker tracking proves to be superior compared to markerless datasets if labeling errors of 6D ground truth exist. Our data generation tools are available online: https://github.com/JHRosskamp/6DPoseDataGenTools",https://openaccess.thecvf.com/content/WACV2024/html/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rosskamp_Effects_of_Markers_in_Training_Datasets_on_the_Accuracy_of_WACV_2024_paper.pdf,,https://github.com/JHRosskamp/6DPoseDataGenTools,,main,Poster,https://ieeexplore.ieee.org/document/10483829/,"['Training', 'Computer vision', 'Pose estimation', 'Training data', 'Manuals', 'Optical imaging', 'Adaptive optics']","['Accurate Estimation', 'Pose Estimation', 'Accuracy Of Pose Estimation', '6D Pose', '6D Pose Estimation', 'Prediction Accuracy', 'Training Data', 'Training Images', 'Time-consuming Task', 'Manual Labeling', 'Object Pose', 'Labeling Errors', 'Tracking Markers', 'Pose Prediction', 'Training Set', 'Deep Learning', 'Percentage Points', 'Color Images', 'Point Cloud', 'Random Locations', 'Image Inpainting', 'Inpainting', 'Real Training Data', 'Real Training', 'Depth Images', 'Real Test', 'Rotation Error', 'Effective Imaging', 'Errors In Dataset', 'Ground Truth Pose']","['Algorithms', 'Datasets and evaluations', 'Algorithms', '3D computer vision']",2,"Collecting training data for pose estimation methods on images is a time-consuming task and usually involves some kind of manual labeling of the 6D pose of objects. This time could be reduced considerably by using marker-based tracking that would allow for automatic labeling of training images. However, images containing markers may reduce the accuracy of pose estimation due to a bias introduced by the markers. In this paper, we analyze the influence of markers in training images on pose estimation accuracy. We investigate the accuracy of estimated poses for three different cases: i) training on images with markers, ii) removing markers by inpainting, and iii) augmenting the dataset with randomly generated markers to reduce spatial learning of marker features. Our results demonstrate that utilizing marker-based techniques is an effective strategy for collecting large amounts of ground truth data for pose prediction. Moreover, our findings suggest that the usage of inpainting techniques do not reduce prediction accuracy. Additionally, we investigate the effect of inaccuracies of labeling in training data on prediction accuracy. We show that the precise ground truth data obtained through marker tracking proves to be superior compared to markerless datasets if labeling errors of 6D ground truth exist. Our data generation tools are available online: https://github.com/JHRosskamp/6DPoseDataGenTools"
Efficient Expansion and Gradient Based Task Inference for Replay Free Incremental Learning,"Soumya Roy, Vinay Verma, Deepak Gupta",Amazon/Duke University; Amazon,50.0,USA,50.0,USA,"This paper proposes a simple but highly efficient expansion-based model for continual learning. The recent feature transformation, masking and factorization-based methods are efficient, but they grow the model only over the global or shared parameter. Therefore, these approaches do not fully utilize the previously learned information because the same task-specific parameter forgets the earlier knowledge. Thus, these approaches show limited transfer learning ability. Moreover, most of these models have constant parameter growth for all tasks, irrespective of the task complexity. Our work proposes a simple filter and channel expansion-based method that grows the model over the previous task parameters and not just over the global parameter. Therefore, it fully utilizes all the previously learned information without forgetting, which results in better knowledge transfer. The growth rate in our proposed model is a function of task complexity; therefore for a simple task, the model has a smaller parameter growth, while for complex tasks, the model requires more parameters to adapt to the current task. Recent expansion-based models show promising results for task incremental learning (TIL). However, for class incremental learning (CIL), prediction of task id is a crucial challenge; hence, their results degrade rapidly as the number of tasks increase. In this work, we propose a robust task prediction method that leverages entropy weighted data augmentations and the model's gradient using pseudo labels. We evaluate our model on various datasets and architectures in the TIL, CIL and generative continual learning settings. The proposed approach shows state-of-the-art results in all these settings. Our extensive ablation studies show the efficacy of the proposed components.",https://openaccess.thecvf.com/content/WACV2024/html/Roy_Efficient_Expansion_and_Gradient_Based_Task_Inference_for_Replay_Free_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Roy_Efficient_Expansion_and_Gradient_Based_Task_Inference_for_Replay_Free_WACV_2024_paper.pdf,,,2312.01188,main,Poster,https://ieeexplore.ieee.org/document/10484061/,"['Adaptation models', 'Transfer learning', 'Predictive models', 'Information filters', 'Data augmentation', 'Entropy', 'Data models']","['Incremental Learning', 'Efficient Expansion', 'Growth Rate', 'Ablation', 'Knowledge Transfer', 'Current Task', 'Global Parameters', 'Previous Tasks', 'Crucial Challenge', 'Pseudo Labels', 'Shared Parameters', 'Model Parameters', 'Feature Maps', 'Average Accuracy', 'Cross-entropy Loss', 'Generative Adversarial Networks', 'Average Growth', 'Task Model', 'Training Tasks', 'Typical Growth', 'CIFAR-100 Dataset', 'Sequential Task', 'Forward Transfer', 'Catastrophic Forgetting', 'Aquatic Mammals', 'Augmented Samples', 'Number Of Filters', 'Mean Gradient', 'Weight Space', 'VGG-16 Architecture']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"This paper proposes a simple but highly efficient expansion-based model for continual learning. The recent feature transformation, masking and factorization-based methods are efficient, but they grow the model only over the global or shared parameter. Therefore, these approaches do not fully utilize the previously learned information because the same task-specific parameter forgets the earlier knowledge. Thus, these approaches show limited transfer learning ability. Moreover, most of these models have constant parameter growth for all tasks, irrespective of the task complexity. Our work proposes a simple filter and channel expansion-based method that grows the model over the previous task parameters and not just over the global parameter. Therefore, it fully utilizes all the previously learned information without forgetting, which results in better knowledge transfer. The growth rate in our proposed model is a function of task complexity; therefore for a simple task, the model has a smaller parameter growth while for complex tasks, the model requires more parameters to adapt to the current task. Recent expansion-based models show promising results for task incremental learning (TIL). However, for class incremental learning (CIL), prediction of task id is a crucial challenge; hence, their results degrade rapidly as the number of tasks increase. In this work, we propose a robust task prediction method that leverages entropy weighted data augmentations and the model’s gradient using pseudo labels. We evaluate our model on various datasets and architectures in the TIL, CIL and generative continual learning settings. The proposed approach shows state-of-the-art results in all these settings. Our extensive ablation studies show the efficacy of the proposed components."
Efficient Explainable Face Verification Based on Similarity Score Argument Backpropagation,"Marco Huber, Anh Thi Luu, Philipp Terhörst, Naser Damer","Paderborn University, Paderborn, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany and Department of Computer Science, TU Darmstadt, Darmstadt, Germany",100.0,Germany,0.0,,"Explainable Face Recognition is gaining growing attention as the use of the technology is gaining ground in security-critical applications. Understanding why two face images are matched or not matched by a given face recognition system is important to operators, users, and developers to increase trust, accountability, develop better systems, and highlight unfair behavior. In this work, we propose a similarity score argument backpropagation (xSSAB) approach that supports or opposes the face-matching decision to visualize spatial maps that indicate similar and dissimilar areas as interpreted by the underlying FR model. Furthermore, we present Patch-LFW, a new explainable face verification benchmark that enables along with a novel evaluation protocol, the first quantitative evaluation of the validity of similarity and dissimilarity maps in explainable face recognition approaches. We compare our efficient approach to state-of-the-art approaches demonstrating a superior trade-off between efficiency and performance. The code as well as the proposed Patch-LFW is publicly available at: https://github.com/marcohuber/xSSAB.",https://openaccess.thecvf.com/content/WACV2024/html/Huber_Efficient_Explainable_Face_Verification_Based_on_Similarity_Score_Argument_Backpropagation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huber_Efficient_Explainable_Face_Verification_Based_on_Similarity_Score_Argument_Backpropagation_WACV_2024_paper.pdf,,https://github.com/marcohuber/xSSAB,,main,Poster,https://ieeexplore.ieee.org/document/10483668/,"['Backpropagation', 'Visualization', 'Computer vision', 'Protocols', 'Frequency modulation', 'Face recognition', 'Computational modeling']","['Similarity Score', 'Face Recognition', 'Benchmark', 'Efficient Approach', 'Face Images', 'Evaluation Protocol', 'Face Recognition Model', 'Deep Learning', 'Error Rate', 'Cross-border', 'Feature Dimension', 'Image Pairs', 'Changes In Output', 'Part Of The Image', 'Single Map', 'Decision Threshold', 'Forward Pass', 'Similarity Map', 'Wrong Decisions', 'Positive Argument', 'False Matches', 'Face Matching', 'Biometric Systems', 'Equal Error Rate', 'Similar Pixels', 'Original Pixel', 'Set Of Arguments']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",3,"Explainable Face Recognition is gaining growing attention as the use of the technology is gaining ground in security-critical applications. Understanding why two face images are matched or not matched by a given face recognition system is important to operators, users, and developers to increase trust, accountability, develop better systems, and highlight unfair behavior. In this work, we propose a similarity score argument backpropagation (xSSAB) approach that supports or opposes the face-matching decision to visualize spatial maps that indicate similar and dissimilar areas as interpreted by the underlying FR model. Furthermore, we present Patch-LFW, a new explainable face verification benchmark that enables along with a novel evaluation protocol, the first quantitative evaluation of the validity of similarity and dissimilarity maps in explainable face recognition approaches. We compare our efficient approach to state-of-the-art approaches demonstrating a superior trade-off between efficiency and performance. The code as well as the proposed Patch-LFW is publicly available at: https://github.com/marcohuber/xSSAB."
Efficient Feature Distillation for Zero-Shot Annotation Object Detection,"Zhuoming Liu, Xuefeng Hu, Ram Nevatia",University of Southern California,100.0,USA,0.0,,"We propose a new setting for detecting unseen objects called Zero-shot Annotation object Detection (ZAD). It expands the zero-shot object detection setting by allowing the novel objects to exist in the training images and restricts the additional information the detector uses to novel category names. Recently, to detect unseen objects, largescale vision-language models (e.g., CLIP) are leveraged by different methods. The distillation-based methods have good overall performance but suffer from a long training schedule caused by two factors. First, existing work creates distillation regions biased to the base categories, which limits the distillation of novel category information. Second, directly using the raw feature from CLIP for distillation neglects the domain gap between the training data of CLIP and the detection datasets, which makes it difficult to learn the mapping from the image region to the vision-language feature space. To solve these problems, we propose Efficient feature distillation for Zero-shot Annotation object Detection (EZAD). Firstly, EZAD adapts the CLIP's feature space to the target detection domain by re-normalizing CLIP; Secondly, EZAD uses CLIP to generate distillation proposals with potential novel category names to avoid the distillation being overly biased toward the base categories. Finally, EZAD takes advantage of semantic meaning for regression to further improve the model performance. As a result, EZAD outperforms the previous distillation-based methods in COCO by 4% with a much shorter training schedule and achieves a 3% improvement on the LVIS dataset. Our code is available at https://github.com/dragonlzm/EZAD",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Efficient_Feature_Distillation_for_Zero-Shot_Annotation_Object_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Efficient_Feature_Distillation_for_Zero-Shot_Annotation_Object_Detection_WACV_2024_paper.pdf,,https://github.com/dragonlzm/EZAD,2303.12145,main,Poster,https://ieeexplore.ieee.org/document/10483901/,"['Training', 'Schedules', 'Annotations', 'Semantics', 'Training data', 'Object detection', 'Detectors']","['Object Detection', 'Feature Distillation', 'Model Performance', 'Training Data', 'Feature Space', 'Image Regions', 'Training Images', 'Base Classifiers', 'Category Information', 'Detection Dataset', 'Training Schedule', 'Category Names', 'Raw Features', 'Domain Gap', 'Classification Accuracy', 'Image Features', 'Convolutional Layers', 'Training Time', 'Large-scale Datasets', 'Classification Datasets', 'Region Proposal Network', 'Frequent Category', 'COCO Dataset', 'Annotated Examples', 'Instances Of Categories', 'Domain Adaptation', 'Meaningful Regions', 'Objective Scores', 'Additional Annotations', 'Base Classes']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",2,"We propose a new setting for detecting unseen objects called Zero-shot Annotation object Detection (ZAD). It expands the zero-shot object detection setting by allowing the novel objects to exist in the training images and restricts the additional information the detector uses to novel category names. Recently, to detect unseen objects, largescale vision-language models (e.g., CLIP) are leveraged by different methods. The distillation-based methods have good overall performance but suffer from a long training schedule caused by two factors. First, existing work creates distillation regions biased to the base categories, which limits the distillation of novel category information. Second, directly using the raw feature from CLIP for distillation neglects the domain gap between the training data of CLIP and the detection datasets, which makes it difficult to learn the mapping from the image region to the vision-language feature space. To solve these problems, we propose Efficient feature distillation for Zero-shot Annotation object Detection (EZAD). Firstly, EZAD adapts the CLIP’s feature space to the target detection domain by re-normalizing CLIP; Secondly, EZAD uses CLIP to generate distillation proposals with potential novel category names to avoid the distillation being overly biased toward the base categories. Finally, EZAD takes advantage of semantic meaning for regression to further improve the model performance. As a result, EZAD outperforms the previous distillation-based methods in COCO by 4% with a much shorter training schedule and achieves a 3% improvement on the LVIS dataset. Our code is available at https://github.com/dragonlzm/EZAD"
Efficient Layout-Guided Image Inpainting for Mobile Use,"Wenbo Li, Yi Wei, Yilin Shen, Hongxia Jin",,,,,,"The layout guidance, which specifies the pixel-wise object distribution, is beneficial to preserving the object boundaries in image inpainting while not hurting model's generalization capability. We aim to design an efficient and robust layout-guided image inpainting method for mobile use, which can achieve the robustness in presence of the mixed scenes where objects with the delicate shape reside next to the hole. Our method is made up of two sub-models, which restore the pixel-information for the hole from coarse to fine, and support each other to overcome the practical challenges encountered when making the whole method lightweight. The layout mask guides the two sub-models, which thus enables the robustness of our method in mixed scenes. We demonstrate the efficiency and robustness of our method via both the experiments and a mobile demo.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Efficient_Layout-Guided_Image_Inpainting_for_Mobile_Use_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Efficient_Layout-Guided_Image_Inpainting_for_Mobile_Use_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484295/,"['Computer vision', 'Shape', 'Layout', 'Robustness', 'Image restoration']","['Image Inpainting', 'Generalization Capability', 'Convolution', 'Training Dataset', 'Input Image', 'Diffusion Method', 'Attention Mechanism', 'Receptive Field', 'Model Refinement', 'Object Shape', 'Gaussian Blur', 'Hadamard Product', 'Attention Weights', 'Shallow Network', 'Large Holes', 'COCO Dataset', 'Contextual Mechanisms', 'Fréchet Inception Distance', 'Coarse Texture', 'Contextual Attention', 'Hole Region', 'Patch Matching', 'Non-parametric', 'Long-range Dependencies', 'Similar Texture', 'Image Plane', 'Low Resolution', 'Image Texture', 'Dropout Rate', 'Airplane']","['Applications', 'Smartphones / end user devices']",,"The layout guidance, which specifies the pixel-wise object distribution, is beneficial to preserving the object boundaries in image inpainting while not hurting model’s generalization capability. We aim to design an efficient and robust layout-guided image inpainting method for mobile use, which can achieve the robustness in presence of the mixed scenes where objects with the delicate shape reside next to the hole. Our method is made up of two sub-models, which restore the pixel-information for the hole from coarse to fine, and support each other to overcome the practical challenges encountered when making the whole method lightweight. The layout mask guides the two sub-models, which thus enables the robustness of our method in mixed scenes. We demonstrate the efficiency and robustness of our method via both the experiments and a mobile demo."
Efficient MAE Towards Large-Scale Vision Transformers,"Qiu Han, Gongjie Zhang, Jiaxing Huang, Peng Gao, Zhang Wei, Shijian Lu","S-Lab, Nanyang Technological University; S-Lab, Nanyang Technological University and Black Sesame Technologies; Shanghai Artificial Intelligence Laboratory",66.66666666666666,Singapore,33.33333333333334,China,"Masked Autoencoder (MAE) has demonstrated superb pre-training efficiency for vision Transformer, thanks to its partial input paradigm and high mask ratio (0.75). However, MAE often suffers from severe performance drop under higher mask ratios, which hinders its potential toward larger-scale vision Transformers. In this work, we identify that the performance drop is largely attributed to the over-dominance of difficult reconstruction targets, as higher mask ratios lead to more sparse visible patches and fewer visual clues for reconstruction. To mitigate this issue, we design Efficient MAE that introduces a novel Difficulty-Flatten Loss and a decoder masking strategy, enabling a higher mask ratio for more efficient pre-training. The Difficulty-Flatten Loss provides balanced supervision on reconstruction targets of different difficulties, mitigating the performance drop under higher mask ratios effectively. Additionally, the decoder masking strategy discards the most difficult reconstruction targets, which further alleviates the optimization difficulty and accelerates the pre-training clearly. Our proposed Efficient MAE introduces 27% and 30% pre-training runtime accelerations for the ViT-Large and ViT-Huge models, provides valuable insights into MAE's optimization, and paves the way for larger-scale vision Transformer pre-training. Code and pre-trained models will be released.",https://openaccess.thecvf.com/content/WACV2024/html/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483982/,"['Measurement', 'Degradation', 'Visualization', 'Computer vision', 'Runtime', 'Computational modeling', 'Transformers']","['Transformer', 'Vision Transformer', 'Visual Clues', 'Optimization Difficulty', 'Masking Strategy', 'Convolutional Neural Network', 'Computational Resources', 'Object Detection', 'Transfer Learning', 'Large Model', 'ImageNet', 'Semantic Segmentation', 'Memory Usage', 'Reconstruction Loss', 'Masked Images', 'Easy Target', 'Patch Density', 'Masked Language Model', 'Hard Targets']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Masked Autoencoder (MAE) has demonstrated superb pre-training efficiency for vision Transformer, thanks to its partial input paradigm and high mask ratio (0.75). However, MAE often suffers from severe performance drop under higher mask ratios, which hinders its potential toward larger-scale vision Transformers. In this work, we identify that the performance drop is largely attributed to the over-dominance of difficult reconstruction targets, as higher mask ratios lead to more sparse visible patches and fewer visual clues for reconstruction. To mitigate this issue, we design Efficient MAE that introduces a novel Difficulty-Flatten Loss and a decoder masking strategy, enabling a higher mask ratio for more efficient pre-training. The Difficulty-Flatten Loss provides balanced supervision on reconstruction targets of different difficulties, mitigating the performance drop under higher mask ratios effectively. Additionally, the decoder masking strategy discards the most difficult reconstruction targets, which further alleviates the optimization difficulty and accelerates the pre-training clearly. Our proposed Efficient MAE introduces 27% and 30% pre-training runtime accelerations for the ViT-Large and ViT-Huge models, provides valuable insights into MAE’s optimization, and paves the way for larger-scale vision Transformer pre-training. Code and pre-trained models will be released."
Efficient Semantic Matching With Hypercolumn Correlation,"Seungwook Kim, Juhong Min, Minsu Cho","Pohang University of Science and Technology (POSTECH), South Korea",100.0,South Korea,0.0,,"Recent studies show that leveraging the match-wise relationships within the 4D correlation map yields significant improvements in establishing semantic correspondences - but at the cost of increased computation and latency. In this work, we focus on the aspect that the performance improvements of recent methods can also largely be attributed to the usage of multi-scale correlation maps, which hold various information ranging from low-level geometric cues to high-level semantic contexts. To this end, we propose HCCNet, an efficient yet effective semantic matching method which exploits the full potential of multi-scale correlation maps, while eschewing the reliance on expensive match-wise relationship mining on the 4D correlation map. Specifically, HCCNet performs feature slicing on the bottleneck features to yield a richer set of intermediate features, which are used to construct a hypercolumn correlation. HCCNet can consequently establish semantic correspondences in an effective manner by reducing the volume of conventional high-dimensional convolution or self-attention operations to efficient point-wise convolutions. HCCNet demonstrates state-of-the-art or competitive performances on the standard benchmarks of semantic matching, while incurring a notably lower latency and computation overhead compared to the existing SoTA methods.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Efficient_Semantic_Matching_With_Hypercolumn_Correlation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Efficient_Semantic_Matching_With_Hypercolumn_Correlation_WACV_2024_paper.pdf,http://cvlab.postech.ac.kr/research/HCCNet,,2311.04336,main,Poster,https://ieeexplore.ieee.org/document/10483888/,"['Visualization', 'Correlation', 'Costs', 'Convolution', 'Image edge detection', 'Semantics', 'Benchmark testing']","['Semantic Matching', 'Convolution Operation', 'Competitive Performance', 'Computational Overhead', 'Intermediate Features', 'Low Computation', 'Open Volume', 'Pointwise Convolution', 'SOTA Methods', 'Geometric Cues', 'Low Computational Overhead', 'Convolutional Neural Network', 'Feature Maps', 'Visual Cues', 'Object Recognition', 'Flow Field', 'Image Pairs', 'Paired Test', 'Hyperbolic Tangent', 'Source Images', 'Intermediate Feature Maps', 'Object Recognition Task', 'Backbone Feature', 'Bottleneck Layer', 'Small-scale Variability', 'Feature Pairs', 'Slice Size', 'Training Image Pairs', 'Density Field', 'Learnable Weight Matrix']","['Algorithms', 'Image recognition and understanding', 'Algorithms', '3D computer vision', 'Algorithms', 'Video recognition and understanding']",,"Recent studies show that leveraging the match-wise relationships within the 4D correlation map yields significant improvements in establishing semantic correspondences - but at the cost of increased computation and latency. In this work, we focus on the aspect that the performance improvements of recent methods can also largely be attributed to the usage of multi-scale correlation maps, which hold various information ranging from low-level geometric cues to high-level semantic contexts. To this end, we propose HCCNet, an efficient yet effective semantic matching method which exploits the full potential of multi-scale correlation maps, while eschewing the reliance on expensive matchwise relationship mining on the 4D correlation map. Specifically, HCCNet performs feature slicing on the bottleneck features to yield a richer set of intermediate features, which are used to construct a hypercolumn correlation. HCCNet can consequently establish semantic correspondences in an effective manner by reducing the volume of conventional high-dimensional convolution or self-attention operations to efficient point-wise convolutions. HCCNet demonstrates state-of-the-art or competitive performances on the standard benchmarks of semantic matching, while incurring a notably lower latency and computation overhead compared to the existing SoTA methods."
Efficient Transferability Assessment for Selection of Pre-Trained Detectors,"Zhao Wang, Aoxue Li, Zhenguo Li, Qi Dou",The Chinese University of Hong Kong; Huawei Noah’s Ark Lab,50.0,Hong Kong,50.0,China,"Large-scale pre-training followed by downstream fine-tuning is an effective solution for transferring deep-learning-based models. Since finetuning all possible pre-trained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner. Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors. To this end, we build up a detector transferability benchmark which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes. Given this zoo, we adopt 6 target datasets from 5 diverse domains as the downstream target tasks for evaluation. Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework. Additionally, we design a complementary metric for evaluating tasks with varying objects. Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32x and requiring a mere 5.2% memory footprint compared to brute-force fine-tuning of all pre-trained detectors. Our assessment code and benchmark will be publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Efficient_Transferability_Assessment_for_Selection_of_Pre-Trained_Detectors_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Efficient_Transferability_Assessment_for_Selection_of_Pre-Trained_Detectors_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484191/,"['Training', 'Measurement', 'Computational modeling', 'Memory management', 'Detectors', 'Benchmark testing', 'Predictive models']","['Assessment Of Efficiency', 'Pre-trained Detector', 'Fine-tuned', 'Object Detection', 'Training Strategy', 'Variety Of Domains', 'Target Domain', 'Segmentation Task', 'Transfer Performance', 'Target Dataset', 'Source Dataset', 'Target Task', 'Memory Footprint', 'Wall-clock Time', 'Linear Model', 'Rate Measurements', 'Large-scale Datasets', 'Detection Model', 'Bounding Box', 'Unmanned Aerial Vehicles', 'Complementary Metrics', 'Multi-scale Features', 'Unique Rate', 'Bounding Box Coordinates', 'Ground-truth Bounding Box', 'Label Matrix', 'SOTA Methods', 'Feature Pyramid Network', 'Series Of Metrics', 'Joint Evaluation']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Large-scale pre-training followed by downstream finetuning is an effective solution for transferring deeplearning-based models. Since finetuning all possible pretrained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner. Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors. To this end, we build up a detector transferability benchmark which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes. Given this zoo, we adopt 7 target datasets from 5 diverse domains as the downstream target tasks for evaluation. Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework. Additionally, we design a complementary metric for evaluating tasks with varying objects. Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32× and requires a mere 5.2% memory footprint compared to brute-force fine-tuning of all pretrained detectors. Our assessment code and benchmark will be publicly available."
EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level Latencies,"Kilian Batzner, Lars Heckler, Rebecca König","MVTec Software GmbH, Technical University of Munich; MVTec Software GmbH",50.0,Germany,50.0,Germany,"Detecting anomalies in images is an important task, especially in real-time computer vision applications. In this work, we focus on computational efficiency and propose a lightweight feature extractor that processes an image in less than a millisecond on a modern GPU. We then use a student-teacher approach to detect anomalous features. We train a student network to predict the extracted features of normal, i.e., anomaly-free training images. The detection of anomalies at test time is enabled by the student failing to predict their features. We propose a training loss that hinders the student from imitating the teacher feature extractor beyond the normal images. It allows us to drastically reduce the computational cost of the student-teacher model, while improving the detection of anomalous features. We furthermore address the detection of challenging logical anomalies that involve invalid combinations of normal local features, for example, a wrong ordering of objects. We detect these anomalies by efficiently incorporating an autoencoder that analyzes images globally. We evaluate our method, called EfficientAD, on 32 datasets from three industrial anomaly detection dataset collections. EfficientAD sets new standards for both the detection and the localization of anomalies. At a latency of two milliseconds and a throughput of six hundred images per second, it enables a fast handling of anomalies. Together with its low error rate, this makes it an economical solution for real-world applications and a fruitful basis for future research.",https://openaccess.thecvf.com/content/WACV2024/html/Batzner_EfficientAD_Accurate_Visual_Anomaly_Detection_at_Millisecond-Level_Latencies_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Batzner_EfficientAD_Accurate_Visual_Anomaly_Detection_at_Millisecond-Level_Latencies_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484326/,"['Training', 'Location awareness', 'Computer vision', 'Visualization', 'Feature extraction', 'Throughput', 'Computational efficiency']","['Anomaly Detection', 'Throughput', 'Computational Cost', 'Student Teachers', 'Training Images', 'Training Loss', 'Collection Of Datasets', 'Normal Images', 'Neural Network', 'Training Set', 'Convolutional Neural Network', 'Convolutional Layers', 'Hidden Layer', 'Mixture Model', 'Detection Results', 'Part Of The Image', 'Low Latency', 'Computational Requirements', 'Pre-trained Network', 'Anomaly Score', 'Structural Anomalies', 'Anomaly Detection Methods', 'Local Anomalies', 'Logical Constraints', 'Hard Loss', 'Constraint Violation', 'Pre-training Dataset', 'Unseen Images', 'Number Of Training Images']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",48,"Detecting anomalies in images is an important task, especially in real-time computer vision applications. In this work, we focus on computational efficiency and propose a lightweight feature extractor that processes an image in less than a millisecond on a modern GPU. We then use a student–teacher approach to detect anomalous features. We train a student network to predict the extracted features of normal, i.e., anomaly-free training images. The detection of anomalies at test time is enabled by the student failing to predict their features. We propose a training loss that hinders the student from imitating the teacher feature extractor beyond the normal images. It allows us to drastically reduce the computational cost of the student–teacher model, while improving the detection of anomalous features. We furthermore address the detection of challenging logical anomalies that involve invalid combinations of normal local features, for example, a wrong ordering of objects. We detect these anomalies by efficiently incorporating an autoencoder that analyzes images globally. We evaluate our method, called EfficientAD, on 32 datasets from three industrial anomaly detection dataset collections. EfficientAD sets new standards for both the detection and the localization of anomalies. At a latency of two milliseconds and a throughput of six hundred images per second, it enables a fast handling of anomalies. Together with its low error rate, this makes it an economical solution for real-world applications and a fruitful basis for future research."
Ego2HandsPose: A Dataset for Egocentric Two-Hand 3D Global Pose Estimation,"Fanqing Lin, Tony Martinez","Brigham Young University, Brigham Young University, Provo, UT; Magic Leap, Inc., 7500 W Sunrise Blvd, Plantation, Florida U.S.A",50.0,USA,50.0,USA,"Color-based two-hand 3D pose estimation in the global coordinate system is essential in many applications. However, there are very few datasets dedicated to this task and no existing dataset supports estimation in a non-laboratory environment. This is largely attributed to the sophisticated data collection process required for 3D hand pose annotations, which also leads to difficulty in obtaining instances with the level of visual diversity needed for estimation in the wild. Progressing towards this goal, a large-scale dataset Ego2Hands was recently proposed to address the task of two-hand segmentation and detection in the wild. The proposed composition-based data generation technique can create two-hand instances with quality, quantity and diversity that generalize well to unseen domains. In this work, we present Ego2HandsPose, an extension of Ego2Hands that contains 3D hand pose annotation and is the first dataset that enables color-based two-hand 3D tracking in unseen domains. To this end, we develop a set of parametric fitting algorithms to enable 1) 3D hand pose annotation using a single image, 2) automatic conversion from 2D to 3D hand poses and 3) accurate two-hand tracking with temporal consistency. We provide incremental quantitative analysis on the multi-stage pipeline and show that training on our dataset achieves state-of-the-art results that significantly outperforms other datasets for the task of egocentric two-hand global 3D pose estimation.",https://openaccess.thecvf.com/content/WACV2024/html/Lin_Ego2HandsPose_A_Dataset_for_Egocentric_Two-Hand_3D_Global_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lin_Ego2HandsPose_A_Dataset_for_Egocentric_Two-Hand_3D_Global_Pose_Estimation_WACV_2024_paper.pdf,,,2206.04927,main,Poster,https://ieeexplore.ieee.org/document/10483984/,"['Training', 'Visualization', 'Three-dimensional displays', 'Annotations', 'Statistical analysis', 'Pose estimation', 'Pipelines']","['Global Estimates', 'Pose Estimation', 'Global Pose Estimation', 'Single Image', 'Fitting Parameters', 'Fitting Algorithm', 'Global Coordinate System', 'Human Pose Estimation', '3D Pose', 'Unseen Domains', 'Right-hand', 'Training Set', 'Left Hand', 'Limited Diversity', 'Manual Annotation', 'Depth Camera', 'Fitting Accuracy', 'Cropped Images', 'Single Camera', 'Global Translation', '2D Keypoints', 'Hand Tracking', 'Global Orientation', '2D Pose', 'Manual Validation', 'Complete Pipeline', 'RGB-D Data', 'Qualitative Examples', 'RGB Camera', 'Endpoint Error']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"Color-based two-hand 3D pose estimation in the global coordinate system is essential in many applications. However, there are very few datasets dedicated to this task and no existing dataset supports estimation in a non-laboratory environment. This is largely attributed to the sophisticated data collection process required for 3D hand pose annotations, which also leads to difficulty in obtaining instances with the level of visual diversity needed for estimation in the wild. Progressing towards this goal, a large-scale dataset Ego2Hands was recently proposed to address the task of two-hand segmentation and detection in the wild. The proposed composition-based data generation technique can create two-hand instances with quality, quantity and diversity that generalize well to unseen domains. In this work, we present Ego2HandsPose, an extension of Ego2Hands that contains 3D hand pose annotation and is the first dataset that enables color-based two-hand 3D tracking in unseen domains. To this end, we develop a set of parametric fitting algorithms to enable 1) 3D hand pose annotation using a single image, 2) automatic conversion from 2D to 3D hand poses and 3) accurate two-hand tracking with temporal consistency. We provide incremental quantitative analysis on the multi-stage pipeline and show that training on our dataset achieves state-of-the-art results that significantly outperforms other datasets for the task of egocentric two-hand global 3D pose estimation."
Egocentric Action Recognition by Capturing Hand-Object Contact and Object State,"Tsukasa Shiota, Motohiro Takagi, Kaori Kumagai, Hitoshi Seshimo, Yushi Aono","NTT Human Informatics Laboratories, NTT Corporation",0.0,,100.0,Japan,"Improving the performance of egocentric action recognition (EAR) requires accurately capturing interactions between actors and objects. In this paper, we propose two learning methods that enable recognition models to capture hand object contact and object state change. We introduce Hand-Object Contact Learning (HOCL), which enables the model to focus on hand-object contact during actions, and Object State Learning (OSL), which enables the model to focus on object state changes caused by hand actions. Evaluation using a CNN-based model and a transformer-based model on the EGTEA, MECCANO, and EPIC-KITCHENS 100 datasets demonstrated the effectiveness of applying HOCL and OSL. Their application improved overall accuracy by up to 2.24% on EGTEA, 3.97% on MECCANO, and 1.49% on EPIC-KITCHENS 100. In addition, HOCL and OSL improved the performance on data with small training samples and one from unfamiliar scenes. Qualitative analysis revealed that their application enabled the models to precisely capture the interaction between actor and object.",https://openaccess.thecvf.com/content/WACV2024/html/Shiota_Egocentric_Action_Recognition_by_Capturing_Hand-Object_Contact_and_Object_State_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shiota_Egocentric_Action_Recognition_by_Capturing_Hand-Object_Contact_and_Object_State_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484440/,"['Training', 'Learning systems', 'Computer vision', 'Analytical models', 'Computational modeling', 'Focusing', 'Data visualization']","['Action Recognition', 'Objective Conditions', 'Contact Conditions', 'Egocentric Action Recognition', 'Recognition Model', 'Hand Actions', 'Small Training Samples', 'Loss Function', 'Training Set', 'Convolutional Neural Network', 'Validation Set', 'Classification Performance', 'Active Learning', 'F1 Score', 'Video Frames', 'Spatiotemporal Characteristics', 'Horizontal Flip', 'Spatiotemporal Information', 'Action Labels', 'Raw Video', 'Action Recognition Model', 'Improve Recognition Performance', '1st Row', 'Inference Phase', 'Dynamic Datasets', 'Entire Video', 'Training Phase', 'Manual Annotation', 'Human Motion']","['Algorithms', 'Video recognition and understanding']",,"Improving the performance of egocentric action recognition (EAR) requires accurately capturing interactions between actors and objects. In this paper, we propose two learning methods that enable recognition models to capture hand-object contact and object state change. We introduce Hand-Object Contact Learning (HOCL), which enables the model to focus on hand-object contact during actions, and Object State Learning (OSL), which enables the model to focus on object state changes caused by hand actions. Evaluation using a CNN-based model and a transformer-based model on the EGTEA, MECCANO, and EPIC-KITCHENS 100 datasets demonstrated the effectiveness of applying HOCL and OSL. Their application improved overall accuracy by up to 2.24% on EGTEA, 3.97% on MECCANO, and 1.49% on EPIC-KITCHENS 100. In addition, HOCL and OSL improved the performance on data with small training samples and one from unfamiliar scenes. Qualitative analysis revealed that their application enabled the models to precisely capture the interaction between actor and object."
Elusive Images: Beyond Coarse Analysis for Fine-Grained Recognition,"Connor Anderson, Matt Gwilliam, Evelyn Gaskin, Ryan Farrell","University of Maryland, College Park, MD; Brigham Young University, Provo, UT",100.0,USA,0.0,,"While the community has seen many advances in recent years to address the challenging problem of Finegrained Visual Categorization (FGVC), progress seems to be slowing--new state-of-the-art methods often distinguish themselves by improving top-1 accuracy by mere tenths of a percent. However, across all of the now-standard FGVC datasets, there remain sizeable portions of the test data that none of the current state-of-the-art (SOTA) models can successfully predict. This paper provides a framework for identifying and studying the errors that current methods make across diverse fine-grained datasets. Three models of difficulty--Prediction Overlap, Prediction Rank and Pairwise Class Confusion--are employed to highlight the most challenging sets of images and classes. Extensive experiments apply a range of standard and SOTA methods, evaluating them on multiple FGVC domains and datasets. Insights acquired from coupling these difficulty paradigms with the careful analysis of experimental results suggest crucial areas for future FGVC research, focusing critically on the set of elusive images that none of the current models can correctly classify. Code is available at catalys1.github.io/elusive-images-fgvc.",https://openaccess.thecvf.com/content/WACV2024/html/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.pdf,,https://catalys1.github.io/elusive-images-fgvc,,main,Poster,https://ieeexplore.ieee.org/document/10483813/,"['Training', 'Analytical models', 'Visualization', 'Schedules', 'Codes', 'Image recognition', 'Focusing']","['Fine-grained Recognition', 'Careful Analysis', 'State Of The Art Methods', 'Top-1 Accuracy', 'Advances In Recent Years', 'Classification Confusion', 'Training Set', 'Aspect Ratio', 'Image Classification', 'Data Augmentation', 'Image Object', 'ImageNet', 'Bounding Box', 'Kullback-Leibler', 'Class I', 'Similar Classification', 'Correct Predictions', 'Object Size', 'Level Of Granularity', 'Spatial Properties', 'Pair Of Classes', 'Measure Of Difficulty', 'iNaturalist', 'Prediction Vector']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations']",1,"While the community has seen many advances in recent years to address the challenging problem of Fine-grained Visual Categorization (FGVC), progress seems to be slowing—new state-of-the-art methods often distinguish themselves by improving top-1 accuracy by mere tenths of a percent. However, across all of the now-standard FGVC datasets, there remain sizeable portions of the test data that none of the current state-of-the-art (SOTA) models can successfully predict. This paper provides a framework for identifying and studying the errors that current methods make across diverse fine-grained datasets. Three models of difficulty—Prediction Overlap, Prediction Rank and Pair-wise Class Confusion—are employed to highlight the most challenging sets of images and classes. Extensive experiments apply a range of standard and SOTA methods, evaluating them on multiple FGVC domains and datasets. Insights acquired from coupling these difficulty paradigms with the careful analysis of experimental results suggest crucial areas for future FGVC research, focusing critically on the set of elusive images that none of the current models can correctly classify. Code is available at catalys1.github.io/elusive-images-fgvc."
Embedding Task Structure for Action Detection,"Michael Peven, Gregory D. Hager",,,,,,"We present a straightforward, flexible method to enhance the accuracy and quality of action detection by expressing temporal and structural relationships of actions in the loss function of a deep network. We describe ways to represent otherwise implicit structure in video data and demonstrate how these structures reflect natural biases that improve network training. Our experiments show that our approach improves both accuracy and edit-distance of action recognition and detection models over a baseline. Our framework leads to improvements over prior work and obtains state-of-the-art results on multiple benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Peven_Embedding_Task_Structure_for_Action_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Peven_Embedding_Task_Structure_for_Action_Detection_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483773/,"['Training', 'Visualization', 'Computer vision', 'Codes', 'Computational modeling', 'Benchmark testing', 'Task analysis']","['Action Detection', 'Loss Function', 'Deep Network', 'Action Recognition', 'Edit Distance', 'Action Recognition Model', 'Semantic', 'Training Data', 'Formation Of Structures', 'Visual Impairment', 'Hierarchical Structure', 'Poisson Distribution', 'Image Classification', 'Cross-entropy Loss', 'Shortest Path', 'Correct Classification', 'Temporal Structure', 'Poisson Process', 'Temporal Model', 'Frequency Of Sequences', 'Temporal Loss', 'Label Distribution', 'Dynamic Bayesian Network', 'Standard Cross-entropy Loss', 'One-hot Encoding', 'Video Dataset', 'Evaluation Dataset', 'ImageNet', 'Video Analysis', 'Neural Network']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"We present a straightforward, flexible method to enhance the accuracy and quality of action detection by expressing temporal and structural relationships of actions in the loss function of a deep network. We describe ways to represent otherwise implicit structure in video data and demonstrate how these structures reflect natural biases that improve network training. Our experiments show that our approach improves both accuracy and edit-distance of action recognition and detection models over a baseline. Our framework leads to improvements over prior work and obtains state-of-the-art results on multiple benchmarks. The code is available here."
Embodied Human Activity Recognition,"Sha Hu, Yu Gong, Greg Mori",Simon Fraser University,100.0,Canada,0.0,,"We study how to utilize the mobility of an embodied agent to improve its ability to recognize human activities. We introduce the embodied human activity recognition problem, where an agent moves in a 3D environment to recognize the category of ongoing human activities. The agent must make movement decisions based on its egocentric observations acquired up to the current time, with the goal of choosing movements to obtain new views that lead to accurate human activity recognition. Towards this goal, we propose a reinforcement learning approach that learns a policy controlling the agent's movements over time. We evaluate our approach with two realistic human activity datasets. Results show that our approach can learn to move effectively to achieve high performance in recognizing human activities.",https://openaccess.thecvf.com/content/WACV2024/html/Hu_Embodied_Human_Activity_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hu_Embodied_Human_Activity_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484307/,"['Computer vision', 'Three-dimensional displays', 'Reinforcement learning', 'Behavioral sciences', 'Human activity recognition', 'Task analysis']","['Human Activities', 'Action Recognition', 'Human Activity Recognition', 'Recognition Accuracy', '3D Environment', 'Time Step', 'Human Bone', '3D Space', 'Start Position', 'State Representation', 'Activity Progression', 'Human Motion', 'Policy Learning', 'Policy Network', 'Autonomous Agents', 'Learning Agent', 'Passive Voice', 'Social Robots', 'Unstructured Environments', 'Task Space', 'Proximal Policy Optimization', 'Frame Selection', 'Action Recognition Model', 'Virtual Camera', 'Trained Agent', 'Onboard Sensors', 'Partial Observation', 'Sensory Input', 'Visual Observation', 'Cross-entropy Loss']","['Algorithms', 'Video recognition and understanding']",,"We study how to utilize the mobility of an embodied agent to improve its ability to recognize human activities. We introduce the embodied human activity recognition problem, where an agent moves in a 3D environment to recognize the category of ongoing human activities. The agent must make movement decisions based on its egocentric observations acquired up to the current time, with the goal of choosing movements to obtain new views that lead to accurate human activity recognition. Towards this goal, we propose a reinforcement learning approach that learns a policy controlling the agent’s movements over time. We evaluate our approach with two realistic human activity datasets. Results show that our approach can learn to move effectively to achieve high performance in recognizing human activities."
EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters,"Bita Azari, Angelica Lim","Simon Fraser University, Burnaby, Canada",100.0,Canada,0.0,,"Recent studies have achieved impressive results in face generation and editing of facial expressions. However, existing approaches either generate a discrete number of facial expressions or have limited control over the emotion of the output image. To overcome this limitation, we introduced EmoStyle, a method to edit facial expressions based on valence and arousal, two continuous emotional parameters that can specify a broad range of emotions. EmoStyle is designed to separate emotions from other facial characteristics and to edit the face to display a desired emotion. We employ the pre-trained generator from StyleGAN2, taking advantage of its rich latent space. We also proposed an adapted inversion method to be able to apply our system on out-of-StyleGAN2 domain images in a one-shot manner. The qualitative and quantitative evaluations show that our approach has the capability to synthesize a wide range of expressions to output high-resolution images.",https://openaccess.thecvf.com/content/WACV2024/html/Azari_EmoStyle_One-Shot_Facial_Expression_Editing_Using_Continuous_Emotion_Parameters_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Azari_EmoStyle_One-Shot_Facial_Expression_Editing_Using_Continuous_Emotion_Parameters_WACV_2024_paper.pdf,,https://bihamta.github.io/emostyle/,,main,Poster,https://ieeexplore.ieee.org/document/10484199/,"['Computer vision', 'Codes', 'Semantics', 'Computer architecture', 'Generators', 'Faces']","['Facial Expressions', 'Facial Expression Editing', 'Latent Space', 'Facial Features', 'Inverse Method', 'Output Image', 'Range Of Emotions', 'Loss Function', 'Input Image', 'Emotional Expressions', 'Generative Adversarial Networks', 'Image Generation', 'Convex Hull', 'Low-resolution Images', 'Basic Emotions', 'Loss Of Identity', 'Reconstruction Loss', 'Low Arousal', 'Conditional Generative Adversarial Network', 'L2 Loss', 'Latent Code', 'Facial Attributes', 'Target Emotion', 'Arousal Values', 'Identity Preservation', 'Valence Values', 'Real Faces', 'Emotional Faces', 'Image Editing', 'Skin Color']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Recent studies have achieved impressive results in face generation and editing of facial expressions. However, existing approaches either generate a discrete number of facial expressions or have limited control over the emotion of the output image. To overcome this limitation, we introduced EmoStyle, a method to edit facial expressions based on valence and arousal, two continuous emotional parameters that can specify a broad range of emotions. EmoStyle is designed to separate emotions from other facial characteristics and to edit the face to display a desired emotion. We employ the pre-trained generator from StyleGAN2, taking advantage of its rich latent space. We also proposed an adapted inversion method to be able to apply our system on real images in a one-shot manner. The qualitative and quantitative evaluations show that our approach has the capability to synthesize a wide range of expressions to output high-resolution images.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Empowering Unsupervised Domain Adaptation With Large-Scale Pre-Trained Vision-Language Models,"Zhengfeng Lai, Haoping Bai, Haotian Zhang, Xianzhi Du, Jiulong Shan, Yinfei Yang, Chen-Nee Chuah, Meng Cao","Apple AI/ML; University of California, Davis",50.0,USA,50.0,USA,"Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source domain to solve the tasks on the unlabeled target domain. Traditional UDA methods face the challenge of the tradeoff between domain alignment and semantic class discriminability, especially when a large domain gap exists between the source and target domain. The efforts of applying large-scale pre-training to bridge the domain gaps remain limited. In this work, we propose that Vision-Language Models (VLMs) can empower UDA tasks due to their training pattern with language alignment and their large-scale pre-trained datasets. For example, CLIP and GLIP have shown promising zero-shot generalization in classification and detection tasks. However, directly fine-tuning these VLMs into downstream tasks may be computationally expensive and not scalable if we have multiple domains that need to be adapted. Therefore, in this work, we first study an efficient adaption of VLMs to preserve the original knowledge while maximizing its flexibility for learning new knowledge. Then, we design a domain-aware pseudo-labeling scheme tailored to VLMs for domain disentanglement. We show the superiority of the proposed methods in four UDA-classification and two UDA-detection benchmarks, with a significant improvement (+9.9%) on DomainNet.",https://openaccess.thecvf.com/content/WACV2024/html/Lai_Empowering_Unsupervised_Domain_Adaptation_With_Large-Scale_Pre-Trained_Vision-Language_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lai_Empowering_Unsupervised_Domain_Adaptation_With_Large-Scale_Pre-Trained_Vision-Language_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484237/,"['Training', 'Bridges', 'Computer vision', 'Adaptation models', 'Computational modeling', 'Semantics', 'Benchmark testing']","['Domain Adaptation', 'Vision-language Models', 'Benchmark', 'Classification Task', 'Large-scale Datasets', 'Target Domain', 'Classism', 'Source Domain', 'Domain Alignment', 'Domain Gap', 'Efficient Adaptation', 'Unlabeled Target Domain', 'Unsupervised Domain Adaptation Methods', 'Labeled Source Domain', 'Visual Representation', 'Extreme Weather', 'Object Detection', 'Visual Features', 'Semantic Information', 'Text Encoder', 'Formation Of Domains', 'Linear Layer', 'Vision Transformer', 'Domain Representation', 'Pseudo Labels', 'Contrastive Loss', 'Visual Concepts', 'Semantic Properties', 'Representative Class']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",1,"Unsupervised Domain Adaptation (UDA) aims to leverage the labeled source domain to solve the tasks on the unlabeled target domain. Traditional UDA methods face the challenge of the tradeoff between domain alignment and semantic class discriminability, especially when a large domain gap exists between the source and target domains. The efforts of applying large-scale pre-training to bridge the domain gaps remain limited. In this work, we propose that Vision-Language Models (VLMs) can empower UDA tasks due to their training pattern with language alignment and their large-scale pre-trained datasets. For example, CLIP and GLIP have shown promising zero-shot generalization in classification and detection tasks. However, directly fine-tuning these VLMs into downstream tasks may be computationally expensive and not scalable if we have multiple domains that need to be adapted. Therefore, in this work, we first study an efficient adaption of VLMs to preserve the original knowledge while maximizing its flexibility for learning new knowledge. Then, we design a domain-aware pseudo-labeling scheme tailored to VLMs for domain disentanglement. We show the superiority of the proposed methods in four UDA-classification and two UDA-detection benchmarks, with a significant improvement (+9.9%) on DomainNet."
Enforcing Sparsity on Latent Space for Robust and Explainable Representations,"Hanao Li, Tian Han","Stevens Institute of Technology, Hoboken, NJ, hli136@stevens.edu; Stevens Institute of Technology, Hoboken, NJ, than6@stevens.edu",100.0,USA,0.0,,"Recently, dense latent variable models have shown promising results, but their distributed and potentially redundant codes make them less interpretable and less robust to noise. On the other hand, sparse representations are more parsimonious, providing better explainability and noise robustness, but it is difficult to enforce sparsity due to the complexity and computational cost involved. In this paper, we propose a novel unsupervised learning approach to enforce sparsity on the latent space for the generator model, utilizing a gradually sparsified spike and slab distribution as our prior. Our model is composed of a top-down generator network that maps the latent variable to the observations. We use maximum likelihood sampling to infer latent variables in the generator's posterior direction, and spike and slab regularization in the inference stage can induce sparsity by pushing non-informative latent dimensions toward zero. Our experiments show that the learned sparse latent representations preserve the majority of the information, and our model can learn disentangled semantics, increase the explainability of the latent codes, and enhance the robustness of the classification and denoising tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Enforcing_Sparsity_on_Latent_Space_for_Robust_and_Explainable_Representations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Enforcing_Sparsity_on_Latent_Space_for_Robust_and_Explainable_Representations_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483787/,"['Codes', 'Sparse approximation', 'Computational modeling', 'Semantics', 'Generators', 'Robustness', 'Noise robustness']","['Latent Space', 'Robust Representation', 'Semantic', 'Latent Variables', 'Representation Learning', 'Maximum Sampling', 'Density Model', 'Sparse Representation', 'Robust Classification', 'Latent Representation', 'Latent Dimensions', 'Latent Code', 'Important Information', 'Maximum Likelihood Estimation', 'Gaussian Noise', 'Generative Adversarial Networks', 'Gaussian Mixture Model', 'Peak Signal-to-noise Ratio', 'Dirac Delta', 'Variational Autoencoder', 'Sparse Coding', 'Sparse Model', 'Prior Term', 'Langevin Dynamics', 'MNIST Dataset', 'Variational Autoencoder Model', 'Standard Gaussian', 'Inference Step', 'Noisy Images', 'Values Of A1']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Recently, dense latent variable models have shown promising results, but their distributed and potentially redundant codes make them less interpretable and less robust to noise. On the other hand, sparse representations are more parsimonious, providing better explainability and noise robustness, but it is difficult to enforce sparsity due to the complexity and computational cost involved. In this paper, we propose a novel unsupervised learning approach to enforce sparsity on the latent space for the generator model, utilizing a gradually sparsified spike and slab distribution as our prior. Our model is composed of a top-down generator network that maps the latent variable to the observations. We use maximum likelihood sampling to infer latent variables in the generator’s posterior direction, and spike and slab regularization in the inference stage can induce sparsity by pushing non-informative latent dimensions toward zero. Our experiments show that the learned sparse latent representations preserve the majority of the information, and our model can learn disentangled semantics, increase the explainability of the latent codes, and enhance the robustness of the classification and denoising tasks."
Enhancing Diverse Intra-Identity Representation for Visible-Infrared Person Re-Identification,"Sejun Kim, Soonyong Gwon, Kisung Seo","Seokyeong University, Seoul, Korea",100.0,South Korea,0.0,,"Visible-Infrared person Re-Identification (VI-ReID) is a challenging task due to modality discrepancy. To reduce modality-gap, existing methods primarily focus on sample diversity, such as data augmentation or generating intermediate modality between Visible and Infrared. However, these methods do not consider the increase in intra-instance variance caused by sample diversity, and they focus on dominant features, which results in a remaining modality gap for hard samples. This limitation hinders performance improvement. We propose Intra-identity Representation Diversification (IRD) based metric learning to handle the intra-instance variance. Specifically IRD method enlarge the Intra-modality Intra-identity Representation Space (IIRS) for each modality within the same identity to learn diverse feature representation abilities. This enables the formation of a shared space capable of representing common features across hetero-modality, thereby reducing the modality gap more effectively. In addition, we introduce a HueGray (HG) data augmentation method, which increases sample diversity simply and effectively. Finally, we propose the Diversity Enhancement Network (DEN) for robustly handling intra-instance variance. The proposed method demonstrates superior performance compared to the state-of-the-art methods on the SYSU-MM01 and RegDB datasets. Notably, on the challenging SYSU-MM01 dataset, our approach achieves remarkable results with a Rank-1 accuracy of 76.36% and a mean Average Precision (mAP) of 71.30%.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Enhancing_Diverse_Intra-Identity_Representation_for_Visible-Infrared_Person_Re-Identification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Enhancing_Diverse_Intra-Identity_Representation_for_Visible-Infrared_Person_Re-Identification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484284/,"['Measurement', 'Computer vision', 'Focusing', 'Data augmentation', 'Task analysis']","['Diverse Representation', 'Visible-infrared Person Re-identification', 'Feature Representation', 'Data Augmentation', 'Representation Of Space', 'Dominant Feature', 'Shared Space', 'Mean Average Precision', 'Metric Learning', 'Sample Characteristics', 'Positive Samples', 'Diverse Sample', 'Infrared Imaging', 'Negative Samples', 'Representation Learning', 'Activation Maps', 'Visible Images', 'Discriminative Learning', 'Triplet Loss', 'Retrieval Results', 'Discriminative Representations', 'Discriminative Feature Representation', 'Visible Camera', 'Discriminative Feature Learning', 'Zero-shot', 'Re-identification Methods']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",4,"Visible-Infrared person Re-Identification (VI-ReID) is a challenging task due to modality discrepancy. To reduce modality-gap, existing methods primarily focus on sample diversity, such as data augmentation or generating intermediate modality between Visible and Infrared. However, these methods do not consider the increase in intra-instance variance caused by sample diversity, and they focus on dominant features, which results in a remaining modality gap for hard samples. This limitation hinders performance improvement. We propose Intra-identity Representation Diversification (IRD) based metric learning to handle the intra-instance variance. Specifically IRD method enlarge the Intra-modality Intra-identity Representation Space (IIRS) for each modality within the same identity to learn diverse feature representation abilities. This enables the formation of a shared space capable of representing common features across hetero-modality, thereby reducing the modality gap more effectively. In addition, we introduce a HueGray (HG) data augmentation method, which increases sample diversity simply and effectively. Finally, we propose the Diversity Enhancement Network (DEN) for robustly handling intra-instance variance. The proposed method demonstrates superior performance compared to the state-of-the-art methods on the SYSU-MM01 and RegDB datasets. Notably, on the challenging SYSU-MM01 dataset, our approach achieves remarkable results with a Rank-1 accuracy of 76.36% and a mean Average Precision (mAP) of 71.30%."
Enhancing Multi-View Pedestrian Detection Through Generalized 3D Feature Pulling,"Sithu Aung, Haesol Park, Hyungjoo Jung, Junghyun Cho","KIST, Republic of Korea; UST, Republic of Korea; KIST, Republic of Korea; KIST, Republic of Korea; UST, Republic of Korea; Yonsei-KIST, Republic of Korea",100.0,South Korea,0.0,,"The main challenge in multi-view pedestrian detection is integrating view-specific features into a unified space for comprehensive end-to-end perception. Prior multi-view detection methods have focused on projecting perspective-view features onto the ground plane, creating a ""bird's eye view"" (BEV) representation of the scene. This paper proposes a simple but effective architecture that utilizes a non-parametric 3D feature-pulling strategy. This strategy directly extracts the corresponding 2D features for each valid voxel within the 3D feature volume, addressing the feature loss that may arise in previous methods. The proposed framework introduces three novel modules, each crafted to bolster the generalization capabilities of multi-view detection systems. Through extensive experiments, the efficacy of the proposed model is demonstrated. The results show a new state-of-the-art accuracy, both in conventional scenarios and particularly in the context of scene generalization benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Aung_Enhancing_Multi-View_Pedestrian_Detection_Through_Generalized_3D_Feature_Pulling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Aung_Enhancing_Multi-View_Pedestrian_Detection_Through_Generalized_3D_Feature_Pulling_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483981/,"['Computer vision', 'Three-dimensional displays', 'Pedestrians', 'Computational modeling', 'Lighting', 'Feature extraction', 'Cameras']","['Pedestrian', '3D Features', 'Pedestrian Detection', 'Ground Plane', '3D Volume', 'Visual Perspective', 'Bird’s Eye', 'Prior Methods', '2D Feature', 'Scene Representation', 'Feature Maps', 'Object Detection', 'Generalization Performance', '3D Space', '3D Coordinates', 'Vertical Dimension', 'Camera View', 'Voxel Volume', 'Pixel Coordinates', 'Homography', 'Multi-view Feature', 'Occupancy Map', 'Frustum', 'Refiner', 'Learning Layer', '3D Object Detection', '3D Voxel', 'Grid Coordinates', 'Variety Of Scenes', 'Evaluation Scenarios']","['Algorithms', 'Image recognition and understanding', 'Algorithms', '3D computer vision']",3,"The main challenge in multi-view pedestrian detection is integrating view-specific features into a unified space for comprehensive end-to-end perception. Prior multi-view detection methods have focused on projecting perspective-view features onto the ground plane, creating a ""bird’s eye view"" (BEV) representation of the scene. This paper proposes a simple but effective architecture that utilizes a nonparametric 3D feature-pulling strategy. This strategy directly extracts the corresponding 2D features for each valid voxel within the 3D feature volume, addressing the feature loss that may arise in previous methods. The proposed framework introduces three novel modules, each crafted to bolster the generalization capabilities of multi-view detection systems. Through extensive experiments, the efficacy of the proposed model is demonstrated. The results show a new state-of-the-art accuracy, both in conventional scenarios and particularly in the context of scene generalization benchmarks."
Enhancing Multimodal Compositional Reasoning of Visual Language Models With Generative Negative Mining,"Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, Volker Tresp","Technical University of Munich; LMU Munich, Siemens AG; LMU Munich, Munich Center for Machine Learning; Technical University of Munich, Munich Center for Machine Learning",100.0,Germany,0.0,,"Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing the image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.",https://openaccess.thecvf.com/content/WACV2024/html/Sahin_Enhancing_Multimodal_Compositional_Reasoning_of_Visual_Language_Models_With_Generative_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sahin_Enhancing_Multimodal_Compositional_Reasoning_of_Visual_Language_Models_With_Generative_WACV_2024_paper.pdf,https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html,https://github.com/ugorsahin/enhancing-multimodal-compositional-reasoning-of-vlm,2311.03964,main,Poster,https://ieeexplore.ieee.org/document/10484294/,"['Training', 'Visualization', 'Computer vision', 'Codes', 'Pipelines', 'Self-supervised learning', 'Cognition']","['Language Model', 'Visual Model', 'Images Of Samples', 'Negative Samples', 'Large-scale Models', 'Reasoning Tasks', 'Benchmark', 'Data Generation', 'Object Detection', 'Image Object', 'Bird Species', 'Image Pairs', 'Image Generation', 'Target Object', 'Self-supervised Learning', 'Contrastive Loss', 'COCO Dataset', 'Image Editing', 'Image Inpainting', 'Seagull']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs’ performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html."
Estimating Blood Alcohol Level Through Facial Features for Driver Impairment Assessment,"Ensiyeh Keshtkaran, Brodie von Berg, Grant Regan, David Suter, Syed Zulqarnain Gilani","Centre for AI&ML, Edith Cowan University, Western Australia; MiX Telematics, Western Australia; Nutrition and Health Innovation Research Institute, ECU, Western Australia",66.66666666666666,Australia,33.33333333333334,Australia,"Drunk driving-related road accidents contribute significantly to the global burden of road injuries. Addressing alcohol-related harm, particularly during safety-critical activities like driving, requires real-time monitoring of an individual's blood alcohol concentration (BAC). We devise an in-vehicle machine learning system that harnesses standard commercial RGB cameras to predict critical levels of BAC. Our system can detect instances of alcohol intoxication impairment as subtle as 0.05 g/dL (WHO recommended legal limit for driving), with an accuracy of 75%, by leveraging the physiological manifestations of alcohol intoxication on a driver's face. This system holds great promise for improving road safety. In tandem, we have compiled a data set of 60 subjects engaged in simulated driving scenarios, spanning three levels of alcohol intoxication. These scenarios were captured and divided into video segments labeled ""sober"", ""low"", and ""severe"" Alcohol Intoxication Impairment (AII), constituting the basis for evaluating our system's performance. To the best of our knowledge, this study is the first to create a large-scale real-life dataset of alcohol intoxication and assess intoxication levels using an off-the-shelf RGB camera to detect drunk driving.",https://openaccess.thecvf.com/content/WACV2024/html/Keshtkaran_Estimating_Blood_Alcohol_Level_Through_Facial_Features_for_Driver_Impairment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Keshtkaran_Estimating_Blood_Alcohol_Level_Through_Facial_Features_for_Driver_Impairment_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484402/,"['Road accidents', 'Law', 'Machine learning', 'Cameras', 'Road safety', 'Real-time systems', 'Standards']","['Facial Features', 'Blood Alcohol', 'Driving Under The Influence', 'Machine Learning', 'Road Safety', 'Legal Limit', 'Machine Learning Systems', 'RGB Camera', 'Drunk Driving', 'Improve Road Safety', 'Level Of Intoxication', 'Training Data', 'Machine Learning Models', 'Long Short-term Memory', 'Head Movements', 'Low Alcohol', 'Bayesian Optimization', 'Machine Learning Framework', 'Breath Test', 'Severe Poisoning', 'Gaze Movements', 'RGB Video', 'Head Pose', 'Alcohol Administration', 'Fatal Accidents', 'Breath Samples', 'Breath Analysis', 'Motion Sickness', 'Baseline System']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Video recognition and understanding', 'Applications', 'Embedded sensing / real-time techniques']",,"Drunk driving-related road accidents contribute significantly to the global burden of road injuries. Addressing alcohol-related harm, particularly during safety-critical activities like driving, requires real-time monitoring of an individual’s blood alcohol concentration (BAC). We devise an in-vehicle machine learning system that harnesses standard commercial RGB cameras to predict critical levels of BAC. Our system can detect instances of alcohol intoxication impairment as subtle as 0.05 g/dL (WHO recommended legal limit for driving), with an accuracy of 75%, by leveraging the physiological manifestations of alcohol intoxication on a driver’s face. This system holds great promise for improving road safety. In tandem, we have compiled a data set of 60 subjects engaged in simulated driving scenarios, spanning three levels of alcohol intoxication. These scenarios were captured and divided into video segments labeled ""sober"",""low’, and ""severe"" Alcohol Intoxication Impairment (AII), constituting the basis for evaluating our system’s performance. To the best of our knowledge, this study is the first to create a large-scale real-life dataset of alcohol intoxication and assess intoxication levels using an off-the-shelf RGB camera to detect drunk driving."
Estimating Fog Parameters From an Image Sequence Using Non-Linear Optimisation,"Yining Ding, Andrew M. Wallace, Sen Wang","Sense Robotics Lab, Imperial College London, London, UK; Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, UK",100.0,UK,0.0,,"Given a sequence of images taken in foggy weather, we seek to estimate the atmospheric light and the scattering coefficient. These are key parameters to characterise the nature of the fog, to reconstruct a clear image (defogging), and to infer scene depth. Existing methods adopt a sequential estimation strategy which is prone to error propagation. In sharp contrast, we take a more systematic approach and jointly estimate these parameters by solving a unified non-linear optimisation problem. Experimental results show that the proposed method is superior to existing ones in terms of both estimation accuracy and precision. Our method further demonstrates how image defogging and depth estimation can be linked to a visual localisation system, contributing to more comprehensive and robust perception in fog.",https://openaccess.thecvf.com/content/WACV2024/html/Ding_Estimating_Fog_Parameters_From_an_Image_Sequence_Using_Non-Linear_Optimisation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ding_Estimating_Fog_Parameters_From_an_Image_Sequence_Using_Non-Linear_Optimisation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484393/,"['Visualization', 'Parameter estimation', 'Systematics', 'Estimation', 'Scattering', 'Data collection', 'Image sequences']","['Nonlinear Programming', 'Accuracy And Precision', 'Estimation Strategy', 'Depth Images', 'Clear Image', 'Depth Estimation', 'Scene Depth', 'Atmospheric Light', 'Histogram', 'Subsequent Steps', 'Single Image', 'Image Regions', 'Autonomous Vehicles', 'Transmission Coefficient', 'Local Map', 'Pixel Location', 'Full Method', 'Residual Term', 'Current Frame', 'Error Metrics', 'Stereo Image Pairs', 'Simultaneous Localization And Mapping', 'Local Frame', 'Visible Increase', 'Optimization Stage', 'Temporal Consistency', 'Row Block', 'Dark Channel']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', '3D computer vision', 'Applications', 'Autonomous Driving']",,"Given a sequence of images taken in foggy weather, we seek to estimate the atmospheric light and the scattering coefficient. These are key parameters to characterise the nature of the fog, to reconstruct a clear image (defogging), and to infer scene depth. Existing methods adopt a sequential estimation strategy which is prone to error propagation. In sharp contrast, we take a more systematic approach and jointly estimate these parameters by solving a unified non-linear optimisation problem. Experimental results show that the proposed method is superior to existing ones in terms of both estimation accuracy and precision. Our method further demonstrates how image defogging and depth estimation can be linked to a visual localisation system, contributing to more comprehensive and robust perception in fog."
EvDNeRF: Reconstructing Event Data With Dynamic Neural Radiance Fields,"Anish Bhattacharya, Ratnesh Madaan, Fernando Cladera, Sai Vemprala, Rogerio Bonatti, Kostas Daniilidis, Ashish Kapoor, Vijay Kumar, Nikolai Matni, Jayesh K. Gupta",University of Pennsylvania; Scaled Foundations; Microsoft; Poly Corporation,25.0,USA,75.0,USA,"We present EvDNeRF, a pipeline for generating event data and training an event-based dynamic NeRF, for the purpose of faithfully reconstructing eventstreams on scenes with rigid and non-rigid deformations that may be too fast to capture with a standard camera. Event cameras register asynchronous per-pixel brightness changes at MHz rates with high dynamic range, making them ideal for observing fast motion with almost no motion blur. Neural radiance fields (NeRFs) offer visual-quality geometric-based learnable rendering, but prior work with events has only considered reconstruction of static scenes. Our EvDNeRF can predict eventstreams of dynamic scenes from a static or moving viewpoint between any desired timestamps, thereby allowing it to be used as an event-based simulator for a given scene. We show that by training on varied batch sizes of events, we can improve test-time predictions of events at fine time resolutions, outperforming baselines that pair standard dynamic NeRFs with event generators. We release our simulated and real datasets, as well as code for multi-view event-based data generation and the training and evaluation of EvDNeRF models.",https://openaccess.thecvf.com/content/WACV2024/html/Bhattacharya_EvDNeRF_Reconstructing_Event_Data_With_Dynamic_Neural_Radiance_Fields_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bhattacharya_EvDNeRF_Reconstructing_Event_Data_With_Dynamic_Neural_Radiance_Fields_WACV_2024_paper.pdf,,https://github.com/anish-bhattacharya/EvDNeRF,2310.02437,main,Poster,https://ieeexplore.ieee.org/document/10483809/,"['Training', 'Computer vision', 'Deformation', 'Dynamics', 'Cameras', 'Rendering (computer graphics)', 'Registers']","['Event Data', 'Neural Radiance Fields', 'Data Generation', 'High Dynamic Range', 'Motion Blur', 'Dynamic Scenes', 'Brightness Changes', 'Scene Reconstruction', 'Neural Field', 'Non-rigid Deformation', 'Dynamic Vision Sensor', 'Time Window', 'Computer Vision', 'Temporal Dimension', 'Image Reconstruction', 'Spatial Dimensions', 'Image Intensity', 'Viewing Angle', 'Peak Signal-to-noise Ratio', 'Simulated Events', 'Event Reconstruction', 'Simulated Scene', 'Background Events', 'Camera Motion', 'Negative Threshold', 'Pixel Location', 'Camera Viewpoint', 'Time Tk', 'Event Window', 'Loss Term']","['Applications', 'Visualization', 'Algorithms', '3D computer vision', 'Algorithms', 'Datasets and evaluations']",4,"We present EvDNeRF, a pipeline for generating event data and training an event-based dynamic NeRF, for the purpose of faithfully reconstructing eventstreams on scenes with rigid and non-rigid deformations that may be too fast to capture with a standard camera. Event cameras register asynchronous per-pixel brightness changes at MHz rates with high dynamic range, making them ideal for observing fast motion with almost no motion blur. Neural radiance fields (NeRFs) offer visual-quality geometric-based learnable rendering, but prior work with events has only considered reconstruction of static scenes. Our EvDNeRF can predict eventstreams of dynamic scenes from a static or moving viewpoint between any desired timestamps, thereby allowing it to be used as an event-based simulator for a given scene. We show that by training on varied batch sizes of events, we can improve test-time predictions of events at fine time resolutions, outperforming baselines that pair standard dynamic NeRFs with event generators. We release our simulated and real datasets, as well as code for multi-view event-based data generation and the training and evaluation of EvDNeRF models 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Evaluation of Video Masked Autoencoders' Performance and Uncertainty Estimations for Driver Action and Intention Recognition,"Koen Vellenga, H. Joe Steinhauer, Göran Falkman, Tomas Björklund","Volvo Car Corporation, Sweden; University of Skövde, Sweden",50.0,Sweden,50.0,Sweden,"Traffic fatalities remain among the leading death causes worldwide. To reduce this figure, car safety is listed as one of the most important factors. To actively support human drivers, it is essential for advanced driving assistance systems to be able to recognize the driver's actions and intentions. Prior studies have demonstrated various approaches to recognize driving actions and intentions based on in-cabin and external video footage. Given the performance of self-supervised video pre-trained (SSVP) Video Masked Autoencoders (VMAEs) on multiple action recognition datasets, we evaluate the performance of SSVP VMAEs on the Honda Research Institute Driving Dataset for driver action recognition (DAR) and on the Brain4Cars dataset for driver intention recognition (DIR). Besides the performance, the application of an artificial intelligence system in a safety-critical environment must be capable to express when it is uncertain about the produced results. Therefore, we also analyze uncertainty estimations produced by a Bayes-by-Backprop last-layer (BBB-LL) and Monte-Carlo (MC) dropout variants of an VMAE. Our experiments show that an VMAE achieves a higher overall performance for both offline DAR and end-to-end DIR compared to the state-of-the-art. The analysis of the BBB-LL and MC dropout models show higher uncertainty estimates for incorrectly classified test instances compared to correctly predicted test instances.",https://openaccess.thecvf.com/content/WACV2024/html/Vellenga_Evaluation_of_Video_Masked_Autoencoders_Performance_and_Uncertainty_Estimations_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Vellenga_Evaluation_of_Video_Masked_Autoencoders_Performance_and_Uncertainty_Estimations_for_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Evidential Uncertainty Quantification: A Variance-Based Perspective,"Ruxiao Duan, Brian Caffo, Harrison X. Bai, Haris I. Sair, Craig Jones",Johns Hopkins University School of Medicine; Johns Hopkins University,100.0,USA,0.0,,"Uncertainty quantification of deep neural networks has become an active field of research and plays a crucial role in various downstream tasks such as active learning. Recent advances in evidential deep learning shed light on the direct quantification of aleatoric and epistemic uncertainties with a single forward pass of the model. Most traditional approaches adopt an entropy-based method to derive evidential uncertainty in classification, quantifying uncertainty at the sample level. However, the variance-based method that has been widely applied in regression problems is seldom used in the classification setting. In this work, we adapt the variance-based approach from regression to classification, quantifying classification uncertainty at the class level. The variance decomposition technique in regression is extended to class covariance decomposition in classification based on the law of total covariance, and the class correlation is also derived from the covariance. Experiments on cross-domain datasets are conducted to illustrate that the variance-based approach not only results in similar accuracy as the entropy-based one in active domain adaptation but also brings information about class-wise uncertainties as well as between-class correlations. The code is available at https://github.com/KerryDRX/EvidentialADA. This alternative means of evidential uncertainty quantification will give researchers more options when class uncertainties and correlations are important in their applications.",https://openaccess.thecvf.com/content/WACV2024/html/Duan_Evidential_Uncertainty_Quantification_A_Variance-Based_Perspective_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Duan_Evidential_Uncertainty_Quantification_A_Variance-Based_Perspective_WACV_2024_paper.pdf,,https://github.com/KerryDRX/EvidentialADA,2311.11367,main,Poster,https://ieeexplore.ieee.org/document/10483638/,"['Deep learning', 'Computer vision', 'Adaptation models', 'Uncertainty', 'Correlation', 'Codes', 'Artificial neural networks']","['Uncertainty Quantification', 'Uncertainty Of Evidence', 'Deep Learning', 'Deep Neural Network', 'Active Learning', 'Class Level', 'Domain Adaptation', 'Set Of Classifiers', 'Epistemic Uncertainty', 'Classification Uncertainty', 'Class Correlation', 'Covariance Matrix', 'Exponential Function', 'Classification Problem', 'Image Segmentation', 'Average Accuracy', 'Uncertainty Estimation', 'Target Prediction', 'Domain Shift', 'Target Domain', 'Dirichlet Distribution', 'Medical Image Segmentation', 'Aleatoric Uncertainty', 'Unlabeled Data', 'Synthetic Images', 'Prediction Uncertainty', 'Target Data', 'Deep Learning Classification', 'Target Label', 'Pseudo Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Uncertainty quantification of deep neural networks has become an active field of research and plays a crucial role in various downstream tasks such as active learning. Recent advances in evidential deep learning shed light on the direct quantification of aleatoric and epistemic uncertainties with a single forward pass of the model. Most traditional approaches adopt an entropy-based method to derive evidential uncertainty in classification, quantifying uncertainty at the sample level. However, the variance-based method that has been widely applied in regression problems is seldom used in the classification setting. In this work, we adapt the variance-based approach from regression to classification, quantifying classification uncertainty at the class level. The variance decomposition technique in regression is extended to class covariance decomposition in classification based on the law of total covariance, and the class correlation is also derived from the covariance. Experiments on cross-domain datasets are conducted to illustrate that the variance-based approach not only results in similar accuracy as the entropy-based one in active domain adaptation but also brings information about class-wise uncertainties as well as between-class correlations. The code is available at https://github.com/KerryDRX/EvidentialADA. This alternative means of evidential uncertainty quantification will give researchers more options when class uncertainties and correlations are important in their applications."
Evolve: Enhancing Unsupervised Continual Learning With Multiple Experts,"Xiaofan Yu, Tajana Rosing, Yunhui Guo",University of Texas at Dallas; University of California San Diego,100.0,USA,0.0,,"Recent years have seen significant progress in unsupervised continual learning methods. Despite their success in controlled settings, their practicality in real-world contexts remains uncertain. In this paper, we first empirically investigate existing self-supervised continual learning methods. We show that even with a replay buffer, existing methods cannot preserve the critical knowledge on videos with temporal-correlated input. Our insight is that the primary challenge of unsupervised continual learning stems from the unpredictable input and the absence of supervision as well as prior knowledge. Drawing inspiration from hybrid AI, we introduce EVOLVE, an innovative framework employing multiple pre-trained models in the cloud, as experts, to bolster existing self-supervised learning methods on local clients. EVOLVE harnesses expert guidance through a novel expert aggregation loss, calculated and returned from the cloud. It also dynamically assigns weights to experts based on their confidence and tailored prior knowledge, thereby offering adaptive supervision for new streaming data. We extensively validate EVOLVE across several real-world data streams with temporal correlation. The results convincingly demonstrate that EVOLVE surpasses the best state-of-the-art unsupervised continual learning method by 6.1-53.7% in top-1 linear evaluation accuracy across various data streams, affirming the efficacy of diverse expert guidance. The codebase is at https://github.com/Orienfish/Evolve.",https://openaccess.thecvf.com/content/WACV2024/html/Yu_Evolve_Enhancing_Unsupervised_Continual_Learning_With_Multiple_Experts_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yu_Evolve_Enhancing_Unsupervised_Continual_Learning_With_Multiple_Experts_WACV_2024_paper.pdf,,https://github.com/Orienfish/Evolve,,main,Poster,https://ieeexplore.ieee.org/document/10483870/,"['Computer vision', 'Adaptation models', 'Correlation', 'Self-supervised learning', 'Streams', 'Videos']","['Unsupervised Learning', 'Incremental Learning', 'Data Streams', 'Self-supervised Learning', 'Critical Knowledge', 'Diverse Expertise', 'Drawing Inspiration', 'Cloud Model', 'Replay Buffer', 'Self-supervised Learning Methods', 'Local Clients', 'Natural Environment', 'Hyperparameters', 'Class Labels', 'Representation Learning', 'Autonomous Vehicles', 'Linear Classifier', 'Null Space', 'Encrypted Data', 'Weight Adjustment', 'Expert Model', 'Dynamic Weight', 'Catastrophic Forgetting', 'Online Optimization', 'kNN Classifier', 'Dynamic Update', 'Labeling Task', 'Dynamic Adjustment']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Recent years have seen significant progress in unsupervised continual learning methods. Despite their success in controlled settings, their practicality in real-world contexts remains uncertain. In this paper, we first empirically investigate existing self-supervised continual learning methods. We show that even with a replay buffer, existing methods cannot preserve the critical knowledge on videos with temporal-correlated input. Our insight is that the primary challenge of unsupervised continual learning stems from the unpredictable input and the absence of supervision as well as prior knowledge. Drawing inspiration from hybrid AI, we introduce Evolve, an innovative framework employing multiple pretrained models in the cloud, as experts, to bolster existing self-supervised learning methods on local clients. Evolve harnesses expert guidance through a novel expert aggregation loss, calculated and returned from the cloud. It also dynamically assigns weights to experts based on their confidence and tailored prior knowledge, thereby offering adaptive supervision for new streaming data. We extensively validate Evolve across several real-world data streams with temporal correlation. The results convincingly demonstrate that Evolve surpasses the best state-of-the-art unsupervised continual learning method by 6.1-53.7% in top-1 linear evaluation accuracy across various data streams, affirming the efficacy of diverse expert guidance. The codebase is at https://github.com/Orienfish/Evolve."
Expanding Expressiveness of Diffusion Models With Limited Data via Self-Distillation Based Fine-Tuning,"Jiwan Hur, Jaehyun Choi, Gyojin Han, Dong-Jae Lee, Junmo Kim","School of Electrical Engineering, KAIST, South Korea",100.0,South Korea,0.0,,"Training diffusion models on limited datasets poses challenges in terms of limited generation capacity and expressiveness, leading to unsatisfactory results in various downstream tasks utilizing pretrained diffusion models, such as domain translation and text-guided image manipulation. In this paper, we propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a methodology to address these challenges by leveraging diverse features from diffusion models pretrained on large source datasets. SDFT distills more general features (shape, colors, etc.) and less domain-specific features (texture, fine details, etc) from the source model, allowing successful knowledge transfer without disturbing the training process on target datasets. The proposed method is not constrained by the specific architecture of the model and thus can be generally adopted to existing frameworks. Experimental results demonstrate that SDFT enhances the expressiveness of the diffusion model with limited datasets, resulting in improved generation capabilities across various downstream tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hur_Expanding_Expressiveness_of_Diffusion_Models_With_Limited_Data_via_Self-Distillation_WACV_2024_paper.pdf,,,2311.01018,main,Poster,https://ieeexplore.ieee.org/document/10484075/,"['Training', 'Computer vision', 'Shape', 'Image color analysis', 'Computational modeling', 'Computer architecture', 'Data models']","['Diffusion Model', 'Large Datasets', 'General Characteristics', 'Diverse Characteristics', 'Fine Details', 'Generalization Capability', 'Limited Dataset', 'Source Model', 'Target Dataset', 'Source Dataset', 'Fine-tuned Model', 'Domain-specific Features', 'Feature Space', 'Input Image', 'Facial Expressions', 'Reversible Process', 'Generative Adversarial Networks', 'Diverse Properties', 'Peak Signal-to-noise Ratio', 'Target Domain', 'Fréchet Inception Distance', 'Initial Noise', 'Facial Attributes', 'Fine-tuning Method', 'Catastrophic Forgetting', 'Large Signal-to-noise Ratio', 'Target Model', 'Pure Noise', 'Weighting Scheme', 'Teacher Network']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Training diffusion models on limited datasets poses challenges in terms of limited generation capacity and expressiveness, leading to unsatisfactory results in various down-stream tasks utilizing pretrained diffusion models, such as domain translation and text-guided image manipulation. In this paper, we propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a methodology to address these challenges by leveraging diverse features from diffusion models pretrained on large source datasets. SDFT distills more general features (shape, colors, etc.) and less domain-specific features (texture, fine details, etc) from the source model, allowing successful knowledge transfer without disturbing the training process on target datasets. The proposed method is not constrained by the specific architecture of the model and thus can be generally adopted to existing frameworks. Experimental results demonstrate that SDFT enhances the expressiveness of the diffusion model with limited datasets, resulting in improved generation capabilities across various downstream tasks."
Expanding Hyperspherical Space for Few-Shot Class-Incremental Learning,"Yao Deng, Xiang Xiang","Key Lab of Image Processing and Intelligent Control, Ministry of Education, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, China",100.0,China,0.0,,"In today's ever-changing world, the ability of machine learning models to continually learn new data without forgetting previous knowledge is of utmost importance. However, in the scenario of few-shot class-incremental learning (FSCIL), where models have limited access to new instances, this task becomes even more challenging. Current methods use prototypes as a replacement for classifiers, where the cosine similarity of instances to these prototypes is used for prediction. However, we have identified that the embedding space created by using the relu activation function is incomplete and crowded for future classes. To address this issue, we propose the Expanding Hyperspherical Space (EHS) method for FSCIL. In EHS, we utilize an odd-symmetric activation function to ensure the completeness and symmetry of embedding space. Additionally, we specify a region for base classes and reserve space for unseen future classes, which increases the distance between class distributions. Pseudo instances are also used to enable the model to anticipate possible upcoming samples. During inference, we provide rectification to the confidence to prevent bias towards base classes. We conducted experiments on benchmark datasets such as CIFAR100 and miniImageNet, which demonstrate that our proposed method achieves state-of-the-art performance.",https://openaccess.thecvf.com/content/WACV2024/html/Deng_Expanding_Hyperspherical_Space_for_Few-Shot_Class-Incremental_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Deng_Expanding_Hyperspherical_Space_for_Few-Shot_Class-Incremental_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484511/,"['Computer vision', 'Prototypes', 'Benchmark testing', 'Data models', 'Power capacitors', 'Task analysis']","['Hypersphere', 'Few-shot Learning', 'Class-incremental Learning', 'Hyperspherical Space', 'Few-shot Class-incremental Learning', 'Activation Function', 'Class Distribution', 'Latent Space', 'ReLU Activation Function', 'Base Classes', 'Class Of Spaces', 'Loss Function', 'Neural Network', 'Model Performance', 'Convolutional Neural Network', 'Distribution Characteristics', 'Feature Space', 'Vector-based', 'Incremental Learning', 'Linear Classifier', 'Catastrophic Forgetting', 'Unseen Classes', 'Upper Pole', 'Current Task', 'Prototypical Network', 'Penalty Factor', 'Loss Term']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",2,"In today’s ever-changing world, the ability of machine learning models to continually learn new data without forgetting previous knowledge is of utmost importance. However, in the scenario of few-shot class-incremental learning (FSCIL), where models have limited access to new instances, this task becomes even more challenging. Current methods use prototypes as a replacement for classifiers, where the cosine similarity of instances to these prototypes is used for prediction. However, we have identified that the embedding space created by using the relu activation function is incomplete and crowded for future classes. To address this issue, we propose the Expanding Hyperspherical Space (EHS) method for FSCIL. In EHS, we utilize an odd-symmetric activation function to ensure the completeness and symmetry of embedding space. Additionally, we specify a region for base classes and reserve space for unseen future classes, which increases the distance between class distributions. Pseudo instances are also used to enable the model to anticipate possible upcoming samples. During inference, we provide rectification to the confidence to prevent bias towards base classes. We conducted experiments on benchmark datasets such as CIFAR100 and miniImageNet, which demonstrate that our proposed method achieves state-of-the-art performance."
Exploiting CLIP for Zero-Shot HOI Detection Requires Knowledge Distillation at Multiple Levels,"Bo Wan, Tinne Tuytelaars","ESAT, KU Leuven",100.0,Belgium,0.0,,"In this paper, we investigate the task of zero-shot human-object interaction (HOI) detection, a novel paradigm for identifying HOIs without the need for task-specific annotations. To address this challenging task, we employ CLIP, a large-scale pre-trained vision-language model (VLM), for knowledge distillation on multiple levels. To this end, we design a multi-branch neural network that leverages CLIP for learning HOI representations at various levels, including global images, local union regions encompassing human-object pairs, and individual instances of humans or objects. To train our model, CLIP is utilized to generate HOI scores for both global images and local union regions that serve as supervision signals. The extensive experiments demonstrate the effectiveness of our novel multi-level CLIP knowledge integration strategy. Notably, the model achieves strong performance, which is even comparable with some fully-supervised and weakly-supervised methods on the public HICO-DET benchmark.",https://openaccess.thecvf.com/content/WACV2024/html/Wan_Exploiting_CLIP_for_Zero-Shot_HOI_Detection_Requires_Knowledge_Distillation_at_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wan_Exploiting_CLIP_for_Zero-Shot_HOI_Detection_Requires_Knowledge_Distillation_at_WACV_2024_paper.pdf,,https://github.com/bobwan1995/Zeroshot-HOI-with-CLIP,2309.05069,main,Poster,https://ieeexplore.ieee.org/document/10483987/,"['Computer vision', 'Codes', 'Annotations', 'Computational modeling', 'Neural networks', 'Benchmark testing', 'Task analysis']","['Human-object Interaction', 'Human-Object Interaction Detection', 'Challenging Task', 'Knowledge Integration', 'Global Image', 'Multilevel Strategy', 'Supervision Signal', 'Union Regions', 'Local Unions', 'Need For Annotation', 'Interactive', 'Learning Models', 'Feature Maps', 'Model Design', 'Object Detection', 'Multilayer Perceptron', 'Bounding Box', 'Object Features', 'Appearance Features', 'Interaction Score', 'Visual Encoding', 'Rare Classes', 'Contextual Cues', 'Multiple Instance Learning', 'Feature Maps Of Images', 'Text Encoder', 'Self-attention Module', 'Fusion Strategy', 'Supervision Training', 'Object Proposals']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",1,"In this paper, we investigate the task of zero-shot human-object interaction (HOI) detection, a novel paradigm for identifying HOIs without the need for task-specific annotations. To address this challenging task, we employ CLIP, a large-scale pre-trained vision-language model (VLM), for knowledge distillation on multiple levels. Specifically, we design a multi-branch neural network that leverages CLIP for learning HOI representations at various levels, including global images, local union regions encompassing human-object pairs, and individual instances of humans or objects. To train our model, CLIP is utilized to generate HOI scores for both global images and local union regions that serve as supervision signals. The extensive experiments demonstrate the effectiveness of our novel multi-level CLIP knowledge integration strategy. Notably, the model achieves strong performance, which is even comparable with some fully-supervised and weakly-supervised methods on the public HICO-DET benchmark. Code is available at https://github.com/bobwan1995/Zeroshot-HOI-with-CLIP."
Exploiting the Signal-Leak Bias in Diffusion Models,"Martin Nicolas Everaert, Athanasios Fitsios, Marco Bocchio, Sami Arpa, Sabine Süsstrunk, Radhakrishna Achanta","Largo.ai, Lausanne, Switzerland; School of Computer and Communication Sciences, EPFL, Switzerland",100.0,Switzerland,0.0,,"There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training.",https://openaccess.thecvf.com/content/WACV2024/html/Everaert_Exploiting_the_Signal-Leak_Bias_in_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Everaert_Exploiting_the_Signal-Leak_Bias_in_Diffusion_Models_WACV_2024_paper.pdf,https://ivrl.github.io/signal-leak-bias/,,2309.15842,main,Poster,https://ieeexplore.ieee.org/document/10484346/,"['Training', 'Adaptation models', 'Image color analysis', 'Image synthesis', 'Frequency-domain analysis', 'Brightness', 'Pipelines']","['Diffusion Model', 'Model Bias', 'Frequency Domain', 'Noise Distribution', 'Specific Style', 'Signal Leakage', 'Pixel Domain', 'Neural Network', 'White Noise', 'Diffusion Process', 'Image Area', 'Natural Images', 'Latent Space', 'Image Generation', 'Lowest Frequency', 'Bright Images', 'Inference Time', 'Distribution Of Images', 'Low-frequency Components', 'Fine-tuned Model', 'Training Distribution', 'Latent Code', 'Brightness Variations', 'Exposure Bias']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Arts / games / social media']",2,"There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training."
Exploring Adversarial Robustness of Vision Transformers in the Spectral Perspective,"Gihyun Kim, Juyeop Kim, Jong-Seok Lee","Yonsei University, Republic of Korea",100.0,South Korea,0.0,,"The Vision Transformer has emerged as a powerful tool for image classification tasks, surpassing the performance of convolutional neural networks (CNNs). Recently, many researchers have attempted to understand the robustness of Transformers against adversarial attacks. However, previous researches have focused solely on perturbations in the spatial domain. This paper proposes an additional perspective that explores the adversarial robustness of Transformers against frequency-selective perturbations in the spectral domain. To facilitate comparison between these two domains, an attack framework is formulated as a flexible tool for implementing attacks on images in both the spatial and spectral domains. The experiments reveal that Transformers rely more on phase and low frequency information, which can render them more vulnerable to frequency-selective attacks than CNNs. This work offers new insights into the properties and adversarial robustness of Transformers.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Exploring_Adversarial_Robustness_of_Vision_Transformers_in_the_Spectral_Perspective_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Exploring_Adversarial_Robustness_of_Vision_Transformers_in_the_Spectral_Perspective_WACV_2024_paper.pdf,,,2208.09602,main,Poster,https://ieeexplore.ieee.org/document/10483795/,"['Deep learning', 'Perturbation methods', 'Frequency-domain analysis', 'Linearity', 'Transformers', 'Robustness', 'High frequency']","['Transformer', 'Vision Transformer', 'Adversarial Robustness', 'Spectral Perspective', 'Convolutional Neural Network', 'Image Classification', 'Spatial Domain', 'Phase Information', 'Spectral Domain', 'Frequency Information', 'Adversarial Attacks', 'Frequency Domain', 'Feature Space', 'Frequency Region', 'Peak Signal-to-noise Ratio', 'Input Space', 'Low-frequency Region', 'Target Model', 'Inverse Fourier Transform', 'High Frequency Band', 'Fast Gradient Sign Method', 'Attack Success Rate', 'Projected Gradient Descent', 'High Frequency Information', 'Magnitude Spectrum', 'Attack Methods', 'Spectral Phase', 'ResNet Model', 'Image X', 'High Frequency Region']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",1,"The Vision Transformer has emerged as a powerful tool for image classification tasks, surpassing the performance of convolutional neural networks (CNNs). Recently, many researchers have attempted to understand the robustness of Transformers against adversarial attacks. However, previous researches have focused solely on perturbations in the spatial domain. This paper proposes an additional perspective that explores the adversarial robustness of Transformers against frequency-selective perturbations in the spectral domain. To facilitate comparison between these two domains, an attack framework is formulated as a flexible tool for implementing attacks on images in the spatial and spectral domains. The experiments reveal that Transformers rely more on phase and low frequency information, which can render them more vulnerable to frequency-selective attacks than CNNs. This work offers new insights into the properties and adversarial robustness of Transformers."
Exploring the Impact of Rendering Method and Motion Quality on Model Performance When Using Multi-View Synthetic Data for Action Recognition,"Stanislav Panev, Emily Kim, Sai Abhishek Si Namburu, Desislava Nikolova, Celso de Melo, Fernando De la Torre, Jessica Hodgins",Carnegie Mellon University; Army Research Laboratory; Technical University of Sofia,66.66666666666666,"Bulgaria, USA",33.33333333333334,USA,"This paper explores the use of synthetic data in a human action recognition (HAR) task to avoid the challenges of obtaining and labeling real-world datasets. We introduce a new dataset suite comprising five datasets, eleven common human activities, three synchronized camera views (aerial and ground) in three outdoor environments, and three visual domains (real and two synthetic). For the synthetic data, two rendering methods (standard computer graphics and neural rendering) and two sources of human motions (motion capture and video-based motion reconstruction) were employed. We evaluated each dataset type by training popular activity recognition models and comparing the performance on the real test data. Our results show that synthetic data achieve slightly lower accuracy (4-8%) than real data. On the other hand, a model pre-trained on synthetic data and fine-tuned on limited real data surpasses the performance of either domain alone. Standard computer graphics (CG)-rendered data delivers better performance than the data generated from the neural-based rendering method. The results suggest that the quality of the human motions in the training data also affects the test results: motion capture delivers higher test accuracy. Additionally, a model trained on CG aerial view synthetic data exhibits greater robustness against camera viewpoint changes than one trained on real data. See the project page: http://humansensinglab.github.io/REMAG/.",https://openaccess.thecvf.com/content/WACV2024/html/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Panev_Exploring_the_Impact_of_Rendering_Method_and_Motion_Quality_on_WACV_2024_paper.pdf,http://humansensinglab.github.io/REMAG/,,,main,Poster,https://ieeexplore.ieee.org/document/10483857/,"['Training', 'Computational modeling', 'Training data', 'Rendering (computer graphics)', 'Cameras', 'Data models', 'Robustness']","['Action Recognition', 'Motion Quality', 'Training Data', 'Motion Capture', 'Real Test', 'Human Motion', 'Camera View', 'Human Activity Recognition', 'Action Recognition Model', 'Gestures', 'Real-world Data', 'Source Images', '3D Mesh', 'Video Sequences', 'Activity Classification', 'Activity Categories', 'Action Classes', 'Motion Data', '3D Graph', 'Synthetic Data Generation', 'RGB Video', 'Video Action Recognition', 'Synthetic Variants', 'Synthetic Training Data', 'Source Motion']",,,"This paper explores the use of synthetic data in a human action recognition (HAR) task to avoid the challenges of obtaining and labeling real-world datasets. We introduce a new dataset suite comprising five datasets, eleven common human activities, three synchronized camera views (aerial and ground) in three outdoor environments, and three visual domains (real and two synthetic). For the synthetic data, two rendering methods (standard computer graphics and neural rendering) and two sources of human motions (motion capture and video-based motion reconstruction) were employed. We evaluated each dataset type by training popular activity recognition models and comparing the performance on the real test data. Our results show that synthetic data achieve slightly lower accuracy (4–8 %) than real data. On the other hand, a model pre-trained on synthetic data and fine-tuned on limited real data surpasses the performance of either domain alone. Standard computer graphics (CG)-rendered data delivers better performance than the data generated from the neural-based rendering method. The results suggest that the quality of the human motions in the training data also affects the test results: motion capture delivers higher test accuracy. Additionally, a model trained on CG aerial view synthetic data exhibits greater robustness against camera viewpoint changes than one trained on real data. See the project page: http://humansensinglab.github.io/REMAG/"
FAKD: Feature Augmented Knowledge Distillation for Semantic Segmentation,"Jianlong Yuan, Minh Hieu Phan, Liyang Liu, Yifan Liu","University of Adelaide; Damo Academy, Alibaba Group; Hupan Lab",66.66666666666666,"Australia, China",33.33333333333334,China,"In this work, we explore data augmentations for knowledge distillation on semantic segmentation. Due the capacity gap, small-sized student networks struggle to discover the discriminative feature space learned by a powerful teacher. Image-level augmentations allow the student to better imitate the teacher by providing extra outputs. However, existing distillation frameworks only augment a limited number of samples, which restricts the learning of a student. Inspired by the recent progress on semantic directions on feature space, this work proposes a feature-level augmented knowledge distillation (FAKD) which infinitely augments features along a semantic direction for optimal knowledge transfer. Furthermore, we introduce novel surrogate loss functions to distill the teacher's knowledge from an infinite number of samples. The surrogate loss is an upper bound of the expected distillation loss over infinite augmented samples. Extensive experiments on four semantic segmentation benchmarks demonstrate that the proposed method boosts the performance of current knowledge distillation methods without any significant overhead. The code will be released at FAKD.",https://openaccess.thecvf.com/content/WACV2024/html/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_FAKD_Feature_Augmented_Knowledge_Distillation_for_Semantic_Segmentation_WACV_2024_paper.pdf,Not provided,Not provided,,main,Poster,https://ieeexplore.ieee.org/document/10484221/,"['Training', 'Knowledge engineering', 'Computer vision', 'Upper bound', 'Codes', 'Semantic segmentation', 'Semantics']","['Semantic Segmentation', 'Loss Function', 'Infinity', 'Feature Space', 'Knowledge Transfer', 'Data Augmentation', 'Student Network', 'Distillation Method', 'Distillation Loss', 'Augmented Samples', 'Training Data', 'Covariance Matrix', 'Feature Maps', 'Image Pixels', 'Spatial Dimensions', 'Transformer Model', 'Decision Boundary', 'Channel Dimension', 'Unseen Data', 'Student Model', 'Intra-class Variance', 'PASCAL VOC', 'Semantic Segmentation Methods', 'Semantic Segmentation Task', 'Limited Training Data', 'Hard Examples', 'Image Augmentation', 'Teacher Network', 'Fully Convolutional Network', 'Augmented Number']","['Algorithms', 'Image recognition and understanding']",4,"In this work, we explore data augmentations for knowledge distillation on semantic segmentation. Due the capacity gap, small-sized student networks struggle to discover the discriminative feature space learned by a powerful teacher. Image-level augmentations allow the student to better imitate the teacher by providing extra outputs. However, existing distillation frameworks only augment a limited number of samples, which restricts the learning of a student. Inspired by the recent progress on semantic directions on feature space, this work proposes a feature-level augmented knowledge distillation (FAKD) which infinitely augments features along a semantic direction for optimal knowledge transfer. Furthermore, we introduce novel surrogate loss functions to distill the teacher’s knowledge from an infinite number of samples. The surrogate loss is an upper bound of the expected distillation loss over infinite augmented samples. Extensive experiments on four semantic segmentation benchmarks demonstrate that the proposed method boosts the performance of current knowledge distillation methods without any significant overhead. The code will be released at FAKD."
FATE: Feature-Agnostic Transformer-Based Encoder for Learning Generalized Embedding Spaces in Flow Cytometry Data,"Lisa Weijler, Florian Kowarsch, Michael Reiter, Pedro Hermosilla, Margarita Maurer-Granofszky, Michael Dworzak",St. Anna CCRI; TU Wien,50.0,Austria,50.0,Austria,"While model architectures and training strategies have become more generic and flexible with respect to different data modalities over the past years, a persistent limitation lies in the assumption of fixed quantities and arrangements of input features. This limitation becomes particularly relevant in scenarios where the attributes captured during data acquisition vary across different samples. In this work, we aim at effectively leveraging data with varying features, without the need to constrain the input space to the intersection of potential feature sets or to expand it to their union. We propose a novel architecture that can directly process data without the necessity of aligned feature modalities by learning a general embedding space that captures the relationship between features across data samples with varying sets of features. This is achieved via a set-transformer architecture augmented by feature-encoder layers, thereby enabling the learning of a shared latent feature space from data originating from heterogeneous feature spaces. The advantages of the model are demonstrated for automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where the features measured during acquisition often vary between samples. Our proposed architecture's capacity to operate seamlessly across incongruent feature spaces is particularly relevant in this context, where data scarcity arises from the low prevalence of the disease. The code is available for research purposes at https://github.com/lisaweijler/FATE.",https://openaccess.thecvf.com/content/WACV2024/html/Weijler_FATE_Feature-Agnostic_Transformer-Based_Encoder_for_Learning_Generalized_Embedding_Spaces_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Weijler_FATE_Feature-Agnostic_Transformer-Based_Encoder_for_Learning_Generalized_Embedding_Spaces_in_WACV_2024_paper.pdf,,https://github.com/lisaweijler/FATE,2311.03314,main,Poster,https://ieeexplore.ieee.org/document/10483581/,"['Training', 'Analytical models', 'Microprocessors', 'Data acquisition', 'Computer architecture', 'Transformers', 'Data models']","['Flow Cytometry', 'Flow Cytometry Data', 'Latent Space', 'Cancer Cells', 'Feature Space', 'Acute Myeloid Leukemia', 'Input Features', 'Data Modalities', 'Training Set', 'Training Data', 'Pediatric Patients', 'Measurement Values', 'Detection Task', 'Input Sequence', 'Rate Set', 'Early Stopping', 'Image Patches', 'Features In Order', 'Acute Myeloid Leukemia Patients', 'Latent Representation', 'Multimodal Learning', 'Samples For Flow Cytometry', 'Feature Encoder', 'Scalar Value', 'Sample Xi', 'Masking Strategy', 'Prediction Head', 'Pediatric Acute Myeloid Leukemia', 'Common Space', 'Cell Populations']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"While model architectures and training strategies have become more generic and flexible with respect to different data modalities over the past years, a persistent limitation lies in the assumption of fixed quantities and arrangements of input features. This limitation becomes particularly relevant in scenarios where the attributes captured during data acquisition vary across different samples. In this work, we aim at effectively leveraging data with varying features, without the need to constrain the input space to the intersection of potential feature sets or to expand it to their union. We propose a novel architecture that can directly process data without the necessity of aligned feature modalities by learning a general embedding space that captures the relationship between features across data samples with varying sets of features. This is achieved via a set-transformer architecture augmented by feature-encoder layers, thereby enabling the learning of a shared latent feature space from data originating from heterogeneous feature spaces. The advantages of the model are demonstrated for automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where the features measured during acquisition often vary between samples. Our proposed architecture’s capacity to operate seamlessly across incongruent feature spaces is particularly relevant in this context, where data scarcity arises from the low prevalence of the disease. The code is available for research purposes at https://github.com/lisaweijler/FATE."
FELGA: Unsupervised Fragment Embedding for Fine-Grained Cross-Modal Association,"Yaoxin Zhuo, Baoxin Li","Arizona State University, Tempe, AZ, USA",100.0,USA,0.0,,"Vision-and-Language Pre-trained (VLP) models have demonstrated their powerful zero-shot ability in multiple downstream tasks. Most of these models are designed to learn joint embeddings of images and their paired sentences, with both modalities considered globally. This does not lead to optimal solutions for applications where what matters more is the local-level cross-modal association, such as the situation where a user may want to retrieve images with query words that link to only small parts of the images. While a VLP model could in principle be retrained to learn a new embedding capturing such fine-grained association, expensive annotation would be needed, making it impractical for big data applications. This paper proposes a novel method named Fragment Embedding by Local and Global Alignment (FELGA), which learns fragment-level embeddings that capture fine-grained cross-modal association through utilizing visual entity proposals and semantic concept proposals in an unsupervised manner. Comprehensive experiments conducted on three VLP models and two datasets demonstrate that FELGA is not limited to specific VLP models and outperforms the original VLP features. In particular, the learned embeddings support cross-modal fragment association tasks including query-driven object discovery and description assignment.",https://openaccess.thecvf.com/content/WACV2024/html/Zhuo_FELGA_Unsupervised_Fragment_Embedding_for_Fine-Grained_Cross-Modal_Association_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhuo_FELGA_Unsupervised_Fragment_Embedding_for_Fine-Grained_Cross-Modal_Association_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483801/,"['Visualization', 'Computer vision', 'Analytical models', 'Correlation', 'Annotations', 'Semantics', 'Big Data applications']","['Cross-modal Associations', 'Local Alignment', 'Semantic Knowledge', 'Global Alignment', 'Unsupervised Manner', 'Association Task', 'Embedding Learning', 'Training Set', 'Similarity Score', 'Visual Representation', 'Object Detection', 'Image Regions', 'Training Images', 'Bounding Box', 'Average Precision', 'Dense Connections', 'Region Proposal', 'Contrastive Loss', 'Sentence Completion', 'Maximum Similarity', 'Pseudo Labels', 'Current Batch', 'Entire Sentence', 'Sparse Connectivity', 'Highest Similarity Score', 'Unseen Classes', 'Label Matrix', 'Object Bounding Boxes', 'Training Batch', 'Unified Representation']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Vision-and-Language Pre-trained (VLP) models have demonstrated their powerful zero-shot ability in multiple downstream tasks. Most of these models are designed to learn joint embeddings of images and their paired sentences, with both modalities considered globally. This does not lead to optimal solutions for applications where what matters more is the local-level cross-modal association, such as the situation where a user may want to retrieve images with query words that link to only small parts of the images. While a VLP model could in principle be retrained to learn a new embedding capturing such fine-grained association, expensive annotation would be needed, making it impractical for big data applications. This paper proposes a novel method named Fragment Embedding by Local and Global Alignment (FELGA), which learns fragment-level embeddings that capture fine-grained cross-modal association through utilizing visual entity proposals and semantic concept proposals in an unsupervised manner. Comprehensive experiments conducted on three VLP models and two datasets demonstrate that FELGA is not limited to specific VLP models and outperforms the original VLP features. In particular, the learned embeddings support cross-modal fragment association tasks including query-driven object discovery and description assignment."
FG-Net: Facial Action Unit Detection With Generalizable Pyramidal Features,"Yufeng Yin, Di Chang, Guoxian Song, Shen Sang, Tiancheng Zhi, Jing Liu, Linjie Luo, Mohammad Soleymani",ByteDance; University of Southern California,50.0,USA,50.0,China,"Automatic detection of facial Action Units (AUs) allows for objective facial expression analysis. Due to the high cost of AU labeling and the limited size of existing benchmarks, previous AU detection methods tend to overfit the dataset, resulting in a significant performance loss when evaluated across corpora. To address this problem, we propose FG-Net for generalizable facial action unit detection. Specifically, FG-Net extracts feature maps from a StyleGAN2 model pre-trained on a large and diverse face image dataset. Then, these features are used to detect AUs with a Pyramid CNN Interpreter, making the training efficient and capturing essential local features. The proposed FG-Net achieves a strong generalization ability for heatmap-based AU detection thanks to the generalizable and semantic-rich features extracted from the pre-trained generative model. Extensive experiments are conducted to evaluate within- and cross-corpus AU detection with the widely-used DISFA and BP4D datasets. Compared with the state-of-the-art, the proposed method achieves superior cross-domain performance while maintaining competitive within-domain performance. In addition, FG-Net is data-efficient and achieves competitive performance even when trained on 1000 samples. Our code will be released at https://github.com/ihp-lab/FG-Net",https://openaccess.thecvf.com/content/WACV2024/html/Yin_FG-Net_Facial_Action_Unit_Detection_With_Generalizable_Pyramidal_Features_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yin_FG-Net_Facial_Action_Unit_Detection_With_Generalizable_Pyramidal_Features_WACV_2024_paper.pdf,,https://github.com/ihp-lab/FG-Net,,main,Poster,https://ieeexplore.ieee.org/document/10483673/,"['Training', 'Heating systems', 'Gold', 'Computer vision', 'Costs', 'Codes', 'Benchmark testing']","['Action Units', 'Facial Action Units', 'Action Unit Detection', 'Facial Action Unit Detection', 'Superior Performance', 'Feature Maps', 'Facial Expressions', 'Generalization Ability', 'Large Image Datasets', 'Strong Generalization Ability', 'Training Set', 'Training Data', 'Input Image', 'F1 Score', 'Multilayer Perceptron', 'Latent Space', 'Semantic Segmentation', 'Baseline Methods', 'Hidden State', 'Considerable Loss', 'Latent Code', 'Self-supervised Learning', 'Facial Action Coding System', 'Domain Adaptation', 'Validation Folds', 'Training Folds', 'Nearby Regions', 'Hierarchical Manner', 'Pyramid Level', 'Real-life Scenarios']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",3,"Automatic detection of facial Action Units (AUs) allows for objective facial expression analysis. Due to the high cost of AU labeling and the limited size of existing benchmarks, previous AU detection methods tend to overfit the dataset, resulting in a significant performance loss when evaluated across corpora. To address this problem, we propose FG-Net for generalizable facial action unit detection. Specifically, FG-Net extracts feature maps from a Style-GAN2 model pre-trained on a large and diverse face image dataset. Then, these features are used to detect AUs with a Pyramid CNN Interpreter, making the training efficient and capturing essential local features. The proposed FG-Net achieves a strong generalization ability for heatmap-based AU detection thanks to the generalizable and semantic-rich features extracted from the pre-trained generative model. Extensive experiments are conducted to evaluate within- and cross-corpus AU detection with the widely-used DISFA and BP4D datasets. Compared with the state-of-the-art, the proposed method achieves superior cross-domain performance while maintaining competitive within-domain performance. In addition, FG-Net is dataefficient and achieves competitive performance even when trained on 1000 samples. Our code will be released at https://github.com/ihp-lab/FG-Net"
FIRE: Food Image to REcipe Generation,"Prateek Chhikara, Dhiraj Chaurasia, Yifan Jiang, Omkar Masur, Filip Ilievski","Information Sciences Institute, USA; Vrije Universiteit Amsterdam, Netherlands; University of Southern California, USA; University of Southern California, USA; Information Sciences Institute, USA",100.0,"Netherlands, USA",0.0,,"Food computing has emerged as a prominent multidisciplinary field of research in recent years. An ambitious goal of food computing is to develop end-to-end intelligent systems capable of autonomously producing recipe information for a food image. Current image-to-recipe methods are retrieval-based and their success depends heavily on the dataset size and diversity, as well as the quality of learned embeddings. Meanwhile, the emergence of powerful attention-based vision and language models presents a promising avenue for accurate and generalizable recipe generation, which has yet to be extensively explored. This paper proposes FIRE, a novel multimodal methodology tailored to recipe generation in the food computing domain, which generates the food title, ingredients, and cooking instructions based on input food images. FIRE leverages the BLIP model to generate titles, utilizes a Vision Transformer with a decoder for ingredient extraction, and employs the T5 model to generate recipes incorporating titles and ingredients as inputs. We showcase two practical applications that can benefit from integrating FIRE with large language model prompting: recipe customization to fit recipes to user preferences and recipe-to-code transformation to enable automated cooking processes. Our experimental findings validate the efficacy of our proposed approach, underscoring its potential for future advancements and widespread adoption in food computing.",https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.pdf,,,2308.14391,main,Poster,https://ieeexplore.ieee.org/document/10484198/,"['Computer vision', 'Computational modeling', 'Transformers', 'Decoding', 'Intelligent systems']","['Food Images', 'Recipe Generation', 'Language Model', 'Visual Model', 'Vision Transformer', 'Image Features', 'Natural Language', 'Recurrent Neural Network', 'Attention Mechanism', 'Hallucinations', 'Food Ingredients', 'Inference Time', 'Binary Cross Entropy', 'Text Similarity', 'Image Captioning', 'Black Bean', 'Encoder-decoder Model', 'Code Representation', 'Cheddar Cheese', 'Image Embedding', 'Advances In Computer Vision', 'Recipe Ingredients', 'Number Of Demonstrations']","['Applications', 'Food science and nutrition']",5,"Food computing has emerged as a prominent multidisciplinary field of research in recent years. An ambitious goal of food computing is to develop end-to-end intelligent systems capable of autonomously producing recipe information for a food image. Current image-to-recipe methods are retrieval-based and their success depends heavily on the dataset size and diversity, as well as the quality of learned embeddings. Meanwhile, the emergence of powerful attention-based vision and language models presents a promising avenue for accurate and generalizable recipe generation, which has yet to be extensively explored. This paper proposes FIRE, a novel multimodal methodology tailored to recipe generation in the food computing domain, which generates the food title, ingredients, and cooking instructions based on input food images. FIRE leverages the BLIP model to generate titles, utilizes a Vision Transformer with a decoder for ingredient extraction, and employs the T5 model to generate recipes incorporating titles and ingredients as inputs. We showcase two practical applications that can benefit from integrating FIRE with large language model prompting: recipe customization to fit recipes to user preferences and recipe-to-code transformation to enable automated cooking processes. Our experimental findings validate the efficacy of our proposed approach, underscoring its potential for future advancements and widespread adoption in food computing."
FIRe: Fast Inverse Rendering Using Directional and Signed Distance Functions,"Tarun Yenamandra, Ayush Tewari, Nan Yang, Florian Bernard, Christian Theobalt, Daniel Cremers","Massachusetts Institute of Technology; University of Bonn; TU Munich, MCML; MPI Informatics, SIC",100.0,"Germany, USA",0.0,,"Neural 3D implicit representations learn priors that are useful for diverse applications, such as single- or multiple-view 3D reconstruction. A major downside of existing approaches while rendering an image is that they require evaluating the network multiple times per camera ray so that the high computational time forms a bottleneck for downstream applications. We address this problem by introducing a novel neural scene representation that we call the directional distance function (DDF). To this end, we learn a signed distance function (SDF) along with our DDF model to represent a class of shapes. Specifically, our DDF is defined on the unit sphere and predicts the distance to the surface along any given direction. Therefore, our DDF allows rendering images with just a single network evaluation per camera ray. Based on our DDF, we present a novel fast algorithm (FIRe) to reconstruct 3D shapes given a posed depth map. We evaluate our proposed method on 3D reconstruction from single-view depth images, where we empirically show that our algorithm reconstructs 3D shapes more accurately and it is more than 15 times faster (per iteration) than competing methods.",https://openaccess.thecvf.com/content/WACV2024/html/Yenamandra_FIRe_Fast_Inverse_Rendering_Using_Directional_and_Signed_Distance_Functions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yenamandra_FIRe_Fast_Inverse_Rendering_Using_Directional_and_Signed_Distance_Functions_WACV_2024_paper.pdf,,,2203.16284,main,Poster,https://ieeexplore.ieee.org/document/10484128/,"['Surface reconstruction', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Rendering (computer graphics)', 'Cameras']","['3D Reconstruction', 'Signed Distance Function', 'Depth Map', 'Neural Representations', 'Depth Images', '3D Shape', 'Unit Sphere', 'Directional Distance', 'Scene Representation', 'Implicit Representation', 'Shape Classification', 'Neural Network', 'Learning Models', 'Single Image', 'Points In Space', 'Point Cloud', 'Reconstruction Algorithm', 'High-dimensional Feature', '3D Point', 'Object Surface', 'Latent Code', 'Grid Features', 'Direct Representation', 'Camera Pose', '3D Grid', 'Random Direction', 'View Synthesis', 'Partial Observation', 'Chamfer Distance', '2D Grid']","['Algorithms', '3D computer vision']",,"Neural 3D implicit representations learn priors that are useful for diverse applications, such as single- or multiple-view 3D reconstruction. A major downside of existing approaches while rendering an image is that they require evaluating the network multiple times per camera ray so that the high computational time forms a bottleneck for downstream applications. We address this problem by introducing a novel neural scene representation that we call the directional distance function (DDF). To this end, we learn a signed distance function (SDF) along with our DDF model to represent a class of shapes. Specifically, our DDF is defined on the unit sphere and predicts the distance to the surface along any given direction. Therefore, our DDF allows rendering images with just a single network evaluation per camera ray. Based on our DDF, we present a novel fast algorithm (FIRe) to reconstruct 3D shapes given a posed depth map. We evaluate our proposed method on 3D reconstruction from single-view depth images, where we empirically show that our algorithm reconstructs 3D shapes more accurately and it is more than 15 times faster (per iteration) than competing methods."
FLORA: Fine-Grained Low-Rank Architecture Search for Vision Transformer,"Chi-Chih Chang, Yuan-Yao Sung, Shixing Yu, Ning-Chi Huang, Diana Marculescu, Kai-Chiang Wu","University of Texas at Austin, Cornell University; National Yang Ming Chiao Tung University; University of Texas at Austin",100.0,"Taiwan, USA",0.0,,"Vision Transformers (ViT) have recently demonstrated success across a myriad of computer vision tasks. However, their elevated computational demands pose significant challenges for real-world deployment. While low-rank approximation stands out as a renowned method to reduce computational loads, efficiently automating the target rank selection in ViT remains a challenge. Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA's efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradtion. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA.",https://openaccess.thecvf.com/content/WACV2024/html/Chang_FLORA_Fine-Grained_Low-Rank_Architecture_Search_for_Vision_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chang_FLORA_Fine-Grained_Low-Rank_Architecture_Search_for_Vision_Transformer_WACV_2024_paper.pdf,,https://github.com/shadowpa0327/FLORA,2311.03912,main,Poster,https://ieeexplore.ieee.org/document/10484479/,"['Training', 'Computer vision', 'Filtering', 'Flora', 'Computer architecture', 'Interference', 'Transformers']","['Vision Transformer', 'Source Code', 'Search Space', 'Hybrid Structure', 'Computational Demands', 'Low-rank Approximation', 'Neural Architecture Search', 'Rank Selection', 'Convolutional Neural Network', 'Eigenvectors', 'Singular Value Decomposition', 'Convolution Kernel', 'Choice Set', 'Transformer Model', 'Computational Overhead', 'Training Approach', 'Linear Mode', 'Embedding Dimension', 'Pareto Front', 'Linear Projection', 'Ranked Set', 'Transformer Architecture', 'Compression Method', 'Transformer Block', 'Linear Embedding']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Vision Transformers (ViT) have recently demonstrated success across a myriad of computer vision tasks. However, their elevated computational demands pose significant challenges for real-world deployment. While low-rank approximation stands out as a renowned method to reduce computational loads, efficiently automating the target rank selection in ViT remains a challenge. Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA’s efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradtion. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA."
FOSSIL: Free Open-Vocabulary Semantic Segmentation Through Synthetic References Retrieval,"Luca Barsellotti, Roberto Amoroso, Lorenzo Baraldi, Rita Cucchiara","University of Modena and Reggio Emilia, Italy and IIT-CNR, Italy; University of Modena and Reggio Emilia, Italy",100.0,Italy,0.0,,"Unsupervised Open-Vocabulary Semantic Segmentation aims to segment an image into regions referring to an arbitrary set of concepts described by text, without relying on dense annotations that are available only for a subset of the categories. Previous works relied on inducing pixel-level alignment in a multi-modal space through contrastive training over vast corpora of image-caption pairs. However, representing a semantic category solely through its textual embedding is insufficient to encompass the wide-ranging variability in the visual appearances of the images associated with that category. In this paper, we propose FOSSIL, a pipeline that enables a self-supervised backbone to perform open-vocabulary segmentation relying only on the visual modality. In particular, we decouple the task into two components: (1) we leverage text-conditioned diffusion models to generate a large collection of visual embeddings, starting from a set of captions. These can be retrieved at inference time to obtain a support set of references for the set of textual concepts. Further, (2) we exploit self-supervised dense features to partition the image into semantically coherent regions. We demonstrate that our approach provides strong performance on different semantic segmentation datasets, without requiring any additional training.",https://openaccess.thecvf.com/content/WACV2024/html/Barsellotti_FOSSIL_Free_Open-Vocabulary_Semantic_Segmentation_Through_Synthetic_References_Retrieval_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Barsellotti_FOSSIL_Free_Open-Vocabulary_Semantic_Segmentation_Through_Synthetic_References_Retrieval_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483609/,"['Training', 'Visualization', 'Sensitivity', 'Semantic segmentation', 'Semantics', 'Prototypes', 'Predictive models']","['Semantic Segmentation', 'Synthetic Reference', 'Diffusion Model', 'Inference Time', 'Set Of Concepts', 'Support Set', 'Segmentation Dataset', 'Coherent Regions', 'Semantic Segmentation Datasets', 'Eigenvectors', 'Weight Matrix', 'Visual Features', 'Intersection Over Union', 'Word Embedding', 'Image Collection', 'Textual Features', 'Set Of Categories', 'Visual Space', 'Self-supervised Learning', 'Objects In The Scene', 'Open-pit', 'Noisy Regions', 'Text Encoder', 'Foreground Objects', 'Input Text', 'Visual Reference', 'Region Proposal', 'Visual Encoding', 'Validation Images', 'Efficient Retrieval']","['Algorithms', 'Image recognition and understanding']",4,"Unsupervised Open-Vocabulary Semantic Segmentation aims to segment an image into regions referring to an arbitrary set of concepts described by text, without relying on dense annotations that are available only for a subset of the categories. Previous works rely on inducing pixel-level alignment in a multi-modal space through contrastive training over vast corpora of image-caption pairs. However, representing a semantic category solely through its textual embedding is insufficient to encompass the wide-ranging variability in the visual appearances of the images associated with that category. In this paper, we propose FOSSIL, a pipeline that enables a self-supervised backbone to perform open-vocabulary segmentation relying only on the visual modality. In particular, we decouple the task into two components: (1) we leverage text-conditioned diffusion models to generate a large collection of visual embeddings, starting from a set of captions. These can be retrieved at inference time to obtain a support set of references for the set of textual concepts. Further, (2) we exploit self-supervised dense features to partition the image into semantically coherent regions. We demonstrate that our approach provides strong performance on different semantic segmentation datasets, without requiring any additional training."
FOUND: Foot Optimization With Uncertain Normals for Surface Deformation Using Synthetic Data,"Oliver Boyne, Gwangbin Bae, James Charles, Roberto Cipolla","Department of Engineering, University of Cambridge, U.K.",100.0,UK,0.0,,"Surface reconstruction from multi-view images is a challenging task, with solutions often requiring a large number of sampled images with high overlap. We seek to develop a method for few-view reconstruction, for the case of the human foot. To solve this task, we must extract rich geometric cues from RGB images, before carefully fusing them into a final 3D object. Our FOUND approach tackles this, with 4 main contributions: (i) SynFoot, a synthetic dataset of 50,000 photorealistic foot images, paired with ground truth surface normals and keypoints; (ii) an uncertainty-aware surface normal predictor trained on our synthetic dataset; (iii) an optimization scheme for fitting a generative foot model to a series of images; and (iv) a benchmark dataset of calibrated images and high resolution ground truth geometry. We show that our normal predictor outperforms all off-the-shelf equivalents significantly on real images, and our optimization scheme outperforms state-of-the-art photogrammetry pipelines, especially for a few-view setting. We release our synthetic dataset and baseline 3D scans to the research community.",https://openaccess.thecvf.com/content/WACV2024/html/Boyne_FOUND_Foot_Optimization_With_Uncertain_Normals_for_Surface_Deformation_Using_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Boyne_FOUND_Foot_Optimization_With_Uncertain_Normals_for_Surface_Deformation_Using_WACV_2024_paper.pdf,,,2310.18279,main,Poster,https://ieeexplore.ieee.org/document/10484090/,"['Surface reconstruction', 'Uncertainty', 'Three-dimensional displays', 'Predictive models', 'Surface fitting', 'Task analysis', 'Image reconstruction']","['Optimal Strategy', 'Challenging Task', 'Reconstruction Method', 'RGB Images', '3D Scanning', 'Photogrammetry', 'Surface Reconstruction', 'Surface Normals', 'Photo-realistic Images', 'Mobile Phone', 'Data Augmentation', 'Point Cloud', 'High Uncertainty', 'Depth Map', 'Inertial Measurement Unit', 'Synthetic Images', 'Orthosis', 'Structure From Motion', 'Label Prediction', 'Normal Map', 'Multi-view Stereo', 'Registration Parameters', 'Angular Error', 'Accurate Surface', 'Scale Ambiguity', 'Domain Gap', 'Dense Point Cloud', 'L2 Loss', 'Ground Truth 3D']","['Applications', 'Commercial / retail', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Applications', 'Biomedical / healthcare / medicine']",1,"Surface reconstruction from multi-view images is a challenging task, with solutions often requiring a large number of sampled images with high overlap. We seek to develop a method for few-view reconstruction, for the case of the human foot. To solve this task, we must extract rich geometric cues from RGB images, before carefully fusing them into a final 3D object. Our FOUND approach tackles this, with 4 main contributions: (i) SynFoot, a synthetic dataset of 50,000 photorealistic foot images, paired with ground truth surface normals and keypoints; (ii) an uncertainty-aware surface normal predictor trained on our synthetic dataset; (iii) an optimization scheme for fitting a generative foot model to a series of images; and (iv) a benchmark dataset of calibrated images and high resolution ground truth geometry. We show that our normal predictor outperforms all off-the-shelf equivalents significantly on real images, and our optimization scheme outperforms state-of-the-art photogrammetry pipelines, especially for a few-view setting. We release our synthetic dataset and baseline 3D scans to the research community."
FPGAN-Control: A Controllable Fingerprint Generator for Training With Synthetic Data,"Alon Shoshan, Nadav Bhonker, Emanuel Ben Baruch, Ori Nizan, Igor Kviatkovsky, Joshua Engelsma, Manoj Aggarwal, Gérard Medioni",Technion - Israel Institute of Technology; Rank One Computing; Amazon,33.33333333333333,Israel,66.66666666666667,USA,"Training fingerprint recognition models using synthetic data has recently gained increased attention in the biometric community as it alleviates the dependency on sensitive personal data. Existing approaches for fingerprint generation are limited in their ability to generate diverse impressions of the same finger, a key property for providing effective data for training recognition models. To address this gap, we present FPGAN-Control, an identity preserving image generation framework which enables control over the fingerprint's image appearance (e.g., fingerprint type, acquisition device, pressure level) of generated fingerprints. We introduce a novel appearance loss that encourages disentanglement between the fingerprint's identity and appearance properties. In our experiments, we used the publicly available NIST SD302 (N2N) dataset for training the FPGAN-Control model. We demonstrate the merits of FPGAN-Control, both quantitatively and qualitatively, in terms of identity preservation level, degree of appearance control, and low synthetic-to-real domain gap. Finally, training recognition models using only synthetic datasets generated by FPGAN-Control lead to recognition accuracies that are on par or even surpass models trained using real data. To the best of our knowledge, this is the first work to demonstrate this.",https://openaccess.thecvf.com/content/WACV2024/html/Shoshan_FPGAN-Control_A_Controllable_Fingerprint_Generator_for_Training_With_Synthetic_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shoshan_FPGAN-Control_A_Controllable_Fingerprint_Generator_for_Training_With_Synthetic_Data_WACV_2024_paper.pdf,,https://atalonshoshan10.github.io/fpgan_control/,,main,Poster,https://ieeexplore.ieee.org/document/10484412/,"['Training', 'Image synthesis', 'Computational modeling', 'Biological system modeling', 'Fingers', 'Fingerprint recognition', 'NIST']","['Fingerprint Generation', 'Pressure Levels', 'Recognition Accuracy', 'Image Generation', 'Recognition Model', 'Acquisition Device', 'Domain Gap', 'Sensitive Personal Data', 'Identity Preservation', 'Types Of Fingerprints', 'Control Levels', 'Data Generation', 'Distance Function', 'Qualitative Results', 'Face Recognition', 'Image Pairs', 'Generative Adversarial Networks', 'Latent Space', 'Similar Appearance', 'Synthetic Images', 'Latent Vector', 'Identical Appearance', 'Appearance Variations', 'Training Batch', 'Biometric Identification', 'Explicit Control', 'Generative Adversarial Networks Training', 'Image Properties']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"Training fingerprint recognition models using synthetic data has recently gained increased attention in the biometric community as it alleviates the dependency on sensitive personal data. Existing approaches for fingerprint generation are limited in their ability to generate diverse impressions of the same finger, a key property for providing effective data for training recognition models. To address this gap, we present FPGAN-Control, an identity preserving image generation framework which enables control over the fingerprint’s image appearance (e.g., fingerprint type, acquisition device, pressure level) of generated fingerprints. We introduce a novel appearance loss that encourages disentanglement between the fingerprint’s identity and appearance properties. In our experiments, we used the publicly available NIST SD302 (N2N) dataset for training the FPGAN-Control model. We demonstrate the merits of FPGAN-Control, both quantitatively and qualitatively, in terms of identity preservation level, degree of appearance control, and low synthetic-to-real domain gap. Finally, training recognition models using only synthetic datasets generated by FPGAN-Control lead to recognition accuracies that are on par or even surpass models trained using real data. To the best of our knowledge, this is the first work to demonstrate this."
FRoG-MOT: Fast and Robust Generic Multiple-Object Tracking by IoU and Motion-State Associations,"Takuya Ogawa, Takashi Shibata, Toshinori Hosoi",NEC Corporation,0.0,,100.0,Japan,"This paper proposes a generic multi-object tracking (MOT) algorithm that is robust to unexpected motion changes for generic objects. Deep learning has dramatically been improving MOT performances. Nevertheless, state-of-the-art tracking algorithms are still sensitive to unexpected motion changes and the generic object target beyond person tracking. This is because standard MOT benchmark datasets such as MOT17 mainly consist of persons in a crowd, often lacking unexpected shape and motion changes; thus, these issues have yet to be focused on. We propose a simple-yet-effective MOT framework that can dynamically improve tracking continuity by associating each target based on adaptively modified motion states. The keys are 1) to represent the target motions using multiple motion states that have weak correlations with each other and 2) to modify those states that have the lowest similarity to past states as outliers. Our approach can overwhelmingly improve trajectory continuity and robustness to unexpected motion changes for generic objects. Comprehensive experiments have confirmed that our framework is comparable to existing state-of-the-art methods on a standard dataset and outperforms those algorithms on the GMOT dataset with an overall 2% improvement in IDF1, a measure of tracking continuity.",https://openaccess.thecvf.com/content/WACV2024/html/Ogawa_FRoG-MOT_Fast_and_Robust_Generic_Multiple-Object_Tracking_by_IoU_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ogawa_FRoG-MOT_Fast_and_Robust_Generic_Multiple-Object_Tracking_by_IoU_and_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483947/,"['Deep learning', 'Computer vision', 'Target tracking', 'Correlation', 'Shape', 'Benchmark testing', 'Robustness']","['Multiple Object Tracking', 'Weak Correlation', 'Shape Changes', 'Tracking Algorithm', 'General Objective', 'Standard Datasets', 'Motion State', 'Motor Changes', 'Unexpected Changes', 'Multi-object Tracking', 'High Speed', 'Similarity Measure', 'Processing Speed', 'Object Detection', 'Sudden Changes', 'Bounding Box', 'Kalman Filter', 'Tracking Performance', 'Object Tracking', 'Adaptive Modifications', 'Derivative Of Position', 'Hypothetical Distribution', 'Laplace Distribution', 'Parametric T-test', 'Motion Velocity']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"This paper proposes a generic multi-object tracking (MOT) algorithm that is robust to unexpected motion changes for generic objects. Deep learning has dramatically been improving MOT performances. Nevertheless, state-of-the-art tracking algorithms are still sensitive to unexpected motion changes and the generic object target beyond person tracking. This is because standard MOT benchmark datasets such as MOT17 mainly consist of persons in a crowd, often lacking unexpected shape and motion changes; thus, these issues have yet to be focused on. We propose a simple-yet-effective MOT framework that can dynamically improve tracking continuity by associating each target based on adaptively modified motion states. The keys are 1) to represent the target motions using multiple motion states that have weak correlations with each other and 2) to modify those states that have the lowest similarity to past states as outliers. Our approach can improve trajectory continuity and robustness to unexpected motion changes for generic objects. Comprehensive experiments have confirmed that our framework is comparable to existing state-of-the-art methods on a standard dataset and outperforms those algorithms on the GMOT dataset with an overall 2% improvement in IDF1, a measure of tracking continuity."
FacadeNet: Conditional Facade Synthesis via Selective Editing,"Yiangos Georgiou, Marios Loizou, Tom Kelly, Melinos Averkiou","KAUST, Saudi Arabia; University of Cyprus/CYENS CoE, Cyprus",100.0,"Cyprus, Saudi Arabia",0.0,,"We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pretrained vision transformer Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods.",https://openaccess.thecvf.com/content/WACV2024/html/Georgiou_FacadeNet_Conditional_Facade_Synthesis_via_Selective_Editing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Georgiou_FacadeNet_Conditional_Facade_Synthesis_via_Selective_Editing_WACV_2024_paper.pdf,,,2311.01240,main,Poster,https://ieeexplore.ieee.org/document/10484527/,"['Deep learning', 'Computer vision', 'Buildings', 'Transformers', 'Windows']","['Deep Learning', 'Generative Adversarial Networks', 'Selective Modulators', 'Conditional Generative Adversarial Network', 'Vision Transformer', 'Building Facades', 'Input Image', 'Single Image', 'Urban Planning', 'Horizontal Axis', 'Autoencoder', 'Diffusion Model', 'Latent Space', 'Design Choices', 'Semantic Segmentation', 'Reference Image', 'Viewing Angle', '3D Environment', 'Heritage Conservation', 'Reconstruction Loss', 'View Direction', 'View Synthesis', 'Latent Code', 'Camera Pose', 'Homography', 'Image Building', 'Synthesis Quality', 'Camera Position', 'Perceptual Similarity', 'Implementation Details']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pretrained vision transformer. Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods."
Face Identity-Aware Disentanglement in StyleGAN,"Adrian Suwała, Bartosz Wójcik, Magdalena Proszewska, Jacek Tabor, Przemysław Spurek, Marek Śmieja","University of Edinburgh; Faculty of Mathematics and Computer Science, Jagiellonian University, Kraków, Poland; Doctoral School of Exact and Natural Sciences, Jagiellonian University, Poland; IDEAS NCBR, Warsaw, Poland",75.0,"Poland, UK",25.0,Poland,"Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models.",https://openaccess.thecvf.com/content/WACV2024/html/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Suwala_Face_Identity-Aware_Disentanglement_in_StyleGAN_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484158/,"['Training', 'Computer vision', 'Correlation', 'Self-supervised learning', 'Motion pictures', 'Faces']","['Image Features', 'Regions Of Space', 'Latent Space', 'Face Images', 'Image Properties', 'Contrastive Loss', 'Person Image', 'Conditional Generative Adversarial Network', 'Movie Frames', 'Facial Attributes', 'Optimization Algorithm', 'Normal Flow', 'Quantitative Way', 'Encoder Network', 'Image X', 'Negative Log-likelihood', 'Latent Code', 'Synthesis Network']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person’s identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person’s identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models."
Face Presentation Attack Detection by Excavating Causal Clues and Adapting Embedding Statistics,"Meiling Fang, Naser Damer","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Darmstadt, Germany",100.0,Germany,0.0,,"Recent face presentation attack detection (PAD) leverages domain adaptation (DA) and domain generalization (DG) techniques to address performance degradation on unknown domains. However, DA-based PAD methods require access to unlabeled target data, while most DG-based PAD solutions rely on a priori, i.e., known domain labels. Moreover, most DA-/DG-based methods are computationally intensive, demanding complex model architectures and/or multi-stage training processes. This paper proposes to model face PAD as a compound DG task from a causal perspective, linking it to model optimization. We excavate the causal factors hidden in the high-level representation via counterfactual intervention. Moreover, we introduce a class-guided MixStyle to enrich feature-level data distribution within classes instead of focusing on domain information. Both class-guided MixStyle and counterfactual intervention components introduce no extra trainable parameters and negligible computational resources. Extensive cross-dataset and analytic experiments demonstrate the effectiveness and efficiency of our method compared to state-of-the-art PADs. The implementation and the trained weights are publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Fang_Face_Presentation_Attack_Detection_by_Excavating_Causal_Clues_and_Adapting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fang_Face_Presentation_Attack_Detection_by_Excavating_Causal_Clues_and_Adapting_WACV_2024_paper.pdf,,https://github.com/meilfang/CF-PAD,2308.14551,main,Poster,https://ieeexplore.ieee.org/document/10483894/,"['Training', 'Degradation', 'Adaptation models', 'Computational modeling', 'Training data', 'Focusing', 'Excavation']","['Presentation Attack', 'Presentation Attack Detection', 'Face Presentation Attack Detection', 'Data Distribution', 'Optimal Model', 'Counterfactual', 'Model Architecture', 'Trainable Parameters', 'Unlabeled Data', 'Target Data', 'Domain Adaptation', 'Domain Generalization', 'Extra Parameters', 'High-level Representations', 'Domain Adaptation Techniques', 'Unlabeled Target Data', 'Training Data', 'Percentage Points', 'Causal Inference', 'Feature Learning', 'Replay Attacks', 'High-level Features', 'Random Shuffling', 'Source Domain', 'Target Domain', 'Floating-point Operations', 'Challenging Scenarios', 'Empirical Risk Minimization', 'Types Of Attacks', 'Inference Phase']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Recent face presentation attack detection (PAD) leverages domain adaptation (DA) and domain generalization (DG) techniques to address performance degradation on unknown domains. However, DA-based PAD methods require access to unlabeled target data, while most DG-based PAD solutions rely on a priori, i.e., known domain labels. Moreover, most DA-/DG-based methods are computationally intensive, demanding complex model architectures and/or multi-stage training processes. This paper proposes to model face PAD as a compound DG task from a causal perspective, linking it to model optimization. We excavate the causal factors hidden in the high-level representation via counterfactual intervention. Moreover, we introduce a class-guided MixStyle to enrich feature-level data distribution within classes instead of focusing on domain information. Both class-guided MixStyle and counterfactual intervention components introduce no extra trainable parameters and negligible computational resources. Extensive cross-dataset and analytic experiments demonstrate the effectiveness and efficiency of our method compared to state-of-the-art PADs. The implementation and the trained weights are publicly available
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
FarSight: A Physics-Driven Whole-Body Biometric System at Large Distance and Altitude,"Feng Liu, Ryan Ashbaugh, Nicholas Chimitt, Najmul Hassan, Ali Hassani, Ajay Jaiswal, Minchul Kim, Zhiyuan Mao, Christopher Perry, Zhiyuan Ren, Yiyang Su, Pegah Varghaei, Kai Wang, Xingguang Zhang, Stanley Chan, Arun Ross, Humphrey Shi, Zhangyang Wang, Anil Jain, Xiaoming Liu","University of Oregon, Eugene OR 97403, USA; Georgia Tech, Atlanta GA 30332, USA; Purdue University, West Lafayette IN 47907, USA; University of Texas at Austin, Austin TX 78712, USA; Michigan State University, East Lansing MI 48824, USA",100.0,USA,0.0,,"Whole-body biometric recognition is an important area of research due to its vast applications in law enforcement, border security, and surveillance. This paper presents the end-to-end design, development and evaluation of FarSight, an innovative software system designed for whole-body (fusion of face, gait and body shape) biometric recognition. FarSight accepts videos from elevated platforms and drones as input and outputs a candidate list of identities from a gallery. The system is designed to address several challenges, including (i) low-quality imagery, (ii) large yaw and pitch angles, (iii) robust feature extraction to accommodate large intra-person variabilities and large inter-person similarities, and (iv) the large domain gap between training and test sets. FarSight combines the physics of imaging and deep learning models to enhance image restoration and biometric feature encoding. We test FarSight's effectiveness using the newly acquired IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) dataset. Notably, FarSight demonstrated a substantial performance increase on the BRIAR dataset, with gains of +11.82% Rank-20 identification and +11.3% TAR@1%FAR.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_FarSight_A_Physics-Driven_Whole-Body_Biometric_System_at_Large_Distance_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_FarSight_A_Physics-Driven_Whole-Body_Biometric_System_at_Large_Distance_and_WACV_2024_paper.pdf,,,2306.17206,main,Poster,https://ieeexplore.ieee.org/document/10484316/,"['Training', 'Deep learning', 'Face recognition', 'Biological system modeling', 'Surveillance', 'Imaging', 'US Department of Homeland Security']","['Hyperopia', 'Biometric Systems', 'Training Set', 'Imagery', 'Deep Learning Models', 'Body Shape', 'Pitch Angle', 'Yaw Angle', 'Facial Shape', 'Domain Gap', 'Border Security', 'Biometric Identification', 'Biometric Characteristics', 'Elevated Platform', 'Robust Feature Extraction', 'Training Data', 'Superior Performance', 'Image Quality', 'Local Features', 'Naked Body', '3D Pose', 'Atmospheric Turbulence', 'Bounding Box', 'Face Recognition', 'Correction Process', '3D Reconstruction', 'Stochastic Phenomena', 'Multiple Modalities', 'Recognition Performance']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",6,"Whole-body biometric recognition is an important area of research due to its vast applications in law enforcement, border security, and surveillance. This paper presents the end-to-end design, development and evaluation of FarSight, an innovative software system designed for whole-body (fusion of face, gait and body shape) biometric recognition. FarSight accepts videos from elevated platforms and drones as input and outputs a candidate list of identities from a gallery. The system is designed to address several challenges, including (i) low-quality imagery, (ii) large yaw and pitch angles, (iii) robust feature extraction to accommodate large intra-person variabilities and large inter-person similarities, and (iv) the large domain gap between training and test sets. FarSight combines the physics of imaging and deep learning models to enhance image restoration and biometric feature encoding. We test FarSight’s effectiveness using the newly acquired IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) dataset. Notably, FarSight demonstrated a substantial performance increase on the BRIAR dataset, with gains of +11.82% Rank-20 identification and +11.30% TAR@1% FAR."
Fast Diffusion EM: A Diffusion Model for Blind Inverse Problems With Application to Deconvolution,"Charles Laroche, Andrés Almansa, Eva Coupeté",GoPro & MAP5; GoPro; CNRS & Université Paris Cité,33.33333333333333,France,66.66666666666667,USA,"Using diffusion models to solve inverse problems is a growing field of research. Current methods assume the degradation to be known and provide impressive results in terms of restoration quality and diversity. In this work, we leverage the efficiency of those models to jointly estimate the restored image and unknown parameters of the degradation model such as blur kernel. In particular, we designed an algorithm based on the well-known Expectation-Minimization (EM) estimation method and diffusion models. Our method alternates between approximating the expected log-likelihood of the inverse problem using samples drawn from a diffusion model and a maximization step to estimate unknown model parameters. For the maximization step, we also introduce a novel blur kernel regularization based on a Plug & Play denoiser. Diffusion models are long to run, thus we provide a fast version of our algorithm. Extensive experiments on blind image deblurring demonstrate the effectiveness of our method when compared to other state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Laroche_Fast_Diffusion_EM_A_Diffusion_Model_for_Blind_Inverse_Problems_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Laroche_Fast_Diffusion_EM_A_Diffusion_Model_for_Blind_Inverse_Problems_WACV_2024_paper.pdf,,https://github.com/claroche-r/FastDiffusionEM,,main,Poster,https://ieeexplore.ieee.org/document/10483663/,"['Degradation', 'Deconvolution', 'Computational modeling', 'Estimation', 'Diffusion processes', 'Approximation algorithms', 'Image restoration']","['Diffusion Model', 'Inverse Problem', 'Unknown Parameters', 'Blur Kernel', 'Mean Square Error', 'Deep Learning', 'Deep Neural Network', 'Diffusion Process', 'Expectation Maximization', 'Score Model', 'Hallucinations', 'Weight Decay', 'Presence Of Noise', 'Forward Model', 'Clear Image', 'Maximum A Posteriori', 'Posterior Samples', 'Kernel Estimation', 'Drawing Samples', 'Blurry Images', 'Classical Diffusion', 'Unobserved Data', 'Blind Deconvolution']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",2,"Using diffusion models to solve inverse problems is a growing field of research. Current methods assume the degradation to be known and provide impressive results in terms of restoration quality and diversity. In this work, we leverage the efficiency of those models to jointly estimate the restored image and unknown parameters of the degradation model such as blur kernel. In particular, we designed an algorithm based on the well-known Expectation-Minimization (EM) estimation method and diffusion models. Our method alternates between approximating the expected log-likelihood of the inverse problem using samples drawn from a diffusion model and a maximization step to estimate unknown model parameters. For the maximization step, we also introduce a novel blur kernel regularization based on a Plug & Play denoiser. Diffusion models are long to run, thus we provide a fast version of our algorithm. Extensive experiments on blind image deblurring demonstrate the effectiveness of our method when compared to other state-of-the-art approaches. Our code is available at https://github.com/claroche-r/FastDiffusionEM."
Fast Sun-Aligned Outdoor Scene Relighting Based on TensoRF,"Yeonjin Chang, Yearim Kim, Seunghyeon Seo, Jung Yi, Nojun Kwak",Seoul National University; Yonsei University,100.0,South Korea,0.0,,"In this work, we introduce our method of outdoor scene relighting for Neural Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF). SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun, thereby achieving a simplified workflow that eliminates the need for environment maps. Our sun-alignment strategy is motivated by the insight that shadows, unlike viewpoint-dependent albedo, are determined by light direction. We directly use the sun direction as an input during shadow generation, simplifying the requirements of the inference process significantly. Moreover, SR-TensoRF leverages the training efficiency of TensoRF by incorporating our proposed cubemap concept, resulting in notable acceleration in both training and rendering processes compared to existing methods.",https://openaccess.thecvf.com/content/WACV2024/html/Chang_Fast_Sun-Aligned_Outdoor_Scene_Relighting_Based_on_TensoRF_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chang_Fast_Sun-Aligned_Outdoor_Scene_Relighting_Based_on_TensoRF_WACV_2024_paper.pdf,,,2311.03965,main,Poster,https://ieeexplore.ieee.org/document/10484205/,"['Training', 'Computer vision', 'Tensors', 'Clouds', 'Pipelines', 'Lighting', 'Rendering (computer graphics)']","['Outdoor Scenes', 'Direct Light', 'Environment Map', 'Functional Need', 'Mean Square Error', 'Control Condition', 'Training Time', 'Multilayer Perceptron', 'Training Images', 'Open Set', '3D Coordinates', 'Quantitative Metrics', 'Spherical Harmonics', 'Latent Vector', '3D Scene', 'Camera Pose', 'Reduce Training Time', 'View Synthesis']","['Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"In this work, we introduce our method of outdoor scene relighting for Neural Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF). SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun, thereby achieving a simplified workflow that eliminates the need for environment maps. Our sun-alignment strategy is motivated by the insight that shadows, unlike viewpoint-dependent albedo, are determined by light direction. We directly use the sun direction as an input during shadow generation, simplifying the requirements of the inference process significantly. Moreover, SR-TensoRF leverages the training efficiency of TensoRF by incorporating our proposed cubemap concept, resulting in notable acceleration in both training and rendering processes compared to existing methods."
Fast and Interpretable Face Identification for Out-of-Distribution Data Using Vision Transformers,"Hai Phan, Cindy X. Le, Vu Le, Yihui He, Anh “Totti” Nguyen",Carnegie Mellon University; Auburn University; Columbia University; Phenikaa University,100.0,"USA, Vietnam",0.0,,"Most face identification approaches employ a Siamese neural network to compare two images at the image embedding level. Yet, this technique can be subject to occlusion (e.g., faces with masks or sunglasses) and out-of-distribution data. DeepFace-EMD (Phan et al. 2022) reaches state-of-the-art accuracy on out-of-distribution data by first comparing two images at the image level, and then at the patch level. Yet, its later patch-wise re-ranking stage admits a large O(n^3 log n) time complexity (for n patches in an image) due to the optimal transport optimization. In this paper, we propose a novel, 2-image Vision Transformers (ViTs) that compares two images at the patch level using cross-attention. After training on 2M pairs of images on CASIA Webface (Yi et al. 2014), our model performs at a comparable accuracy as DeepFace-EMD on out-of-distribution data, yet at an inference speed more than twice as fast as DeepFace-EMD (Phan et al. 2022). In addition, via a human study, our model shows promising explainability through the visualization of cross-attention. We believe our work can inspire more explorations in using ViTs for face identification.",https://openaccess.thecvf.com/content/WACV2024/html/Phan_Fast_and_Interpretable_Face_Identification_for_Out-of-Distribution_Data_Using_Vision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Phan_Fast_and_Interpretable_Face_Identification_for_Out-of-Distribution_Data_Using_Vision_WACV_2024_paper.pdf,,,2311.02803,main,Poster,,,,,,
FastCLIPstyler: Optimisation-Free Text-Based Image Style Transfer Using Style Representations,"Ananda Padhmanabhan Suresh, Sanjana Jain, Pavit Noinongyao, Ankush Ganguly, Ukrit Watchareeruetai, Aubin Samacoits","Sertis Vision Lab, 597/5 Sukhumvit Road, Watthana, Bangkok, 10110, Thailand",0.0,,100.0,Thailand,"In recent years, language-driven artistic style transfer has emerged as a new type of style transfer technique, eliminating the need for a reference style image by using natural language descriptions of the style. The first model to achieve this, called CLIPstyler, has demonstrated impressive stylisation results. However, its lengthy optimisation procedure at runtime for each query limits its suitability for many practical applications. In this work, we present FastCLIPstyler, a generalised text-based image style transfer model capable of stylising images in a single forward pass for arbitrary text inputs. Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for compatibility with resource-constrained devices. Through quantitative and qualitative comparisons with state-of-the-art approaches, we demonstrate that our models achieve superior stylisation quality based on measurable metrics while offering significantly improved runtime efficiency, particularly on edge devices.",https://openaccess.thecvf.com/content/WACV2024/html/Suresh_FastCLIPstyler_Optimisation-Free_Text-Based_Image_Style_Transfer_Using_Style_Representations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Suresh_FastCLIPstyler_Optimisation-Free_Text-Based_Image_Style_Transfer_Using_Style_Representations_WACV_2024_paper.pdf,,,2210.03461,main,Poster,https://ieeexplore.ieee.org/document/10484351/,"['Measurement', 'Computer vision', 'Runtime', 'Image edge detection', 'Computational modeling', 'Natural languages', 'Optimization']","['Style Transfer', 'Style Image', 'Style Representation', 'Single Pass', 'Edge Devices', 'Resource-constrained Devices', 'Natural Language Descriptions', 'Loss Function', 'Neural Network', 'Semantic', 'Structural Similarity', 'Convolutional Neural Network', 'Mobile Devices', 'Quantitative Evaluation', 'Convolutional Neural Network Model', 'Data Security', 'Latent Space', 'Prediction Network', 'Supplementary Section', 'Inference Time', 'Low-power Devices', 'Range Query', 'Loss Term', 'Undesirable Artifacts', 'Artistic Quality', 'Real-world Objects', 'Need For Optimization']","['Applications', 'Arts / games / social media', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities']",,"In recent years, language-driven artistic style transfer has emerged as a new type of style transfer technique, eliminating the need for a reference style image by using natural language descriptions of the style. The first model to achieve this, called CLIPstyler, has demonstrated impressive stylisation results. However, its lengthy optimisation procedure at runtime for each query limits its suitability for many practical applications. In this work, we present FastCLIPstyler, a generalised text-based image style transfer model capable of stylising images in a single forward pass for arbitrary text inputs. Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for compatibility with resource-constrained devices. Through quantitative and qualitative comparisons with state-of-the-art approaches, we demonstrate that our models achieve superior stylisation quality based on measurable metrics while offering significantly improved runtime efficiency, particularly on edge devices."
FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices With a Simple Super-Resolution Pipeline,"Chien-Yu Lin, Qichen Fu, Thomas Merth, Karren Yang, Anurag Ranjan","University of Washington, Seattle, WA, USA; Apple, Inc., Cupertino, CA, USA",50.0,USA,50.0,USA,"Super-resolution (SR) techniques have recently been proposed to upscale the outputs of neural radiance fields (NeRF) and generate high-quality images with enhanced inference speeds. However, existing NeRF+SR methods increase training overhead by using extra input features, loss functions, or expensive training procedures such as knowledge distillation. In this paper, we aim to leverage SR for efficiency gains without costly training or architectural changes. Specifically, we build a simple NeRF+SR pipeline that directly combines existing modules, and we propose a lightweight augmentation technique, random patch sampling, for training. Compared to existing NeRF+SR methods, our pipeline mitigates the SR computing overhead and can be trained up to 23x faster, making it feasible to run on consumer devices such as the Apple MacBook. Experiments show that our pipeline can upscale NeRF outputs by 2-4x while maintaining high quality, increasing inference speeds by up to 18x on an NVIDIA V100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but effective technique for improving the efficiency of NeRF models for consumer devices.",https://openaccess.thecvf.com/content/WACV2024/html/Lin_FastSR-NeRF_Improving_NeRF_Efficiency_on_Consumer_Devices_With_a_Simple_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lin_FastSR-NeRF_Improving_NeRF_Efficiency_on_Consumer_Devices_With_a_Simple_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484463/,"['Training', 'Privacy', 'Portable computers', 'Computational modeling', 'Pipelines', 'Superresolution', 'Memory management']","['Neural Radiance Fields', 'Random Sampling', 'Computational Overhead', 'Extra Features', 'Extra Training', 'Training Overhead', 'Sample Patches', 'High-resolution Images', 'Training Time', 'Model Size', 'Training Images', 'Reasonable Time', 'Grid Size', 'Convolution Kernel', 'Viewing Angle', 'Line Of Work', 'Peak Signal-to-noise Ratio', 'Major Objective', 'Light Field', 'Bilinear Interpolation', 'Super-resolution Model', 'View Synthesis', 'Real-world Scenes', 'Scene Dataset', 'Patch Extraction', '3D Scene', 'Number Of Rays']","['Applications', 'Smartphones / end user devices', 'Algorithms', '3D computer vision', 'Applications', 'Virtual / augmented reality']",4,"Super-resolution (SR) techniques have recently been proposed to upscale the outputs of neural radiance fields (NeRF) and generate high-quality images with enhanced inference speeds. However, existing NeRF+SR methods increase training overhead by using extra input features, loss functions, and/or expensive training procedures such as knowledge distillation. In this paper, we aim to leverage SR for efficiency gains without costly training or architectural changes. Specifically, we build a simple NeRF+SR pipeline that directly combines existing modules, and we propose a lightweight augmentation technique, random patch sampling, for training. Compared to existing NeRF+SR methods, our pipeline mitigates the SR computing overhead and can be trained up to 23× faster, making it feasible to run on consumer devices such as the Apple MacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4× while maintaining high quality, increasing inference speeds by up to 18× on an NVIDIA V100 GPU and 12.8× on an M1 Pro chip. We conclude that SR can be a simple but effective technique for improving the efficiency of NeRF models for consumer devices."
Favoring One Among Equals - Not a Good Idea: Many-to-One Matching for Robust Transformer Based Pedestrian Detection,"K.N. Ajay Shastry, K. Ravi Sri Teja, Aditya Nigam, Chetan Arora","Indian Institute of Technology, Delhi; Indian Institute of Technology, Mandi",100.0,India,0.0,,"We investigate the reasons for lower performance of transformer based pedestrian detection models compared to convolutional neural network (CNN) based ones. CNN models generate dense pedestrian proposals, refine each proposal individually, and follow it up with non-maximal-suppression (NMS) to generate sparse predictions. In contrast, transformer models select one proposal per ground-truth (GT) pedestrian box and backpropagate positive gradient from them. All other proposals, many of them highly similar to the selected ones, are passed negative gradient. Though this leads to sparse predictions, obviating the need of NMS, the arbitrary selection of one among many similar proposals, hinders effective training, and lower accuracy of pedestrian detection. To mitigate the problem, instead of commonly used Kuhn-Munkres matching algorithm, we propose Min-cost-flow based formulation, and incorporate constraints such as, each ground truth box is matched to atleast one proposal, and many equally good proposals can be matched to a single ground truth box. We propose first transformer based pedestrian detection model incorporating our matching algorithm. Extensive experiments reveal that our approach achieves a miss rate (lower is better) of 3.7 / 17.4 / 21.8 / 8.3 / 2.0 on Eurocity / TJU-traffic / TJU-campus / Cityperson / Caltech datasets compared to 4.7 / 18.7 / 24.8 / 8.5 / 3.1 by the current SOTA. Code is available at https://ajayshastry08.github.io/flow_matcher",https://openaccess.thecvf.com/content/WACV2024/html/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shastry_Favoring_One_Among_Equals_-_Not_a_Good_Idea_Many-to-One_WACV_2024_paper.pdf,,https://ajayshastry08.github.io/flow_matcher,,main,Poster,https://ieeexplore.ieee.org/document/10484281,"['Training', 'Computer vision', 'Pedestrians', 'Costs', 'Codes', 'Predictive models', 'Transformers']","['Pedestrian Detection', 'Convolutional Neural Network', 'Convolutional Neural Network Model', 'Matching Algorithm', 'Transformer Model', 'Missing Rate', 'Negative Gradient', 'Positive Gradient', 'Ground-truth Box', 'Minimum Cost Flow', 'Model Performance', 'Low Capacity', 'Object Detection', 'Benchmark Datasets', 'Bounding Box', 'Confidence Score', 'L1-norm', 'Matching Strategy', 'Minimum Flow', 'Region Proposal Network', 'Intersection Over Union Score', 'Sink Node', 'Proposal Generation', 'Matching Quality', 'Cost Matrix', 'Gradient Scale', 'Computer Vision Problems', 'CNN-based Models', 'Decoder Layer']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving']",,"We investigate the reasons for lower performance of transformer based pedestrian detection models compared to convolutional neural network (CNN) based ones. CNN models generate dense pedestrian proposals, refine each proposal individually, and follow it up with non-maximal-suppression (NMS) to generate sparse predictions. In contrast, transformer models select one proposal per groundtruth (GT) pedestrian box and backpropagate positive gradient from them. All other proposals, many of them highly similar to the selected ones, are passed negative gradient. Though this leads to sparse predictions, obviating the need of NMS, the arbitrary selection of one among many similar proposals, hinders effective training, and lower accuracy of pedestrian detection. To mitigate the problem, instead of commonly used Kuhn-Munkres matching algorithm, we propose Min-cost-flow based formulation, and incorporate constraints such as, each ground truth box is matched to atleast one proposal, and many equally good proposals can be matched to a single ground truth box. We propose first transformer based pedestrian detection model incorporating our matching algorithm. Extensive experiments reveal that our approach achieves a miss rate (lower is better) of 3.7 / 17.4 / 21.8 / 8.3 / 2.0 on Eurocity / TJU-traffic / TJUcampus / Cityperson / Caltech datasets compared to 4.7 /18.7 / 24.8 / 8.5 / 3.1 by the current SOTA. Code is available at https://ajayshastry08.github.io/flow_matcher"
Feed-Forward Latent Domain Adaptation,"Ondrej Bohdal, Da Li, Shell Xu Hu, Timothy Hospedales","School of Informatics, The University of Edinburgh, UK; School of Informatics, The University of Edinburgh, UK; Samsung AI Center Cambridge, UK; Samsung AI Center Cambridge, UK",100.0,UK,0.0,,"We study a new highly-practical problem setting that enables resource-constrained edge devices to adapt a pre-trained model to their local data distributions. Recognizing that device's data are likely to come from multiple latent domains that include a mixture of unlabelled domain-relevant and domain-irrelevant examples, we focus on the comparatively under-studied problem of latent domain adaptation. Considering limitations of edge devices, we aim to only use a pre-trained model and adapt it in a feed-forward way, without using back-propagation and without access to the source data. Modelling these realistic constraints bring us to the novel and practically important problem setting of feed-forward latent domain adaptation. Our solution is to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. The resulting framework leads to consistent improvements over strong ERM baselines. We also show that our framework sometimes even improves on the upper bound of domain-supervised adaptation, where only domain-relevant instances are provided for adaptation. This suggests that human annotated domain labels may not always be optimal, and raises the possibility of doing better through automated instance selection.",https://openaccess.thecvf.com/content/WACV2024/html/Bohdal_Feed-Forward_Latent_Domain_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bohdal_Feed-Forward_Latent_Domain_Adaptation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484008/,"['Adaptation models', 'Computer vision', 'Upper bound', 'Data models', 'Real-time systems']","['Domain Adaptation', 'Latent Domain', 'Benchmark', 'Network Of Relationships', 'Average Performance', 'Projection Matrix', 'Number Of Domains', 'Target Domain', 'SET Domain', 'Relevant Examples', 'Residual Connection', 'Attention Weights', 'Test Task', 'Support Set', 'Test Instances', 'Query Image', 'Query Examples', 'Unlabeled Examples']","['Applications', 'Smartphones / end user devices', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"We study a new highly-practical problem setting that enables resource-constrained edge devices to adapt a pre-trained model to their local data distributions. Recognizing that device’s data are likely to come from multiple latent domains that include a mixture of unlabelled domain-relevant and domain-irrelevant examples, we focus on the comparatively under-studied problem of latent domain adaptation. Considering limitations of edge devices, we aim to only use a pre-trained model and adapt it in a feed-forward way, without using back-propagation and without access to the source data. Modelling these realistic constraints bring us to the novel and practically important problem setting of feed-forward latent domain adaptation. Our solution is to meta-learn a network capable of embedding the mixed-relevance target dataset and dynamically adapting inference for target examples using cross-attention. The resulting framework leads to consistent improvements over strong ERM baselines. We also show that our framework sometimes even improves on the upper bound of domain-supervised adaptation, where only domain-relevant instances are provided for adaptation. This suggests that human annotated domain labels may not always be optimal, and raises the possibility of doing better through automated instance selection."
Few-Shot Event Classification in Images Using Knowledge Graphs for Prompting,"Golsa Tahmasebzadeh, Matthias Springstein, Ralph Ewerth, Eric Müller-Budack","L3S Research Center, Leibniz University Hannover, Hannover, Germany; TIB – Leibniz Information Centre for Science and Technology, Hannover, Germany",50.0,Germany,50.0,Germany,"Event classification in images plays a vital role in multimedia analysis especially with the prevalence of fake news on social media and the Web. The majority of approaches for event classification rely on large sets of labeled training data. However, image labels for fine-grained event instances (e.g., 2016 Summer Olympics) can be sparse, incorrect, ambiguous, etc. A few approaches have addressed the lack of labeled data for event classification but cover only few events. Moreover, vision-language models that allow for zero-shot and few-shot classification with prompting have not yet been extensively exploited. In this paper, we propose four different techniques to create hard prompts including knowledge graph information from Wikidata and Wikipedia as well as an ensemble approach for zero-shot event classification. We also integrate prompt learning for state-of-the-art vision-language models to address few-shot event classification. Experimental results on six benchmarks including a new dataset comprising event instances from various domains, such as politics and natural disasters, show that our proposed approaches require much fewer training images than supervised baselines and the state-of-the-art while achieving better results.",https://openaccess.thecvf.com/content/WACV2024/html/Tahmasebzadeh_Few-Shot_Event_Classification_in_Images_Using_Knowledge_Graphs_for_Prompting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tahmasebzadeh_Few-Shot_Event_Classification_in_Images_Using_Knowledge_Graphs_for_Prompting_WACV_2024_paper.pdf,,https://github.com/TIBHannover/PromptImageEvent,,main,Poster,https://ieeexplore.ieee.org/document/10484460/,"['Training', 'Social networking (online)', 'Disasters', 'Training data', 'Knowledge graphs', 'Encyclopedias', 'Internet']","['Image Classification', 'Event Classification', 'Training Data', 'Natural Disasters', 'Classification Approach', 'Training Images', 'Few-shot Classification', 'News On Social Media', 'Support Vector Machine', 'Class Labels', 'Benchmark Datasets', 'Learning Classifiers', 'News Articles', 'Training Examples', 'Fewer Samples', 'Real-world Effectiveness', 'Linear Probe', 'Effects Of Labels', 'Body Of The Text', 'Vision Transformer', 'Image Encoder', 'Text Encoder', 'External Knowledge Sources', 'Few-shot Learning', 'Context Vector', 'Lack Of Training Data', 'Definition Of Events', 'Training Details']","['Applications', 'Arts / games / social media', 'Algorithms', 'Vision + language and/or other modalities']",,"Event classification in images plays a vital role in multimedia analysis especially with the prevalence of fake news on social media and the Web. The majority of approaches for event classification rely on large sets of labeled training data. However, image labels for fine-grained event instances (e.g., 2016 Summer Olympics) can be sparse, incorrect, ambiguous, etc. A few approaches have addressed the lack of labeled data for event classification but cover only few events. Moreover, vision-language models that allow for zero-shot and few-shot classification with prompting have not yet been extensively exploited. In this paper, we propose four different techniques to create hard prompts including knowledge graph information from Wikidata and Wikipedia as well as an ensemble approach for zero-shot event classification. We also integrate prompt learning for state-of-the-art vision-language models to address few-shot event classification. Experimental results on six benchmarks including a new dataset comprising event instances from various domains, such as politics and natural disasters, show that our proposed approaches require much fewer training images than supervised baselines and the state-of-the-art while achieving better results."
Few-Shot Generative Model for Skeleton-Based Human Action Synthesis Using Cross-Domain Adversarial Learning,"Kenichiro Fukushi, Yoshitaka Nozaki, Kosuke Nishihara, Kentaro Nakahara","Biometrics Research Laboratories, NEC Corporation, Japan",0.0,,100.0,Japan,"We propose few-shot generative models of skeleton-based human actions on limited samples of the target domain. We exploit large public datasets as a source of motion variations by introducing novel cross-domain and entropy regularization losses that effectively transfer the diversity of the motions contained in the source to the target domain. First, target samples are divided into patches, which are a set of short motion clips. For each patch, we search for a reference motion from the source dataset that is similar to the patch. Next, in adversarial training, our cross-domain regularization encourages the generated sequences to resemble the reference motion at the patch level. Entropy regularization prevents mode collapse by forcing the generator to follow the distribution of the source dataset. Experiments are performed on public datasets where we utilize three action classes from NTU RGB+D 120 as the target and all data of 60 action classes in NTU RGB+D as the source. Ten samples for each target action class, 30 in total, are selected as target data. The results demonstrate that data augmented with the proposed method improve recognition accuracy by 28 % using a ST-GCN classifier.",https://openaccess.thecvf.com/content/WACV2024/html/Fukushi_Few-Shot_Generative_Model_for_Skeleton-Based_Human_Action_Synthesis_Using_Cross-Domain_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fukushi_Few-Shot_Generative_Model_for_Skeleton-Based_Human_Action_Synthesis_Using_Cross-Domain_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Few-Shot Shape Recognition by Learning Deep Shape-Aware Features,"Wenlong Shi, Changsheng Lu, Ming Shao, Yinjie Zhang, Siyu Xia, Piotr Koniusz","School of Automation, Southeast University; University of Massachusetts, Dartmouth; The Australian National University",100.0,"Australia, China, USA",0.0,,"Traditional shape descriptors have been gradually replaced by convolutional neural networks due to their superior performance in feature extraction and classification. The state-of-the-art methods recognize object shapes via image reconstruction or pixel classification. However, these methods are biased toward texture information and overlook the essential shape descriptions, thus, they fail to generalize to unseen shapes. We are the first to propose a few-shot shape descriptor (FSSD) to recognize object shapes given only one or a few samples. We employ an embedding module for FSSD to extract transformation-invariant shape features. Secondly, we develop a dual attention mechanism to decompose and reconstruct the shape features via learnable shape primitives. In this way, any shape can be formed through a finite set basis, and the learned representation model is highly interpretable and extendable to unseen shapes. Thirdly, we propose a decoding module to include the supervision of shape masks and edges and align the original and reconstructed shape features, enforcing the learned features to be more shape-aware. Lastly, all the proposed modules are assembled into a few-shot shape recognition scheme. Experiments on five datasets show that our FSSD significantly improves the shape classification compared to the state-of-the-art under the few-shot setting.",https://openaccess.thecvf.com/content/WACV2024/html/Shi_Few-Shot_Shape_Recognition_by_Learning_Deep_Shape-Aware_Features_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shi_Few-Shot_Shape_Recognition_by_Learning_Deep_Shape-Aware_Features_WACV_2024_paper.pdf,,,2312.01315,main,Poster,https://ieeexplore.ieee.org/document/10484321/,"['Computer vision', 'Shape', 'Image edge detection', 'Computer architecture', 'Network architecture', 'Feature extraction', 'Robustness']","['Shape Recognition', 'Convolutional Network', 'Convolutional Neural Network', 'Image Reconstruction', 'Feature Learning', 'Attention Mechanism', 'Shape Features', 'Representation Learning', 'Object Shape', 'Shape Descriptors', 'Perform Feature Extraction', 'Shape Classification', 'Decoder Module', 'Traditional Descriptors', 'Dual Attention', 'Embedding Module', 'Loss Function', 'Training Data', 'Decoding', 'Autonomic System', 'Few-shot Learning', 'Salient Object Detection', 'Number Of Heads', 'Query Sample', 'Peak Signal-to-noise Ratio', 'Shape Representation', 'Decoding Method', 'Shape Context', 'Bounding Box', 'Geometric Primitives']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Low-level and physics-based vision']",1,"Traditional shape descriptors have been gradually replaced by convolutional neural networks due to their superior performance in feature extraction and classification. The state-of-the-art methods recognize object shapes via image reconstruction or pixel classification. However, these methods are biased toward texture information and overlook the essential shape descriptions, thus, they fail to generalize to unseen shapes. We are the first to propose a few-shot shape descriptor (FSSD) to recognize object shapes given only one or a few samples. We employ an embedding module for FSSD to extract transformation-invariant shape features. Secondly, we develop a dual attention mechanism to decompose and reconstruct the shape features via learnable shape primitives. In this way, any shape can be formed through a finite set basis, and the learned representation model is highly interpretable and extendable to unseen shapes. Thirdly, we propose a decoding module to include the supervision of shape masks and edges and align the original and reconstructed shape features, enforcing the learned features to be more shape-aware. Lastly, all the proposed modules are assembled into a few-shot shape recognition scheme. Experiments on five datasets show that our FSSD significantly improves the shape classification compared to the state-of-the-art under the few-shot setting."
FinderNet: A Data Augmentation Free Canonicalization Aided Loop Detection and Closure Technique for Point Clouds in 6-DOF Separation.,"Sudarshan S. Harithas, Gurkirat Singh, Aneesh Chavan, Sarthak Sharma, Suraj Patni, Chetan Arora, Madhava Krishna","Robotics Research Center, IIIT Hyderabad; IIT Delhi",100.0,India,0.0,,"We focus on the problem of LiDAR point cloud based loop detection (or Finding) and closure (LDC) for mobile robots. State-of-the-art (SOTA) methods directly generate learned embeddings from a given point cloud, require large data augmentation, and are not robust to wide viewpoint variations in 6 Degrees-of-Freedom (DOF). Moreover, the absence of strong priors in an unstructured point cloud leads to highly inaccurate LDC. In this original approach, we propose independent roll and pitch canonicalization of point clouds using a common dominant ground plane. We discretize the canonicalized point clouds along the axis perpendicular to the ground plane leads to images simi- lar to digital elevation maps (DEMs), which expose strong spatial priors in the scene. Our experiments show that LDC based on learnt embeddings from such DEMs is not only data efficient but also significantly more robust, and generalizable than the current SOTA. We report an (aver- age precision for loop detection, mean absolute transla- tion/rotation error) improvement of (8.4, 16.7/5.43)% on the KITTI08 sequence, and (11.0, 34.0/25.4)% on GPR10 sequence, over the current SOTA. To further test the ro- bustness of our technique on point clouds in 6-DOF motion we create and opensource a custom dataset called Lidar- UrbanFly Dataset (LUF) which consists of point clouds ob- tained from a LiDAR mounted on a quadrotor. More details on our website https://gsc2001.github.io/FinderNet/",https://openaccess.thecvf.com/content/WACV2024/html/Harithas_FinderNet_A_Data_Augmentation_Free_Canonicalization_Aided_Loop_Detection_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Harithas_FinderNet_A_Data_Augmentation_Free_Canonicalization_Aided_Loop_Detection_and_WACV_2024_paper.pdf,,https://gsc2001.github.io/FinderNet/,,main,Poster,,,,,,
Fine-Grained Alignment for Cross-Modal Recipe Retrieval,"Muntasir Wahed, Xiaona Zhou, Tianjiao Yu, Ismini Lourentzou",Virginia Tech,100.0,USA,0.0,,"Vision-language pre-trained models have exhibited significant advancements in various multimodal and unimodal tasks in recent years, including cross-modal recipe retrieval. However, a persistent challenge in multimodal frameworks is the lack of alignment between the encoders of different modalities. Although previous works addressed image and recipe embedding alignment, the alignment of individual recipe components has been overlooked. To address this gap, we present Fine-grained Alignment for Recipe Embeddings (FARM), a cross-modal retrieval approach that aligns the encodings of recipe components, including titles, ingredients, and instructions, within a shared representation space alongside corresponding image embeddings. Moreover, we introduce a hyperbolic loss function to effectively capture the similarity information inherent in recipe classes. FARM improves Recall@1 by 1.4% for image-to-recipe and 1.0 for recipe-to-image retrieval. Additionally, FARM achieves up to 6.1% and 15.1% performance improvement in image-to-recipe retrieval tasks, when just one and two components of the recipe are available, respectively. Comprehensive qualitative analysis of retrieved images for various recipes showcases the semantic capabilities of our trained models. Code is available at https://github.com/PLAN-Lab/FARM.",https://openaccess.thecvf.com/content/WACV2024/html/Wahed_Fine-Grained_Alignment_for_Cross-Modal_Recipe_Retrieval_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wahed_Fine-Grained_Alignment_for_Cross-Modal_Recipe_Retrieval_WACV_2024_paper.pdf,,https://github.com/PLAN-Lab/FARM,,main,Poster,https://ieeexplore.ieee.org/document/10484525/,"['Computer vision', 'Analytical models', 'Image coding', 'Codes', 'Semantics', 'Robustness', 'Task analysis']","['Cross-modal Retrieval', 'Fine-grained Alignment', 'Recipe Retrieval', 'Loss Function', 'Lack Of Alignment', 'Image Embedding', 'Alignment Of Components', 'Positive Samples', 'Hierarchical Structure', 'Percentage Points', 'Negative Samples', 'Class Labels', 'Semantic Information', 'Latent Space', 'Textual Descriptions', 'Final Loss', 'Negative Curvature', 'Linear Projection', 'List Of Ingredients', 'Hyperbolic Space', 'Triplet Loss', 'Visual Question Answering', 'Projection Layer', 'Image Encoder']","['Algorithms', 'Vision + language and/or other modalities']",2,"Vision-language pre-trained models have exhibited significant advancements in various multimodal and unimodal tasks in recent years, including cross-modal recipe retrieval. However, a persistent challenge in multimodal frameworks is the lack of alignment between the encoders of different modalities. Although previous works addressed image and recipe embedding alignment, the alignment of individual recipe components has been overlooked. To address this gap, we present Fine-grained Alignment for Recipe eMbeddings (FARM), a cross-modal retrieval approach that aligns the encodings of recipe components, including titles, ingredients, and instructions, within a shared representation space alongside corresponding image embeddings. Moreover, we introduce a hyperbolic loss function to effectively capture the similarity information inherent in recipe classes. FARM improves Recall@1 by 1.4% for image-to-recipe and 1.0% for recipe-to-image retrieval. Additionally, FARM achieves up to 6.1% and 15.1% performance improvement in image-to-recipe retrieval tasks, when just one and two components of the recipe are available, respectively. Comprehensive qualitative analysis of retrieved images for various recipes showcases the semantic capabilities of our trained models. Code is available at https://github.com/PLAN-Lab/FARM."
Fingervein Verification Using Convolutional Multi-Head Attention Network,"Raghavendra Ramachandra, Sushma Venkatesh","Norwegian University of Science and Technology (NTNU), Norway; AiBA AS, Norway",50.0,Norway,50.0,Norway,"Biometric verification systems are deployed in various security-based access-control applications that require user-friendly and reliable user verification. Among the different biometric characteristics, fingervein biometrics have been extensively studied owing to their reliable verification performance. Furthermore, fingervein patterns reside inside the skin and are not visible outside; therefore, they possess inherent resistance to presentation attacks and degradation due to external factors. In this study, we introduce a novel fingervein verification technique using a convolutional multihead attention network, VeinAtnNet. The proposed VeinAtnNet is designed to achieve light weight with a smaller number of learnable parameters while extracting discriminant information from both normal and enhanced fingervein images. The proposed VeinAtnNet was trained on the newly constructed fingervein dataset with 300 unique fingervein patterns that were captured in multiple sessions to obtain 92 samples per unique fingervein. Extensive experiments were performed on the newly collected dataset FV-300 and the publicly available FV-USM fingervein dataset. The performance of the proposed method was compared with five state-of-the-art fingervein verification systems, indicating the efficacy of the proposed VeinAtnNet.",https://openaccess.thecvf.com/content/WACV2024/html/Ramachandra_Fingervein_Verification_Using_Convolutional_Multi-Head_Attention_Network_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ramachandra_Fingervein_Verification_Using_Convolutional_Multi-Head_Attention_Network_WACV_2024_paper.pdf,,,2310.16808,main,Poster,https://ieeexplore.ieee.org/document/10483711/,"['Degradation', 'Computer vision', 'Head', 'Biometrics (access control)', 'Convolution', 'Fingers', 'Skin']","['Attention Mechanism', 'Atrous Convolution', 'Convolutional Attention Network', 'Finger-vein Verification', 'Performance Of Method', 'Extensive Experiments', 'Light Weight', 'Learnable Parameters', 'Multiple Sessions', 'Performance Verification', 'Biometric Systems', 'Biometric Characteristics', 'Reliability Verification', 'Deep Learning', 'Support Vector Machine', 'Superior Performance', 'Quantitative Results', 'Convolutional Layers', 'Unique Identification', 'Data Subject', 'Convolution Module', 'Equal Error Rate', 'Multi-head Self-attention', 'Consecutive Layers', 'Binary Code', 'Rotation Variations', 'Accuracy Verification', 'Template Matching', 'Deep Learning Techniques', 'Conv Layer']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Smartphones / end user devices']",1,"Biometric verification systems are deployed in various security-based access-control applications that require user-friendly and reliable person verification. Among the different biometric characteristics, fingervein biometrics have been extensively studied owing to their reliable verification performance. Furthermore, fingervein patterns reside inside the skin and are not visible outside; therefore, they possess inherent resistance to presentation attacks and degradation due to external factors. In this paper, we introduce a novel fingervein verification technique using a convolutional multihead attention network called VeinAtnNet. The proposed VeinAtnNet is designed to achieve light weight with a smaller number of learnable parameters while extracting discriminant information from both normal and enhanced fingervein images. The proposed VeinAtnNet was trained on the newly constructed fingervein dataset with 300 unique fingervein patterns that were captured in multiple sessions to obtain 92 samples per unique fingervein. Extensive experiments were performed on the newly collected dataset FV-300 and the publicly available FV-USM and FV-PolyU fingervein dataset. The performance of the proposed method was compared with five state-of-the-art fingervein verification systems, indicating the efficacy of the proposed VeinAtnNet."
FishTrack23: An Ensemble Underwater Dataset for Multi-Object Tracking,"Matthew Dawkins, Jack Prior, Bryon Lewis, Robin Faillettaz, Thompson Banez, Mary Salvi, Audrey Rollo, Julien Simon, Matthew Campbell, Matthew Lucero, Aashish Chaudhary, Benjamin Richards, Anthony Hoogs","NOAA NMFS, USA; California Department of Fish and Wildlife, USA; DECOD, Ifremer, INRAE, L’Institut Agro, Lorient, France; Kitware Inc., USA",75.0,"France, USA",25.0,USA,"Tracking and classifying fish in optical underwater imagery presents several challenges which are encountered less frequently in terrestrial domains. Video may contain large schools comprised of many individuals, dynamic natural backgrounds, highly variable target scales, volatile collection conditions, and non-fish moving confusers including debris, marine snow, and other organisms. Additionally, there is a lack of large public datasets for algorithm evaluation available in this domain. The contributions of this paper is three fold. First, we present the FishTrack23 dataset which provides a large quantity of expert-annotated fish groundtruth tracks, in imagery and video collected across a range of different backgrounds, locations, collection conditions, and organizations. Approximately 850k bounding boxes across 26k tracks are included in the release of the ensemble, with potential for future growth in later releases. Second, we evaluate improvements upon baseline object detectors, trackers and classifiers on the dataset. Lastly, we integrate these methods into web and desktop interfaces to expedite annotation generation on new datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Dawkins_FishTrack23_An_Ensemble_Underwater_Dataset_for_Multi-Object_Tracking_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dawkins_FishTrack23_An_Ensemble_Underwater_Dataset_for_Multi-Object_Tracking_WACV_2024_paper.pdf,https://viame.kitware.com/#/collections,,,main,Poster,https://ieeexplore.ieee.org/document/10483862/,"['Computer vision', 'Target tracking', 'Annotations', 'Snow', 'Heuristic algorithms', 'Organizations', 'Detectors']","['Multi-object Tracking', 'Imagery', 'Object Detection', 'Bounding Box', 'Marine Snow', 'Fish Species', 'Recurrent Neural Network', 'Frame Rate', 'Habitat Types', 'Ambient Light', 'Artificial Light', 'Tracking Algorithm', 'Object Tracking', 'Common Carp', 'Seabream', 'Juvenile Fish', 'Baseline Algorithms', 'Channel Catfish', 'Fish Habitat', 'Ictaluri', 'Autonomous Underwater Vehicles', 'GoPro Hero', 'Lepomis', 'Largemouth Bass', 'Bay Of Biscay', 'Grayscale', 'Lower Frame']","['Applications', 'Animals / Insects', 'Algorithms', 'Datasets and evaluations']",,"Tracking and classifying fish in optical underwater imagery presents several challenges which are encountered less frequently in terrestrial domains. Video may contain large schools comprised of many individuals, dynamic natural backgrounds, highly variable target scales, volatile collection conditions, and non-fish moving confusers including debris, marine snow, and other organisms. Additionally, there is a lack of large public datasets for algorithm evaluation available in this domain. The contributions of this paper is three fold. First, we present the FishTrack23 dataset which provides a large quantity of expert-annotated fish groundtruth tracks, in imagery and video collected across a range of different backgrounds, locations, collection conditions, and organizations. Approximately 850k bounding boxes across 26k tracks are included in the release of the ensemble, with potential for future growth in later releases. Second, we evaluate improvements upon baseline object detectors, trackers and classifiers on the dataset. Lastly, we integrate these methods into web and desktop interfaces to expedite annotation generation on new datasets."
Fixed Pattern Noise Removal for Multi-View Single-Sensor Infrared Camera,"Arnaud Barral, Pablo Arias, Axel Davy","Universit ´e Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, France; Universitat Pompeu Fabra, Dept. of Information and Communication Technologies, Spain",100.0,"France, Spain",0.0,,"Fixed pattern noise (FPN) is a temporally coherent noise present on videos due to the non-uniformities in the response of the imaging sensor. It is a common problem for infrared videos which degrades the quality of the observation and hinders subsequent applications. In this work we introduce a generalization of the FPN removal problem where the input data consists of several different sequences with the same FPN. This is motivated by infrared cameras that capture multiple views with a single sensor via a periodic motion pattern of a mirror or the camera itself, such as those used in surveillance. This multi-view setting allows for a much more accurate estimation of the FPN in comparison with the standard FPN removal problem from a single view. We propose a novel energy minimization approach for multi-view FPN removal, and two optimization algorithms that can be applied both in an off-line and on-line manner. In addition, we show that the proposed energy can be adapted to the problem of FPN removal from a single view with a rolling window approach, obtaining a significant improvement over the state of the art. We demonstrate the performance of the proposed method with synthetic data and real data from surveillance infrared cameras.",https://openaccess.thecvf.com/content/WACV2024/html/Barral_Fixed_Pattern_Noise_Removal_for_Multi-View_Single-Sensor_Infrared_Camera_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Barral_Fixed_Pattern_Noise_Removal_for_Multi-View_Single-Sensor_Infrared_Camera_WACV_2024_paper.pdf,,https://github.com/centreborelli/multiview-fpn,,main,Poster,https://ieeexplore.ieee.org/document/10484118/,"['AWGN', 'Surveillance', 'Noise', 'Estimation', 'Cameras', 'Minimization', 'Standards']","['Infrared Imaging', 'Fixed Pattern Noise', 'Optimization Algorithm', 'Energy Minimization', 'Single Sensor', 'Single View', 'Rolling Window', 'Convolutional Neural Network', 'Step Size', 'White Noise', 'Single Image', 'Adam Optimizer', 'Single Frame', 'Still Images', 'Online Methods', 'Maximum A Posteriori', 'Motion Estimation', 'Noisy Images', 'Realistic Case', 'Slow Variables', 'Single Video', 'Total Variation Regularization', 'Online Optimization', 'Noise Structure', 'Automatic Differentiation', 'Frame Index', 'High-pass Temporal Filtering', 'Image Statistics', 'Total Variance', 'Noise Model']","['Algorithms', 'Low-level and physics-based vision', 'Applications', 'Embedded sensing / real-time techniques']",2,"Fixed pattern noise (FPN) is a temporally coherent noise present on videos due to the non-uniformities in the response of the imaging sensor. It is a common problem for infrared videos which degrades the quality of the observation and hinders subsequent applications. In this work we introduce a generalization of the FPN removal problem where the input data consists of several different sequences with the same FPN. This is motivated by infrared cameras that capture multiple views with a single sensor via a periodic motion pattern of a mirror or the camera itself, such as those used in surveillance. This multi-view setting allows for a much more accurate estimation of the FPN in comparison with the standard FPN removal problem from a single view. We propose a novel energy minimization approach for multi-view FPN removal, and two optimization algorithms that can be applied both in an off-line and online manner. In addition, we show that the proposed energy can be adapted to the problem of FPN removal from a single view with a rolling window approach, obtaining a significant improvement over the state of the art. We demonstrate the performance of the proposed method with synthetic data and real data from surveillance infrared cameras."
Fixing Overconfidence in Dynamic Neural Networks,"Lassi Meronen, Martin Trapp, Andrea Pilzer, Le Yang, Arno Solin","Xi’an Jiaotong University; Aalto University, Saab Finland Oy; Aalto University; NVIDIA",75.0,"China, Finland",25.0,USA,"Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error.",https://openaccess.thecvf.com/content/WACV2024/html/Meronen_Fixing_Overconfidence_in_Dynamic_Neural_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Meronen_Fixing_Overconfidence_in_Dynamic_Neural_Networks_WACV_2024_paper.pdf,,,2302.06359,main,Poster,https://ieeexplore.ieee.org/document/10484487/,"['Deep learning', 'Adaptation models', 'Computer vision', 'Uncertainty', 'Computational modeling', 'Neural networks', 'Decision making']","['Neural Network', 'Dynamic Network', 'Dynamic Neural Network', 'Deep Learning', 'Computational Cost', 'Computational Efficiency', 'Deep Learning Models', 'Uncertainty Estimation', 'Model Size', 'ImageNet', 'Limited Budget', 'Uncertainty Quantification', 'Aleatory', 'Epistemic Uncertainty', 'Posterior Probability', 'Deep Neural Network', 'Standard Model', 'Classification Task', 'Image Classification', 'Efficient Implementation', 'Top-1 Accuracy', 'Laplace Approximation', 'Intermediate Class', 'Efficient Approximation', 'Top-5 Accuracy', 'Posterior Mode', 'Accurate Point', 'Maximum A Posteriori', 'Correct Label', 'Approximate Inference']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error."
FocusTune: Tuning Visual Localization Through Focus-Guided Sampling,"Son Tung Nguyen, Alejandro Fontan, Michael Milford, Tobias Fischer","Queensland University of Technology, Brisbane, Australia",100.0,Australia,0.0,,"We propose FocusTune, a focus-guided sampling technique to improve the performance of visual localization algorithms. FocusTune directs a scene coordinate regression model towards regions critical for 3D point triangulation by exploiting key geometric constraints. Specifically, rather than uniformly sampling points across the image for training the scene coordinate regression model, we instead re-project 3D scene coordinates onto the 2D image plane and sample within a local neighborhood of the re-projected points. While our proposed sampling strategy is generally applicable, we showcase FocusTune by integrating it with the recently introduced Accelerated Coordinate Encoding (ACE) model. Our results demonstrate that FocusTune both improves or matches state-of-the-art performance whilst keeping ACE's appealing low storage and compute requirements, for example reducing translation error from 25 to 19 and 17 to 15 cm for single and ensemble models, respectively, on the Cambridge Landmarks dataset. This combination of high performance and low compute and storage requirements is particularly promising for applications in areas like mobile robotics and augmented reality. We made our code available at https://github.com/sontung/focus-tune.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_FocusTune_Tuning_Visual_Localization_Through_Focus-Guided_Sampling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_FocusTune_Tuning_Visual_Localization_Through_Focus-Guided_Sampling_WACV_2024_paper.pdf,,https://github.com/sontung/focus-tune,2311.02872,main,Poster,https://ieeexplore.ieee.org/document/10483746/,"['Location awareness', 'Training', 'Visualization', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Computational modeling']","['Visual Localization', 'Triangulation', 'Computational Requirements', '3D Point', 'Geometric Constraints', 'Mobile Robot', 'Storage Requirements', 'Translation Error', 'Encoding Model', '2D Image Plane', 'Neural Network', 'Large Datasets', 'Training Time', 'Image Pixels', 'Multilayer Perceptron', 'Training Images', 'Local Method', '3D Point Cloud', 'Pixel Coordinates', 'Median Error', 'Camera Pose', 'Structure From Motion', 'Simultaneous Localization And Mapping', 'Sampling Radius', 'Query Image', 'Reprojection Error', 'Environment Map', '2D Pixel', 'Part Of The Scene', 'Variation Of Radius']","['Algorithms', '3D computer vision']",4,"We propose FocusTune, a focus-guided sampling technique to improve the performance of visual localization algorithms. FocusTune directs a scene coordinate regression model towards regions critical for 3D point triangulation by exploiting key geometric constraints. Specifically, rather than uniformly sampling points across the image for training the scene coordinate regression model, we instead re-project 3D scene coordinates onto the 2D image plane and sample within a local neighborhood of the reprojected points. While our proposed sampling strategy is generally applicable, we showcase FocusTune by integrating it with the recently introduced Accelerated Coordinate Encoding (ACE) model. Our results demonstrate that FocusTune both improves or matches state-of-the-art performance whilst keeping ACE’s appealing low storage and compute requirements, for example reducing translation error from 25 to 19 and 17 to 15 cm for single and ensemble models, respectively, on the Cambridge Landmarks dataset. This combination of high performance and low compute and storage requirements is particularly promising for applications in areas like mobile robotics and augmented reality. We made our code available at https://github.com/sontung/focus-tune."
Foundation Model Assisted Weakly Supervised Semantic Segmentation,"Xiaobo Yang, Xiaojin Gong","Zhejiang University, Hangzhou, China",100.0,China,0.0,,"This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014. Our code will be released upon acceptance.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Foundation_Model_Assisted_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.pdf,,,2312.03585,main,Poster,https://ieeexplore.ieee.org/document/10483591/,"['Training', 'Computer vision', 'Codes', 'Semantic segmentation', 'Task analysis', 'Image classification']","['Foundation Model', 'Weakly Supervised Semantic Segmentation', 'Classification Task', 'Image Classification', 'Segmentation Task', 'Two-stage Method', 'Image Classification Tasks', 'Contrastive Loss', 'Pseudo Labels', 'Class Activation Maps', 'Image-level Labels', 'Cross-entropy Loss', 'Class Labels', 'Activation Maps', 'Vision Tasks', 'Training Loss', 'Set Of Classes', 'Word Embedding', 'Unique Context', 'Self-supervised Learning', 'Multi-label Classification Task', 'Background Class', 'Validation Images', 'Final Segmentation', 'MS COCO Dataset', 'Final Segmentation Results', 'Voting Scheme', 'PASCAL VOC Dataset', 'Binary Cross-entropy Loss']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",7,"This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAMbased seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014. Our code will be released upon acceptance."
Framework-Agnostic Semantically-Aware Global Reasoning for Segmentation,"Mir Rayat Imtiaz Hossain, Leonid Sigal, James J. Little","University of British Columbia; University of British Columbia, Vector Institute for AI; University of British Columbia, Vector Institute for AI, Canada CIFAR AI Chair",100.0,Canada,0.0,,"Recent advances in pixel-level tasks (e.g. segmentation) illustrate the benefit of of long-range interactions between aggregated region-based representations that can enhance local features. However, such aggregated representations, often in the form of attention, fail to model the underlying semantics of the scene (e.g. individual objects and, by extension, their interactions). In this work, we address the issue by proposing a component that learns to project image features into latent representations and reason between them using a transformer encoder to generate contextualized and scene-consistent representations which are fused with original image features. Our design encourages the latent regions to represent semantic concepts by ensuring that the activated regions are spatially disjoint and the union of such regions corresponds to a connected object segment. The proposed semantic global reasoning (SGR) component is end-to-end trainable and can be easily added to a wide variety of backbones (CNN or transformer-based) and segmentation heads (per-pixel or mask classification) to consistently improve the segmentation results on different datasets. In addition, our latent tokens are semantically interpretable and diverse and provide a rich set of features that can be transferred to downstream tasks like object detection and segmentation, with improved performance. Furthermore, we also proposed metrics to quantify the semantics of latent tokens at both class & instance level.",https://openaccess.thecvf.com/content/WACV2024/html/Hossain_Framework-Agnostic_Semantically-Aware_Global_Reasoning_for_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hossain_Framework-Agnostic_Semantically-Aware_Global_Reasoning_for_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484526/,"['Measurement', 'Instance segmentation', 'Computer vision', 'Head', 'Semantics', 'Object detection', 'Transformers']","['Image Features', 'Object Detection', 'Individual Objects', 'Consistent Improvement', 'Object Segmentation', 'Latent Representation', 'Transformer Encoder', 'Union Regions', 'Features Of The Original Image', 'Learning Rate', 'Feature Maps', 'Global Context', 'Object Classification', 'Semantic Segmentation', 'Discrete Distribution', 'Segmentation Task', 'Feature Aggregation', 'Focal Loss', 'Number Of Concepts', 'Graph Convolution', 'Segmentation Framework', 'Object Instances', 'Individual Instances', 'Cost Matrix', 'MS COCO Dataset', 'Position Embedding', 'Similar Backbone', 'Object Detection Task', 'Intermediate Representation', 'ImageNet']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Recent advances in pixel-level tasks (e.g. segmentation) illustrate the benefit of of long-range interactions between aggregated region-based representations that can enhance local features. However, such aggregated representations, often in the form of attention, fail to model the underlying semantics of the scene (e.g. individual objects and, by extension, their interactions). In this work, we address the issue by proposing a component that learns to project image features into latent representations and reason between them using a transformer encoder to generate contextualized and scene-consistent representations which are fused with original image features. Our design encourages the latent regions to represent semantic concepts by ensuring that the activated regions are spatially disjoint and the union of such regions corresponds to a connected object segment. The proposed semantic global reasoning (SGR) component is end-to-end trainable and can be easily added to a wide variety of backbones (CNN or transformer-based) and segmentation heads (per-pixel or mask classification) to consistently improve the segmentation results on different datasets. In addition, our latent tokens are semantically interpretable and diverse and provide a rich set of features that can be transferred to downstream tasks like object detection and segmentation, with improved performance. Furthermore, we also proposed metrics to quantify the semantics of latent tokens at both class & instance level."
FreMIM: Fourier Transform Meets Masked Image Modeling for Medical Image Segmentation,"Wenxuan Wang, Jing Wang, Chen Chen, Jianbo Jiao, Yuanxiu Cai, Shanshan Song, Jiangyun Li","School of Automation and Electrical Engineering, University of Science and Technology Beijing; Center for Research in Computer Vision, University of Central Florida; School of Computer Science, University of Birmingham",100.0,"China, UK, USA",0.0,,"The research community has witnessed the powerful potential of self-supervised Masked Image Modeling (MIM), which enables the models capable of learning visual representation from unlabeled data. In this paper, to incorporate both the crucial global structural information and local details for dense prediction tasks, we alter the perspective to the frequency domain and present a new MIM-based framework named FreMIM for self-supervised pre-training to better accomplish medical image segmentation tasks. Based on the observations that the detailed structural information mainly lies in the high-frequency components and the high-level semantics are abundant in the low-frequency counterparts, we further incorporate multi-stage supervision to guide the representation learning during the pre-training phase. Extensive experiments on three benchmark datasets show the superior advantage of our FreMIM over previous state-of-the-art MIM methods. Compared with various baselines trained from scratch, our FreMIM could consistently bring considerable improvements to model performance. The code will be made publicly available at https://github.com/jingw193/FreMIM.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_FreMIM_Fourier_Transform_Meets_Masked_Image_Modeling_for_Medical_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_FreMIM_Fourier_Transform_Meets_Masked_Image_Modeling_for_Medical_Image_WACV_2024_paper.pdf,,https://github.com/jingw193/FreMIM,2304.10864,main,Poster,https://ieeexplore.ieee.org/document/10484231/,"['Representation learning', 'Image segmentation', 'Visualization', 'Frequency-domain analysis', 'Semantics', 'Benchmark testing', 'Data models']","['Fourier Transform', 'Medical Imaging', 'Masked Images', 'Medical Image Segmentation', 'Model Performance', 'Structural Information', 'Frequency Domain', 'Extensive Experiments', 'Benchmark Datasets', 'Global Information', 'Representation Learning', 'Considerable Improvement', 'Local Details', 'Detailed Structural Information', 'Medical Tasks', 'High-level Semantics', 'Global Structural Information', 'Learning Models', 'Hierarchical Structure', 'Local Features', 'Self-supervised Learning', 'Masking Strategy', 'Fourier Spectrum', 'Foreground Pixels', 'Discrete Fourier Transform', 'Vision Transformer', 'Raw Pixel', 'Dice Score', 'Feature Representation', 'Reconstruction Task']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",10,"The research community has witnessed the powerful potential of self-supervised Masked Image Modeling (MIM), which enables the models capable of learning visual representation from unlabeled data. In this paper, to incorporate both the crucial global structural information and local details for dense prediction tasks, we alter the perspective to the frequency domain and present a new MIM-based framework named FreMIM for self-supervised pre-training to better accomplish medical image segmentation tasks. Based on the observations that the detailed structural information mainly lies in the high-frequency components and the high-level semantics are abundant in the low-frequency counterparts, we further incorporate multi-stage supervision to guide the representation learning during the pre-training phase. Extensive experiments on three benchmark datasets show the superior advantage of our FreMIM over previous state-of-the-art MIM methods. Compared with various baselines trained from scratch, our FreMIM could consistently bring considerable improvements to model performance. The code will be publicly available at https://github.com/jingw193/FreMIM."
Frequency Attention for Knowledge Distillation,"Cuong Pham, Van-Anh Nguyen, Trung Le, Dinh Phung, Gustavo Carneiro, Thanh-Toan Do","Department of Data Science and AI, Monash University, Australia; Centre for Vision, Speech and Signal Processing, University of Surrey, United Kingdom",100.0,"Australia, UK",0.0,,"Knowledge distillation is an attractive approach for learning compact deep neural networks, which learns a lightweight student model by distilling knowledge from a complex teacher model. Attention-based knowledge distillation is a specific form of intermediate feature-based knowledge distillation that uses attention mechanisms to encourage the student to better mimic the teacher. However, most of the previous attention-based distillation approaches perform attention in the spatial domain, which primarily affects local regions in the input image. This may not be sufficient when we need to capture the broader context or global information necessary for effective knowledge transfer. In frequency domain, since each frequency is determined from all pixels of the image in spatial domain, it can contain global information about the image. Inspired by the benefits of the frequency domain, we propose a novel module that functions as an attention mechanism in the frequency domain. The module consists of a learnable global filter that can adjust the frequencies of student's features under the guidance of the teacher's features, which encourages the student's features to have patterns similar to the teacher's features. We then propose an enhanced knowledge review-based distillation model by leveraging the proposed frequency attention module. The extensive experiments with various teacher and student architectures on image classification and object detection benchmark datasets show that the proposed approach outperforms other knowledge distillation methods.",https://openaccess.thecvf.com/content/WACV2024/html/Pham_Frequency_Attention_for_Knowledge_Distillation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pham_Frequency_Attention_for_Knowledge_Distillation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484193/,"['Knowledge engineering', 'Computer vision', 'Frequency-domain analysis', 'Computational modeling', 'Object detection', 'Computer architecture', 'Benchmark testing']","['Deep Neural Network', 'Frequency Domain', 'Image Classification', 'Object Detection', 'Attention Mechanism', 'Teacher Model', 'Global Information', 'Spatial Domain', 'Student Model', 'Teacher Guidance', 'Image Object Detection', 'Distillation Method', 'Convolutional Neural Network', 'Convolutional Layers', 'Classification Task', 'Feature Maps', 'Fast Fourier Transform', 'ImageNet', 'High-pass Filter', 'Geometric Structure', 'Top-1 Accuracy', 'CIFAR-100 Dataset', 'Form Of Attention', 'Attention Map', 'MS COCO Dataset', 'Frequency Domain Features', 'Fusion Function', 'Spatial Attention Map', 'Distillation Loss', 'COCO Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Embedded sensing / real-time techniques']",6,"Knowledge distillation is an attractive approach for learning compact deep neural networks, which learns a lightweight student model by distilling knowledge from a complex teacher model. Attention-based knowledge distillation is a specific form of intermediate feature-based knowledge distillation that uses attention mechanisms to encourage the student to better mimic the teacher. However, most of the previous attention-based distillation approaches perform attention in the spatial domain, which primarily affects local regions in the input image. This may not be sufficient when we need to capture the broader context or global information necessary for effective knowledge transfer. In frequency domain, since each frequency is determined from all pixels of the image in spatial domain, it can contain global information about the image. Inspired by the benefits of the frequency domain, we propose a novel module that functions as an attention mechanism in the frequency domain. The module consists of a learnable global filter that can adjust the frequencies of student’s features under the guidance of the teacher’s features, which encourages the student’s features to have patterns similar to the teacher’s features. We then propose an enhanced knowledge review-based distillation model by leveraging the proposed frequency attention module. The extensive experiments with various teacher and student architectures on image classification and object detection benchmark datasets show that the proposed approach outperforms other knowledge distillation methods."
From Chaos to Calibration: A Geometric Mutual Information Approach To Target-Free Camera LiDAR Extrinsic Calibration,"Jack Borer, Jeremy Tschirner, Florian Ölsner, Stefan Milz",Spleenlab GmbH,0.0,,100.0,Germany,"Sensor fusion is vital for the safe and robust operation of autonomous vehicles. Accurate extrinsic sensor to sensor calibration is necessary to accurately fuse multiple sensor's data in a common spatial reference frame. In this paper, we propose a target free extrinsic calibration algorithm that requires no ground truth training data, artificially constrained motion trajectories, hand engineered features or offline optimization and that is accurate, precise and extremely robust to initialization error. Most current research on online camera-LiDAR extrinsic calibration requires ground truth training data which is impossible to capture at scale. We revisit analytical mutual information based methods first proposed in 2012 and demonstrate that geometric features provide a robust information metric for camera-LiDAR extrinsic calibration. We demonstrate our proposed improvement using the KITTI and KITTI-360 fisheye data set.",https://openaccess.thecvf.com/content/WACV2024/html/Borer_From_Chaos_to_Calibration_A_Geometric_Mutual_Information_Approach_To_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Borer_From_Chaos_to_Calibration_A_Geometric_Mutual_Information_Approach_To_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484411/,"['Laser radar', 'Soft sensors', 'Robot vision systems', 'Training data', 'Sensor fusion', 'Cameras', 'Calibration']","['Mutual Information', 'Geometric Information', 'Extrinsic Calibration', 'Geometric Features', 'Autonomous Vehicles', 'Degrees Of Freedom', 'Image Intensity', 'Camera Images', 'Error Range', '3D Point', 'Depth Camera', 'Coordinate Frame', 'Pixel Coordinates', 'Statistical Dependence', 'Fiducial Markers', 'Lidar Data', 'Rotation Parameters', 'Camera Model', 'Depth Features', 'Calibration Values', 'Monocular Depth Estimation', 'Virtual Sensors', 'LiDAR Point Clouds', 'Pinhole Camera Model', 'Calibration Information', 'Constrained Optimization', 'Robotic System', 'Image Frames', 'Joint Distribution']","['Applications', 'Robotics', 'Applications', 'Autonomous Driving', 'Applications', 'Remote Sensing']",2,"Sensor fusion is vital for the safe and robust operation of autonomous vehicles. Accurate extrinsic sensor to sensor calibration is necessary to accurately fuse multiple sensor’s data in a common spatial reference frame. In this paper, we propose a target free extrinsic calibration algorithm that requires no ground truth training data, artificially constrained motion trajectories, hand engineered features or offline optimization and that is accurate, precise and extremely robust to initialization error.Most current research on online camera-LiDAR extrinsic calibration requires ground truth training data which is impossible to capture at scale. We revisit analytical mutual information based methods first proposed in 2012 and demonstrate that geometric features provide a robust information metric for camera-LiDAR extrinsic calibration. We demonstrate our proposed improvement using the KITTI and KITTI-360 fisheye data set."
From Denoising Training To Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation,"Ruxue Wen, Hangjie Yuan, Dong Ni, Wenbo Xiao, Yaoyao Wu","Zhejiang University, Hangzhou, Zhejiang, China",100.0,China,0.0,,"In medical image segmentation, domain generalization poses a significant challenge due to domain shifts caused by variations in data acquisition devices and other factors. These shifts are particularly pronounced in the most common scenario, which involves only single-source domain data due to privacy concerns. To address this, we draw inspiration from the self-supervised learning paradigm that effectively discourages overfitting to the source domain. We propose the Denoising Y-Net (DeY-Net), a novel approach incorporating an auxiliary denoising decoder into the basic U-Net architecture. The auxiliary decoder aims to perform denoising training, augmenting the domain-invariant representation that facilitates domain generalization. Furthermore, this paradigm provides the potential to utilize unlabeled data. Building upon denoising training, we propose Denoising Test Time Adaptation (DeTTA) that further: (i) adapts the model to the target domain in a sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive experiments conducted on widely-adopted liver segmentation benchmarks demonstrate significant domain generalization improvements over our baseline and state-of-the-art results compared to other methods. Code is available at https://github.com/WenRuxue/DeTTA.",https://openaccess.thecvf.com/content/WACV2024/html/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wen_From_Denoising_Training_To_Test-Time_Adaptation_Enhancing_Domain_Generalization_for_WACV_2024_paper.pdf,,https://github.com/WenRuxue/DeTTA,2310.20271,main,Poster,https://ieeexplore.ieee.org/document/10484180/,"['Training', 'Image segmentation', 'Adaptation models', 'Noise reduction', 'Liver', 'Self-supervised learning', 'Data models']","['Medical Imaging', 'Image Segmentation', 'Domain Generalization', 'Medical Image Segmentation', 'Test-time Adaptation', 'Domain Shift', 'Target Domain', 'Unlabeled Data', 'Source Domain', 'Self-supervised Learning', 'Training Set', 'Test Data', 'Deep Neural Network', 'Healthy Patients', 'Data Augmentation', 'Autoencoder', 'Single Domain', 'Generalization Performance', 'Unseen Domains', 'Segmentation Task', 'Test Volume', 'Segmentation Results', 'Self-supervised Task', 'Improve Generalization Performance', 'Challenging Dataset', 'Neighboring Pixels', 'Auxiliary Task', 'Domain Adaptation']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"In medical image segmentation, domain generalization poses a significant challenge due to domain shifts caused by variations in data acquisition devices and other factors. These shifts are particularly pronounced in the most common scenario, which involves only single-source domain data due to privacy concerns. To address this, we draw inspiration from the self-supervised learning paradigm that effectively discourages overfitting to the source domain. We propose the Denoising Y-Net (DeY-Net), a novel approach incorporating an auxiliary denoising decoder into the basic U-Net architecture. The auxiliary decoder aims to perform denoising training, augmenting the domain-invariant representation that facilitates domain generalization. Furthermore, this paradigm provides the potential to utilize unlabeled data. Building upon denoising training, we propose Denoising Test Time Adaptation (DeTTA) that further: (i) adapts the model to the target domain in a sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive experiments conducted on widely-adopted liver segmentation benchmarks demonstrate significant domain generalization improvements over our baseline and state-of-the-art results compared to other methods. Code is available at https://github.com/WenRuxue/DeTTA."
Fully-Automatic Reflection Removal for 360-Degree Images,"Jonghyuk Park, Hyeona Kim, Eunpil Park, Jae-Young Sim","Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea",100.0,South Korea,0.0,,"Reflection removal (RR) is a technique to reconstruct the transmitted scene behind the glass from a mixed image taken through glass. In 360-degree images, the mixed image region and the reference image region capturing the reflected scene exist together, and the mixed image is often restored by using the information of reference image. In this paper, we first propose a fully-automatic end-to-end RR framework for 360-degree images which automatically detects the mixed and reference image regions and removes the reflection artifacts in the mixed image by using the reference information simultaneously. We devise a transformer based U-Net architecture with horizontal windowing scheme to capture the long-range dependencies between the mixed and reference images via the self-attention mechanism and suppress the reflection artifacts by using the reference information. We also construct a training dataset of 360-degree images by synthesizing realistic reflection artifacts considering diverse geometric relation and photometric variation between the mixed and reference images. The experimental results show that the proposed method detects the mixed and reference image regions reliably without user-annotation and achieves better performance of RR compared with the state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Fully-Automatic_Reflection_Removal_for_360-Degree_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Fully-Automatic_Reflection_Removal_for_360-Degree_Images_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483708/,"['Training', 'Laplace equations', 'Training data', 'Glass', 'Transformers', 'Reflection', 'Windows']","['360-degree Images', 'Reflection Removal', 'Image Regions', 'Reference Image', 'Geometric Relationship', 'Self-attention Mechanism', 'Imaging Framework', 'Convolutional Neural Network', 'Single Image', 'Human-computer Interaction', 'Multiple Images', 'Image Pairs', 'Dot Product', 'Gaussian Blur', 'Reference Region', 'Linear Space', 'Ground Truth Image', 'Reflectance Images', 'Network Image', 'Transmission Images', 'Laplacian Pyramid', 'Low Dynamic Range', 'Transformer Block', 'Zero-shot', 'Gamma Correction']","['Algorithms', 'Low-level and physics-based vision']",2,"Reflection removal (RR) is a technique to reconstruct the transmitted scene behind the glass from a mixed image taken through glass. In 360-degree images, the mixed image region and the reference image region capturing the reflected scene exist together, and the mixed image is often restored by using the information of reference image. In this paper, we first propose a fully-automatic end-to-end RR framework for 360-degree images which automatically detects the mixed and reference image regions and removes the reflection artifacts in the mixed image by using the reference information simultaneously. We devise a transformer based U-Net architecture with horizontal windowing scheme to capture the long-range dependencies between the mixed and reference images via the self-attention mechanism and suppress the reflection artifacts by using the reference information. We also construct a training dataset of 360-degree images by synthesizing realistic reflection artifacts considering diverse geometric relation and photometric variation between the mixed and reference images. The experimental results show that the proposed method detects the mixed and reference image regions reliably without user-annotation and achieves better performance of RR compared with the state-of-the-art methods."
FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions,"Noam Rotstein, David Bensaïd, Shaked Brody, Roy Ganz, Ron Kimmel",Technion - Israel Institute of Technology,100.0,Israel,0.0,,"The advent of vision-language pre-training techniques enhanced substantial progress in the development of models for image captioning. However, these models frequently produce generic captions and may omit semantically important image details. This limitation can be traced back to the image-text datasets; while their captions typically offer a general description of image content, they frequently omit salient details. Considering the magnitude of these datasets, manual reannotation is impractical, emphasizing the need for an automated approach. To address this challenge, we leverage existing captions and explore augmenting them with visual details using ""frozen"" vision experts including an object detector, an attribute recognizer, and an Optical Character Recognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such vision experts with the original captions using a large language model (LLM), yielding comprehensive image descriptions. We automatically curate a training set of 12M image-enriched caption pairs. These pairs undergo extensive evaluation through both quantitative and qualitative analyses. Subsequently, this data is utilized to train a captioning generation BLIP-based model. This model outperforms current state-of-the-art approaches, producing more precise and detailed descriptions, demonstrating the effectiveness of the proposed data-centric approach. We release this large-scale dataset of enriched image-caption pairs for the community.",https://openaccess.thecvf.com/content/WACV2024/html/Rotstein_FuseCap_Leveraging_Large_Language_Models_for_Enriched_Fused_Image_Captions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rotstein_FuseCap_Leveraging_Large_Language_Models_for_Enriched_Fused_Image_Captions_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484490/,"['Training', 'Surveys', 'Visualization', 'Computer vision', 'Fuses', 'Optical character recognition', 'Training data']","['Language Model', 'Image Captioning', 'Large Language Models', 'Semantic', 'Object Detection', 'Large-scale Datasets', 'Image Descriptors', 'Human Studies', 'Training Data', 'Computer Vision', 'Bounding Box', 'Model Architecture', 'Source Model', 'Original Ones', 'Optical Character Recognition', 'Retrieval Performance', 'COCO Dataset', 'Pre-training Data']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",7,"The advent of vision-language pre-training techniques enhanced substantial progress in the development of models for image captioning. However, these models frequently produce generic captions and may omit semantically important image details. This limitation can be traced back to the image-text datasets; while their captions typically offer a general description of image content, they frequently omit salient details. Considering the magnitude of these datasets, manual reannotation is impractical, emphasizing the need for an automated approach. To address this challenge, we leverage existing captions and explore augmenting them with visual details using ""frozen"" vision experts including an object detector, an attribute recognizer, and an Optical Character Recognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such vision experts with the original captions using a large language model (LLM), yielding comprehensive image descriptions. We automatically curate a training set of 12M image-enriched caption pairs. These pairs undergo extensive evaluation through both quantitative and qualitative analyses. Subsequently, this data is utilized to train a captioning generation BLIP-based model. This model outperforms current state-of-the-art approaches, producing more precise and detailed descriptions, demonstrating the effectiveness of the proposed data-centric approach. We release this large-scale dataset of enriched image-caption pairs for the community."
G-CASCADE: Efficient Cascaded Graph Convolutional Decoding for 2D Medical Image Segmentation,"Md Mostafijur Rahman, Radu Marculescu",The University of Texas at Austin,100.0,USA,0.0,,"In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other hierarchical encoders for general-purpose semantic and medical image segmentation tasks. The implementation can be found at: https://github.com/SLDGroup/G-CASCADE.",https://openaccess.thecvf.com/content/WACV2024/html/Rahman_G-CASCADE_Efficient_Cascaded_Graph_Convolutional_Decoding_for_2D_Medical_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_G-CASCADE_Efficient_Cascaded_Graph_Convolutional_Decoding_for_2D_Medical_Image_WACV_2024_paper.pdf,,https://github.com/SLDGroup/G-CASCADE,,main,Poster,https://ieeexplore.ieee.org/document/10484506/,"['Image segmentation', 'Convolution', 'Semantic segmentation', 'Semantics', 'Transformers', 'Skin', 'Decoding']","['Graph Convolution', 'Medical Image Segmentation', '2D Medical Image Segmentation', 'Feature Maps', 'Semantic Segmentation', 'Fewer Parameters', 'Convolutional Block', 'Long-range Dependencies', 'Medical Tasks', 'Semantic Segmentation Task', 'Atrous Convolution', 'Transformer Encoder', 'Dice Score', 'Convolutional Neural Network', 'Attention Mechanism', 'Convolution Operation', 'Batch Normalization', 'Axial Slices', 'Spatial Attention', 'Multi-scale Features', 'Graph Convolutional Network', 'Vision Transformer', 'Spatial Attention Mechanism', 'Transformer-based Methods', 'Spatial Attention Module', 'State Of The Art Methods', 'Skip Connections', 'Convolutional Attention Module', 'Decoding Stage', 'Segmentation Map']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",10,"In this paper, we are the first to propose a new graph convolution-based decoder namely, Cascaded Graph Convolutional Attention Decoder (G-CASCADE), for 2D medical image segmentation. G-CASCADE progressively refines multi-stage feature maps generated by hierarchical transformer encoders with an efficient graph convolution block. The encoder utilizes the self-attention mechanism to capture long-range dependencies, while the decoder refines the feature maps preserving long-range information due to the global receptive fields of the graph convolution block. Rigorous evaluations of our decoder with multiple transformer encoders on five medical image segmentation tasks (i.e., Abdomen organs, Cardiac organs, Polyp lesions, Skin lesions, and Retinal vessels) show that our model outperforms other state-of-the-art (SOTA) methods. We also demonstrate that our decoder achieves better DICE scores than the SOTA CASCADE decoder with 80.8% fewer parameters and 82.3% fewer FLOPs. Our decoder can easily be used with other hierarchical encoders for general-purpose semantic and medical image segmentation tasks. The implementation can be found at: https://github.com/SLDGroup/G-CASCADE."
"GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo","Vibhas K. Vats, Sripad Joshi, David J. Crandall, Md. Alimoor Reza, Soon-heung Jung",Indiana University Bloomington; Drake University; ETRI,66.66666666666666,USA,33.33333333333334,South Korea,"Traditional multi-view stereo (MVS) methods rely heavily on photometric and geometric consistency constraints, but newer machine learning-based MVS methods check geometric consistency across multiple source views only as a post-processing step. In this paper, we present a novel approach that explicitly encourages geometric consistency of reference view depth maps across multiple source views at different scales during learning (see Fig. 1). We find that adding this geometric consistency loss significantly accelerates learning by explicitly penalizing geometrically inconsistent pixels, reducing the training iteration requirements to nearly half that of other MVS methods. Our extensive experiments show that our approach achieves a new state-of-the-art on the DTU and BlendedMVS datasets, and competitive results on the Tanks and Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt to enforce multi-view, multi-scale geometric consistency during learning.",https://openaccess.thecvf.com/content/WACV2024/html/Vats_GC-MVSNet_Multi-View_Multi-Scale_Geometrically-Consistent_Multi-View_Stereo_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Vats_GC-MVSNet_Multi-View_Multi-Scale_Geometrically-Consistent_Multi-View_Stereo_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483284/,"['Training', 'Computer vision', 'Pipelines', 'Benchmark testing', 'Cognition']","['Multi-view Stereo', 'Extensive Experiments', 'Learning-based Methods', 'Depth Map', 'Training Iterations', 'Geometric Constraints', 'Geometric Consistency', 'Model Performance', 'Hyperparameters', 'Supplemental Material', 'Cross-entropy Loss', 'Point Cloud', 'Batch Normalization', '3D Point', 'Feature Matching', 'Focal Loss', 'Depth Estimation', 'Number Of Hypotheses', 'Back Projection', 'Feature Pyramid Network', 'Cost Volume', 'Geometric Cues', 'Point Cloud Reconstruction', 'Stable Training', 'Deformable Layer', 'Number Of Planes', 'Ground Truth Depth', 'Element-wise Multiplication', 'Feature Maps', 'Camera Intrinsic Parameters']","['Algorithms', '3D computer vision']",4,"Traditional multi-view stereo (MVS) methods rely heavily on photometric and geometric consistency constraints, but newer machine learning-based MVS methods check geometric consistency across multiple source views only as a post-processing step. In this paper, we present a novel approach that explicitly encourages geometric consistency of reference view depth maps across multiple source views at different scales during learning (see Fig. 1). We find that adding this geometric consistency loss significantly accelerates learning by explicitly penalizing geometrically inconsistent pixels, reducing the training iteration requirements to nearly half that of other MVS methods. Our extensive experiments show that our approach achieves a new state-of-the-art on the DTU and BlendedMVS datasets, and competitive results on the Tanks and Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt to enforce multi-view, multi-scale geometric consistency during learning."
GC-VTON: Predicting Globally Consistent and Occlusion Aware Local Flows With Neighborhood Integrity Preservation for Virtual Try-On,"Hamza Rawal, Muhammad Junaid Ahmad, Farooq Zaman","Motive; Information Technology University, Lahore, Pakistan",50.0,Pakistan,50.0,USA,"Flow based garment warping is an integral part of image-based virtual try-on networks. However, optimizing a single flow predicting network for simultaneous global boundary alignment and local texture preservation results in sub-optimal flow fields. Moreover, dense flows are inherently not suited to handle intricate conditions like garment occlusion by body parts or by other garments. Forcing flows to handle the above issues results in various distortions like texture squeezing, and stretching. In this work, we propose a novel approach where we disentangle the global boundary alignment and local texture preserving tasks via our GlobalNet and LocalNet modules. A consistency loss is then employed between the two modules which harmonizes the local flows with the global boundary alignment. Additionally, we explicitly handle occlusions by predicting body-parts visibility mask, which is used to mask out the occluded regions in the warped garment. The masking prevents the LocalNet from predicting flows that distort texture to compensate for occlusions. We also introduce a novel regularization loss (NIPR), that defines a criteria to identify the regions in the warped garment where texture integrity is violated (squeezed or stretched). NIPR subsequently penalizes the flow in those regions to ensure regular and coherent warps that preserve the texture in local neighborhoods. Evaluation on a widely used virtual try-on dataset demonstrates strong performance of our network compared to the current SOTA methods.",https://openaccess.thecvf.com/content/WACV2024/html/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rawal_GC-VTON_Predicting_Globally_Consistent_and_Occlusion_Aware_Local_Flows_With_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483607/,"['Computer vision', 'Clothing', 'Distortion', 'Task analysis']","['Local Flow', 'Global Consistency', 'Virtual Try-on', 'Distortion', 'Body Parts', 'Flow Field', 'Local Neighborhood', 'Global Alignment', 'Consistency Loss', 'Regularization Loss', 'Local Texture', 'Occluded Regions', 'Local Preservation', 'Global Boundary', 'Visible Light', 'Global Change', 'Neighboring Pixels', 'Global Flows', 'Generation Module', 'Fréchet Inception Distance', 'L1 Loss', 'Target Person', 'Neighboring Pairs']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"Flow based garment warping is an integral part of image-based virtual try-on networks. However, optimizing a single flow predicting network for simultaneous global boundary alignment and local texture preservation results in sub-optimal flow fields. Moreover, dense flows are inherently not suited to handle intricate conditions like garment occlusion by body parts or by other garments. Forcing flows to handle the above issues results in various distortions like texture squeezing, and stretching. In this work, we propose a novel approach where we disentangle the global boundary alignment and local texture preserving tasks via our GlobalNet and LocalNet modules. A consistency loss is then employed between the two modules which harmonizes the local flows with the global boundary alignment. Additionally, we explicitly handle occlusions by predicting body-parts visibility mask, which is used to mask out the occluded regions in the warped garment. The masking prevents the LocalNet from predicting flows that distort texture to compensate for occlusions. We also introduce a novel regularization loss (NIPR), that defines a criteria to identify the regions in the warped garment where texture integrity is violated (squeezed or stretched). NIPR subsequently penalizes the flow in those regions to ensure regular and coherent warps that preserve the texture in local neighborhoods. Evaluation on a widely used virtual try-on dataset demonstrates strong performance of our network compared to the current SOTA methods."
GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot Learning,"Guangyue Xu, Joyce Chai, Parisa Kordjamshidi",Michigan State University; University of Michigan,100.0,USA,0.0,,"Pre-trained vision-language models (VLMs) have achieved promising success in many fields, specially with prompt learning paradigm. However, designing proper textual prompts to adapt VLMs for downstream tasks is still challenging. In this work, we propose GIPCOL (Graph-Injected soft Prompting for COmpositional Learning) to better explore the compositional zero-shot learning (CZSL) ability of VLMs within the prompt-based learning framework. The soft prompt in GIPCOL is structured and consists of the prefix learnable vectors, attribute label and object label. In addition, the attribute and object labels in the soft prompt are designated as nodes in a compositional graph. The compositional graph is constructed based on the compositional structure of the objects and attributes extracted from the training data and consequently feeds the updated concept representation into the soft prompt to capture this compositional structure for a better CZSL learning. With the new prompting strategy, GIPCOL achieves state-of-the-art AUC results on all three CZSL benchmarks, including MIT-States, UT-Zappos, and C-GQA datasets in both closed and open settings compared to previous non-CLIP as well as CLIP-based methods. We analyze when and why GIPCOL operates well given the CLIP backbone and its training data limitations, and our findings shed light on designing prompts for CZSL.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_GIPCOL_Graph-Injected_Soft_Prompting_for_Compositional_Zero-Shot_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_GIPCOL_Graph-Injected_Soft_Prompting_for_Compositional_Zero-Shot_Learning_WACV_2024_paper.pdf,,,2311.05729,main,Poster,https://ieeexplore.ieee.org/document/10484069/,"['Training', 'Computer vision', 'Image recognition', 'Zero-shot learning', 'Computational modeling', 'Training data', 'Benchmark testing']","['Zero-shot', 'Training Data', 'Composite Structure', 'Learning Ability', 'Nodes In The Graph', 'Open Set', 'Conceptual Representations', 'Closed And Open', 'Object Labels', 'Training Time', 'Learnable Parameters', 'Language Model', 'Graph Convolutional Network', 'Graph Neural Networks', 'Embedding Vectors', 'Autonomous Agents', 'Composition Information', 'Relevant Images', 'Image X', 'Representation Of Composition', 'Open World', 'Visual Encoding', 'Graph Neural Network Model', 'Composite Vector', 'Node Embeddings', 'Pre-training Data', 'Pre-trained Language Models']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Pre-trained vision-language models (VLMs) have achieved promising success in many fields, especially with prompt learning paradigm. In this work, we propose GIPCOL (Graph-Injected Soft Prompting for Compositional Learning) to better explore the compositional zero-shot learning (CZSL) ability of VLMs within the prompt-based learning framework. The soft prompt in GIPCOL is structured and consists of the prefix learnable vectors, attribute label and object label. In addition, the attribute and object labels in the soft prompt are designated as nodes in a compositional graph. The compositional graph is constructed based on the compositional structure of the objects and attributes extracted from the training data and consequently feeds the updated concept representation into the soft prompt to capture this compositional structure for a better prompting for CZSL. With the new prompting strategy, GIPCOL achieves state-of-the-art AUC results on all three CZSL benchmarks, including MIT-States, UT-Zappos, and C-GQA datasets in both closed and open settings compared to previous non-CLIP as well as CLIP-based methods. We analyze when and why GIPCOL operates well given the CLIP backbone and its training data limitations, and our findings shed light on designing more effective prompts for CZSL."
GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation With Large Domain Gap,"Hyogun Lee, Kyungho Bae, Seong Jong Ha, Yumin Ko, Gyeong-Moon Park, Jinwoo Choi","AI Center, CJ Corporation; Kyung Hee University; NCSOFT",33.33333333333333,South Korea,66.66666666666667,South Korea,"In this work, we tackle the challenging problem of unsupervised video domain adaptation (UVDA) for action recognition. We specifically focus on scenarios with a substantial domain gap, in contrast to existing works primarily deal with small domain gaps between labeled source domains and unlabeled target domains. To establish a more realistic setting, we introduce a novel UVDA scenario, denoted as Kinetics->BABEL, with a more considerable domain gap in terms of both temporal dynamics and background shifts. To tackle the temporal shift, i.e., action duration difference between the source and target domains, we propose a global-local view alignment approach. To mitigate the background shift, we propose to learn temporal order sensitive representations by temporal order learning and background invariant representations by background augmentation. We empirically validate that the proposed method shows significant improvement over the existing methods on the Kinetics->BABEL dataset with a large domain gap.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_GLAD_Global-Local_View_Alignment_and_Background_Debiasing_for_Unsupervised_Video_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_GLAD_Global-Local_View_Alignment_and_Background_Debiasing_for_Unsupervised_Video_WACV_2024_paper.pdf,,https://github.com/KHU-VLL/GLAD,2311.12467,main,Poster,https://ieeexplore.ieee.org/document/10484218/,"['Computer vision', 'Codes', 'Target recognition', 'Focusing', 'Benchmark testing']","['Domain Adaptation', 'Domain Gap', 'Large Domain Gap', 'Duration Of Action', 'Real Sets', 'Temporal Order', 'Action Recognition', 'Target Domain', 'Source Domain', 'Temporal Shift', 'Temporal Learning', 'Unlabeled Target Domain', 'Labeled Source Domain', 'Data Sources', 'Convolutional Neural Network', 'Video Frames', 'Target Data', 'Linear Classifier', 'Low Standard Deviation', 'Domain Classifier', 'Local View', 'Earth Mover’s Distance', 'Temporal Gap', 'Video Action Recognition', 'Dynamic Datasets', 'Temporal Distance', 'Background Distribution', 'Source Video', 'Challenging Benchmark', 'Static Background']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In this work, we tackle the challenging problem of unsupervised video domain adaptation (UVDA) for action recognition. We specifically focus on scenarios with a substantial domain gap, in contrast to existing works primarily deal with small domain gaps between labeled source domains and unlabeled target domains. To establish a more realistic setting, we introduce a novel UVDA scenario, denoted as Kinetics→BABEL, with a more considerable domain gap in terms of both temporal dynamics and background shifts. To tackle the temporal shift, i.e., action duration difference between the source and target domains, we propose a global-local view alignment approach. To mitigate the background shift, we propose to learn temporal order sensitive representations by temporal order learning and background invariant representations by background augmentation. We empirically validate that the proposed method shows significant improvement over the existing methods on the Kinetics→BABEL dataset with a large domain gap. The code is available at https://github.com/KHU-VLL/GLAD."
GRIT: GAN Residuals for Paired Image-to-Image Translation,"Saksham Suri, Moustafa Meshry, Larry S. Davis, Abhinav Shrivastava","University of Maryland, College Park",100.0,USA,0.0,,"Current Image-to-Image translation (I2I) frameworks rely heavily on reconstruction losses, where the output needs to match a given ground truth image. An adversarial loss is commonly utilized as a secondary loss term, mainly to add more realism to the output. Compared to unconditional GANs, I2I translation frameworks have more supervisory signals, but still their output shows more artifacts and does not reach the same level of realism achieved by unconditional GANs. We study the performance gap, in terms of photo-realism, between I2I translation and unconditional GAN frameworks. Based on our observations, we propose a modified architecture and training objective to address this realism gap. Our proposal relaxes the role of reconstruction losses, to act as regularizers instead of doing all the heavy lifting which is common in current I2I frameworks. Furthermore, our proposed formulation decouples the optimization of reconstruction and adversarial objectives and removes pixel-wise constraints on the final output. This allows for a set of stochastic but realistic variations of any target output image.",https://openaccess.thecvf.com/content/WACV2024/html/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Suri_GRIT_GAN_Residuals_for_Paired_Image-to-Image_Translation_WACV_2024_paper.pdf,http://cs.umd.edu/~sakshams/grit,,,main,Poster,https://ieeexplore.ieee.org/document/10483587,"['Training', 'Image synthesis', 'Impedance matching', 'Image edge detection', 'Noise', 'Proposals', 'Spatial resolution']","['Generative Adversarial Networks', 'Final Output', 'Training Objective', 'Ground Truth Image', 'Reconstruction Loss', 'Heavy Lifting', 'Translation Framework', 'Local Variations', 'Latent Space', 'Pixel Level', 'Low-frequency Components', 'Image Synthesis', 'Semantic Labels', 'Semantic Map', 'Latent Code', 'Spatial Noise', 'Noise Map', 'StyleGAN', 'Generative Adversarial Networks Loss']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Current Image-to-Image translation (I2I) frameworks rely heavily on reconstruction losses, where the output needs to match a given ground truth image. An adversarial loss is commonly utilized as a secondary loss term, mainly to add more realism to the output. Compared to unconditional GANs, I2I translation frameworks have more supervisory signals, but still their output shows more artifacts and does not reach the same level of realism achieved by unconditional GANs. We study the performance gap, in terms of photo-realism, between I2I translation and unconditional GAN frameworks. Based on our observations, we propose a modified architecture and training objective to address this realism gap. Our proposal relaxes the role of reconstruction losses, to act as regularizers instead of doing all the heavy lifting which is common in current I2I frameworks. Furthermore, our proposed formulation decouples the optimization of reconstruction and adversarial objectives and removes pixel-wise constraints on the final output. This allows for a set of stochastic but realistic variations of any target output image. Our project page can be accessed at cs.umd.edu/~sakshams/grit."
GTP-ViT: Efficient Vision Transformers via Graph-Based Token Propagation,"Xuwei Xu, Sen Wang, Yudong Chen, Yanping Zheng, Zhewei Wei, Jiajun Liu","The University of Queensland, Australia; The University of Queensland, Australia; CSIRO Data61, Australia; Renmin University of China, China; CSIRO Data61, Australia; Renmin University of China, China",100.0,"Australia, China",0.0,,"Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computational complexity while preserving essential information of eliminated tokens. Combined with an innovative token selection strategy, GTP can efficiently identify image tokens to be propagated. Extensive experiments have validated GTP's effectiveness, demonstrating both efficiency and performance improvements. Specifically, GTP decreases the computational complexity of both DeiT-S and DeiT-B by up to 26% with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and remarkably surpasses the state-of-the-art token merging method on various backbones at an even faster inference speed. The source code is available in the supplementary material.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_GTP-ViT_Efficient_Vision_Transformers_via_Graph-Based_Token_Propagation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_GTP-ViT_Efficient_Vision_Transformers_via_Graph-Based_Token_Propagation_WACV_2024_paper.pdf,,https://github.com/Ackesnal/GTP-ViT,,main,Poster,https://ieeexplore.ieee.org/document/10484062/,"['Computer vision', 'Source coding', 'Computational modeling', 'Merging', 'Broadcasting', 'Transformers', 'Computational complexity']","['Vision Transformer', 'Fine-tuned', 'Computational Complexity', 'Computer Vision', 'Extensive Experiments', 'Accuracy Drop', 'Inference Speed', 'Feature Maps', 'Large Model', 'Raw Images', 'Feed-forward Network', 'Semantic Similarity', 'Graph Convolutional Network', 'Graph Neural Networks', 'Attention Map', 'Softmax Activation', 'Degree Matrix', 'Types Of Graphs', 'Adjacency Matrix Of Graph', 'Top-1 Accuracy', 'Semantic Graph', 'Sparse Graph', 'Multi-head Self-attention']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens’ information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computational complexity while preserving essential information of eliminated tokens. Combined with an innovative token selection strategy, GTP can efficiently identify image tokens to be propagated. Extensive experiments have validated GTP’s effectiveness, demonstrating both efficiency and performance improvements. Specifically, GTP decreases the computational complexity of both DeiT-S and DeiT-B by up to 26% with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and remarkably surpasses the state-of-the-art token merging method on various backbones at an even faster inference speed. The source code is available at https://github.com/Ackesnal/GTP-ViT."
GazeGNN: A Gaze-Guided Graph Neural Network for Chest X-Ray Classification,"Bin Wang, Hongyi Pan, Armstrong Aboah, Zheyuan Zhang, Elif Keles, Drew Torigian, Baris Turkbey, Elizabeth Krupinski, Jayaram Udupa, Ulas Bagci",University of Pennsylvania; Northwestern University; Emory University; National Cancer Institute,100.0,USA,0.0,,"Eye tracking research is important in computer vision because it can help us understand how humans interact with the visual world. Specifically for high-risk applications, such as in medical imaging, eye tracking can help us to comprehend how radiologists and other medical professionals search, analyze, and interpret images for diagnostic and clinical purposes. Hence, the application of eye tracking techniques in disease classification has become increasingly popular in recent years. Contemporary works usually transform gaze information collected by eye tracking devices into visual attention maps (VAMs) to supervise the learning process. However, this is a time-consuming preprocessing step, which stops us from applying eye tracking to radiologists' daily work. To solve this problem, we propose a novel gaze-guided graph neural network (GNN), GazeGNN, to leverage raw eye-gaze data without being converted into VAMs. In GazeGNN, to directly integrate eye gaze into image classification, we create a unified representation graph that models both images and gaze pattern information. With this benefit, we develop a real-time, real-world, end-to-end disease classification algorithm for the first time in the literature. This achievement demonstrates the practicality and feasibility of integrating real-time eye tracking techniques into the daily work of radiologists. To our best knowledge, GazeGNN is the first work that adopts GNN to integrate image and eye-gaze data. Our experiments on the public chest X-ray dataset show that our proposed method exhibits the best classification performance compared to existing methods. The code is available.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_GazeGNN_A_Gaze-Guided_Graph_Neural_Network_for_Chest_X-Ray_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_GazeGNN_A_Gaze-Guided_Graph_Neural_Network_for_Chest_X-Ray_Classification_WACV_2024_paper.pdf,,https://github.com/ukaukaaaa/GazeGNN,,main,Poster,https://ieeexplore.ieee.org/document/10484310/,"['Training', 'Visualization', 'Computer vision', 'Gaze tracking', 'Transforms', 'Graph neural networks', 'Real-time systems']","['Chest X-ray', 'Graph Neural Networks', 'Chest X-ray Classification', 'Medical Imaging', 'Classification Of Diseases', 'Graphical Representation', 'Image Classification', 'Daily Work', 'Eye-tracking', 'Attention Map', 'Time In The Literature', 'Real-time Technique', 'Eye-tracking Techniques', 'Deep Learning', 'Classification Model', 'Training Dataset', 'Congestive Heart Failure', 'K-nearest Neighbor', 'Regular Grid', 'Nodes In The Graph', 'Position Embedding', 'Chest X-ray Images', 'Graph Convolution', 'Fixation Time', 'Fully-connected Layer', 'Average AUC', 'Image Patches', 'Limited Training Data', 'Classification Head', 'Inference Time']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",11,"Eye tracking research is important in computer vision because it can help us understand how humans interact with the visual world. Specifically for high-risk applications, such as in medical imaging, eye tracking can help us to comprehend how radiologists and other medical professionals search, analyze, and interpret images for diagnostic and clinical purposes. Hence, the application of eye tracking techniques in disease classification has become increasingly popular in recent years. Contemporary works usually transform gaze information collected by eye tracking devices into visual attention maps (VAMs) to supervise the learning process. However, this is a time-consuming preprocessing step, which stops us from applying eye tracking to radiologists’ daily work. To solve this problem, we propose a novel gaze-guided graph neural network (GNN), GazeGNN, to leverage raw eye-gaze data without being converted into VAMs. In GazeGNN, to directly integrate eye gaze into image classification, we create a unified representation graph that models both images and gaze pattern information. With this benefit, we develop a real-time, real-world, end-to-end disease classification algorithm for the first time in the literature. This achievement demonstrates the practicality and feasibility of integrating real-time eye tracking techniques into the daily work of radiologists. To our best knowledge, GazeGNN is the first work that adopts GNN to integrate image and eye-gaze data. Our experiments on the public chest X-ray dataset show that our proposed method exhibits the best classification performance compared to existing methods. The code is available at https://github.com/ukaukaaaa/GazeGNN."
Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation,"Joshua Niemeijer, Manuel Schwonberg, Jan-Aike Termöhlen, Nico M. Schmidt, Tim Fingscheidt",Technische Universität Braunschweig; DLR; CARIAD SE,66.66666666666666,Germany,33.33333333333334,Germany,"When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly. Domain adaptation methods try to overcome this issue, but need samples from the target domain. However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data. We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion model to generate a pseudo-target domain with diverse text prompts. In contrast to existing methods, this allows to control the style and content of the generated images and to introduce a high diversity. In a second step, we train a generalizing model by adapting towards this pseudo-target domain. We outperform previous approaches by a large margin across various datasets and architectures without using any real data. For the generalization from GTA5, we improve state-of-the-art mIoU performance by 3.8% absolute on average and for SYNTHIA by 11.8% absolute, marking a big step for the generalization performance on these benchmarks. Code is available at https://github.com/JNiemeijer/DIDEX",https://openaccess.thecvf.com/content/WACV2024/html/Niemeijer_Generalization_by_Adaptation_Diffusion-Based_Domain_Extension_for_Domain-Generalized_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Niemeijer_Generalization_by_Adaptation_Diffusion-Based_Domain_Extension_for_Domain-Generalized_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/JNiemeijer/DIDEX,,main,Poster,https://ieeexplore.ieee.org/document/10484417/,"['Training', 'Adaptation models', 'Microwave integrated circuits', 'Computer vision', 'Codes', 'Semantic segmentation', 'Training data']","['Semantic Segmentation', 'Diffusion Model', 'Large Margin', 'Target Domain', 'Domain Adaptation', 'Domain Generalization', 'Domain Adaptation Methods', 'Contralateral', 'Highway', 'Real-world Data', 'Increase In Performance', 'Domain Shift', 'Image Generation', 'Synthetic Images', 'Source Domain', 'Self-supervised Learning', 'Source Dataset', 'Adaptive Step', 'Adverse Weather Conditions', 'Semantic Consistency', 'Unsupervised Domain Adaptation Methods', 'Style Transfer', 'Vision Transformer', 'High-quality Photographs']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",3,"When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly. Domain adaptation methods try to overcome this issue, but need samples from the target domain. However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data. We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion model to generate a pseudo-target domain with diverse text prompts. In contrast to existing methods, this allows to control the style and content of the generated images and to introduce a high diversity. In a second step, we train a generalizing model by adapting towards this pseudo-target domain. We outperform previous approaches by a large margin across various datasets and architectures without using any real data. For the generalization from GTA5, we improve state-of-the-art mIoU performance by 3.8 % absolute on average and for SYNTHIA by 11.8 % absolute, marking a big step for the generalization performance on these benchmarks. Code is available at https://github.com/JNiemeijer/DIDEX"
Generalizing to Unseen Domains in Diabetic Retinopathy Classification,"Chamuditha Jayanga Galappaththige, Gayal Kuruppu, Muhammad Haris Khan","Mohamed bin Zayed University of Artificial Intelligence, UAE.",100.0,UAE,0.0,,"Diabetic retinopathy (DR) is caused by long-standing diabetes and is among the fifth leading cause for visual impairment. The prospects of early diagnosis and treatment could be helpful in curing the disease, however, the detection procedure is rather challenging and mostly tedious. Therefore, automated diabetic retinopathy classification using deep learning techniques has gained interest in the medical imaging community. Akin to several other real-world applications of deep learning, the typical assumption of i.i.d data is also violated in DR classification that relies on deep learning. Therefore, developing DR classification methods robust to unseen distributions is of great value. In this paper, we study the problem of generalizing a model to unseen distributions or domains (a.k.a domain generalization) in DR classification. To this end, we propose a simple and effective domain generalization (DG) approach that achieves self-distillation in vision transformers (ViT) via a novel prediction softening mechanism. This prediction softening is an adaptive convex combination of one-hot labels with the model's own knowledge. We perform extensive experiments on challenging open-source DR classification datasets under both multi-source and more challenging single-source DG settings with three different ViT backbones to establish the efficacy and applicability of our approach against competing methods. For the first time, we report the performance of several state-of-the-art domain generalization (DG) methods on open-source DR classification datasets after conducting thorough experiments. Finally, our method is also capable of delivering improved calibration performance than other methods, showing its suitability for safety-critical applications, including healthcare. We hope that our contributions would instigate more DG research across the medical imaging community.",https://openaccess.thecvf.com/content/WACV2024/html/Galappaththige_Generalizing_to_Unseen_Domains_in_Diabetic_Retinopathy_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Galappaththige_Generalizing_to_Unseen_Domains_in_Diabetic_Retinopathy_Classification_WACV_2024_paper.pdf,,github.com/Chumsy0725/SPSD-ViT,2310.17255,main,Poster,https://ieeexplore.ieee.org/document/10483961/,"['Deep learning', 'Diabetic retinopathy', 'Adaptation models', 'Visual impairment', 'Predictive models', 'Transformers', 'Calibration']","['Unseen Domains', 'Diabetic Retinopathy Classification', 'Deep Learning', 'Medical Imaging', 'Convex Combination', 'Domain Generalization', 'Cause Of Visual Impairment', 'Calibration Performance', 'Vision Transformer', 'Combination Of Labeling', 'Convolutional Neural Network', 'Superior Performance', 'Validation Set', 'Reliable Model', 'Final Classification', 'Kullback-Leibler', 'Target Domain', 'Training Iterations', 'Source Domain', 'Fundus Images', 'Training Domain', 'Empirical Risk Minimization', 'Intermediate Class', 'Moderate Diabetic Retinopathy', 'Cotton Wool Spots', 'Mild Diabetic Retinopathy', 'Diabetic Retinopathy Severity', 'Transformer Block', 'Medical Image Analysis', 'Adaptation Phase']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Diabetic retinopathy (DR) is caused by long-standing diabetes and is among the fifth leading cause for visual impairment. The prospects of early diagnosis and treatment could be helpful in curing the disease, however, the detection procedure is rather challenging and mostly tedious. Therefore, automated diabetic retinopathy classification using deep learning techniques has gained interest in the medical imaging community. Akin to several other real-world applications of deep learning, the typical assumption of i.i.d data is also violated in DR classification that relies on deep learning. Therefore, developing DR classification methods robust to unseen distributions is of great value. In this paper, we study the problem of generalizing a model to unseen distributions or domains (a.k.a domain generalization) in DR classification. To this end, we propose a simple and effective domain generalization (DG) approach that achieves self-distillation in vision transformers (ViT) via a novel prediction softening mechanism. This prediction softening is an adaptive convex combination of one-hot labels with the model’s own knowledge. We perform extensive experiments on challenging open-source DR classification datasets under both multi-source and more challenging single-source DG settings with three different ViT backbones to establish the efficacy and applicability of our approach against competing methods. For the first time, we report the performance of several state-of-the-art domain generalization (DG) methods on open-source DR classification datasets after conducting thorough experiments. Finally, our method is also capable of delivering improved calibration performance than other methods, showing its suitability for safety-critical applications, including health-care. We hope that our contributions would instigate more DG research across the medical imaging community. Code is available at github.com/Chumsy0725/SPSD-ViT."
Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models,"Minxing Zhang, Ning Yu, Rui Wen, Michael Backes, Yang Zhang",CISPA Helmholtz Center for Information Security; Salesforce Research,50.0,Germany,50.0,USA,"Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary non-member datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC > 0.99 against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC > 0.90. As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Generated_Distributions_Are_All_You_Need_for_Membership_Inference_Attacks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Generated_Distributions_Are_All_You_Need_for_Membership_Inference_Attacks_WACV_2024_paper.pdf,,https://github.com/minxingzhang/MIAGM,2310.19410,main,Poster,https://ieeexplore.ieee.org/document/10484149/,"['Training', 'Data privacy', 'Privacy', 'Visualization', 'Publishing', 'Computational modeling', 'Training data']","['Inference Attacks', 'Membership Inference', 'Membership Inference Attacks', 'Training Data', 'Training Dataset', 'Autoencoder', 'Generative Adversarial Networks', 'Diffusion Model', 'Implicit Function', 'Privacy Leakage', 'Denoising', 'Variety Of Applications', 'Negative Samples', 'Super-resolution', 'Color Images', 'ImageNet', 'Image Generation', 'Types Of Attacks', 'Prevalence Settings', 'Privacy Risks', 'Auxiliary Dataset', 'Attack Performance', 'Relaxed Assumptions', 'Image Inpainting', 'Artifact Reduction', 'Training Distribution', 'High True Positive Rate', 'Query Sample', 'Open Image', 'Unusable Data']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",3,"Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary nonmember datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC > 0.99 against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC > 0.90. As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Generation of Upright Panoramic Image From Non-Upright Panoramic Image,"Jingguo Liu, Heyu Chen, Shigang Li, Jianfeng Li","College of Electronic and Information Engineering, Southwest University, China; Graduate School of Information sciences, Hiroshima City University, Japan",100.0,"China, Japan",0.0,,"The inclination of a spherical camera results in nonupright panoramic images. To carry out upright adjustment, traditional methods estimate camera inclination angles firstly, and then resample the image in terms of the estimated rotation to generate upright image. Since sampling an image is a time-consuming processing, a lookup table is usually used to achieve a high processing speed; however, the content of a lookup table depends on the rotational angles and needs extra memory to store also. In this paper we propose a new approach for panorama upright adjustment, which directly generates an upright panoramic image from an input nonupright one without rotation estimation and lookup tables as an intermediate processing. The proposed approach formulates panorama upright adjustment as a pixelwise image-to-image mapping problem, and the mapping is directly generated from an input nonupright panoramic image via an end-to-end neural network. As shown in the experiment of this paper, the proposed method results in a lightweight network, as less as 163MB, with high processing speed, as great as 9ms, for a 256x512 pixel panoramic image.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Generation_of_Upright_Panoramic_Image_From_Non-Upright_Panoramic_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Generation_of_Upright_Panoramic_Image_From_Non-Upright_Panoramic_Image_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483802/,"['Computer vision', 'Neural networks', 'Estimation', 'Cameras']","['Panoramic Images', 'Neural Network', 'Lookup Table', 'Inclination Angle', 'Intermediate Process', 'Root Mean Square Error Of Cross-validation', 'Mapping Problem', 'Deep Learning', 'Convolutional Neural Network', 'Image Quality', 'Training Phase', 'Pedestrian', 'Image Pairs', 'Angle Range', 'Coordinates Of Points', 'Optical Flow', 'Graph Convolutional Network', 'Depth Estimation', 'L1 Loss', 'Perceptual Loss', 'Pixel Displacement', 'Optical Flow Estimation', 'Disparity Estimation', 'Fréchet Inception Distance', 'Computer Vision Problems', 'Traditional Pipeline', 'Stereo Images', 'Random Angle', 'Ablation Analysis', 'Unsupervised Learning']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', '3D computer vision', 'Applications', 'Virtual / augmented reality']",1,"The inclination of a spherical camera results in nonupright panoramic images. To carry out upright adjustment, traditional methods estimate camera inclination angles firstly, and then resample the image in terms of the estimated rotation to generate upright image. Since sampling an image is a time-consuming processing, a lookup table is usually used to achieve a high processing speed; however, the content of a lookup table depends on the rotational angles and needs extra memory to store also. In this paper we propose a new approach for panorama upright adjustment, which directly generates an upright panoramic image from an input nonupright one without rotation estimation and lookup tables as an intermediate processing. The proposed approach formulates panorama upright adjustment as a pixelwise image-to-image mapping problem, and the mapping is directly generated from an input nonupright panoramic image via an end-to-end neural network. As shown in the experiment of this paper, the proposed method results in a lightweight network, as less as 163MB, with high processing speed, as great as 9ms, for a 256×512 pixel panoramic image."
Glance To Count: Learning To Rank With Anchors for Weakly-Supervised Crowd Counting,"Zheng Xiong, Liangyu Chai, Wenxi Liu, Yongtuo Liu, Sucheng Ren, Shengfeng He",,,,,,"Crowd image is arguably one of the most laborious data to annotate. In this paper, we devote to reduce the massive demand of densely labeled crowd data, and propose a novel weakly-supervised setting, in which we leverage the binary ranking of two images with highcontrast crowd counts as training guidance. To enable training under this new setting, we convert the crowd count regression problem to a ranking potential prediction problem. In particular, we tailor a Siamese Ranking Network that predicts the potential scores of two images indicating the ordering of the counts. Hence, the ultimate goal is to assign appropriate potentials for all the crowd images to ensure their orderings obey the ranking labels. On the other hand, potentials reveal the relative crowd sizes but cannot yield an exact crowd count. We resolve this problem by introducing ""anchors"" during the inference stage. Concretely, anchors are a few images with count labels used for referencing the corresponding counts from potential scores by a simple linear mapping function. We conduct extensive experiments to study various combinations of supervision, and we show that the proposed method outperforms existing weakly-supervised methods without additional labeling effort by a large margin.",https://openaccess.thecvf.com/content/WACV2024/html/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xiong_Glance_To_Count_Learning_To_Rank_With_Anchors_for_Weakly-Supervised_WACV_2024_paper.pdf,,,2205.14659,main,Poster,https://ieeexplore.ieee.org/document/10484124/,"['Training', 'Computer vision', 'Image resolution', 'Codes', 'Labeling']","['Learning To Rank', 'Crowd Counting', 'Linear Function', 'Inference Stage', 'Labeling Effort', 'Training Set', 'Feature Maps', 'Number Of Images', 'Image Object', 'Density Map', 'Image Pairs', 'Counting Method', 'Distribution Of Diversity', 'Count Levels', 'Activity Counts', 'Number Of Labels', 'CNN-based Methods', 'Branch Network', 'Siamese Network', 'Regression-based Methods', 'Paired Rank', 'Auxiliary Task', 'Weak Labels', 'Crowd Density', 'Partial Labels', 'Regression Loss', 'Annotation Process', 'Multi-scale Feature Fusion', 'Receptive Field', 'Population Distribution']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Crowd image is arguably one of the most laborious data to annotate. In this paper, we aim to reduce the massive demand for densely labeled crowd data, and propose a novel weakly-supervised setting, in which we leverage the binary ranking of two images with high-contrast crowd counts as training guidance. To enable training under this new setting, we convert the crowd count regression problem to a ranking potential prediction problem. In particular, we tailor a Siamese Ranking Network that predicts the potential scores of two images indicating the ordering of the counts. Hence, the ultimate goal is to assign appropriate potentials for all the crowd images to ensure their orderings obey the ranking labels. On the other hand, potentials reveal the relative crowd sizes but cannot yield an exact crowd count. We resolve this problem by introducing ""anchors"" during the inference stage. Concretely, anchors are a few images with count labels used for referencing the corresponding counts from potential scores by a simple linear mapping function. We conduct extensive experiments to study various combinations of supervision, and we show that our method outperforms existing weakly-supervised methods by a large margin without additional labeling effort. The code is available at https://github.com/pandaszzzzz/CCRanking."
Global Occlusion-Aware Transformer for Robust Stereo Matching,"Zihua Liu, Yizhou Li, Masatoshi Okutomi","Tokyo Institute of Technology, Japan",100.0,Japan,0.0,,"Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, the performance in the ill-conditioned regions, such as the occluded regions, remains a bottleneck. Due to the limited receptive field, existing CNN-based methods struggle to handle these ill-conditioned regions effectively. To address this issue, this paper introduces a novel attention-based stereo-matching network called Global Occlusion-Aware Transformer (GOAT) to exploit long-range dependency and occlusion-awareness global context for disparity estimation. In the GOAT architecture, a parallel disparity and occlusion estimation module PDO is proposed to estimate the initial disparity map and the occlusion mask using a parallel attention mechanism. To further enhance the disparity estimates in the occluded regions, an occlusion-aware global aggregation module (OGA) is proposed. This module aims to refine the disparity in the occluded regions by leveraging restricted global correlation within the focus scope of the occluded areas. Extensive experiments were conducted on several public benchmark datasets including SceneFlow, KITTI 2015, and Middlebury. The results show that the proposed GOAT demonstrates outstanding performance among all benchmarks, particularly in the occluded regions.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Global_Occlusion-Aware_Transformer_for_Robust_Stereo_Matching_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Global_Occlusion-Aware_Transformer_for_Robust_Stereo_Matching_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483808/,"['Computer vision', 'Correlation', 'Estimation', 'Benchmark testing', 'Transformers', 'Iterative methods']","['Global Transformation', 'Stereo Matching', 'Attention Mechanism', 'Receptive Field', 'Global Context', 'Regional Performance', 'Global Module', 'CNN-based Methods', 'Parallel Mechanism', 'Global Correlation', 'Disparity Estimation', 'Occluded Regions', 'Disparity Map', 'Distinct Features', 'Convolutional Neural Network', 'Spatial Information', 'Local Features', 'Pedestrian', 'Multiple Datasets', 'Cost Volume', 'Left View', 'Endpoint Error', 'Phase Matching', 'Feature Aggregation', 'Positional Encoding', 'Stereo Images', 'Optical Flow', 'Stereo Image Pairs', 'Attention Map']","['Algorithms', '3D computer vision']",3,"Despite the remarkable progress facilitated by learning-based stereo-matching algorithms, the performance in the ill-conditioned regions, such as the occluded regions, remains a bottleneck. Due to the limited receptive field, existing CNN-based methods struggle to handle these ill-conditioned regions effectively. To address this issue, this paper introduces a novel attention-based stereo-matching network called Global Occlusion-Aware Transformer (GOAT) to exploit long-range dependency and occlusion-awareness global context for disparity estimation. In the GOAT architecture, a parallel disparity and occlusion estimation module (PDO) is proposed to estimate the initial disparity map and the occlusion mask using a parallel attention mechanism. To further enhance the disparity estimates in the occluded regions, an occlusion-aware global aggregation module (OGA) is proposed. This module aims to refine the disparity in the occluded regions by leveraging restricted global correlation within the focus scope of the occluded areas. Extensive experiments were conducted on several public benchmark datasets including SceneFlow [15], KITTI 2015 [16], and Middlebury [19]. The results show that proposed GOAT demonstrates outstanding performance among all benchmarks, particularly in the occluded regions."
Gradient Coreset for Federated Learning,"Durga Sivasubramanian, Lokesh Nagalapatti, Rishabh Iyer, Ganesh Ramakrishnan",IIT Bombay; University of Texas at Dallas,100.0,"India, USA",0.0,,"Federated Learning (FL) is used to learn machine learning models with data that is partitioned across multiple clients, including resource-constrained edge devices. It is therefore important to devise solutions that are efficient in terms of compute, communication, and energy consumption, while ensuring compliance with the FL framework's privacy requirements. Conventional approaches to these problems select a weighted subset of the training dataset, known as coreset, and learn by fitting models on it. Such coreset selection approaches are also known to be robust to data noise. However, these approaches rely on the overall statistics of the training data and are not easily extendable to the FL setup. In this paper, we propose an algorithm called Gradient based Coreset for Robust and Efficient Federated Learning (GCFL) that selects a coreset at each client, only every K communication rounds and derives updates only from it, assuming the availability of a small validation dataset at the server. We demonstrate that our coreset selection technique is highly effective in accounting for noise in clients' data. We conduct experiments using four real-world datasets and show that GCFL is (1) more compute and energy efficient than FL, (2) robust to various kinds of noise in both the feature space and labels, (3) preserves the privacy of the validation dataset, and (4) introduces a small communication overhead but achieves significant gains in performance, particularly in cases when the clients' data is noisy.",https://openaccess.thecvf.com/content/WACV2024/html/Sivasubramanian_Gradient_Coreset_for_Federated_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sivasubramanian_Gradient_Coreset_for_Federated_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484449/,"['Training', 'Privacy', 'Federated learning', 'Computational modeling', 'Noise', 'Fitting', 'Training data']","['Federated Learning', 'Training Dataset', 'Machine Learning Models', 'Small Datasets', 'Validation Dataset', 'Real-world Datasets', 'Subset Of Dataset', 'Communication Overhead', 'Edge Devices', 'Kinds Of Noise', 'Communication Rounds', 'Multiple Clients', 'Federated Learning Framework', 'Model Performance', 'Entire Dataset', 'Noisy Data', 'Local Facilities', 'Computational Requirements', 'Computational Overhead', 'Types Of Noise', 'Label Noise', 'Federated Learning Model', 'Federated Learning Algorithm', 'Noisy Set', 'CIFAR-100 Dataset', 'Different Types Of Noise', 'Differential Privacy', 'Client Data', 'Communication Cost', 'Data Chunks']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",1,"Federated Learning (FL) is used to learn machine learning models with data that is partitioned across multiple clients, including resource-constrained edge devices. It is therefore important to devise solutions that are efficient in terms of compute, communication, and energy consumption, while ensuring compliance with the FL framework’s privacy requirements. Conventional approaches to these problems select a weighted subset of the training dataset, known as coreset, and learn by fitting models on it. Such coreset selection approaches are also known to be robust to data noise. However, these approaches rely on the overall statistics of the training data and are not easily extendable to the FL setup.In this paper, we propose an algorithm called Gradient based Coreset for Robust and Efficient Federated Learning (Gcfl) that selects a coreset at each client, only every K communication rounds and derives updates only from it, assuming the availability of a small validation dataset at the server. We demonstrate that our coreset selection technique is highly effective in accounting for noise in clients’ data. We conduct experiments using four real-world datasets and show that Gcfl is (1) more compute and energy efficient than FL, (2) robust to various kinds of noise in both the feature space and labels, (3) preserves the privacy of the validation dataset, and (4) introduces a small communication overhead but achieves significant gains in performance, particularly in cases when the clients’ data is noisy."
Gradient-Guided Knowledge Distillation for Object Detectors,"Qizhen Lan, Qing Tian","Dept. of Computer Science, Bowling Green State University, Ohio, USA and Dept. of Computer Science, University of Alabama at Birmingham, Alabama, USA",100.0,USA,0.0,,"Deep learning models have demonstrated remarkable success in object detection, yet their complexity and computational intensity pose a barrier to deploying them in real-world applications (e.g., self-driving perception). Knowledge Distillation (KD) is an effective way to derive efficient models. However, only a small number of KD methods tackle object detection. Also, most of them focus on mimicking the plain features of the teacher model but rarely consider how the features contribute to the final detection. In this paper, we propose a novel approach for knowledge distillation in object detection, named Gradient-guided Knowledge Distillation (GKD). Our GKD uses gradient information to identify and assign more weights to features that significantly impact the detection loss, allowing the student to learn the most relevant features from the teacher. Furthermore, we present bounding-box-aware multi-grained feature imitation (BMFI) to further improve the KD performance. Experiments on the KITTI and COCO Traffic datasets demonstrate our method's efficacy in knowledge distillation for object detection. On one-stage and two-stage detectors, our GKD-BMFI leads to an average of 5.1% and 3.8% mAP improvement, respectively, beating various state-of-the-art KD methods. Our codes are available at: https://github.com/lanqz7766/GKD.",https://openaccess.thecvf.com/content/WACV2024/html/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lan_Gradient-Guided_Knowledge_Distillation_for_Object_Detectors_WACV_2024_paper.pdf,,https://github.com/lanqz7766/GKD,2303.04240,main,Poster,https://ieeexplore.ieee.org/document/10483913/,"['Deep learning', 'Computer vision', 'Codes', 'Computational modeling', 'Object detection', 'Detectors', 'Feature extraction']","['Object Detection', 'Deep Learning Models', 'Teacher Model', 'KITTI Dataset', 'Two-stage Detectors', 'Training Set', 'Convolutional Neural Network', 'Computer Vision', 'Feature Maps', 'Pedestrian', 'Image Object', 'Intersection Over Union', 'Bounding Box', 'Student Model', 'Faster R-CNN', 'Channel Attention', 'Feature Pyramid Network', 'Object Detection Methods', 'Region Proposal Network', 'Foreground Objects', 'Distillation Method', 'Ground-truth Bounding Box', 'One-stage Detectors', 'ResNet-50 Backbone', 'Distillation Process', 'Validation Set', 'Robust Feature Representation', 'Gradient Loss', 'Distillation Loss', 'Whistle']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Deep learning models have demonstrated remarkable success in object detection, yet their complexity and computational intensity pose a barrier to deploying them in real-world applications (e.g., self-driving perception). Knowledge Distillation (KD) is an effective way to derive efficient models. However, only a small number of KD methods tackle object detection. Also, most of them focus on mimicking the plain features of the teacher model but rarely consider how the features contribute to the final detection. In this paper, we propose a novel approach for knowledge distillation in object detection, named Gradient-guided Knowledge Distillation (GKD). Our GKD uses gradient information to identify and assign more weights to features that significantly impact the detection loss, allowing the student to learn the most relevant features from the teacher. Furthermore, we present bounding-box-aware multi-grained feature imitation (BMFI) to further improve the KD performance. Experiments on the KITTI and COCO-Traffic datasets demonstrate our method’s efficacy in knowledge distillation for object detection. On one-stage and two-stage detectors, our GKD-BMFI leads to an average of 5.1% and 3.8% mAP improvement, respectively, beating various state-of-the-art KD methods. Our codes are available at: https://github.com/lanqz7766/GKD."
Gradual Source Domain Expansion for Unsupervised Domain Adaptation,"Thomas Westfechtel, Hao-Wei Yeh, Dexuan Zhang, Tatsuya Harada","The University of Tokyo; The University of Tokyo, RIKEN",100.0,Japan,0.0,,"Unsupervised domain adaptation (UDA) tries to overcome the need of a large labeled dataset by transferring knowledge from a source dataset, with lots of labeled data, to a target dataset, that has no labeled data. Since there are no labels in the target domain, early misalignment might propagate into the later stages and lead to an error build-up. In order to overcome this problem, we propose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA task several times from scratch, but each time expands the source dataset with target data. In particular, the highest scoring target data of the previous run are employed as pseudo-source samples with their respective pseudo-label. Using this strategy, the pseudo source samples induce knowledge extracted from the previous run directly from the start of the new training. This helps align the two domains better especially in the early training epochs. In this study, we first introduce a strong baseline network and apply our GSDE strategy to it. We conduct experiments and ablation studies on three benchmarks (Office-31, OfficeHome, and DomainNet) and outperform state-of-the-art methods. We further show that the proposed GSDE strategy can improve the accuracy of a variety of different state-of-the-art UDA approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Westfechtel_Gradual_Source_Domain_Expansion_for_Unsupervised_Domain_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Westfechtel_Gradual_Source_Domain_Expansion_for_Unsupervised_Domain_Adaptation_WACV_2024_paper.pdf,,,2311.09599,main,Poster,https://ieeexplore.ieee.org/document/10484328/,"['Training', 'Computer vision', 'Benchmark testing', 'Task analysis']","['Domain Adaptation', 'Source Domain', 'Gradual Expansion', 'Target Domain', 'Target Data', 'Target Dataset', 'Source Dataset', 'Start Of Training', 'Data Sources', 'Feature Space', 'Data Augmentation', 'Training Stage', 'Version Of Task', 'Decrease In Accuracy', 'Decision Boundary', 'Classification Loss', 'Domain Classifier', 'Minimum Entropy', 'Pseudo Labels', 'Unsupervised Domain Adaptation Methods', 'Domain Adaptation Methods', 'Adaptive Loss', 'Strong Prior', 'Domain-invariant Features', 'Bottleneck Layer', 'Parallel Layers', 'Best-performing Algorithm', 'Expansion Strategy', 'Increase In Accuracy']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Unsupervised domain adaptation (UDA) tries to overcome the need for a large labeled dataset by transferring knowledge from a source dataset, with lots of labeled data, to a target dataset, that has no labeled data. Since there are no labels in the target domain, early misalignment might propagate into the later stages and lead to an error build-up. In order to overcome this problem, we propose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA task several times from scratch, each time reinitializing the network weights, but each time expands the source dataset with target data. In particular, the highest-scoring target data of the previous run are employed as pseudo-source samples with their respective pseudo-label. Using this strategy, the pseudo-source samples induce knowledge extracted from the previous run directly from the start of the new training. This helps align the two domains better, especially in the early training epochs. In this study, we first introduce a strong baseline network and apply our GSDE strategy to it. We conduct experiments and ablation studies on three benchmarks (Office-31, OfficeHome, and DomainNet) and outperform state-of-the-art methods. We further show that the proposed GSDE strategy can improve the accuracy of a variety of different state-of-the-art UDA approaches."
Grafting Vision Transformers,"Jongwoo Park, Kumara Kahatapitiya, Donghyun Kim, Shivchander Sudalairaj, Quanfu Fan, Michael S. Ryoo",Stony Brook University; MIT-IBM Watson AI Lab; Amazon,66.66666666666666,USA,33.33333333333334,USA,"Vision Transformers (ViTs) have recently become the state-of-the-art across many computer vision tasks. In contrast to convolutional networks (CNNs), ViTs enable global information sharing even within shallow layers of a network, i.e., among high-resolution features. However, this perk was later overlooked with the success of pyramid architectures such as Swin Transformer, which show better performance-complexity trade-offs. In this paper, we present a simple and efficient add-on component (termed GrafT) that considers global dependencies and multi-scale information throughout the network, in both high- and low-resolution features alike. It has the flexibility of branching out at arbitrary depths and shares most of the parameters and computations of the backbone. GrafT shows consistent gains over various well-known models which includes both hybrid and pure Transformer types, both homogeneous and pyramid structures, and various self-attention methods. In particular, it largely benefits mobile-size models by providing high-level semantics. On the ImageNet-1k dataset, GrafT delivers +3.9%, +1.4%, and +1.9% top-1 accuracy improvement to DeiT-T, Swin-T, and MobileViT-XXS, respectively. The code and models are at https://github.com/jongwoopark7978/Grafting-Vision-Transformer.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Grafting_Vision_Transformers_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Grafting_Vision_Transformers_WACV_2024_paper.pdf,,https://github.com/jongwoopark7978/Grafting-Vision-Transformer,2210.15943,main,Poster,https://ieeexplore.ieee.org/document/10483970/,"['Computer vision', 'Codes', 'Computational modeling', 'Semantics', 'Information sharing', 'Computer architecture', 'Transformers']","['Grafting', 'Vision Transformer', 'Convolutional Network', 'Global Information', 'Homogeneous Structure', 'High-resolution Features', 'Top-1 Accuracy', 'Multi-scale Information', 'Low-resolution Feature', 'High-level Semantics', 'Local Information', 'Feature Maps', 'Object Detection', 'Receptive Field', 'Increase In Parameters', 'Final Feature', 'Semantic Segmentation', 'Average Pooling', 'Multi-scale Features', 'Skip Connections', 'Bilinear Interpolation', 'Horizontal Level', 'Vertical Layers', 'Coarse Features', 'Inductive Bias', 'Self-attention Mechanism', 'Linear Projection', 'Small-scale Datasets', 'Multi-scale Representation', 'Single Shot Detector']","['Algorithms', 'Image recognition and understanding']",,"Vision Transformers (ViTs) have recently become the state-of-the-art across many computer vision tasks. In contrast to convolutional networks (CNNs), ViTs enable global information sharing even within shallow layers of a network, i.e., among high-resolution features. However, this perk was later overlooked with the success of pyramid architectures such as Swin Transformer, which show better performance-complexity trade-offs. In this paper, we present a simple and efficient add-on component (termed GrafT) that considers global dependencies and multi-scale information throughout the network, in both high- and low-resolution features alike. It has the flexibility of branching out at arbitrary depths and shares most of the parameters and computations of the backbone. GrafT shows consistent gains over various well-known models which includes both hybrid and pure Transformer types, both homogeneous and pyramid structures, and various self-attention methods. In particular, it largely benefits mobile-size models by providing high-level semantics. On the ImageNet-1k dataset, GrafT delivers +3.9%, +1.4%, and +1.9% top-1 accuracy improvement to DeiT-T, Swin-T, and MobViTXXS, respectively. Our code and models are at https://github.com/jongwoopark7978/Grafting-Vision-Transformer."
Graph Neural Networks for End-to-End Information Extraction From Handwritten Documents,"Yessine Khanfir, Marwa Dhiaf, Emna Ghodhbani, Ahmed Cheikh Rouhou, Yousri Kessentini","Digital Research Centre of Sfax, Tunisia; SM@RTS: Laboratory of Signals, Systems, Artificial Intelligence and Networks; InstaDeep",33.33333333333333,Tunisia,66.66666666666667,USA,"Automating Information Extraction (IE) from handwritten documents is a challenging task due to the wide variety of handwriting styles, the presence of noise, and the lack of labeled data. In this work, we propose an end-to-end encoder-decoder model, that incorporates transformers and Graph Convolutional Networks (GCN), to jointly perform Handwritten Text Recognition (HTR) and Named Entity Recognition (NER). The proposed architecture is mainly composed of two parts: a Sparse Graph Transformer Encoder (SGTE), to capture efficient representations of input text images while controlling the propagation of information through the model. The SGTE is followed by a transformer decoder enhanced with a GCN that combines the outputs of the last SGTE layer and the Multi-Head Attention (MHA) block to reinforce the alignment of visual features to characters and Named Entity (NE) tags, resulting in more robust learned representations. The proposed model shows promising results and achieves state-of-the-art performance on the IAM dataset, and in the ICDAR 2017 Information Extraction competition using the Esposalles database.",https://openaccess.thecvf.com/content/WACV2024/html/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Khanfir_Graph_Neural_Networks_for_End-to-End_Information_Extraction_From_Handwritten_Documents_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Graph(Graph): A Nested Graph-Based Framework for Early Accident Anticipation,"Nupur Thakur, PrasanthSai Gouripeddi, Baoxin Li","Arizona State University, Tempe, AZ, USA",100.0,USA,0.0,,"Anticipating traffic accidents early using dashcam videos is an important task for ensuring road safety and building reliable intelligent autonomous vehicles. However, factors like high traffic on the roads, different types of accidents, limited angles of vision, etc. make this task very challenging. Using the early frames, a lot of existing methods predict a large number of false positives which poses a huge risk for all vehicles on the road. In this paper, we propose a novel end-to-end learning, nested graph-based framework named Graph(Graph) for early accident anticipation. It uses interactions between the objects in the same as well as the neighboring frames along with the global features to make precise predictions as early as possible. This way it is able to embed the local as well as global temporal information into the extracted features. Graph(Graph) outperforms state-of-the-art methods on different datasets by a large margin demonstrating its effectiveness. With empirical evidence, we highlight the importance of each component in Graph(Graph) and show their effect on the final performance. Our code is available at https://github.com/thakurnupur/Graph-Graph.",https://openaccess.thecvf.com/content/WACV2024/html/Thakur_GraphGraph_A_Nested_Graph-Based_Framework_for_Early_Accident_Anticipation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Thakur_GraphGraph_A_Nested_Graph-Based_Framework_for_Early_Accident_Anticipation_WACV_2024_paper.pdf,,https://github.com/thakurnupur/Graph-Graph,,main,Poster,https://ieeexplore.ieee.org/document/10483975/,"['Computer vision', 'Buildings', 'Feature extraction', 'Road safety', 'Reliability', 'Data mining', 'Task analysis']","['Early Anticipation', 'Global Features', 'Temporal Information', 'Global Information', 'Autonomous Vehicles', 'Spatial Information', 'Information Flow', 'Feature Information', 'Recurrent Neural Network', 'Bounding Box', 'Fully-connected Layer', 'Word Embedding', 'Spatiotemporal Characteristics', 'Pre-trained Network', 'Self-driving', 'Graph Neural Networks', 'Previous Frame', 'Object Interaction', 'Final Probability', 'Object In Frame', 'Global Feature Extraction', 'Graph Attention', 'Frame Features', 'Node Embeddings', 'Video Frames', 'Street Scenes', 'Convolutional Neural Network', 'Average Precision', 'Object Labels']","['Applications', 'Autonomous Driving', 'Algorithms', 'Video recognition and understanding']",1,"Anticipating traffic accidents early using dashcam videos is an important task for ensuring road safety and building reliable intelligent autonomous vehicles. However, factors like high traffic on the roads, different types of accidents, limited angles of vision, etc. make this task very challenging. Using the early frames, a lot of existing methods predict a large number of false positives which poses a huge risk for all vehicles on the road. In this paper, we propose a novel end-to-end learning, nested graph-based framework named Graph(Graph) for early accident anticipation. It uses interactions between the objects in the same as well as the neighboring frames along with the global features to make precise predictions as early as possible. This way it is able to embed the local as well as global temporal information into the extracted features. Graph(Graph) outperforms state-of-the-art methods on different datasets by a large margin demonstrating its effectiveness. With empirical evidence, we highlight the importance of each component in Graph(Graph) and show their effect on the final performance. Our code is available at https://github.com/thakurnupur/Graph-Graph."
GraphFill: Deep Image Inpainting Using Graphs,"Shashikant Verma, Aman Sharma, Roopa Sheshadri, Shanmuganathan Raman","Samsung R&D Institute Bangalore, India; Indian Institute of Technology Gandhinagar, India",100.0,India,0.0,,"We present a novel coarser-to-finer approach for deep graphical image inpainting that utilizes GraphFill, a graph neural network-based deep learning framework, and a lightweight generative baseline network. We construct a pyramidal graph for the input-masked image by reducing it into superpixels, each representing a node in the graph. The proposed pyramidal approach facilitates the transfer of global context from coarser to finer pyramid levels, enabling GraphFill to estimate plausible information for unknown node values in the graph. The estimated information is used to fill in the masked region, which a Refine Network then refines. Furthermore, we propose a resolution-robust pyramidal graph construction method, allowing for efficient inpainting of high-resolution images with relatively fewer computations. Our proposed network, trained on Places and CelebA-HQ datasets, demonstrates competitive performance compared to existing methods while using fewer learning parameters. We conduct thorough ablation studies to evaluate the effectiveness of each component in the GraphFill Network for improved performance. Our proposed lightweight model for image inpainting is efficient in real-world scenarios, as it can be easily deployed on mobile devices with limited resources.",https://openaccess.thecvf.com/content/WACV2024/html/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Verma_GraphFill_Deep_Image_Inpainting_Using_Graphs_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483644/,"['Performance evaluation', 'Deep learning', 'Computer vision', 'Image resolution', 'Corporate acquisitions', 'Computational modeling', 'Computer architecture']","['Deep Image', 'Image Inpainting', 'Deep Learning', 'High-resolution Images', 'Mobile Devices', 'Global Context', 'Real-world Scenarios', 'Nodes In The Graph', 'Masked Images', 'Pyramid Level', 'Values In Graphs', 'Refinement Network', 'Neural Network', 'Convolutional Neural Network', 'Graphical Representation', 'Image Pixels', 'Receptive Field', 'Generative Adversarial Networks', 'Graph Convolutional Network', 'Simple Linear Iterative Clustering', 'Foreground Regions', 'Masked Area', 'Graph Neural Networks', 'Validation Split', 'Edge Connectivity', 'Background Regions', 'Node Features', 'Variational Autoencoder']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"We present a novel coarser-to-finer approach for deep graphical image inpainting that utilizes GraphFill, a graph neural network-based deep learning framework, and a lightweight generative baseline network. We construct a pyramidal graph for the input-masked image by reducing it into superpixels, each representing a node in the graph. The proposed pyramidal approach facilitates the transfer of global context from coarser to finer pyramid levels, enabling GraphFill to estimate plausible information for unknown node values in the graph. The estimated information is used to fill in the masked region, which a Refine Network then refines. Furthermore, we propose a resolution-robust pyramidal graph construction method, allowing for efficient inpainting of high-resolution images with relatively fewer computations. Our proposed GAN-based network is trained in adversarial settings on Places365 and CelebA-HQ datasets and demonstrates competitive performance compared to existing methods while using fewer learning parameters. We conduct thorough ablation studies to evaluate the effectiveness of each component in the GraphFill Network for improved performance. Our proposed lightweight model for image inpainting is efficient in real-world scenarios, as it can be easily deployed on mobile devices with limited resources."
Group-Wise Contrastive Bottleneck for Weakly-Supervised Visual Representation Learning,"Boon Peng Yap, Beng Koon Ng","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",100.0,Singapore,0.0,,"Coarse or weak labels can serve as a cost-effective solution to the problem of visual representation learning. When fine-grained labels are unavailable, weak labels can provide some form of supervisory signals to guide the representation learning process. Some examples of weak labels include image captions, visual attributes and coarse-grained object categories. In this work, we consider the semantic grouping relationship that exists within certain types of weak labels and propose a group-wise contrastive bottleneck module to leverage this relationship. The semantic group may contain labels that are related to a general concept, such as the colour or shape of objects. Using the group-wise bottleneck module, we disentangle the global image features into multiple group features and apply contrastive learning in a group-wise manner to maximize the similarity of positive pairs within each semantic group. The positive pairs are defined based on the similarity of the labels captured by each group. To learn a more robust representation, we introduce a reconstruction objective where an image feature is reconstructed back from the disentangled features, and this reconstruction is encouraged to be consistent with the feature obtained from a different augmented view of the same image. We empirically verify the efficacy of the proposed method on several datasets in the context of visual attribute learning, fair representation learning and hierarchical label learning. The experimental results indicate that our proposed method outperforms prior weakly-supervised methods and is flexible in adapting to different representation learning settings.",https://openaccess.thecvf.com/content/WACV2024/html/Yap_Group-Wise_Contrastive_Bottleneck_for_Weakly-Supervised_Visual_Representation_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yap_Group-Wise_Contrastive_Bottleneck_for_Weakly-Supervised_Visual_Representation_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484408/,"['Representation learning', 'Visualization', 'Computer vision', 'Shape', 'Image color analysis', 'Semantics', 'Self-supervised learning']","['Representation Learning', 'Visual Learning', 'Visual Representation Learning', 'Semantic Similarity', 'Visual Properties', 'Self-supervised Learning', 'Robust Representation', 'Type Labels', 'Weak Labels', 'Semantic Groups', 'Class Labels', 'Multilayer Perceptron', 'Hashtags', 'Linear Layer', 'Hair Color', 'Skin Type', 'Reconstruction Loss', 'Contrastive Loss', 'Binary Label', 'Disentangled Representation', 'Top-5 Accuracy', 'Prior Approaches', 'Global Representation', 'Top-1 Accuracy', 'Sensitive Attributes', 'Contrast Objective', 'Zero-shot', 'Intermediate Representation', 'Quality Of Representations']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Coarse or weak labels can serve as a cost-effective solution to the problem of visual representation learning. When fine-grained labels are unavailable, weak labels can provide some form of supervisory signals to guide the representation learning process. Some examples of weak labels include image captions, visual attributes and coarse-grained object categories. In this work, we consider the semantic grouping relationship that exists within certain types of weak labels and propose a group-wise contrastive bottleneck module to leverage this relationship. The semantic group may contain labels that are related to a general concept, such as the colour or shape of objects. Using the group-wise bottleneck module, we disentangle the global image features into multiple group features and apply contrastive learning in a group-wise manner to maximize the similarity of positive pairs within each semantic group. The positive pairs are defined based on the similarity of the labels captured by each group. To learn a more robust representation, we introduce a reconstruction objective where an image feature is reconstructed back from the disentangled features, and this reconstruction is encouraged to be consistent with the feature obtained from a different augmented view of the same image. We empirically verify the efficacy of the proposed method on several datasets in the context of visual attribute learning, fair representation learning and hierarchical label learning. The experimental results indicate that our proposed method outperforms prior weakly-supervised methods and is flexible in adapting to different representation learning settings."
Guided Cluster Aggregation: A Hierarchical Approach to Generalized Category Discovery,"Jona Otholt, Christoph Meinel, Haojin Yang","Hasso Plattner Institute, University of Potsdam, Germany",100.0,Germany,0.0,,"Despite advances in image recognition, recognizing novel categories in unlabeled data remains challenging for machine learning methods, even though humans can perform this task with ease. A recently developed setting to tackle this problem is Generalized Category Discovery (GCD), in which the task is to, given a labeled dataset, classify an unlabeled dataset, where the unlabeled dataset contains both known classes and novel classes that do not appear in the labeled data. Existing GCD methods mostly focus on learning strong image representations, on which they then apply a clustering algorithm such as k-means. Despite obtaining good performance, they do not fully exploit the potential of the learned features due to the simple nature of the clustering mechanism. To address this issue, we make use of the fact that local neighborhoods in self-supervised feature spaces are highly homogeneous. We leverage this observation to develop Guided Cluster Aggregation (GCA), a hierarchical approach that first groups the data into small clusters of high purity, then aggregates them into larger clusters. Experiments show that GCA outperforms semi-supervised k-means in most cases, especially in fine-grained classification tasks. Code available at https://github.com/J- L- O/guided-cluster-aggregation.",https://openaccess.thecvf.com/content/WACV2024/html/Otholt_Guided_Cluster_Aggregation_A_Hierarchical_Approach_to_Generalized_Category_Discovery_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Otholt_Guided_Cluster_Aggregation_A_Hierarchical_Approach_to_Generalized_Category_Discovery_WACV_2024_paper.pdf,,https://github.com/J-L-O/guided-cluster-aggregation,,main,Poster,https://ieeexplore.ieee.org/document/10483806/,"['Computer vision', 'Image recognition', 'Codes', 'Aggregates', 'Clustering algorithms', 'Machine learning', 'Semisupervised learning']","['Hierarchical Approach', 'Category Discovery', 'Small Clusters', 'Image Recognition', 'Local Neighborhood', 'Image Representation', 'Unlabeled Data', 'Use Of The Fact', 'K-nearest Neighbor', 'Target Class', 'Class Assignment', 'Semi-supervised Learning', 'Cluster C', 'Ground-truth Class', 'Unlabeled Set', 'Clustering Loss', 'Cluster Head', 'Pre-trained Feature']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Despite advances in image recognition, recognizing novel categories in unlabeled data remains challenging for machine learning methods, even though humans can perform this task with ease. A recently developed setting to tackle this problem is Generalized Category Discovery (GCD), in which the task is to, given a labeled dataset, classify an unlabeled dataset, where the unlabeled dataset contains both known classes and novel classes that do not appear in the labeled data. Existing GCD methods mostly focus on learning strong image representations, on which they then apply a clustering algorithm such as k-means. Despite obtaining good performance, they do not fully exploit the potential of the learned features due to the simple nature of the clustering mechanism. To address this issue, we make use of the fact that local neighborhoods in self-supervised feature spaces are highly homogeneous. We leverage this observation to develop Guided Cluster Aggregation (GCA), a hierarchical approach that first groups the data into small clusters of high purity, then aggregates them into larger clusters. Experiments show that GCA outperforms semi-supervised k-means in most cases, especially in fine-grained classification tasks. Code available at https://github.com/J-L-O/guidedcluster-aggregation."
Guided Distillation for Semi-Supervised Instance Segmentation,"Tariq Berrada, Camille Couprie, Karteek Alahari, Jakob Verbeek","FAIR, Meta; Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK",50.0,France,50.0,USA,"Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel ""guided burn-in"" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on the Cityscapes dataset we improve mask-AP from 23.7 to 33.9 when using labels for 10% of images, and on the COCO dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1% of the training data.",https://openaccess.thecvf.com/content/WACV2024/html/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Berrada_Guided_Distillation_for_Semi-Supervised_Instance_Segmentation_WACV_2024_paper.pdf,,,2308.02668,main,Poster,https://ieeexplore.ieee.org/document/10483916/,"['Instance segmentation', 'Training', 'Training data', 'Computer architecture', 'Object detection', 'Transformers', 'Feature extraction']","['Instance Segmentation', 'Teacher Model', 'Training Images', 'Unlabeled Data', 'Student Model', 'COCO Dataset', 'Architecture For Segmentation', 'Instance Segmentation Methods', 'Object Detection', 'Data Augmentation', 'Bounding Box', 'Segmentation Model', 'Semantic Segmentation', 'Segmentation Task', 'Pseudo Labels', 'ResNet-50 Backbone', 'Unlabeled Images', 'Training Pipeline', 'Backbone Architecture', 'Backbone Feature', 'Bipartite Matching']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel ""guided burn-in"" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on the Cityscapes dataset we improve mask-AP from 23.7 to 33.9 when using labels for 10% of images, and on the COCO dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1% of the training data."
HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities,"Shristi Das Biswas, Adarsh Kosta, Chamika Liyanagedera, Marco Apolinario, Kaushik Roy","Purdue University, West Lafayette, Indiana, USA",100.0,USA,0.0,,"Event cameras detect changes in per-pixel intensity to generate asynchronous 'event streams'. They offer great potential for accurate semantic map retrieval in real-time autonomous systems owing to their much higher temporal resolution and high dynamic range (HDR) compared to conventional cameras. However, existing implementations for event-based segmentation suffer from sub-optimal performance since these temporally dense events only measure the varying component of a visual signal, limiting their ability to encode dense spatial context compared to frames. To address this issue, we propose a hybrid end-to-end learning framework HALSIE, utilizing three key concepts to reduce inference cost by up to 20x versus prior art while retaining similar performance: First, a simple and efficient cross-domain learning scheme to extract complementary spatio-temporal embeddings from both frames and events. Second, a specially designed dual-encoder scheme with Spiking Neural Network (SNN) and Artificial Neural Network (ANN) branches to minimize latency while retaining cross-domain feature aggregation. Third, a multi-scale cue mixer to model rich representations of the fused embeddings. These qualities of HALSIE allow for a very lightweight architecture achieving state-of-the-art segmentation performance on DDD-17, MVSEC, and DSEC-Semantic datasets with up to 33x higher parameter efficiency and favorable inference cost (17.9mJ per cycle). Our ablation study also brings new insights into effective design choices that can prove beneficial for research across other vision tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Biswas_HALSIE_Hybrid_Approach_to_Learning_Segmentation_by_Simultaneously_Exploiting_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Biswas_HALSIE_Hybrid_Approach_to_Learning_Segmentation_by_Simultaneously_Exploiting_Image_WACV_2024_paper.pdf,,,2211.10754,main,Poster,https://ieeexplore.ieee.org/document/10484121/,"['Visualization', 'Costs', 'Semantic segmentation', 'Semantics', 'Feature extraction', 'Cameras', 'Task analysis']","['Neural Network', 'Artificial Neural Network', 'Autonomic System', 'Per Cycle', 'Spiking Neural Networks', 'High Dynamic Range', 'Semantic Map', 'Prior Art', 'Event Stream', 'Dynamic Vision Sensor', 'Inference Cost', 'Training Set', 'Feature Maps', 'Spatial Features', 'Dense Network', 'Temporal Features', 'Grayscale Images', 'Information Retrieval', 'Semantic Segmentation', 'Temporal Cues', 'Spatial Feature Extraction', 'Spatial Cues', 'Neuromorphic Hardware', 'Representation Of Events', 'Event Window', 'Temporal Correlation', 'Camera Frame', 'Training Details', 'Heaviside Function']","['Applications', 'Autonomous Driving', 'Applications', 'Robotics', 'Applications', 'Smartphones / end user devices']",3,"Event cameras detect changes in per-pixel intensity to generate asynchronous ‘event streams’. They offer great potential for accurate semantic map retrieval in real-time autonomous systems owing to their much higher temporal resolution and high dynamic range (HDR) compared to conventional cameras. However, existing implementations for event-based segmentation suffer from sub-optimal performance since these temporally dense events only measure the varying component of a visual signal, limiting their ability to encode dense spatial context compared to frames. To address this issue, we propose a hybrid end-to-end learning framework HALSIE, utilizing three key concepts to reduce inference cost by up to 20× versus prior art while retaining similar performance: First, a simple and efficient cross-domain learning scheme to extract complementary spatio-temporal embeddings from both frames and events. Second, a specially designed dual-encoder scheme with Spiking Neural Network (SNN) and Artificial Neural Network (ANN) branches to minimize latency while retaining cross-domain feature aggregation. Third, a multi-scale cue mixer to model rich representations of the fused embeddings. These qualities of HALSIE allow for a very lightweight architecture achieving state-of-the-art segmentation performance on DDD-17, MVSEC, and DSEC-Semantic datasets with up to 33× higher parameter efficiency and favorable inference cost (17.9mJ per cycle). Our ablation study also brings new insights into effective design choices that can prove beneficial for research across other vision tasks."
HAMMER: Learning Entropy Maps To Create Accurate 3D Models in Multi-View Stereo,"Rafael Weilharter, Friedrich Fraundorfer","Institute of Computer Graphics and Vision, Graz University of Technology",100.0,Austria,0.0,,"While the majority of recent Multi-View Stereo Networks estimates a depth map per reference image, their performance is then only evaluated on the fused 3D model obtained from all images. This approach makes a lot of sense since ultimately the point cloud is the result we are mostly interested in. On the flip side, it often leads to a burdensome manual search for the right fusion parameters in order to score well on the public benchmarks. In this work, we tackle the aforementioned problem with HAMMER, a Hierarchical And Memory-efficient MVSNet with Entropy-filtered Reconstructions. We propose to learn a filtering mask based on entropy, which, in combination with a simple two-view geometric verification, is sufficient to generate high quality 3D models of any input scene. Distinct from existing works, a tedious manual parameter search for the fusion step is not required. Furthermore, we take several precautions to keep the memory requirements for our method very low in the training as well as in the inference phase. Our method only requires 6 GB of GPU memory during training, while 3.6 GB are enough to process 1920 x 1024 images during inference. Experiments show that HAMMER ranks amongst the top published methods on the DTU and Tanks and Temples benchmarks in the official metrics, especially when keeping the fusion parameters fixed.",https://openaccess.thecvf.com/content/WACV2024/html/Weilharter_HAMMER_Learning_Entropy_Maps_To_Create_Accurate_3D_Models_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Weilharter_HAMMER_Learning_Entropy_Maps_To_Create_Accurate_3D_Models_in_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483817/,"['Training', 'Point cloud compression', 'Measurement', 'Solid modeling', 'Three-dimensional displays', 'Memory management', 'Graphics processing units']","['Multi-view Stereo', 'Entropy Map', 'Benchmark', 'Point Cloud', 'GB Memory', 'Reference Image', 'Depth Map', 'Convolutional Layers', 'Feature Maps', 'Single Image', 'Number Of Images', '3D Space', 'Learning-based Methods', 'Source Images', 'Residual Block', 'Depth Range', 'Depth Estimation', 'Interval Scale', 'Consistency Checks', 'Structure From Motion', 'Cost Volume', 'Depth Interval', 'Pixel Error', 'Geometric Consistency', 'Depth Error', 'Regular Network', 'Homography', '3D Point', 'Input Image', 'Deep Neural Network']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"While the majority of recent Multi-View Stereo Networks estimates a depth map per reference image, their performance is then only evaluated on the fused 3D model obtained from all images. This approach makes a lot of sense since ultimately the point cloud is the result we are mostly interested in. On the flip side, it often leads to a burdensome manual search for the right fusion parameters in order to score well on the public benchmarks. In this work, we tackle the aforementioned problem with HAMMER, a Hierarchical And Memory-efficient MVSNet with Entropy-filtered Reconstructions. We propose to learn a filtering mask based on entropy, which, in combination with a simple two-view geometric verification, is sufficient to generate high quality 3D models of any input scene. Distinct from existing works, a tedious manual parameter search for the fusion step is not required. Furthermore, we take several precautions to keep the memory requirements for our method very low in the training as well as in the inference phase. Our method only requires 6 GB of GPU memory during training, while 3.6 GB are enough to process 1920×1024 images during inference. Experiments show that HAMMER ranks amongst the top published methods on the DTU and Tanks and Temples benchmarks in the official metrics, especially when keeping the fusion parameters fixed."
HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation,"Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen Zhao, Haocheng Feng, Jingtuo Liu, Errui Ding","Department of Computer Vision Technology (VIS), Baidu Inc., China",100.0,China,0.0,,"In this paper, we study Text-to-3D content generation leveraging 2D diffusion priors to enhance the quality and detail of the generated 3D models. Recent progresses in text-to-3D have shown that employing high-resolution (e.g., 512 x 512) renderings can lead to the production of high-quality 3D models using latent diffusion priors. To enable rendering at even higher resolutions, which has the poten tial to further augment the quality and detail of the models, we propose a novel approach that combines multiple noise estimation processes with a pretrained diffusion prior. Distinct from the Bar-Tal et al.s' study which binds multiple denoised results [1] to generate images from texts, our approach integrates the computation of scoring distillation losses such as SDS loss and VSD loss which are essential techniques for the 3D content generation with 2D diffusion priors. We experimentally evaluated the proposed approach on XXX. The results show that the proposed approach can generate high-quality details more than the baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_HD-Fusion_Detailed_Text-to-3D_Generation_Leveraging_Multiple_Noise_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_HD-Fusion_Detailed_Text-to-3D_Generation_Leveraging_Multiple_Noise_Estimation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484433/,"['Training', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Image resolution', 'Computational modeling', 'Noise']","['Noise Estimation', 'High-quality Models', '3D Model Generation', '2D Diffusion', 'Stage 2', 'Multilayer Perceptron', 'Diffusion Model', 'Latent Space', 'Prediction Network', '3D Coordinates', 'Normal Images', 'Latent Representation', 'Standard Resolution', 'Signed Distance Function', 'Neural Field', 'SOTA Methods', 'Latent Image']","['Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",5,"In this paper, we study Text-to-3D content generation leveraging 2D diffusion priors to enhance the quality and detail of the generated 3D models. Recent progress [11] in text-to-3D has shown that employing high-resolution (e.g., 512 × 512) renderings can lead to the production of high-quality 3D models using latent diffusion priors. To enable rendering at even higher resolutions, which has the potential to further augment the quality and detail of the models, we propose a novel approach that combines multiple noise estimation processes with a pretrained 2D diffusion prior. Distinct from the Bar-Tal et al.s’ study which binds multiple denoised results [1] to generate images from texts, our approach integrates the computation of scoring distillation losses such as SDS loss and VSD loss which are essential techniques for the 3D content generation with 2D diffusion priors. We experimentally evaluated the proposed approach. The results show that the proposed approach can generate high-quality details compared to the baselines."
HDMNet: A Hierarchical Matching Network With Double Attention for Large-Scale Outdoor LiDAR Point Cloud Registration,"Weiyi Xue, Fan Lu, Guang Chen",Tongji University,100.0,China,0.0,,"Outdoor LiDAR point clouds are typically large-scale and complexly distributed. To achieve efficient and accurate registration, emphasizing the similarity among local regions and prioritizing global local-to-local matching is of utmost importance, subsequent to which accuracy can be enhanced through cost-effective fine registration. In this paper, a novel hierarchical neural network with double attention named HDMNet is proposed for large-scale outdoor LiDAR point cloud registration. Specifically, A novel feature consistency enhanced double-soft matching network is introduced to achieve two-stage matching with high flexibility while enlarging the receptive field with high efficiency in a patch-to-patch manner, which significantly improves the registration performance. Moreover, in order to further utilize the sparse matching information from deeper layer, we develop a novel trainable embedding mask to incorporate the confidence scores of correspondences obtained from pose estimation of deeper layer, eliminating additional computations. The high-confidence keypoints in the sparser point cloud of the deeper layer correspond to a high-confidence spatial neighborhood region in shallower layer, which will receive more attention, while the features of non-key regions will be masked. Extensive experiments are conducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate the high accuracy and efficiency of the proposed HDMNet.",https://openaccess.thecvf.com/content/WACV2024/html/Xue_HDMNet_A_Hierarchical_Matching_Network_With_Double_Attention_for_Large-Scale_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xue_HDMNet_A_Hierarchical_Matching_Network_With_Double_Attention_for_Large-Scale_WACV_2024_paper.pdf,,,2310.18874,main,Poster,https://ieeexplore.ieee.org/document/10484177/,"['Point cloud compression', 'Computer vision', 'Laser radar', 'Computer network reliability', 'Pose estimation', 'Neural networks', 'Reliability']","['Point Cloud', 'Matching Network', 'LiDAR Point Clouds', 'Point Cloud Registration', 'Large-scale Outdoor', 'Double Attention', 'Deeper Layers', 'Extensive Experiments', 'Confidence Score', 'Shallow Layers', 'Layer Region', 'Sparse Point Cloud', 'Fine Registration', 'Hierarchical Neural Network', 'Geometric Features', 'Learning-based Methods', 'Corresponding Points', 'Matching Process', 'Neighboring Points', 'KITTI Dataset', 'Iterative Closest Point', 'Original Point Cloud', 'Matching Strategy', 'Global Registration', 'Random Sample Consensus', 'Deepest Layer', 'Matching Module', 'Descriptor Space', 'Nearest Neighbor Points']","['Algorithms', '3D computer vision']",1,"Outdoor LiDAR point clouds are typically large-scale and complexly distributed. To achieve efficient and accurate registration, emphasizing the similarity among local regions and prioritizing global local-to-local matching is of utmost importance, subsequent to which accuracy can be enhanced through cost-effective fine registration. In this paper, a novel hierarchical neural network with double attention named HDMNet is proposed for large-scale outdoor LiDAR point cloud registration. Specifically, A novel feature consistency enhanced double-soft matching network is introduced to achieve two-stage matching with high flexibility while enlarging the receptive field with high efficiency in a patch-to-patch manner, which significantly improves the registration performance. Moreover, in order to further utilize the sparse matching information from deeper layer, we develop a novel trainable embedding mask to incorporate the confidence scores of correspondences obtained from pose estimation of deeper layer, eliminating additional computations. The high-confidence keypoints in the sparser point cloud of the deeper layer correspond to a high-confidence spatial neighborhood region in shallower layer, which will receive more attention, while the features of non-key regions will be masked. Extensive experiments are conducted on two large-scale outdoor LiDAR point cloud datasets to demonstrate the high accuracy and efficiency of the proposed HDMNet."
HELA-VFA: A Hellinger Distance-Attention-Based Feature Aggregation Network for Few-Shot Classification,"Gao Yu Lee, Tanmoy Dam, Daniel Puiu Poenar, Vu N. Duong, Md Meftahul Ferdaus","Nanyang Technological University (NTU), Singapore; University of New Orleans, USA",100.0,"Singapore, USA",0.0,,"Enabling effective learning using only a few presented examples is a crucial but difficult computer vision objective. Few-shot learning have been proposed to address the challenges, and more recently variational inference-based approaches are incorporated to enhance few-shot classification performances. However, the current dominant strategy utilized the Kullback-Leibler (KL) divergences to find the log marginal likelihood of the target class distribution, while neglecting the possibility of other probabilistic comparative measures, as well as the possibility of incorporating attention in the feature extraction stages, which can increase the effectiveness of the few-shot model. To this end, we proposed the HELlinger-Attention Variational Feature Aggregation network (HELA-VFA), which utilized the Hellinger distance along with attention in the encoder to fulfill the aforementioned gaps. We show that our approach enables the derivation of an alternate form of the lower bound commonly presented in prior works, thus making the variational optimization feasible and be trained on the same footing in a given setting. Extensive experiments performed on four benchmarked few-shot classification datasets demonstrated the feasibility and superiority of our approach relative to the State-Of-The-Arts (SOTAs) approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_HELA-VFA_A_Hellinger_Distance-Attention-Based_Feature_Aggregation_Network_for_Few-Shot_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_HELA-VFA_A_Hellinger_Distance-Attention-Based_Feature_Aggregation_Network_for_Few-Shot_Classification_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
HMP: Hand Motion Priors for Pose and Shape Estimation From Video,"Enes Duran, Muhammed Kocabas, Vasileios Choutas, Zicong Fan, Michael J. Black","MPI for Intelligent Systems, Tübingen, Germany; MPI for Intelligent Systems, Tübingen, Germany; University of Tübingen, Germany; MPI for Intelligent Systems, Tübingen, Germany; ETH Zurich, Switzerland",100.0,"Germany, Switzerland",0.0,,"Understanding how humans interact with the world necessitates accurate 3D hand pose estimation, a task complicated by the hand's high degree of articulation, frequent occlusions, self-occlusions, and rapid motions. While most existing methods rely on single-image inputs, videos have useful cues to address aforementioned issues. However, existing video-based 3D hand datasets are insufficient for training feedforward models to generalize to in-the-wild scenarios. On the other hand, we have access to large human motion capture datasets which also include hand motions, e.g. AMASS. Therefore, we develop a generative motion prior specific for hands, trained on the AMASS dataset which features diverse and high-quality hand motions. This motion prior is then employed for video-based 3D hand motion estimation following a latent optimization approach. Our integration of a robust motion prior significantly enhances performance, especially in occluded scenarios. It produces stable, temporally consistent results that surpass conventional single-frame methods. We demonstrate our method's efficacy via qualitative and quantitative evaluations on the HO3D and DexYCB datasets, with special emphasis on an occlusion-focused subset of HO3D. Code is available at https://hmp.is.tue.mpg.de",https://openaccess.thecvf.com/content/WACV2024/html/Duran_HMP_Hand_Motion_Priors_for_Pose_and_Shape_Estimation_From_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Duran_HMP_Hand_Motion_Priors_for_Pose_and_Shape_Estimation_From_WACV_2024_paper.pdf,,https://hmp.is.tue.mpg.de,,main,Poster,https://ieeexplore.ieee.org/document/10483600/,"['Training', 'Solid modeling', 'Three-dimensional displays', 'Shape', 'Motion estimation', 'Pose estimation', 'Motion capture']","['Pose Estimation', 'Hand Motion', 'Shape Estimation', 'Motion Prior', 'Motion Capture', 'Human Motion', 'Large Motion', 'Time Step', 'Shape Parameter', 'Bounding Box', '3D Shape', 'Variational Autoencoder', 'Partial Observation', 'Global Translation', 'Motion Blur', 'Human Pose Estimation', '3D Pose', 'Latent Code', 'Motion Sequences', 'Global Orientation', 'Hand Shape', '2D Keypoints', '3D Joint', 'Parallel Optimization', 'Pose Estimation Methods', 'Pose Parameters', 'RGB Video', 'Optimization Process', 'Single Image', 'Objective Function']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision']",4,"Understanding how humans interact with the world necessitates accurate 3D hand pose estimation, a task complicated by the hand’s high degree of articulation, frequent occlusions, self-occlusions, and rapid motions. While most existing methods rely on single-image inputs, videos have useful cues to address aforementioned issues. However, existing video-based 3D hand datasets are insufficient for training feedforward models to generalize to in-the-wild scenarios. On the other hand, we have access to large human motion capture datasets which also include hand motions, e.g. AMASS. Therefore, we develop a generative motion prior specific for hands, trained on the AMASS dataset which features diverse and high-quality hand motions. This motion prior is then employed for video-based 3D hand motion estimation following a latent optimization approach. Our integration of a robust motion prior significantly enhances performance, especially in occluded scenarios. It produces stable, temporally consistent results that surpass conventional single-frame methods. We demonstrate our method’s efficacy via qualitative and quantitative evaluations on the HO3D and DexYCB datasets, with special emphasis on an occlusion-focused subset of HO3D. Code is available at https://hmp.is.tue.mpg.de"
HaGRID -- HAnd Gesture Recognition Image Dataset,"Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Roman Kraynov, Andrei Makhliarchuk","SaluteDevices, Russia",0.0,,100.0,Russia,"This paper introduces an enormous dataset, HaGRID (HAnd Gesture Recognition Image Dataset), to build a hand gesture recognition (HGR) system concentrating on interaction with devices to manage them. That is why all 18 chosen gestures are endowed with the semiotic function and can be interpreted as a specific action. Although the gestures are static, they were picked up, especially for the ability to design several dynamic gestures. It allows the trained model to recognize not only static gestures such as 'like' and 'stop' but also 'swipes' and 'drag and drop' dynamic gestures. The HaGRID contains 554,800 images and bounding box annotations with gesture labels to solve hand detection and gesture classification tasks. The low variability in context and subjects of other datasets was the reason for creating the dataset without such limitations. Utilizing crowdsourcing platforms allowed us to collect samples recorded by 37,583 subjects in at least as many scenes with subject-to-camera distances from 0.5 to 4 meters in various natural light conditions. The influence of the diversity characteristics was assessed in ablation study experiments. Also, we demonstrate the HaGRID ability to be used for pretraining models in HGR tasks. The HaGRID and pre-trained models are publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Kapitanov_HaGRID_--_HAnd_Gesture_Recognition_Image_Dataset_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kapitanov_HaGRID_--_HAnd_Gesture_Recognition_Image_Dataset_WACV_2024_paper.pdf,https://gitlab.aicloud.sbercloud.ru/rndcv/hagrid,https://github.com/hukenovs/hagrid,,main,Poster,https://ieeexplore.ieee.org/document/10484421/,"['Training', 'Meters', 'Crowdsourcing', 'Computer vision', 'Computational modeling', 'Lighting', 'Gesture recognition']","['Hand Gestures', 'Gesture Recognition', 'Hand Gesture Recognition', 'Light Conditions', 'Classification Task', 'Bounding Box', 'Crowdsourcing', 'Detection Task', 'Bounding Box Annotations', 'Gesture Classification', 'Training Set', 'High-resolution Images', 'Validation Set', 'Classification Problem', 'Human-computer Interaction', 'Heterogeneous Characteristics', 'Variable Light', 'Filtering Stage', 'Home Automation', 'Extra Classes', 'Virtual Assistant', 'Mining Tasks']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Image recognition and understanding']",10,"This paper introduces an enormous dataset, HaGRID (HAnd Gesture Recognition Image Dataset), to build a hand gesture recognition (HGR) system concentrating on interaction with devices to manage them. That is why all 18 chosen gestures are endowed with the semiotic function and can be interpreted as a specific action. Although the gestures are static, they were picked up, especially for the ability to design several dynamic gestures. It allows the trained model to recognize not only static gestures such as ""like"" and ""stop"" but also ""swipes"" and ""drag and drop"" dynamic gestures. The HaGRID contains 554,800 images and bounding box annotations with gesture labels to solve hand detection and gesture classification tasks. The low variability in context and subjects of other datasets was the reason for creating the dataset without such limitations. Utilizing crowdsourcing platforms allowed us to collect samples recorded by 37,583 subjects in at least as many scenes with subject-to-camera distances from 0.5 to 4 meters in various natural light conditions. The influence of the diversity characteristics was assessed in ablation study experiments. Also, we demonstrate the HaGRID ability to be used for pretraining models in HGR tasks. The HaGRID and pre-trained models are publicly available
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">12</sup>
."
HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information,"Heitor Rapela Medeiros, Fidel A. Guerrero Peña, Masih Aminbeidokhti, Thomas Dubail, Eric Granger, Marco Pedersoli","Dept. of Systems Engineering, ETS Montreal, Canada; LIVIA, Dept. of Systems Engineering, ETS Montreal, Canada",100.0,Canada,0.0,,"A powerful way to adapt a visual recognition model to a new domain is through image translation. However, common image translation approaches only focus on generating data from the same distribution as the target domain. Given a cross-modal application, such as pedestrian detection from aerial images, with a considerable shift in data distribution between infrared (IR) to visible (RGB) images, a translation focused on generation might lead to poor performance as the loss focuses on irrelevant details for the task. In this paper, we propose HalluciDet, an IR-RGB image translation model for object detection. Instead of focusing on reconstructing the original image on the IR modality, it seeks to reduce the detection loss of an RGB detector, and therefore avoids the need to access RGB data. This model produces a new image representation that enhances objects of interest in the scene and greatly improves detection performance. We empirically compare our approach against state-of-the-art methods for image translation and for fine-tuning on IR, and show that our HalluciDet improves detection accuracy in most cases by exploiting the privileged information encoded in a pre-trained RGB detector. Code: https://github.com/heitorrapela/HalluciDet.",https://openaccess.thecvf.com/content/WACV2024/html/Medeiros_HalluciDet_Hallucinating_RGB_Modality_for_Person_Detection_Through_Privileged_Information_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Medeiros_HalluciDet_Hallucinating_RGB_Modality_for_Person_Detection_Through_Privileged_Information_WACV_2024_paper.pdf,,https://github.com/heitorrapela/HalluciDet,,main,Poster,https://ieeexplore.ieee.org/document/10483993/,"['Training', 'Adaptation models', 'Visualization', 'Pedestrians', 'Detectors', 'Object detection', 'Image representation']","['Privileged Information', 'RGB Modality', 'Object Detection', 'Target Domain', 'Translational Model', 'Pedestrian Detection', 'Loss Function', 'Computer Vision', 'Infrared Imaging', 'Color Images', 'Bounding Box', 'Detection Task', 'Generative Adversarial Networks', 'RGB Images', 'Representation Of Space', 'Self-driving', 'Faster R-CNN', 'Reconstruction Loss', 'Region Proposal', 'Intermediate Representation', 'Infrared Data', 'One-stage Detectors', 'Two-stage Detectors', 'Translation Technique', 'Fine-tuned Model', 'Region Proposal Network']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"A powerful way to adapt a visual recognition model to a new domain is through image translation. However, common image translation approaches only focus on generating data from the same distribution as the target domain. Given a cross-modal application, such as pedestrian detection from aerial images, with a considerable shift in data distribution between infrared (IR) to visible (RGB) images, a translation focused on generation might lead to poor performance as the loss focuses on irrelevant details for the task. In this paper, we propose HalluciDet, an IR-RGB image translation model for object detection. Instead of focusing on reconstructing the original image on the IR modality, it seeks to reduce the detection loss of an RGB detector, and therefore avoids the need to access RGB data. This model produces a new image representation that enhances objects of interest in the scene and greatly improves detection performance. We empirically compare our approach against state-of-the-art methods for image translation and for fine-tuning on IR, and show that our HalluciDet improves detection accuracy in most cases by exploiting the privileged information encoded in a pre-trained RGB detector. Code: https://github.com/heitorrapela/HalluciDet."
Handformer2T: A Lightweight Regression-Based Model for Interacting Hands Pose Estimation From a Single RGB Image,"Pengfei Zhang, Deying Kong","University of California, Irvine, Irvine, CA 92617, USA; Google, USA",50.0,USA,50.0,USA,"Despite its extensive range of potential applications in virtual reality and augmented reality, 3D hand pose estimation from RGB image remains a very challenging problem. The appearance confusions between the two hands and their joints, along with severe hand-hand occlusion and self-occlusion, makes it even more difficult in the senario of interacting hands. Previous methods deal with this problem at the joint level and generally use a heatmap-based method for coordinate prediction. In this paper, we propose a regression-based method that can deal with joint regression at the hand level, which makes the model much more lightweight and memory efficient. To achieve this, we design a novel Pose Query Enhancer (PQE) module, which takes the coarse joint prediction for each hand and refine the prediction iteratively. The key idea of PQE is to make the regression model focus more on the information near proposed joint prediction by manually sampling the feature map. Since we always adopt the transformer on hand level, our model remains lightweight amd memory friendly with this module. Experiments on public benchmarks demonstrate that our model achieves state-of-the-art performance with higher throughput, while requiring less memory and time.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Handformer2T_A_Lightweight_Regression-Based_Model_for_Interacting_Hands_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Handformer2T_A_Lightweight_Regression-Based_Model_for_Interacting_Hands_Pose_Estimation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484197/,"['Solid modeling', 'Three-dimensional displays', 'Computational modeling', 'Pose estimation', 'Memory management', 'Focusing', 'Transformers']","['Single Image', 'RGB Images', 'Pose Estimation', 'Computational Complexity', 'Extensive Experiments', 'Fast Speed', 'Space Complexity', 'Virtual Reality Applications', 'Image Features', 'Left Hand', 'Input Image', 'Feature Maps', 'Model Size', 'Human-computer Interaction', 'Flow Model', 'Feature Fusion', 'Joint Position', 'Inference Speed', 'Human Pose Estimation', 'Regression-based Methods', 'Refiner', 'Multi-head Self-attention', 'Maximum Mean Discrepancy', 'Frames Per Second', 'Single Hand', 'Endpoint Error', 'Large Public Datasets']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",2,"Despite its extensive range of potential applications in virtual reality and augmented reality, 3D interacting hand pose estimation from RGB image remains a very challenging problem, due to appearance confusions between keypoints of the two hands, and severe hand-hand occlusion. Due to their ability to capture long range relationships between keypoints, transformer-based methods have gained popularity in the research community. However, the existing methods usually deploy tokens at keypoint level, which inevitably results in high computational and memory complexity. In this paper, we propose a simple yet novel mechanism, i.e., hand-level tokenization, in our transformer based model, where we deploy only one token for each hand. With this novel design, we also propose a pose query enhancer module, which can refine the pose prediction iteratively, by focusing on features guided by previous coarse pose predictions. As a result, our proposed model, Handformer2T, can achieve high performance while maintaining lightweight. Extensive experiments on public benchmarks demonstrate that our model can achieve state-of-the-art performance on interacting-hand pose estimation with higher throughput, less memory and faster speed."
Hard Sample-Aware Consistency for Low-Resolution Facial Expression Recognition,"Bokyeung Lee, Kyungdeuk Ko, Jonghwan Hong, Hanseok Ko","Department of Electrical and Computer Engineering, Korea University",100.0,South Korea,0.0,,"Facial expression recognition (FER) plays a pivotal role in computer vision applications, encompassing video understanding and human-computer interaction. Despite notable advancements in FER, performance still falters when handling low-resolution facial images encountered in real-world scenarios and datasets. While consistency constraint techniques have garnered attention for generating robust convolutional neural network models that accommodate input variations through augmentation, their efficacy is diminished in the realm of low-resolution FER. This decline in performance can be attributed to augmented samples that networks struggle to extract expressive features. In this paper, we identify hard samples that cause an overfitting problem when considering various degrees of resolution and propose novel hard sample-aware consistency (HSAC) loss functions, which include combined attention consistency and label distribution learning. The combined attention consistency aligns an attention map from multi-scale low-resolution images with an appropriate target attention map by combining activation maps from high-resolution and flipped low-resolution images. We measure the classification difficulty for low-resolution face images and adaptively apply label distribution learning by combining the original target and predictions of high-resolution input. Our HSAC empowers the network to achieve generalization by effectively managing hard samples. Extensive experiments on various FER datasets demonstrate the superiority of our proposed method over existing approaches for multi-scale low-resolution images. Furthermore, we achieved a new state-of-the-art performance of 90.97% on the original RAF-DB dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Hard_Sample-Aware_Consistency_for_Low-Resolution_Facial_Expression_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Hard_Sample-Aware_Consistency_for_Low-Resolution_Facial_Expression_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483694/,"['Human computer interaction', 'Computer vision', 'Image resolution', 'Image recognition', 'Face recognition', 'Computational modeling', 'Feature extraction']","['Facial Expressions', 'Face Recognition', 'Facial Expression Recognition', 'Loss Function', 'Convolutional Neural Network', 'Artificial Neural Network', 'High-resolution Images', 'Human-computer Interaction', 'Activation Maps', 'Face Images', 'Low-resolution Images', 'Attention Map', 'Original Target', 'Consistency Loss', 'Multi-scale Image', 'Consistency Constraint', 'Classification Difficulty', 'Robustness Of Neural Networks', 'Distribution Characteristics', 'Feature Maps', 'Downsampling Factor', 'Class Activation Maps', 'Left Figure', 'Noisy Labels', 'Label Noise', 'Discriminative Features', 'Horizontal Flip', 'Real Scenarios', 'Generative Adversarial Networks', 'Original Label']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Psychology and cognitive science']",2,"Facial expression recognition (FER) plays a pivotal role in computer vision applications, encompassing video understanding and human-computer interaction. Despite notable advancements in FER, performance still falters when handling low-resolution facial images encountered in real-world scenarios and datasets. While consistency constraint techniques have garnered attention for generating robust convolutional neural network models that accommodate input variations through augmentation, their efficacy is diminished in the realm of low-resolution FER. This decline in performance can be attributed to augmented samples that networks struggle to extract expressive features. In this paper, we identify hard samples that cause an overfitting problem when considering various degrees of resolution and propose novel hard sample-aware consistency (HSAC) loss functions, which include combined attention consistency and label distribution learning. The combined attention consistency aligns an attention map from multi-scale low-resolution images with an appropriate target attention map by combining activation maps from high-resolution and flipped low-resolution images. We measure the classification difficulty for low-resolution face images and adaptively apply label distribution learning by combining the original target and predictions of high-resolution input. Our HSAC empowers the network to achieve generalization by effectively managing hard samples. Extensive experiments on various FER datasets demonstrate the superiority of our proposed method over existing approaches for multiscale low-resolution images. Furthermore, we achieved a new state-of-the-art performance of 90.97% on the original RAF-DB dataset."
Hard-Label Based Small Query Black-Box Adversarial Attack,"Jeonghwan Park, Paul Miller, Niall McLaughlin","Queen’s University Belfast, United Kingdom",100.0,Canada,0.0,,"We consider the hard-label based black-box adversarial attack setting which solely observes the target model's predicted class. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white-box surrogate models and black-box target model. However, the majority of the methods adopting this approach are soft-label based to take the full advantage of zeroth-order optimisation. Unlike mainstream methods, we propose a new practical setting of hard-label based attack with an optimisation process guided by a pre-trained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard-label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rate compared to the benchmarks, especially at the small query budgets as 100 and 250.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Hard-Label_Based_Small_Query_Black-Box_Adversarial_Attack_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Hard-Label_Based_Small_Query_Black-Box_Adversarial_Attack_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484251/,"['Computer vision', 'Computational modeling', 'Closed box', 'Computer architecture', 'Benchmark testing', 'Predictive models', 'Prediction algorithms']","['Adversarial Attacks', 'Small Query', 'Black-box Adversarial Attack', 'Benchmark', 'Alternative Models', 'Optimization Process', 'Target Model', 'Attack Success', 'Attack Methods', 'Attack Success Rate', 'Black-box Attacks', 'Set Of Attacks', 'Deep Neural Network', 'Scaling Factor', 'Object Classification', 'Gradient Approximation', 'Gradient Direction', 'Decision Boundary', 'Deep Neural Network Model', 'ImageNet Dataset', 'Adversarial Examples', 'Boolean Function', 'White-box Attack', 'Benchmark Methods', 'Attack Performance', 'Perturbation Vector', 'Tuning Process', 'True Class']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",1,"We consider the hard-label based black-box adversarial attack setting which solely observes the target model’s predicted class. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the adversarial transferability between white-box surrogate models and black-box target model. However, the majority of the methods adopting this approach are soft-label based to take the full advantage of zeroth-order optimisation. Unlike mainstream methods, we propose a new practical setting of hard-label based attack with an optimisation process guided by a pre-trained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard-label based black-box attack across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rate compared to the benchmarks, especially at the small query budgets as 100 and 250."
Hardware Aware Evolutionary Neural Architecture Search Using Representation Similarity Metric,"Nilotpal Sinha, Abd El Rahman Shabayek, Anis Kacem, Peyman Rostami, Carl Shneider, Djamila Aouada","SnT, University of Luxembourg",100.0,Luxembourg,0.0,,"Hardware-aware Neural Architecture Search (HW-NAS) is a technique used to automatically design the architecture of a neural network for a specific task and target hardware. However, evaluating the performance of candidate architectures is a key challenge in HW-NAS, as it requires significant computational resources. To address this challenge, we propose an efficient hardware-aware evolution-based NAS approach called HW-EvRSNAS. Our approach re-frames the neural architecture search problem as finding an architecture with performance similar to that of a reference model for a target hardware, while adhering to a cost constraint for that hardware. This is achieved through a representation similarity metric known as Representation Mutual Information (RMI) employed as a proxy performance evaluator. It measures the mutual information between the hidden layer representations of a reference model and those of sampled architectures using a single training batch. We also use a penalty term that penalizes the search process in proportion to how far an architecture's hardware cost is from the desired hardware cost threshold. This resulted in a significantly reduced search time compared to the literature that reached up to 8000x speedups resulting in lower CO2 emissions. The proposed approach is evaluated on two different search spaces while using lower computational resources. Furthermore, our approach is thoroughly examined on six different edge devices under various hardware cost constraints.",https://openaccess.thecvf.com/content/WACV2024/html/Sinha_Hardware_Aware_Evolutionary_Neural_Architecture_Search_Using_Representation_Similarity_Metric_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sinha_Hardware_Aware_Evolutionary_Neural_Architecture_Search_Using_Representation_Similarity_Metric_WACV_2024_paper.pdf,,,2311.03923,main,Poster,https://ieeexplore.ieee.org/document/10484380/,"['Measurement', 'Training', 'Computer vision', 'Costs', 'Neural networks', 'Computer architecture', 'Search problems']","['Similarity Measure', 'Representational Similarity', 'Neural Architecture Search', 'Neural Network', 'Greenhouse Gas', 'Computational Resources', 'Search Space', 'Mutual Information', 'Reference Model', 'Penalty Term', 'Single Batch', 'Hardware Cost', 'Edge Devices', 'Specific Hardware', 'Representation Layer', 'Hidden Representation', 'Lower CO2 Emissions', 'Deep Neural Network', 'Performance Metrics', 'Search Algorithm', 'Hardware Constraints', 'Cost Metrics', 'Latency Constraints', 'Rejection Sampling', 'Search Costs', 'Representation Of Architecture', 'Raspberry Pi', 'Lower Carbon Footprint', 'Internet Of Things', 'Search Method']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Hardware-aware Neural Architecture Search (HW-NAS) is a technique used to automatically design the architecture of a neural network for a specific task and target hardware. However, evaluating the performance of candidate architectures is a key challenge in HW-NAS, as it requires significant computational resources. To address this challenge, we propose an efficient hardware-aware evolution-based NAS approach called HW-EvRSNAS. Our approach re-frames the neural architecture search problem as finding an architecture with performance similar to that of a reference model for a target hardware, while adhering to a cost constraint for that hardware. This is achieved through a representation similarity metric known as Representation Mutual Information (RMI) employed as a proxy performance evaluator. It measures the mutual information between the hidden layer representations of a reference model and those of sampled architectures using a single training batch. We also use a penalty term that penalizes the search process in proportion to how far an architecture’s hardware cost is from the desired hardware cost threshold. This resulted in a significantly reduced search time compared to the literature that reached up to 8000× speedups resulting in lower CO
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf>
 emissions. The proposed approach is evaluated on two different search spaces while using lower computational resources. Furthermore, our approach is thoroughly examined on six different edge devices under various hardware cost constraints."
Harnessing the Power of Multi-Lingual Datasets for Pre-Training: Towards Enhancing Text Spotting Performance,"Alloy Das, Sanket Biswas, Ayan Banerjee, Josep Lladós, Umapada Pal, Saumik Bhattacharya","CVPR Unit, Indian Statistical Institute, Kolkata; ECE, Indian Institute of Technology, Kharagpur; CVC, Universitat Aut `onoma de Barcelona",100.0,"India, Spain",0.0,,"The adaptation capability to a wide range of domains is crucial for scene text spotting models when deployed to real-world conditions. However, existing state-of-the-art approaches usually incorporate scene text detection and recognition simply by pretraining on natural scene image datasets, which do not directly exploit the feature interaction between multiple domains. In this work, we investigate the problem of domain-adapted scene text spotting, i.e., training a model on multi-domain source data such that it can directly adapt to target domains rather than being specialized for a specific domain or scenario. Further, we investigate a transformer baseline called Swin-TESTR to focus on solving scene-text spotting for both regular (ICDAR2015) and arbitrary-shaped scene text (CTW1500, TotalText) along with an exhaustive evaluation. The results clearly demonstrate the potential of intermediate representations on text spotting benchmarks across multiple domains (e.g. language, synth to real, and documents) both in terms of accuracy and model efficiency.",https://openaccess.thecvf.com/content/WACV2024/html/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Das_Harnessing_the_Power_of_Multi-Lingual_Datasets_for_Pre-Training_Towards_Enhancing_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483782/,"['Training', 'Adaptation models', 'Computer vision', 'Text recognition', 'Text detection', 'Benchmark testing', 'Transformers']","['Bilingual', 'Pre-training Dataset', 'Multilingual Dataset', 'Text Spotting', 'Feature Representation', 'Target Domain', 'Optical Character Recognition', 'Intermediate Representation', 'Recognition Task', 'Domain Shift', 'Performance Gain', 'Complex Datasets', 'Edge Length', 'Input Space', 'Domain Adaptation', 'Source Domain', 'Output Space', 'Benchmark Evaluation', 'Real Domain', 'Short Edges', 'Extract Visual Features', 'Transformer Decoder']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",2,"The adaptation capability to a wide range of domains is crucial for scene text spotting models when deployed to real-world conditions. However, existing SOTA approaches usually incorporate scene text detection and recognition simply by pretraining on natural scene text datasets, which do not directly exploit the intermediate feature representations between multiple domains. Here, we investigate the problem of domain-adaptive scene text spotting, i.e., training a model on multi-domain source data such that it can directly adapt to target domains rather than being specialized for a specific domain or scenario. Further, we investigate a transformer baseline called Swin-TESTR to focus on solving scene-text spotting for both regular and arbitraryshaped text along with an exhaustive evaluation. The results demonstrate the potential of intermediate representations to gain significant performance on text spotting benchmarks across multiple domains (e.g. language, synth-to-real, and documents). both in terms of accuracy and efficiency."
HashReID: Dynamic Network With Binary Codes for Efficient Person Re-Identification,"Kshitij Nikhal, Yujunrong Ma, Shuvra S. Bhattacharyya, Benjamin S. Riggan","University of Maryland, College Park, 8223 Paint Branch Dr, College Park, MD 20742; University of Nebraska-Lincoln, 1400 R St, Lincoln, NE 68588",100.0,USA,0.0,,"Biometric applications, such as person re-identification (ReID), are often deployed on energy constrained devices. While recent ReID methods prioritize high retrieval performance, they often come with large computational costs and high search time, rendering them less practical in real-world settings. In this work, we propose an input-adaptive network with multiple exit blocks, that can terminate computation early if the retrieval is straightforward or noisy, saving a lot of computation. To assess the complexity of the input, we introduce a temporal-based classifier driven by a new training strategy.  Furthermore, we adopt a binary hash code generation approach instead of relying on continuous-valued features, which significantly improves the search process by a factor of 20. To ensure similarity preservation, we utilize a new ranking regularizer that bridges the gap between continuous and binary features. Extensive analysis of our proposed method is conducted on three datasets: Market1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government Collection). Using our approach, more than 70% of the samples with compact hash codes exit early on the Market1501 dataset, saving 80% of the networks computational cost and improving over other hash-based methods by 60%. These results demonstrate a significant improvement over dynamic networks and showcase comparable accuracy performance to conventional ReID methods.",https://openaccess.thecvf.com/content/WACV2024/html/Nikhal_HashReID_Dynamic_Network_With_Binary_Codes_for_Efficient_Person_Re-Identification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nikhal_HashReID_Dynamic_Network_With_Binary_Codes_for_Efficient_Person_Re-Identification_WACV_2024_paper.pdf,,,2308.11900,main,Poster,https://ieeexplore.ieee.org/document/10483895/,"['Training', 'Performance evaluation', 'Computer vision', 'Biometrics (access control)', 'Government', 'Binary codes', 'Rendering (computer graphics)']","['Dynamic Network', 'Binary Code', 'Computational Cost', 'Hash Function', 'Continuous Features', 'Complex Input', 'Large Computational Cost', 'Re-identification Methods', 'Training Set', 'Feature Representation', 'Spatial Dimensions', 'Unmanned Aerial Vehicles', 'Hyperbolic Tangent', 'Whole-body Imaging', 'Batch Normalization Layer', 'Elastic Net', 'Sign Function', 'Hamming Distance', 'Gated Recurrent Unit', 'Person Image', 'Query Sample', 'Stage Of Network', 'High-dimensional Representation', 'Triplet Loss', 'Score Map', 'Gallery Images', 'Early Layers', 'Difficulty Of Sampling']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Applications', 'Embedded sensing / real-time techniques']",2,"Biometric applications, such as person re-identification (ReID), are often deployed on energy constrained devices. While recent ReID methods prioritize high retrieval performance, they often come with large computational costs and high search time, rendering them less practical in real-world settings. In this work, we propose an input-adaptive network with multiple exit blocks, that can terminate computation early if the retrieval is straightforward or noisy, saving a lot of computation. To assess the complexity of the input, we introduce a temporal-based classifier driven by a new training strategy. Furthermore, we adopt a binary hash code generation approach instead of relying on continuous-valued features, which significantly improves the search process by a factor of 20. To ensure similarity preservation, we utilize a new ranking regularizer that bridges the gap between continuous and binary features. Extensive analysis of our proposed method is conducted on three datasets: Market1501, MSMT17 (Multi-Scene Multi-Time), and the BGC1 (BRIAR Government Collection). Using our approach, more than 70% of the samples with compact hash codes exit early on the Market1501 dataset, saving 80% of the networks computational cost and improving over other hash-based methods by 60%. These results demonstrate a significant improvement over dynamic networks and showcase comparable accuracy performance to conventional ReID methods."
Have We Ever Encountered This Before? Retrieving Out-of-Distribution Road Obstacles From Driving Scenes,"Youssef Shoeb, Robin Chan, Gesina Schwalbe, Azarm Nowzad, Fatma Güney, Hanno Gottschalk",Koc University; Technische Universit¨at Berlin; Bielefeld University; Continental AG,75.0,"Germany, Turkey",25.0,Germany,"In the life cycle of highly automated systems operating in an open and dynamic environment, the ability to adjust to emerging challenges is crucial. For systems integrating data-driven AI-based components, rapid responses to deployment issues require fast access to related data for testing and reconfiguration. In the context of automated driving, this especially applies to road obstacles that were not included in the training data, commonly referred to as out-of-distribution (OoD) road obstacles. Given the availability of large uncurated recordings of driving scenes, a pragmatic approach is to query a database to retrieve similar scenarios featuring the same safety concerns due to OoD road obstacles. In this work, we extend beyond identifying OoD road obstacles in video streams and offer a comprehensive approach to extract sequences of OoD road obstacles using text queries, thereby proposing a way of curating a collection of OoD data for subsequent analysis. Our proposed method leverages the recent advances in OoD segmentation and multi-modal foundation models to identify and efficiently extract safety-relevant scenes from unlabeled videos. We present a first approach for the novel task of text-based OoD object retrieval, which addresses the question ""Have we ever encountered this before?"".",https://openaccess.thecvf.com/content/WACV2024/html/Shoeb_Have_We_Ever_Encountered_This_Before_Retrieving_Out-of-Distribution_Road_Obstacles_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shoeb_Have_We_Ever_Encountered_This_Before_Retrieving_Out-of-Distribution_Road_Obstacles_WACV_2024_paper.pdf,,,2309.04302,main,Poster,https://ieeexplore.ieee.org/document/10483586/,"['Image segmentation', 'Roads', 'Training data', 'Streaming media', 'Safety', 'Recording', 'Task analysis']","['Segmentation Model', 'Foundation Model', 'Text Query', 'Intersection Over Union', 'Segmentation Method', 'Final Prediction', 'Latent Space', 'Semantic Segmentation', 'Single Frame', 'Tracking Performance', 'Consecutive Frames', 'Video Sequences', 'Sequence Of Frames', 'Image Retrieval', 'Sequence Retrieval', 'Retrieval Performance', 'Query Image', 'Semantic Segmentation Models', 'Unknown Objects', 'Sequence Of Objects', 'Multiple Object Tracking', 'Perfect Detection', 'Content-based Image Retrieval', 'False Positive', 'Tracking Module', 'Crop Rotation', 'Visual Similarity', 'Autonomous Vehicles', 'Object Tracking', 'Tracking Information']","['Applications', 'Autonomous Driving', 'Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"In the life cycle of highly automated systems operating in an open and dynamic environment, the ability to adjust to emerging challenges is crucial. For systems integrating data-driven AI-based components, rapid responses to deployment issues require fast access to related data for testing and reconfiguration. In the context of automated driving, this especially applies to road obstacles not included in the training data, commonly referred to as out-of-distribution (OoD) road obstacles. Given the availability of large uncurated driving scene recordings, a pragmatic approach is to query a database to retrieve similar scenarios featuring the same safety concerns due to OoD road obstacles. In this work, we extend beyond identifying OoD road obstacles in video streams and offer a comprehensive approach to extract sequences of OoD road obstacles using text queries, thereby proposing a way of curating a collection of OoD data for subsequent analysis. Our proposed method leverages the recent advances in OoD segmentation and multi-modal foundation models to identify and efficiently extract safety-relevant scenes from unlabeled videos. We present a first approach for the novel task of text-based OoD object retrieval, which addresses the question ""Have we ever encountered this before?""."
Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation,"Zeyu Lu, Chengyue Wu, Xinyuan Chen, Yaohui Wang, Lei Bai, Yu Qiao, Xihui Liu",The University of Hong Kong; Shanghai Artificial Intelligence Laboratory; Shanghai Jiao Tong University,66.66666666666666,"China, Hong Kong",33.33333333333334,China,"Diffusion models have attained impressive visual quality for image synthesis. However, how to interpret and manipulate the latent space of diffusion models has not been extensively explored. Prior work diffusion autoencoders encode the semantic representations into a semantic latent code, which fails to reflect the rich information of details and the intrinsic feature hierarchy. To mitigate those limitations, we propose Hierarchical Diffusion Autoencoders (HDAE) that exploit the fine-grained-to-abstract and low-level-to-high-level feature hierarchy for the latent space of diffusion models. The hierarchical latent space of HDAE inherently encodes different abstract levels of semantics and provides more comprehensive semantic representations. In addition, we propose a truncated-feature-based approach for disentangled image manipulation. We demonstrate the effectiveness of our proposed approach with extensive experiments and applications on image reconstruction, style mixing, controllable interpolation, detail-preserving and disentangled image manipulation, and multi-modal semantic image synthesis.",https://openaccess.thecvf.com/content/WACV2024/html/Lu_Hierarchical_Diffusion_Autoencoders_and_Disentangled_Image_Manipulation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lu_Hierarchical_Diffusion_Autoencoders_and_Disentangled_Image_Manipulation_WACV_2024_paper.pdf,,,2304.11829,main,Poster,https://ieeexplore.ieee.org/document/10483780/,"['Interpolation', 'Visualization', 'Computer vision', 'Codes', 'Image synthesis', 'Semantics', 'Aerospace electronics']","['Use Of Imaging', 'Image Reconstruction', 'Extensive Experiments', 'Level Characteristics', 'Diffusion Model', 'Space Model', 'Latent Space', 'Multimodal Imaging', 'Semantic Representations', 'Image Synthesis', 'Image Editing', 'Latent Code', 'Different Levels Of Features', 'Feature Maps', 'Statistical Distribution', 'Perception Of Quality', 'High-level Features', 'Low-level Features', 'Single Vector', 'Linear Classifier', 'Semantic Vectors', 'Disentangled Representation', 'Reconstructed Image Quality', 'Latent Representation', 'U-Net Structure', 'Forward Process', 'Single Feature Vector']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Arts / games / social media']",,"Diffusion models have attained impressive visual quality for image synthesis. However, how to probe and manipulate the latent space of diffusion models has not been extensively explored. Prior work diffusion autoencoders encode the semantic representations with a single latent code, neglecting the low-level details and leading to entangled representations. To mitigate those limitations, we propose Hierarchical Diffusion Autoencoders (HDAE) that exploits the coarse-to-fine feature hierarchy for the latent space of diffusion models. Our HDAE converges 2+ times faster and encodes richer and more comprehensive coarse-to-fine representations of images. The hierarchical latent space inherently disentangles different semantic levels of features. Furthermore, we propose a truncated feature based approach for disentangled image manipulation. We demonstrate the effectiveness of our proposed HDAE with extensive experiments and applications on image reconstruction, style mixing, controllable interpolation, image editing, and multi-modal semantic image synthesis. The code will be released upon acceptance."
Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis,"Shangbang Long, Siyang Qin, Yasuhisa Fujii, Alessandro Bissacco, Michalis Raptis",Google Research,0.0,,100.0,USA,"We propose Hierarchical Text Spotter (HTS), a novel method for the joint task of word-level text spotting and geometric layout analysis. HTS can recognize text in an image and identify its 4-level hierarchical structure: characters, words, lines, and paragraphs. The proposed HTS is characterized by two novel components: (1) a Unified-Detector-Polygon (UDP) that produces Bezier Curve polygons of text lines and an affinity matrix for paragraph grouping between detected lines; (2) a Line-to-Character-to-Word (L2C2W) recognizer that splits lines into characters and further merges them back into words. HTS achieves state-of-the-art results on multiple word-level text spotting benchmark datasets as well as geometric layout analysis tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Long_Hierarchical_Text_Spotter_for_Joint_Text_Spotting_and_Layout_Analysis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Long_Hierarchical_Text_Spotter_for_Joint_Text_Spotting_and_Layout_Analysis_WACV_2024_paper.pdf,,,2310.17674,main,Poster,https://ieeexplore.ieee.org/document/10483893/,"['High-temperature superconductors', 'Computer vision', 'Image recognition', 'Text recognition', 'Layout', 'Benchmark testing', 'Character recognition']","['Spot Analysis', 'Layout Analysis', 'Text Spotting', 'Hierarchical Text', 'Recognizable', 'Multiple Datasets', 'Analysis Tasks', 'Image Texture', 'Geometric Analysis', 'Text Lines', 'Affinity Matrix', 'Bezier Curve', 'Aspect Ratio', 'Input Image', 'Bounding Box', 'Control Points', 'Feed-forward Network', 'Fraction Of Data', 'Minimum Bounding Box', 'Prediction Head', 'Word Level', 'Optical Character Recognition', 'Groups Of Words', 'L1 Loss', 'Bounding Box Annotations', 'Quality Of Space', 'Target Dataset', 'Transformer Decoder']","['Algorithms', 'Image recognition and understanding']",,"We propose Hierarchical Text Spotter (HTS), a novel method for the joint task of word-level text spotting and geometric layout analysis. HTS can recognize text in an image and identify its 4-level hierarchical structure: characters, words, lines, and paragraphs. The proposed HTS is characterized by two novel components: (1) a Unified-DetectorPolygon (UDP) that produces Bezier Curve polygons of text lines and an affinity matrix for paragraph grouping between detected lines; (2) a Line-to-Character-to-Word (L2C2W) recognizer that splits lines into characters and further merges them back into words. HTS achieves stateof-the-art results on multiple word-level text spotting benchmark datasets as well as geometric layout analysis tasks."
High-Fidelity Pseudo-Labels for Boosting Weakly-Supervised Segmentation,"Arvi Jonnarth, Yushan Zhang, Michael Felsberg","Husqvarna Group, Huskvarna, Sweden; University of KwaZulu-Natal, Durban, South Africa and Department of Electrical Engineering, Linkoping University, Sweden; Department of Electrical Engineering, Linkoping University, Sweden",66.66666666666666,"South Africa, Sweden",33.33333333333334,Sweden,"Image-level weakly-supervised semantic segmentation (WSSS) reduces the usually vast data annotation cost by surrogate segmentation masks during training. The typical approach involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. Our work is based on two techniques for improving CAMs; importance sampling, which is a substitute for GAP, and the feature similarity loss, which utilizes a heuristic that object contours almost always align with color edges in images. However, both are based on the multinomial posterior with softmax, and implicitly assume that classes are mutually exclusive, which turns out suboptimal in our experiments. Thus, we reformulate both techniques based on binomial posteriors of multiple independent binary problems. This has two benefits; their performance is improved and they become more general, resulting in an add-on method that can boost virtually any WSSS method. This is demonstrated on a wide variety of baselines on the PASCAL VOC dataset, improving the region similarity and contour quality of all implemented state-of-the-art methods. Experiments on the MS COCO dataset further show that our proposed add-on is well-suited for large-scale settings. Our code implementation is available at https://github.com/arvijj/hfpl.",https://openaccess.thecvf.com/content/WACV2024/html/Jonnarth_High-Fidelity_Pseudo-Labels_for_Boosting_Weakly-Supervised_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jonnarth_High-Fidelity_Pseudo-Labels_for_Boosting_Weakly-Supervised_Segmentation_WACV_2024_paper.pdf,,https://github.com/arvijj/hfpl,2304.02621,main,Poster,https://ieeexplore.ieee.org/document/10483720/,"['Training', 'Image segmentation', 'Monte Carlo methods', 'Image color analysis', 'Semantic segmentation', 'Image edge detection', 'Estimation']","['Weakly-supervised Segmentation', 'Mutually Exclusive', 'Softmax', 'Image Regions', 'Classification Network', 'Semantic Segmentation', 'Global Pooling', 'Global Average Pooling', 'Image Edge', 'Class Activation Maps', 'Object Contour', 'Edge Coloring', 'Convolutional Feature Maps', 'Image Pixels', 'Class Labels', 'Average Performance', 'Intersection Over Union', 'Training Images', 'Classification Score', 'Classification Loss', 'Global Max Pooling', 'Discriminative Regions', 'Segmentation Performance', 'Conditional Random Field', 'Final Loss', 'Image X', 'Test Set Performance', 'Image-level Labels', 'Gating Function', 'Weak Supervision']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Image-level weakly-supervised semantic segmentation (WSSS) reduces the usually vast data annotation cost by surrogate segmentation masks during training. The typical approach involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. Our work is based on two techniques for improving CAMs; importance sampling, which is a substitute for GAP, and the feature similarity loss, which utilizes a heuristic that object contours almost always align with color edges in images. However, both are based on the multinomial posterior with softmax, and implicitly assume that classes are mutually exclusive, which turns out suboptimal in our experiments. Thus, we reformulate both techniques based on binomial posteriors of multiple independent binary problems. This has two benefits; their performance is improved and they become more general, resulting in an add-on method that can boost virtually any WSSS method. This is demonstrated on a wide variety of baselines on the PASCAL VOC dataset, improving the region similarity and contour quality of all implemented stateof-the-art methods. Experiments on the MS COCO dataset further show that our proposed add-on is well-suited for large-scale settings. Our code implementation is available at https://github.com/arvijj/hfpl."
High-Fidelity Zero-Shot Texture Anomaly Localization Using Feature Correspondence Analysis,"Andrei-Timotei Ardelean, Tim Weyrich",Friedrich-Alexander-Universität Erlangen-Nürnberg,100.0,Germany,0.0,,"We propose a novel method for Zero-Shot Anomaly Localization on textures. The task refers to identifying abnormal regions in an otherwise homogeneous image. To obtain a high-fidelity localization, we leverage a bijective mapping derived from the 1-dimensional Wasserstein Distance. As opposed to using holistic distances between distributions, the proposed approach allows pinpointing the non-conformity of a pixel in a local context with increased precision. By aggregating the contribution of the pixel to the errors of all nearby patches we obtain a reliable anomaly score estimate. We validate our solution on several datasets and obtain more than a 40% reduction in error over the previous state of the art on the MVTec AD dataset in a zero-shot setting. Also see https://reality.tf.fau.de/pub/ardelean2024highfidelity.html.",https://openaccess.thecvf.com/content/WACV2024/html/Ardelean_High-Fidelity_Zero-Shot_Texture_Anomaly_Localization_Using_Feature_Correspondence_Analysis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ardelean_High-Fidelity_Zero-Shot_Texture_Anomaly_Localization_Using_Feature_Correspondence_Analysis_WACV_2024_paper.pdf,http://reality.tf.fau.de/pub/ardelean2024highfidelity.html,,2304.06433,main,Poster,https://ieeexplore.ieee.org/document/10484100/,"['Location awareness', 'Computer vision', 'Art', 'Reliability', 'Task analysis']","['Anomaly Localization', 'Anomaly Score', 'Sample Set', 'Performance Of Method', 'Running Time', 'Feature Maps', 'Single Image', 'K-nearest Neighbor', 'Receptive Field', 'Generative Adversarial Networks', 'Patch Size', 'Image Patches', 'Anomaly Detection', 'General Objective', 'Prior Art', 'Foundation Model', 'Fabric Texture', 'Anomaly Detection Methods', 'Histogram Method']","['Algorithms', 'Image recognition and understanding']",2,"We propose a novel method for Zero-Shot Anomaly Localization on textures. The task refers to identifying abnormal regions in an otherwise homogeneous image. To obtain a high-fidelity localization, we leverage a bijective mapping derived from the 1-dimensional Wasserstein Distance. As opposed to using holistic distances between distributions, the proposed approach allows pinpointing the non-conformity of a pixel in a local context with increased precision. By aggregating the contribution of the pixel to the errors of all nearby patches, we obtain a reliable anomaly score estimate. We validate our solution on several datasets and obtain more than a 40% reduction in error over the previous state of the art on the MVTec AD dataset in a zero-shot setting. Also see reality.tf.fau.de/pub/ardelean2024highfidelity.html."
Holistic Representation Learning for Multitask Trajectory Anomaly Detection,"Alexandros Stergiou, Brent De Weerdt, Nikos Deligiannis","Vrije Universiteit Brussel, Belgium & imec, Belgium",100.0,Belgium,0.0,,"Video anomaly detection deals with the recognition of abnormal events in videos. Apart from the visual signal, video anomaly detection has also been addressed with skeleton sequences. We propose a holistic representation of skeleton trajectories to learn expected motions across segments at different times. Our approach uses multitask learning to reconstruct any continuous unobserved temporal segment of the trajectory allowing the extrapolation of past and future segments and the interpolation of in-between segments. We use an end-to-end attention-based encoder-decoder to encode temporally occluded trajectories, jointly learn latent representations of the occluded trajectory segments, and reconstruct trajectories of expected motions across different temporal segments. Extensive experiments over three trajectory-based video anomaly detection datasets show the advantages and effectiveness of our method with state-of-the-art results on the detection of anomalies in skeleton trajectories",https://openaccess.thecvf.com/content/WACV2024/html/Stergiou_Holistic_Representation_Learning_for_Multitask_Trajectory_Anomaly_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Stergiou_Holistic_Representation_Learning_for_Multitask_Trajectory_Anomaly_Detection_WACV_2024_paper.pdf,,https://github.com/alexandrosstergiou/TrajREC,2311.01851,main,Poster,https://ieeexplore.ieee.org/document/10483598/,"['Representation learning', 'Visualization', 'Interpolation', 'Extrapolation', 'Computer vision', 'Tensors', 'Motion segmentation']","['Representation Learning', 'Anomaly Detection', 'Holistic Representation', 'Interpolation', 'Latent Representation', 'Multi-task Learning', 'Instance Segmentation', 'Representative Trajectories', 'Trajectory Segments', 'Continuous Segments', 'Video Events', 'Decoding', 'Normal Behavior', 'Bounding Box', 'Latent Space', 'Input Space', 'Temporal Localization', 'Trajectory Length', 'Time Ti', 'AUC Score', 'Past Trajectories', 'Normal Segments', 'Part Of Trajectory', 'Ground Truth Trajectory', 'Normal Trajectory', 'Test Videos', 'Understanding Of Trajectories']","['Algorithms', 'Video recognition and understanding']",2,"Video anomaly detection deals with the recognition of abnormal events in videos. Apart from the visual signal, video anomaly detection has also been addressed with the use of skeleton sequences. We propose a holistic representation of skeleton trajectories to learn expected motions across segments at different times. Our approach uses multitask learning to reconstruct any continuous unobserved temporal segment of the trajectory allowing the extrapolation of past or future segments and the interpolation of in-between segments. We use an end-to-end attention-based encoder-decoder. We encode temporally occluded trajectories, jointly learn latent representations of the occluded segments, and reconstruct trajectories based on expected motions across different temporal segments. Extensive experiments on three trajectory-based video anomaly detection datasets show the advantages and effectiveness of our approach with state-of-the-art results on anomaly detection in skeleton trajectories
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">†</sup>
."
How Do Deepfakes Move? Motion Magnification for Deepfake Source Detection,"Ilke Demir, Umur Aybars Çiftçi","Intel Labs, Santa Clara, CA; Binghamton University, Binghamton, NY",50.0,USA,50.0,USA,"With the proliferation of deep generative models, deepfakes are improving in quality and quantity everyday. However, there are subtle authenticity signals in pristine videos, not replicated by current generative models. We contrast the movement in deepfakes and authentic videos by motion magnification towards building a generalized deepfake source detector. The sub-muscular motion in faces has different interpretations per different generative models, which is reflected in their generative residue. Our approach exploits the difference between real motion and the amplified generative artifacts, by combining deep and traditional motion magnification, to detect whether a video is fake and its source generator if so. Evaluating our approach on two multi-source datasets, we obtain 97.77% and 94.03% for video source detection. Our approach performs at least 4.08% better than the prior deepfake source detector and other complex architectures. We also analyze magnification amount, phase extraction window, backbone network, sample counts, and sample lengths. Finally, we report our results on skin tones and genders to assess the model bias.",https://openaccess.thecvf.com/content/WACV2024/html/Demir_How_Do_Deepfakes_Move_Motion_Magnification_for_Deepfake_Source_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Demir_How_Do_Deepfakes_Move_Motion_Magnification_for_Deepfake_Source_Detection_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484238/,"['Deepfakes', 'Computer vision', 'Computational modeling', 'Buildings', 'Detectors', 'Computer architecture', 'Skin']","['Skin Color', 'Real Motion', 'Deep Generative Models', 'Source Detector', 'Neural Network', 'Complex Network', 'Detection Accuracy', 'Motion Artifacts', 'Generative Adversarial Networks', 'Phase Variation', 'Motion Parameters', 'Motion Patterns', 'Human Motion', 'Large Motion', 'Face Detection', 'Video Dataset', 'Adversarial Attacks', 'Shape Representation', 'Real Class', 'Dual Representation', 'Real Videos']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"With the proliferation of deep generative models, deepfakes are improving in quality and quantity everyday. However, there are subtle authenticity signals in pristine videos, not replicated by current generative models. We contrast the movement in deepfakes and authentic videos by motion magnification towards building a generalized deepfake source detector. The sub-muscular motion in faces has different interpretations per different generative models, which is reflected in their generative residue. Our approach exploits the difference between real motion and the amplified generative artifacts, by combining deep and traditional motion magnification, to detect whether a video is fake and its source generator if so. Evaluating our approach on two multi-source datasets, we obtain 97.77% and 94.03% for video source detection. Our approach performs at least 4.08% better than the prior deepfake source detector and other complex architectures. We also analyze magnification amount, phase extraction window, backbone network, sample counts, and sample lengths. Finally, we report our results on skin tones and genders to assess the model bias."
Human Motion Aware Text-to-Video Generation With Explicit Camera Control,"Taehoon Kim, ChanHee Kang, JaeHyuk Park, Daun Jeong, ChangHee Yang, Suk-Ju Kang, Kyeongbo Kong",Pukyong National University; Sogang University; Pusan National University,100.0,South Korea,0.0,,"With the rise in expectations related to generative models, text-to-video (T2V) models are being actively studied. Existing text-to-video models have limitations such as in generating complex movements replicating human motions. These model often generate unintended human motions, and the scale of the subject is incorrect. To overcome these limitations and generate high-quality videos that depict human motion under plausible viewing angles, we propose a two stage framework in this study. In the first stage a text-driven human motion generation network generates three-dimensional (3D) human motion from input text prompts and then motion-to-skeleton projection module projects generated motions onto a two-dimensional (2D) skeleton. In the second stage, the projected skeletons are used to generate a video in which the movements of a subject are well-represented. We demonstrated that the proposed framework quantitatively and qualitatively outperforms the existing T2V models. Previously reported human motion generation models use texts only or texts and human skeletons. However, our framework only uses texts and outputs a video related to human motion. Moreover, our framework benefits from using skeleton as an additional condition in the text-to-human motion generation networks. To the best of our knowledge, our framework is the first of its kind that uses text-driven human motion generation networks to generate high-quality videos related to human motions. The corresponding codes are available at https://github.com/CSJasper/HMTV.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Human_Motion_Aware_Text-to-Video_Generation_With_Explicit_Camera_Control_WACV_2024_paper.pdf,,https://github.com/CSJasper/HMTV,,main,Poster,https://ieeexplore.ieee.org/document/10484108/,"['Knowledge engineering', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Two-dimensional displays', 'Punching', 'Cameras']","['Human Motion', 'Human Bone', 'Viewing Angle', 'Subjective Scale', 'Input Text', 'Subject Movement', 'Motion Generation', 'Quantitative Results', 'Qualitative Results', 'Diffusion Model', 'Lateral View', 'Top View', 'Projection Matrix', 'Scale Of The Problem', 'Action Classes', 'Variational Autoencoder', 'Camera Position', 'Generation Stage', '2D Projection', '3D Motion', 'Video Quality', 'Video Output', 'Homogeneous Coordinates', 'Camera Movement']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Vision + language and/or other modalities']",,"With the rise in expectations related to generative models, text-to-video (T2V) models are being actively studied. Existing text-to-video models have limitations such as in generating complex movements replicating human motions. These model often generate unintended human motions, and the scale of the subject is incorrect. To overcome these limitations and generate high-quality videos that depict human motion under plausible viewing angles, we propose a two stage framework in this study. In the first stage a text-driven human motion generation network generates three-dimensional (3D) human motion from input text prompts and then motion-to-skeleton projection module projects generated motions onto a two-dimensional (2D) skeleton. In the second stage, the projected skeletons are used to generate a video in which the movements of a subject are well-represented. We demonstrated that the proposed framework quantitatively and qualitatively outperforms the existing T2V models. Previously reported human motion generation models use texts only or texts and human skeletons. However, our framework only uses texts and outputs a video related to human motion. Moreover, our framework benefits from using skeleton as an additional condition in the text-to-human motion generation networks. To the best of our knowledge, our framework is the first of its kind that uses text-driven human motion generation networks to generate high-quality videos related to human motions. The corresponding codes are available at https://github.com/CSJasper/HMTV."
Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields,"Yifan Wang, Yi Gong, Yuan Zeng",SUSTech,0.0,,100.0,China,"Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity scene reconstruction for novel view synthesis. However, NeRF requires hundreds of network evaluations per pixel to approximate a volume rendering integral, making it slow to train. Caching NeRFs into explicit data structures can effectively enhance rendering speed but at the cost of higher memory usage. To address these issues, we present Hyb-NeRF, a novel neural radiance field with a multi-resolution hybrid encoding that achieves efficient neural modeling and fast rendering, which also allows for high-quality novel view synthesis. The key idea of Hyb-NeRF is to represent the scene using different encoding strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits coherence and compact memory of learnable positional features at coarse resolutions and the fast optimization speed and local details of hash-based feature grids at fine resolutions. In addition, to further boost performance, we embed cone tracing-based Fourier features in our learnable positional encoding that eliminates encoding ambiguity and reduces aliasing artifacts. Extensive experiments on both synthetic and real-world datasets show that Hyb-NeRF achieves faster rendering speed with better rending quality and even a lower memory footprint in comparison to previous state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Hyb-NeRF_A_Multiresolution_Hybrid_Encoding_for_Neural_Radiance_Fields_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Hyb-NeRF_A_Multiresolution_Hybrid_Encoding_for_Neural_Radiance_Fields_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483678/,"['Computer vision', 'Costs', 'Computational modeling', 'Memory management', 'Rendering (computer graphics)', 'Data structures', 'Encoding']","['Radiance Field', 'Neural Radiance Fields', 'Hybrid Encoding', 'Data Structure', 'Finer Resolution', 'Level Of Resolution', 'Position Features', 'Memory Footprint', 'Positional Encoding', 'Neural Field', 'View Synthesis', 'Encoding Strategies', 'Grid Features', 'High-dimensional', 'Training Time', 'Multilayer Perceptron', 'Trainable Parameters', 'Final Level', 'Neural Representations', 'Hash Function', 'Learned Weights', 'Scene Representation', 'Voxel Grid', 'Real-world Scenes', 'Peak Signal-to-noise Ratio', 'Camera Pose', 'View Direction', 'Scene Geometry', 'High-resolution Grid', 'Scene Details']","['Algorithms', '3D computer vision']",1,"Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity scene reconstruction for novel view synthesis. However, NeRF requires hundreds of network evaluations per pixel to approximate a volume rendering integral, making it slow to train. Caching NeRFs into explicit data structures can effectively enhance rendering speed but at the cost of higher memory usage. To address these issues, we present Hyb-NeRF, a novel neural radiance field with a multi-resolution hybrid encoding that achieves efficient neural modeling and fast rendering, which also allows for high-quality novel view synthesis. The key idea of Hyb-NeRF is to represent the scene using different encoding strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits memory-efficiency learnable positional features at coarse resolutions and the fast optimization speed and local details of hash-based feature grids at fine resolutions. In addition, to further boost performance, we embed cone tracing-based features in our learnable positional encoding that eliminates encoding ambiguity and reduces aliasing artifacts. Extensive experiments on both synthetic and real-world datasets show that Hyb-NeRF achieves faster rendering speed with better rending quality and even a lower memory footprint in comparison to previous state-of-the-art methods."
Hybrid Neural Diffeomorphic Flow for Shape Representation and Generation via Triplane,"Kun Han, Shanlin Sun, Thanh-Tung Le, Xiangyi Yan, Haoyu Ma, Chenyu You, Xiaohui Xie","University of California, Irvine, USA; Yale University, USA",100.0,USA,0.0,,"Deep Implicit Functions (DIFs) have gained popularity in 3D computer vision due to their compactness and continuous representation capabilities. However, addressing dense correspondences and semantic relationships across DIF-encoded shapes remains a critical challenge, limiting their applications in texture transfer and shape analysis. Moreover, recent endeavors in 3D shape generation using DIFs often neglect correspondence and topology preservation. This paper presents HNDF (Hybrid Neural Diffeomorphic Flow), a method that implicitly learns the underlying representation and decomposes intricate dense correspondences into explicitly axis-aligned triplane features. To avoid suboptimal representations trapped in local minima, we propose hybrid supervision that captures both local and global correspondences. Unlike conventional approaches that directly generate new 3D shapes, we further explore the idea of shape generation with deformed template shape via diffeomorphic flows, where the deformation is encoded by the generated triplane features. Leveraging a pre-existing 2D diffusion model, we produce high-quality and diverse 3D diffeomorphic flows through generated triplanes features, ensuring topological consistency with the template shape. Extensive experiments on medical image organ segmentation datasets evaluate the effectiveness of HNDF in 3D shape representation and generation.",https://openaccess.thecvf.com/content/WACV2024/html/Han_Hybrid_Neural_Diffeomorphic_Flow_for_Shape_Representation_and_Generation_via_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Han_Hybrid_Neural_Diffeomorphic_Flow_for_Shape_Representation_and_Generation_via_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483710/,"['Computer vision', 'Solid modeling', 'Image segmentation', 'Three-dimensional displays', 'Limiting', 'Shape', 'Deformation']","['Shape Representation', 'Diffusion Model', '3D Shape', 'Medical Datasets', 'Representation Capability', 'Dense Correspondence', 'Random Noise', 'Ordinary Differential Equations', 'Point Cloud', 'Representative Methods', 'Corresponding Points', 'Entire Space', '2D Plane', '3D Mesh', 'Local Deformation', 'Reconstruction Loss', 'Latent Vector', 'Shape Space', 'Global Vector', 'Destination Point', 'Fréchet Inception Distance', 'Signed Distance Function', 'Shape Reconstruction', 'Planar Features', 'Latent Code', 'Neural Field', 'Flow Velocity', 'Velocity Vector', 'Template Space', 'Multilayer Perceptron']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', '3D computer vision']",1,"Deep Implicit Functions (DIFs) have gained popularity in 3D computer vision due to their compactness and continuous representation capabilities. However, addressing dense correspondences and semantic relationships across DIF-encoded shapes remains a critical challenge, limiting their applications in texture transfer and shape analysis. Moreover, recent endeavors in 3D shape generation using DIFs often neglect correspondence and topology preservation. This paper presents HNDF (Hybrid Neural Diffeomorphic Flow), a method that implicitly learns the underlying representation and decomposes intricate dense correspondences into explicitly axis-aligned triplane features. To avoid suboptimal representations trapped in local minima, we propose hybrid supervision that captures both local and global correspondences. Unlike conventional approaches that directly generate new 3D shapes, we further explore the idea of shape generation with deformed template shape via diffeomorphic flows, where the deformation is encoded by the generated triplane features. Leveraging a pre-existing 2D diffusion model, we produce high-quality and diverse 3D diffeomorphic flows through generated triplanes features, ensuring topological consistency with the template shape. Extensive experiments on medical image organ segmentation datasets evaluate the effectiveness of HNDF in 3D shape representation and generation."
Hybrid Sample Synthesis-Based Debiasing of Classifier in Limited Data Setting,"Piyush Arora, Pratik Mazumder","Indian Institute of Technology Jodhpur, India",100.0,India,0.0,,"Deep learning models are known to suffer from the problem of bias, and researchers have been exploring methods to address this issue. However, most of these methods require prior knowledge of the bias and are not always practical. In this paper, we focus on a more practical setting with no prior information about the bias. Generally, in this setting, there are a large number of bias-aligned samples that cause the model to produce biased predictions and a few bias-conflicting samples that do not conform to the bias. If the training data is limited, the influence of the bias-aligned samples may become even stronger on the model predictions, and we experimentally demonstrate that existing debiasing techniques suffer severely in such cases. In this paper, we examine the effects of unknown bias in small dataset regimes and present a novel approach to mitigate this issue. The proposed approach directly addresses the issue of the extremely low occurrence of bias-conflicting samples in limited data settings through the synthesis of hybrid samples that can be used to reduce the effect of bias. We perform extensive experiments on several benchmark datasets and experimentally demonstrate the effectiveness of our proposed approach in addressing any unknown bias in the presence of limited data. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN debiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when only 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a bias-conflicting sample ratio of 0.05.",https://openaccess.thecvf.com/content/WACV2024/html/Arora_Hybrid_Sample_Synthesis-Based_Debiasing_of_Classifier_in_Limited_Data_Setting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Arora_Hybrid_Sample_Synthesis-Based_Debiasing_of_Classifier_in_Limited_Data_Setting_WACV_2024_paper.pdf,,,2312.08288,main,Poster,https://ieeexplore.ieee.org/document/10484451/,"['Deep learning', 'Computer vision', 'Training data', 'Predictive models', 'Benchmark testing', 'Data models']","['Limited Dataset', 'Hybrid Samples', 'Prediction Model', 'Training Data', 'Deep Learning Models', 'Effect Of Bias', 'Prediction Bias', 'Unknown Bias', 'Sample Set', 'Training Dataset', 'Deep Neural Network', 'Image Features', 'False Positive Rate', 'Cross-entropy Loss', 'Multiple Datasets', 'Data Bias', 'Batch Of Samples', 'Ablation Experiments', 'Sufficient Variation', 'Type Of Bias', 'Training Data Points', 'Limited Training Data', 'Dataset Bias', 'Material For Further Details', 'Sufficient Training Data', 'MNIST Dataset', 'Blue Sky', 'Target Model']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Deep learning models are known to suffer from the problem of bias, and researchers have been exploring methods to address this issue. However, most of these methods require prior knowledge of the bias and are not always practical. In this paper, we focus on a more practical setting with no prior information about the bias. Generally, in this setting, there are a large number of bias-aligned samples that cause the model to produce biased predictions and a few bias-conflicting samples that do not conform to the bias. If the training data is limited, the influence of the bias-aligned samples may become even stronger on the model predictions, and we experimentally demonstrate that existing debiasing techniques suffer severely in such cases. In this paper, we examine the effects of unknown bias in small dataset regimes and present a novel approach to mitigate this issue. The proposed approach directly addresses the issue of the extremely low occurrence of bias-conflicting samples in limited data settings through the synthesis of hybrid samples that can be used to reduce the effect of bias. We perform extensive experiments on several benchmark datasets and experimentally demonstrate the effectiveness of our proposed approach in addressing any unknown bias in the presence of limited data. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN debiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when only 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a bias-conflicting sample ratio of 0.05."
HyperMix: Out-of-Distribution Detection and Classification in Few-Shot Settings,"Nikhil Mehta, Kevin J. Liang, Jing Huang, Fu-Jen Chu, Li Yin, Tal Hassner","Duke University, Meta; Meta",50.0,USA,50.0,USA,"Out-of-distribution (OOD) detection is an important topic for real-world machine learning systems, but settings with limited in-distribution samples have been underexplored. Such few-shot OOD settings are challenging, as models have scarce opportunities to learn the data distribution before being tasked with identifying OOD samples. Indeed, we demonstrate that recent state-of-the-art OOD methods fail to outperform simple baselines in the few-shot setting. We thus propose a hypernetwork framework called HyperMix, using Mixup on the generated classifier parameters, as well as a natural out-of-episode outlier exposure technique that does not require an additional outlier dataset. We conduct experiments on CIFAR-FS and MiniImageNet, significantly outperforming other OOD methods in the few-shot regime.",https://openaccess.thecvf.com/content/WACV2024/html/Mehta_HyperMix_Out-of-Distribution_Detection_and_Classification_in_Few-Shot_Settings_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mehta_HyperMix_Out-of-Distribution_Detection_and_Classification_in_Few-Shot_Settings_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484336,"['Computer vision', 'Computational modeling', 'Machine learning', 'Data models', 'Task analysis', 'Standards']","['Detector Set', 'Few-shot Setting', 'Simple Baseline', 'Classification Task', 'Stochastic Gradient Descent', 'Mixed Samples', 'Mahalanobis Distance', 'Linear Classifier', 'Decision Boundary', 'Augmentation Techniques', 'Base Classes', 'Generalization Error', 'Class Weights', 'Support Set', 'Query Set', 'Few-shot Learning', 'Stochastic Gradient Descent Optimizer', 'Categorical Cross-entropy', 'Noisy Labels', 'Query Sample', 'Few-shot Classification', 'Unlabeled Examples', 'Augmented Samples', 'Beta Distribution', 'Evaluation Protocol']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Out-of-distribution (OOD) detection is an important topic for real-world machine learning systems, but settings with limited in-distribution samples have been underexplored. Such few-shot OOD settings are challenging, as models have scarce opportunities to learn the data distribution before being tasked with identifying OOD samples. Indeed, we demonstrate that recent state-of-the-art OOD methods fail to outperform simple baselines in the few-shot setting. We thus propose a hypernetwork framework called HyperMix, using Mixup on the generated classifier parameters, as well as a natural out-of-episode outlier exposure technique that does not require an additional outlier dataset. We conduct experiments on CIFAR-FS and MiniImageNet, significantly outperforming other OOD methods in the few-shot regime."
Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin,"Gabriel Moreira, Manuel Marques, João Paulo Costeira, Alexander Hauptmann","Institute for Systems and Robotics, Instituto Superior Técnico; Language Technologies Institute, Carnegie Mellon University",100.0,"Portugal, USA",0.0,,"Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincare ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.",https://openaccess.thecvf.com/content/WACV2024/html/Moreira_Hyperbolic_vs_Euclidean_Embeddings_in_Few-Shot_Learning_Two_Sides_of_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Moreira_Hyperbolic_vs_Euclidean_Embeddings_in_Few-Shot_Learning_Two_Sides_of_WACV_2024_paper.pdf,,,2309.10013,main,Poster,https://ieeexplore.ieee.org/document/10483798/,"['Representation learning', 'Geometry', 'Computer vision', 'Image recognition', 'Neural networks', 'Euclidean distance', 'Benchmark testing']","['Few-shot Learning', 'Euclidean Embedding', 'Hyperbolic Embedding', 'Benchmark', 'Representation Learning', 'Image Recognition', 'Representation Of Space', 'Few-shot Classification', 'Prototypical Network', 'Hyperbolic Space', 'Machine Learning', 'Centroid', 'Supplemental Material', 'Computer Vision', 'Euclidean Space', 'Lower Dimension', 'Word Embedding', 'Community Detection', 'Self-supervised Learning', 'Effective Radius', 'Hyperbolic Geometry', 'Geodesic Distance', 'Exponential Map', 'Ball Model', 'Link Prediction', 'WordNet', 'Image Encoder', 'Drude Model', 'Angular Distance']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",3,"Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincaré ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension."
I-AI: A Controllable & Interpretable AI System for Decoding Radiologists' Intense Focus for Accurate CXR Diagnoses,"Trong Thang Pham, Jacob Brecheisen, Anh Nguyen, Hien Nguyen, Ngan Le","University of Liverpool, Liverpool, UK; University of Houston, Houston, Texas, USA 77204; AICV Lab, University of Arkansas, Fayetteville, AR, USA 72703",100.0,"UK, USA",0.0,,"In the field of chest X-ray (CXR) diagnosis, existing works often focus solely on determining where a radiologist looks, typically through tasks such as detection, segmentation, or classification. However, these approaches are often designed as black-box models, lacking interpretability. In this paper, we introduce Interpretable Artificial Intelligence (I-AI) a novel and unified controllable interpretable pipeline for decoding the intense focus of radiologists in CXR diagnosis. Our I-AI addresses three key questions: where a radiologist looks, how long they focus on specific areas, and what findings they diagnose. By capturing the intensity of the radiologist's gaze, we provide a unified solution that offers insights into the cognitive process underlying radiological interpretation. Unlike current methods that rely on black-box machine learning models, which can be prone to extracting erroneous information from the entire input image during the diagnosis process, we tackle this issue by effectively masking out irrelevant information. Our proposed I-AI leverages a vision-language model, allowing for precise control over the interpretation process while ensuring the exclusion of irrelevant features. To train our I-AI model, we utilize an eye gaze dataset to extract anatomical gaze information and generate ground truth heatmaps. Through extensive experimentation, we demonstrate the efficacy of our method. We showcase that the attention heatmaps, designed to mimic radiologists' focus, encode sufficient and relevant information, enabling accurate classification tasks using only a portion of CXR. The code, checkpoints, and data are at https://github.com/UARK-AICV/IAI.",https://openaccess.thecvf.com/content/WACV2024/html/Pham_I-AI_A_Controllable__Interpretable_AI_System_for_Decoding_Radiologists_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pham_I-AI_A_Controllable__Interpretable_AI_System_for_Decoding_Radiologists_WACV_2024_paper.pdf,,https://github.com/UARK-AICV/IAI,,main,Poster,,,,,,
ICF-SRSR: Invertible Scale-Conditional Function for Self-Supervised Real-World Single Image Super-Resolution,"Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee","Dept. of ECE & ASRI, IPAI, Seoul National University, Seoul, Korea; Dept. of ECE & ASRI, Seoul National University, Seoul, Korea",100.0,South Korea,0.0,,"Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice due to the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. Using the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of our method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets. The code is available from this link.",https://openaccess.thecvf.com/content/WACV2024/html/Neshatavar_ICF-SRSR_Invertible_Scale-Conditional_Function_for_Self-Supervised_Real-World_Single_Image_Super-Resolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Neshatavar_ICF-SRSR_Invertible_Scale-Conditional_Function_for_Self-Supervised_Real-World_Single_Image_Super-Resolution_WACV_2024_paper.pdf,,https://github.com/reyhanehneshat/ICF-SRSR,,main,Poster,https://ieeexplore.ieee.org/document/10484455/,"['Training', 'Computer vision', 'Codes', 'Superresolution', 'Training data', 'Benchmark testing', 'Image restoration']","['Single Image', 'Real-world Images', 'Invertible Function', 'Single Image Super-resolution', 'Training Data', 'Input Image', 'Image Pairs', 'Real-world Scenarios', 'Original Input', 'Low-resolution Images', 'Training Pairs', 'Real Training', 'Down-sampling Operation', 'Self-supervised Manner', 'Super-resolution Task', 'Real Pairs', 'Public Benchmark Datasets', 'Structural Similarity', 'Convolutional Neural Network', 'High-resolution Images', 'Peak Signal-to-noise Ratio', 'Real-world Datasets', 'Training Images', 'Super-resolution Model', 'Image X', 'Scaling Factor', 'Few-shot Learning', 'Image Resampling', 'Super-resolution Network', 'Paired Data']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice due to the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. Using the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of our method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets. The code is available from this link."
IDD-AW: A Benchmark for Safe and Robust Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather,"Furqan Ahmed Shaik, Abhishek Reddy, Nikhil Reddy Billa, Kunal Chaudhary, Sunny Manchanda, Girish Varma","Machine Learning Lab, IIIT Hyderabad; DYSL AI, DRDO",50.0,India,50.0,India,"Large-scale deployment of fully autonomous vehicles requires a very high degree of robustness to unstructured traffic, weather conditions, and should prevent unsafe mispredictions. While there are several datasets and benchmarks focusing on segmentation for drive scenes, they are not specifically focused on safety and robustness issues. We introduce the IDD-AW dataset, which provides 5000 pairs of high-quality images with pixel-level annotations, captured under rain, fog, low light, and snow in unstructured driving conditions. As compared to other adverse weather datasets, we provide i.) more annotated images, ii.) paired Near-Infrared (NIR) image for each frame, iii.) larger label set with a 4-level label hierarchy to capture unstructured traffic conditions. We benchmark state-of-the-art models for semantic segmentation in IDD-AW. We also propose a new metric called ""Safe mean Intersection over Union (Safe mIoU)"" for hierarchical datasets which penalizes dangerous mispredictions that are not captured in the traditional definition of mean Intersection over Union (mIoU). The results show that IDD-AW is one of the most challenging datasets to date for these tasks. The dataset and code will be available here: https://iddaw.github.io.",https://openaccess.thecvf.com/content/WACV2024/html/Shaik_IDD-AW_A_Benchmark_for_Safe_and_Robust_Segmentation_of_Drive_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shaik_IDD-AW_A_Benchmark_for_Safe_and_Robust_Segmentation_of_Drive_WACV_2024_paper.pdf,,http://iddaw.github.io,,main,Poster,https://ieeexplore.ieee.org/document/10484270/,"['Measurement', 'Rain', 'Semantic segmentation', 'Snow', 'Semantics', 'Benchmark testing', 'Robustness']","['Benchmark', 'Extreme Weather', 'Low Light', 'Autonomous Vehicles', 'Semantic Segmentation', 'Image Frames', 'Near-infrared Imaging', 'Semantic Segmentation Models', 'Misprediction', 'Adverse Conditions', 'Pedestrian', 'Large-scale Datasets', 'Important Class', 'Multispectral', 'Light Signal', 'RGB Images', 'Labeled Data Set', 'Segmentation Quality', 'Class Hierarchy', 'Adverse Weather Conditions', 'Distance Tree', 'Street Scenes', 'Semantic Annotation']","['Algorithms', 'Datasets and evaluations', 'Applications', 'Autonomous Driving']",1,"Large-scale deployment of fully autonomous vehicles requires a very high degree of robustness to unstructured traffic, weather conditions, and should prevent unsafe mispredictions. While there are several datasets and benchmarks focusing on segmentation for drive scenes, they are not specifically focused on safety and robustness issues. We introduce the IDD-AW dataset, which provides 5000 pairs of high-quality images with pixel-level annotations, captured under rain, fog, low light, and snow in unstructured driving conditions. As compared to other adverse weather datasets, we provide i.) more annotated images, ii.) paired Near-Infrared (NIR) image for each frame, iii.) larger label set with a 4-level label hierarchy to capture unstructured traffic conditions. We benchmark state-of-the-art models for semantic segmentation in IDD-AW. We also propose a new metric called ""Safe mean Intersection over Union (Safe mIoU)"" for hierarchical datasets which penalizes dangerous mispredictions that are not captured in the traditional definition of mean Intersection over Union (mIoU). The results show that IDD-AW is one of the most challenging datasets to date for these tasks. The dataset and code will be available here: http://iddaw.github.io."
IKEA Ego 3D Dataset: Understanding Furniture Assembly Actions From Ego-View 3D Point Clouds,"Yizhak Ben-Shabat, Jonathan Paul, Eviatar Segev, Oren Shrout, Stephen Gould","Technion, Israel Institute of Technology; Australian National University",100.0,"Australia, Israel",0.0,,"We propose a novel dataset for ego-view 3D point cloud action recognition. While there has been extensive research on understanding human actions in RGB videos in recent years, the exploration of its 3D point cloud counterpart has been relatively limited. Furthermore, RGB ego-view datasets are rapidly growing, however, 3D point cloud ego-view datasets are scarce at best. Existing 3D datasets are limited in several ways, some include actions that are distinguishable by full-body motion while others use a distant static sensor that hinders the recognition of small objects. We introduce a new point cloud action recognition dataset---the IKEA Ego 3D dataset. It includes sequences of point clouds captured from an ego-view using a HoloLens 2 device. The dataset consists of approximately 493k frames and 56 classes of intricate furniture assembly actions of four different furniture types. We evaluate the performance of various state-of-the-art 3D action recognition methods on the proposed dataset and show that it is very challenging.",https://openaccess.thecvf.com/content/WACV2024/html/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ben-Shabat_IKEA_Ego_3D_Dataset_Understanding_Furniture_Assembly_Actions_From_Ego-View_WACV_2024_paper.pdf,https://sitzikbs.github.io/IKEAEgo3D.github.io/,https://github.com/sitzikbs/IKEAEgo3D,,main,Poster,https://ieeexplore.ieee.org/document/10484319/,"['Point cloud compression', 'Performance evaluation', 'Computer vision', 'Three-dimensional displays', 'Focusing', 'Benchmark testing', 'Task analysis']","['Point Cloud', '3D Point Cloud', '3D Datasets', 'Action Recognition', 'Action Classes', 'RGB Video', 'Action Recognition Datasets', 'Receptive Field', 'Temporal Information', 'Current Dataset', 'Motion Detection', 'Depth Data', 'Global Representation', 'Temporal Representation', 'Temporal Smoothing', 'Linearizable', 'RGB Data', 'Post-processing Stage', 'Action Recognition Task', 'Point Cloud Representation', 'Coffee Table']","['Algorithms', 'Datasets and evaluations', 'Algorithms', '3D computer vision']",,"We propose a novel dataset for ego-view 3D point cloud action recognition. While there has been extensive research on understanding human actions in RGB videos in recent years, the exploration of its 3D point cloud counterpart has been relatively limited. Furthermore, RGB ego-view datasets are rapidly growing, however, 3D point cloud ego-view datasets are scarce at best. Existing 3D datasets are limited in several ways, some include actions that are distinguishable by full-body motion while others use a distant static sensor that hinders the recognition of small objects. We introduce a new point cloud action recognition dataset—the IKEA Ego 3D dataset. It includes sequences of point clouds captured from an ego-view using a HoloLens 2 device. The dataset consists of approximately 493k frames and 56 classes of intricate furniture assembly actions of four different furniture types. We evaluate the performance of various state-of-the-art 3D action recognition methods on the proposed dataset and show that it is very challenging."
INCODE: Implicit Neural Conditioning With Prior Knowledge Embeddings,"Amirhossein Kazerouni, Reza Azad, Alireza Hosseini, Dorit Merhof, Ulas Bagci",Northwestern University; University of Regensburg; Iran University of Science and Technology; University of Tehran; RWTH Aachen University,100.0,"Germany, Iran, USA",0.0,,"Implicit Neural Representations (INRs) have revolutionized signal representation by leveraging neural networks to provide continuous and smooth representations of complex data. However, existing INRs face limitations in capturing fine-grained details, handling noise, and adapting to diverse signal types. To address these challenges, we introduce INCODE, a novel approach that enhances the control of the sinusoidal-based activation function in INRs using deep prior knowledge. INCODE comprises a harmonizer network and a composer network, where the harmonizer network dynamically adjusts key parameters of the activation function. Through a task-specific pre-trained model, INCODE adapts the task-specific parameters to optimize the representation process. Our approach not only excels in representation, but also extends its prowess to tackle complex tasks such as audio, image, and 3D shape reconstructions, as well as intricate challenges such as neural radiance fields (NeRFs), and inverse problems, including denoising, super-resolution, inpainting, and CT reconstruction. Through comprehensive experiments, INCODE demonstrates its superiority in terms of robustness, accuracy, quality, and convergence rate, broadening the scope of signal representation.",https://openaccess.thecvf.com/content/WACV2024/html/Kazerouni_INCODE_Implicit_Neural_Conditioning_With_Prior_Knowledge_Embeddings_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kazerouni_INCODE_Implicit_Neural_Conditioning_With_Prior_Knowledge_Embeddings_WACV_2024_paper.pdf,https://xmindflow.github.io/incode,https://github.com/xmindflow/incode,2310.18846,main,Poster,https://ieeexplore.ieee.org/document/10483666/,"['Knowledge engineering', 'Three-dimensional displays', 'Shape', 'Superresolution', 'Noise reduction', 'Noise', 'Robustness']","['Neural Network', 'Activation Function', 'Denoising', 'Convergence Rate', 'Image Reconstruction', 'Inverse Problem', 'Neural Representations', '3D Shape', 'Signal Representation', 'Inpainting', 'Learning Process', 'Learning Rate', 'Convolutional Neural Network', 'Continuous-time', 'Intersection Over Union', 'Multilayer Perceptron', 'Signal Values', 'Spatial Coordinates', 'Fine Details', 'Image Representation', 'Latent Code', 'PSNR Values', 'Representational Capacity', 'Task Representations', 'Mel-frequency Cepstral Coefficients', 'Vertical Shift', 'Radon Transform', 'Signed Distance Function', 'Implicit Function', 'Multilayer Perceptron Architecture']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",1,"Implicit Neural Representations (INRs) have revolutionized signal representation by leveraging neural networks to provide continuous and smooth representations of complex data. However, existing INRs face limitations in capturing fine-grained details, handling noise, and adapting to diverse signal types. To address these challenges, we introduce INCODE, a novel approach that enhances the control of the sinusoidal-based activation function in INRs using deep prior knowledge. INCODE comprises a harmonizer network and a composer network, where the harmonizer network dynamically adjusts key parameters of the activation function. Through a task-specific pre-trained model, INCODE adapts the task-specific parameters to optimize the representation process. Our approach not only excels in representation, but also extends its prowess to tackle complex tasks such as audio, image, and 3D shape reconstructions, as well as intricate challenges such as neural radiance fields (NeRFs), and inverse problems, including denoising, super-resolution, inpainting, and CT reconstruction. Through comprehensive experiments, INCODE demonstrates its superiority in terms of robustness, accuracy, quality, and convergence rate, broadening the scope of signal representation. Please visit the project’s website for details on the proposed method and access to the code."
IR-FRestormer: Iterative Refinement With Fourier-Based Restormer for Accelerated MRI Reconstruction,"Mohammad Zalbagi Darestani, Vishwesh Nath, Wenqi Li, Yufan He, Holger R. Roth, Ziyue Xu, Daguang Xu, Reinhard Heckel, Can Zhao",Technical University of Munich; Rice University; NVIDIA,66.66666666666666,"Germany, USA",33.33333333333334,USA,"Accelerated magnetic resonance imaging (MRI) aims to reconstruct high-quality MR images from a set of under-sampled measurements. State-of-the-art methods for this task use deep learning, which offers high reconstruction accuracy and fast runtimes. In this work, we propose a new state-of-the-art reconstruction model for accelerated MRI reconstruction. Our model is the first to combine the power of deep neural networks with iterative refinement for this task. For the neural network component of our method, we utilize a transformer-based architecture as transformers are state-of-the-art in various image reconstruction tasks. However, a major drawback of transformers which has limited their emergence among the state-of-the-art MRI models is that they are often memory inefficient for high-resolution inputs. To address this limitation, we propose a transformer-based model which uses parameter-free Fourier-based attention modules, achieving 2x more memory efficiency. We evaluate our model on the largest publicly available MRI dataset, the fastMRI dataset, and achieve on-par performance with other state-of-the-art methods on the dataset's leaderboard.",https://openaccess.thecvf.com/content/WACV2024/html/Darestani_IR-FRestormer_Iterative_Refinement_With_Fourier-Based_Restormer_for_Accelerated_MRI_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Darestani_IR-FRestormer_Iterative_Refinement_With_Fourier-Based_Restormer_for_Accelerated_MRI_Reconstruction_WACV_2024_paper.pdf,https://fastmri.org/leaderboards/,,,main,Poster,https://ieeexplore.ieee.org/document/10483832/,"['Deep learning', 'Runtime', 'Magnetic resonance imaging', 'Memory management', 'Transformers', 'Brain modeling', 'Image restoration']","['Magnetic Resonance Imaging', 'Iterative Refinement', 'Magnetic Resonance Imaging Reconstruction', 'Neural Network', 'Deep Learning', 'Transformer', 'Image Reconstruction', 'Attention Module', 'Reconstruction Accuracy', 'Reconstruction Model', 'Memory Efficiency', 'Magnetic Resonance Imaging Datasets', 'Training Set', 'Optimization Problem', 'Convolutional Neural Network', 'Gradient Descent', 'Number Of Steps', 'GB Memory', 'Update Rule', 'Final Submission', 'Refinement Step', 'Structural Similarity Index Measure', 'Vision Transformer', 'Coil Sensitivity', 'Image X', 'Ground Truth Image', 'Proximal Operator', 'Problem Setup', 'GPU Memory']","['Applications', 'Biomedical / healthcare / medicine']",3,"Accelerated magnetic resonance imaging (MRI) aims to reconstruct high-quality MR images from a set of under-sampled measurements. State-of-the-art methods for this task use deep learning, which offers high reconstruction accuracy and fast runtimes. In this work, we propose a new state-of-the-art reconstruction model for accelerated MRI reconstruction. Our model is the first to combine the power of deep neural networks with iterative refinement for this task. For the neural network component of our method, we utilize a transformer-based architecture as transformers are state-of-the-art in various image reconstruction tasks. However, a major drawback of transformers which has limited their emergence among the state-of-the-art MRI models is that they are often memory inefficient for high-resolution inputs. To address this limitation, we propose a transformer-based model which uses parameter-free Fourier-based attention modules, achieving 2× more memory efficiency. We evaluate our model on the largest publicly available MRI dataset, the fastMRI dataset [46], and achieve on-par performance with other state-of-the-art
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
 methods on the dataset’s leaderboard
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
."
ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification,"Nicolas Gorlo, Kenneth Blomqvist, Francesco Milano, Roland Siegwart","Autonomous Systems Lab, ETH Zürich",100.0,Switzerland,0.0,,"Most object-level mapping systems in use today make use of an upstream learned object instance segmentation model. If we want to teach them about a new object or segmentation class, we need to build a large dataset and retrain the system. To build spatial AI systems that can quickly be taught about new objects, we need to effectively solve the problem of single-shot object detection, instance segmentation and re-identification. So far there is neither a method fulfilling all of these requirements in unison nor a benchmark that could be used to test such a method. Addressing this, we propose ISAR, a benchmark and baseline method for single- and few-shot object Instance Segmentation And Re-identification, in an effort to accelerate the development of algorithms that can robustly detect, segment, and re-identify objects from a single or a few sparse training examples. We provide a semi-synthetic dataset of video sequences with ground-truth semantic annotations, a standardized evaluation pipeline, and a baseline method. Our benchmark aligns with the emerging research trend of unifying Multi-Object Tracking, Video Object Segmentation, and Re-identification.",https://openaccess.thecvf.com/content/WACV2024/html/Gorlo_ISAR_A_Benchmark_for_Single-_and_Few-Shot_Object_Instance_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gorlo_ISAR_A_Benchmark_for_Single-_and_Few-Shot_Object_Instance_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484475/,"['Instance segmentation', 'Training', 'Three-dimensional displays', 'Annotations', 'Tracking', 'Video sequences', 'Object segmentation']","['Instance Segmentation', 'Object Instances', 'Object Detection', 'Object Classification', 'Baseline Methods', 'Training Examples', 'Video Sequences', 'Object Segmentation', 'Semantic Annotation', 'Data Structure', 'Performance Of Method', 'Benchmark Datasets', 'Bounding Box', 'Entire Sequence', 'Segmentation Method', 'RGB Images', 'Dot Product', 'Small Objects', 'Video Data', 'Training Sequences', 'Scene Context', 'Sequence Of Frames', 'Bounding Box Annotations', 'RGB-D Images', 'Temporal Consistency', 'Subsequent Frames', 'Objects In Context']","['Algorithms', 'Datasets and evaluations', 'Algorithms', '3D computer vision', 'Algorithms', 'Video recognition and understanding']",1,"Most object-level mapping systems in use today make use of an upstream learned object instance segmentation model. If we want to teach them about a new object or segmentation class, we need to build a large dataset and retrain the system. To build spatial AI systems that can quickly be taught about new objects, we need to effectively solve the problem of single-shot object detection, instance segmentation and re-identification. So far there is neither a method fulfilling all of these requirements in unison nor a benchmark that could be used to test such a method. Addressing this, we propose ISAR, a benchmark and baseline method for single- and few-shot object Instance Segmentation And Re-identification, in an effort to accelerate the development of algorithms that can robustly detect, segment, and reidentify objects from a single or a few sparse training examples. We provide a semi-synthetic dataset of video sequences with ground-truth semantic annotations, a standardized evaluation pipeline, and a baseline method. Our benchmark aligns with the emerging research trend of unifying Multi-Object Tracking, Video Object Segmentation, and Re-identification."
Identifying Label Errors in Object Detection Datasets by Loss Inspection,"Marius Schubert, Tobias Riedlinger, Karsten Kahl, Daniel Kröll, Sebastian Schoenen, Siniša Šegvić, Matthias Rottmann","ControlExpert GmbH, Germany; University of Zagreb, Croatia; IZMD, University of Wuppertal, Germany",66.66666666666666,"Croatia, Germany",33.33333333333334,Germany,"Labeling datasets for supervised object detection is a dull and time-consuming task. Errors can be easily introduced during annotation and overlooked during review, yielding inaccurate benchmarks and performance degradation of deep neural networks trained on noisy labels. In this work, we introduce a benchmark for label error detection methods on object detection datasets as well as a theoretically underpinned label error detection method and a number of baselines. We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets. For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages' classification and regression losses. The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter. We compare our method to four baselines: a naive one without deep learning, the object detector's score, the entropy of the classification softmax distribution and a probability margin based method from related work. We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently, which we also derive theoretically. Furthermore, we detect real label errors a) on commonly used test datasets in object detection and b) on a proprietary dataset. In both cases we achieve low false positives rates, i.e., we detect label errors with a precision for a) of up to 71.5% and for b) with 97%.",https://openaccess.thecvf.com/content/WACV2024/html/Schubert_Identifying_Label_Errors_in_Object_Detection_Datasets_by_Loss_Inspection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Schubert_Identifying_Label_Errors_in_Object_Detection_Datasets_by_Loss_Inspection_WACV_2024_paper.pdf,,,2303.06999,main,Poster,https://ieeexplore.ieee.org/document/10484530/,"['Training', 'Reviews', 'Computational modeling', 'Object detection', 'Benchmark testing', 'Noise measurement', 'Object recognition']","['Object Detection', 'Errors In Dataset', 'Labeling Errors', 'Object Detection Dataset', 'Training Set', 'Deep Neural Network', 'Test Dataset', 'Error Detection', 'Simulation Error', 'Regression Loss', 'Baseline Number', 'Noisy Labels', 'Real Error', 'Two-stage Object Detection', 'Training Data', 'Image Classification', 'Types Of Errors', 'Intersection Over Union', 'Detection Task', 'AUROC Values', 'Original Training Data', 'Noisy Data', 'Manual Review', 'Non-maximum Suppression', 'Different Types Of Errors', 'PASCAL VOC', 'Correct Label', 'Training Labels', 'Validation Split']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding']",,"Labeling datasets for supervised object detection is a dull and time-consuming task. Errors can be easily introduced during annotation and overlooked during review, yielding inaccurate benchmarks and performance degradation of deep neural networks trained on noisy labels. In this work, we introduce a benchmark for label error detection methods on object detection datasets as well as a theoretically underpinned label error detection method and a number of baselines. We simulate four different types of randomly introduced label errors on train and test sets of well-labeled object detection datasets. For our label error detection method we assume a two-stage object detector to be given and consider the sum of both stages’ classification and regression losses. The losses are computed with respect to the predictions and the noisy labels including simulated label errors, aiming at detecting the latter. We compare our method to four baselines: a naive one without deep learning, the object detector’s score, the entropy of the classification softmax distribution and a probability margin based method from related work. We outperform all baselines and demonstrate that among the considered methods, ours is the only one that detects label errors of all four types efficiently, which we also derive theoretically. Furthermore, we detect real label errors a) on commonly used test datasets in object detection and b) on a proprietary dataset. In both cases we achieve low false positives rates, i.e., we detect label errors with a precision for a) of up to 71.5% and for b) with 97%."
Image Denoising and the Generative Accumulation of Photons,"Alexander Krull, Hector Basevi, Benjamin Salmon, Andre Zeug, Franziska Müller, Samuel Tonks, Leela Muppala, Aleš Leonardis","Centre of Membrane Proteins and Receptors, Universities of Nottingham and Birmingham, UK; School of Computer Science, University of Birmingham, UK; Medizinische Hochschule Hannover, Germany",100.0,"Germany, UK",0.0,,"We present a fresh perspective on shot noise corrupted images and noise removal. By viewing image formation as the sequential accumulation of photons on a detector grid, we show that a network trained to predict where the next photon could arrive is in fact solving the minimum mean square error (MMSE) denoising task. This new perspective allows us to make three contributions: (i) We present a new strategy for self-supervised denoising. (ii) We present a new method for sampling from the posterior of possible solutions by iteratively sampling and adding small numbers of photons to the image. (iii) We derive a full generative model by starting this process from an empty canvas. We call this approach generative accumulation of photons (GAP). We evaluate our method quantitatively and qualitatively on 4 new fluorescence microscopy datasets, which will be made available to the community. We find that it outperforms its baselines or performs on-par.",https://openaccess.thecvf.com/content/WACV2024/html/Krull_Image_Denoising_and_the_Generative_Accumulation_of_Photons_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Krull_Image_Denoising_and_the_Generative_Accumulation_of_Photons_WACV_2024_paper.pdf,,,2307.06607,main,Poster,https://ieeexplore.ieee.org/document/10484261/,"['Computer vision', 'Microscopy', 'Noise reduction', 'Noise', 'Mean square error methods', 'Detectors', 'Fluorescence']","['Denoising', 'Mean Square Error', 'New Perspective', 'Network Training', 'Image Formation', 'Shot Noise', 'Minimum Mean Square Error', 'Minimum Mean Square', 'Microscopy Datasets', 'Training Data', 'Convolutional Neural Network', 'Input Image', 'Image Pixels', 'Clean Data', 'Paired Data', 'Generative Adversarial Networks', 'Image Generation', 'Noisy Data', 'Clear Image', 'Microscopy Data', 'Noisy Images', 'Photon Counting', 'Image X', 'Variational Autoencoder', 'Individual Photons', 'Convolutional Neural Network Training', 'Default Hyperparameters', 'Noisy Observations', 'Single-molecule Localization', 'Markov Random Field']","['Algorithms', 'Low-level and physics-based vision', 'Applications', 'Biomedical / healthcare / medicine']",1,"We present a fresh perspective on shot noise corrupted images and noise removal. By viewing image formation as the sequential accumulation of photons on a detector grid, we show that a network trained to predict where the next photon could arrive is in fact solving the minimum mean square error (MMSE) denoising task. This new perspective allows us to make three contributions: i. We present a new strategy for self-supervised denoising, ii. We present a new method for sampling from the posterior of possible solutions by iteratively sampling and adding small numbers of photons to the image. iii. We derive a full generative model by starting this process from an empty canvas. We call this approach generative accumulation of photons (GAP). We evaluate our method quantitatively and qualitatively on 4 new fluorescence microscopy datasets, which will be made available to the community. We find that it outperforms its baselines or performs on-par."
Image Labels Are All You Need for Coarse Seagrass Segmentation,"Scarlett Raine, Ross Marchant, Brano Kusy, Frederic Maire, Tobias Fischer","QUT Centre for Robotics, Queensland University of Technology, Australia; CSIRO Data61, Australia; Image Analytics, Australia",66.66666666666666,Australia,33.33333333333334,Australia,"Seagrass meadows serve as critical carbon sinks, but estimating the amount of carbon they store requires knowledge of the seagrass species present. Underwater and surface vehicles equipped with machine learning algorithms can help to accurately estimate the composition and extent of seagrass meadows at scale. However, previous approaches for seagrass detection and classification have required supervision from patch-level labels. In this paper, we reframe seagrass classification as a weakly supervised coarse segmentation problem where image-level labels are used during training (25 times fewer labels compared to patch-level labeling) and patch-level outputs are obtained at inference time. To this end, we introduce SeaFeats, an architecture that uses unsupervised contrastive pre-training and feature similarity, and SeaCLIP, a model that showcases the effectiveness of large language models as a supervisory signal in domain-specific applications. We demonstrate that an ensemble of SeaFeats and SeaCLIP leads to highly robust performance. Our method outperforms previous approaches that require patch-level labels on the multi-species 'DeepSeagrass' dataset by 6.8% (absolute) for the class-weighted F1 score, and by 12.1% (absolute) for the seagrass presence/absence F1 score on the 'Global Wetlands' dataset. We also present two case studies for real-world deployment: outlier detection on the Global Wetlands dataset, and application of our method on imagery collected by the FloatyBoat autonomous surface vehicle.",https://openaccess.thecvf.com/content/WACV2024/html/Raine_Image_Labels_Are_All_You_Need_for_Coarse_Seagrass_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Raine_Image_Labels_Are_All_You_Need_for_Coarse_Seagrass_Segmentation_WACV_2024_paper.pdf,,,2303.00973,main,Poster,https://ieeexplore.ieee.org/document/10484505/,"['Surveys', 'Training', 'Biological system modeling', 'Semantic segmentation', 'Estimation', 'Vegetation mapping', 'Robot sensing systems']","['Similar Characteristics', 'Learning Algorithms', 'F1 Score', 'Autonomous Vehicles', 'Amount Of Carbon', 'Outlier Detection', 'Inference Time', 'Underwater Vehicles', 'Seagrass Meadows', 'Seagrass Species', 'Supervisory Signal', 'Autonomous Surface Vehicles', 'Image-level Labels', 'Loss Function', 'Image Classification', 'Image Regions', 'ImageNet', 'Training Images', 'Latent Space', 'Domain Experts', 'Underwater Image', 'Classification Head', 'Material For Further Details', 'Image Patches', 'Blue Carbon', 'Pretext Task', 'Supplementary Materials For Details', 'Weak Supervision', 'Self-supervised Learning', 'Transect Surveys']","['Applications', 'Environmental monitoring / climate change / ecology', 'Applications', 'Robotics']",,"Seagrass meadows serve as critical carbon sinks, but estimating the amount of carbon they store requires knowledge of the seagrass species present. Underwater and surface vehicles equipped with machine learning algorithms can help to accurately estimate the composition and extent of seagrass meadows at scale. However, previous approaches for seagrass detection and classification have required supervision from patch-level labels. In this paper, we reframe seagrass classification as a weakly supervised coarse segmentation problem where image-level labels are used during training (25 times fewer labels compared to patch-level labeling) and patch-level outputs are obtained at inference time. To this end, we introduce SeaFeats, an architecture that uses unsupervised contrastive pre-training and feature similarity, and SeaCLIP, a model that showcases the effectiveness of large language models as a supervisory signal in domain-specific applications. We demonstrate that an ensemble of SeaFeats and SeaCLIP leads to highly robust performance. Our method outperforms previous approaches that require patch-level labels on the multi-species ‘DeepSeagrass’ dataset by 6.8% (absolute) for the class-weighted F1 score, and by 12.1% (absolute) for the seagrass presence/absence F1 score on the ‘Global Wetlands’ dataset. We also present two case studies for real-world deployment: outlier detection on the Global Wetlands dataset, and application of our method on imagery collected by the FloatyBoat autonomous surface vehicle."
Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction,"Minsu Kim, Jaewon Lee, Byeonghun Lee, Sunghoon Im, Kyong Hwan Jin","DGIST, Republic of Korea; Korea University, Republic of Korea",100.0,South Korea,0.0,,"Existing frameworks for image stitching often provide visually reasonable stitchings. However, they suffer from blurry artifacts and disparities in illumination, depth level, etc. Although the recent learning-based stitchings relax such disparities, the required methods impose sacrifice of image qualities failing to capture high-frequency details for stitched images. To address the problem, we propose a novel approach, implicit Neural Image Stitching (NIS) that extends arbitrary-scale super-resolution. Our method estimates Fourier coefficients of images for quality-enhancing warps. Then, the suggested model blends color mismatches and misalignment in the latent space and decodes the features into RGB values of stitched images. Our experiments show that our approach achieves improvement in resolving the low-definition imaging of the previous deep image stitching with favorable accelerated image-enhancing methods. Our source code is available at https://github.com/minshu-kim/NIS.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Implicit_Neural_Image_Stitching_With_Enhanced_and_Blended_Feature_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Implicit_Neural_Image_Stitching_With_Enhanced_and_Blended_Feature_Reconstruction_WACV_2024_paper.pdf,,https://github.com/minshu-kim/NIS,2309.01409,main,Poster,https://ieeexplore.ieee.org/document/10483846/,"['Training', 'Image color analysis', 'Source coding', 'Superresolution', 'Pipelines', 'Lighting', 'Imaging']","['Decoding', 'Image Quality', 'Latent Space', 'Fourier Coefficients', 'Learning Rate', 'Latent Variables', 'Image Pairs', 'Target Image', 'Reference Image', 'Frequency Estimation', 'Displacement Vector', 'Image Alignment', 'Phase Estimation', 'Implicit Representation', 'Warped Image', 'Training Configurations']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Low-level and physics-based vision']",2,"Existing frameworks for image stitching often provide visually reasonable stitchings. However, they suffer from blurry artifacts and disparities in illumination, depth level, etc. Although the recent learning-based stitchings relax such disparities, the required methods impose sacrifice of image qualities failing to capture high-frequency details for stitched images. To address the problem, we propose a novel approach, implicit Neural Image Stitching (NIS) that extends arbitrary-scale super-resolution. Our method estimates Fourier coefficients of images for quality-enhancing warps. Then, the suggested model blends color mismatches and misalignment in the latent space and decodes the features into RGB values of stitched images. Our experiments show that our approach achieves improvement in resolving the low-definition imaging of the previous deep image stitching with favorable accelerated image-enhancing methods. Our source code is available at https://github.com/minshu-kim/NIS."
Implicit Neural Representation for Change Detection,"Peter Naylor, Diego Di Carlo, Arianna Traviglia, Makoto Yamada, Marco Fiorucci","RIKEN AIP, Kyoto, Japan; Istituto Italiano di Tecnologia, Venice, Italy; RIKEN AIP, Kyoto, Japan; Now at ESA/ESRIN, Φ-lab, Italy; RIKEN AIP, Kyoto, Japan; OIST, Okinawa, Japan",66.66666666666666,"Italy, Japan",33.33333333333334,Italy,"Identifying changes in a pair of 3D aerial LiDAR point clouds, obtained during two distinct time periods over the same geographic region presents a significant challenge due to the disparities in spatial coverage and the presence of noise in the acquisition system. The most commonly used approaches to detecting changes in point clouds are based on supervised methods which necessitate extensive labelled data often unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Implicit Neural Representation (INR) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. INR offers a grid-agnostic representation for encoding bi-temporal point clouds, with unmatched spatial support that can be regularised to enhance high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset comprising simulated LiDAR point clouds for urban sprawling. This dataset encompasses diverse challenging scenarios, varying in resolutions, input modalities and noise levels. This enables a comprehensive multi-scenario evaluation, comparing our method with the current state-of-the-art approach. We outperform the previous methods by a margin of 10% in the intersection over union metric. In addition, we put our techniques to practical use by applying them in a real-world scenario to identify instances of illicit excavation of archaeological sites and validate our results by comparing them with findings from field experts.",https://openaccess.thecvf.com/content/WACV2024/html/Naylor_Implicit_Neural_Representation_for_Change_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Naylor_Implicit_Neural_Representation_for_Change_Detection_WACV_2024_paper.pdf,,,2307.15428,main,Poster,https://ieeexplore.ieee.org/document/10483630/,"['Point cloud compression', 'Surface reconstruction', 'Laser radar', 'Three-dimensional displays', 'Shape', 'Noise', 'Urban planning']","['Change Detection', 'Implicit Neural Representation', 'Real-world Applications', 'Intersection Over Union', 'Point Cloud', 'Urban Growth', 'Gaussian Mixture Model', 'LiDAR Point Clouds', 'Shape Reconstruction', 'Pair Changes', 'False Positive', 'Spatial Resolution', 'Altitude', 'Low Resolution', 'Deep Neural Network', 'Feature Maps', '3D Reconstruction', 'Digital Elevation Model', 'Simulated Datasets', 'Bounding Box', 'Low Noise', 'High Noise', 'Surface Reconstruction', 'Lidar Data', 'Optimal Transport', 'Positional Encoding', 'Final Row', 'Regularization Term', 'Photogrammetry', 'Spatial Coordinates']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Remote Sensing']",1,"Identifying changes in a pair of 3D aerial LiDAR point clouds, obtained during two distinct time periods over the same geographic region presents a significant challenge due to the disparities in spatial coverage and the presence of noise in the acquisition system. The most commonly used approaches to detecting changes in point clouds are based on supervised methods which necessitate extensive labelled data often unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Implcit Neural Represenation (INR) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. INR offers a grid-agnostic representation for encoding bi-temporal point clouds, with unmatched spatial support that can be regularised to enhance high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset comprising simulated LiDAR point clouds for urban sprawling. This dataset encompasses diverse challenging scenarios, varying in resolutions, input modalities and noise levels. This enables a comprehensive multi-scenario evaluation, comparing our method with the current state-of-the-art approach. We outperform the previous methods by a margin of 10% in the intersection over union metric. In addition, we put our techniques to practical use by applying them in a real-world scenario to identify instances of illicit excavation of archaeological sites and validate our results by comparing them with findings from field experts."
Improved Techniques for Quantizing Deep Networks With Adaptive Bit-Widths,"Ximeng Sun, Rameswar Panda, Chun-Fu Richard Chen, Naigang Wang, Bowen Pan, Aude Oliva, Rogerio Feris, Kate Saenko","Boston University; Boston University, MIT-IBM Watson AI Lab; MIT; MIT-IBM Watson AI Lab, MIT; IBM Research; MIT-IBM Watson AI Lab, IBM Research",83.33333333333334,USA,16.666666666666657,USA,"Quantizing deep networks with adaptive bit-widths is a promising technique for efficient inference across many devices and resource constraints. In contrast to static methods that repeat the quantization process and train different models for different constraints, adaptive quantization enables us to flexibly adjust the bit-widths of a single deep network during inference for instant adaptation in different scenarios. While existing research shows encouraging results on common image classification benchmarks, this paper investigates how to train such adaptive networks more effectively. Specifically, we present two novel techniques for quantizing deep neural networks with adaptive bit-widths of weights and activations. First, we propose a collaborative strategy to choose a high-precision ""teacher"" for transferring knowledge to the low-precision ""student"" while jointly optimizing the model with all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic block swapping method by randomly replacing the blocks in the lower-precision student network with the corresponding blocks in the higher-precision teacher network. Extensive experiments on multiple image classification datasets and novel video classification experiments, well demonstrate the efficacy of our approach over state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.pdf,,,2103.01435,main,Poster,https://ieeexplore.ieee.org/document/10484410/,"['Knowledge engineering', 'Adaptation models', 'Computer vision', 'Adaptive systems', 'Quantization (signal)', 'Collaboration', 'Artificial neural networks']","['Deep Network', 'Neural Network', 'Deep Neural Network', 'Image Classification', 'Resource Constraints', 'Knowledge Transfer', 'Image Dataset', 'Single Network', 'Video Analysis', 'Teacher Network', 'Student Network', 'Quantum', 'Model Performance', 'High Precision', 'ImageNet', 'Space Model', 'Optimal Network', 'Distance In Space', 'Batch Normalization Layer', 'Progressive Training', 'Dynamic Selection', 'Distillation Loss', 'Joint Training', 'Least Significant Bit', 'Network Quantization', 'Separate Parameters', 'Quantization Scheme', 'SOTA Methods', 'Optimization Difficulty', 'Backward Propagation']","['Algorithms', 'Image recognition and understanding']",1,"Quantizing deep networks with adaptive bit-widths is a promising technique for efficient inference across many devices and resource constraints. In contrast to static methods that repeat the quantization process and train different models for different constraints, adaptive quantization enables us to flexibly adjust the bit-widths of a single deep network during inference for instant adaptation in different scenarios. While existing research shows encouraging results on common image classification benchmarks, this paper investigates how to train such adaptive networks more effectively. Specifically, we present two novel techniques for quantizing deep neural networks with adaptive bit-widths of weights and activations. First, we propose a collaborative strategy to choose a high-precision ""teacher"" for transferring knowledge to the low-precision ""student"" while jointly optimizing the model with all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic block swapping method by randomly replacing the blocks in the lower-precision student network with the corresponding blocks in the higher-precision teacher network. Extensive experiments on multiple image and video classification datasets, well demonstrate the efficacy of our approach over state-of-the-art methods."
Improved Topological Preservation in 3D Axon Segmentation and Centerline Detection Using Geometric Assessment-Driven Topological Smoothing (GATS),"Nina I. Shamsi, Alec S. Xu, Lars A. Gjesteby, Laura J. Brattain","MIT Lincoln Laboratory, Lexington, MA 02421, USA",100.0,USA,0.0,,"Automated axon tracing via fully supervised learning requires large amounts of 3D brain imagery, which is time consuming and laborious to obtain. It also requires expertise. Thus, there is a need for more efficient segmentation and centerline detection techniques to use in conjunction with automated annotation tools. Topology-preserving methods ensure that segmented components maintain geometric connectivity, which is especially meaningful for applications where volumetric data is used, and these methods often make use of morphological thinning algorithms as the thinned outputs can be useful for both segmentation and centerline detection of curvilinear structures. Current morphological thinning approaches used in conjunction with topology-preserving methods are prone to over-thinning and require manual configuration of hyperparameters.  We propose an automated approach for morphological smoothing using geometric assessment of the radius of tubular structures in brain microscopy volumes, and apply average pooling to prevent over-thinning. We use this approach to formulate a loss function, which we call Geometric Assessment-driven Topological Smoothing loss, or GATS. Our approach increased segmentation and centerline detection evaluation metrics by 2%-5% across multiple datasets, and improved the Betti error rates by 9%. Our ablation study showed that geometric assessment of tubular structures achieved higher segmentation and centerline detection scores, and using average pooling for morphological smoothing in place of thinning algorithms reduced the Betti errors. We observed increased topological preservation during automated annotation of 3D axons volumes from models trained with GATS.",https://openaccess.thecvf.com/content/WACV2024/html/Shamsi_Improved_Topological_Preservation_in_3D_Axon_Segmentation_and_Centerline_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shamsi_Improved_Topological_Preservation_in_3D_Axon_Segmentation_and_Centerline_Detection_WACV_2024_paper.pdf,,,2311.04116,main,Poster,https://ieeexplore.ieee.org/document/10484355/,"['Measurement', 'Axons', 'Image segmentation', 'Solid modeling', 'Smoothing methods', 'Three-dimensional displays', 'Annotations']","['Segmentation Detection', 'Axonal Segments', 'Topology Preservation', '3D Centerline', 'Centerline Detection', 'Loss Function', 'Average Pooling', '3D Volume', 'Volumetric Data', 'Training Time', 'Hyperparameter Tuning', 'Binary Image', 'Maximum Radius', 'Voxel Resolution', 'Segmentation Quality', 'Input Volume', 'Adjusted Rand Index', 'Dice Loss', 'Smoothing Algorithm', 'Average Pooling Operation', '3D U-Net', 'Betti Numbers', 'Medial Axis', 'Pixel Radius', 'Scikit-image']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Automated axon tracing via fully supervised learning requires large amounts of 3D brain imagery, which is time consuming and laborious to obtain. It also requires expertise. Thus, there is a need for more efficient segmentation and centerline detection techniques to use in conjunction with automated annotation tools. Topology-preserving methods ensure that segmented components maintain geometric connectivity, which is especially meaningful for applications where volumetric data is used, and these methods often make use of morphological thinning algorithms as the thinned outputs can be useful for both segmentation and centerline detection of curvilinear structures. Current morphological thinning approaches used in conjunction with topology-preserving methods are prone to over-thinning and require manual configuration of hyperparameters.We propose an automated approach for morphological smoothing using geometric assessment of the radius of tubular structures in brain microscopy volumes, and apply average pooling to prevent over-thinning. We use this approach to formulate a loss function, which we call Geometric Assessment-driven Topological Smoothing loss, or GATS. Our approach increased segmentation and center-line detection evaluation metrics by 2%-5% across multiple datasets, and improved the Betti error rates by 9%. Our ablation study showed that geometric assessment of tubular structures achieved higher segmentation and centerline detection scores, and using average pooling for morphological smoothing in place of thinning algorithms reduced the Betti errors. We observed increased topological preservation during automated annotation of 3D axons volumes from models trained with GATS."
Improving Fairness Using Vision-Language Driven Image Augmentation,"Moreno D'Incà, Christos Tzelepis, Ioannis Patras, Nicu Sebe",University of Trento; Queen Mary University of London,100.0,"Italy, UK",0.0,,"Fairness is crucial when training a deep-learning discriminative model, especially in the facial domain. Models tend to correlate specific characteristics (such as age and skin color) with unrelated attributes (downstream tasks), resulting in biases which do not correspond to reality. It is common knowledge that these correlations are present in the data and are then transferred to the models during training. This paper proposes a method to mitigate these correlations to improve fairness. To do so, we learn interpretable and meaningful paths lying in the semantic space of a pre-trained diffusion model (DiffAE) -- such paths being supervised by contrastive text dipoles. That is, we learn to edit protected characteristics (age and skin color). These paths are then applied to augment images to improve the fairness of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on several downstream tasks with age and skin color as protected characteristics. As a proxy for fairness, we compute the difference in accuracy with respect to the protected characteristics. Quantitative results show how the augmented images help the model improve the overall accuracy, the aforementioned metric, and the disparity of equal opportunity. Code is available at: https://github.com/Moreno98/Vision-Language-Bias-Control.",https://openaccess.thecvf.com/content/WACV2024/html/DInca_Improving_Fairness_Using_Vision-Language_Driven_Image_Augmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/DInca_Improving_Fairness_Using_Vision-Language_Driven_Image_Augmentation_WACV_2024_paper.pdf,,https://github.com/Moreno98/Vision-Language-Bias-Control,,main,Poster,https://ieeexplore.ieee.org/document/10483568/,"['Training', 'Measurement', 'Correlation', 'Image color analysis', 'Semantics', 'Natural languages', 'Skin']","['Image Augmentation', 'Skin Color', 'Difference In Accuracy', 'Diffusion Model', 'Model Discrimination', 'Protected Characteristics', 'Semantic Space', 'Training Set', 'Specific Properties', 'Behavioral Model', 'Natural Language', 'Major Classes', 'Binary Classification', 'Data Augmentation', 'Generative Adversarial Networks', 'Latent Space', 'Face Images', 'Black People', 'White People', 'Synthetic Images', 'Sensitive Attributes', 'Dataset Bias', 'Minority Class', 'Path In Space', 'Causal Graph', 'Skin Aging', 'Facial Attributes', 'Qualitative Results', 'Adversarial Training', 'Images Of Samples']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",1,"Fairness is crucial when training a deep-learning discriminative model, especially in the facial domain. Models tend to correlate specific characteristics (such as age and skin color) with unrelated attributes (downstream tasks), resulting in biases which do not correspond to reality. It is common knowledge that these correlations are present in the data and are then transferred to the models during training (e.g., [35]). This paper proposes a method to mitigate these correlations to improve fairness. To do so, we learn interpretable and meaningful paths lying in the semantic space of a pre-trained diffusion model (DiffAE) [27] –such paths being supervised by contrastive text dipoles. That is, we learn to edit protected characteristics (age and skin color). These paths are then applied to augment images to improve the fairness of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on several downstream tasks with age and skin color as protected characteristics. As a proxy for fairness, we compute the difference in accuracy with respect to the protected characteristics. Quantitative results show how the augmented images help the model improve the overall accuracy, the aforementioned metric, and the disparity of equal opportunity. Code is available at: https://github.com/Moreno98/Vision-Language-Bias-Control."
Improving Fairness in Deepfake Detection,"Yan Ju, Shu Hu, Shan Jia, George H. Chen, Siwei Lyu","Indiana University–Purdue University Indianapolis; Carnegie Mellon University; University at Buffalo, State University of New York",100.0,USA,0.0,,"Despite the development of effective deepfake detectors in recent years, recent studies have demonstrated that biases in the data used to train these detectors can lead to disparities in detection accuracy across different races and genders. This can result in different groups being unfairly targeted or excluded from detection, allowing undetected deepfakes to manipulate public opinion and erode trust in a deepfake detection model. While existing studies have focused on evaluating fairness of deepfake detectors, to the best of our knowledge, no method has been developed to encourage fairness in deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions that handle both the setting where demographic information (e.g., annotations of race and gender) is available as well as the case where this information is absent. Fundamentally, both approaches can be used to convert many existing deepfake detectors into ones that encourages fairness. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexibility of our approach in improving deepfake detection fairness. Our code is available at https://github.com/littlejuyan/DF_Fairness.",https://openaccess.thecvf.com/content/WACV2024/html/Ju_Improving_Fairness_in_Deepfake_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ju_Improving_Fairness_in_Deepfake_Detection_WACV_2024_paper.pdf,,https://github.com/littlejuyan/DF_Fairness,2306.16635,main,Poster,https://ieeexplore.ieee.org/document/10483741/,"['Training', 'Deepfakes', 'Computer vision', 'Codes', 'Annotations', 'Detectors']","['Deepfake Detection', 'Loss Function', 'Demographic Data', 'Detection Accuracy', 'Detection Model', 'Training Data', 'Optimization Problem', 'Hyperparameters', 'Validation Set', 'False Positive Rate', 'Detection Performance', 'Stochastic Gradient Descent', 'Loss Value', 'Empirical Estimates', 'Demographic Groups', 'Real Examples', 'Balanced Dataset', 'Binary Search', 'Sensitive Attributes', 'Raw Features', 'Fair Performance', 'Conditional Value At Risk', 'Notions Of Fairness', 'Latent Groups', 'Original Detection', 'Influence Of Class', 'Gradient Descent']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",12,"Despite the development of effective deepfake detectors in recent years, recent studies have demonstrated that biases in the data used to train these detectors can lead to disparities in detection accuracy across different races and genders. This can result in different groups being unfairly targeted or excluded from detection, allowing undetected deepfakes to manipulate public opinion and erode trust in a deepfake detection model. While existing studies have focused on evaluating fairness of deepfake detectors, to the best of our knowledge, no method has been developed to encourage fairness in deepfake detection at the algorithm level. In this work, we make the first attempt to improve deepfake detection fairness by proposing novel loss functions that handle both the setting where demographic information (e.g., annotations of race and gender) is available as well as the case where this information is absent. Fundamentally, both approaches can be used to convert many existing deepfake detectors into ones that encourages fairness. Extensive experiments on four deepfake datasets and five deepfake detectors demonstrate the effectiveness and flexibility of our approach in improving deep-fake detection fairness. Our code is available at https://github.com/littlejuyan/DF_Fairness."
Improving Graph Networks Through Selection-Based Convolution,"David Hart, Bryan Morse",East Carolina University; Brigham Young University,100.0,USA,0.0,,"Graph Convolutional Networks (GCNs) provide a general framework that can learn in a variety of data domains, such as 3D geometry, social networks, and chemical structures. GCNs, however, often ignore intrinsic relationships among nodes in the graph, and these relationships need to be learned indirectly during the training process through mechanisms such as attention or local-kernel approximation. This paper introduces selection-based graph convolution, a method for preserving these intrinsic relationships within the graph convolution operator which provides improved performance over attention-based counterparts on various tasks. We demonstrate the effectiveness of selection to improve the performance of many types of GCNs on tasks such as spatial graph classification. Furthermore, we demonstrate the ability to improve state-of-the-art graph networks for road traffic estimation and molecular property prediction.",https://openaccess.thecvf.com/content/WACV2024/html/Hart_Improving_Graph_Networks_Through_Selection-Based_Convolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hart_Improving_Graph_Networks_Through_Selection-Based_Convolution_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483758/,"['Training', 'Geometry', 'Three-dimensional displays', 'Convolution', 'Social networking (online)', 'Estimation', 'Spatial databases']","['Network Graph', 'Social Networks', 'Nodes In The Graph', 'Graph Convolutional Network', 'Graph Convolution', 'Spatial Classification', 'Complex Network', 'Differentiable Function', 'Point Cloud', 'Edge Weights', 'Kinds Of Data', 'Directional Selection', 'Graph Structure', 'Graph Data', 'Fewer Parameters', 'Node Positions', 'Node Features', 'Learned Weights', 'Graph Neural Networks', 'Group Of Nodes', 'Regular Graphs', 'Traffic Prediction', 'Different Kinds Of Data', 'Aggregation Step', 'Standard Convolution', 'Nearby Nodes', 'Network Design', '3D Data', 'Source Node', 'Node Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', '3D computer vision']",,"Graph Convolutional Networks (GCNs) provide a general framework that can learn in a variety of data domains, such as 3D geometry, social networks, and chemical structures. GCNs, however, often ignore intrinsic relationships among nodes in the graph, and these relationships need to be learned indirectly during the training process through mechanisms such as attention or local-kernel approximation. This paper introduces selection-based graph convolution, a method for preserving these intrinsic relationships within the graph convolution operator which provides improved performance over attention-based counterparts on various tasks. We demonstrate the effectiveness of selection to improve the performance of many types of GCNs on tasks such as spatial graph classification. Furthermore, we demonstrate the ability to improve state-of-the-art graph networks for road traffic estimation and molecular property prediction."
Improving Normalization With the James-Stein Estimator,"Seyedalireza Khoshsirat, Chandra Kambhamettu","Video/Image Modeling and Synthesis (VIMS) Lab, University of Delaware",100.0,USA,0.0,,"Stein's paradox holds considerable sway in high-dimensional statistics, highlighting that the sample mean, traditionally considered the de facto estimator, might not be the most efficacious in higher dimensions. To address this, the James-Stein estimator proposes an enhancement by steering the sample means toward a more centralized mean vector. In this paper, first, we establish that normalization layers in deep learning use inadmissible estimators for mean and variance. Next, we introduce a novel method to employ the James-Stein estimator to improve the estimation of mean and variance within normalization layers. We evaluate our method on different computer vision tasks: image classification, semantic segmentation, and 3D object classification. Through these evaluations, it is evident that our improved normalization layers consistently yield superior accuracy across all tasks without extra computational burden. Moreover, recognizing that a plethora of shrinkage estimators surpass the traditional estimator in performance, we study two other prominent shrinkage estimators: Ridge and LASSO. Additionally, we provide visual representations to intuitively demonstrate the impact of shrinkage on the estimated layer statistics. Finally, we study the effect of regularization and batch size on our modified batch normalization. The studies show that our method is less sensitive to batch size and regularization, improving accuracy under various setups.",https://openaccess.thecvf.com/content/WACV2024/html/Khoshsirat_Improving_Normalization_With_the_James-Stein_Estimator_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Khoshsirat_Improving_Normalization_With_the_James-Stein_Estimator_WACV_2024_paper.pdf,,,2312.00313,main,Poster,https://ieeexplore.ieee.org/document/10484239/,"['Computer vision', 'Visualization', 'Three-dimensional displays', 'Semantic segmentation', 'Estimation', 'Transformers', 'Vectors']","['James Stein Estimator', 'Deep Learning', 'Computer Vision', 'Batch Size', 'Image Classification', 'Sample Mean', 'Batch Normalization', 'Normalization Layer', 'Semantic Segmentation', 'Regularization Effect', 'Deep Network', 'Deep Neural Network', 'Large Networks', 'Improvement In Accuracy', 'Standard Estimates', 'Point Cloud', 'Batch Normalization Layer', 'ImageNet Dataset', 'Basis Of Estimates', 'Normalization Techniques', 'Lasso Estimator', 'Accuracy Enhancement', 'Small Batch Size', 'Shrinkage Effect']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",2,"Stein’s paradox holds considerable sway in high-dimensional statistics, highlighting that the sample mean, traditionally considered the de facto estimator, might not be the most efficacious in higher dimensions. To address this, the James-Stein estimator proposes an enhancement by steering the sample means toward a more centralized mean vector. In this paper, first, we establish that normalization layers in deep learning use inadmissible estimators for mean and variance. Next, we introduce a novel method to employ the James-Stein estimator to improve the estimation of mean and variance within normalization layers. We evaluate our method on different computer vision tasks: image classification, semantic segmentation, and 3D object classification. Through these evaluations, it is evident that our improved normalization layers consistently yield superior accuracy across all tasks without extra computational burden. Moreover, recognizing that a plethora of shrinkage estimators surpass the traditional estimator in performance, we study two other prominent shrinkage estimators: Ridge and LASSO. Additionally, we provide visual representations to intuitively demonstrate the impact of shrinkage on the estimated layer statistics. Finally, we study the effect of regularization and batch size on our modified batch normalization. The studies show that our method is less sensitive to batch size and regularization, improving accuracy under various setups."
Improving Open-Set Semi-Supervised Learning With Self-Supervision,"Erik Wallin, Lennart Svensson, Fredrik Kahl, Lars Hammarstrand","Saab AB, Chalmers University of Technology; Chalmers University of Technology",100.0,Sweden,0.0,,"Open-set semi-supervised learning (OSSL) embodies a practical scenario within semi-supervised learning, wherein the unlabeled training set encompasses classes absent from the labeled set. Many existing OSSL methods assume that these out-of-distribution data are harmful and put effort into excluding data belonging to unknown classes from the training objective. In contrast, we propose an OSSL framework that facilitates learning from all unlabeled data through self-supervision. Additionally, we utilize an energy-based score to accurately recognize data belonging to the known classes, making our method well-suited for handling uncurated data in deployment. We show through extensive experimental evaluations that our method yields state-of-the-art results on many of the evaluated benchmark problems in terms of closed-set accuracy and open-set recognition when compared with existing methods for OSSL. Our code is available at https://github.com/walline/ssl-tf2-sefoss.",https://openaccess.thecvf.com/content/WACV2024/html/Wallin_Improving_Open-Set_Semi-Supervised_Learning_With_Self-Supervision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wallin_Improving_Open-Set_Semi-Supervised_Learning_With_Self-Supervision_WACV_2024_paper.pdf,,https://github.com/walline/ssl-tf2-sefoss,2301.10127,main,Poster,https://ieeexplore.ieee.org/document/10484257/,"['Training', 'Computer vision', 'Codes', 'Employment', 'Focusing', 'Semisupervised learning', 'Benchmark testing']","['Semi-supervised Learning', 'Training Set', 'Extensive Evaluation', 'Unlabeled Data', 'Training Objective', 'Unlabeled Set', 'Extensive Experimental Evaluation', 'Learning Rate', 'Validation Set', 'Data Augmentation', 'Increase In Accuracy', 'Training Step', 'Confidence Threshold', 'Domain Adaptation', 'Prediction Confidence', 'Adaptive Threshold', 'Energy Score', 'Inliers', 'Training Signal', 'Hinge Loss', 'Pre-training Phase', 'Consistency Regularization', 'Labeled Training Set']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Open-set semi-supervised learning (OSSL) embodies a practical scenario within semi-supervised learning, wherein the unlabeled training set encompasses classes absent from the labeled set. Many existing OSSL methods assume that these out-of-distribution data are harmful and put effort into excluding data belonging to unknown classes from the training objective. In contrast, we propose an OSSL framework that facilitates learning from all unlabeled data through self-supervision. Additionally, we utilize an energy-based score to accurately recognize data belonging to the known classes, making our method well-suited for handling uncurated data in deployment. We show through extensive experimental evaluations that our method yields state-of-the-art results on many of the evaluated benchmark problems in terms of closed-set accuracy and open-set recognition when compared with existing methods for OSSL. Our code is available at https://github.com/walline/ssl-tf2-sefoss."
Improving Vision-and-Language Reasoning via Spatial Relations Modeling,"Cheng Yang, Rui Xu, Ye Guo, Peixiang Huang, Yiru Chen, Wenkui Ding, Zhongyuan Wang, Hong Zhou",Peking University; Kuaishou Technology; Zhejiang University,66.66666666666666,China,33.33333333333334,China,"Visual commonsense reasoning (VCR) is a challenging multi-modal task, which requires high-level cognition and commonsense reasoning ability about the real world. In recent years, large-scale pre-training approaches have been developed and promoted the state-of-the-art performance of VCR. However, the existing approaches almost employ the BERT-like objectives to learn multi-modal representations. These objectives motivated from the text-domain are insufficient for the excavation on the complex scenario of visual modality. Most importantly, the spatial distribution of the visual objects is basically neglected. To address the above issue, we propose to construct the spatial relation graph based on the given visual scenario. Further, we design two pre- training tasks named object position regression (OPR) and spatial relation classification (SRC) to learn to reconstruct the spatial relation graph respectively. Quantitative analysis suggests that the proposed method can guide the representations to maintain more spatial context and facilitate the attention on the essential visual regions for reasoning. We achieve the state-of-the-art results on VCR and two other vision-and-language reasoning tasks VQA, and NLVR2.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Improving_Vision-and-Language_Reasoning_via_Spatial_Relations_Modeling_WACV_2024_paper.pdf,,,2311.05298,main,Poster,https://ieeexplore.ieee.org/document/10483664,"['Visualization', 'Analytical models', 'Graphical models', 'Statistical analysis', 'Computational modeling', 'Excavation', 'Task analysis']","['Spatial Relationship', 'Challenging Task', 'Complex Scenarios', 'Spatial Context', 'Visual Modality', 'Visual Object', 'Reasoning Tasks', 'Spatial Classification', 'Multimodal Representation', 'Visual Question Answering', 'Visual Reasoning', 'Pre-training Tasks', 'Semantic', 'Visual Features', 'Intersection Over Union', 'Spatial Model', 'Representation Learning', 'Position Vector', 'Value Of Node', 'Individual Categories', 'Masked Language Model', 'Tokenized', 'Scene Graph', 'Position Embedding', 'Masked Images', 'Text Modality', 'Object Regions', 'Token Embedding', 'Masking Strategy', 'Self-supervised Learning']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",,"Visual commonsense reasoning (VCR) is a challenging multi-modal task, which requires high-level cognition and commonsense reasoning ability about the real world. In recent years, large-scale pre-training approaches have been developed and promoted the state-of-the-art performance of VCR. However, the existing approaches almost employ the BERT-like objectives to learn multi-modal representations. These objectives motivated from the text-domain are insufficient for the excavation on the complex scenario of visual modality. Most importantly, the spatial distribution of the visual objects is basically neglected. To address the above issue, we propose to construct the spatial relation graph based on the given visual scenario. Further, we design two pre-training tasks named object position regression (OPR) and spatial relation classification (SRC) to learn to reconstruct the spatial relation graph respectively. Quantitative analysis suggests that the proposed method can guide the representations to maintain more spatial context and facilitate the attention on the essential visual regions for reasoning. We achieve the state-of-the-art results on VCR and two other vision-and-language reasoning tasks VQA, and NLVR
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
."
Improving the Effectiveness of Deep Generative Data,"Ruyu Wang, Sabrina Schmedding, Marco F. Huber","Bosch Center for Artiﬁcial Intelligence, Renningen, Germany; Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Stuttgart, Germany; Fraunhofer Institute for Manufacturing Engineering and Automation (IPA), Stuttgart, Germany",66.66666666666666,Germany,33.33333333333334,Germany,"Recent deep generative models (DGMs) such as generative adversarial networks (GANs) and diffusion probabilistic models (DPMs) have shown their impressive ability in generating high-fidelity photorealistic images. Although looking appealing to human eyes, training a model on purely synthetic images for downstream image processing tasks like image classification often results in an undesired performance drop compared to training on real data. Previous works have demonstrated that enhancing a real dataset with synthetic images from DGMs can be beneficial. However, the improvements were subjected to certain circumstances and yet were not comparable to adding the same number of real images. In this work, we propose a new taxonomy to describe factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. Extensive experiments on multiple datasets showcase that our method outperforms baselines on downstream classification tasks both in case of training on synthetic only (Synthetic-to-Real) and training on a mix of real and synthetic data (Data Augmentation), particularly in the data-scarce scenario.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Improving_the_Effectiveness_of_Deep_Generative_Data_WACV_2024_paper.pdf,,,2311.03959,main,Poster,https://ieeexplore.ieee.org/document/10483885/,"['Training', 'Image processing', 'Taxonomy', 'Probabilistic logic', 'Generative adversarial networks', 'Data models', 'Task analysis']","['Image Classification', 'Data Augmentation', 'Generative Adversarial Networks', 'Synthetic Images', 'Deep Generative Models', 'Impressive Ability']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"Recent deep generative models (DGMs) such as generative adversarial networks (GANs) and diffusion probabilistic models (DPMs) have shown their impressive ability in generating high-fidelity photorealistic images. Although looking appealing to human eyes, training a model on purely synthetic images for downstream image processing tasks like image classification often results in an undesired performance drop compared to training on real data. Previous works have demonstrated that enhancing a real dataset with synthetic images from DGMs can be beneficial. However, the improvements were subjected to certain circumstances and yet were not comparable to adding the same number of real images. In this work, we propose a new taxonomy to describe factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. Extensive experiments on multiple datasets showcase that our method outperforms baselines on downstream classification tasks both in case of training on synthetic only (Synthetic-to-Real) and training on a mix of real and synthetic data (Data Augmentation), particularly in the data-scarce scenario."
Improving the Fairness of the Min-Max Game in GANs Training,"Zhaoyu Zhang, Yang Hua, Hui Wang, Seán McLoone",Queen’s University Belfast,100.0,Canada,0.0,,"Generative adversarial networks (GANs) have achieved great success and become more and more popular in recent years. However, understanding of the min-max game in GANs training is still limited. In this paper, we first utilize information game theory to analyze the min-max game in GANs and introduce a new viewpoint on the GANs training that the min-max game in existing GANs is unfair during training, leading to sub-optimal convergence. To tackle this, we propose a novel GAN called Information Gap GAN (IGGAN), which consists of one generator (G) and two discriminators (D1 and D2). Specifically, we apply different data augmentation methods to D1 and D2, respectively. The information gap between different data augmentation methods can change the information received by each player in the min-max game and lead to all three players G, D1 and D2 in IGGAN obtaining incomplete information, which improves the fairness of the min-max game, yielding better convergence. We conduct extensive experiments for large-scale and limited data settings on several common datasets with two backbones, i.e., BigGAN and StyleGAN2. The results demonstrate that IGGAN can achieve a higher Inception Score (IS) and a lower Frechet Inception Distance (FID) compared with other GANs. Codes are available at https://github.com/zzhang05/IGGAN",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Improving_the_Fairness_of_the_Min-Max_Game_in_GANs_Training_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Improving_the_Fairness_of_the_Min-Max_Game_in_GANs_Training_WACV_2024_paper.pdf,,https://github.com/zzhang05/IGGAN,,main,Poster,https://ieeexplore.ieee.org/document/10483766/,"['Training', 'Computer vision', 'Codes', 'Games', 'Generative adversarial networks', 'Data augmentation', 'Generators']","['Generative Adversarial Networks', 'Generative Adversarial Networks Training', 'Minimax Game', 'Data Augmentation', 'Game Theory', 'Information Gap', 'Data Augmentation Methods', 'Fréchet Inception Distance', 'Complete Information', 'Sample Space', 'Considerable Improvement', 'Standard Datasets', 'Semicontinuous Function', 'CIFAR-100 Dataset']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Generative adversarial networks (GANs) have achieved great success and become more and more popular in recent years. However, understanding of the min-max game in GANs training is still limited. In this paper, we first utilize information game theory to analyze the min-max game in GANs and introduce a new viewpoint on the GANs training that the min-max game in existing GANs is unfair during training, leading to sub-optimal convergence. To tackle this, we propose a novel GAN called Information Gap GAN (IGGAN), which consists of one generator (G) and two discriminators (D
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf>
 and D
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf>
). Specifically, we apply different data augmentation methods to D
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf>
 and D
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf>
, respectively. The information gap between different data augmentation methods can change the information received by each player in the min-max game and lead to all three players G, D
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf>
 and D
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</inf>
 in IGGAN obtaining incomplete information, which improves the fairness of the min-max game, yielding better convergence. We conduct extensive experiments for large-scale and limited data settings on several common datasets with two backbones, i.e., BigGAN and StyleGAN2. The results demonstrate that IGGAN can achieve a higher Inception Score (IS) and a lower Fréchet Inception Distance (FID) compared with other GANs. Codes are available at https://github.com/zzhang05/IGGAN"
Improving the Leaking of Augmentations in Data-Efficient GANs via Adaptive Negative Data Augmentation,"Zhaoyu Zhang, Yang Hua, Guanxiong Sun, Hui Wang, Seán McLoone","Queen’s University Belfast; Queen’s University Belfast, Huawei UKRD",100.0,Canada,0.0,,"Data augmentation (DA) has shown its effectiveness in training Data-Efficient GANs (DE-GANs). However, applying DA in DE-GANs results in transforming the distributions of generated data and real data to augmented distributions of generated data and real data. This augmentation process could produce some out-of-distribution samples, known as the leaking of augmentations problem, which is highly undesirable in DE-GANs training. Although some methods propose ""leaking-free"" DAs for DE-GANs, we theoretically and practically argue that the leaking of augmentations problem still exists in these methods. To alleviate the leaking of augmentations in DE-GANs, in this paper, we propose a simple yet effective method called adaptive negative data augmentation (ANDA) for DE-GANs, with a negligible computational cost increase. Specifically, ANDA adaptively augments the augmented distribution of generated data using the augmented distribution of negative real data, where the negative real data is produced by applying negative data augmentation (NDA) on the real data. In this case, potential leaking samples can be presented as ""fake"" instances to the discriminator adaptively, which avoids the generator (G) learning such samples, thus resulting in better performance. Extensive experiments on several datasets with different DE-GANs demonstrate that ANDA can effectively alleviate the leaking of augmentations problem during training and achieve better performance. Codes are available at https://github.com/zzhang05/ANDA",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Improving_the_Leaking_of_Augmentations_in_Data-Efficient_GANs_via_Adaptive_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Improving_the_Leaking_of_Augmentations_in_Data-Efficient_GANs_via_Adaptive_WACV_2024_paper.pdf,,https://github.com/zzhang05/ANDA,,main,Poster,https://ieeexplore.ieee.org/document/10484339/,"['Training', 'Computer vision', 'Codes', 'Data augmentation', 'Generators', 'Computational efficiency']","['Data Augmentation', 'Generative Adversarial Networks', 'Adaptive Data Augmentation', 'Negative Data Augmentation', 'Negligible Cost', 'Real Data Distribution', 'Augmentation Process', 'Objective Function', 'Real Samples', 'Kullback-Leibler', 'Limited Dataset', 'Augmentation Methods', 'Real Distribution', 'Hyperparameter Values', 'Data Augmentation Methods', 'Reference Distribution', 'Jensen-Shannon Divergence', 'Generative Adversarial Networks Training', 'Fake Images', 'Optimal Discrimination']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Data augmentation (DA) has shown its effectiveness in training Data-Efficient GANs (DE-GANs). However, applying DA in DE-GANs results in transforming the distributions of generated data and real data to augmented distributions of generated data and real data. This augmentation process could produce some out-of-distribution samples, known as the leaking of augmentations problem, which is highly undesirable in DE-GANs training. Although some methods propose ""leaking-free"" DAs for DE-GANs, we theoretically and practically argue that the leaking of augmentations problem still exists in these methods. To alleviate the leaking of augmentations in DE-GANs, in this paper, we propose a simple yet effective method called adaptive negative data augmentation (ANDA) for DE-GANs, with a negligible computational cost increase. Specifically, ANDA adaptively augments the augmented distribution of generated data using the augmented distribution of negative real data, where the negative real data is produced by applying negative data augmentation (NDA) on the real data. In this case, potential leaking samples can be presented as ""fake"" instances to the discriminator adaptively, which avoids the generator (G) learning such samples, thus resulting in better performance. Extensive experiments on several datasets with different DE-GANs demonstrate that ANDA can effectively alleviate the leaking of augmentations problem during training and achieve better performance. Codes are available at https://github.com/zzhang05/ANDA"
Incorporating Physics Principles for Precise Human Motion Prediction,"Yufei Zhang, Jeffrey O. Kephart, Qiang Ji",Rensselaer Polytechnic Institute; IBM Research,50.0,USA,50.0,USA,"A variety of real-world applications rely on accurate predictions of 3D human motion from their past observations. While existing methods have made notable progress, their predictions over subsecond horizons can still be off by many centimeters. In this paper, we argue that achieving precise human motion prediction requires characterizing the fundamental physics principles governing body movements. We introduce PhysMoP, a novel framework that incorporates Physics for human Motion Prediction. PhysMoP estimates the body configuration of the next frame by solving the Euler-Lagrange equations, a set of Ordinary Different Equations describing the physical motion rules. To limit the inherent problem of error accumulation over time, PhysMoP leverages a data-driven model and iteratively guides the physics-based prediction via a fusion model. Through extensive experiments, we demonstrate that PhysMoP significantly outperforms existing approaches at subsecond prediction horizons. For example, at a prediction horizon of 80 msec, PhysMoP outperforms traditional data-driven approaches by a factor of 10 or more.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Incorporating_Physics_Principles_for_Precise_Human_Motion_Prediction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Incorporating_Physics_Principles_for_Precise_Human_Motion_Prediction_WACV_2024_paper.pdf,,https://github.com/zhangy76/PhysMoP,,main,Poster,https://ieeexplore.ieee.org/document/10484216/,"['Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Neural networks', 'Predictive models', 'Mathematical models', 'Data models']","['Physical Principles', 'Human Motion', 'Motion Prediction', 'Precise Motion', 'Human Motion Prediction', 'Prediction Accuracy', 'Body Movements', 'Data-driven Models', 'Prediction Horizon', 'Error Accumulation', '3D Motion', 'Lagrange Equations', 'Physical Motion', 'Body Configuration', 'Neural Network', 'Recurrent Neural Network', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Joint Angles', 'Graph Convolutional Network', 'Physics-based Models', 'Data-driven Estimation', 'Short-term Prediction', 'Future Motion', 'Long-term Prediction', '3D Body', 'Restricted Boltzmann Machine', 'Inverse Dynamics', 'Body Joints', 'Fusion Weights']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"A variety of real-world applications rely on accurate predictions of 3D human motion from their past observations. While existing methods have made notable progress, their predictions over subsecond horizons can still be off by many centimeters. In this paper, we argue that achieving precise human motion prediction requires characterizing the fundamental physics principles governing body movements. We introduce PhysMoP, a novel framework that incorporates Physics for human Motion Prediction. PhysMoP estimates the body configuration of the next frame by solving the Euler-Lagrange equations, a set of Ordinary Different Equations describing the physical motion rules. To limit the inherent problem of error accumulation over time, PhysMoP leverages a data-driven model and iteratively guides the physics-based prediction via a fusion model. Through extensive experiments, we demonstrate that PhysMoP significantly outperforms existing approaches at subsecond prediction horizons. For example, at a prediction horizon of 80 msec, PhysMoP outperforms traditional data-driven approaches by a factor of 10 or more."
Increasing Biases Can Be More Efficient Than Increasing Weights,"Carlo Metta, Marco Fantozzi, Andrea Papini, Gianluca Amato, Matteo Bergamaschi, Silvia Giulia Galfrè, Alessandro Marchetti, Michelangelo Vegliò, Maurizio Parton, Francesco Morandin","University of Chieti-Pescara, Italy; ISTI-CNR Pisa, Italy; Scuola Normale Superiore, Pisa, Italy; University of Parma, Italy; University of Pisa, Italy; University of Padova, Italy",83.33333333333334,Italy,16.666666666666657,Italy,"We introduce a novel computational unit for neural networks that features multiple biases, challenging the traditional perceptron structure. This unit emphasizes the importance of preserving uncorrupted information as it is passed from one unit to the next, applying activation functions later in the process with specialized biases for each unit. Through both empirical and theoretical analyses, we show that by focusing on increasing biases rather than weights, there is potential for significant enhancement in a neural network model's performance. This approach offers an alternative perspective on optimizing information flow within neural networks. Commented source code at https://github.com/CuriosAI/dac-dev.",https://openaccess.thecvf.com/content/WACV2024/html/Metta_Increasing_Biases_Can_Be_More_Efficient_Than_Increasing_Weights_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Metta_Increasing_Biases_Can_Be_More_Efficient_Than_Increasing_Weights_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483732/,"['Weight measurement', 'Computer vision', 'Analytical models', 'Source coding', 'Computational modeling', 'Neural networks', 'Focusing']","['Neural Network', 'Activation Function', 'Artificial Neural Network', 'Computing Units', 'Convolutional Layers', 'Batch Normalization', 'Trainable Parameters', 'Increase In Complexity', 'Residual Block', 'ReLU Activation', 'Residual Network', 'Skip Connections', 'Input Channels', 'Standard Units', 'Test Error', 'Global Average Pooling', 'Units In Layer', 'Image Classification Tasks', 'Weight Wi', 'Nonlinear Filter', 'Depthwise Convolution', 'ResNet Network', 'Addition Of Mn', 'Biological Neurons', 'Variety Of Architectures', 'Test Accuracy', 'Increase In Width', 'Bioinspired', 'Linear Operator', 'Marginal Increase']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"We introduce a novel computational unit for neural networks that features multiple biases, challenging the traditional perceptron structure. This unit emphasizes the importance of preserving uncorrupted information as it is passed from one unit to the next, applying activation functions later in the process with specialized biases for each unit. Through both empirical and theoretical analyses, we show that by focusing on increasing biases rather than weights, there is potential for significant enhancement in a neural network model’s performance. This approach offers an alternative perspective on optimizing information flow within neural networks. See source code [5]."
Indoor Visual Localization Using Point and Line Correspondences in Dense Colored Point Cloud,"Yuya Matsumoto, Gaku Nakano, Kazumine Ogura",NEC Corporation,0.0,,100.0,Japan,"We propose a novel pipeline called Loc-PL that uses both points and lines for indoor visual localization in dense colored point cloud. Loc-PL utilizes the spatially complementary relationship between points and lines to address challenging indoor issues. There are two successive camera pose estimation modules. The first improves robustness against repetitive patterns by considering the geometric consistency of points and lines. The second utilizes points and lines to refine poses by Perspective-m-Point-n-Line (PmPnL) and circumvents unstable localization due to locally concentrated matches caused by less-textured environments. The modules use different schemes to obtain line correspondences; the first finds line matches using RANSAC, which is effective for image pairs with large viewpoint gaps, and the second utilizes rendered images from dense point cloud to get them by feature line matching. In addition, we develop a simple but effective module for evaluating the correctness of camera poses using matched point distances across two images. The experimental results on a large dataset, InLoc, show that Loc-PL achieves the state-of-the-art in four out of six scores.",https://openaccess.thecvf.com/content/WACV2024/html/Matsumoto_Indoor_Visual_Localization_Using_Point_and_Line_Correspondences_in_Dense_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Matsumoto_Indoor_Visual_Localization_Using_Point_and_Line_Correspondences_in_Dense_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484200/,"['Location awareness', 'Point cloud compression', 'Visualization', 'Computer vision', 'Pose estimation', 'Pipelines', 'Green products']","['Point Cloud', 'Corresponding Points', 'Dense Point Cloud', 'Visual Localization', 'Indoor Visual Localization', 'Local Concentration', 'Image Pairs', 'Challenging Issue', 'Repetitive Patterns', 'Pose Estimation', 'Simple Modules', 'Camera Pose', 'Line Matching', 'Camera Pose Estimation', 'Similarity Measure', 'Image Pixels', 'Large-scale Datasets', 'Indoor Environments', 'Line Segment', 'Query Image', 'Inliers', 'Image Database', '3D Point', 'Feature Point Matching', 'Image Retrieval', 'Improve Localization Accuracy', 'Feature Points', 'Dynamic Objects', 'Viewpoint Changes']","['Algorithms', '3D computer vision', 'Applications', 'Robotics']",,"We propose a novel pipeline called Loc-PL that uses both points and lines for indoor visual localization in dense colored point cloud. Loc-PL utilizes the spatially complementary relationship between points and lines to address challenging indoor issues. There are two successive camera pose estimation modules. The first improves robustness against repetitive patterns by considering the geometric consistency of points and lines. The second utilizes points and lines to refine poses by Perspective-m-Point-n-Line (PmPnL) and circumvents unstable localization due to locally concentrated matches caused by less-textured environments. The modules use different schemes to obtain line correspondences; the first finds line matches using RANSAC, which is effective for image pairs with large viewpoint gaps, and the second utilizes rendered images from dense point cloud to get them by feature line matching. In addition, we develop a simple but effective module for evaluating the correctness of camera poses using matched point distances across two images. The experimental results on a large dataset, InLoc, show that Loc-PL achieves the state-of-the-art in four out of six scores."
IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting,"Tim J. Schoonbeek, Tim Houben, Hans Onvlee, Peter H.N. de With, Fons van der Sommen","ASML Research, Netherlands; Eindhoven University of Technology, Netherlands",100.0,Netherlands,0.0,,"Although action recognition for procedural tasks has received notable attention, it has a fundamental flaw in that no measure of success for actions is provided. This limits the applicability of such systems especially within the industrial domain, since the outcome of procedural actions is often significantly more important than the mere execution. To address this limitation, we define the novel task of procedure step recognition (PSR), focusing on recognizing the correct completion and order of procedural steps. Alongside the new task, we also present the multi-modal IndustReal dataset. Unlike currently available datasets, IndustReal contains procedural errors (such as omissions) as well as execution errors. A significant part of these errors are exclusively present in the validation and test sets, making IndustReal suitable to evaluate robustness of algorithms to new, unseen mistakes. Additionally, to encourage reproducibility and allow for scalable approaches trained on synthetic data, the 3D models of all parts are publicly available. Annotations and benchmark performance are provided for action recognition and assembly state detection, as well as the new PSR task. IndustReal, along with the code and model weights, is available at https://github.com/TimSchoonbeek/IndustReal.",https://openaccess.thecvf.com/content/WACV2024/html/Schoonbeek_IndustReal_A_Dataset_for_Procedure_Step_Recognition_Handling_Execution_Errors_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Schoonbeek_IndustReal_A_Dataset_for_Procedure_Step_Recognition_Handling_Execution_Errors_WACV_2024_paper.pdf,,https://github.com/TimSchoonbeek/IndustReal,2310.17323,main,Poster,https://ieeexplore.ieee.org/document/10484389/,"['Solid modeling', 'Three-dimensional displays', 'Scalability', 'Benchmark testing', 'Reproducibility of results', 'Robustness', 'Task analysis']","['Step Of Procedure', 'Execution Errors', 'Egocentric Videos', 'Action Recognition', 'Correct Order', 'Performance Benchmarks', 'Task Procedure', 'Completion Of Step', 'Multimodal Dataset', 'Procedural Errors', 'Industrial Domains', 'False Positive', 'System Performance', 'Convolutional Neural Network', 'F1 Score', '3D Printing', 'Object Detection', 'Eye-tracking', 'Complete Activities', 'Objective Conditions', 'Action Recognition Task', 'Executive Order', 'Execution Of Procedures', 'Assembly Procedure', 'Industrial Settings', 'Stereo Images', 'Edit Distance', 'Procedural Knowledge', 'Task Definition', 'Correct Execution']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Video recognition and understanding']",6,"Although action recognition for procedural tasks has received notable attention, it has a fundamental flaw in that no measure of success for actions is provided. This limits the applicability of such systems especially within the industrial domain, since the outcome of procedural actions is often significantly more important than the mere execution. To address this limitation, we define the novel task of procedure step recognition (PSR), focusing on recognizing the correct completion and order of procedural steps. Alongside the new task, we also present the multi-modal IndustReal dataset. Unlike currently available datasets, IndustReal contains procedural errors (such as omissions) as well as execution errors. A significant part of these errors are exclusively present in the validation and test sets, making IndustReal suitable to evaluate robustness of algorithms to new, unseen mistakes. Additionally, to encourage reproducibility and allow for scalable approaches trained on synthetic data, the 3D models of all parts are publicly available. Annotations and benchmark performance are provided for action recognition and assembly state detection, as well as the new PSR task. IndustReal, along with the code and model weights, is available at: https://github.com/TimSchoonbeek/IndustReal."
InfraParis: A Multi-Modal and Multi-Task Autonomous Driving Dataset,"Gianni Franchi, Marwane Hariat, Xuanlong Yu, Nacim Belkhir, Antoine Manzanera, David Filliat","Safrantech, Safran Group; U2IS, ENSTA Paris, IP Paris; SATIE, Paris-Saclay University; U2IS, ENSTA Paris, IP Paris",75.0,France,25.0,France,"Current deep neural networks (DNNs) for autonomous driving computer vision are typically trained on specific datasets that only involve a single type of data and urban scenes. Consequently, these models struggle to handle new objects, noise, nighttime conditions, and diverse scenarios, which is essential for safety-critical applications. Despite ongoing efforts to enhance the resilience of computer vision DNNs, progress has been sluggish, partly due to the absence of benchmarks featuring multiple modalities. We introduce a novel and versatile dataset named InfraParis that supports multiple tasks across three modalities: RGB, depth, and infrared. We assess various state-of-the-art baseline techniques, encompassing models for the tasks of semantic segmentation, object detection, and depth estimation.",https://openaccess.thecvf.com/content/WACV2024/html/Franchi_InfraParis_A_Multi-Modal_and_Multi-Task_Autonomous_Driving_Dataset_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Franchi_InfraParis_A_Multi-Modal_and_Multi-Task_Autonomous_Driving_Dataset_WACV_2024_paper.pdf,https://ensta-u2is.github.io/infraParis/,https://github.com/ensta-u2is/infraParis,2309.15751,main,Poster,https://ieeexplore.ieee.org/document/10484403/,"['Computer vision', 'Uncertainty', 'Computational modeling', 'Semantic segmentation', 'Object detection', 'Multitasking', 'Data models']","['Benchmark', 'Deep Neural Network', 'Object Detection', 'Semantic Segmentation', 'Multiple Tasks', 'Variety Of Scenarios', 'Depth Estimation', 'Urban Scenes', 'Training Set', 'Field Of View', 'Validation Set', 'Infrared Imaging', 'Pedestrian', 'Autonomous Vehicles', 'RGB Images', 'Projection Matrix', 'Olympic Games', 'Depth Values', 'Domain Adaptation', 'Foundation Model', 'Multimodal Dataset', 'KITTI Dataset', 'Road Markings', 'Monocular Depth Estimation']","['Algorithms', 'Datasets and evaluations', 'Applications', 'Autonomous Driving']",2,"Current deep neural networks (DNNs) for autonomous driving computer vision are typically trained on specific datasets that only involve a single type of data and urban scenes. Consequently, these models struggle to handle new objects, noise, nighttime conditions, and diverse scenarios, which is essential for safety-critical applications. Despite ongoing efforts to enhance the resilience of computer vision DNNs, progress has been sluggish, partly due to the absence of benchmarks featuring multiple modalities. We introduce a novel and versatile dataset named InfraParis that supports multiple tasks across three modalities: RGB, depth, and infrared. We assess various state-of-the-art baseline techniques, encompassing models for the tasks of semantic segmentation, object detection, and depth estimation. More visualizations and the download link for InfraParis are available at https://enstau2is.github.io/infraParis/."
Instruct Me More! Random Prompting for Visual In-Context Learning,"Jiahao Zhang, Bowen Wang, Liangzhi Li, Yuta Nakashima, Hajime Nagahara","Osaka University, Japan",100.0,Japan,0.0,,"Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at https://github.com/Jackieam/InMeMo.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Instruct_Me_More_Random_Prompting_for_Visual_In-Context_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Instruct_Me_More_Random_Prompting_for_Visual_In-Context_Learning_WACV_2024_paper.pdf,,https://github.com/Jackieam/InMeMo,2311.03648,main,Poster,https://ieeexplore.ieee.org/document/10483971/,"['Training', 'Visualization', 'Computer vision', 'Computational modeling', 'Perturbation methods', 'Object detection', 'Interference']","['Computer Vision', 'Instructive', 'Image Pairs', 'Segmentation Task', 'Object Segmentation', 'Large-scale Models', 'Segmentation Detection', 'Query Image', 'Input-output Pairs', 'Updated Model Parameters', 'Training Set', 'Input Image', 'Visual Features', 'Bounding Box', 'Domain Shift', 'Language Model', 'Ground Truth Labels', 'Strong Generalization', 'Inpainting', 'COCO Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",3,"Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at https://github.com/Jackieam/InMeMo."
Interaction Region Visual Transformer for Egocentric Action Anticipation,"Debaditya Roy, Ramanathan Rajendiran, Basura Fernando","Institute of High-Performance Computing, Agency for Science, Technology and Research, Singapore; Institute of High-Performance Computing, Agency for Science, Technology and Research, Singapore; Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore",66.66666666666666,Singapore,33.33333333333334,Singapore,"Human-object interaction (HOI) and temporal dynamics along the motion paths are the most important visual cues for egocentric action anticipation. Especially, interaction regions covering objects and the human hand reveal significant visual cues to predict future human actions. However, how to incorporate and capture these important visual cues in modern video Transformer architecture remains a challenge, especially because integrating inductive biases into Transformers is hard. We leverage the effective MotionFormer that models motion dynamics to incorporate interaction regions using spatial cross-attention and further infuse contextual information using trajectory cross-attention to obtain an interaction-centric video representation for action anticipation. We term our model InAViT which achieves state-of-the-art action anticipation performance on large-scale egocentric datasets EPICKTICHENS100 (EK100) and EGTEA Gaze+. On the EK100 evaluation server, InAViT is on top of the public leader board (at the time of submission) where it outperforms the second-best model by 3.3% on mean-top5 recall. We will release the code.",https://openaccess.thecvf.com/content/WACV2024/html/Roy_Interaction_Region_Visual_Transformer_for_Egocentric_Action_Anticipation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Roy_Interaction_Region_Visual_Transformer_for_Egocentric_Action_Anticipation_WACV_2024_paper.pdf,,https://github.com/LAHAproject/InAViT,,main,Poster,https://ieeexplore.ieee.org/document/10484435/,"['Visualization', 'Computer vision', 'Codes', 'Computational modeling', 'Dynamics', 'Computer architecture', 'Transformers']","['Interaction Region', 'Activity Prediction', 'Vision Transformer', 'Future Actions', 'Human Hand', 'Large Datasets', 'Interaction Model', 'Bounding Box', 'Mean Accuracy', 'Visible Changes', 'Video Clips', 'Object Features', 'Spatial Attention', 'Changes In Appearance', 'Action Execution', 'Change Objectives', 'Attention Layer', 'Object Appearance', 'Top-1 Accuracy', 'Object In Frame', 'Hand Changes', 'Appearance Of Regions', 'Hand Trajectory', 'Hand Representation', 'Biggest Improvement']","['Algorithms', 'Video recognition and understanding']",4,"Human-object interaction (HOI) and temporal dynamics along the motion paths are the most important visual cues for egocentric action anticipation. Especially, interaction regions covering objects and the human hand reveal significant visual cues to predict future human actions. However, how to incorporate and capture these important visual cues in modern video Transformer architecture remains a challenge. We leverage the effective MotionFormer that models motion dynamics to incorporate interaction regions using spatial cross-attention and further infuse contextual information using trajectory cross-attention to obtain an interaction-centric video representation for action anticipation. We term our model InAViT which achieves state-of-the-art action anticipation performance on large-scale egocentric datasets EPICKTICHENS100 (EK100) and EGTEA Gaze+. On the EK100 evaluation server, InAViT is on top of the public leader board (at the time of submission) where it outperforms the second-best model by 3.3% on mean-top5 recall. The code is available
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Interactive Network Perturbation Between Teacher and Students for Semi-Supervised Semantic Segmentation,"Hyuna Cho, Injun Choi, Suha Kwak, Won Hwa Kim",Pohang University of Science and Technology (POSTECH),100.0,South Korea,0.0,,"The current golden standard of semi-supervised semantic segmentation is to generate and exploit pseudo-supervision on unlabeled images. This approach is however susceptible to the quality of pseudo-supervision--training often becomes unstable particularly at early stages and biased to incorrect supervision. To address these issues, we propose a new semi-supervised learning framework, dubbed Guided Pseudo Supervision (GPS). GPS comprises three networks, i.e., a teacher and two separate students. The teacher is first trained with a small set of labeled data and provides stable initial pseudo-supervision on the unlabeled data to the students. The students interactively train each other under the supervision of the teacher, and once they are sufficiently trained, they offer feedback supervision to the teacher so that the teacher improves in subsequent iterations. This strategy enables more stable and faster convergence than previous works, and consequently, GPS achieved state-of-the-art performance on Pascal VOC 2012 and Cityscapes datasets in various experiment settings.",https://openaccess.thecvf.com/content/WACV2024/html/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cho_Interactive_Network_Perturbation_Between_Teacher_and_Students_for_Semi-Supervised_Semantic_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483722/,"['Training', 'Computer vision', 'Semantic segmentation', 'Perturbation methods', 'Computational modeling', 'Semisupervised learning', 'Benchmark testing']",,"['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"The current golden standard of semi-supervised semantic segmentation is to generate and exploit pseudo-supervision on unlabeled images. This approach is however susceptible to the quality of pseudo-supervision—training often becomes unstable particularly at early stages and biased to incorrect supervision. To address these issues, we propose a new semi-supervised learning framework, dubbed Guided Pseudo Supervision (GPS). GPS comprises three networks, i.e., a teacher and two separate students. The teacher is first trained with a small set of labeled data and provides stable initial pseudo-supervision on the unlabeled data to the students. The students interactively train each other under the supervision of the teacher, and once they are sufficiently trained, they offer feedback supervision to the teacher so that the teacher improves in subsequent iterations. This strategy enables more stable and faster convergence than previous works, and consequently, GPS achieved state-of-the-art performance on Pascal VOC 2012 and Cityscapes datasets in various experiment settings."
Interactive Segmentation for Diverse Gesture Types Without Context,"Josh Myers-Dean, Yifei Fan, Brian Price, Wilson Chan, Danna Gurari",Adobe Research; University of Colorado Boulder; University of Texas at Austin,66.66666666666666,USA,33.33333333333334,USA,"Interactive segmentation entails a human marking an image to guide how a model either creates or edits a segmentation. Our work addresses limitations of existing methods: they either only support one gesture type for marking an image (e.g., either clicks or scribbles) or require knowledge of the gesture type being employed, and require specifying whether marked regions should be included versus excluded in the final segmentation. We instead propose a simplified interactive segmentation task where a user only must mark an image, where the input can be of any gesture type without specifying the gesture type. We support this new task by introducing the first interactive segmentation dataset with multiple gesture types as well as a new evaluation metric capable of holistically evaluating interactive segmentation algorithms. We then analyze numerous interactive segmentation algorithms, including ones adapted for our novel task. While we observe promising performance overall, we also highlight areas for future improvement. To facilitate further extensions of this work, we publicly share our new dataset at https://github.com/joshmyersdean/dig.",https://openaccess.thecvf.com/content/WACV2024/html/Myers-Dean_Interactive_Segmentation_for_Diverse_Gesture_Types_Without_Context_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Myers-Dean_Interactive_Segmentation_for_Diverse_Gesture_Types_Without_Context_WACV_2024_paper.pdf,,https://github.com/joshmyersdean/dig,2307.10518,main,Poster,https://ieeexplore.ieee.org/document/10484472/,"['Measurement', 'Image segmentation', 'Computer vision', 'Task analysis']","['Interactive Segmentation', 'Gesture Types', 'Segmentation Task', 'Final Segmentation', 'Scribble', 'Segmentation Dataset', 'Rectangular', 'Supplemental Material', 'Percentage Points', 'User Study', 'Part Of Region', 'Image Regions', 'Bounding Box', 'Segmented Regions', 'Extreme Points', 'Object Segmentation', 'Test Split', 'Top Performers', 'Human Input', 'Salient Object', 'Previous Segment', 'Disparities In Performance', 'Euclidean Distance Function', 'Negative Context']","['Applications', 'Arts / games / social media', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",1,"Interactive segmentation entails a human marking an image to guide how a model either creates or edits a segmentation. Our work addresses limitations of existing methods: they either only support one gesture type for marking an image (e.g., either clicks or scribbles) or require knowledge of the gesture type being employed, and require specifying whether marked regions should be included versus excluded in the final segmentation. We instead propose a simplified interactive segmentation task where a user only must mark an image, where the input can be of any gesture type without specifying the gesture type. We support this new task by introducing the first interactive segmentation dataset with multiple gesture types as well as a new evaluation metric capable of holistically evaluating interactive segmentation algorithms. We then analyze numerous interactive segmentation algorithms, including ones adapted for our novel task. While we observe promising performance overall, we also highlight areas for future improvement. To facilitate further extensions of this work, we publicly share our new dataset at https://github.com/joshmyersdean/dig."
Interpretable Object Recognition by Semantic Prototype Analysis,"Qiyang Wan, Ruiping Wang, Xilin Chen","Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China",100.0,China,0.0,,"People can usually give reasons for recognizing a particular object as a specific category, using various means such as body language (by pointing out) and natural language (by telling). This inspires us to develop a recognition model with such principles to explain the recognition process to enhance human trust. We propose Semantic Prototype Analysis Network (SPANet), an interpretable object recognition approach that enables models to explicate the decision process more lucidly and comprehensibly to humans by ""pointing out where to focus"" and ""telling about why it is"" simultaneously. With the proposed method, some part prototypes with semantic concepts will be provided to elaborate on the classification together with a group of visualized samples to achieve both part-wise and semantic interpretability. The results of extensive experiments demonstrate that SPANet is able to recognize objects almost as well as the non-interpretable models, at the same time generating intelligible explanations for its decision process.",https://openaccess.thecvf.com/content/WACV2024/html/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484212/,"['Visualization', 'Computer vision', 'Analytical models', 'Semantics', 'Natural languages', 'Prototypes', 'Object recognition']","['Object Recognition', 'Semantic Prototypes', 'Natural Language', 'Body Language', 'Recognition Model', 'Semantic Knowledge', 'Semantic Interpretation', 'Training Set', 'Decision Tree', 'Local Features', 'Input Image', 'User Study', 'Class Labels', 'Latent Space', 'Concept Of Learning', 'Visual Interpretation', 'Framework Of Method', 'Sample Categories', 'Semantic Labels', 'Car Model', 'Vision Transformer', 'Form Of Explanation', 'Reconstruction Module', 'Gene-based Methods', 'Semantic Description', 'Fine-tuning Process', 'Local Image Features', 'Similarity Score', 'Image Patches', 'Feature Maps']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"People can usually give reasons for recognizing a particular object as a specific category, using various means such as body language (by pointing out) and natural language (by telling). This inspires us to develop a recognition model with such principles to explain the recognition process to enhance human trust. We propose Semantic Prototype Analysis Network (SPANet), an interpretable object recognition approach that enables models to explicate the decision process more lucidly and comprehensibly to humans by ""pointing out where to focus"" and ""telling about why it is"" simultaneously. With the proposed method, some part prototypes with semantic concepts will be provided to elaborate on the classification together with a group of visualized samples to achieve both part-wise and semantic interpretability. The results of extensive experiments demonstrate that SPANet is able to recognize objects almost as well as the non-interpretable models, at the same time generating intelligible explanations for its decision process."
Intrinsic Hand Avatar: Illumination-Aware Hand Appearance and Shape Reconstruction From Monocular RGB Video,"Pratik Kalshetti, Parag Chaudhuri",Indian Institute of Technology Bombay,100.0,India,0.0,,"Reconstructing a user-specific hand avatar is essential for a personalized experience in augmented and virtual reality systems. Current state-of-the-art avatar reconstruction methods use implicit representations to capture detailed geometry and appearance combined with neural rendering. However, these methods rely on a complicated multi-view setup, do not explicitly handle environment lighting leading to baked-in illumination and self-shadows, and require long hours for training. We present a method to reconstruct a hand avatar from a monocular RGB video of a user's hand in arbitrary hand poses captured under real-world environment lighting. Specifically, our method jointly optimizes shape, appearance, and lighting parameters using a realistic shading model in a differentiable rendering framework incorporating Monte Carlo path tracing. Despite relying on physically-based rendering, our method can complete the reconstruction within minutes. In contrast to existing work, our method disentangles intrinsic properties of the underlying appearance and environment lighting, leading to realistic self-shadows. We compare our method with state-of-the-art hand avatar reconstruction methods and observe that it outperforms them on all commonly used metrics. We also evaluate our method on our captured dataset to emphasize its generalization capability. Finally, we demonstrate applications of our intrinsic hand avatar on novel pose synthesis and relighting. We plan to release our code to aid further research.",https://openaccess.thecvf.com/content/WACV2024/html/Kalshetti_Intrinsic_Hand_Avatar_Illumination-Aware_Hand_Appearance_and_Shape_Reconstruction_From_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kalshetti_Intrinsic_Hand_Avatar_Illumination-Aware_Hand_Appearance_and_Shape_Reconstruction_From_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483601/,"['Training', 'Measurement', 'Geometry', 'Solid modeling', 'Monte Carlo methods', 'Shape', 'Avatars']","['Hand Shape', 'Avatar Hand', 'Monocular RGB Video', 'Light Environment', 'Real-world Environments', 'Model Parameters', 'Denoising', 'Training Time', 'Material Parameters', 'Image Collection', 'Normal Error', 'Immersive Experience', 'High Dynamic Range', 'Human Hand', 'Triangular Mesh', 'Mesh Model', 'RGB Camera', 'Virtual Avatar', 'Probe Light', 'Monte Carlo Integration', 'Normal Map', 'Accurate Geometry', 'Signed Distance Function']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision']",3,"Reconstructing a user-specific hand avatar is essential for a personalized experience in augmented and virtual reality systems. Current state-of-the-art avatar reconstruction methods use implicit representations to capture detailed geometry and appearance combined with neural rendering. However, these methods rely on a complicated multi-view setup, do not explicitly handle environment lighting leading to baked-in illumination and self-shadows, and require long hours for training. We present a method to reconstruct a hand avatar from a monocular RGB video of a user’s hand in arbitrary hand poses captured under real-world environment lighting. Specifically, our method jointly optimizes shape, appearance, and lighting parameters using a realistic shading model in a differentiable rendering framework incorporating Monte Carlo path tracing. Despite relying on physically-based rendering, our method can complete the reconstruction within minutes. In contrast to existing work, our method disentangles intrinsic properties of the underlying appearance and environment lighting, leading to realistic self-shadows. We compare our method with state-of-the-art hand avatar reconstruction methods and observe that it outperforms them on all commonly used metrics. We also evaluate our method on our captured dataset to emphasize its generalization capability. Finally, we demonstrate applications of our intrinsic hand avatar on novel pose synthesis and relighting. We plan to release our code to aid further research."
Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection,"Kyle Buettner, Adriana Kovashka","Department of Computer Science, University of Pittsburgh, PA, USA; Intelligent Systems Program, University of Pittsburgh, PA, USA",100.0,USA,0.0,,"Vision-language alignment learned from image-caption pairs has been shown to benefit tasks like object recognition and detection. Methods are mostly evaluated in terms of how well object class names are learned, but captions also contain rich attribute context that should be considered when learning object alignment. It is unclear how methods use this context in learning, as well as whether models succeed when tasks require attribute and object understanding. To address this gap, we conduct extensive analysis of the role of attributes in vision-language models. We specifically measure model sensitivity to the presence and meaning of attribute context, gauging influence on object embeddings through unsupervised phrase grounding and classification via description methods. We further evaluate the utility of attribute context in training for open-vocabulary object detection, fine-grained text-region retrieval, and attribution tasks. Our results show that attribute context can be wasted when learning alignment for detection, attribute meaning is not adequately considered in embeddings, and describing classes by only their attributes is ineffective. A viable strategy that we find to increase benefits from attributes is contrastive training with adjective-based negative captions.",https://openaccess.thecvf.com/content/WACV2024/html/Buettner_Investigating_the_Role_of_Attribute_Context_in_Vision-Language_Models_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Buettner_Investigating_the_Role_of_Attribute_Context_in_Vision-Language_Models_for_WACV_2024_paper.pdf,,,2303.10093,main,Poster,https://ieeexplore.ieee.org/document/10484194/,"['Training', 'Computer vision', 'Analytical models', 'Sensitivity', 'Grounding', 'Computational modeling', 'Object detection']","['Object Detection', 'Object Recognition', 'Vision-language Models', 'Learning Objectives', 'Class Assignment', 'Role In Properties', 'Fine-tuned', 'Random Sampling', 'Vocabulary', 'Negative Samples', 'Bounding Box', 'Detection Task', 'Dot Product', 'Word Embedding', 'Self-supervised Learning', 'Base Classes', 'Faster R-CNN', 'Class Weights', 'Ground Objects', 'Brown Bears', 'Plausible Case', 'Image Encoder', 'Order Perturbation', 'Bounding Box Annotations', 'Large Birds']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Vision-language alignment learned from image-caption pairs has been shown to benefit tasks like object recognition and detection. Methods are mostly evaluated in terms of how well object class names are learned, but captions also contain rich attribute context that should be considered when learning object alignment. It is unclear how methods use this context in learning, as well as whether models succeed when tasks require attribute and object understanding. To address this gap, we conduct extensive analysis of the role of attributes in vision-language models. We specifically measure model sensitivity to the presence and meaning of attribute context, gauging influence on object embeddings through unsupervised phrase grounding and classification via description methods. We further evaluate the utility of attribute context in training for open-vocabulary object detection, fine-grained text-region retrieval, and attribution tasks. Our results show that attribute context can be wasted when learning alignment for detection, attribute meaning is not adequately considered in embeddings, and describing classes by only their attributes is ineffective. A viable strategy that we find to increase benefits from attributes is contrastive training with adjective-based negative captions."
Iterative Multi-Granular Image Editing Using Diffusion Models,"K. J. Joseph, Prateksha Udhayanan, Tripti Shukla, Aishwarya Agarwal, Srikrishna Karanam, Koustava Goswami, Balaji Vasan Srinivasan","Adobe Research, Bangalore, India",0.0,,100.0,USA,"Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control operation for multi-granular control. We introduce a new benchmark dataset to evaluate our newly proposed setting. We conduct exhaustive quantitatively and qualitatively evaluation against recent state-of-the-art approaches adapted to our task, to being out the mettle of EMILIE. We hope our work would attract attention to this newly identified, pragmatic problem setting.",https://openaccess.thecvf.com/content/WACV2024/html/Joseph_Iterative_Multi-Granular_Image_Editing_Using_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Joseph_Iterative_Multi-Granular_Image_Editing_Using_Diffusion_Models_WACV_2024_paper.pdf,,,2309.00613,main,Poster,https://ieeexplore.ieee.org/document/10483816/,"['Training', 'Adaptation models', 'Visualization', 'Image synthesis', 'Process control', 'Modulation', 'Benchmark testing']","['Diffusion Model', 'Image Editing', 'Iterative Image', 'Benchmark Datasets', 'Image Synthesis', 'Creative Professionals', 'Semantic', 'Input Image', 'Diffusion Process', 'Generative Adversarial Networks', 'Latent Space', 'Textual Descriptions', 'Realistic Images', 'Version Of Image', 'Inset Image', 'Previous Editions', 'Energy-based Model']","['Applications', 'Commercial / retail', 'Applications', 'Arts / games / social media', 'Applications', 'Virtual / augmented reality']",2,"Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control operation for multi-granular control. We introduce a new benchmark dataset to evaluate our newly proposed setting. We conduct exhaustive quantitatively and qualitatively evaluation against recent state-of-the-art approaches adapted to our task, to being out the mettle of EMILIE. We hope our work would attract attention to this newly identified, pragmatic problem setting."
JOADAA: Joint Online Action Detection and Action Anticipation,"Mohammed Guermal, Abid Ali, Rui Dai, François Brémond","Universit ´e Cote d’Azur, France; Inria, France",100.0,France,0.0,,"Action anticipation involves forecasting future actions by connecting the past events to future ones. However, this reasoning ignores the real-life hierarchy of events which is considered to be of three main parts: past, present, and future. We argue that considering these three main parts and their dependencies could improve performance. On the other hand, online action detection is the task of predicting actions in a streaming manner. In this case, one has access only to the past and present information. Therefore, in online action detection (OAD) the existing approaches miss semantics or future information which limits the performance of existing approaches. To sum up, for both of these tasks, the complete set of knowledge (past-present-future) is missing, which makes it challenging to infer action dependencies achieving good performances. To address this limitation, we propose fusing both tasks in one uniform architecture. By combining action anticipation and online action detection, our approach can cover the missing dependencies of future information in online action detection. This method, referred as JOADAA, presents a uniform model that jointly performs action anticipation and online action detection. We validate our proposed model on three challenging datasets: THUMOS, which is a sparsely annotated dataset with one action per time step, CHARADES and Multi-THUMOS, two densely annotated datasets, with more complex scenarios. JOADAA achieves SOTA results on these benchmarks for both tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Guermal_JOADAA_Joint_Online_Action_Detection_and_Action_Anticipation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Guermal_JOADAA_Joint_Online_Action_Detection_and_Action_Anticipation_WACV_2024_paper.pdf,,,2309.06130,main,Poster,https://ieeexplore.ieee.org/document/10484333/,"['Computer vision', 'Fuses', 'Computational modeling', 'Semantics', 'Noise', 'Computer architecture', 'Transformers']","['Online Activities', 'Activity Prediction', 'Action Detection', 'Online Action Detection', 'Time Step', 'Future Actions', 'Annotated Dataset', 'Past Information', 'Future Information', 'Future Ones', 'Transformer', 'Computer Vision', 'Feature Maps', 'Attention Mechanism', 'Ongoing Activity', 'Types Of Datasets', 'Self-driving', 'Current Frame', 'Global Knowledge', 'Long-range Dependencies', 'Temporal Convolutional Network', 'Frame Features', 'Global Dependencies', 'Transformer Decoder', 'Transformer Encoder', 'Simple Datasets', 'Updated Feature', 'Future Frames', 'Online Detection', 'Long-term Information']","['Algorithms', 'Video recognition and understanding']",1,"Action anticipation involves forecasting future actions by connecting past events to future ones. However, this reasoning ignores the real-life hierarchy of events which is considered to be composed of three main parts: past, present, and future. We argue that considering these three main parts and their dependencies could improve performance. On the other hand, online action detection is the task of predicting actions in a streaming manner. In this case, one has access only to the past and present information. Therefore, in online action detection (OAD) the existing approaches miss semantics or future information which limits their performance. To sum up, for both of these tasks, the complete set of knowledge (past-present-future) is missing, which makes it challenging to infer action dependencies, therefore having low performances. To address this limitation, we propose to fuse both tasks into a single uniform architecture. By combining action anticipation and online action detection, our approach can cover the missing dependencies of future information in online action detection. This method referred to as JOADAA, presents a uniform model that jointly performs action anticipation and online action detection. We validate our proposed model on three challenging datasets: THUMOS’14, which is a sparsely annotated dataset with one action per time step, CHARADES, and Multi-THUMOS, two densely annotated datasets with more complex scenarios. JOADAA achieves SOTA results on these benchmarks for both tasks."
Joint 3D Shape and Motion Estimation From Rolling Shutter Light-Field Images,"Hermès McGriff, Renato Martins, Nicolas Andreff, Cédric Demonceaux","Université de Lorraine, CNRS, Inria, LORIA; Université de Franche-Comté, CNRS UMR 6174 FEMTO-ST; Université de Bourgogne, CNRS UMR 6303 ICB",100.0,France,0.0,,"In this paper, we propose an approach to address the problem of 3D reconstruction of scenes from a single image captured by a light-field camera equipped with a rolling shutter sensor. Our method leverages the 3D information cues present in the light-field and the motion information provided by the rolling shutter effect. We present a generic model for the imaging process of this sensor and a two-stage algorithm that minimizes the re-projection error while considering the position and motion of the camera in a motion-shape bundle adjustment estimation strategy. Thereby, we provide an instantaneous 3D shape-and-pose-and-velocity sensing paradigm. To the best of our knowledge, this is the first study to leverage this type of sensor for this purpose. We also present a new benchmark dataset composed of different light-fields showing rolling shutter effects, which can be used as a common base to improve the evaluation and tracking the progress in the field. We demonstrate the effectiveness and advantages of our approach through several experiments conducted for different scenes and types of motions. The source code and dataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF",https://openaccess.thecvf.com/content/WACV2024/html/McGriff_Joint_3D_Shape_and_Motion_Estimation_From_Rolling_Shutter_Light-Field_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/McGriff_Joint_3D_Shape_and_Motion_Estimation_From_Rolling_Shutter_Light-Field_WACV_2024_paper.pdf,,https://github.com/ICB-Vision-AI/RSLF,2311.01292,main,Poster,https://ieeexplore.ieee.org/document/10483623/,"['Solid modeling', 'Three-dimensional displays', 'Shape', 'Tracking', 'Motion estimation', 'Source coding', 'Robot vision systems']","['Motion Estimation', 'Shape Estimation', '3D Joint', 'Rolling Shutter', 'Light Field Images', 'Camera Motion', 'Reprojection Error', 'Bundle Adjustment', 'Camera Array', 'Point Cloud', 'Depth Map', 'Image Point', 'Model Projections', 'Linear Velocity', 'Depth Estimation', 'Coordinate Frame', 'Center Of Rotation', 'Principal Plane', 'Plan View', 'Projection Point', 'Microlens Array', '3D Scene', 'Camera Movement', 'Multi-view Stereo', 'Scene Geometry', 'Conventional Camera', 'Camera Pose', 'Scene Structure', 'Qualitative Examples', 'Scene Point']","['Algorithms', '3D computer vision', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Low-level and physics-based vision']",1,"In this paper, we propose an approach to address the problem of 3D reconstruction of scenes from a single image captured by a light-field camera equipped with a rolling shutter sensor. Our method leverages the 3D information cues present in the light-field and the motion information provided by the rolling shutter effect. We present a generic model for the imaging process of this sensor and a two-stage algorithm that minimizes the re-projection error while considering the position and motion of the camera in a motion-shape bundle adjustment estimation strategy. Thereby, we provide an instantaneous 3D shape-and-pose-and-velocity sensing paradigm. To the best of our knowledge, this is the first study to leverage this type of sensor for this purpose. We also present a new benchmark dataset composed of different light-fields showing rolling shutter effects, which can be used as a common base to improve the evaluation and tracking the progress in the field. We demonstrate the effectiveness and advantages of our approach through several experiments conducted for different scenes and types of motions. The source code and dataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF."
Joint Depth Prediction and Semantic Segmentation With Multi-View SAM,"Mykhailo Shvets, Dongxu Zhao, Marc Niethammer, Roni Sengupta, Alexander C. Berg","University of North Carolina, Chapel Hill; University of California, Irvine",100.0,USA,0.0,,"Multi-task approaches to joint depth and segmentation prediction are well-studied for monocular images. Yet, predictions from a single-view are inherently limited, while multiple views are available in many robotics applications. On the other end of the spectrum, video-based and full 3D methods require numerous frames to perform reconstruction and segmentation. With this work we propose a Multi-View Stereo (MVS) technique for depth prediction that benefits from rich semantic features of the Segment Anything Model (SAM). This enhanced depth prediction, in turn, serves as a prompt to our Transformer-based semantic segmentation decoder. We report the mutual benefit that both tasks enjoy in our quantitative and qualitative studies on the ScanNet dataset. Our approach consistently outperforms single-task MVS and segmentation models, along with multi-task monocular methods.",https://openaccess.thecvf.com/content/WACV2024/html/Shvets_Joint_Depth_Prediction_and_Semantic_Segmentation_With_Multi-View_SAM_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shvets_Joint_Depth_Prediction_and_Semantic_Segmentation_With_Multi-View_SAM_WACV_2024_paper.pdf,,,2311.00134,main,Poster,https://ieeexplore.ieee.org/document/10483783/,"['Three-dimensional displays', 'Semantic segmentation', 'Semantics', 'Estimation', 'Predictive models', 'Multitasking', 'Transformers']","['Semantic Segmentation', 'Depth Prediction', 'Joint Depth', 'End Of The Spectrum', 'Segmentation Model', 'Semantic Features', 'Robotic Applications', 'Rich Features', 'Segmentation Prediction', 'Multi-view Stereo', 'Joint Segmentation', 'Root Mean Square Error', 'Training Set', 'Decoding', 'Image Features', 'Multiple Images', 'Depth Map', 'Range Of Tasks', 'Feature Matching', 'Segmentation Map', 'Cost Volume', 'Depth Estimation', '2D Feature', 'Vision Transformer', 'Image Embedding', 'Multi-task Learning', 'Monocular Depth', '3D Scene', 'Multi-view Images', 'Depth Error']","['Algorithms', 'Image recognition and understanding', 'Algorithms', '3D computer vision']",1,"Multi-task approaches to joint depth and segmentation prediction are well-studied for monocular images. Yet, predictions from a single-view are inherently limited, while multiple views are available in many robotics applications. On the other end of the spectrum, video-based and full 3D methods require numerous frames to perform reconstruction and segmentation. With this work we propose a Multi-View Stereo (MVS) technique for depth prediction that benefits from rich semantic features of the Segment Anything Model (SAM). This enhanced depth prediction, in turn, serves as a prompt to our Transformer-based semantic segmentation decoder. We report the mutual benefit that both tasks enjoy in our quantitative and qualitative studies on the ScanNet dataset. Our approach consistently outperforms single-task MVS and segmentation models, along with multi-task monocular methods."
Kaizen: Practical Self-Supervised Continual Learning With Continual Fine-Tuning,"Chi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Cecilia Mascolo, Akhil Mathur","Nokia Bell Labs, UK; University of Cambridge, UK",50.0,UK,50.0,UK,"Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully designed loss function. By using a set of comprehensive evaluation metrics reflecting different aspects of continual learning, we demonstrated that Kaizen significantly outperforms previous SSL models in competitive vision benchmarks, with up to 16.5% accuracy improvement on split CIFAR-100. Kaizen is able to balance the trade-off between knowledge retention and learning from new data with an end-to-end model, paving the way for practical deployment of continual learning systems.",https://openaccess.thecvf.com/content/WACV2024/html/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.pdf,,https://github.com/dr-bell/kaizen,,main,Poster,https://ieeexplore.ieee.org/document/10484222/,"['Training', 'Measurement', 'Computer vision', 'Adaptation models', 'Computational modeling', 'Self-supervised learning', 'Feature extraction']","['Incremental Learning', 'Self-supervised Learning', 'Loss Function', 'Vision Tasks', 'Knowledge Retention', 'Catastrophic Forgetting', 'Model Performance', 'Deep Learning', 'Final Step', 'Learning Models', 'Supervised Learning', 'Classification Task', 'Continuous Process', 'Unsupervised Learning', 'Average Accuracy', 'Data Streams', 'Unlabeled Data', 'Current Task', 'Task Data', 'Performance Of Different Methods', 'Self-supervised Learning Methods', 'Previous Tasks', 'Continuous Learning Process', 'Forward Transfer', 'Labeling Task', 'Categorical Cross-entropy Loss', 'Real-world Scenarios', 'Classifier Training']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully designed loss function. By using a set of comprehensive evaluation metrics reflecting different aspects of continual learning, we demonstrated that Kaizen significantly outperforms previous SSL models in competitive vision benchmarks, with up to 16.5% accuracy improvement on split CIFAR-100. Kaizen is able to balance the trade-off between knowledge retention and learning from new data with an end-to-end model, paving the way for practical deployment of continual learning systems."
LAVSS: Location-Guided Audio-Visual Spatial Audio Separation,"Yuxin Ye, Wenming Yang, Yapeng Tian","Department of Computer Science, The University of Texas at Dallas, USA; Shenzhen International Graduate School, Tsinghua University, China",100.0,"China, USA",0.0,,"Existing machine learning research has achieved promising results in monaural audio-visual separation (MAVS). However, most MAVS methods purely consider what the sound source is, not where it is located. This can be a problem in VR/AR scenarios, where listeners need to be able to distinguish between similar audio sources located in different directions. To address this limitation, we have generalized MAVS to spatial audio separation and proposed LAVSS: a location-guided audio-visual spatial audio separator. LAVSS is inspired by the correlation between spatial audio and visual location. We introduce the phase difference carried by binaural audio as spatial cues, and we utilize positional representations of sounding objects as additional modality guidance. We also leverage multi-level cross-modal attention to perform visual-positional collaboration with audio features. In addition, we adopt a pre-trained monaural separator to transfer knowledge from rich mono sounds to boost spatial audio separation. This exploits the correlation between monaural and binaural channels. Experiments on the FAIR-Play dataset demonstrate the superiority of the proposed LAVSS over existing benchmarks of audio-visual separation. Our project page: https://yyx666660.github.io/LAVSS/.",https://openaccess.thecvf.com/content/WACV2024/html/Ye_LAVSS_Location-Guided_Audio-Visual_Spatial_Audio_Separation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ye_LAVSS_Location-Guided_Audio-Visual_Spatial_Audio_Separation_WACV_2024_paper.pdf,https://yyx666660.github.io/LAVSS/,https://github.com/yyx666660/LAVSS,2310.20446,main,Poster,https://ieeexplore.ieee.org/document/10483896/,"['Visualization', 'Computer vision', 'Correlation', 'Particle separators', 'Spatial audio', 'Transfer learning', 'Collaboration']","['Environmentally Friendly', 'Spatial Separation', 'Audio Separation', 'Sound Source', 'Spatial Cues', 'Similar Sources', 'Position Representation', 'Convolutional Layers', 'Spatial Information', 'Visual Features', 'Transfer Learning', 'Bounding Box', 'Video Frames', 'Network Input', 'Appearance Features', 'External Dataset', 'Source Separation', 'Position Features', 'Separate Networks', 'Multi-scale Network', 'Signal-to-interference Ratio', 'Position Embedding', 'Object Regions', 'Left Channels', 'Positional Encoding', 'Visual Guidance', 'Frequency Dimension', 'Separation Performance', 'Audiovisual Speech', 'Visual Information']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Existing machine learning research has achieved promising results in monaural audio-visual separation (MAVS). However, most MAVS methods purely consider what the sound source is, not where it is located. This can be a problem in VR/AR scenarios, where listeners need to be able to distinguish between similar audio sources located in different directions. To address this limitation, we have generalized MAVS to spatial audio separation and proposed LAVSS: a location-guided audio-visual spatial audio separator. LAVSS is inspired by the correlation between spatial audio and visual location. We introduce the phase difference carried by binaural audio as spatial cues, and we utilize positional representations of sounding objects as additional modality guidance. We also leverage multi-level cross-modal attention to perform visual-positional collaboration with audio features. In addition, we adopt a pre-trained monaural separator to transfer knowledge from rich mono sounds to boost spatial audio separation. This exploits the correlation between monaural and binaural channels. Experiments on the FAIR-Play dataset demonstrate the superiority of the proposed LAVSS over existing benchmarks of audio-visual separation. Our project page: https://yyx666660.github.io/LAVSS/."
LIVENet: A Novel Network for Real-World Low-Light Image Denoising and Enhancement,"Dhruv Makwana, Gayatri Deshmukh, Onkar Susladkar, Sparsh Mittal, Sai Chandra Teja R.",IIT Roorkee; Green PMU Semi Pvt Ltd; Independent Researcher,33.33333333333333,India,66.66666666666667,India,"Low-light image enhancement (LLIE) is the process of improving the quality of images taken in low-light conditions while striking a balance between enhancing image illumination and maintaining their natural appearance. This involves reducing noise, enhancing details, and correcting colors, all while avoiding artifacts such as halo effects or color distortions. We propose LIVENet, a novel deep neural network that jointly performs noise reduction on lowlight images and enhances illumination and texture details. LIVENet has two stages: the image enhancement stage and the refinement stage. For the image enhancement stage, we propose a Latent Subspace Denoising Block (LSDB) that uses a low-rank representation of low light features to suppress the noise and predict a noise-free grayscale image. We propose enhancing an RGB image by eliminating noise. This is done by converting it into YCbCr color space and replacing the noisy luminance (Y) channel with the predicted noise-free grayscale image. LIVENet also predicts the transmission map and atmospheric light in the image enhancement stage. LIVENet produces an enhanced image with rich color and illumination by feeding them to an atmospheric scattering model. In the refinement stage, the texture information from the grayscale image is incorporated into the improved image using a Spatial Feature Transform (SFT) layer. Experiments on different datasets demonstrate that LIVENet's enhanced images consistently outperform previous techniques across various quality metrics. The source code can be obtained from https://github.com/CandleLabAI/LiveNet.",https://openaccess.thecvf.com/content/WACV2024/html/Makwana_LIVENet_A_Novel_Network_for_Real-World_Low-Light_Image_Denoising_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Makwana_LIVENet_A_Novel_Network_for_Real-World_Low-Light_Image_Denoising_and_WACV_2024_paper.pdf,,https://github.com/CandleLabAI/LiveNet,,main,Poster,https://ieeexplore.ieee.org/document/10483686/,"['Image color analysis', 'Atmospheric modeling', 'Source coding', 'Noise reduction', 'Lighting', 'Transforms', 'Gray-scale']","['Image Enhancement', 'Low-light Image', 'Low-light Image Enhancement', 'Network For Image Denoising', 'Image Quality', 'Deep Neural Network', 'Grayscale Images', 'RGB Images', 'Color Space', 'Low Light Conditions', 'Halo Effect', 'Color Distortion', 'Transmission Map', 'Diffuse Solar Radiation', 'Atmospheric Light', 'Noise-free Image', 'Transformer', 'Local Information', 'Image Regions', 'Unsupervised Methods', 'Real Color', 'Image Noise', 'L1 Loss', 'Dark Images', 'RGB Color Space', 'Color Information', 'Color Histogram', 'Dark Regions', 'Human Vision', 'Output Image']","['Applications', 'Visualization', 'Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Low-light image enhancement (LLIE) is the process of improving the quality of images taken in low-light conditions while striking a balance between enhancing image illumination and maintaining their natural appearance. This involves reducing noise, enhancing details, and correcting colors, all while avoiding artifacts such as halo effects or color distortions. We propose LIVENet, a novel deep neural network that jointly performs noise reduction on lowlight images and enhances illumination and texture details. LIVENet has two stages: the image enhancement stage and the refinement stage. For the image enhancement stage, we propose a Latent Subspace Denoising Block (LSDB) that uses a low-rank representation of low-light features to suppress the noise and predict a noise-free grayscale image. We propose enhancing an RGB image by eliminating noise. This is done by converting it into YCbCr color space and replacing the noisy luminance (Y) channel with the predicted noise-free grayscale image. LIVENet also predicts the transmission map and atmospheric light in the image enhancement stage. LIVENet produces an enhanced image with rich color and illumination by feeding them to an atmospheric scattering model. In the refinement stage, the texture information from the grayscale image is incorporated into the improved image using a Spatial Feature Transform (SFT) layer. Experiments on different datasets demonstrate that LIVENet’s enhanced images consistently outperform previous techniques across various quality metrics. The source code can be obtained from https://github.com/CandleLabAI/LiveNet."
"LInKs ""Lifting Independent Keypoints"" - Partial Pose Lifting for Occlusion Handling With Improved Accuracy in 2D-3D Human Pose Estimation","Peter Hardy, Hansung Kim","University of Southampton, Vision Learning and Control, ECS",100.0,UK,0.0,,"We present LInKs, a novel unsupervised learning method to recover 3D human poses from 2D kinematic skeletons obtained from a single image, even when occlusions are present. Our approach follows a unique two-step process, which involves first lifting the occluded 2D pose to the 3D domain, followed by filling in the occluded parts using the partially reconstructed 3D coordinates. This lift-then-fill approach leads to significantly more accurate results compared to models that complete the pose in 2D space alone. Additionally, we improve the stability and likelihood estimation of normalising flows through a custom sampling function replacing PCA dimensionality reduction used in prior work. Furthermore, we are the first to investigate if different parts of the 2D kinematic skeleton can be lifted independently which we find by itself reduces the error of current lifting approaches. We attribute this to the reduction of long-range keypoint correlations. In our detailed evaluation, we quantify the error under various realistic occlusion scenarios, showcasing the versatility and applicability of our model. Our results consistently demonstrate the superiority of handling all types of occlusions in 3D space when compared to others that complete the pose in 2D space. Our approach also exhibits consistent accuracy in scenarios without occlusion, as evidenced by a 7.9% reduction in reconstruction error compared to prior works on the Human3.6M dataset. Furthermore, our method excels in accurately retrieving complete 3D poses even in the presence of occlusions, making it highly applicable in situations where complete 2D pose information is unavailable.",https://openaccess.thecvf.com/content/WACV2024/html/Hardy_LInKs_Lifting_Independent_Keypoints_-_Partial_Pose_Lifting_for_Occlusion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hardy_LInKs_Lifting_Independent_Keypoints_-_Partial_Pose_Lifting_for_Occlusion_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
LP-OVOD: Open-Vocabulary Object Detection by Linear Probing,"Chau Pham, Truong Vu, Khoi Nguyen",VinAI Research,0.0,,100.0,Vietnam,"This paper addresses the challenging problem of open-vocabulary object detection (OVOD) where an object detector must identify both seen and unseen classes in test images without labeled examples of the unseen classes in training. A typical approach for OVOD is to use joint text-image embeddings of CLIP to assign box proposals to their closest text label. However, this method has a critical issue: many low-quality boxes, such as over- and under-covered-object boxes, have the same similarity score as high-quality boxes since CLIP is not trained on exact object location information. To address this issue, we propose a novel method, LP-OVOD, that discards low-quality boxes by training a sigmoid linear classifier on pseudo labels retrieved from the top relevant region proposals to the novel text. Notably, LP-OVOD seamlessly integrates the knowledge distillation technique from ViLD, resulting in a new state-of-the-art OVOD approach. Experimental results on COCO affirm the superior performance of our approach over prior work, achieving 40.5 in AP_novel using ResNet50 as the backbone and without external datasets or knowing novel classes in training. Our code will be available at https://github.com/VinAIResearch/LP-OVOD.",https://openaccess.thecvf.com/content/WACV2024/html/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pham_LP-OVOD_Open-Vocabulary_Object_Detection_by_Linear_Probing_WACV_2024_paper.pdf,,https://github.com/VinAIResearch/LP-OVOD,,main,Poster,https://ieeexplore.ieee.org/document/10484509/,"['Training', 'Computer vision', 'Codes', 'Object detection', 'Detectors', 'Proposals', 'Object recognition']","['Object Detection', 'Linear Probe', 'Classifier Training', 'Linear Classifier', 'External Dataset', 'Region Proposal', 'Pseudo Labels', 'Unseen Classes', 'Learning Rate', 'Aspect Ratio', 'Feature Classification', 'Image Object', 'Stochastic Gradient Descent', 'Training Images', 'Bounding Box', 'False Negative Rate', 'Classification Score', 'Base Classes', 'Faster R-CNN', 'Objective Scores', 'Classification Head', 'COCO Dataset', 'Region Proposal Network', 'Image Embedding', 'ResNet-50 Backbone', 'Joint Training', 'Softmax Classifier', 'Proposal Network', 'Training Set', 'High False Negative Rate']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",5,"This paper addresses the challenging problem of open-vocabulary object detection (OVOD) where an object detector must identify both seen and unseen classes in test images without labeled examples of the unseen classes in training. A typical approach for OVOD is to use joint text-image embeddings of CLIP to assign box proposals to their closest text label. However, this method has a critical issue: many low-quality boxes, such as over- and under-covered-object boxes, have the same similarity score as high-quality boxes since CLIP is not trained on exact object location information. To address this issue, we propose a novel method, LP-OVOD, that discards low-quality boxes by training a sigmoid linear classifier on pseudo labels retrieved from the top relevant region proposals to the novel text. Notably, LP-OVOD seamlessly integrates the knowledge distillation technique from ViLD, resulting in a new state-of-the-art OVOD approach. Experimental results on COCO affirm the superior performance of our approach over prior work, achieving 40.5 in AP
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">novel</inf>
 using ResNet50 as the backbone and without external datasets or knowing novel classes in training. Our code will be available at https://github.com/VinAIResearch/LP-OVOD."
Label Augmentation As Inter-Class Data Augmentation for Conditional Image Synthesis With Imbalanced Data,"Kai Katsumata, Duc Minh Vo, Hideki Nakayama","The University of Tokyo, Japan",100.0,Japan,0.0,,"Conditional image synthesis performs admirably when trained on well-constructed and balanced datasets. However, in practice, training datasets frequently contain minorities (i.e., a class with a few samples), known as imbalanced data, which causes difficulties in learning generative models. To address conditional image synthesis with imbalanced data, we analyze a diversity issue of label-preserving data augmentation and an affinity issue of non-label-preserving data augmentation. From this observation, we present label augmentation, which works as inter-class data augmentation that effectively augments data by predicting a new label for a given image using the prediction of a pretrained image classification model (i.e., probabilities for each class). We incorporate our label augmentation into the discriminator of a seminal conditional generative adversarial network (GAN) model, proposing Softlabel-GAN. Using class probabilities extracts class-invariant and shared features between similar classes, achieving data augmentation with high affinity and diversity. Our experiments on imbalanced datasets show that Softlabel-GAN produces images with high quality and diversity while being hardly affected by the number of samples in each class. Code: https://github.com/raven38/softlabel-gan.",https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Label_Augmentation_As_Inter-Class_Data_Augmentation_for_Conditional_Image_Synthesis_WACV_2024_paper.pdf,,https://github.com/raven38/softlabel-gan,,main,Poster,https://ieeexplore.ieee.org/document/10484457/,"['Training', 'Image synthesis', 'Training data', 'Predictive models', 'Data augmentation', 'Feature extraction', 'Generative adversarial networks']","['Data Augmentation', 'Imbalanced Data', 'Conditional Image Synthesis', 'Label Augmentation', 'High Diversity', 'Classification Of Samples', 'Generative Adversarial Networks', 'Similar Classification', 'Class Probabilities', 'Imbalanced Datasets', 'Balanced Dataset', 'Conditional Generative Adversarial Network', 'Training Data', 'Low Affinity', 'Latent Variables', 'Number Of Images', 'Diffusion Model', 'Image Generation', 'Augmentation Methods', 'Probability Vector', 'Fréchet Inception Distance', 'Minority Class', 'Geometric Transformation', 'Generative Adversarial Networks Training', 'Balanced Data', 'Image X', 'Color Histogram', 'Diversity Score', 'Data Constraints', 'Minority Samples']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Arts / games / social media']",,"Conditional image synthesis performs admirably when trained on well-constructed and balanced datasets. However, in practice, training datasets frequently contain minorities (i.e., a class with a few samples), known as imbalanced data, which causes difficulties in learning generative models. To address conditional image synthesis with imbalanced data, we analyze a diversity issue of label-preserving data augmentation and an affinity issue of non-label-preserving data augmentation. From this observation, we present label augmentation, which works as inter-class data augmentation that effectively augments data by predicting a new label for a given image using the prediction of a pretrained image classification model (i.e., probabilities for each class). We incorporate our label augmentation into the discriminator of a seminal conditional generative adversarial network (GAN) model, proposing Softlabel-GAN. Using class probabilities extracts class-invariant and shared features between similar classes, achieving data augmentation with high affinity and diversity. Our experiments on imbalanced datasets show that Softlabel-GAN produces images with high quality and diversity while being hardly affected by the number of samples in each class. Code: https://github.com/raven38/softlabel-gan."
Label Shift Estimation for Class-Imbalance Problem: A Bayesian Approach,"Changkun Ye, Russell Tsuchida, Lars Petersson, Nick Barnes","Australian National University, Canberra ACT Australia; Data61 CSIRO, Acton ACT Australia",100.0,Australia,0.0,,"As a type of distribution shift, label shift occurs when the source and target domains have different label distributions P(Y) but identical conditional distributions of data given labels P(X | Y). Under a Bayesian framework, we propose a novel Maximum A Posteriori (MAP) model and a novel posterior sampling model for the label shift problem. We prove the MAP objective admits a unique optimum and derive an EM algorithm that converges to the global optimum. We propose a novel Adaptive Prior Learning (APL) model to adaptively select prior parameters given data. We use the Markov Chain Monte Carlo (MCMC) method in our posterior sampling model to estimate and correct for label shift. Our methods can effectively resolve class imbalance problems on large-scale datasets without fine-tuning the classifier. Experiments show that our model outperforms existing methods on a variety of label shift settings. Our code is available at https://github.com/ChangkunYe/MAPLS/",https://openaccess.thecvf.com/content/WACV2024/html/Ye_Label_Shift_Estimation_for_Class-Imbalance_Problem_A_Bayesian_Approach_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ye_Label_Shift_Estimation_for_Class-Imbalance_Problem_A_Bayesian_Approach_WACV_2024_paper.pdf,,https://github.com/ChangkunYe/MAPLS/,,main,Poster,https://ieeexplore.ieee.org/document/10483983/,"['Adaptation models', 'Computer vision', 'Monte Carlo methods', 'Codes', 'Computational modeling', 'Estimation', 'Data models']","['Class Imbalance Problem', 'Learning Models', 'Markov Chain', 'Markov Chain Monte Carlo', 'Expectation Maximization', 'Large-scale Datasets', 'Bayesian Framework', 'Class Imbalance', 'Target Domain', 'Adaptive Learning', 'Source Domain', 'Maximum A Posteriori', 'Posterior Samples', 'Prior Parameters', 'Label Distribution', 'Posterior Model', 'Model Performance', 'Training Set', 'High-dimensional', 'Maximum Likelihood Estimation', 'Posterior Mode', 'Long-tailed Distribution', 'Target Label', 'Classifier Training', 'Unlabeled Data', 'Sampling Error', 'Distribution Of Categories', 'Top-1 Accuracy', 'Empirical Risk Minimization', 'Unlabeled Target Domain']","['Algorithms', 'Image recognition and understanding']",1,"As a type of distribution shift, label shift occurs when the source and target domains have different label distributions $\mathbb{P}(Y)$ but identical conditional distributions of data given labels $\mathbb{P}(X|Y)$. Under a Bayesian framework, we propose a novel Maximum A Posteriori (MAP) model and a novel posterior sampling model for the label shift problem. We prove the MAP objective admits a unique optimum and derive an EM algorithm that converges to the global optimum. We propose a novel Adaptive Prior Learning (APL) model to adaptively select prior parameters given data. We use the Markov Chain Monte Carlo (MCMC) method in our posterior sampling model to estimate and correct for label shift. Our methods can effectively resolve class imbalance problems on large-scale datasets without fine-tuning the classifier. Experiments show that our model outperforms existing methods on a variety of label shift settings. Our code is available at https://github.com/ChangkunYe/MAPLS/."
Label-Free Synthetic Pretraining of Object Detectors,"Hei Law, Jia Deng",Princeton University,100.0,USA,0.0,,"We propose a new approach, Synthetic Optimized Layout with Instance Detection (SOLID), to pretrain object detectors with synthetic images. Our ""SOLID"" approach consists of two main components: (1) generating synthetic images using a collection of unlabelled 3D models with optimized scene arrangement; (2) pretraining an object detector on ""instance detection"" task - given a query image depicting an object, detecting all instances of the exact same object in a target image. Our approach does not need any semantic labels for pretraining and allows the use of arbitrary, diverse 3D models. Experiments on COCO show that with optimized data generation and a proper pretraining task, synthetic data can be highly effective data for pretraining object detectors. In particular, pretraining on rendered images achieves performance competitive with pretraining on real images while using significantly less computing resources.",https://openaccess.thecvf.com/content/WACV2024/html/Law_Label-Free_Synthetic_Pretraining_of_Object_Detectors_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Law_Label-Free_Synthetic_Pretraining_of_Object_Detectors_WACV_2024_paper.pdf,,https://github.com/princeton-vl/SOLID,2208.04268,main,Poster,https://ieeexplore.ieee.org/document/10484067/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Semantics', 'Layout', 'Detectors']","['Object Detection', 'Image Object', 'Target Image', 'Synthetic Images', 'Semantic Labels', 'Collection Of Models', 'Query Image', 'Pre-training Tasks', 'Data Augmentation', 'Bounding Box', 'Number Of Objects', 'Small Objects', '3D Shape', 'Self-supervised Learning', 'Base Classes', 'Objects In The Scene', 'Faster R-CNN', 'Embedding Vectors', 'Manual Labeling', 'Arbitrary Shape', 'Memory Bank', 'Mask R-CNN', '3D Bounding Box', 'Object Instances', 'Complexity Metrics', 'Object Counting', 'Viewpoint Changes', 'Good Indicator Of Performance', 'Semantic Segmentation', 'Input Resolution']","['Algorithms', 'Image recognition and understanding']",1,"We propose a new approach, Synthetic Optimized Layout with Instance Detection (SOLID), to pretrain object detectors with synthetic images. Our ""SOLID"" approach consists of two main components: (1) generating synthetic images using a collection of unlabelled 3D models with optimized scene arrangement; (2) pretraining an object detector on ""instance detection"" task—given a query image depicting an object, detecting all instances of the exact same object in a target image. Our approach does not need any semantic labels for pretraining and allows the use of arbitrary, diverse 3D models. Experiments on COCO show that with optimized data generation and a proper pretraining task, synthetic data can be highly effective data for pretraining object detectors. In particular, pretraining on rendered images achieves performance competitive with pretraining on real images while using significantly less computing resources. Code is available at https://github.com/princeton-vl/SOLID."
Late to the Party? On-Demand Unlabeled Personalized Federated Learning,"Ohad Amosy, Gal Eyal, Gal Chechik","Bar-Ilan University, Israel; NVIDIA Research, Israel; Bar Ilan University, Israel",66.66666666666666,Israel,33.33333333333334,USA,"In Federated Learning (FL), multiple clients collaborate to learn a shared model through a central server while keeping data decentralized. Personalized Federated Learning (PFL) further extends FL by learning a personalized model per client. In both FL and PFL, all clients participate in the training process and their labeled data are used for training. However, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own unlabeled data. Here, we introduce a new learning setup, On-Demand Unlabeled PFL (OD-PFL), where a system trained on a set of clients, needs to be later applied to novel unlabeled clients at inference time. We propose a novel approach to this problem, ODPFL-HN, which learns to produce a new model for the late-to-the-party client. Specifically, we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client. Evaluated on five benchmark datasets, we find that ODPFL-HN generalizes better than the current FL and PFL methods, especially when the novel client has a large shift from training clients. We also analyzed the generalization error for novel clients, and showed analytically and experimentally how novel clients can apply differential privacy to protect their data.",https://openaccess.thecvf.com/content/WACV2024/html/Amosy_Late_to_the_Party_On-Demand_Unlabeled_Personalized_Federated_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Amosy_Late_to_the_Party_On-Demand_Unlabeled_Personalized_Federated_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484105/,"['Training', 'Differential privacy', 'Privacy', 'Computer vision', 'Federated learning', 'Perturbation methods', 'Computational modeling']","['Benchmark Datasets', 'Large Shift', 'Unlabeled Data', 'Inference Time', 'Model Of Personality', 'Generalization Error', 'Federated Learning', 'Differential Privacy', 'Learning Setup', 'Global Model', 'Data Privacy', 'Domain Shift', 'Model-based Approach', 'Target Model', 'Domain Adaptation', 'Multi-task Learning', 'Pooling Operation', 'Dirichlet Allocation', 'Deep Set', 'Client Data', 'iNaturalist', 'Federated Learning Model']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"In Federated Learning (FL), multiple clients collaborate to learn a shared model through a central server while keeping data decentralized. Personalized Federated Learning (PFL) further extends FL by learning a personalized model per client. In both FL and PFL, all clients participate in the training process and their labeled data are used for training. However, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own unlabeled data.Here, we introduce a new learning setup, On-Demand Unlabeled PFL (OD-PFL), where a system trained on a set of clients, needs to be later applied to novel unlabeled clients at inference time. We propose a novel approach to this problem, ODPFL-HN, which learns to produce a new model for the late-to-the-party client. Specifically, we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client. Evaluated on five benchmark datasets, we find that ODPFL-HN generalizes better than the current FL and PFL methods, especially when the novel client has a large shift from training clients. We also analyzed the generalization error for novel clients, and showed analytically and experimentally how novel clients can apply differential privacy"
Latent Feature-Guided Diffusion Models for Shadow Removal,"Kangfu Mei, Luis Figueroa, Zhe Lin, Zhihong Ding, Scott Cohen, Vishal M. Patel",Adobe Research; Johns Hopkins University,50.0,USA,50.0,USA,"Recovering textures beneath shadows has remained a challenging problem due to the inherent difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine details of shadow regions during the diffusion process. Our method improves the process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, which has been a limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate the potential local optimum of model optimization by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach, where it outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset and outperforms the previous best method by 82% in terms of RMSE on the DeSOBA dataset for instance-level shadow removal.",https://openaccess.thecvf.com/content/WACV2024/html/Mei_Latent_Feature-Guided_Diffusion_Models_for_Shadow_Removal_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mei_Latent_Feature-Guided_Diffusion_Models_for_Shadow_Removal_WACV_2024_paper.pdf,https://kfmei.page/shadow-diffusion/,,2312.02156,main,Poster,https://ieeexplore.ieee.org/document/10483579/,"['Training', 'Computer vision', 'Computational modeling', 'Noise', 'Diffusion processes', 'Image restoration']","['Diffusion Model', 'Removal Model', 'Shadow Removal', 'Root Mean Square Error', 'Feature Space', 'Diffusion Process', 'Local Optimum', 'Latent Space', 'Learning Spaces', 'Latent Features', 'Network Diffusion', 'Shadow Regions', 'Shadow Images', 'Fine-tuned', 'Denoising', 'Latent Variables', 'Visual Comparison', 'Effective Removal', 'Color Map', 'Handcrafted Features', 'Image X', 'Peak Signal-to-noise Ratio', 'Fusion Strategy', 'Shadowing Effect', 'Inference Time']","['Algorithms', 'Computational photography', 'image and video synthesis']",3,"Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process. Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset."
Latent-Guided Exemplar-Based Image Re-Colorization,"Wenjie Yang, Ning Xu, Yifei Fan","; Adobe, Academy of Art University; Shanghai Jiao Tong University",100.0,"China, USA",0.0,,"Exemplar-based re-colorization transfers colors from a reference to a colored or grayscale source image, accounting for the semantic correspondences between the two. Existing grayscale colorization methods usually predict only the chromatic aberration while maintaining the source's luminance. Consequently, the result's color may diverge from the reference due to such luminance difference. On the other hand, global photorealistic stylization without segmentation cannot handle scenarios where different parts of the scene need different colors. To overcome this issue, we propose a novel and effective method for re-colorization: 1) We first exploit the spatial-adaptive latent space of SpaceEdit in the context of the re-colorization task and achieve re-colorization via latent maps prediction through a proposed network. 2) We then delve into SpaceEdit's self-reconstruct latent codes and maps to better characterize the global style and local color property, based on which we construct a novel loss to supervise re-colorization. Qualitative and quantitative results show that our method outperforms previous works by generating superior outputs with more consistent colors and global styles based on references.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_Latent-Guided_Exemplar-Based_Image_Re-Colorization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Latent-Guided_Exemplar-Based_Image_Re-Colorization_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484094/,"['Image segmentation', 'Computer vision', 'Codes', 'Image color analysis', 'Semantics', 'Gray-scale', 'Task analysis']","['Qualitative Results', 'Grayscale Images', 'Latent Space', 'Source Images', 'Latent Code', 'Local Color', 'Fine-tuned', 'Input Image', 'Mean Absolute Error', 'Color Images', 'Image Pairs', 'Reference Image', 'Color Features', 'Output Image', 'Similar Color', 'Segmentation Map', 'Color Information', 'Quantitative Performance', 'L1 Loss', 'Severe Artifacts', 'Color Transformation', 'Style Transfer', 'Similar Style', 'Coding Properties', 'Color Histogram', 'Image Editing']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Low-level and physics-based vision']",,"Exemplar-based re-colorization transfers colors from a reference to a colored or grayscale source image, accounting for the semantic correspondences between the two. Existing grayscale colorization methods usually predict only the chromatic aberration while maintaining the source’s luminance. Consequently, the result’s color may diverge from the reference due to such luminance difference. On the other hand, global photorealistic stylization without segmentation cannot handle scenarios where different parts of the scene need different colors. To overcome this issue, we propose a novel and effective method for re-colorization: 1) We first exploit the spatial-adaptive latent space of SpaceEdit in the context of the re-colorization task and achieve re-colorization via latent maps prediction through a proposed network. 2) We then delve into SpaceEdit’s self-reconstruct latent codes and maps to better characterize the global style and local color property, based on which we construct a novel loss to supervise re-colorization. Qualitative and quantitative results show that our method outperforms previous works by generating superior outputs with more consistent colors and global styles based on references."
LatentDR: Improving Model Generalization Through Sample-Aware Latent Degradation and Restoration,"Ran Liu, Sahil Khose, Jingyun Xiao, Lakshmi Sathidevi, Keerthan Ramnath, Zsolt Kira, Eva L. Dyer","Georgia Institute of Technology, Atlanta, GA, 30332",100.0,USA,0.0,,"Despite significant advances in deep learning, models often struggle to generalize well to new, unseen domains, especially when training data is limited. To address this challenge, we propose a novel approach for distribution-aware latent augmentation that leverages the relationships across samples to guide the augmentation procedure. Our approach first degrades the samples stochastically in the latent space, mapping them to augmented labels, and then restores the samples from their corrupted versions during training. This process confuses the classifier in the degradation step and restores the overall class distribution of the original samples, promoting diverse intra-class/cross-domain variability. We extensively evaluate our approach on a diverse set of datasets and tasks, including domain generalization benchmarks and medical imaging datasets with strong domain shift, where we show our approach achieves significant improvements over existing methods for latent space augmentation. We further show that our method can be flexibly adapted to long-tail recognition tasks, demonstrating its versatility in building more generalizable models. Code is at https://github.com/nerdslab/LatentDR.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_LatentDR_Improving_Model_Generalization_Through_Sample-Aware_Latent_Degradation_and_Restoration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_LatentDR_Improving_Model_Generalization_Through_Sample-Aware_Latent_Degradation_and_Restoration_WACV_2024_paper.pdf,,https://github.com/nerdslab/LatentDR,2308.14596,main,Poster,https://ieeexplore.ieee.org/document/10483935/,"['Degradation', 'Deep learning', 'Training', 'Adaptation models', 'Training data', 'Space mapping', 'Self-supervised learning']","['Improve Model Generalization', 'Training Data', 'Domain Shift', 'Latent Space', 'Degradation Step', 'Domain Generalization', 'Advances In Deep Learning', 'Medical Datasets', 'Medical Image Datasets', 'Unseen Domains', 'Hyperparameters', 'Image Classification', 'Data Augmentation', 'Attention Mechanism', 'Training Protocol', 'Average Pooling', 'Batch Of Samples', 'Augmentation Methods', 'Self-supervised Learning', 'Empirical Risk Minimization', 'Spatial Operation', 'Data In Feature Space', 'Data Augmentation Methods', 'Mixed Operator', 'Mini-batch Training', 'Multi-head Self-attention', 'Downstream Classification', 'Training Domain', 'Transformer Layers']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Biomedical / healthcare / medicine']",,"Despite significant advances in deep learning, models often struggle to generalize well to new, unseen domains, especially when training data is limited. To address this challenge, we propose a novel approach for distribution-aware latent augmentation that leverages the relationships across samples to guide the augmentation procedure. Our approach first degrades the samples stochastically in the latent space, mapping them to augmented labels, and then restores the samples from their corrupted versions during training. This process confuses the classifier in the degradation step and restores the overall class distribution of the original samples, promoting diverse intra-class/cross-domain variability. We extensively evaluate our approach on a diverse set of datasets and tasks, including domain generalization benchmarks and medical imaging datasets with strong domain shift, where we show our approach achieves significant improvements over existing methods for latent space augmentation. We further show that our method can be flexibly adapted to long-tail recognition tasks, demonstrating its versatility in building more generalizable models. https://github.com/nerdslab/LatentDR."
LatentPaint: Image Inpainting in Latent Space With Diffusion Models,"Ciprian Corneanu, Raghudeep Gadde, Aleix M. Martinez",Amazon,0.0,,100.0,USA,"Image inpainting is generally done using either a domain-specific (preconditioned) model or a generic model that is postconditioned at inference time. Preconditioned models are fast at inference time but extremely costly to train, requiring training on each domain they are applied to. Postconditioned models do not require any domain-specific training but are slow during inference, requiring multiple forward and backward passes to converge to a desirable solution. Here, we derive an approach that does not require any domain specific training, yet is fast at inference time. To solve the costly inference computational time, we perform the forward-backward fusion step on a latent space rather than the image space. This is solved with a newly proposed propagation module in the diffusion process. Experiments on a number of domains demonstrate our approach attains or improves state-of-the-art results with the advantages of preconditioned and postconditioned models and none of their disadvantages.",https://openaccess.thecvf.com/content/WACV2024/html/Corneanu_LatentPaint_Image_Inpainting_in_Latent_Space_With_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Corneanu_LatentPaint_Image_Inpainting_in_Latent_Space_With_Diffusion_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483967/,"['Training', 'Computer vision', 'Runtime', 'Costs', 'Computational modeling', 'Diffusion processes', 'Proposals']","['Diffusion Model', 'Latent Space', 'Image Inpainting', 'Preconditioning', 'Diffusion Process', 'Image Space', 'Inference Time', 'Generation Module', 'Postconditioning', 'Denoising', 'Gaussian Noise', 'Reversible Process', 'Global Information', 'Semantic Segmentation', 'Latent Representation', 'Variational Autoencoder', 'Visual Domain', 'Masked Images', 'Pixel Spacing', 'Markov Property', 'Fréchet Inception Distance', 'Missing Parts', 'Forward Process']","['Algorithms', 'Computational photography', 'image and video synthesis']",12,"Image inpainting using diffusion models is generally done using either preconditioned models, i.e. image conditioned models fine-tuned for the painting task, or postconditioned models, i.e. unconditioned models repurposed for the painting task at inference time. Preconditioned models are fast at inference time but extremely costly to train. Postconditioned models do not require any training but are slow during inference, requiring multiple forward and backward passes to converge to a desirable solution. Here, we derive an approach that does not require expensive training, yet is fast at inference time. To solve the costly inference computational time, we perform the forward-backward fusion step on a latent space rather than the image space. This is solved with a newly proposed propagation module in the diffusion process. Experiments on a number of domains demonstrate our approach attains or improves state-of-the-art results with the advantages of preconditioned and postconditioned models and none of their disadvantages."
LaughTalk: Expressive 3D Talking Head Generation With Laughter,"Kim Sung-Bin, Lee Hyun, Da Hye Hong, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh","KRAFTON; Sookmyung Women’s University; Dept. of Electrical Engineering and Grad. School of Artificial Intelligence, POSTECH; Institute for Convergence Research and Education in Advanced Technology, Yonsei University; Dept. of Electrical Engineering and Grad. School of Artificial Intelligence, POSTECH",80.0,South Korea,20.0,South Korea,"Laughter is a unique expression, essential to affirmative social interactions of humans. Although current 3D talking head generation methods produce convincing verbal articulations, they often fail to capture the vitality and subtleties of laughter and smiles despite their importance in social context. In this paper, we introduce a novel task to generate 3D talking heads capable of both articulate speech and authentic laughter. Our newly curated dataset comprises 2D laughing videos paired with pseudo-annotated and human-validated 3D FLAME parameters and vertices. Given our proposed dataset, we present a strong baseline with a two-stage training scheme: the model first learns to talk and then acquires the ability to express laughter. Extensive experiments demonstrate that our method performs favorably compared to existing approaches in both talking head generation and expressing laughter signals. We further explore potential applications on top of our proposed method for rigging realistic avatars.",https://openaccess.thecvf.com/content/WACV2024/html/Sung-Bin_LaughTalk_Expressive_3D_Talking_Head_Generation_With_Laughter_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sung-Bin_LaughTalk_Expressive_3D_Talking_Head_Generation_With_Laughter_WACV_2024_paper.pdf,https://laughtalk.github.io/,https://github.com/laughtalk,2311.00994,main,Poster,https://ieeexplore.ieee.org/document/10484300/,"['Training', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Lips', 'Fires', 'Synchronization']","['Two-stage Training', '3D Head', 'Facial Expressions', 'User Study', '3D Reconstruction', 'Video Clips', '3D Mesh', 'Verbal Expression', 'Verbal Cues', 'Facial Movements', 'Face Representation', 'Non-verbal Signals', 'Lip Movements', 'Mesh Vertices', '3D Face', 'Temporal Convolutional Network', 'Task Dataset', '2D Video', 'Speech Articulation', '3D Landmarks', 'Transformer Decoder', 'Lip-sync', '3D Animation', 'Transformer Encoder', 'Synchronization Accuracy', 'Audio Input', 'Facial Shape', 'Speech Input', 'Non-verbal Expressions', 'Video Dataset']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",1,"Laughter is a unique expression, essential to affirmative social interactions of humans. Although current 3D talking head generation methods produce convincing verbal articulations, they often fail to capture the vitality and subtleties of laughter and smiles despite their importance in social context. In this paper, we introduce a novel task to generate 3D talking heads capable of both articulate speech and authentic laughter. Our newly curated dataset comprises 2D laughing videos paired with pseudo-annotated and human-validated 3D FLAME parameters and vertices. Given our proposed dataset, we present a strong baseline with a two-stage training scheme: the model first learns to talk and then acquires the ability to express laughter. Extensive experiments demonstrate that our method performs favorably compared to existing approaches in both talking head generation and expressing laughter signals. We further explore potential applications on top of our proposed method for rigging realistic avatars."
Layer-Wise Auto-Weighting for Non-Stationary Test-Time Adaptation,"Junyoung Park, Jin Kim, Hyeongjun Kwon, Ilhoon Yoon, Kwanghoon Sohn","Yonsei University, Korea Institute of Science and Technology (KIST); Yonsei University",100.0,South Korea,0.0,,"Given the inevitability of domain shifts during inference in real-world applications, test-time adaptation (TTA) is essential for model adaptation after deployment. However, the real-world scenario of continuously changing target distributions presents challenges including catastrophic forgetting and error accumulation. Existing TTA methods for non-stationary domain shifts, while effective, incur excessive computational load, making them impractical for on-device settings. In this paper, we introduce a layer-wise auto-weighting algorithm for continual and gradual TTA that autonomously identifies layers for preservation or concentrated adaptation. By leveraging the Fisher Information Matrix (FIM), we first design the learning weight to selectively focus on layers associated with log-likelihood changes while preserving unrelated ones. Then, we further propose an exponential min-max scaler to make certain layers nearly frozen while mitigating outliers. This minimizes forgetting and error accumulation, leading to efficient adaptation to non-stationary target distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our method outperforms conventional continual and gradual TTA approaches while significantly reducing computational load, highlighting the importance of FIM-based learning weight in adapting to continuously or gradually shifting target domains.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Layer-Wise_Auto-Weighting_for_Non-Stationary_Test-Time_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Layer-Wise_Auto-Weighting_for_Non-Stationary_Test-Time_Adaptation_WACV_2024_paper.pdf,,https://github.com/junia3/LayerwiseTTA,2311.05858,main,Poster,https://ieeexplore.ieee.org/document/10484145/,"['Computer vision', 'Adaptation models', 'Computational modeling', 'Load modeling']","['Test-time Adaptation', 'Domain Shift', 'Computational Load', 'Target Domain', 'Target Distribution', 'Learned Weights', 'Error Accumulation', 'Fisher Information Matrix', 'Catastrophic Forgetting', 'Loss Function', 'Corruption', 'Learning Rate', 'Deep Neural Network', 'Second Derivative', 'Hyperparameter Tuning', 'Defocus', 'Target Data', 'Hessian Matrix', 'Domain Adaptation', 'Source Domain', 'Consistency Loss', 'Pseudo Labels', 'Minimum Entropy', 'Online Manner', 'Continuous Adaptation', 'Gradual Adaptation', 'Current Batch', 'Unlabeled Target Data', 'Performance Comparison']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Given the inevitability of domain shifts during inference in real-world applications, test-time adaptation (TTA) is essential for model adaptation after deployment. However, the real-world scenario of continuously changing target distributions presents challenges including catastrophic forgetting and error accumulation. Existing TTA methods for non-stationary domain shifts, while effective, incur excessive computational load, making them impractical for on-device settings. In this paper, we introduce a layer-wise auto-weighting algorithm for continual and gradual TTA that autonomously identifies layers for preservation or concentrated adaptation. By leveraging the Fisher Information Matrix (FIM), we first design the learning weight to selectively focus on layers associated with log-likelihood changes while preserving unrelated ones. Then, we further propose an exponential min-max scaler to make certain layers nearly frozen while mitigating outliers. This minimizes forgetting and error accumulation, leading to efficient adaptation to non-stationary target distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our method outperforms conventional continual and gradual TTA approaches while significantly reducing computational load, highlighting the importance of FIM-based learning weight in adapting to continuously or gradually shifting target domains.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Learn To Unlearn for Deep Neural Networks: Minimizing Unlearning Interference With Gradient Projection,"Tuan Hoang, Santu Rana, Sunil Gupta, Svetha Venkatesh","A2I2, Deakin University",100.0,Australia,0.0,,"Recent data-privacy laws have sparked interest in machine unlearning, which involves removing the effect of specific training samples from a learnt model as if they were never present in the original training dataset. The challenge of machine unlearning is to discard information about the ""forget"" data in the learnt model without altering the knowledge about the remaining dataset and to do so more efficiently than the naive retraining approach. To achieve this, we adopt a projected-gradient based learning method, named as Projected-Gradient Unlearning (PGU), in which the model takes steps in the orthogonal direction to the gradient subspaces deemed unimportant for the retaining dataset, so as to its knowledge is preserved. By utilizing Stochastic Gradient Descent (SGD) to update the model weights, our method can efficiently scale to any model and dataset size. We provide empirically evidence to demonstrate that our unlearning method can produce models that behave similar to models retrained from scratch across various metrics even when the training dataset is no longer accessible. Our code is available at https://github.com/hnanhtuan/projected_gradient_unlearning.",https://openaccess.thecvf.com/content/WACV2024/html/Hoang_Learn_To_Unlearn_for_Deep_Neural_Networks_Minimizing_Unlearning_Interference_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hoang_Learn_To_Unlearn_for_Deep_Neural_Networks_Minimizing_Unlearning_Interference_WACV_2024_paper.pdf,,https://github.com/hnanhtuan/projected-gradient-unlearning,2312.04095,main,Poster,https://ieeexplore.ieee.org/document/10483632/,"['Training', 'Measurement', 'Learning systems', 'Data privacy', 'Computer vision', 'Training data', 'Stochastic processes']","['Gradient Projection', 'Data Model', 'Training Dataset', 'Data Privacy', 'Stochastic Gradient Descent', 'Training Set', 'Training Data', 'Receiver Operating Characteristic Curve', 'Learning Rate', 'Convolutional Layers', 'Subset Of Data', 'Vector-based', 'Airplane', 'Area Under Curve', 'General Data Protection Regulation', 'Increase In Error', 'Linear Layer', 'Trust Region', 'Weight Space', 'Differential Privacy', 'Model Retraining', 'Subset Of The Training Data', 'Catastrophic Forgetting', 'Input Feature Vector', 'Limited Scalability', 'Entropy Of Distribution', 'Input Representation', 'Inference Attacks', 'Validation Set', 'Random Chance']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",3,"Recent data-privacy laws have sparked interest in machine unlearning, which involves removing the effect of specific training samples from a learnt model as if they were never present in the original training dataset. The challenge of machine unlearning is to discard information about the ""forget"" data in the learnt model without altering the knowledge about the remaining dataset and to do so more efficiently than the naive retraining approach. To achieve this, we adopt a projected-gradient based learning method, named as Projected-Gradient Unlearning (PGU), in which the model takes steps in the orthogonal direction to the gradient subspaces deemed unimportant for the retaining dataset, so as to its knowledge is preserved. By utilizing Stochastic Gradient Descent (SGD) to update the model weights, our method can efficiently scale to any model and dataset size. We provide empirically evidence to demonstrate that our unlearning method can produce models that behave similar to models retrained from scratch across various metrics even when the training dataset is no longer accessible. Our code is available at https://github.com/hnanhtuan/projected_gradient_unlearning."
Learnable Cube-Based Video Encryption for Privacy-Preserving Action Recognition,"Yuchi Ishikawa, Masayoshi Kondo, Hirokatsu Kataoka","LY Corporation*, Tokyo, Japan",0.0,,100.0,Japan,"With the development of cloud services and machine learning, there has been an inevitable need to enhance privacy and security when serving video recognition models. Although existing image encryption methods can be used to address this issue, applying them frame by frame to videos is insufficient in two respects: model performance degradation and security strength. In this paper, we propose a novel encryption approach for privacy-preserving action recognition. It consists of two encrypting operations; Learnable Cube-based Video Encryption (LCVE) and ViT Scrambling. LCVE is video encryption based on spatio-temporal cubes, which has a large key space and can provide robust privacy protection. ViT Scrambling encrypts the Vision Transformer (ViT) model, which enables it to recognize the encrypted videos in the same manner as unencrypted videos without modifying the model architecture or fine-tuning on the encrypted data. We evaluate our method in an action recognition task with seven datasets containing a variety of action classes as well as motion and visual patterns. Empirical results demonstrate that LCVE combined with ViT Scrambling can preserve video privacy while recognizing action in encrypted videos as well as unencrypted videos. As a result, our approach outperforms existing privacy-preserving action recognition methods.",https://openaccess.thecvf.com/content/WACV2024/html/Ishikawa_Learnable_Cube-Based_Video_Encryption_for_Privacy-Preserving_Action_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ishikawa_Learnable_Cube-Based_Video_Encryption_for_Privacy-Preserving_Action_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483907/,"['Training', 'Privacy', 'Visualization', 'Computer architecture', 'Transformers', 'Data models', 'Encryption']","['Action Recognition', 'Video Encryption', 'Model Performance', 'Cloud Computing', 'Performance Degradation', 'Model Architecture', 'Plaintext', 'Privacy Protection', 'Security Level', 'Recognition Model', 'Motion Patterns', 'Action Classes', 'Encrypted Data', 'Recognition Approach', 'Key Space', 'Encryption Method', 'Vision Transformer', 'Image Encryption', 'Video Recognition', 'Action Recognition Task', 'Positional Encoding', 'Key Security', 'Large-scale Datasets', 'Federated Learning', 'Data Privacy', 'Secret Key', 'Encryption Key', 'Action Recognition Model', 'Transformer Encoder', 'Linear Projection']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",4,"With the development of cloud services and machine learning, there has been an inevitable need to enhance privacy and security when serving video recognition models. Although existing image encryption methods can be used to address this issue, applying them frame by frame to videos is insufficient in two respects: model performance degradation and security strength. In this paper, we propose a novel encryption approach for privacy-preserving action recognition. It consists of two encrypting operations; Learnable Cube-based Video Encryption (LCVE) and ViT Scrambling. LCVE is video encryption based on spatio-temporal cubes, which has a large key space and can provide robust privacy protection. ViT Scrambling encrypts the Vision Transformer (ViT) model, which enables it to recognize the encrypted videos in the same manner as unencrypted videos without modifying the model architecture or fine-tuning on the encrypted data. We evaluate our method in an action recognition task with seven datasets containing a variety of action classes as well as motion and visual patterns. Empirical results demonstrate that LCVE combined with ViT Scrambling can preserve video privacy while recognizing action in encrypted videos as well as unencrypted videos. As a result, our approach outperforms existing privacy-preserving action recognition methods."
Learning Better Keypoints for Multi-Object 6DoF Pose Estimation,"Yangzheng Wu, Michael Greenspan","RCV Lab, Dept. of Electrical and Computer Engineering, Ingenuity Labs, Queen’s University, Kingston, Ontario, Canada",100.0,Canada,0.0,,"We address the problem of keypoint selection, and find that the performance of 6DoF pose estimation methods can be improved when pre-defined keypoint locations are learned, rather than being heuristically selected as has been the standard approach. We found that accuracy and efficiency can be improved by training a graph network to select a set of disperse keypoints with similarly distributed votes. These votes, learned by a regression network to accumulate evidence for the keypoint locations, can be regressed more accurately compared to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by a combined loss measuring both Wasserstein distance and dispersion, learns the color and geometry features of the target objects to estimate optimal keypoint locations. Experiments demonstrate the keypoints selected by KeyGNet improved the accuracy for all evaluation metrics of all seven datasets tested, for three keypoint voting methods. The challenging Occlusion LINEMOD dataset notably improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvement for all objects, of between +1% and +21.5%. There was also a notable increase in performance when transitioning from single object to multiple object training using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap for Occlusion LINEMOD.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_Learning_Better_Keypoints_for_Multi-Object_6DoF_Pose_Estimation_WACV_2024_paper.pdf,,,2308.07827,main,Poster,https://ieeexplore.ieee.org/document/10484471/,"['Training', 'Geometry', 'Degradation', 'Histograms', 'Heuristic algorithms', 'Pose estimation', 'Loss measurement']","['Pose Estimation', '6-DoF Pose Estimation', 'Multiple Objects', 'Single Object', 'Network Graph', 'Regression Network', 'Keypoint Locations', 'Minimum Distance', 'Cross-entropy', 'Point Cloud', 'Bounding Box', 'Kullback-Leibler', 'Least-squares Fitting', 'Single Network', 'Graph Convolutional Network', 'Loss Term', 'Object Surface', 'Surface Geometry', 'Inference Speed', 'Object Dataset', 'Simultaneous Localization And Mapping', 'Visible Surface', 'Ground Truth Pose', 'Direct Regression', 'Human Pose Estimation', 'Gradient Penalty', 'Voting Scheme', 'Similarity Measure', 'Performance Gap']","['Algorithms', 'Image recognition and understanding']",1,"We address the problem of keypoint selection, and find that the performance of 6DoF pose estimation methods can be improved when pre-defined keypoint locations are learned, rather than being heuristically selected as has been the standard approach. We found that accuracy and efficiency can be improved by training a graph network to select a set of disperse keypoints with similarly distributed votes. These votes, learned by a regression network to accumulate evidence for the keypoint locations, can be regressed more accurately compared to previous heuristic keypoint algorithms. The proposed KeyGNet, supervised by a combined loss measuring both Wasserstein distance and dispersion, learns the color and geometry features of the target objects to estimate optimal keypoint locations. Experiments demonstrate the keypoints selected by KeyGNet improved the accuracy for all evaluation metrics of all seven datasets tested, for three keypoint voting methods. The challenging Occlusion LINEMOD dataset notably improved ADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvement for all objects, of between +1% and +21.5%. There was also a notable increase in performance when transitioning from single object to multiple object training using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap for Occlusion LINEMOD."
Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization,"Prathmesh Bele, Valay Bundele, Avigyan Bhattacharya, Ankit Jha, Gemma Roig, Biplab Banerjee","Goethe University Frankfurt, Germany; Indian Institute of Technology Bombay, India",100.0,"Germany, India",0.0,,"Single-source open-domain generalization (SS-ODG) addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain. However, these methods struggle with visually fine-grained open-closed data, often misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model's ability to generalize. To overcome these limitations, we propose a novel framework called SODG-NET that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, in contrast to the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multiclass classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-NET compared to the literature.",https://openaccess.thecvf.com/content/WACV2024/html/Bele_Learning_Class_and_Domain_Augmentations_for_Single-Source_Open-Domain_Generalization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bele_Learning_Class_and_Domain_Augmentations_for_Single-Source_Open-Domain_Generalization_WACV_2024_paper.pdf,,,2311.02599,main,Poster,https://ieeexplore.ieee.org/document/10483701/,"['Training', 'Measurement', 'Computer vision', 'Benchmark testing']","['Open Domain', 'Classification Of Samples', 'Single Domain', 'Target Domain', 'Mixed Strategy', 'Source Domain', 'Unlabeled Target Domain', 'Labeled Source Domain', 'Training Data', 'Paired Samples', 'Cognitive Domains', 'Input Image', 'Feature Maps', 'Class Labels', 'Average Performance', 'Image Pairs', 'Backbone Network', 'Unknown Samples', 'Domain Adaptation', 'Loss Term', 'Crossmatch', 'Open Class', 'Closed And Open', 'Empirical Risk Minimization', 'Digital Dataset', 'Domain Generalization', 'Distinctive Style', 'Style Features', 'Accuracy Acc', 'Unseen Domains']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Single-source open-domain generalization (SS-ODG) addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain. However, these methods struggle with visually fine-grained open-closed data, often misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model’s ability to generalize. To overcome these limitations, we propose a novel framework called SODG-Net that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, in contrast to the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multiclass classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-Net compared to the literature."
Learning Generalizable Perceptual Representations for Data-Efficient No-Reference Image Quality Assessment,"Suhas Srinath, Shankhanil Mitra, Shika Rao, Rajiv Soundararajan","Indian Institute of Science, Bengaluru, India 560012",100.0,India,0.0,,"No-reference (NR) image quality assessment (IQA) is an important tool in enhancing the user experience in diverse visual applications. A major drawback of state-of-the-art NR-IQA techniques is their reliance on a large number of human annotations to train models for a target IQA application. To mitigate this requirement, there is a need for unsupervised learning of generalizable quality representations that capture diverse distortions. We enable the learning of low-level quality features agnostic to distortion types by introducing a novel quality-aware contrastive loss. Further, we leverage the generalizability of vision-language models by fine-tuning one such model to extract high-level image quality information through relevant text prompts. The two sets of features are combined to effectively predict quality by training a simple regressor with very few samples on a target dataset. Additionally, we design zero-shot quality predictions from both pathways in a completely blind setting. Our experiments on diverse datasets encompassing various distortions show the generalizability of the features and their superior performance in the data-efficient and zero-shot settings.",https://openaccess.thecvf.com/content/WACV2024/html/Srinath_Learning_Generalizable_Perceptual_Representations_for_Data-Efficient_No-Reference_Image_Quality_Assessment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Srinath_Learning_Generalizable_Perceptual_Representations_for_Data-Efficient_No-Reference_Image_Quality_Assessment_WACV_2024_paper.pdf,,https://github.com/suhas-srinath/GRepQ,2312.04838,main,Poster,https://ieeexplore.ieee.org/document/10484391/,"['Image quality', 'Training', 'Visualization', 'Computer vision', 'Annotations', 'Distortion', 'Feature extraction']","['Image Quality', 'No-reference Image Quality Assessment', 'No-reference Image Quality', 'Superior Performance', 'Information Quality', 'Prediction Quality', 'Representation Learning', 'Low-level Features', 'Target Dataset', 'Contrastive Loss', 'Lower Levels Of Quality', 'Distortion Types', 'Low Quality', 'Image Features', 'Similarity Measure', 'Entire Dataset', 'Multiple Datasets', 'High-level Features', 'Evaluation Dataset', 'Support Vector Regression', 'Low-level Model', 'Self-supervised Learning', 'High-level Model', 'Mean Opinion Score', 'Batch Of Images', 'High And Low Levels', 'Image Encoder', 'Feature Encoder', 'JPEG Compression', 'Content Bias']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"No-reference (NR) image quality assessment (IQA) is an important tool in enhancing the user experience in diverse visual applications. A major drawback of state-of-the-art NR-IQA techniques is their reliance on a large number of human annotations to train models for a target IQA application. To mitigate this requirement, there is a need for unsupervised learning of generalizable quality representations that capture diverse distortions. We enable the learning of low-level quality features agnostic to distortion types by introducing a novel quality-aware contrastive loss. Further, we leverage the generalizability of vision-language models by fine-tuning one such model to extract high-level image quality information through relevant text prompts. The two sets of features are combined to effectively predict quality by training a simple regressor with very few samples on a target dataset. Additionally, we design zero-shot quality predictions from both pathways in a completely blind setting. Our experiments on diverse datasets encompassing various distortions show the generalizability of the features and their superior performance in the data-efficient and zero-shot settings."
Learning Intra-Class Multimodal Distributions With Orthonormal Matrices,"Jumpei Goto, Yohei Nakata, Kiyofumi Abe, Yasunori Ishii, Takayoshi Yamashita","Panasonic Holdings Corporation, Japan; Chubu University, Japan",50.0,Japan,50.0,Japan,"In this paper, we address the challenges of representing feature distributions which have multimodality within a class in deep neural networks. Existing online clustering methods employ sub-centroids to capture intra-class variations. However, conducting online clustering faces some limitations, i.e., online clustering assigns only a single subcentroid to a feature vector extracted from a backbone and ignores the relationship between the other sub-centroids and the feature vector, and updating sub-centroids in an online clustering manner incurs significant storage costs. To address these limitations, we propose a novel method utilizing orthonormal matrices instead of sub-centroids for relaxing discrete assignments into continuous assignments. We update the orthonormal matrices using a gradient-based method, which eliminates the need for online clustering or additional storage. Experimental results on the CIFAR and ImageNet datasets exhibit that the proposed method outperforms current online clustering techniques in classification accuracy, sub-category discovery, and transferability, providing an efficient solution to the challenges posed by complex recognition targets.",https://openaccess.thecvf.com/content/WACV2024/html/Goto_Learning_Intra-Class_Multimodal_Distributions_With_Orthonormal_Matrices_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Goto_Learning_Intra-Class_Multimodal_Distributions_With_Orthonormal_Matrices_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484104/,"['Lead acid batteries', 'Computer vision', 'Costs', 'Image recognition', 'Target recognition', 'Computational modeling', 'Clustering methods']","['Bimodal', 'Orthogonal Matrix', 'Deep Neural Network', 'Distribution Characteristics', 'ImageNet Dataset', 'Gradient-based Methods', 'Intra-class Variance', 'Additional Storage', 'Need For Storage', 'Optimization Problem', 'Learning Rate', 'Convolutional Layers', 'Classification Task', 'Weight Matrix', 'Training Phase', 'Input Features', 'Positive Matrix', 'Weight Vector', 'Backbone Network', 'Single Vector', 'CIFAR-100 Dataset', 'Top-1 Accuracy', 'Nearest Centroid', 'Non-negative Matrix Factorization', 'Inference Phase', 'Training Characteristics', 'Top-5 Accuracy', 'Huge Cost', 'Gradient Descent Method', 'Cross-entropy Loss']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding']",1,"In this paper, we address the challenges of representing feature distributions which have multimodality within a class in deep neural networks. Existing online clustering methods employ sub-centroids to capture intra-class variations. However, conducting online clustering faces some limitations, i.e., online clustering assigns only a single sub-centroid to a feature vector extracted from a backbone and ignores the relationship between the other sub-centroids and the feature vector, and updating sub-centroids in an online clustering manner incurs significant storage costs. To address these limitations, we propose a novel method utilizing orthonormal matrices instead of sub-centroids for relaxing discrete assignments into continuous assignments. We update the orthonormal matrices using a gradient-based method, which eliminates the need for online clustering or additional storage. Experimental results on the CIFAR and ImageNet datasets exhibit that the proposed method outperforms current online clustering techniques in classification accuracy, sub-category discovery, and transferability, providing an efficient solution to the challenges posed by complex recognition targets."
Learning Low-Rank Latent Spaces With Simple Deterministic Autoencoder: Theoretical and Empirical Insights,"Alokendu Mazumder, Tirthajit Baruah, Bhartendu Kumar, Rishab Sharma, Vishwajeet Pattanaik, Punit Rathore","TCS Research, Bengaluru; Dayananda Sagar College of Engineering, Bengaluru; Indian Institute of Science, Bengaluru",66.66666666666666,India,33.33333333333334,India,"The autoencoder is an unsupervised learning paradigm that aims to create a compact latent representation of data by minimizing the reconstruction loss. However, it tends to overlook the fact that most data (images) are embedded in a lower-dimensional latent space, which is crucial for effective data representation. To address this limitation, we propose a novel approach called Low-Rank Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to adaptively learn a low-dimensional latent space while preserving the basic objective of an autoencoder. This helps embed the data in a lower-dimensional latent space while preserving important information. It is a simple autoencoder extension that learns low-rank latent space. Theoretically, we establish a tighter error bound for our model. Empirically, our model's superiority shines through various tasks such as image generation and downstream classification. Both theoretical and practical outcomes highlight the importance of acquiring low-dimensional embeddings.",https://openaccess.thecvf.com/content/WACV2024/html/Mazumder_Learning_Low-Rank_Latent_Spaces_With_Simple_Deterministic_Autoencoder_Theoretical_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mazumder_Learning_Low-Rank_Latent_Spaces_With_Simple_Deterministic_Autoencoder_Theoretical_and_WACV_2024_paper.pdf,,,2310.16194,main,Poster,https://ieeexplore.ieee.org/document/10483963/,"['Representation learning', 'Computer vision', 'Adaptation models', 'Image synthesis', 'Reliability theory', 'Task analysis', 'Covariance matrices']","['Latent Space', 'Deterministic Autoencoder', 'Unsupervised Learning', 'Low-dimensional Space', 'Reconstruction Loss', 'Downstream Classification', 'Convolutional Neural Network', 'Gradient Descent', 'Gaussian Noise', 'Singular Value', 'Generative Adversarial Networks', 'Generalization Capability', 'Matrix M', 'Gaussian Mixture Model', 'Penalty Parameter', 'Variational Autoencoder', 'Linear Layer', 'Self-supervised Learning', 'Latent Vector', 'MNIST Dataset', 'Fréchet Inception Distance', 'Nuclear Norm', 'Latent Dimensions', 'Nuclear Norm Minimization', 'Low-rank Constraint', 'Dimensional Latent Space', 'Random Noise', 'Representation Learning', 'Penalty Term', 'Vanilla']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"The autoencoder is an unsupervised learning paradigm that aims to create a compact latent representation of data by minimizing the reconstruction loss. However, it tends to overlook the fact that most data (images) are embedded in a lower-dimensional latent space, which is crucial for effective data representation. To address this limitation, we propose a novel approach called Low-Rank Autoencoder (LoRAE). In LoRAE, we incorporated a low-rank regularizer to adaptively learn a low-dimensional latent space while preserving the basic objective of an autoencoder. This helps embed the data in a lower-dimensional latent space while preserving important information. It is a simple autoencoder extension that learns low-rank latent space. Theoretically, we establish a tighter error bound for our model. Empirically, our model’s superiority shines through various tasks such as image generation and downstream classification. Both theoretical and practical outcomes highlight the importance of acquiring low-dimensional embeddings."
Learning Quality Labels for Robust Image Classification,"Xiaosong Wang, Ziyue Xu, Dong Yang, Leo Tam, Holger Roth, Daguang Xu","Nvidia Corporation, CA, US",0.0,,100.0,USA,"Current deep learning paradigms largely benefit from the tremendous amount of annotated data. However, the quality of the annotations often varies among labelers. Multi-observer studies have been conducted to examine the annotation variances (by labeling the same data multiple times) and their effects on critical applications like medical image analysis. In this paper, we demonstrate how multiple sets of annotations (either hand-labeled or algorithm-generated) can be utilized together and mutually benefit the learning of classification tasks. The concept of learning-to-vote is introduced to sample quality label sets for each data entry on-the-fly during the training. Specifically, a meta-training-based label-sampling module is designed to achieve refined labels (weighted sum of attended ones) that benefit the model learning the most through additional back-propagations. We apply the learning-to-vote scheme on the classification task of a synthetic noisy CIFAR-10 to prove the concept and then demonstrate superior results (3-5% increase on average in multiple disease classification AUCs) on the chest x-ray images from a hospital-scale dataset (MIMIC-CXR) and hand-labeled dataset (OpenI) in comparison to regular training paradigms.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Learning_Quality_Labels_for_Robust_Image_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Learning_Quality_Labels_for_Robust_Image_Classification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483956/,"['Training', 'Annotations', 'Supervised learning', 'MIMICs', 'Noise measurement', 'Labeling', 'Task analysis']","['Image Classification', 'Medical Imaging', 'Classification Task', 'Chest X-ray', 'Medical Image Analysis', 'Annotation Quality', 'Chest X-ray Images', 'Deep Learning', 'Image Features', 'Confusion Matrix', 'Attention Mechanism', 'Multi-label', 'Majority Voting', 'Global Average Pooling', 'Textual Features', 'Binary Cross-entropy Loss', 'Gradient Flow', 'Radiology Reports', 'Multiple Labels', 'Global Average Pooling Layer', 'Noisy Labels', 'Multiple Annotations', 'Meta Learning', 'Label Noise', 'Uncertain Cases', 'Single Annotation']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",,"Supervised learning paradigms largely benefit from the tremendous amount of annotated data. However, the quality of the annotations often varies among labelers. Multi-observer studies have been conducted to examine the annotation variances (by labeling the same data multiple times) to see how it affects critical applications like medical image analysis. In this paper, we demonstrate how multiple sets of annotations (either hand-labeled or algorithm-generated) can be utilized together and mutually benefit the learning of classification tasks. A scheme of learning-to-vote is introduced to sample quality label sets for each data entry on-the-fly during the training. Specifically, a label-sampling module is designed to achieve refined labels (weighted sum of attended ones) that benefit the model learning the most through additional back-propagations. We apply the learning-to-vote scheme on the classification task of a synthetic noisy CIFAR-10 to prove the concept and then demonstrate superior results (3-5% increase on average in multiple disease classification AUCs) on the chest x-ray images from a hospital-scale dataset (MIMIC-CXR) and hand-labeled dataset (OpenI) in comparison to regular training paradigms."
Learning Residual Elastic Warps for Image Stitching Under Dirichlet Boundary Condition,"Minsu Kim, Yongjun Lee, Woo Kyoung Han, Kyong Hwan Jin","DGIST, Republic of Korea; Korea University, Republic of Korea",100.0,South Korea,0.0,,"Trendy suggestions for learning-based elastic warps enable the deep image stitchings to align images exposed to large parallax errors. Despite the remarkable alignments, the methods struggle with occasional holes or discontinuity between overlapping and non-overlapping regions of a target image as the applied training strategy mostly focuses on overlap region alignment. As a result, they require additional modules such as seam finder and image inpainting for hiding discontinuity and filling holes, respectively. In this work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with Dirichlet boundary condition and boost performances by residual learning for recurrent misalign correction. Specifically, REwarp predicts a homography and a Thin-plate Spline (TPS) under the boundary constraint for discontinuity and hole-free image stitching. Our experiments show the favorable aligns and the competitive computational costs of REwarp compared to the existing stitching methods. Our source code is available at https://github.com/minshu-kim/REwarp.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Learning_Residual_Elastic_Warps_for_Image_Stitching_Under_Dirichlet_Boundary_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Learning_Residual_Elastic_Warps_for_Image_Stitching_Under_Dirichlet_Boundary_WACV_2024_paper.pdf,,https://github.com/minshu-kim/REwarp,2309.01406,main,Poster,https://ieeexplore.ieee.org/document/10483589/,"['Training', 'Computer vision', 'Image resolution', 'Source coding', 'Boundary conditions', 'Real-time systems', 'Filling']","['Dirichlet Boundary Conditions', 'Target Image', 'Overlap Region', 'Non-overlapping Regions', 'Thin-plate Spline', 'Image Inpainting', 'Control Points', 'Radial Basis Function', 'Performance Gain', 'Displacement Vector', 'Global Alignment', 'Image Alignment', 'Uniform Grid', 'Cost Volume', 'Warp Field']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Trendy suggestions for learning-based elastic warps enable the deep image stitchings to align images exposed to large parallax errors. Despite the remarkable alignments, the methods struggle with occasional holes or discontinuity between overlapping and non-overlapping regions of a target image as the applied training strategy mostly focuses on overlap region alignment. As a result, they require additional modules such as seam finder and image inpainting for hiding discontinuity and filling holes, respectively. In this work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with Dirichlet boundary condition and boost performances by residual learning for recurrent misalign correction. Specifically, REwarp predicts a homography and a Thin-plate Spline (TPS) under the boundary constraint for discontinuity and hole-free image stitching. Our experiments show the favorable aligns and the competitive computational costs of REwarp compared to the existing stitching methods. Our source code is available at https://github.com/minshu-kim/REwarp."
Learning Robust Deep Visual Representations From EEG Brain Recordings,"Prajwal Singh, Dwip Dalal, Gautam Vashishtha, Krishna Miyapuram, Shanmuganathan Raman","BRAIN Lab‡, IIT Gandhinagar, India; CVIG Lab†, IIT Gandhinagar, India",100.0,India,0.0,,"Decoding the human brain has been a hallmark of neuroscientists and Artificial Intelligence researchers alike. Reconstruction of visual images from brain Electroencephalography (EEG) signals has garnered a lot of interest due to its applications in brain-computer interfacing. This study proposes a two-stage method where the first step is to obtain EEG-derived features for robust learning of deep representations and subsequently utilize the learned representation for image generation and classification. We demonstrate the generalizability of our feature extraction pipeline across three different datasets using deep-learning architectures with supervised and contrastive learning methods. We have performed the zero-shot EEG classification task to support the generalizability claim further. We observed that a subject invariant linearly separable visual representation was learned using EEG data alone in an unimodal setting that gives better k-means accuracy as compared to a joint representation learning between EEG and images. Finally, we propose a novel framework to transform unseen images into the EEG space and reconstruct them with approximation, showcasing the potential for image reconstruction from EEG signals. Our proposed image synthesis method from EEG shows 62.9% and 36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz datasets, which is better than state-of-the-art performance in GAN.",https://openaccess.thecvf.com/content/WACV2024/html/Singh_Learning_Robust_Deep_Visual_Representations_From_EEG_Brain_Recordings_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Singh_Learning_Robust_Deep_Visual_Representations_From_EEG_Brain_Recordings_WACV_2024_paper.pdf,,https://github.com/prajwalsingh/EEGStyleGAN-ADA,2310.16532,main,Poster,https://ieeexplore.ieee.org/document/10484160/,"['Representation learning', 'Visualization', 'Image synthesis', 'Transforms', 'Self-supervised learning', 'Feature extraction', 'Electroencephalography']","['Visual Representation', 'Image Classification', 'Image Reconstruction', 'Generative Adversarial Networks', 'EEG Data', 'Representation Learning', 'Image Generation', 'EEG Signals', 'Image Synthesis', 'Linearly Separable', 'Joint Representation', 'EEG Classification', 'Unseen Images', 'Brain Activity', 'Positive Samples', 'Batch Size', 'Visual Features', 'Discriminative Features', 'Image Representation', 'EEG Features', 'Image Retrieval', 'LSTM Network', 'Triplet Loss', 'Image Retrieval Task', 'Fréchet Inception Distance', 'EEG Dataset', 'Pre-trained Encoder', 'Object Dataset', 'Supervised Classification Method']","['Applications', 'Biomedical / healthcare / medicine']",,"Decoding the human brain has been a hallmark of neuroscientists and Artificial Intelligence researchers alike. Reconstruction of visual images from brain Electroencephalography (EEG) signals has garnered a lot of interest due to its applications in brain-computer interfacing. This study proposes a two-stage method where the first step is to obtain EEG-derived features for robust learning of deep representations and subsequently utilize the learned representation for image generation and classification. We demonstrate the generalizability of our feature extraction pipeline across three different datasets using deep-learning architectures with supervised and contrastive learning methods. We have performed the zero-shot EEG classification task to support the generalizability claim further. We observed that a subject invariant linearly separable visual representation was learned using EEG data alone in an unimodal setting that gives better k-means accuracy as compared to a joint representation learning between EEG and images. Finally, we propose a novel framework to transform unseen images into the EEG space and reconstruct them with approximation, showcasing the potential for image reconstruction from EEG signals. Our proposed image synthesis method from EEG shows 62.9% and 36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz datasets, which is better than state-of-the-art performance in GAN 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Learning Saliency From Fixations,"Yasser Abdelaziz Dahou Djilali, Kevin McGuinness, Noel O’Connor","Dublin City University, Ireland",100.0,Ireland,0.0,,"We present a novel approach for saliency prediction in images, leveraging parallel decoding in transformers to learn saliency solely from fixation maps. Models typically rely on continuous saliency maps, to overcome the difficulty of optimizing for the discrete fixation map. We attempt to replicate the experimental setup that generates saliency datasets. Our approach treats saliency prediction as a direct set prediction problem, via a global loss that enforces unique fixations prediction through bipartite matching and a transformer encoder-decoder architecture. By utilizing a fixed set of learned fixation queries, the cross-attention reasons over the image features to directly output the fixation points, distinguishing it from other modern saliency predictors. Our approach, named Saliency TRansformer (SalTR) achieves remarkable results on the Salicon benchmark.",https://openaccess.thecvf.com/content/WACV2024/html/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Djilali_Learning_Saliency_From_Fixations_WACV_2024_paper.pdf,,https://github.com/YasserdahouML/SalTR,,main,Poster,,,,,,
Learning To Adapt CLIP for Few-Shot Monocular Depth Estimation,"Xueting Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, Zhihai He","Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; Pengcheng Laboratory, Shenzhen, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China",66.66666666666666,China,33.33333333333334,China,"Pre-trained Visual-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic elements. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6% in terms of MARE.",https://openaccess.thecvf.com/content/WACV2024/html/Hu_Learning_To_Adapt_CLIP_for_Few-Shot_Monocular_Depth_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hu_Learning_To_Adapt_CLIP_for_Few-Shot_Monocular_Depth_Estimation_WACV_2024_paper.pdf,,,2311.01034,main,Poster,https://ieeexplore.ieee.org/document/10483934/,"['Training', 'Adaptation models', 'Visualization', 'Costs', 'Semantics', 'Data preprocessing', 'Estimation']","['Depth Estimation', 'Monocular Depth Estimation', 'Contrastive Language-Image Pre-training', 'Semantic', 'Input Image', 'Depth Values', 'Training Costs', 'Depth Distribution', 'Input Text', 'Mean Absolute Relative Error', 'Training Scenes', 'V2 Dataset', 'Root Mean Square Error', 'Support Vector Machine', 'Image Features', 'Codebook', 'RGB Images', 'Patch Size', 'Image Encoder', 'Textual Features', 'Text Encoder', 'Scene Features', 'Depth Prediction', 'Self-supervised Learning', 'Ground Truth Depth', 'Language Model', 'Error Metrics', 'Scene Classification']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",4,"Pre-trained Vision-Language Models (VLMs), such as CLIP, have shown enhanced performance across a range of tasks that involve the integration of visual and linguistic modalities. When CLIP is used for depth estimation tasks, the patches, divided from the input images, can be combined with a series of semantic descriptions of the depth information to obtain similarity results. The coarse estimation of depth is then achieved by weighting and summing the depth values, called depth bins, corresponding to the predefined semantic descriptions. The zero-shot approach circumvents the computational and time-intensive nature of traditional fully-supervised depth estimation methods. However, this method, utilizing fixed depth bins, may not effectively generalize as images from different scenes may exhibit distinct depth distributions. To address this challenge, we propose a few-shot-based method which learns to adapt the VLMs for monocular depth estimation to balance training costs and generalization capabilities. Specifically, it assigns different depth bins for different scenes, which can be selected by the model during inference. Additionally, we incorporate learnable prompts to preprocess the input text to convert the easily human-understood text into easily model-understood vectors and further enhance the performance. With only one image per scene for training, our extensive experiment results on the NYU V2 and KITTI dataset demonstrate that our method outperforms the previous state-of-the-art method by up to 10.6% in terms of MARE
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Learning To Compose SuperWeights for Neural Parameter Allocation Search,"Piotr Teterwak, Soren Nelson, Nikoli Dryden, Dina Bashkirova, Kate Saenko, Bryan A. Plummer",ETH Zürich; Physical Sciences Inc; Boston University,66.66666666666666,"Switzerland, USA",33.33333333333334,USA,"Neural parameter allocation search (NPAS) automates parameter sharing by obtaining weights for a network given an arbitrary, fixed parameter budget. Prior work has two major drawbacks we aim to address. First, there is a dis- connect in the sharing pattern between the search and train- ing steps, where weights are warped for layers of different sizes during the search to measure similarity, but not during training, resulting in reduced performance. To address this, we generate layer weights by learning to compose sets of SuperWeights, which represent a group of trainable parameters. These SuperWeights are created to be large enough so they can be used to represent any layer in the network, but small enough that they are computationally efficient. The second drawback we address is the method of measuring similarity between shared parameters. Whereas prior work compared the weights themselves, we argue this does not take into account the amount of conflict between the shared weights. Instead, we use gradient information to identify layers with shared weights that wish to diverge from each other. We demonstrate that our SuperWeight Networks consistently boost performance over the state-of-the-art on the ImageNet and CIFAR datasets in the NPAS setting. We further show that our approach can generate parameters for many network architectures using the same set of weights. This enables us to support tasks like efficient ensembling and anytime prediction, outperforming fully-parameterized ensembles with 17% fewer parameters.",https://openaccess.thecvf.com/content/WACV2024/html/Teterwak_Learning_To_Compose_SuperWeights_for_Neural_Parameter_Allocation_Search_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Teterwak_Learning_To_Compose_SuperWeights_for_Neural_Parameter_Allocation_Search_WACV_2024_paper.pdf,,https://github.com/piotr-teterwak/SuperWeights,2312.01274,main,Poster,https://ieeexplore.ieee.org/document/10483583/,"['Training', 'Weight measurement', 'Computer vision', 'Computational modeling', 'Computer architecture', 'Network architecture', 'Size measurement']","['Network Layer', 'Trainable Parameters', 'Parameters In Group', 'Fewer Parameters', 'Weights Of Layer', 'Search Step', 'Shared Parameters', 'Shared Weights', 'Contralateral', 'Neural Network', 'Ensemble Model', 'Single Network', 'Inference Time', 'ImageNet Dataset', 'Ensemble Members', 'Variety Of Architectures', 'Time Budget', 'Top-1 Accuracy', 'NetworkX', 'Priority Queue', 'Number Of Ensemble Members', 'Heterogeneous Ensemble']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Neural parameter allocation search (NPAS) automates parameter sharing by obtaining weights for a network given an arbitrary, fixed parameter budget. Prior work has two major drawbacks we aim to address. First, there is a disconnect in the sharing pattern between the search and training steps, where weights are warped for layers of different sizes during the search to measure similarity, but not during training, resulting in reduced performance. To address this, we generate layer weights by learning to compose sets of SuperWeights, which represent a group of trainable parameters. These SuperWeights are created to be large enough so they can be used to represent any layer in the network, but small enough that they are computationally efficient. The second drawback we address is the method of measuring similarity between shared parameters. Whereas prior work compared the weights themselves, we argue this does not take into account the amount of conflict between the shared weights. Instead, we use gradient information to identify layers with shared weights that wish to diverge from each other. We demonstrate that our SuperWeight Networks consistently boost performance over the state-of-the-art on the ImageNet and CIFAR datasets in the NPAS setting. We further show that our approach can generate parameters for many network architectures using the same set of weights. This enables us to support tasks like efficient ensembling and anytime prediction, outperforming fully-parameterized ensembles with 17% fewer parameters
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Learning To Generate Training Datasets for Robust Semantic Segmentation,"Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi","National University of Singapore; U2IS, ENSTA Paris, Institut Polytechnique de Paris; U2IS, ENSTA Paris, Institut Polytechnique de Paris, SATIE, Universit ´e Paris-Saclay; valeo.ai",75.0,"France, Singapore",25.0,France,"Semantic segmentation methods have advanced significantly. Still, their robustness to real-world perturbations and object types not seen during training remains a challenge, particularly in safety-critical applications. We propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness in the face of real-world perturbations, distribution shifts, and out-of-distribution samples. Our results suggest that this approach could be valuable in safety-critical applications, where the reliability of perception modules such as semantic segmentation is of utmost importance and comes with a limited computational budget in inference. We release our code at https://github.com/ENSTA-U2IS/robusta.",https://openaccess.thecvf.com/content/WACV2024/html/Hariat_Learning_To_Generate_Training_Datasets_for_Robust_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hariat_Learning_To_Generate_Training_Datasets_for_Robust_Semantic_Segmentation_WACV_2024_paper.pdf,,github.com/ENSTA-U2IS/robusta,2308.02535,main,Poster,https://ieeexplore.ieee.org/document/10484317/,"['Training', 'Semantic segmentation', 'Perturbation methods', 'Computer network reliability', 'Transformers', 'Reliability engineering', 'Generative adversarial networks']","['Semantic Segmentation', 'Robust Segmentation', 'Semantic Segmentation Datasets', 'Robust Semantic Segmentation', 'Segmentation Model', 'Robust Network', 'Conditional Generative Adversarial Network', 'Safety-critical Applications', 'Face Of Perturbations', 'Training Set', 'High-resolution Images', 'Deep Neural Network', 'Normalization Layer', 'Latent Space', 'Robust Algorithm', 'Defocus', 'Outlier Detection', 'Synthetic Images', 'Translational Model', 'Fréchet Inception Distance', 'Aleatoric Uncertainty', 'Realistic Images', 'Epistemic Uncertainty', 'Input Label', 'Urban Scenes', 'State Of The Art Methods', 'Negative Log-likelihood', 'Binary Cross Entropy']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Datasets and evaluations']",1,"Semantic segmentation methods have advanced significantly. Still, their robustness to real-world perturbations and object types not seen during training remains a challenge, particularly in safety-critical applications. We propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness in the face of real-world perturbations, distribution shifts, and out-of-distribution samples. Our results suggest that this approach could be valuable in safety-critical applications, where the reliability of perception modules such as semantic segmentation is of utmost importance and comes with a limited computational budget in inference. We release our code at github.com/ENSTA-U2IS/robusta."
Learning To Recognize Occluded and Small Objects With Partial Inputs,"Hasib Zunair, A. Ben Hamza","CIISE, Concordia University, Montreal, QC, Canada",100.0,Canada,0.0,,"Recognizing multiple objects in an image is challenging due to occlusions, and becomes even more so when the objects are small. While promising, existing multi-label image recognition models do not explicitly learn context-based representations, and hence struggle to correctly recognize small and occluded objects. Intuitively, recognizing occluded objects requires knowledge of partial input, and hence context. Motivated by this intuition, we propose Masked Supervised Learning (MSL), a single-stage, model-agnostic learning paradigm for multi-label image recognition. The key idea is to learn context-based representations using a masked branch and to model label co-occurrence using label consistency. Experimental results demonstrate the simplicity, applicability and more importantly the competitive performance of MSL against previous state-of-the-art methods on standard multi-label image recognition benchmarks. In addition, we show that MSL is robust to random masking and demonstrate its effectiveness in recognizing non-masked objects. Code and pretrained models are available on GitHub.",https://openaccess.thecvf.com/content/WACV2024/html/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zunair_Learning_To_Recognize_Occluded_and_Small_Objects_With_Partial_Inputs_WACV_2024_paper.pdf,,Available on GitHub,2310.18517,main,Poster,https://ieeexplore.ieee.org/document/10483572/,"['Training', 'Computer vision', 'Image recognition', 'Supervised learning', 'Object detection', 'Data models', 'Task analysis']","['Small Objects', 'Part Of The Input', 'Supervised Learning', 'Image Object', 'Image Recognition', 'Occluded Objects', 'Label Consistency', 'High-resolution', 'Neural Network', 'Convolutional Neural Network', 'Input Image', 'Object Detection', 'Recurrent Neural Network', 'Self-driving', 'Combined Network', 'Graph Neural Networks', 'Masked Images', 'Graph-based Methods', 'Input Resolution', 'Examples Of Predictions', 'Masking Strategy', 'Transformer-based Methods']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Recognizing multiple objects in an image is challenging due to occlusions, and becomes even more so when the objects are small. While promising, existing multi-label image recognition models do not explicitly learn context-based representations, and hence struggle to correctly recognize small and occluded objects. Intuitively, recognizing occluded objects requires knowledge of partial input, and hence context. Motivated by this intuition, we propose Masked Supervised Learning (MSL), a single-stage, model-agnostic learning paradigm for multi-label image recognition. The key idea is to learn context-based representations using a masked branch and to model label co-occurrence using label consistency. Experimental results demonstrate the simplicity, applicability and more importantly the competitive performance of MSL against previous state-of-the-art methods on standard multi-label image recognition benchmarks. In addition, we show that MSL is robust to random masking and demonstrate its effectiveness in recognizing non-masked objects. Code and pretrained models are available on GitHub."
Learning Transferable Representations for Image Anomaly Localization Using Dense Pretraining,"Haitian He, Sarah Erfani, Mingming Gong, Qiuhong Ke",The University of Melbourne; Monash University,100.0,Australia,0.0,,"Image anomaly localization (IAL) is widely applied in fault detection and industrial inspection domains to discover anomalous patterns in images at the pixel level. The unique challenge of this task is the lack of comprehensive anomaly samples for model training. The state-of-the-art methods train end-to-end models that leverage outlier exposure to simulate pseudo anomalies, but they show poor transferability to new datasets due to the inherent bias to the synthesized outliers during training. Recently, two-stage instance-level self-supervised learning (SSL) has shown potential in learning generic representations for IAL. However, we hypothesize that dense-level SSL is more compatible as IAL requires pixel-level prediction. In this paper, we bridge these gaps by proposing a two-stage, dense pre-training model tailored for the IAL task. More specifically, our model utilizes dual positive-pair selection criteria and dual feature scales to learn more effective representations. Through extensive experiments, we show that our learned representations achieve significantly better anomaly localization performance among two-stage models, while requiring almost half the convergence time. Moreover, our learned representations have better transferability to unseen datasets. Code is available at https://github. com/terrlo/DS2.",https://openaccess.thecvf.com/content/WACV2024/html/He_Learning_Transferable_Representations_for_Image_Anomaly_Localization_Using_Dense_Pretraining_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/He_Learning_Transferable_Representations_for_Image_Anomaly_Localization_Using_Dense_Pretraining_WACV_2024_paper.pdf,,https://github.com/terrlo/DS2,,main,Poster,https://ieeexplore.ieee.org/document/10483602/,"['Training', 'Location awareness', 'Representation learning', 'Computer vision', 'Fault detection', 'Self-supervised learning', 'Inspection']","['Anomaly Localization', 'Localization Performance', 'Representation Learning', 'Density Model', 'Two-stage Model', 'Image Pattern', 'Pixel Level', 'Self-supervised Learning', 'Anomalous Patterns', 'Feature Maps', 'Object Detection', 'Image Regions', 'Semantic Segmentation', 'Multi-scale Features', 'Categorical Model', 'Normal Images', 'Standard Deviation Score', 'Output Feature Map', 'Cut-and-paste', 'Self-supervised Task', 'Pre-training Tasks', 'Two-stage Framework', 'Screw Head', 'Local Anomalies', 'Small Batch Size', 'Semantic Regions', 'Size Of The Training Dataset', 'Clustering Tendency', 'Batch Size']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Image anomaly localization (IAL) is widely applied in fault detection and industrial inspection domains to discover anomalous patterns in images at the pixel level. The unique challenge of this task is the lack of comprehensive anomaly samples for model training. The state-of-the-art methods train end-to-end models that leverage outlier exposure to simulate pseudo anomalies, but they show poor transferability to new datasets due to the inherent bias to the synthesized outliers during training. Recently, two-stage instance-level self-supervised learning (SSL) has shown potential in learning generic representations for IAL. However, we hypothesize that dense-level SSL is more compatible as IAL requires pixel-level prediction. In this paper, we bridge these gaps by proposing a two-stage, dense pretraining model tailored for the IAL task. More specifically, our model utilizes dual positive-pair selection criteria and dual feature scales to learn more effective representations. Through extensive experiments, we show that our learned representations achieve significantly better anomaly localization performance among two-stage models, while requiring almost half the convergence time. Moreover, our learned representations have better transferability to unseen datasets. Code is available at https://github.com/terrlo/DS2."
Learning Visual Body-Shape-Aware Embeddings for Fashion Compatibility,"Kaicheng Pang, Xingxing Zou, Waikeung Wong","Laboratory for Artificial Intelligence in Design, Hong Kong SAR; School of Fashion and Textiles, The Hong Kong Polytechnic University",50.0,Hong Kong,50.0,Hong Kong,"Body shape is a crucial factor in outfit recommendation. Previous studies that directly used body measurement data to investigate the relationship between body shape and outfit have achieved limited performance due to oversimplified body shape representations. This paper proposes a Visual Body-shape-Aware Network (ViBA-Net) to improve the fashion compatibility model's awareness of human body shape through visual-level information. Specifically, ViBA-Net consists of three modules: a body-shape embedding module, which extracts visual and anthropometric features of body shape from a newly introduced large-scale body shape dataset; an outfit embedding module, which learns the outfit representation based on visual features extracted from a try-on image and textual features extracted from fashion attributes; and a joint embedding module, which jointly models the relationship between the representations of body shape and outfit. ViBA-Net is designed to generate attribute-level explanations for the evaluation results based on the computed attention weights. The effectiveness of ViBA-Net is evaluated on two mainstream datasets through qualitative and quantitative analysis. Data and code are released.",https://openaccess.thecvf.com/content/WACV2024/html/Pang_Learning_Visual_Body-Shape-Aware_Embeddings_for_Fashion_Compatibility_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pang_Learning_Visual_Body-Shape-Aware_Embeddings_for_Fashion_Compatibility_WACV_2024_paper.pdf,,https://github.com/BenjaminPang/ViBA-Net,,main,Poster,,,,,,
Learning the What and How of Annotation in Video Object Segmentation,"Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos","LIX, Ecole Polytechnique, CNRS, Institut Polytechnique de Paris; Technical University of Denmark, Pioneer Center for AI",100.0,"Denmark, France",0.0,,"Video Object Segmentation (VOS) is crucial for several applications, from video editing to video data generation. Training a VOS model requires an abundance of manually labeled training videos. The de-facto traditional way of annotating objects requires humans to draw detailed segmentation masks on the target objects at each video frame. This annotation process, however, is tedious and time-consuming. To reduce this annotation cost, in this paper, we propose EVA-VOS, a human-in-the-loop annotation framework for video object segmentation. Unlike the traditional approach, we introduce an agent that predicts iteratively both which frame (""What"") to annotate and which annotation type (""How"") to use. Then, the annotator annotates only the selected frame that is used to update a VOS module, leading to significant gains in annotation time. We conduct experiments on the MOSE and the DAVIS datasets and we show that: (a) EVA-VOS leads to masks with accuracy close to the human agreement 3.5x faster than the standard way of annotating videos; (b) our frame selection achieves state-of-the-art performance; (c) EVA-VOS yields significant performance gains in terms of annotation time compared to all other methods and baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Delatolas_Learning_the_What_and_How_of_Annotation_in_Video_Object_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Delatolas_Learning_the_What_and_How_of_Annotation_in_Video_Object_WACV_2024_paper.pdf,https://eva-vos.compute.dtu.dk/,,2311.04414,main,Poster,https://ieeexplore.ieee.org/document/10484219,"['Training', 'Computer vision', 'Costs', 'Annotations', 'Object segmentation', 'Manuals', 'Performance gain']","['Video Object Segmentation', 'Video Frames', 'Standard Way', 'Target Object', 'Video Editing', 'Frame Selection', 'Feature Space', 'Maximum Distance', 'F-value', 'Bounding Box', 'Hours Of Time', 'Manual Annotation', 'Scribble', 'Segmentation Quality', 'Low Budget', 'Parallel Branches', 'Suitable Type', 'Reinforcement Learning Agent', 'Random Baseline', 'Video Object', 'Proximal Policy Optimization', 'Image Encoder']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Datasets and evaluations']",1,"Video Object Segmentation (VOS) is crucial for several applications, from video editing to video data generation. Training a VOS model requires an abundance of manually labeled training videos. The de-facto traditional way of annotating objects requires humans to draw detailed segmentation masks on the target objects at each video frame. This annotation process, however, is tedious and time-consuming. To reduce this annotation cost, in this paper, we propose EVA-VOS, a human-in-the-loop annotation framework for video object segmentation. Unlike the traditional approach, we introduce an agent that predicts iteratively both which frame (""What"") to annotate and which annotation type (""How"") to use. Then, the annotator annotates only the selected frame that is used to update a VOS module, leading to significant gains in annotation time. We conduct experiments on the MOSE and the DAVIS datasets and we show that: (a) EVA-VOS leads to masks with accuracy close to the human agreement 3.5× faster than the standard way of annotating videos; (b) our frame selection achieves state-of-the-art performance; (c) EVA-VOS yields significant performance gains in terms of annotation time compared to all other methods and baselines."
Learning to Detour: Shortcut Mitigating Augmentation for Weakly Supervised Semantic Segmentation,"JuneHyoung Kwon, Eunju Lee, Yunsung Cho, YoungBin Kim","Graduate School of Advanced Imaging Science, Multimedia & Film, Chung-Ang University, Korea; Department of Artificial Intelligence, Chung-Ang University, Korea",100.0,South Korea,0.0,,"Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels. However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance. In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features. Our approach disentangles the object-relevant and background features. We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations. SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions. In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric. The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kwon_Learning_to_Detour_Shortcut_Mitigating_Augmentation_for_Weakly_Supervised_Semantic_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483779/,"['Location awareness', 'Training', 'Measurement', 'Computer vision', 'Costs', 'Correlation', 'Semantic segmentation']","['Semantic Segmentation', 'Weakly Supervised Semantic Segmentation', 'Background Characteristics', 'Classifier Training', 'Target Object', 'Dataset Bias', 'Disentangled Representation', 'Pixel-level Labels', 'Training Dataset', 'Classification Task', 'Data Augmentation', 'Class Labels', 'Airplane', 'Representation Of Space', 'Local Map', 'Augmentation Methods', 'Background Regions', 'Diverse Representation', 'Self-supervised Learning', 'Class Activation Maps', 'Image X', 'Predicted Class Label', 'Object Regions', 'Target Label', 'Weak Labels', 'Discriminative Regions', 'Contrastive Loss', 'Backbone Network', 'Railway Line']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Weakly supervised semantic segmentation (WSSS) employing weak forms of labels has been actively studied to alleviate the annotation cost of acquiring pixel-level labels. However, classifiers trained on biased datasets tend to exploit shortcut features and make predictions based on spurious correlations between certain backgrounds and objects, leading to a poor generalization performance. In this paper, we propose shortcut mitigating augmentation (SMA) for WSSS, which generates synthetic representations of object-background combinations not seen in the training data to reduce the use of shortcut features. Our approach disentangles the object-relevant and background features. We then shuffle and combine the disentangled representations to create synthetic features of diverse object-background combinations. SMA-trained classifier depends less on contexts and focuses more on the target object when making predictions. In addition, we analyzed the behavior of the classifier on shortcut usage after applying our augmentation using an attribution method-based metric. The proposed method achieved the improved performance of semantic segmentation result on PASCAL VOC 2012 and MS COCO 2014 datasets."
Learning to Read Analog Gauges from Synthetic Data,"Juan Leon-Alcazar, Yazeed Alnumay, Cheng Zheng, Hassane Trigui, Sahejad Patel, Bernard Ghanem","Aramco, Thuwal, Saudi Arabia; KAUST, Thuwal, Saudi Arabia",100.0,Saudi Arabia,0.0,,"Manually reading and logging gauge data is time-inefficient, and the effort increases according to the number of gauges available. We present a pipeline that automates the reading of analog gauges. We propose a two-stage CNN pipeline that identifies the key structural components of an analog gauge and outputs an angular reading. To facilitate the training of our approach, a synthetic dataset is generated thus obtaining a set of realistic analog gauges with their corresponding annotation. To validate our proposal, an additional real-world dataset was collected with 4.813 manually curated images. When compared against state-of-the-art methodologies, our method shows a significant improvement of 4.55 in the average error, which is a 52% relative improvement. The resources for this project will be made available at: https://github.com/fuankarion/automatic-gauge-reading.",https://openaccess.thecvf.com/content/WACV2024/html/Leon-Alcazar_Learning_to_Read_Analog_Gauges_from_Synthetic_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Leon-Alcazar_Learning_to_Read_Analog_Gauges_from_Synthetic_Data_WACV_2024_paper.pdf,,https://github.com/fuankarion/automatic-gauge-reading,2308.14583,main,Poster,https://ieeexplore.ieee.org/document/10483985/,"['Training', 'Bridges', 'Computer vision', 'Annotations', 'Pipelines', 'Proposals', 'Noise measurement']","['Learning To Read', 'Real-world Datasets', 'Training Data', 'Deep Models', 'Semantic Segmentation', 'Artificial Light', 'Needle Tip', 'Video Capture', 'Baseline Scenario', 'Segmentation Dataset', 'Landmark Localization', 'Industrial Facilities', 'Ground Truth Values', 'Segmentation Step', 'Hough Transform', 'Domain Gap', 'Needle Position', 'Synthetic Training Data', 'Key Landmarks', 'Presence Of Shadows', 'Automatic Reading', 'Gauge Locations', 'Computer Vision', 'Angular Position', 'Max Values', 'Daylight', 'Optical Character Recognition', 'Deep Learning', 'Detection Step', 'Video Clips']","['Applications', 'Structural engineering / civil engineering']",2,"Manually reading and logging gauge data is time-inefficient, and the effort increases according to the number of gauges available. We present a pipeline that automates the reading of analog gauges. We propose a two-stage CNN pipeline that identifies the key structural components of an analog gauge and outputs an angular reading. To facilitate the training of our approach, a synthetic dataset is generated thus obtaining a set of realistic analog gauges with their corresponding annotation. To validate our proposal, an additional real-world dataset was collected with 4.813 manually curated images. When compared against state-of-the-art methodologies, our method shows a significant improvement of 4.55
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">◦</sup>
 in the average error, which is a 52% relative improvement. The resources for this project will be made available at: https://github.com/fuankarion/automatic-gauge-reading."
Learning-Based Spotlight Position Optimization for Non-Line-of-Sight Human Localization and Posture Classification,"Sreenithy Chandran, Tatsuya Yatagawa, Hiroyuki Kubo, Suren Jayasuriya",Arizona State University; Chiba University; Hitotsubashi University,100.0,"Japan, USA",0.0,,"Non-line-of-sight imaging (NLOS) is the process of estimating information about a scene that is hidden from the direct line of sight of the camera. NLOS imaging typically requires time-resolved detectors and a laser source for illumination, which are both expensive and computationally intensive to handle. In this paper, we propose an NLOS-based localization and posture classification technique that works on a system of an off-the-shelf projector and camera. We leverage a message-passing neural network to learn a scene geometry and predict the best position to be spotlighted by the projector that can maximize the NLOS signal. The training of the neural network is performed in an end-to-end manner. Therefore, the ground truth spotlighted position is unnecessary during the training, and the network parameters are optimized to maximize the NLOS performance. Unlike prior deep-learning-based NLOS techniques that assume planar relay walls, our system allows us to handle line-of-sight scenes where scene geometries are more arbitrary. Our method demonstrates state-of-the-art performance in object localization and position classification using both synthetic and real scenes.",https://openaccess.thecvf.com/content/WACV2024/html/Chandran_Learning-Based_Spotlight_Position_Optimization_for_Non-Line-of-Sight_Human_Localization_and_Posture_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chandran_Learning-Based_Spotlight_Position_Optimization_for_Non-Line-of-Sight_Human_Localization_and_Posture_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
LensNeRF: Rethinking Volume Rendering Based on Thin-Lens Camera Model,"Min-Jung Kim, Gyojung Gu, Jaegul Choo","Graduate School of AI, KAIST",100.0,South Korea,0.0,,"Recent advances in Neural Radiance Field (NeRF) show promising results in rendering realistic novel view images. However, NeRF and its variants assume that input images are captured using a pinhole camera and that subjects in images are always all-in-focus by tacit agreement. In this paper, we propose aperture-aware NeRF optimization and rendering methods using a thin-lens model (dubbed LensNeRF), which allows defocus images of any aperture size as input and output. To generalize a pinhole camera model to a thin-lens camera model in NeRF framework, we define multiple rays originating from the aperture area, solving world-to-pixel scale ambiguity. Also, we propose in-focus loss that assigns the given pixel color to points on the focus plane to alleviate the color ambiguity caused by the use of multiple rays. For the rigorous evaluation of the proposed method, we collect a real forward-facing dataset with different F-numbers for each viewpoint. Experimental results demonstrate that our method successfully fuses an aperture-size adjustable thin-lens camera model into the NeRF architecture, showing favorable qualitative and quantitative results compared to baseline models. The dataset will be made available.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_LensNeRF_Rethinking_Volume_Rendering_Based_on_Thin-Lens_Camera_Model_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_LensNeRF_Rethinking_Volume_Rendering_Based_on_Thin-Lens_Camera_Model_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484286/,"['Solid modeling', 'Computer vision', 'Image color analysis', 'Fuses', 'Computer architecture', 'Apertures', 'Cameras']","['Camera Model', 'Volume Rendering', 'Quantitative Results', 'Input Image', 'Qualitative Results', 'Aperture Size', 'Pixel Color', 'Pinhole Camera', 'Aperture Area', 'Pinhole Camera Model', 'Scale Ambiguity', 'Training Phase', '3D Space', 'Focal Length', 'Baseline Methods', 'Sample Position', 'Image Patches', '3D Point', 'Two-stage Method', 'Outer Loop', 'View Synthesis', 'Focal Length Of Lens', 'Blurred Images', 'Classic Task', 'World Space', 'Explicit Representation', 'Aperture Radius', 'Blur Kernel', 'Gradient Flow', 'Principal Plane']","['Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Recent advances in Neural Radiance Field (NeRF) show promising results in rendering realistic novel view images. However, NeRF and its variants assume that input images are captured using a pinhole camera and that subjects in images are always all-in-focus by tacit agreement. In this paper, we propose aperture-aware NeRF optimization and rendering methods using a thin-lens model (dubbed LensNeRF), which allows defocus images of any aperture size as input and output. To generalize a pinhole camera model to a thin-lens camera model in NeRF framework, we define multiple rays originating from the aperture area, solving world-to-pixel scale ambiguity. Also, we propose in-focus loss that assigns the given pixel color to points on the focus plane to alleviate the color ambiguity caused by the use of multiple rays. For the rigorous evaluation of the proposed method, we collect a real forward-facing dataset with different F-numbers for each viewpoint. Experimental results demonstrate that our method successfully fuses an aperture-size adjustable thin-lens camera model into the NeRF architecture, showing favorable qualitative and quantitative results compared to baseline models. The dataset will be made available."
Let the Beat Follow You - Creating Interactive Drum Sounds From Body Rhythm,"Xiulong Liu, Kun Su, Eli Shlizerman",University of Washington,100.0,USA,0.0,,"It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music as these happen could create a unique interactive experience. Creating such an experience is challenging and cannot be achieved with existing methods since it requires a real-time translation of related visual cues into in-rhythm sounds. In this work, we propose a novel learning-based system, called 'InteractiveBeat', which generates an evolving interactive soundtrack for a camera input that captures person's movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with update rules. For training and evaluation of InteractiveBeat, in addition to training on well-annotated large-scale dance database, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. We evaluate InteractiveBeat in two scenarios: i) laboratory setting, ii) prerecorded videos of movements from in-the-wild videos, and develop 'live' demo prototype of the system. Our results on evaluations show that the system can generate interactive rhythmic drums with higher accuracy than existing methods and achieves non-cumulative latency of 34ms. This allows InteractiveBeat to be synchronized with the video stream and to react to movements in real-time.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Let_the_Beat_Follow_You_-_Creating_Interactive_Drum_Sounds_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Let_the_Beat_Follow_You_-_Creating_Interactive_Drum_Sounds_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483878/,"['Visualization', 'Databases', 'Pipelines', 'Prototypes', 'Streaming media', 'Rhythm', 'Cameras']","['Interactive', 'Drum Sounds', 'Body Movements', 'Video Camera', 'Human Movement', 'Soundtrack', 'Background Music', 'Learning-based System', 'Human Body Movements', 'Neural Network', 'Real-Time System', 'Human Studies', 'Deep Neural Network', 'General System', 'Dynamic Programming', 'Inference Time', 'Human Motion', 'Gated Recurrent Unit', 'Motion Estimation', 'Sound Effects', 'Compact Network', 'Total Latency', 'Natural Sounds', 'Hidden Size', 'Style Transfer', 'Camera Frame', 'Inference Speed', 'First-order Difference', 'Objective Metrics']","['Applications', 'Arts / games / social media', 'Applications', 'Embedded sensing / real-time techniques', 'Applications', 'Virtual / augmented reality']",,"It is often the case that human body movements include rhythmic patterns. A video camera system that captures these patterns and responds to them with rhythmic sounds or music, as these happen, could create a unique interactive experience. Creating such an experience requires a real-time translation of related visual cues into in-rhythm sounds and warrants novel real-time methods. In this work, we propose a novel learning-based system, called ‘InteractiveBeat’, which generates an evolving interactive soundtrack for a camera input that captures person’s movements. InteractiveBeat infers body skeleton keypoints and translates them into drum rhythms using a series of sequence models. It then implements a conditional drum generation network for generating polyphonic drum sounds based on the rhythms. To guarantee real-time function, these models are integrated into a time-evolving pipeline with rules for updates. InteractiveBeat is trained and evaluated on a well-annotated large-scale dance database (AIST), and in addition, we collected a dataset of in-the-wild videos with people performing movements of various activities that correspond to background music. Furthermore, we develop a ‘live’ demo prototype of the system. Our evaluation results show that the system can generate interactive rhythmic drums more accurately than existing methods and achieves a non-cumulative latency of 34ms (approx. 30 fps). This allows InteractiveBeat to be synchronized with the video stream and react to real-time movements."
Let's Observe Them Over Time: An Improved Pedestrian Attribute Recognition Approach,"Kamalakar Vijay Thakare, Debi Prosad Dogra, Heeseung Choi, Haksub Kim, Ig-Jae Kim","Artificial Intelligence and Robotics Institute, Korea Institute of Science and Technology, Seoul 02792, Republic of Korea; Yonsei-KIST Convergence Research Institute, Yonsei University, Seoul 03722, Republic of Korea; Indian Institute of Technology, Bhubaneswar, Odisha, 752050, India",100.0,"India, South Korea",0.0,,"Despite poor image quality, occlusions, and small training datasets, recent pedestrian attribute recognition (PAR) methods have achieved considerable performance. However, leveraging only spatial information of different attributes limits their reliability and generalizability. This paper introduces a multi-perspective approach to reduce over-dependence on spatial clues of a single perspective and exploits other aspects available in multiple perspectives. In order to tackle image quality and occlusions, we exploit different spatial clues present across images and handpick the best attribute-specific features to classify. Precisely, we extract the class-activation energy of each attribute and correlate it with the corresponding energy present across other images using the proposed Self-Attentive Cross Relation Module. In the next stage, we fuse this correlation information with similar clues accumulated from the other images. Lastly, we train a classification neural network using combined correlation information with two different losses. We have validated our method on four widely used PAR datasets, namely Market1501, PETA, PA-100k, and Duke. Our method achieves superior performance over most existing methods, demonstrating the effectiveness of a multi-perspective approach in PAR.",https://openaccess.thecvf.com/content/WACV2024/html/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Thakare_Lets_Observe_Them_Over_Time_An_Improved_Pedestrian_Attribute_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484407,"['Image quality', 'Training', 'Computer vision', 'Pedestrians', 'Correlation', 'Image recognition', 'Fuses']","['Attribute Recognition', 'Pedestrian Attribute', 'Pedestrian Attribute Recognition', 'Neural Network', 'Training Dataset', 'Multiple Perspectives', 'Poor Image Quality', 'Training Set', 'Convolutional Neural Network', 'Activation Energy', 'Input Image', 'F1 Score', 'Recurrent Neural Network', 'Precision And Recall', 'Bounding Box', 'Multiple Images', 'Upper Body', 'Mean Accuracy', 'Confidence Score', 'Real-world Scenarios', 'Energy Score', 'Target Layer', 'Key Vector', 'Energy Vector', 'Prediction Vector', 'Softmax Operation', 'Baseline Network', 'Number Of Images']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Despite poor image quality, occlusions, and small training datasets, recent pedestrian attribute recognition (PAR) methods have achieved considerable performance. However, leveraging only spatial information of different attributes limits their reliability and generalizability. This paper introduces a multi-perspective approach to reduce over-dependence on spatial clues of a single perspective and exploits other aspects available in multiple perspectives. In order to tackle image quality and occlusions, we exploit different spatial clues present across images and handpick the best attribute-specific features to classify. Precisely, we extract the class-activation energy of each attribute and correlate it with the corresponding energy present across other images using the proposed Self-Attentive Cross Relation Module. In the next stage, we fuse this correlation information with similar clues accumulated from the other images. Lastly, we train a classification neural network using combined correlation information with two different losses. We have validated our method on four widely used PAR datasets, namely Market1501, PETA, PA-100k, and Duke. Our method achieves superior performance over most existing methods, demonstrating the effectiveness of a multi-perspective approach in PAR."
Letting 3D Guide the Way: 3D Guided 2D Few-Shot Image Classification,"Jiajing Chen, Minmin Yang, Senem Velipasalar","Syracuse University, Electrical Engineering and Computer Science Dept., Syracuse, NY, USA",100.0,USA,0.0,,"Existing few-shot image classification networks aim to perform prediction on images belonging to classes that were not seen during training, with only a few labeled images, which are randomly picked from the same image pool as the support set. However, this traditional approach has two main issues: (i) in real-world applications, since support images are randomly picked, the angle they were captured from can be very different from that of the query image, causing the images to look very different and making it hard to match them; (ii) since support and query images, for both training and testing, are sampled from the same image pool, models can overfit the dataset, especially if the image pool contains images with similar color, texture or view angle. Thus, good performance on a dataset does not reflect a model's real ability. To address these issues, we propose a novel few-shot learning approach referred to as the 3D guided 2D (3DG2D) few-shot image classification. In our proposed approach, the queries are 2D images, and the support set is composed of 3D mesh data, providing different views of an object, in contrast to randomly picked images providing a single view. From each 3D mesh, 14 projection images are generated from different angles. Thus, these projections have significant variance among themselves. To address this challenge, we also propose the Angle Inference Module (AIM), which is used to infer the view angle of a query image so that more attention is given to projection images corresponding to the same view angle as the query image to achieve better prediction performance. We perform experiments on ModelNet40, Toys4K and ShapeNet datasets with 4-fold cross validation, and show that our 3DG2D few-shot classification approach consistently outperforms the state-of-the-art baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Letting_3D_Guide_the_Way_3D_Guided_2D_Few-Shot_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Letting_3D_Guide_the_Way_3D_Guided_2D_Few-Shot_Image_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484292/,"['Training', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Image color analysis', 'Image classification', 'Testing']","['Image Classification', 'Few-shot Image Classification', '2D Images', 'Viewing Angle', 'Projection Images', '3D Mesh', 'Similar Color', 'Support Set', 'Objective View', 'Query Image', 'Few-shot Learning', 'Few-shot Classification', '4-fold Cross-validation', 'Convolutional Neural Network', 'Classification Task', 'Learning Ability', 'Image Object', 'Intersection Over Union', 'Class I', 'RGB Images', 'Query Features', '3D Data', 'Earth Mover’s Distance', '3D Projection', 'Support For Projects', 'Class Prototypes', 'Query Set', 'Intersection Over Union Value', 'Images In Set', 'Network Inference']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', '3D computer vision']",,"Existing few-shot image classification networks aim to perform prediction on images belonging to classes that were not seen during training, with only a few labeled images, which are randomly picked from the same image pool as the support set. However, this traditional approach has two main issues: (i) in real-world applications, since support images are randomly picked, the angle they were captured from can be very different from that of the query image, causing the images to look very different and making it hard to match them; (ii) since support and query images, for both training and testing, are sampled from the same image pool, models can overfit the dataset, especially if the image pool contains images with similar color, texture or view angle. Thus, good performance on a dataset does not reflect a model’s real ability. To address these issues, we propose a novel few-shot learning approach referred to as the 3D guided 2D (3DG2D) few-shot image classification. In our proposed approach, the queries are 2D images, and the support set is composed of 3D mesh data, providing different views of an object, in contrast to randomly picked images providing a single view. From each 3D mesh, 14 projection images are generated from different angles. Thus, these projections have significant variance among themselves. To address this challenge, we also propose the Angle Inference Module (AIM), which is used to infer the view angle of a query image so that more attention is given to projection images corresponding to the same view angle as the query image to achieve better prediction performance. We perform experiments on ModelNet40, Toys4K and ShapeNet datasets with 4-fold cross validation, and show that our 3DG2D few-shot classification approach consistently outperforms the state-of-the-art baselines."
"Leveraging Bitstream Metadata for Fast, Accurate, Generalized Compressed Video Quality Enhancement","Max Ehrlich, Jon Barker, Namitha Padmanabhan, Larry Davis, Andrew Tao, Bryan Catanzaro, Abhinav Shrivastava","University of Maryland, College Park; NVIDIA",50.0,USA,50.0,USA,"Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bitstream. This allows our single model to handle a variety of different compression quality settings which required an ensemble of models in prior work.",https://openaccess.thecvf.com/content/WACV2024/html/Ehrlich_Leveraging_Bitstream_Metadata_for_Fast_Accurate_Generalized_Compressed_Video_Quality_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ehrlich_Leveraging_Bitstream_Metadata_for_Fast_Accurate_Generalized_Compressed_Video_Quality_WACV_2024_paper.pdf,,,2202.00011,main,Poster,https://ieeexplore.ieee.org/document/10484011/,"['Quantization (signal)', 'Social networking (online)', 'Computational modeling', 'Rate-distortion', 'Video compression', 'Metadata', 'Throughput']","['Video Compression', 'Video Quality Enhancement', 'Throughput', 'Deep Learning', 'Compression Set', 'Loss Function', 'Time And Space', 'Convolutional Neural Network', 'Perception Of Quality', 'Single Frame', 'Over Space', 'Optical Flow', 'Quantization Parameter', 'Motion Estimation', 'Previous Frame', 'Regression Loss', 'Motion Vector', 'Compression Algorithm', 'Motion Compensation', 'Difference Of Gaussian', 'Target Frame', 'Group Of Pictures', 'Deformable Convolution', 'Restoration Tasks', 'Pixel Block', 'Scale Space', 'Reconstruction Accuracy', 'Benchmark']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Applications', 'Smartphones / end user devices']",,"Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many compression settings, quality loss is still noticeable. These settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput. Furthermore, we condition our model on quantization data which is readily available in the bit-stream. This allows our single model to handle a variety of different compression quality settings which required an ensemble of models in prior work."
Leveraging Next-Active Objects for Context-Aware Anticipation in Egocentric Videos,"Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue","Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT); Department of Computer Science, University of Verona, Italy; Department of Management, Information and Production Engineering, University of Bergamo, Dalmine, Italy; Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT); Department of Electrical, Electronics and Telecommunication Engineering and Naval Architecture (DITEN), University of Genoa, Italy",100.0,Italy,0.0,,"Objects are crucial for understanding human-object interactions. By identifying the relevant objects, one can also predict potential future interactions or actions that may occur with these objects. In this paper, we study the problem of Short-Term Object interaction anticipation (STA) and propose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a multi-modal end-to-end transformer network, that attends to objects in observed frames in order to anticipate the next-active-object (NAO) and, eventually, to guide the model to predict context-aware future actions. The task is challenging since it requires anticipating future action along with the object with which the action occurs and the time after which the interaction will begin, a.k.a. the time to contact (TTC). Compared to existing video modeling architectures for action anticipation, NAOGAT captures the relationship between objects and the global scene context in order to predict detections for the next active object and anticipate relevant future actions given these detections, leveraging the objects' dynamics to improve accuracy. One of the key strengths of our approach, in fact, is its ability to exploit the motion dynamics of objects within a given clip, which is often ignored by other models, and separately decoding the object-centric and motion-centric information. Through our experiments, we show that our model outperforms existing methods on two separate datasets, Ego4D and EpicKitchens-100 (""Unseen Set""), as measured by several additional metrics, such as time to contact, and next-active-object localization.",https://openaccess.thecvf.com/content/WACV2024/html/Thakur_Leveraging_Next-Active_Objects_for_Context-Aware_Anticipation_in_Egocentric_Videos_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Thakur_Leveraging_Next-Active_Objects_for_Context-Aware_Anticipation_in_Egocentric_Videos_WACV_2024_paper.pdf,sanketsans.github.io/wacv24,,2308.08303,main,Poster,https://ieeexplore.ieee.org/document/10484395/,"['Privacy', 'Tracking', 'Reviews', 'Dynamics', 'Predictive models', 'Transformers', 'Time measurement']","['Egocentric Videos', 'Future Actions', 'Global Context', 'Activity Prediction', 'Frame In Order', 'Interactive', 'Convolutional Neural Network', 'Temporal Dimension', 'Object Detection', 'Attention Mechanism', 'Class Labels', 'Intersection Over Union', 'Bounding Box', 'Video Clips', 'Causal Model', 'Action Recognition', 'Motion Information', 'L1 Loss', 'Encoder Output', 'Transformer Encoder', 'Frame Features', 'Transformer Decoder', 'Future Frames', 'Object Trajectory', 'RGB Features', 'Predicted Bounding Box', 'Average Precision']","['Applications', 'Virtual / augmented reality']",6,"Objects are crucial for understanding human-object interactions. By identifying the relevant objects, one can also predict potential future interactions or actions that may occur with these objects. In this paper, we study the problem of Short-Term Object interaction anticipation (STA) and propose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a multi-modal end-to-end transformer network, that attends to objects in observed frames in order to anticipate the next-active-object (NAO) and, eventually, to guide the model to predict context-aware future actions. The task is challenging since it requires anticipating future action along with the object with which the action occurs and the time after which the interaction will begin, a.k.a. the time to contact (TTC). Compared to existing video modeling architectures for action anticipation, NAOGAT captures the relationship between objects and the global scene context in order to predict detections for the next active object and anticipate relevant future actions given these detections, leveraging the objects’ dynamics to improve accuracy. One of the key strengths of our approach, in fact, is its ability to exploit the motion dynamics of objects within a given clip , which is often ignored by other models, and separately decoding the object-centric and motion-centric information. Through our experiments, we show that our model outperforms existing methods on two separate datasets, Ego4D and EpicKitchens-100 (""Unseen Set""), as measured by several additional metrics, such as time to contact, and next-active-object localization. The code can be found on project page : sanketsans.github.io/wacv24"
Leveraging Synthetic Data To Learn Video Stabilization Under Adverse Conditions,"Abdulrahman Kerim, Washington L. S. Ramos, Leandro Soriano Marcolino, Erickson R. Nascimento, Richard Jiang","Lancaster University, UK; University for the Creative Arts, UK; UFMG, Brazil; UFMG, Brazil; Microsoft; Lancaster University, UK",83.33333333333334,"Brazil, UK",16.666666666666657,USA,"Stabilization plays a central role in improving the quality of videos. However, current methods perform poorly under adverse conditions. In this paper, we propose a synthetic-aware adverse weather video stabilization algorithm that dispenses real data for training, relying solely on synthetic data. Our approach leverages specially generated synthetic data to avoid the feature extraction issues faced by current methods. To achieve this, we present a novel data generator to produce the required training data with an automatic ground-truth extraction procedure. We also propose a new dataset, VSAC105Real, and compare our method to five recent video stabilization algorithms using two benchmarks. Our method generalizes well on real-world videos across all weather conditions and does not require large-scale synthetic training data. Implementations for our proposed video stabilization algorithm, generator, and datasets are available at https://github.com/A-Kerim/SyntheticData4VideoStabilization_WACV_2024.",https://openaccess.thecvf.com/content/WACV2024/html/Kerim_Leveraging_Synthetic_Data_To_Learn_Video_Stabilization_Under_Adverse_Conditions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kerim_Leveraging_Synthetic_Data_To_Learn_Video_Stabilization_Under_Adverse_Conditions_WACV_2024_paper.pdf,,https://github.com/A-Kerim/SyntheticData4VideoStabilization,2208.12763,main,Poster,https://ieeexplore.ieee.org/document/10483841/,"['Training', 'Computer vision', 'Training data', 'Benchmark testing', 'Feature extraction', 'Generators', 'Data mining']","['Adverse Conditions', 'Video Stabilization', 'Training Data', 'Extreme Weather', 'Synthetic Training Data', 'Real Training Data', 'Standard Conditions', 'Unsupervised Methods', 'Virtual World', 'Semantic Segmentation', 'Optical Flow', 'Affine Transformation', 'Consecutive Frames', 'Depth Estimation', 'Motion Estimation', 'Savitzky-Golay Filter', 'Affinity Matrix', 'Smooth Trajectory', 'Adverse Weather Conditions', 'Synthetic Data Generation', '3D World', 'Rain And Snow', 'Camera Motion', 'Point In Frame', 'Homography Matrix', 'Challenging Conditions', 'Low Texture', 'Real Videos', 'Characteristics Of Resilience']","['Algorithms', 'Video recognition and understanding']",1,"Stabilization plays a central role in improving the quality of videos. However, current methods perform poorly under adverse conditions. In this paper, we propose a synthetic-aware adverse weather video stabilization algorithm that dispenses real data for training, relying solely on synthetic data. Our approach leverages specially generated synthetic data to avoid the feature extraction issues faced by current methods. To achieve this, we present a novel data generator to produce the required training data with an automatic ground-truth extraction procedure. We also propose a new dataset, VSAC105Real, and compare our method to five recent video stabilization algorithms using two benchmarks. Our method generalizes well on real-world videos across all weather conditions and does not require large-scale synthetic training data. Implementations for our proposed video stabilization algorithm, generator, and datasets are available at https://github.com/A-Kerim/SyntheticData4VideoStabilization_WACV_2024."
Leveraging Task-Specific Pre-Training To Reason Across Images and Videos,"Arka Sadhu, Ram Nevatia",University of Southern California,100.0,USA,0.0,,"We explore the Reasoning Across Images and Video (RAIV) task, which requires models to reason on a pair of visual inputs comprising various combinations of images and/or videos. Previous work in this area has been limited to image pairs focusing primarily on the existence and/or cardinality of objects. To address this, we leverage existing datasets with rich annotations to generate semantically meaningful queries about actions, objects, and their relationships. We introduce new datasets that encompass visually similar inputs, reasoning over images, across images and videos, or across videos. Recognizing the distinct nature of RAIV compared to existing pre-training objectives which work on single image-text pairs, we explore task-specific pre-training, wherein a pre-trained model is trained on an objective similar to downstream tasks without utilizing fine-tuning datasets. Experiments with several state-of-the-art pre-trained image-language models reveal that task-specific pre-training significantly enhances performance on downstream datasets, even in the absence of additional pre-training data. We provide further ablative studies to guide future work.",https://openaccess.thecvf.com/content/WACV2024/html/Sadhu_Leveraging_Task-Specific_Pre-Training_To_Reason_Across_Images_and_Videos_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sadhu_Leveraging_Task-Specific_Pre-Training_To_Reason_Across_Images_and_Videos_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483649/,"['Visualization', 'Computer vision', 'Image recognition', 'Annotations', 'Focusing', 'Cognition', 'Data models']","['Image Pairs', 'Visual Input', 'Visual Similarity', 'Random Sampling', 'Modeling Framework', 'Natural Language', 'Object Detection', 'Multiple Datasets', 'Multiple-choice Questions', 'Language Model', 'Common Objects', 'Self-supervised Learning', 'Binary Cross-entropy Loss', 'Target Dataset', 'Reasoning Tasks', 'Set Of Videos', 'Vision Transformer', 'Visual Question Answering', 'Semantic Role', 'Pre-training Dataset', 'Pre-training Tasks', 'Masked Language Model', 'Dataset Bias', 'Semantic Labels', 'Multiple-choice', 'Majority Voting']","['Algorithms', 'Vision + language and/or other modalities']",,"We explore the task of Reasoning Across Images and Video (RAIV), which requires models to reason on a pair of visual inputs comprising various combinations of images and/or videos. Previous work in this area has been limited to image pairs focusing primarily on the existence and/or cardinality of objects. To address this, we leverage existing datasets with rich annotations to generate semantically meaningful queries about actions, objects, and their relationships. We introduce new datasets that encompass visually similar inputs, reasoning over images, across images and videos, or across videos. Recognizing the distinct nature of RAIV compared to existing pre-training objectives which work on single image-text pairs, we explore task-specific pre-training, wherein a pre-trained model is trained on an objective similar to downstream tasks without utilizing fine-tuning datasets. Experiments with several state-of-the-art pre-trained image-language models reveal that task-specific pre-training significantly enhances performance on downstream datasets, even in the absence of additional pre-training data. We provide further ablative studies to guide future work."
Leveraging the Power of Data Augmentation for Transformer-Based Tracking,"Jie Zhao, Johan Edstedt, Michael Felsberg, Dong Wang, Huchuan Lu",Dalian University of Technology; Linköping University,100.0,"China, Sweden",0.0,,"Due to long-distance correlation and powerful pretrained models, transformer-based methods have initiated a breakthrough in visual object tracking performance. Previous works focus on designing effective architectures suited for tracking, but ignore that data augmentation is equally crucial for training a well-performing model. In this paper, we first explore the impact of general data augmentations on transformer-based trackers via systematic experiments, and reveal the limited effectiveness of these common strategies. Motivated by experimental observations, we then propose two data augmentation methods customized for tracking. First, we optimize existing random cropping via a dynamic search radius mechanism and simulation for boundary samples. Second, we propose a token-level feature mixing augmentation strategy, which enables the model against challenges like background interference. Extensive experiments on two transformer-based trackers and six benchmarks demonstrate the effectiveness and data efficiency of our methods, especially under challenging settings, like one-shot tracking and small image resolutions. Code is available at https://github.com/zj5559/DATr.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Leveraging_the_Power_of_Data_Augmentation_for_Transformer-Based_Tracking_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_Leveraging_the_Power_of_Data_Augmentation_for_Transformer-Based_Tracking_WACV_2024_paper.pdf,,https://github.com/zj5559/DATr,2309.08264,main,Poster,https://ieeexplore.ieee.org/document/10483807/,"['Training', 'Visualization', 'Target tracking', 'Systematics', 'Image resolution', 'Interference', 'Benchmark testing']","['Data Augmentation', 'Benchmark', 'Image Resolution', 'Mixed Strategy', 'Background Interference', 'Random Cropping', 'Small Resolution', 'Transformer-based Methods', 'Training Set', 'Convolutional Neural Network', 'Diverse Sample', 'Training Phase', 'Large-scale Datasets', 'Deep Convolutional Neural Network', 'Bounding Box', 'Failure Cases', 'Random Strategy', 'Transformer Model', 'Simulated Samples', 'Horizontal Flip', 'Search Region', 'Data Augmentation Approach', 'Dynamic Selection', 'Track Model', 'Unsolved Challenge', 'Convolutional Neural Networks Backbone', 'Random Step', 'Fast Motion', 'Training Subsets', 'Combination Of Transformation']","['Algorithms', 'Video recognition and understanding']",1,"Due to long-distance correlation and powerful pretrained models, transformer-based methods have initiated a breakthrough in visual object tracking performance. Previous works focus on designing effective architectures suited for tracking, but ignore that data augmentation is equally crucial for training a well-performing model. In this paper, we first explore the impact of general data augmentations on transformer-based trackers via systematic experiments, and reveal the limited effectiveness of these common strategies. Motivated by experimental observations, we then propose two data augmentation methods customized for tracking. First, we optimize existing random cropping via a dynamic search radius mechanism and simulation for boundary samples. Second, we propose a token-level feature mixing augmentation strategy, which enables the model against challenges like background interference. Extensive experiments on two transformer-based trackers and six benchmarks demonstrate the effectiveness and data efficiency of our methods, especially under challenging settings, like one-shot tracking and small image resolutions. Code is available at https://github.com/zj5559/DATr."
LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis,"Di Chang, Yufeng Yin, Zongjian Li, Minh Tran, Mohammad Soleymani","Institute for Creative Technologies, University of Southern California",100.0,USA,0.0,,"Facial expression analysis is an important tool for human-computer interaction. In this paper, we introduce LibreFace, an open-source toolkit for facial expression analysis. This open-source toolbox offers real-time and offline analysis of facial behavior through deep learning models, including facial action unit (AU) detection, AU intensity estimation, and facial expression recognition. To accomplish this, we employ several techniques, including the utilization of a large-scale pre-trained network, feature-wise knowledge distillation, and task-specific fine-tuning. These approaches are designed to effectively and accurately analyze facial expressions by leveraging visual information, thereby facilitating the implementation of real-time interactive applications. In terms of Action Unit (AU) intensity estimation, we achieve a Pearson Correlation Coefficient (PCC) of 0.63 on DISFA, which is 7% higher than the performance of OpenFace 2.0 while maintaining highly-efficient inference that runs two times faster than OpenFace 2.0. Despite being compact, our model also demonstrates competitive performance to state-of-the-art facial expression analysis methods on AffecNet, FFHQ, and RAF-DB.",https://openaccess.thecvf.com/content/WACV2024/html/Chang_LibreFace_An_Open-Source_Toolkit_for_Deep_Facial_Expression_Analysis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chang_LibreFace_An_Open-Source_Toolkit_for_Deep_Facial_Expression_Analysis_WACV_2024_paper.pdf,,https://github.com/ihp-lab/LibreFace,2308.10713,main,Poster,https://ieeexplore.ieee.org/document/10484436/,"['Knowledge engineering', 'Human computer interaction', 'Deep learning', 'Gold', 'Analytical models', 'Visualization', 'Face recognition']","['Facial Expressions', 'Pearson Correlation', 'Human-computer Interaction', 'Face Recognition', 'Volume Estimation', 'Facial Expression Recognition', 'Action Units', 'Facial Action Units', 'Convolutional Neural Network', 'Support Vector Machine', 'Deep Neural Network', 'Recurrent Neural Network', 'Teacher Model', 'Generative Adversarial Networks', 'Facial Features', 'Face Images', 'Linear Classifier', 'Deep Learning-based Methods', 'Student Model', 'Image Alignment', 'Landmark Detection', 'Facial Action Coding System', 'Vision Transformer', 'Face Dataset', 'Traditional Machine Learning Methods', 'Affective Computing', 'Facial Muscle Movements', 'Pre-trained Encoder', 'Facial Muscles', 'Histogram Of Gradients']","['Applications', 'Psychology and cognitive science']",3,"Facial expression analysis is an important tool for human-computer interaction. In this paper, we introduce LibreFace, an open-source toolkit for facial expression analysis. This open-source toolbox offers real-time and offline analysis of facial behavior through deep learning models, including facial action unit (AU) detection, AU intensity estimation, and facial expression recognition. To accomplish this, we employ several techniques, including the utilization of a large-scale pre-trained network, feature-wise knowledge distillation, and task-specific fine-tuning. These approaches are designed to effectively and accurately analyze facial expressions by leveraging visual information, thereby facilitating the implementation of real-time interactive applications. In terms of Action Unit (AU) intensity estimation, we achieve a Pearson Correlation Coefficient (PCC) of 0.63 on DISFA, which is 7% higher than the performance of OpenFace 2.0 [4] while maintaining highly-efficient inference that runs two times faster than OpenFace 2.0 [4]. Despite being compact, our model also demonstrates competitive performance to state-of-the-art facial expression analysis methods on AffecNet, FFHQ, and RAF-DB. Our code will be released at https://github.com/ihp-lab/LibreFace"
LidarCLIP or: How I Learned To Talk to Point Clouds,"Georg Hess, Adam Tonderski, Christoffer Petersson, Kalle Åström, Lennart Svensson","Zenseact, Chalmers University of Technology; Lund University; Chalmers University of Technology; Zenseact, Lund University",100.0,Sweden,0.0,,"Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL*E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that LidarCLIP outperforms existing attempts to use CLIP for point clouds by a large margin. Finally, we leverage our compatibility with CLIP to explore a range of applications, such as point cloud captioning and lidar-to-image generation, without any additional training. Code and pre-trained models at https://github.com/atonderski/lidarclip.",https://openaccess.thecvf.com/content/WACV2024/html/Hess_LidarCLIP_or_How_I_Learned_To_Talk_to_Point_Clouds_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hess_LidarCLIP_or_How_I_Learned_To_Talk_to_Point_Clouds_WACV_2024_paper.pdf,,github.com/atonderski/lidarclip,2212.06858,main,Poster,https://ieeexplore.ieee.org/document/10484207/,"['Point cloud compression', 'Training', 'Visualization', 'Computer vision', 'Laser radar', 'Semantic segmentation', 'Object detection']","['Point Cloud', 'Image Features', 'Visual Modality', 'Image Domain', 'Lidar Data', 'Mean Square Error', 'Time Of Day', 'Natural Language', 'Object Detection', 'Bounding Box', 'Latent Space', 'Semantic Segmentation', 'Self-supervised Learning', 'Training Pairs', 'LiDAR Sensor', 'LiDAR Point Clouds', 'Image Encoder', 'Fréchet Inception Distance', 'Image Embedding', 'Text Encoder', 'Large Trucks']","['Applications', 'Autonomous Driving', 'Algorithms', 'Vision + language and/or other modalities']",7,"Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL•E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of Lidar-CLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that LidarCLIP outperforms existing attempts to use CLIP for point clouds by a large margin. Finally, we leverage our compatibility with CLIP to explore a range of applications, such as point cloud captioning and lidar-to-image generation, without any additional training. Code and pre-trained models at github.com/atonderski/lidarclip."
Lightweight Delivery Detection on Doorbell Cameras,"Pirazh Khorramshahi, Zhe Wu, Tianchen Wang, Luke DeLuccia, Hongcheng Wang",Comcast Applied AI Research; Qualcomm Technologies; Amazon,33.33333333333333,USA,66.66666666666667,USA,"Despite recent advances in video-based action recognition and robust spatio-temporal modeling, most of the proposed approaches rely on the abundance of computational resources to afford running huge and computation-intensive convolutional or transformer-based neural networks to obtain satisfactory results. This limits the deployment of such models on edge devices with limited power and computing resources. In this work we investigate an important smart home application, video based delivery detection, and present a simple and lightweight pipeline for this task that can run on resource-constrained doorbell cameras. Our proposed pipeline relies on motion cues to generate a set of coarse activity proposals followed by their classification with a mobile-friendly 3DCNN network. For training we design a novel semi-supervised attention module that helps the network to learn robust spatio-temporal features and adopt an evidence-based optimization objective that allows for quantifying the uncertainty of predictions made by the network. Experimental results on our curated delivery dataset shows the significant effectiveness of our pipeline compared to alternatives and highlights the benefits of our training phase novelties to achieve free and considerable inference-time performance gains.",https://openaccess.thecvf.com/content/WACV2024/html/Khorramshahi_Lightweight_Delivery_Detection_on_Doorbell_Cameras_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Khorramshahi_Lightweight_Delivery_Detection_on_Doorbell_Cameras_WACV_2024_paper.pdf,,,2305.07812,main,Poster,https://ieeexplore.ieee.org/document/10483889/,"['Training', 'Uncertainty', 'Computational modeling', 'System performance', 'Pipelines', 'Predictive models', 'Cameras']","['Limited Power', 'Action Recognition', 'Smart Home', 'Edge Devices', 'Prediction Accuracy', 'Transformer', 'Validation Set', 'F1 Score', 'Feature Maps', 'Motion Detection', 'Precision-recall Curve', 'Motion Tracking', 'Floating-point Operations', 'Score Map', 'Surveillance Cameras', 'Evidence Theory', 'Logic Of Theory', 'Front Door', 'Dirichlet Distribution', 'Human Activity Recognition', 'Motion Events', 'Video Action Recognition', 'Uncertain Predictions', 'Security Cameras', 'Backbone Architecture', 'Intersection Over Union', 'Temporal Interactions', 'False Positive Rate', 'Temporal Information']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Commercial / retail']",,"Despite recent advances in video-based action recognition and robust spatio-temporal modeling, most of the proposed approaches rely on the abundance of computational resources to afford running huge and computation-intensive convolutional or transformer-based neural networks to obtain satisfactory results. This limits the deployment of such models on edge devices with limited power and computing resources. In this work we investigate an important smart home application, video based delivery detection, and present a simple and lightweight pipeline for this task that can run on resource-constrained doorbell cameras. Our method relies on motion cues to generate a set of coarse activity proposals followed by their classification with a mobile-friendly 3DCNN network. To train we design a novel semi-supervised attention module that helps the network to learn robust spatio-temporal features and adopt an evidence-based optimization objective that allows for quantifying the uncertainty of predictions made by the network. Experimental results on our curated delivery dataset shows the significant effectiveness of our pipeline and highlights the benefits of our training phase novelties to achieve free and considerable inference-time performance gains."
Lightweight Portrait Matting via Regional Attention and Refinement,"Yatao Zhong, Ilya Zharkov",Microsoft,0.0,,100.0,USA,"We present a lightweight model for high resolution portrait matting. The model does not use any auxiliary inputs such as trimaps or background captures and achieves real time performance for HD videos and near real time for 4K. Our model is built upon a two-stage framework with a low resolution network for coarse alpha estimation followed by a refinement network for local region improvement. However, a naive implementation of the two-stage model suffers from poor matting quality if not utilizing any auxiliary inputs. We address the performance gap by leveraging the vision transformer (ViT) as the backbone of the low resolution network, motivated by the observation that the tokenization step of ViT can reduce spatial resolution while retain as much pixel information as possible. To inform local regions of the context, we propose a novel cross region attention (CRA) module in the refinement network to propagate the contextual information across the neighboring regions. We demonstrate that our method achieves superior results and outperforms other baselines on three benchmark datasets while only uses 1/20 of the FLOPS compared to the existing state-of-the-art model.",https://openaccess.thecvf.com/content/WACV2024/html/Zhong_Lightweight_Portrait_Matting_via_Regional_Attention_and_Refinement_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhong_Lightweight_Portrait_Matting_via_Regional_Attention_and_Refinement_WACV_2024_paper.pdf,,,2311.03770,main,Poster,https://ieeexplore.ieee.org/document/10484092/,"['Heart', 'Computer vision', 'Estimation', 'Benchmark testing', 'Transformers', 'Real-time systems', 'Tokenization']","['Portrait Matting', 'High-resolution', 'Low Resolution', 'Contextual Information', 'Attention Module', 'Neighboring Regions', 'Pixel Information', 'High-definition Video', 'Two-stage Framework', 'Vision Transformer', 'Refinement Network', 'Training Time', 'Positive Bias', 'Input Image', 'K-nearest Neighbor', 'Attention Mechanism', 'Lookup Table', 'Amount Of Computation', 'Inference Time', 'Alpha Matte', 'Sum Of Absolute Differences', 'Reduction In Resolution', 'Search Range', 'Large Kernel', 'Low-resolution Model', 'Maximum A Posteriori', 'Default Model']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Image recognition and understanding']",,"We present a lightweight model for high resolution portrait matting. The model does not use any auxiliary inputs such as trimaps or background captures and achieves real time performance for HD videos and near real time for 4K. Our model is built upon a two-stage framework with a low resolution network for coarse alpha estimation followed by a refinement network for local region improvement. However, a naive implementation of the two-stage model suffers from poor matting quality if not utilizing any auxiliary inputs. We address the performance gap by leveraging the vision transformer (ViT) as the backbone of the low resolution network, motivated by the observation that the tokenization step of ViT can reduce spatial resolution while retain as much pixel information as possible. To inform local regions of the context, we propose a novel cross region attention (CRA) module in the refinement network to propagate the contextual information across the neighboring regions. We demonstrate that our method achieves superior results and outperforms other baselines on three benchmark datasets while only uses 1/20 of the FLOPS compared to the existing state-of-the-art model."
Lightweight Thermal Super-Resolution and Object Detection for Robust Perception in Adverse Weather Conditions,"Pranjay Shyam, HyunJin Yoo","Faurecia IRYStec Inc., Montreal, Canada",0.0,,100.0,Canada,"In this work, we examine the potential application of thermal cameras in improving perception capabilities in adverse weather conditions like snow, night-time driving, and haze, focusing on retaining the performance of Advanced Driver Assistance Systems (ADAS), thus enhancing its functionality and safety characteristics. While thermal sensors offer the advantage of robust information capture in adverse weather conditions, their integration is plagued with issues surrounding poor feature capture in normal conditions, low imaging resolution, and high sensor costs. We address the former by formulating the problem definition as information switching wherein thermal images are selected when visible images are degraded. Furthermore, we consider a single object detector for RGB and thermal images to ensure low latency. We propose utilizing a learnable projection function that translates the thermal image into RGB color space, thus providing minimal modifications to the underlying object detector. We address the issues of low imaging resolution and cost by proposing a novel procedure that combines super-resolution and object detection, enabling the utilization of low-resolution and low-cost uncooled thermal imaging sensors. To ensure the complete pipeline meets the actual deployment requirements of real-time inference on resource-constrained devices, we introduce a lightweight super-resolution algorithm, implementing optimizations within the network structure followed by global pruning. In addition, to improve the feature representations extracted by lightweight encoders, we propose a bidirectional feature pyramid network to enhance the feature representation. We demonstrate the efficacy of the proposed mechanism through extensive simulated evaluations on automotive datasets such as FLIR, KAIST, DENSE, and Freiburg Thermal.",https://openaccess.thecvf.com/content/WACV2024/html/Shyam_Lightweight_Thermal_Super-Resolution_and_Object_Detection_for_Robust_Perception_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shyam_Lightweight_Thermal_Super-Resolution_and_Object_Detection_for_Robust_Perception_in_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483657/,"['Image sensors', 'Superresolution', 'Lighting', 'Object detection', 'Detectors', 'Thermal sensors', 'Sensor phenomena and characterization']","['Extreme Weather', 'Object Detection', 'Adverse Weather Conditions', 'Lightweight Object', 'Robust Perception', 'Lightweight Object Detection', 'Infrared Imaging', 'Adverse Conditions', 'RGB Images', 'Low-resolution Images', 'Feature Pyramid Network', 'Bidirectional Network', 'Thermal Sensors', 'Advanced Driver Assistance Systems', 'RGB Color Space', 'Resource-constrained Devices', 'Super-resolution Algorithms', 'Quantitative Data', 'Convolution', 'Transformer', 'Real-time Object Detection', 'Super-resolution Network', 'Learning Rate Of 1e', 'RGB Space', 'Residual Block', 'Thermal Imagery', 'Multi-scale Features', 'Computational Efficiency', 'Image Quality', 'Performance Of Detection Algorithms']","['Applications', 'Autonomous Driving']",1,"In this work, we examine the potential application of thermal cameras in improving perception capabilities in adverse weather conditions like snow, night-time driving, and haze, focusing on retaining the performance of Advanced Driver Assistance Systems (ADAS), thus enhancing its functionality and safety characteristics. While thermal sensors offer the advantage of robust information capture in adverse weather conditions, their integration is plagued with issues surrounding poor feature capture in normal conditions, low imaging resolution, and high sensor costs. We address the former by formulating the problem definition as information switching wherein thermal images are selected when visible images are degraded. Furthermore, we consider a single object detector for RGB and thermal images to ensure low latency. We propose utilizing a learnable projection function that translates the thermal image into RGB color space, thus providing minimal modifications to the underlying object detector. We address the issues of low imaging resolution and cost by proposing a novel procedure that combines super-resolution and object detection, enabling the utilization of low-resolution and low-cost uncooled thermal imaging sensors. To ensure the complete pipeline meets the actual deployment requirements of real-time inference on resource-constrained devices, we introduce a lightweight super-resolution algorithm, implementing optimizations within the network structure followed by global pruning. In addition, to improve the feature representations extracted by lightweight encoders, we propose a bidirectional feature pyramid network to enhance the feature representation. We demonstrate the efficacy of the proposed mechanism through extensive simulated evaluations on automotive datasets such as FLIR, KAIST, DENSE, and Freiburg Thermal."
"Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders","Srijan Das, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, Michael S. Ryoo",UNC Charlotte; BITS Pilani Hyderabad; Indian Institute of Information Technology Guwahati; Stony Brook University; Delhi Technological University,100.0,"India, USA",0.0,,"Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViT pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability.",https://openaccess.thecvf.com/content/WACV2024/html/Das_Limited_Data_Unlimited_Potential_A_Study_on_ViTs_Augmented_by_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Das_Limited_Data_Unlimited_Potential_A_Study_on_ViTs_Augmented_by_WACV_2024_paper.pdf,,https://github.com/dominickrei/Limited-data-vits,2310.20704,main,Poster,https://ieeexplore.ieee.org/document/10484258/,"['Training', 'Computer vision', 'Deepfakes', 'Codes', 'Training data', 'Self-supervised learning', 'Transformers']","['Computer Vision', 'Primary Task', 'Joint Optimization', 'Self-supervised Learning', 'Auxiliary Task', 'Inductive Bias', 'Vision Transformer', 'Self-supervised Task', 'Convolutional Neural Network', 'Classification Accuracy', 'Image Classification', 'Small Datasets', 'Image Reconstruction', 'ImageNet', 'Training Epochs', 'Latent Representation', 'Attention Weights', 'Joint Training', 'Image X', 'Medical Datasets', 'CIFAR-100 Dataset', 'Self-supervised Learning Methods', 'Transformer Encoder', 'Transformer Block', 'Small-scale Datasets', 'Transformer Layers', 'Small Size Of The Dataset', 'Magnitude Of Eigenvalues', 'Pretext Task', 'Classification Task']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",8,"Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability. Our code is available at https://github.com/dominickrei/Limited-data-vits."
Link Prediction for Flow-Driven Spatial Networks,"Bastian Wittmann, Johannes C. Paetzold, Chinmay Prabhakar, Daniel Rueckert, Bjoern Menze",University of Zurich; Technical University of Munich,100.0,"Germany, Switzerland",0.0,,"Link prediction algorithms aim to infer the existence of connections (or links) between nodes in network-structured data and are typically applied to refine the connectivity among nodes. In this work, we focus on link prediction for flow-driven spatial networks, which are embedded in a Euclidean space and relate to physical exchange and transportation processes (e.g., blood flow in vessels or traffic flow in road networks). To this end, we propose the Graph Attentive Vectors (GAV) link prediction framework. GAV models simplified dynamics of physical flow in spatial networks via an attentive, neighborhood-aware message-passing paradigm, updating vector embeddings in a constrained manner. We evaluate GAV on eight flow-driven spatial networks given by whole-brain vessel graphs and road networks. GAV demonstrates superior performances across all datasets and metrics and outperformed the state-of-the-art on the ogbl-vessel benchmark at the time of submission by 12% (98.38 vs. 87.98 AUC). All code is publicly available on GitHub.",https://openaccess.thecvf.com/content/WACV2024/html/Wittmann_Link_Prediction_for_Flow-Driven_Spatial_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wittmann_Link_Prediction_for_Flow-Driven_Spatial_Networks_WACV_2024_paper.pdf,,https://github.com/bwittmann/GAV,2303.14501,main,Poster,https://ieeexplore.ieee.org/document/10483830/,"['Measurement', 'Computer vision', 'Codes', 'Roads', 'Transportation', 'Benchmark testing', 'Prediction algorithms']","['Spatial Network', 'Link Prediction', 'Road Network', 'Euclidean Space', 'Traffic Flow', 'Flow Dynamics', 'Network Flow', 'Embedding Vectors', 'Blood Flow In Vessels', 'Physical Flow', 'Direction Of Change', 'Validation Set', 'Graphical Representation', 'Binary Classification', 'Flow Direction', 'Central Node', 'Heuristic Algorithm', 'Local Neighborhood', 'Positive Link', 'Existence Probability', 'Negative Link', 'Magnitude Of Flow', 'Target Node', 'Intermediate Representation', 'Line Graph', 'Common Neighbors', 'Node Representations', 'Incoming Messages', 'Distinct Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",3,"Link prediction algorithms aim to infer the existence of connections (or links) between nodes in network-structured data and are typically applied to refine the connectivity among nodes. In this work, we focus on link prediction for flow-driven spatial networks, which are embedded in a Euclidean space and relate to physical exchange and transportation processes (e.g., blood flow in vessels or traffic flow in road networks). To this end, we propose the Graph Attentive Vectors (GAV) link prediction framework. GAV models simplified dynamics of physical flow in spatial networks via an attentive, neighborhood-aware message-passing paradigm, updating vector embeddings in a constrained manner. We evaluate GAV on eight flow-driven spatial networks given by whole-brain vessel graphs and road networks. GAV demonstrates superior performances across all datasets and metrics and outperformed the state-of-the-art on the ogbl-vessel benchmark at the time of submission by 12% (98.38 vs. 87.98 AUC). All code is publicly available on GitHub.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Linking Convolutional Kernel Size to Generalization Bias in Face Analysis CNNs,"Hao Liang, Josue Ortega Caro, Vikram Maheshri, Ankit B. Patel, Guha Balakrishnan","Yale University, New Haven, CO, USA; Rice University, Houston, TX, USA; Houston University, Houston, TX, USA",100.0,USA,0.0,,"Training dataset biases are by far the most scrutinized factors when explaining algorithmic biases of neural networks. In contrast, hyperparameters related to the neural network architecture have largely been ignored even though different network parameterizations are known to induce different implicit biases over learned features. For example, convolutional kernel size is known to affect the frequency content of features learned in CNNs. In this work, we present a causal framework for linking an architectural hyperparameter to out-of-distribution algorithmic bias. Our framework is experimental, in that we train several versions of a network with an intervention to a specific hyperparameter, and measure the resulting causal effect of this choice on performance bias when a particular out-of-distribution image perturbation is applied. In our experiments, we focused on measuring the causal relationship between convolutional kernel size and face analysis classification bias across different subpopulations (race/gender), with respect to high-frequency image details. We show that modifying kernel size, even in one layer of a CNN, changes the frequency content of learned features significantly across data subgroups leading to biased generalization performance even in the presence of a balanced dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Liang_Linking_Convolutional_Kernel_Size_to_Generalization_Bias_in_Face_Analysis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liang_Linking_Convolutional_Kernel_Size_to_Generalization_Bias_in_Face_Analysis_WACV_2024_paper.pdf,,,2302.03750,main,Poster,https://ieeexplore.ieee.org/document/10483800/,"['Training', 'Computer vision', 'Perturbation methods', 'Neural networks', 'Computer architecture', 'Size measurement', 'Convolutional neural networks']","['Convolutional Neural Network', 'Convolution Kernel', 'Convolution Kernel Size', 'Neural Network', 'Training Dataset', 'Hyperparameters', 'Feature Learning', 'Performance Bias', 'Implicit Bias', 'Frequency Content', 'Content Features', 'Discrimination Data', 'Algorithmic Bias', 'Deep Network', 'Deep Neural Network', 'Computer Vision', 'Frequency Domain', 'Dummy Variables', 'Racial Groups', 'Face Recognition', 'Adversarial Attacks', 'Protective Properties', 'Test Set Of Images', 'Energy Injection', 'Effect Of Architecture', 'Frequency Bias', 'Choice Architecture', 'Fourier Domain', 'Causal Analysis', 'Discrete Fourier Transform']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Applications', 'Social good']",,"Training dataset biases are by far the most scrutinized factors when explaining algorithmic biases of neural networks. In contrast, hyperparameters related to the neural network architecture have largely been ignored even though different network parameterizations are known to induce different implicit biases over learned features. For example, convolutional kernel size is known to affect the frequency content of features learned in CNNs. In this work, we present a causal framework for linking an architectural hyperparameter to out-of-distribution algorithmic bias. Our framework is experimental, in that we train several versions of a network with an intervention to a specific hyperparameter, and measure the resulting causal effect of this choice on performance bias when a particular out-of-distribution image perturbation is applied. In our experiments, we focused on measuring the causal relationship between convolutional kernel size and face analysis classification bias across different subpopulations (race/gender), with respect to high-frequency image details. We show that modifying kernel size, even in one layer of a CNN, changes the frequency content of learned features significantly across data subgroups leading to biased generalization performance even in the presence of a balanced dataset."
LipAT: Beyond Style Transfer for Controllable Neural Simulation of Lipstick Using Cosmetic Attributes,"Amila Silva, Olga Moskvyak, Alexander Long, Ravi Garg, Stephen Gould, Gil Avraham, Anton van den Hengel",Australian National University; The University of Melbourne; The University of Adelaide; Amazon,75.0,Australia,25.0,USA,"Lipstick virtual try-on (VTO) experiences have become widespread across the e-commerce sector and assist users in eliminating the guesswork of shopping online. However, such experiences still lack in both realism and accuracy. In this work, we propose LipAT, a neural framework that blends the strengths of Physics-Based Rendering (PBR) and Neural Style Transfer (NST) approaches to directly apply lipstick onto face images given lipstick attributes (e.g., colour, finish type). LipAT consists of a physics aware neural lipstick application module (LAM) to apply lipstick on face images given its attributes and Lipstick Refiner Module (LRM) to improve the realism by refining the imperfections. Unlike the NST approaches, LipAT allows precise and controllable lipstick attribute preservation, without requiring crude approximations and inference of various intertwined environment factors (e.g., scene lighting, face structure etc) involved in image generation that is required for accurate PBR. We propose an experimental framework with quantitative metrics to evaluate different desirable aspects of the lipstick attribute driven try-on alongside user studies to further validate our findings. Our results show that LipAT considerably outperforms fully-automated PBR approaches in preserving realism and the NST approaches in preserving various lipstick attributes such as finish types.",https://openaccess.thecvf.com/content/WACV2024/html/Silva_LipAT_Beyond_Style_Transfer_for_Controllable_Neural_Simulation_of_Lipstick_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Silva_LipAT_Beyond_Style_Transfer_for_Controllable_Neural_Simulation_of_Lipstick_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483757/,"['Measurement', 'Image synthesis', 'Scalability', 'Refining', 'Lighting', 'Controllability', 'Rendering (computer graphics)']","['Style Transfer', 'Lipstick', 'User Study', 'Image Generation', 'Face Images', 'Quantitative Metrics', 'Neural Modulation', 'Transfer Approach', 'Neural Framework', 'Refiner', 'Feature Maps', 'Quantitative Evaluation', 'Image Regions', 'Bounding Box', 'Hybrid Approach', 'Target Image', 'Supplementary Materials For Details', '3D Mesh', 'Manhattan Distance', 'Image Synthesis', 'Fréchet Inception Distance', 'Structural Similarity Index Measure', 'Material For More Details', 'Lip Region', 'Specular Component', 'Deep Generative Models', 'Gamma Correction', 'Light Profiles', 'Person Image', 'Image Filtering']","['Applications', 'Commercial / retail', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Lipstick virtual try-on (VTO) experiences have become widespread across the e-commerce sector and assist users in eliminating the guesswork of shopping online. However, such experiences still lack in both realism and accuracy. In this work, we propose LipAT, a neural framework that blends the strengths of Physics-Based Rendering (PBR) and Neural Style Transfer (NST) approaches to directly apply lipstick onto face images given lipstick attributes (e.g., colour, finish type). LipAT consists of a physics aware neural lipstick application module (LAM) to apply lipstick on face images given its attributes and Lipstick Refiner Module (LRM) to improve the realism by refining the imperfections. Unlike the NST approaches, LipAT allows precise and controllable lipstick attribute preservation, without requiring crude approximations and inference of various intertwined environment factors (e.g., scene lighting, face structure etc) involved in image generation that is required for accurate PBR. We propose an experimental framework with quantitative metrics to evaluate different desirable aspects of the lipstick attribute driven try-on alongside user studies to further validate our findings. Our results show that LipAT considerably outperforms fully-automated PBR approaches in preserving realism and the NST approaches in preserving various lipstick attributes such as finish types."
Localization and Manipulation of Immoral Visual Cues for Safe Text-to-Image Generation,"Seongbeom Park, Suhong Moon, Seunghyun Park, Jinkyu Kim","CSE, Korea University; EECS, UC Berkeley; NAVER Cloud AI",66.66666666666666,"South Korea, USA",33.33333333333334,South Korea,"Current text-to-image generation methods produce high-resolution and high-quality images, but they should not produce immoral images that may contain inappropriate content from the perspective of commonsense morality. Conventional approaches, however, often neglect these ethical concerns, and existing solutions are often limited to ensure moral compatibility. To address this, we propose a novel method that has three main capabilities: (1) our model recognizes the degree of visual commonsense immorality of a given generated image, (2) our model localizes immoral visual (and textual) attributes that make the image visually immoral, and (3) our model manipulates such immoral visual cues into a morally-qualifying alternative. We conduct experiments with various text-to-image generation models, including the state-of-the-art Stable Diffusion model, demonstrating the efficacy of our ethical image manipulation approach. Our human study further confirms that ours is indeed able to generate morally-satisfying images from immoral ones.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Localization_and_Manipulation_of_Immoral_Visual_Cues_for_Safe_Text-to-Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Localization_and_Manipulation_of_Immoral_Visual_Cues_for_Safe_Text-to-Image_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484413,"['Location awareness', 'Ethics', 'Visualization', 'Computer vision', 'Analytical models', 'Image recognition', 'Computational modeling']","['Human Studies', 'High-resolution Images', 'Use Of Imaging', 'Diffusion Model', 'Ethical Perspective', 'Inappropriate Content', 'Semantic', 'Supplemental Material', 'Input Image', 'Image Regions', 'Large-scale Datasets', 'Generative Adversarial Networks', 'Image Generation', 'Happy Faces', 'Visual Properties', 'Variational Autoencoder', 'Loss Of Identity', 'Improve Image Quality', 'Input Text', 'Image Captioning', 'Image Inpainting', 'Moral Content', 'MS COCO Dataset', '2nd Column', 'Alternative Words', 'Blur Kernel', 'Images Of Children', 'Textural Properties', 'Text Encoder']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Vision + language and/or other modalities']",1,"Current text-to-image generation methods produce high-resolution and high-quality images, but they should not produce immoral images that may contain inappropriate content from the perspective of commonsense morality. Conventional approaches, however, often neglect these ethical concerns, and existing solutions are often limited to ensure moral compatibility. To address this, we propose a novel method that has three main capabilities: (1) our model recognizes the degree of visual commonsense immorality of a given generated image, (2) our model localizes immoral visual (and textual) attributes that make the image visually immoral, and (3) our model manipulates such immoral visual cues into a morally-qualifying alternative. We conduct experiments with various text-to-image generation models, including the state-of-the-art Stable Diffusion model, demonstrating the efficacy of our ethical image manipulation approach. Our human study further confirms that ours is indeed able to generate morally-satisfying images from immoral ones."
Location-Aware Self-Supervised Transformers for Semantic Segmentation,"Mathilde Caron, Neil Houlsby, Cordelia Schmid",Google Research,0.0,,100.0,USA,"Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-text alignment a la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain networks with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme to mine dense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangement. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Caron_Location-Aware_Self-Supervised_Transformers_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Caron_Location-Aware_Self-Supervised_Transformers_for_Semantic_Segmentation_WACV_2024_paper.pdf,,,2212.02400,main,Poster,https://ieeexplore.ieee.org/document/10483669/,"['Location awareness', 'Visualization', 'Computer vision', 'Semantic segmentation', 'Neural networks', 'Estimation', 'Self-supervised learning']","['Semantic Segmentation', 'Fine-tuned', 'Image Classification', 'Visuospatial', 'Prediction Task', 'Object Parts', 'Self-supervised Learning', 'Global Level', 'Object Detection', 'Data Augmentation', 'Multilayer Perceptron', 'Task Difficulty', 'Cluster Assignment', 'Depth Estimation', 'Transfer Performance', 'Image Statistics', 'Global Objective', 'Box Regression', 'Jigsaw Puzzle', 'Semantic Understanding', 'Vision Transformer', 'Patch Level', 'Spatial Understanding', 'Patch Position', 'Color Jittering', 'Patch Features', 'Pretext Task', 'Representation Learning', 'Patch Pairs', 'Validation Accuracy']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",2,"Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-text alignment à la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain networks with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme to mine dense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangement. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets."
Longformer: Longitudinal Transformer for Alzheimer's Disease Classification With Structural MRIs,"Qiuhui Chen, Qiang Fu, Hao Bai, Yi Hong","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China",100.0,China,0.0,,"Structural magnetic resonance imaging (sMRI), especially longitudinal sMRI, is often used to monitor and capture disease progression during the clinical diagnosis of Alzheimer's Disease (AD). However, current methods neglect AD's progressive nature and have mostly relied on a single image for recognizing AD. In this paper, we consider the problem of leveraging the longitudinal MRIs of a subject for AD classification. To address the challenges of missing data, data demand, and subtle changes over time in learning longitudinal 3D MRIs, we propose a novel model LongFormer, which is a hybrid 3D CNN and transformer design to learn from image and longitudinal flow pairs. Our model can fully leverage all images in a dataset and effectively fuse spatiotemporal features for classification. We evaluate our model on three datasets, i.e., ADNI, OASIS, and AIBL, and compare it to eight baseline algorithms. Our proposed LongFormer achieves state-of-the-art performance in classifying AD and NC subjects from all these three public datasets. Our source code is available online.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Longformer_Longitudinal_Transformer_for_Alzheimers_Disease_Classification_With_Structural_MRIs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Longformer_Longitudinal_Transformer_for_Alzheimers_Disease_Classification_With_Structural_MRIs_WACV_2024_paper.pdf,,https://github.com/Qybc/LongFormer,,main,Poster,https://ieeexplore.ieee.org/document/10483639/,"['Solid modeling', 'Three-dimensional displays', 'Image recognition', 'Fuses', 'Magnetic resonance imaging', 'Source coding', 'Transformers']","['Magnetic Resonance Imaging', 'Alzheimer’s Disease', 'Subtle Changes', 'Single Image', 'Image Dataset', 'Public Datasets', 'Image Pairs', 'Alzheimer’s Disease Neuroimaging Initiative', 'Alzheimer’s Disease Subjects', 'Normal Control Subjects', '3D Magnetic Resonance Imaging', 'Alzheimer’s Disease Classification', 'Longitudinal Flow', 'Image Features', 'Mild Cognitive Impairment', 'Magnetic Resonance Imaging Scans', 'Longitudinal Changes', 'Optical Flow', 'Diagnosis Of Alzheimer’s Disease', 'Backbone Network', 'Embedding Module', 'Prior Imaging', 'Query Features', 'Current Image', 'Scans Of Subjects', 'Deformable Layer', 'Deformation Field', 'Longitudinal Imaging', 'Vision Transformer', 'Input Pair']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",4,"Structural magnetic resonance imaging (sMRI), especially longitudinal sMRI, is often used to monitor and capture disease progression during the clinical diagnosis of Alzheimer's Disease (AD). However, current methods neglect AD’s progressive nature and have mostly relied on a single image for recognizing AD. In this paper, we consider the problem of leveraging the longitudinal MRIs of a subject for AD classification. To address the challenges of missing data, data demand, and subtle changes over time in learning longitudinal 3D MRIs, we propose a novel model LongFormer, which is a hybrid 3D CNN and transformer design to learn from image and longitudinal flow pairs. Our model can fully leverage all images in a dataset and effectively fuse spatiotemporal features for classification. We evaluate our model on three datasets, i.e., ADNI, OASIS, and AIBL, and compare it to eight baseline algorithms. Our proposed LongFormer achieves state-of-the-art performance in classifying AD and NC subjects from all three public datasets. Our source code is available online at https://github.com/Qybc/LongFormer."
Lost Your Style? Navigating With Semantic-Level Approach for Text-To-Outfit Retrieval,"Junkyu Jang, Eugene Hwang, Sung-Hyuk Park","KAIST, College of Business, Seoul, Korea",100.0,South Korea,0.0,,"Fashion stylists have historically bridged the gap between consumers' desires and perfect outfits, which involve intricate combinations of colors, patterns, and materials. Although recent advancements in fashion recommendation systems have made strides in outfit compatibility prediction and complementary item retrieval, these systems rely heavily on pre-selected customer choices. Therefore, we introduce a groundbreaking approach to fashion recommendations: text-to-outfit retrieval task that generates a complete outfit set based solely on textual descriptions given by users. Our model is devised at three semantic levels--item, style, and outfit--where each level progressively aggregates data to form a coherent outfit recommendation based on textual input. Here, we leverage strategies similar to those in the contrastive language-image pretraining model to address the intricate-style matrix within the outfit sets. Using the Maryland Polyvore and Polyvore Outfit datasets, our approach significantly outperformed state-of-the-art models in text-video retrieval tasks, solidifying its effectiveness in the fashion recommendation domain. This research not only pioneers a new facet of fashion recommendation systems, but also introduces a method that captures the essence of individual style preferences through textual descriptions.",https://openaccess.thecvf.com/content/WACV2024/html/Jang_Lost_Your_Style_Navigating_With_Semantic-Level_Approach_for_Text-To-Outfit_Retrieval_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jang_Lost_Your_Style_Navigating_With_Semantic-Level_Approach_for_Text-To-Outfit_Retrieval_WACV_2024_paper.pdf,,,2311.02122,main,Poster,,,,,,
M33D: Learning 3D Priors Using Multi-Modal Masked Autoencoders for 2D Image and Video Understanding,"Muhammad Abdullah Jamal, Omid Mohareri","Intuitive Surgical Inc., Sunnyvale, CA",100.0,USA,0.0,,"We present a new pre-training strategy called M^ 3 3D (Multi-Modal Masked 3D) built based on Multi-modal masked autoencoders that can leverage 3D priors and learned cross-modal representations in RGB-D data. We integrate two major self-supervised learning frameworks; Masked Image Modeling (MIM) and contrastive learning; aiming to effectively embed masked 3D priors and modality complementary features to enhance the correspondence between modalities. In contrast to recent approaches which are either focusing on specific downstream tasks or require multi-view correspondence, we show that our pre-training strategy is ubiquitous, enabling improved representation learning that can transfer into improved performance on various downstream tasks such as video action recognition, video action detection, 2D semantic segmentation and depth estimation. Experiments show that M^ 3 3D outperforms the existing state-of-the-art approaches on ScanNet, NYUv2, UCF-101 and OR-AR, particularly with an improvement of +1.3% mIoU against Mask3D on ScanNet semantic segmentation. We further evaluate our method on low-data regime and demonstrate its superior data efficiency compared to current state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Jamal_M33D_Learning_3D_Priors_Using_Multi-Modal_Masked_Autoencoders_for_2D_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jamal_M33D_Learning_3D_Priors_Using_Multi-Modal_Masked_Autoencoders_for_2D_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
MACP: Efficient Model Adaptation for Cooperative Perception,"Yunsheng Ma, Juanwu Lu, Can Cui, Sicheng Zhao, Xu Cao, Wenqian Ye, Ziran Wang","University of Virginia, Charlottesville, VA, USA; Purdue University, West Lafayette, IN, USA; Tsinghua University, Beijing, China; University of Illinois Urbana-Champaign, Champaign, IL, USA",100.0,"China, USA",0.0,,"Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to ""see through the occlusions"", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs. Our source code is available at https://github.com/PurdueDigitalTwin/MACP.",https://openaccess.thecvf.com/content/WACV2024/html/Ma_MACP_Efficient_Model_Adaptation_for_Cooperative_Perception_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ma_MACP_Efficient_Model_Adaptation_for_Cooperative_Perception_WACV_2024_paper.pdf,,https://github.com/PurdueDigitalTwin/MACP,2310.16870,main,Poster,https://ieeexplore.ieee.org/document/10483951/,"['Training', 'Location awareness', 'Adaptation models', 'Costs', 'Computational modeling', 'Vehicular ad hoc networks', 'Information sharing']","['Cooperative Perception', 'Tuning Parameter', 'Communication Cost', 'Automated Vehicles', 'Convolutional Layers', 'Feature Maps', 'Intersection Over Union', 'Point Cloud', 'Bounding Box', 'Domain Shift', 'Average Precision', 'Output Feature Map', 'Feature Encoder', 'Adaptive Modulation', 'Fine-tuned Model', 'Early Fusion', 'Late Fusion', 'Communication Constraints', 'Compression Factor', 'Point Cloud Features', '3D Object Detection', 'V2V Communication', 'Prediction Head', 'Pre-trained Parameters', 'Convolution Operation', '3D Bounding Box', 'Data Transmission', 'Output Feature Vector', 'Object Detection', 'Low-dimensional Space']","['Algorithms', '3D computer vision']",5,"Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to ""see through the occlusions"", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs. Our ource code is available at https://github.com/PurdueDigitalTwin/MACP."
MAELi: Masked Autoencoder for Large-Scale LiDAR Point Clouds,"Georg Krispel, David Schinagl, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof","Graz University of Technology, Christian Doppler Laboratory for Embedded Machine Learning; Graz University of Technology",100.0,Austria,0.0,,"The sensing process of large-scale LiDAR point clouds inevitably causes large blind spots, i.e. regions not visible to the sensor. We demonstrate how these inherent sampling properties can be effectively utilized for self-supervised representation learning by designing a highly effective pre-training framework that considerably reduces the need for tedious 3D annotations to train state-of-the-art object detectors. Our Masked AutoEncoder for LiDAR point clouds (MAELi) intuitively leverages the sparsity of LiDAR point clouds in both the encoder and decoder during reconstruction. This results in more expressive and useful initialization, which can be directly applied to downstream perception tasks, such as 3D object detection or semantic segmentation for autonomous driving. In a novel reconstruction approach, MAELi distinguishes between empty and occluded space and employs a new masking strategy that targets the LiDAR's inherent spherical projection. Thereby, without any ground truth whatsoever and trained on single frames only, MAELi obtains an understanding of the underlying 3D scene geometry and semantics. To demonstrate the potential of MAELi, we pre-train backbones in an end-to-end manner and show the effectiveness of our unsupervised pre-trained weights on the tasks of 3D object detection and semantic segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Krispel_MAELi_Masked_Autoencoder_for_Large-Scale_LiDAR_Point_Clouds_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Krispel_MAELi_Masked_Autoencoder_for_Large-Scale_LiDAR_Point_Clouds_WACV_2024_paper.pdf,,,2212.07207,main,Poster,https://ieeexplore.ieee.org/document/10483752/,"['Point cloud compression', 'Laser radar', 'Three-dimensional displays', 'Annotations', 'Semantic segmentation', 'Semantics', 'Object detection']","['Point Cloud', 'LiDAR Point Clouds', 'Masked Autoencoder', 'Object Detection', 'Representation Learning', 'Semantic Segmentation', 'Perceptual Task', 'Self-supervised Learning', 'Reconstruction Approach', 'Pre-trained Weights', '3D Detection', '3D Object Detection', 'Masking Strategy', 'Active Site', 'Training Set', 'Feature Maps', 'Feature Representation', 'Limited Resolution', 'Geometric Structure', 'Data Instances', 'Original Point Cloud', 'Point Cloud Reconstruction', 'LiDAR Sensor', 'Ground Truth Samples', 'Object Reconstruction', 'Ground Truth Object', 'Angular Resolution', 'Active Voxels', 'Unsupervised Way', 'Single GPU']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"The sensing process of large-scale LiDAR point clouds inevitably causes large blind spots, i.e. regions not visible to the sensor. We demonstrate how these inherent sampling properties can be effectively utilized for self-supervised representation learning by designing a highly effective pretraining framework that considerably reduces the need for tedious 3D annotations to train state-of-the-art object detectors. Our Masked AutoEncoder for LiDAR point clouds (MAELi) intuitively leverages the sparsity of LiDAR point clouds in both the encoder and decoder during reconstruction. This results in more expressive and useful initialization, which can be directly applied to downstream perception tasks, such as 3D object detection or semantic segmentation for autonomous driving. In a novel reconstruction approach, MAELi distinguishes between empty and occluded space and employs a new masking strategy that targets the LiDAR’s inherent spherical projection. Thereby, without any ground truth whatsoever and trained on single frames only, MAELi obtains an understanding of the underlying 3D scene geometry and semantics. To demonstrate the potential of MAELi, we pre-train backbones in an end-to-end manner and show the effectiveness of our unsupervised pre-trained weights on the tasks of 3D object detection and semantic segmentation."
MAdVerse: A Hierarchical Dataset of Multi-Lingual Ads From Diverse Sources and Categories,"Amruth Sagar, Rishabh Srivastava, Rakshitha R. T., Venkata Kesav Venna, Ravi Kiran Sarvadevabhatla","KLE Tech, India; CVIT, IIIT Hyderabad, India",50.0,India,50.0,India,"The convergence of computer vision and advertising has sparked substantial interest lately. Existing advertisement datasets often derive from subsets of established data with highly specialized annotations or feature diverse annotations without a cohesive taxonomy among ad images. Notably, no datasets encompass diverse advertisement styles or semantic grouping at various levels of granularity for a better understanding of ads. Our work addresses this gap by introducing MAdVerse, an extensive, multilingual compilation of more than 50,000 ads from the web, social media websites and e-newspapers. Advertisements are hierarchically grouped with uniform granularity into 11 categories, divided into 51 sub-categories, and 524 fine-grained brands at leaf level, each featuring ads in various languages. Furthermore, we provide comprehensive baseline classification results for various pertinent prediction tasks within the realm of advertising analysis. Specifically, these tasks include hierarchical ad classification, source classification, multilingual classification and inducing hierarchy in existing ad datasets. The dataset, code and models are available on the project page https://madverse24.github.io/",https://openaccess.thecvf.com/content/WACV2024/html/Sagar_MAdVerse_A_Hierarchical_Dataset_of_Multi-Lingual_Ads_From_Diverse_Sources_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sagar_MAdVerse_A_Hierarchical_Dataset_of_Multi-Lingual_Ads_From_Diverse_Sources_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484474/,"['Computer vision', 'Technological innovation', 'Social networking (online)', 'Annotations', 'Semantics', 'Taxonomy', 'Linguistics']","['Variety Of Sources', 'Hierarchical Datasets', 'Social Media', 'Level Of Granularity', 'Source Class', 'Semantic Groups', 'Social Media Platforms', 'Consumer Behavior', 'Online Advertising', 'Levels Of Hierarchy', 'Feature Fusion', 'Multiple Languages', 'Hierarchical Classification', 'Newspaper Advertisements', 'Graphic Design', 'Types Of Metrics', 'Google Images', 'Brand Identity', 'Semantic Embedding', 'Instagram', 'Popular Social Media Platforms', 'Classification Head', 'Vision Transformer', 'Marathi', 'Soft Labels', 'Facebook']","['Applications', 'Commercial / retail', 'Algorithms', 'Datasets and evaluations']",,"The convergence of computer vision and advertising has sparked substantial interest lately. Existing advertisement datasets are either subsets of existing datasets with specialized annotations or feature diverse annotations without a cohesive taxonomy among ad images. Notably, no datasets encompass diverse advertisement styles or semantic grouping at various levels of granularity. Our work addresses this gap by introducing MAdVerse, an extensive, multilingual compilation of more than 50,000 ads from the web, social media websites, and e-newspapers. Advertisements are hierarchically grouped with uniform granularity into 11 categories, divided into 51 sub-categories, and 524 fine-grained brands at leaf level, each featuring ads in various languages. We provide comprehensive baseline classification results for prediction tasks within the realm of advertising analysis. These tasks include hierarchical ad classification, source classification, multilingual classification, and inducing hierarchy in existing ad datasets.The dataset, code and models are available on the project page https://madverse24.github.io/"
MEGANet: Multi-Scale Edge-Guided Attention Network for Weak Boundary Polyp Segmentation,"Nhat-Tan Bui, Dinh-Hieu Hoang, Quang-Thuc Nguyen, Minh-Triet Tran, Ngan Le","University of Science, and John von Neumann Institute, VNU-HCM, Vietnam National University, Ho Chi Minh City, Vietnam; AICV Lab, University of Arkansas, Fayetteville, Arkansas, USA",100.0,"USA, Vietnam",0.0,,"Efficient polyp segmentation in healthcare plays a critical role in enabling early diagnosis of colorectal cancer. However, the segmentation of polyps presents numerous challenges, including the intricate distribution of backgrounds, variations in polyp sizes and shapes, and indistinct boundaries. Defining the boundary between the foreground (i.e. polyp itself) and the background (surrounding tissue) is difficult. To mitigate these challenges, we propose Multi-Scale Edge-Guided Attention Network (MEGANet) tailored specifically for polyp segmentation within colonoscopy images. This network draws inspiration from the fusion of a classical edge detection technique with an attention mechanism. By combining these techniques, MEGANet effectively preserves high-frequency information, notably edges and boundaries, which tend to erode as neural networks deepen. MEGANet is designed as an end-to-end framework, encompassing three key modules: an encoder, which is responsible for capturing and abstracting the features from the input image, a decoder, which focuses on salient features, and the Edge-Guided Attention module (EGA) that employs the Laplacian Operator to accentuate polyp boundaries. Extensive experiments, both qualitative and quantitative, on five benchmark datasets, demonstrate that our MEGANet outperforms other existing SOTA methods under six evaluation metrics. Our code is available at https://github.com/UARK-AICV/MEGANet.",https://openaccess.thecvf.com/content/WACV2024/html/Bui_MEGANet_Multi-Scale_Edge-Guided_Attention_Network_for_Weak_Boundary_Polyp_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bui_MEGANet_Multi-Scale_Edge-Guided_Attention_Network_for_Weak_Boundary_Polyp_Segmentation_WACV_2024_paper.pdf,,https://github.com/UARK-AICV/MEGANet,2309.03329,main,Poster,https://ieeexplore.ieee.org/document/10484210/,"['Measurement', 'Location awareness', 'Technological innovation', 'Image segmentation', 'Laplace equations', 'Shape', 'Image edge detection']","['Multi-scale Network', 'Weak Boundary', 'Polyp Segmentation', 'Colorectal Cancer', 'Input Image', 'Attention Mechanism', 'Benchmark Datasets', 'Colonoscopy', 'Attention Module', 'Edge Detection', 'Polyp Size', 'SOTA Methods', 'Medical Imaging', 'Convolutional Neural Network', 'Decoding', 'Feature Maps', 'Multiple Scales', 'Stochastic Gradient Descent', 'High-level Features', 'Higher Layers', 'Laplacian Pyramid', 'Edge Information', 'Ith Layer', 'Semantic Gap', 'Laplacian Of Gaussian', 'Difference Of Gaussian', 'Attention Map', 'Edge Extraction', 'Channel Attention', 'Edge Details']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",15,"Efficient polyp segmentation in healthcare plays a critical role in enabling early diagnosis of colorectal cancer. However, the segmentation of polyps presents numerous challenges, including the intricate distribution of backgrounds, variations in polyp sizes and shapes, and indistinct boundaries. Defining the boundary between the foreground (i.e. polyp itself) and the background (surrounding tissue) is difficult. To mitigate these challenges, we propose Multi-Scale Edge-Guided Attention Network (MEGANet) tailored specifically for polyp segmentation within colonoscopy images. This network draws inspiration from the fusion of a classical edge detection technique with an attention mechanism. By combining these techniques, MEGANet effectively preserves high-frequency information, notably edges and boundaries, which tend to erode as neural networks deepen. MEGANet is designed as an end-to-end framework, encompassing three key modules: an encoder, which is responsible for capturing and abstracting the features from the input image, a decoder, which focuses on salient features, and the Edge-Guided Attention module (EGA) that employs the Laplacian Operator to accentuate polyp boundaries. Extensive experiments, both qualitative and quantitative, on five benchmark datasets, demonstrate that our MEGANet outperforms other existing SOTA methods under six evaluation metrics. Our code is available at https://github.com/UARK-AICV/MEGANet."
MFT: Long-Term Tracking of Every Pixel,"Michal Neoral, Jonáš Šerých, Jiří Matas","CMP Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague",100.0,Czech Republic,0.0,,"We propose MFT -- Multi-Flow dense Tracker -- a novel method for dense, pixel-level, long-term tracking. The approach exploits optical flows estimated not only between consecutive frames, but also for pairs of frames at logarithmically spaced intervals. It selects the most reliable sequence of flows on the basis of estimates of its geometric accuracy and the probability of occlusion, both provided by a pre-trained CNN. We show that MFT achieves competitive performance on the TAP-Vid benchmark, outperforming baselines by a significant margin, and tracking densely orders of magnitude faster than the state-of-the-art point-tracking methods. The method is insensitive to medium-length occlusions and it is robustified by estimating flow with respect to the reference frame, which reduces drift.",https://openaccess.thecvf.com/content/WACV2024/html/Neoral_MFT_Long-Term_Tracking_of_Every_Pixel_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Neoral_MFT_Long-Term_Tracking_of_Every_Pixel_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484299/,"['Computer vision', 'Uncertainty', 'Image resolution', 'Codes', 'Video sequences', 'Benchmark testing', 'Streaming media']","['Long-term Tracking', 'Reference Frame', 'Optical Flow', 'Consecutive Frames', 'Pair Of Frames', 'Aspect Ratio', 'Global Optimization', 'Flow Field', 'Video Frames', 'Position Error', 'Sequence Of Frames', 'Feature Matching', 'Current Frame', 'Point Tracking', 'Single GPU', 'Computer Vision Problems', 'Flow Set', 'Video Editing', 'Accurate Flow', 'Optical Flow Estimation', 'Uncertainty Map', 'Original Aspect Ratio', 'Cost Volume', 'Optical Flow Method', 'Query Point', 'Position Time', 'Motion Estimation', 'Error Accumulation', 'Bilinear Interpolation', 'Computer Vision']","['Algorithms', 'Video recognition and understanding']",14,"We propose MFT – Multi-Flow dense Tracker – a novel method for dense, pixel-level, long-term tracking. The approach exploits optical flows estimated not only between consecutive frames, but also for pairs of frames at logarithmically spaced intervals. It selects the most reliable sequence of flows on the basis of estimates of its geometric accuracy and the probability of occlusion, both provided by a pre-trained CNN. We show that MFT achieves competitive performance on the TAP-Vid benchmark, outperforming baselines by a significant margin, and tracking densely orders of magnitude faster than the state-of-the-art point-tracking methods. The method is insensitive to medium-length occlusions and it is robustified by estimating flow with respect to the reference frame, which reduces drift."
MGM-AE: Self-Supervised Learning on 3D Shape Using Mesh Graph Masked Autoencoders,"Zhangsihao Yang, Kaize Ding, Huan Liu, Yalin Wang",Arizona State University; Northwestern University,100.0,USA,0.0,,"The challenges of applying self-supervised learning to 3D mesh data include difficulties in explicitly modeling and leveraging geometric topology information and designing appropriate pretext tasks and augmentation methods for irregular mesh topology. In this paper, we propose a novel approach for pre-training models on large-scale, unlabeled datasets using graph masking on a mesh graph composed of faces. Our method, Mesh Graph Masked Autoencoders (MGM-AE), utilizes masked autoencoding to pre-train the model and extract important features from the data. Our pre-trained model outperforms prior state-of-the-art mesh encoders in shape classification and segmentation benchmarks, achieving 90.8% accuracy on ModelNet40 and 78.5 mIoU on ShapeNet. The best performance is obtained when the model is trained and evaluated under different masking ratios. Our approach demonstrates effectiveness in pre-training models on large-scale, unlabeled datasets and its potential for improving performance on downstream tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_MGM-AE_Self-Supervised_Learning_on_3D_Shape_Using_Mesh_Graph_Masked_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_MGM-AE_Self-Supervised_Learning_on_3D_Shape_Using_Mesh_Graph_Masked_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484397/,"['Three-dimensional displays', 'Shape', 'Transfer learning', 'Self-supervised learning', 'Benchmark testing', 'Feature extraction', 'Data models']","['3D Shape', 'Self-supervised Learning', 'Grid Graph', '3D Mesh', 'Geometric Information', 'Topological Information', 'Pretext Task', 'Segmentation Benchmark', 'Extract Important Features', 'Training Data', 'Convolution', 'Support Vector Machine', 'Classification Task', 'Local System', 'Test Accuracy', 'Transfer Learning', 'Point Cloud', 'Object Classification', 'Nodes In The Graph', 'Segmentation Task', 'Node Features', 'Ground Truth Points', 'Graph Attention', 'Graph Neural Networks', 'Local Coordinate System', 'Inherent Information', 'Pre-trained Weights', 'Mesh Representation', 'Adjacent Vertices', 'Neural Network']","['Algorithms', '3D computer vision', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"The challenges of applying self-supervised learning to 3D mesh data include difficulties in explicitly modeling and leveraging geometric topology information and designing appropriate pretext tasks and augmentation methods for irregular mesh topology. In this paper, we propose a novel approach for pre-training models on large-scale, unlabeled datasets using graph masking on a mesh graph composed of faces. Our method, Mesh Graph Masked Autoencoders (MGM-AE), utilizes masked autoencoding to pre-train the model and extract important features from the data. Our pre-trained model outperforms prior state-of-the-art mesh encoders in shape classification and segmentation benchmarks, achieving 90.8% accuracy on ModelNet40 and 78.5 mIoU on ShapeNet. The best performance is obtained when the model is trained and evaluated under different masking ratios. Our approach demonstrates effectiveness in pre-training models on large-scale, unlabeled datasets and its potential for improving performance on downstream tasks."
MICS: Midpoint Interpolation To Learn Compact and Separated Representations for Few-Shot Class-Incremental Learning,"Solang Kim, Yuho Jeong, Joon Sung Park, Sung Whan Yoon","Nuvilab Inc., Republic of Korea; Graduate School of Artiﬁcial Intelligence, Ulsan National Institute of Science and Technology (UNIST), Republic of Korea",50.0,South Korea,50.0,South Korea,"Few-shot class-incremental learning (FSCIL) aims to learn a classification model for continually accepting novel classes with a few samples. The key of FSCIL is the joint success of the following two training stages: Base training stage to classify base classes and Incremental training stage with sequential learning of novel classes. However, recent efforts show a tendency to focus on one of the stages, or separately design strategies for each stage, so that less effort has been paid to devise a consistent strategy across the consecutive stages. In this paper, we first emphasize the particular aspects of the successful FSCIL algorithm that are worthwhile to consistently pursue during both stages, i.e., intra-class compactness and inter-class separability of the representation, which allows a model to reserve feature space in between current classes for preparing the acceptance of novel classes in the future. To achieve these aspects, we propose a mixup-based FSCIL method called MICS, which theoretically guarantees to enlarge the thickness of the margin space between different classes, leading to outstanding performance on the existing benchmarks. Code is available at https://github.com/solangii/MICS.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_MICS_Midpoint_Interpolation_To_Learn_Compact_and_Separated_Representations_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_MICS_Midpoint_Interpolation_To_Learn_Compact_and_Separated_Representations_for_WACV_2024_paper.pdf,,https://github.com/solangii/MICS,,main,Poster,https://ieeexplore.ieee.org/document/10483994/,"['Training', 'Microwave integrated circuits', 'Interpolation', 'Computer vision', 'Codes', 'Computational modeling', 'Benchmark testing']","['Compact Representation', 'Few-shot Learning', 'Separate Representations', 'Class-incremental Learning', 'Few-shot Class-incremental Learning', 'Benchmark', 'Base Classes', 'Marginal Spaces', 'Inter-class Separability', 'Class Labels', 'Stochastic Gradient Descent', 'Representation Of Space', 'Incremental Learning', 'Beta Distribution', 'Classifier For Class', 'Input Space', 'Final Session', 'Prior Methods', 'Classification Index', 'Class Prototypes', 'Soft Labels', 'Paste Samples', 'Images Look', 'Catastrophic Forgetting']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Few-shot class-incremental learning (FSCIL) aims to learn a classification model for continually accepting novel classes with a few samples. The key of FSCIL is the joint success of the following two training stages: Base training stage to classify base classes and Incremental training stage with sequential learning of novel classes. However, recent efforts show a tendency to focus on one of the stages, or separately design strategies for each stage, so that less effort has been paid to devise a consistent strategy across the consecutive stages. In this paper, we first emphasize the particular aspects of the successful FSCIL algorithm that are worthwhile to consistently pursue during both stages, i.e., intra-class compactness and inter-class separability of the representation, which allows a model to reserve feature space in between current classes for preparing the acceptance of novel classes in the future. To achieve these aspects, we propose a mixup-based FSCIL method called MICS, which theoretically guarantees to enlarge the thickness of the margin space between different classes, leading to outstanding performance on the existing benchmarks. Code is available at https://github.com/solangii/MICS."
MIDAS: Mixing Ambiguous Data With Soft Labels for Dynamic Facial Expression Recognition,"Ryosuke Kawamura, Hideaki Hayashi, Noriko Takemura, Hajime Nagahara","Osaka University; Kyushu Institute of Technology; Fujitsu Research of America, Inc.",66.66666666666666,Japan,33.33333333333334,USA,"Dynamic facial expression recognition (DFER) is an important task in the field of computer vision. To apply automatic DFER in practice, it is necessary to accurately recognize ambiguous facial expressions, which often appear in data in the wild. In this paper, we propose MIDAS, a data augmentation method for DFER, which augments ambiguous facial expression data with soft labels consisting of probabilities for multiple emotion classes. In MIDAS, the training data are augmented by convexly combining pairs of video frames and their corresponding emotion class labels, which can also be regarded as an extension of mixup to soft-labeled video data. This simple extension is remarkably effective in DFER with ambiguous facial expression data. To evaluate MIDAS, we conducted experiments on the DFEW dataset. The results demonstrate that the model trained on the data augmented by MIDAS outperforms the existing state-of-the-art method trained on the original dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Kawamura_MIDAS_Mixing_Ambiguous_Data_With_Soft_Labels_for_Dynamic_Facial_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kawamura_MIDAS_Mixing_Ambiguous_Data_With_Soft_Labels_for_Dynamic_Facial_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484244/,"['Computer vision', 'Annotations', 'Face recognition', 'Training data', 'Data augmentation', 'Data models', 'Task analysis']","['Facial Expressions', 'Face Recognition', 'Facial Expression Recognition', 'Ambiguous Data', 'Soft Labels', 'Dynamic Facial Expressions', 'Training Data', 'Data Augmentation', 'Emotion Categories', 'Simple Extension', 'Field Task', 'Pair Of Frames', 'Multiple Emotions', 'Ambiguous Expressions', 'Loss Function', 'Training Dataset', 'Classification Accuracy', 'Partition Coefficient', 'Long Short-term Memory', 'Disgust', 'Empirical Risk', 'Video Clips', 'Combination Of Emotions', 'Mixed Strategy', 'Object Recognition', 'Single Class', 'Gated Recurrent Unit', 'Action Recognition', 'Deep Learning Architectures', 'Face Detection']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"Dynamic facial expression recognition (DFER) is an important task in the field of computer vision. To apply automatic DFER in practice, it is necessary to accurately recognize ambiguous facial expressions, which often appear in data in the wild. In this paper, we propose MIDAS, a data augmentation method for DFER, which augments ambiguous facial expression data with soft labels consisting of probabilities for multiple emotion classes. In MIDAS, the training data are augmented by convexly combining pairs of video frames and their corresponding emotion class labels, which can also be regarded as an extension of mixup to soft-labeled video data. This simple extension is remarkably effective in DFER with ambiguous facial expression data. To evaluate MIDAS, we conducted experiments on the DFEW dataset. The results demonstrate that the model trained on the data augmented by MIDAS outperforms the existing state-of-the-art method trained on the original dataset."
MIST: Medical Image Segmentation Transformer With Convolutional Attention Mixing (CAM) Decoder,"Md Motiur Rahman, Shiva Shokouhmand, Smriti Bhatt, Miad Faezipour","Purdue University, West Lafayette, IN 47907, USA",100.0,USA,0.0,,"One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts- a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connection, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves the segmentation performance significantly. Our model with data and code is publicly available on GitHub.",https://openaccess.thecvf.com/content/WACV2024/html/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_MIST_Medical_Image_Segmentation_Transformer_With_Convolutional_Attention_Mixing_CAM_WACV_2024_paper.pdf,,https://github.com/Rahman-Motiur/MIST,2310.19898,main,Poster,https://ieeexplore.ieee.org/document/10483964/,"['Convolutional codes', 'Image segmentation', 'Computational modeling', 'Semantics', 'Transformers', 'Decoding', 'Kernel']","['Medical Imaging', 'Image Segmentation', 'Medical Image Segmentation', 'Synapse', 'Local Context', 'Spatial Information', 'Spatial Dimensions', 'Receptive Field', 'Spatial Attention', 'Skip Connections', 'Segmentation Performance', 'Long-range Dependencies', 'Deep Convolution', 'Multi-head Self-attention', 'Spatial Attention Module', 'Vision Transformer', 'Contralateral', 'Computed Tomography', 'Convolutional Neural Network', 'Feature Maps', 'Dice Score', 'Convolutional Neural Network Model', 'Global Context', 'Natural Language Processing Tasks', 'U-Net Model', 'Decoder Block', 'Linear Projection', 'Attention Mechanism', 'CNN-based Models', 'Convolution Operation']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Biomedical / healthcare / medicine']",4,"One of the common and promising deep learning approaches used for medical image segmentation is transformers, as they can capture long-range dependencies among the pixels by utilizing self-attention. Despite being successful in medical image segmentation, transformers face limitations in capturing local contexts of pixels in multimodal dimensions. We propose a Medical Image Segmentation Transformer (MIST) incorporating a novel Convolutional Attention Mixing (CAM) decoder to address this issue. MIST has two parts- a pre-trained multi-axis vision transformer (MaxViT) is used as an encoder, and the encoded feature representation is passed through the CAM decoder for segmenting the images. In the CAM decoder, an attention-mixer combining multi-head self-attention, spatial attention, and squeeze and excitation attention modules is introduced to capture long-range dependencies in all spatial dimensions. Moreover, to enhance spatial information gain, deep and shallow convolutions are used for feature extraction and receptive field expansion, respectively. The integration of low-level and high-level features from different network stages is enabled by skip connection, allowing MIST to suppress unnecessary information. The experiments show that our MIST transformer with CAM decoder outperforms the state-of-the-art models specifically designed for medical image segmentation on the ACDC and Synapse datasets. Our results also demonstrate that adding the CAM decoder with a hierarchical transformer improves the segmentation performance significantly. Our model with data and code is publicly available on GitHub 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
MITFAS: Mutual Information Based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition,"Ruiqi Xian, Xijun Wang, Dinesh Manocha",University of Maryland - College Park,100.0,USA,0.0,,"We present a novel approach for action recognition in UAV videos. Our formulation is designed to handle occlusion and viewpoint changes caused by the movement of a UAV. We use the concept of mutual information to compute and align the regions corresponding to human action or motion in the temporal domain. This enables our recognition model to learn from the key features associated with the motion. We also propose a novel frame sampling method that uses joint mutual information to acquire the most informative frame sequence in UAV videos. We have integrated our approach with X3D and evaluated the performance on multiple datasets. In practice, we achieve 18.9% improvement in Top-1 accuracy over current state-of-the-art methods on UAV-Human, 7.3% improvement on Drone-Action, and 7.16% improvement on NEC Drones. The code is available at https://github.com/Ricky-Xian/MITFAS.",https://openaccess.thecvf.com/content/WACV2024/html/Xian_MITFAS_Mutual_Information_Based_Temporal_Feature_Alignment_and_Sampling_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xian_MITFAS_Mutual_Information_Based_Temporal_Feature_Alignment_and_Sampling_for_WACV_2024_paper.pdf,,https://github.com/Ricky-Xian/MITFAS,2303.02575,main,Poster,https://ieeexplore.ieee.org/document/10484464/,"['Computer vision', 'Codes', 'Computational modeling', 'X3D', 'Autonomous aerial vehicles', 'Sampling methods', 'Mutual information']","['Mutual Information', 'Action Recognition', 'Temporal Sampling', 'Feature Alignment', 'Temporal Alignment', 'Video Action Recognition', 'Aerial Video', 'Sampling Method', 'Human Activities', 'Unmanned Aerial Vehicles', 'Recognition Model', 'Sequence Of Frames', 'Temporal Domain', 'Human Motion', 'Recognition Approach', 'Top-1 Accuracy', 'Frame Information', 'Viewpoint Changes', 'Video Recognition', 'Similarity Measure', 'Temporal Frame', 'Small Resolution', 'Video Frames', 'Pixel In Frame', 'Redundant Information', 'Bounding Box', 'Dynamic Background', 'Alignment Method', 'Video Capture', 'Peak Signal-to-noise Ratio']","['Algorithms', 'Video recognition and understanding', 'Applications', 'Robotics']",4,"We present a novel approach for action recognition in UAV videos. Our formulation is designed to handle occlusion and viewpoint changes caused by the movement of a UAV. We use the concept of mutual information to compute and align the regions corresponding to human action or motion in the temporal domain. This enables our recognition model to learn from the key features associated with the motion. We also propose a novel frame sampling method that uses joint mutual information to acquire the most informative frame sequence in UAV videos. We have integrated our approach with X3D and evaluated the performance on multiple datasets. In practice, we achieve 18.9% improvement in Top-1 accuracy over current state-of-the-art methods on UAV-Human [30], 7.3% improvement on Drone-Action [41], and 7.16% improvement on NEC Drones [7]. The code is available at https://github.com/Ricky-Xian/MITFAS"
MIVC: Multiple Instance Visual Component for Visual-Language Models,"Wenyi Wu, Qi Li, Wenliang Zhong, Junzhou Huang","Amazon, 410 Terry Ave N, Seattle, WA; The University of Texas at Arlington, 500 UTA Boulevard, Arlington, TX; Amazon, 424 5th Ave, New York, NY",33.33333333333333,USA,66.66666666666667,USA,"Vision-language models have been widely explored across a wide range of tasks and achieve satisfactory performance. However, it's under-explored how to consolidate entity understanding through a varying number of images and to align it with the pre-trained language models for generative tasks. In this paper, we propose MIVC, a general multiple instance visual component to bridge the gap between various image inputs with off-the-shelf vision-language models by aggregating visual representations in a permutation-invariant fashion through a neural network. We show that MIVC could be plugged into the visual-language models to improve the model performance consistently on visual question answering, classification and captioning tasks on a public available e-commerce dataset with multiple images per product. Furthermore, we show that the component provides insight into the contribution of each image to the downstream tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_MIVC_Multiple_Instance_Visual_Component_for_Visual-Language_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_MIVC_Multiple_Instance_Visual_Component_for_Visual-Language_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484211/,"['Visualization', 'Computer vision', 'Computational modeling', 'Neural networks', 'Question answering (information retrieval)', 'Electronic commerce', 'Task analysis']","['Visual Components', 'Neural Network', 'Input Image', 'Visual Representation', 'Number Of Images', 'Multiple Images', 'Question Answering', 'Task Model', 'Language Model', 'Wide Range Of Tasks', 'Pre-trained Language Models', 'Visual Question Answering', 'Contralateral', 'Training Set', 'Single Image', 'Attention Mechanism', 'Max-pooling', 'Average Pooling', 'Single Instance', 'Image Representation', 'Multiple Instance Learning', 'Vision Transformer', 'Image Captioning', 'Multimodal Model', 'E-commerce Websites', 'Piece Of Text', 'Pooling Strategy', 'Foundation Model', 'Aforementioned Data', 'Multiple Representations']","['Applications', 'Commercial / retail', 'Algorithms', 'Image recognition and understanding']",,"Vision-language models have been widely explored across a wide range of tasks and achieve satisfactory performance. However, it’s under-explored how to consolidate entity understanding through a varying number of images and to align it with the pre-trained language models for generative tasks. In this paper, we propose MIVC, a general multiple instance visual component to bridge the gap between various image inputs with off-the-shelf vision-language models by aggregating visual representations in a permutation-invariant fashion through a neural network. We show that MIVC could be plugged into the visual-language models to improve the model performance consistently on visual question answering, classification and captioning tasks on a public available e-commerce dataset with multiple images per product. Furthermore, we show that the component provides insight into the contribution of each image to the downstream tasks."
MOPA: Modular Object Navigation With PointGoal Agents,"Sonia Raychaudhuri, Tommaso Campari, Unnat Jain, Manolis Savva, Angel X. Chang",University of Padova; Meta AI; Simon Fraser University,66.66666666666666,"Canada, Italy",33.33333333333334,USA,"We propose a simple but effective modular approach MOPA (Modular ObjectNav with PointGoal agents) to systematically investigate the inherent modularity of the object navigation task in Embodied AI. MOPA consists of four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore the environment, and (d) a navigation module to move to identified target objects. We show that we can effectively reuse a pretrained PointGoal agent as the navigation model instead of learning to navigate from scratch, thus saving time and compute. We also compare various exploration strategies for MOPA and find that a simple uniform strategy significantly outperforms more advanced exploration methods.",https://openaccess.thecvf.com/content/WACV2024/html/Raychaudhuri_MOPA_Modular_Object_Navigation_With_PointGoal_Agents_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Raychaudhuri_MOPA_Modular_Object_Navigation_With_PointGoal_Agents_WACV_2024_paper.pdf,https://3dlg-hcvc.github.io/mopa,,2304.03696,main,Poster,https://ieeexplore.ieee.org/document/10484063/,"['Analytical models', 'Systematics', 'Navigation', 'Computational modeling', 'Transfer learning', 'Semantics', 'Object detection']","['Object Navigation', 'Object Detection', 'Modularity', 'Target Object', 'Exploration Strategy', 'Modular Approach', 'Navigation Task', 'Semantic Map', 'Uniform Strategy', 'Frontier', 'Validation Set', 'Shortest Path', 'Path Planning', 'Global Policy', 'Complex Strategies', 'Deep Reinforcement Learning', 'Natural Objects', 'Current Goals', 'Semantic Labels', 'Failure Analysis', 'Breadth-first Search', 'Planning Module', 'Environment Map', 'Vision Transformer', 'Navigation Strategies', 'Geodesic Distance', 'Occupancy Map', 'Supplement For Details', 'Ground Truth Information', 'Machine Vision']","['Algorithms', 'Vision + language and/or other modalities']",1,"We propose a simple but effective modular approach MOPA (Modular ObjectNav with PointGoal agents) to systematically investigate the inherent modularity of the object navigation task in Embodied AI. MOPA consists of four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore the environment, and (d) a navigation module to move to identified target objects. We show that we can effectively reuse a pretrained PointGoal agent as the navigation model instead of learning to navigate from scratch, thus saving time and compute. We also compare various exploration strategies for MOPA and find that a simple uniform strategy significantly outperforms more advanced exploration methods."
MPT: Mesh Pre-Training With Transformers for Human Pose and Mesh Reconstruction,"Kevin Lin, Chung-Ching Lin, Lin Liang, Zicheng Liu, Lijuan Wang",Microsoft,0.0,,100.0,USA,"Traditional methods of reconstructing 3D human pose and mesh from single images rely on paired image-mesh datasets, which can be difficult and expensive to obtain. Due to this limitation, model scalability is constrained as well as reconstruction performance. Towards addressing the challenge, we introduce Mesh Pre-Training (MPT), an effective pre-training strategy that leverages large amounts of MoCap data to effectively perform pre-training at scale. We introduce the use of MoCap-generated heatmaps as input representations to the mesh regression transformer and propose a Masked Heatmap Modeling approach for improving pre-training performance. This study demonstrates that pre-training using the proposed MPT allows our models to perform effective inference without requiring fine-tuning. We further show that fine-tuning the pre-trained MPT model considerably improves the accuracy of human mesh reconstruction from single images. Experimental results show that MPT outperforms previous state-of-the-art methods on Human3.6M and 3DPW datasets. As a further application, we benchmark and study MPT on the task of 3D hand reconstruction, showing that our generic pre-training scheme generalizes well to hand pose estimation and achieves promising reconstruction performance.",https://openaccess.thecvf.com/content/WACV2024/html/Lin_MPT_Mesh_Pre-Training_With_Transformers_for_Human_Pose_and_Mesh_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lin_MPT_Mesh_Pre-Training_With_Transformers_for_Human_Pose_and_Mesh_WACV_2024_paper.pdf,,,2211.13357,main,Poster,https://ieeexplore.ieee.org/document/10483700/,"['Heating systems', 'Computer vision', 'Three-dimensional displays', 'Scalability', 'Computational modeling', 'Pose estimation', 'Benchmark testing']","['Human Pose', 'Mesh Reconstruction', 'Single Image', '3D Mesh', 'Pose Estimation', 'Input Representation', 'Reconstruction Performance', '3D Pose', 'Training Data', 'Feature Maps', '3D Reconstruction', '3D Space', 'RGB Images', 'Transformer Model', 'Motion Analysis', 'Parametric Approach', 'Graph Convolutional Network', 'Human Motion', 'Target Dataset', '3D Graph', '3D Joint', 'Body Joints', 'Virtual Camera', 'Mesh Vertices', 'Vertex Coordinates', '2D Pose', 'Fine-tuning Stage', 'Pre-training Stage', 'Human Pose Estimation', '3D Datasets']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"Traditional methods of reconstructing 3D human pose and mesh from single images rely on paired image-mesh datasets, which can be difficult and expensive to obtain. Due to this limitation, model scalability is constrained as well as reconstruction performance. Towards addressing the challenge, we introduce Mesh Pre-Training (MPT), an effective pre-training strategy that leverages large amounts of MoCap data to effectively perform pre-training at scale. We introduce the use of MoCap-generated heatmaps as input representations to the mesh regression transformer and propose a Masked Heatmap Modeling approach for improving pre-training performance. This study demonstrates that pre-training using the proposed MPT allows our models to perform effective inference without requiring fine-tuning. We further show that fine-tuning the pre-trained MPT model considerably improves the accuracy of human mesh reconstruction from single images. Experimental results show that MPT outperforms previous state-of-the-art methods on Human3.6M and 3DPW datasets. As a further application, we benchmark and study MPT on the task of 3D hand reconstruction, showing that our generic pre-training scheme generalizes well to hand pose estimation and achieves promising reconstruction performance."
MS-EVS: Multispectral Event-Based Vision for Deep Learning Based Face Detection,"Saad Himmi, Vincent Parret, Ajad Chhatkuli, Luc Van Gool","Computer Vision Laboratory, ETH Zurich, Switzerland; Stuttgart Laboratory 1, Sony Semiconductor Solutions Europe, Sony Europe B.V.",100.0,"Germany, Switzerland",0.0,,"Event-based sensing is a relatively new imaging modality that enables low latency, low power, high temporal resolution and high dynamic range acquisition. These properties make it a highly desirable sensor for edge applications and in high dynamic range environments. As of today, most event-based sensors are monochromatic (grayscale), capturing light from a wide spectral range over the visible, in a single channel. In this paper, we introduce multispectral events and study their advantages. In particular, we consider multiple bands in the visible and near-infrared range, and explore their potential compared to monochromatic events and conventional multispectral imaging for the face detection task. We further release the first large scale bimodal face detection datasets, with RGB videos and their simulated color events, N-MobiFace and N-YoutubeFaces, and a smaller dataset with multispectral videos and events, N-SpectralFace. We find that early fusion of multispectral events significantly improves the face detection performance, compared to the early fusion of conventional multispectral images. This result shows that polychromatic events carry relatively more useful information about the scene than conventional multispectral/color images do, with respect to their monochromatic equivalent. To the best of our knowledge, our proposed method is the first exploratory research on multispectral events, specifically including near infrared data.",https://openaccess.thecvf.com/content/WACV2024/html/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Himmi_MS-EVS_Multispectral_Event-Based_Vision_for_Deep_Learning_Based_Face_Detection_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483790/,"['Image color analysis', 'Multispectral imaging', 'Prototypes', 'Gray-scale', 'Sensors', 'High dynamic range', 'Face detection']","['Deep Learning', 'Face Detection', 'Event-based Vision', 'Dynamic Range', 'Conventional Imaging', 'Single Channel', 'Multispectral Images', 'Low Latency', 'Multiple Bands', 'Visible And Near-infrared', 'Simulated Events', 'High Dynamic Range', 'Visible Light', 'Spectral Bands', 'Object Detection', 'Large-scale Datasets', 'Real Effect', 'Face Recognition', 'Grayscale Images', 'Hyperspectral', 'Dynamic Vision Sensor', 'Multispectral Data', 'Multispectral Camera', 'Intersection Over Union Threshold', 'Long-pass Filter', 'Multispectral Bands', 'Mean Average Precision', 'Conventional Camera', 'Visible Bands', 'Channel Performance']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Applications', 'Embedded sensing / real-time techniques']",1,"Event-based sensing is a relatively new imaging modality that enables low latency, low power, high temporal resolution and high dynamic range acquisition. These properties make it a highly desirable sensor for edge applications and in high dynamic range environments. As of today, most event-based sensors are monochromatic (grayscale), capturing light from a wide spectral range over the visible, in a single channel. In this paper, we introduce multispectral events and study their advantages. In particular, we consider multiple bands in the visible and near-infrared range, and explore their potential compared to monochromatic events and conventional multispectral imaging for the face detection task. We further release the first large scale bimodal face detection datasets, with RGB videos and their simulated color events, N-MobiFace and N-YoutubeFaces, and a smaller dataset with multispectral videos and events, N-SpectralFace. We find that early fusion of multispectral events significantly improves the face detection performance, compared to the early fusion of conventional multi-spectral images. This result shows that multispectral events carry relatively more useful information about the scene than conventional multispectral images do, with respect to their grayscale equivalent. To the best of our knowledge, our proposed method is the first exploratory research on multispectral events, specifically including near infrared data."
MSCC: Multi-Scale Transformers for Camera Calibration,"Xu Song, Hao Kang, Atsunori Moteki, Genta Suzuki, Yoshie Kobayashi, Zhiming Tan","Fujitsu Limited; Fujitsu R&D Center Co., Ltd.",50.0,USA,50.0,Japan,"Camera calibration is very important for some vision tasks, like rendering 3D scenes, environment reconstruction, and self-localization, etc. In this paper, we propose a framework of multi-scale transformers for camera calibration. With the input of a single image, the multi-scale features output from the model's backbone are utilized to estimate camera parameters. At the same time, we show that the way of coarse-to-fine is effective to locate global structures and detailed features in the image, by studying the attention response of horizon line estimation. Moreover, deep supervision is proven to get more precise results and accelerated training. Our method outperforms all the state-of-the-art methods by objective and subjective experiments on Google Street View dataset and Pano360.",https://openaccess.thecvf.com/content/WACV2024/html/Song_MSCC_Multi-Scale_Transformers_for_Camera_Calibration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Song_MSCC_Multi-Scale_Transformers_for_Camera_Calibration_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483610/,"['Training', 'Three-dimensional displays', 'Cameras', 'Transformers', 'Rendering (computer graphics)', 'Calibration', 'Internet']","['Camera Calibration', 'Image Features', 'Single Image', 'Global Features', 'Vision Tasks', 'Multi-scale Features', 'Deep Supervision', 'Solid Line', 'Convolutional Neural Network', 'Input Image', 'Vertical Line', 'Focal Length', 'Global Information', 'Feed-forward Network', 'High-level Features', 'Line Segment', 'Low-level Features', 'Attention Map', 'Intrinsic Parameters', 'Transformer Encoder', 'Extrinsic Parameters', 'Line Loss', 'World Coordinate System', 'Geometric Cues', 'Outdoor Scenes', 'Camera Model', 'Decoder Layer', 'Field Of View', 'Good Generalization Capability']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",1,"Camera calibration is very important for some vision tasks, like rendering 3D scenes, environment reconstruction, and self-localization, etc. In this paper, we propose a framework of multi-scale transformers for camera calibration. With the input of a single image, the multi-scale features output from the model’s backbone are utilized to estimate camera parameters. At the same time, we show that the way of coarse-to-fine is effective to locate global structures and detailed features in the image, by studying the attention response of horizon line estimation. Moreover, deep supervision is proven to get more precise results and accelerated training. Our method outperforms all the state-of-the-art methods by objective and subjective experiments on Google Street View dataset and Pano360."
MagneticPillars: Efficient Point Cloud Registration Through Hierarchized Birds-Eye-View Cell Correspondence Refinement,"Kai Fischer, Martin Simon, Stefan Milz, Patrick Mäder","Valeo Schalter und Sensoren GmbH, Ilmenau University of Technology; Spleenlab GmbH, Ilmenau University of Technology; Ilmenau University of Technology",100.0,Germany,0.0,,"Recent point cloud registration approaches often deal with a consecutive determination of coarse and fine feature correspondences for hierarchical pose refinement. Due to the unordered nature of point clouds, a common way to generate a subsampled representation for the coarse matching step is by applying 3D-sensitive convolution approaches. However, expensive grouping mechanisms such as nearest neighbour search have to be used to determine the associated fine features, generating individual associations for each point cloud and leading to an increased overall runtime. Furthermore current methods often tend to predict deficient point correspondences and rely on additional filtering by expensive registration backends like RANSAC impeding their application in time critical systems. To overcome these challenges, we present MagneticPillars utilizing a Birds-Eye-View (BEV) grid representation, entailing fixed affiliations between coarse and fine feature cells. We show that by extracting correspondences in this manner, a small amount of key points is already sufficient to achieve an accurate pose estimation without external optimization methods like RANSAC. We evaluate our approach on two autonomous driving datasets for the task of point cloud registration by applying SVD as the backend, where we outperform recent state-of-the-art methods, reducing the rotation and translation error by 12% and 40%, respectively, and to top it all off, cutting runtime in half.",https://openaccess.thecvf.com/content/WACV2024/html/Fischer_MagneticPillars_Efficient_Point_Cloud_Registration_Through_Hierarchized_Birds-Eye-View_Cell_Correspondence_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fischer_MagneticPillars_Efficient_Point_Cloud_Registration_Through_Hierarchized_Birds-Eye-View_Cell_Correspondence_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
MarsLS-Net: Martian Landslides Segmentation Network and Benchmark Dataset,"Sidike Paheding, Abel A. Reyes, A. Rajaneesh, K.S. Sajinkumar, Thomas Oommen",Michigan Technological University; University of Mississippi; Fairfield University; University of Kerala,100.0,"India, USA",0.0,,"Martian landslide segmentation is a challenging task compared to the same task on Earth. One of the reasons is that vegetation is typically lost or significantly less compared to its surroundings in the regions of landslide on Earth. In contrast, Mars is a desert planet, and there is no vegetation to aid landslide detection and segmentation. Recent work has demonstrated the strength of vision transformer (ViT) based deep learning models for various computer vision tasks. Inspired by the multi-head attention mechanism in ViT, which can model the global long-range spatial correlation between local regions in the input image, we hypothesize self-attention mechanism can effectively capture pertinent contextual information for the Martian landslide segmentation task. Furthermore, considering parameter efficiency or model size is another important factor for deep learning algorithms, we construct a new feature representation block, namely Progressively Expanded Neuron Attention (PEN-Attention), to extract more relevant features with significantly fewer trainable parameters. Overall, we refer to our deep learning architecture as the Martian landslide segmentation network (MarsLS-Net). In addition to the new architecture, we introduce a new multi-modal Martian landslide segmentation dataset for the first time, which will be made publicly available at https://github.com/MAIN-Lab/Multimodal-Martian-Landslides-Dataset",https://openaccess.thecvf.com/content/WACV2024/html/Paheding_MarsLS-Net_Martian_Landslides_Segmentation_Network_and_Benchmark_Dataset_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Paheding_MarsLS-Net_Martian_Landslides_Segmentation_Network_and_Benchmark_Dataset_WACV_2024_paper.pdf,,https://github.com/MAIN-Lab/Multimodal-Martian-Landslides-Dataset,,main,Poster,https://ieeexplore.ieee.org/document/10483716/,"['Earth', 'Deep learning', 'Image segmentation', 'Computational modeling', 'Neurons', 'Vegetation mapping', 'Computer architecture']","['Deep Learning', 'Deep Models', 'Attention Mechanism', 'Segmentation Task', 'Deep Learning Architectures', 'Self-attention Mechanism', 'Vision Transformer', 'Multi-head Attention Mechanism', 'Convolutional Network', 'Convolutional Neural Network', 'Machine Learning Methods', 'Feature Maps', 'Digital Elevation Model', 'Normalized Difference Vegetation Index', 'Segmentation Model', 'Latent Space', 'Deep Network Architecture', 'Debris Flow', 'Traditional Machine Learning Methods', 'Performance Of Architectures', 'Object-based Image Analysis', 'Multi-head Self-attention', 'Rockslide', 'Landslide Mapping', 'Progressive Expansion', 'Landslide Types', 'RGB Data', 'Very High Resolution', 'Morphological Operations', 'Landslide Inventory']","['Applications', 'Remote Sensing', 'Applications', 'Environmental monitoring / climate change / ecology']",1,"Martian landslide segmentation is a challenging task compared to the same task on Earth. One of the reasons is that vegetation is typically lost or significantly less compared to its surroundings in the regions of landslide on Earth. In contrast, Mars is a desert planet, and there is no vegetation to aid landslide detection and segmentation. Recent work has demonstrated the strength of vision transformer (ViT) based deep learning models for various computer vision tasks. Inspired by the multi-head attention mechanism in ViT, which can model the global longrange spatial correlation between local regions in the input image, we hypothesize self-attention mechanism can effectively capture pertinent contextual information for the Martian landslide segmentation task. Furthermore, considering parameter efficiency or model size is another important factor for deep learning algorithms, we construct a new feature representation block, namely Progressively Expanded Neuron Attention (PEN-Attention), to extract more relevant features with significantly fewer trainable parameters. Overall, we refer to our deep learning architecture as the Martian landslide segmentation network (MarsLS-Net). In addition to the new architecture, we introduce a new multi-modal Martian landslide segmentation dataset for the first time, which will be made publicly available at https://github.com/MAIN-Lab/Multimodal-Martian-Landslides-Dataset"
MaskConver: Revisiting Pure Convolution Model for Panoptic Segmentation,"Abdullah Rashwan, Jiageng Zhang, Ali Taalimi, Fan Yang, Xingyi Zhou, Chaochao Yan, Liang-Chieh Chen, Yeqing Li",ByteDance; Google,0.0,,100.0,China,"In recent years, transformer-based models have dominated panoptic segmentation, thanks to their strong modeling capabilities and their unified representation for both semantic and instance classes as global binary masks. In this paper, we revisit pure convolution model and propose a novel panoptic architecture named MaskConver. MaskConver proposes to fully unify things and stuff representation by predicting their centers. To that extent, it creates a lightweight class embedding module that can break the ties when multiple centers co-exist in the same location. Furthermore, our study shows that the decoder design is critical in ensuring that the model has sufficient context for accurate detection and segmentation. We introduce a powerful ConvNeXt-UNet decoder that closes the performance gap between convolution- and transformer-based models. With ResNet50 backbone, our MaskConver achieves 53.6% PQ on the COCO panoptic val set, out-performing the modern convolution-based model, Panoptic FCN, by 9.3% as well as transformer-based models such as Mask2Former (+1.7% PQ) and kMaX-DeepLab (+0.6% PQ). Additionally, MaskConver with a MobileNet backbone reaches 37.2% PQ, improving over Panoptic-DeepLab by +6.4% under the same FLOPs/latency constraints. A further optimized version of MaskConver achieves 29.7% PQ, while running in real-time on mobile devices. The code and model weights will be publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rashwan_MaskConver_Revisiting_Pure_Convolution_Model_for_Panoptic_Segmentation_WACV_2024_paper.pdf,,https://github.com/tensorflow/models/tree/master/official/projects/maskconver,2312.06052,main,Poster,https://ieeexplore.ieee.org/document/10483637/,"['Convolutional codes', 'Convolution', 'Semantics', 'Transformers', 'Vectors', 'Real-time systems', 'Mobile handsets']","['Panoptic Segmentation', 'Mobile Devices', 'ResNet-50 Backbone', 'Convolutional Neural Network', 'Convolutional Layers', 'Central Point', 'Bounding Box', 'Unique Way', 'Semantic Segmentation', 'Segmentation Task', 'Channel Size', 'Feature Pyramid', 'Feature Pyramid Network', 'Non-maximum Suppression', 'COCO Dataset', 'Long-range Information', 'Depthwise Convolution', 'Transformer Block', 'High-level Semantics', 'L5 Level', 'Prediction Head', 'Larger Kernel Size', 'Transformer Decoder', 'Object Detection', 'Center Of Box', 'Feature Maps', 'Center Of The Bounding Box']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"In recent years, transformer-based models have dominated panoptic segmentation, thanks to their strong modeling capabilities and their unified representation for both semantic and instance classes as global binary masks. In this paper, we revisit pure convolution model and propose a novel panoptic architecture named MaskConver. MaskConver proposes to fully unify things and stuff representation by predicting their centers. To that extent, it creates a lightweight class embedding module that can break the ties when multiple centers co-exist in the same location. Furthermore, our study shows that the decoder design is critical in ensuring that the model has sufficient context for accurate detection and segmentation. We introduce a powerful ConvNeXt-UNet decoder that closes the performance gap between convolution- and transformer-based models. With ResNet50 backbone, our MaskConver achieves 53.6% PQ on the COCO panoptic val set, outperforming the modern convolution-based model, Panoptic FCN, by 9.3% as well as transformer-based models such as Mask2Former (+1.7% PQ) and kMaX-DeepLab (+0.6% PQ). Additionally, MaskConver with a MobileNet backbone reaches 37.2% PQ, improving over Panoptic-DeepLab by +6.4% under the same FLOPs/latency constraints. A further optimized version of MaskConver achieves 29.7% PQ, while running in real-time on mobile devices. The code and model weights will be publicly available.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Masked Collaborative Contrast for Weakly Supervised Semantic Segmentation,"Fangwen Wu, Jingxuan He, Yufei Yin, Yanbin Hao, Gang Huang, Lechao Cheng","University of Science and Technology of China, Hefei, China; Zhejiang Lab, Hangzhou, China; Zhejiang University, Hangzhou, China",100.0,China,0.0,,"This study introduces an efficacious approach, Masked Collaborative Contrast (MCC), to highlight semantic regions in weakly supervised semantic segmentation. MCC adroitly draws inspiration from masked image modeling and contrastive learning to devise a novel framework that induces keys to contract toward semantic regions. Unlike prevalent techniques that directly eradicate patch regions in the input image when generating masks, we scrutinize the neighborhood relations of patch tokens by exploring masks considering keys on the affinity matrix. Moreover, we generate positive and negative samples in contrastive learning by utilizing the masked local output and contrasting it with the global output. Elaborate experiments on commonly employed datasets evidences that the proposed MCC mechanism effectively aligns global and local perspectives within the image, attaining impressive performance. The source code is available at https://github.com/fwu11/MCC.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_Masked_Collaborative_Contrast_for_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_Masked_Collaborative_Contrast_for_Weakly_Supervised_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/fwu11/MCC,2305.08491,main,Poster,https://ieeexplore.ieee.org/document/10484051/,"['Computer vision', 'Semantic segmentation', 'Source coding', 'Computational modeling', 'Semantics', 'Collaboration', 'Self-supervised learning']","['Semantic Segmentation', 'Weakly Supervised Semantic Segmentation', 'Input Image', 'Self-supervised Learning', 'Affinity Matrix', 'Local Information', 'Multivariate Methods', 'Latent Space', 'Segmentation Performance', 'Saliency Map', 'Pseudo Labels', 'Loss Of Affinity', 'Class Activation Maps', 'Transformer Layers', 'Transformer Encoder', 'Local View', 'Vision Transformer', 'Segmentation Prediction', 'MS COCO Dataset', 'Transformer Block', 'PASCAL VOC Dataset', 'Image-level Labels', 'VOC Dataset', 'Momentum Factor', 'Local Instance', 'Image Object', 'Intermediate Layer', 'Attention Map', 'Contrastive Loss', 'Segmentation Labels']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"This study introduces an efficacious approach, Masked Collaborative Contrast (MCC), to highlight semantic regions in weakly supervised semantic segmentation. MCC adroitly draws inspiration from masked image modeling and contrastive learning to devise a novel framework that induces keys to contract toward semantic regions. Unlike prevalent techniques that directly eradicate patch regions in the input image when generating masks, we scrutinize the neighborhood relations of patch tokens by exploring masks considering keys on the affinity matrix. Moreover, we generate positive and negative samples in contrastive learning by utilizing the masked local output and contrasting it with the global output. Elaborate experiments on commonly employed datasets evidences that the proposed MCC mechanism effectively aligns global and local perspectives within the image, attaining impressive performance. The source code is available at https://github.com/fwu11/MCC."
Masked Event Modeling: Self-Supervised Pretraining for Event Cameras,"Simon Klenk, David Bonello, Lukas Koestler, Nikita Araslanov, Daniel Cremers","Technical University of Munich, Munich Center for Machine Learning",100.0,Germany,0.0,,"Event cameras asynchronously capture brightness changes with low latency, high temporal resolution, and high dynamic range. However, annotation of event data is a costly and laborious process, which limits the use of deep learning methods for classification and other semantic tasks with the event modality. To reduce the dependency on labeled event data, we introduce Masked Event Modeling (MEM), a self-supervised framework for events. Our method pretrains a neural network on unlabeled events, which can originate from any event camera recording. Subsequently, the pretrained model is finetuned on a downstream task, leading to a consistent improvement of the task accuracy. For example, our method reaches state-of-the-art classification accuracy across three datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy of previous work by significant margins. When tested on real-world event data, MEM is even superior to supervised RGB-based pretraining. The models pretrained with MEM are also label-efficient and generalize well to the dense task of semantic image segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Klenk_Masked_Event_Modeling_Self-Supervised_Pretraining_for_Event_Cameras_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Klenk_Masked_Event_Modeling_Self-Supervised_Pretraining_for_Event_Cameras_WACV_2024_paper.pdf,,https://github.com/tum-vision/mem,2212.10368,main,Poster,https://ieeexplore.ieee.org/document/10484185/,"['Micromechanical devices', 'Semantic segmentation', 'Semantics', 'Training data', 'Self-supervised learning', 'Cameras', 'Data models']","['Dynamic Vision Sensor', 'Self-supervised Pretraining', 'Neural Network', 'Fine-tuned', 'Classification Task', 'Event Data', 'Semantic Segmentation', 'Low Latency', 'High Dynamic Range', 'Semantic Segmentation Task', 'Brightness Changes', 'Top-1 Accuracy', 'Classification Of Samples', 'ImageNet', 'Object Classification', 'Latent Space', 'Patch Size', 'Low Power Consumption', 'Optical Flow', 'Largest Dataset', 'Graph Neural Networks', 'Self-supervised Learning', 'Pretext Task', 'RGB Data', 'Input Patch', 'Random Initialization', 'Pre-training Phase', 'Pre-training Stage', 'Domain Gap', 'Evidence Lower Bound']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Event cameras asynchronously capture brightness changes with low latency, high temporal resolution, and high dynamic range. However, annotation of event data is a costly and laborious process, which limits the use of deep learning methods for classification and other semantic tasks with the event modality. To reduce the dependency on labeled event data, we introduce Masked Event Modeling (MEM), a self-supervised framework for events. Our method pretrains a neural network on unlabeled events, which can originate from any event camera recording. Subsequently, the pretrained model is finetuned on a downstream task, leading to a consistent improvement of the task accuracy. For example, our method reaches state-of-the-art classification accuracy across three datasets, N-ImageNet, N-Cars, and N-Caltech101, increasing the top-1 accuracy of previous work by significant margins. When tested on real-world event data, MEM is even superior to supervised RGB-based pretraining. The models pretrained with MEM are also label-efficient and generalize well to the dense task of semantic image segmentation."
"Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where","Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu",IBM Research; National Yang Ming Chiao Tung University,50.0,Taiwan,50.0,USA,"While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for one view in a contrastive sample-pair the randomly-sampled masking regions could be overly concentrated on important/salient objects thus resulting in misleading contrastiveness to the other view. To this end, we propose to explicitly take the saliency constraint into consideration in which the masked regions are more evenly distributed among the foreground and background for realizing the masking-based augmentation. Moreover, we introduce hard negative samples by masking larger regions of salient patches in an input image. Extensive experiments conducted on various datasets, contrastive learning mechanisms, and downstream tasks well verify the efficacy as well as the superior performance of our proposed method with respect to several state-of-the-art baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Chin_Masking_Improves_Contrastive_Self-Supervised_Learning_for_ConvNets_and_Saliency_Tells_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chin_Masking_Improves_Contrastive_Self-Supervised_Learning_for_ConvNets_and_Saliency_Tells_WACV_2024_paper.pdf,,https://github.com/joycenerd/Saliency-Guided-Masking-for-ConvNets,2309.12757,main,Poster,https://ieeexplore.ieee.org/document/10483950/,"['Training', 'Computer vision', 'Codes', 'Self-supervised learning', 'Transformers', 'Tokenization', 'Convolutional neural networks']","['Convolutional Neural Network', 'Self-supervised Learning', 'Input Image', 'Negative Samples', 'Learning Strategies', 'Augmentation Methods', 'General Operations', 'Masked Images', 'Vision Transformer', 'Positive Samples', 'Local Network', 'Object Detection', 'High-pass Filter', 'Positive View', 'Image Patches', 'Instance Segmentation', 'Salient Information', 'Feature Encoder', 'Saliency Map', 'Image X', 'Masking Strategy', 'Contrast Objective', 'Siamese Network', 'COCO Dataset', 'Foreground Objects', 'Original Input Image', 'Key Branch', 'Background Objects', 'Pretext Task', 'Self-supervised Learning Methods']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for one view in a contrastive sample-pair the randomly-sampled masking regions could be overly concentrated on important/salient objects thus resulting in misleading contrastiveness to the other view. To this end, we propose to explicitly take the saliency constraint into consideration in which the masked regions are more evenly distributed among the foreground and background for realizing the masking-based augmentation. Moreover, we introduce hard negative samples by masking larger regions of salient patches in an input image. Extensive experiments conducted on various datasets, contrastive learning mechanisms, and downstream tasks well verify the efficacy as well as the superior performance of our proposed method with respect to several state-of-the-art baselines. Our code is publicly available at: https://github.com/joycenerd/Saliency-Guided-Masking-for-ConvNets"
Maximum Knowledge Orthogonality Reconstruction With Gradients in Federated Learning,"Feng Wang, Senem Velipasalar, M. Cenk Gursoy","EECS Department, Syracuse University, Syracuse, NY, 13244",100.0,USA,0.0,,"Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work, revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and completely analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' input data. Our proposed method reconstructs a mathematically proven high quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct the input images from clients' gradient updates. We evaluate MKOR's performance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with the state-of-the-art works. The results show that MKOR outperforms the existing approaches, and draws attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Maximum_Knowledge_Orthogonality_Reconstruction_With_Gradients_in_Federated_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Maximum_Knowledge_Orthogonality_Reconstruction_With_Gradients_in_Federated_Learning_WACV_2024_paper.pdf,,https://github.com/wfwf10/MKOR,2310.19222,main,Poster,https://ieeexplore.ieee.org/document/10484110/,"['Data privacy', 'Privacy', 'Federated learning', 'Computational modeling', 'Neural networks', 'Pressing', 'Reconstruction algorithms']","['Federated Learning', 'Neural Network', 'Analysis Approach', 'Batch Size', 'Network Parameters', 'Privacy Protection', 'ImageNet Dataset', 'Small Batch', 'MNIST Dataset', 'Gradient Update', 'Considerable Amount Of Work', 'CIFAR-100 Dataset', 'Convolutional Neural Network', 'Convolutional Layers', 'Local Minima', 'Multiple Layers', 'Generative Adversarial Networks', 'Fully-connected Layer', 'Peak Signal-to-noise Ratio', 'Major Parameters', 'Optimization-based Methods', 'Structural Similarity Index Measure', 'Input Channels', 'Original Architecture', 'Differential Privacy', 'Federated Learning Framework', 'Input Regions', 'VGG-16 Network', 'Parameter Server', 'Threat Model']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Federated learning (FL) aims at keeping client data local to preserve privacy. Instead of gathering the data itself, the server only collects aggregated gradient updates from clients. Following the popularity of FL, there has been considerable amount of work revealing the vulnerability of FL approaches by reconstructing the input data from gradient updates. Yet, most existing works assume an FL setting with unrealistically small batch size, and have poor image quality when the batch size is large. Other works modify the neural network architectures or parameters to the point of being suspicious, and thus, can be detected by clients. Moreover, most of them can only reconstruct one sample input from a large batch. To address these limitations, we propose a novel and analytical approach, referred to as the maximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients' data. Our proposed method reconstructs a mathematically proven high-quality image from large batches. MKOR only requires the server to send secretly modified parameters to clients and can efficiently and inconspicuously reconstruct images from clients' gradient updates. We evaluate MKOR’s performance on MNIST, CIFAR-100, and ImageNet datasets and compare it with the state-of-the-art baselines. The results show that MKOR outperforms the existing approaches, and draw attention to a pressing need for further research on the privacy protection of FL so that comprehensive defense approaches can be developed. The code is available at: https://github.com/wfwf10/MKOR."
Med-DANet V2: A Flexible Dynamic Architecture for Efficient Medical Volumetric Segmentation,"Haoran Shen, Yifu Zhang, Wenxuan Wang, Chen Chen, Jing Liu, Shanshan Song, Jiangyun Li","School of Automation and Electrical Engineering, University of Science and Technology Beijing; National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; Center for Research in Computer Vision, University of Central Florida",100.0,"China, USA",0.0,,"Recent works have shown that the computational efficiency of 3D medical image (e.g. CT and MRI) segmentation can be impressively improved by dynamic inference based on slice-wise complexity. As a pioneering work, a dynamic architecture network for medical volumetric segmentation (i.e. Med-DANet) has achieved a favorable accuracy and efficiency trade-off by dynamically selecting a suitable 2D candidate model from the pre-defined model bank for different slices. However, the issues of incomplete data analysis, high training costs, and the two-stage pipeline in Med-DANet require further improvement. To this end, this paper further explores a unified formulation of the dynamic inference framework from the perspective of both the data itself and the model structure. For each slice of the input volume, our proposed method dynamically selects an important foreground region for segmentation based on the policy generated by our Decision Network and Crop Position Network. Besides, we propose to insert a stage-wise quantization selector to the employed segmentation model (e.g. U-Net) for dynamic architecture adapting. Extensive experiments on BraTS 2019 and 2020 show that our method achieves comparable or better performance than previous state-of-the-art methods with much less model complexity. Compared with previous methods Med-DANet and TransBTS with dynamic and static architecture respectively, our framework improves the model efficiency by up to nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019. Code will be available at https://github.com/Rubics-Xuan/Med-DANet.",https://openaccess.thecvf.com/content/WACV2024/html/Shen_Med-DANet_V2_A_Flexible_Dynamic_Architecture_for_Efficient_Medical_Volumetric_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shen_Med-DANet_V2_A_Flexible_Dynamic_Architecture_for_Efficient_Medical_Volumetric_WACV_2024_paper.pdf,,https://github.com/Rubics-Xuan/Med-DANet,2310.18656,main,Poster,https://ieeexplore.ieee.org/document/10483875/,"['Training', 'Adaptation models', 'Image segmentation', 'Three-dimensional displays', 'Quantization (signal)', 'Magnetic resonance imaging', 'Pipelines']","['Flexible Architecture', 'Volumetric Segmentation', 'Dynamic Architecture', 'Magnetic Resonance Imaging', 'Complex Models', 'Medical Imaging', 'Image Segmentation', 'Dynamic Network', 'Extensive Experiments', 'Segmentation Model', 'Segmentation Results', 'Candidate Models', 'Medical Image Segmentation', 'Dynamic Selection', 'Foreground Regions', 'Decision Network', 'Dynamic Inference', 'Computational Complexity', 'Medical Data', '3D Network', 'Dynamic Adaptation', 'Cropped Images', 'Policy Network', 'Brain Tumor Segmentation', 'Quantization Bits', 'Dice Score', 'Hausdorff Distance', '3D Magnetic Resonance Imaging', 'Segmentation Task', 'Candidate Network']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",,"Recent works have shown that the computational efficiency of 3D medical image (e.g. CT and MRI) segmentation can be impressively improved by dynamic inference based on slice-wise complexity. As a pioneering work, a dynamic architecture network for medical volumetric segmentation (i.e. Med-DANet [44]) has achieved a favorable accuracy and efficiency trade-off by dynamically selecting a suitable 2D candidate model from the pre-defined model bank for different slices. However, the issues of incomplete data analysis, high training costs, and the two-stage pipeline in Med-DANet require further improvement. To this end, this paper further explores a unified formulation of the dynamic inference framework from the perspective of both the data itself and the model structure. For each slice of the input volume, our proposed method dynamically selects an important foreground region for segmentation based on the policy generated by our Decision Network and Crop Position Network. Besides, we propose to insert a stage-wise quantization selector to the employed segmentation model (e.g. U-Net) for dynamic architecture adapting. Extensive experiments on BraTS 2019 and 2020 show that our method achieves comparable or better performance than previous state-of-the-art methods with much less model complexity. Compared with previous methods Med-DANet and TransBTS with dynamic and static architecture respectively, our framework improves the model efficiency by up to nearly 4.1 and 17.3 times with comparable segmentation results on BraTS 2019. Code will be available at https://github.com/Rubics-Xuan/Med-DANet."
Membership Inference Attack Using Self Influence Functions,"Gilad Cohen, Raja Giryes",Tel Aviv University,100.0,Israel,0.0,,"Member inference (MI) attacks aim to determine if a specific data sample was used to train a machine learning model. Thus, MI is a major privacy threat to models trained on private sensitive data, such as medical records. In MI attacks one may consider the black-box settings, where the model's parameters and activations are hidden from the adversary, or the white-box case where they are available to the attacker. In this work, we focus on the latter and present a novel MI attack for it that employs influence functions, or more specifically the samples' self-influence scores, to perform MI prediction. The proposed method is evaluated on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets using various architectures such as AlexNet, ResNet, and DenseNet. Our new attack method achieves new state-of-the-art (SOTA) results for MI even with limited adversarial knowledge, and is effective against MI defense methods such as data augmentation and differential privacy. Our code is available at https: //github.com/giladcohen/sif_mi_attack.",https://openaccess.thecvf.com/content/WACV2024/html/Cohen_Membership_Inference_Attack_Using_Self_Influence_Functions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cohen_Membership_Inference_Attack_Using_Self_Influence_Functions_WACV_2024_paper.pdf,,https://github.com/giladcohen/sif_mi_attack,2205.13680,main,Poster,https://ieeexplore.ieee.org/document/10484386/,"['Training', 'Differential privacy', 'Estimation', 'Computer architecture', 'Machine learning', 'Predictive models', 'Data augmentation']","['Inference Attacks', 'Membership Inference', 'Membership Inference Attacks', 'Machine Learning Models', 'Data Augmentation', 'Specific Sample', 'AlexNet', 'Differential Privacy', 'Training Set', 'Training Data', 'Test Samples', 'Learning Algorithms', 'Computation Time', 'Deep Neural Network', 'Validation Set', 'Inference Time', 'Types Of Attacks', 'Target Model', 'Training Set Size', 'Balanced Accuracy', 'Black-box Attacks', 'Empirical Risk', 'Image X', 'Adversarial Attacks', 'Attack Performance', 'Backward Pass', 'Real Setup', 'Probability Vector', 'White-box Attack']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Member inference (MI) attacks aim to determine if a specific data sample was used to train a machine learning model. Thus, MI is a major privacy threat to models trained on private sensitive data, such as medical records. In MI attacks one may consider the black-box settings, where the model’s parameters and activations are hidden from the adversary, or the white-box case where they are available to the attacker. In this work, we focus on the latter and present a novel MI attack for it that employs influence functions, or more specifically the samples’ self-influence scores, to perform MI prediction. The proposed method is evaluated on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets using various architectures such as AlexNet, ResNet, and DenseNet. Our new attack method achieves new state-of-the-art (SOTA) results for MI even with limited adversarial knowledge, and is effective against MI defense methods such as data augmentation and differential privacy. Our code is available at https://github.com/giladcohen/sif_mi_attack."
Meta-Learned Attribute Self-Interaction Network for Continual and Generalized Zero-Shot Learning,"Vinay Verma, Nikhil Mehta, Kevin J. Liang, Aakansha Mishra, Lawrence Carin","IITG; KAUST; FAIR, Meta; Duke University",75.0,"India, Saudi Arabia, USA",25.0,USA,"Zero-shot learning (ZSL) is a promising approach to generalizing a model to categories unseen during training by leveraging class attributes, but challenges remain. Recently, methods using generative models to combat bias towards classes seen during training have pushed state of the art, but these generative models can be slow or computationally expensive to train. Also, these generative models assume that the attribute vector of each unseen class is available a priori at training, which is not always practical. Additionally, while many previous ZSL methods assume a one-time adaptation to unseen classes, in reality, the world is always changing, necessitating a constant adjustment of deployed models. Models unprepared to handle a sequential stream of data are likely to experience catastrophic forgetting. We propose a Meta-learned Attribute self-Interaction Network (MAIN) for continual ZSL. By pairing attribute self-interaction trained using meta-learning with inverse regularization of the attribute encoder, we are able to outperform state-of-the-art results without leveraging the unseen class attributes while also being able to train our models substantially faster (>100x) than expensive generative-based approaches. We demonstrate this with experiments on five standard ZSL datasets (CUB, aPY, AWA1, AWA2, and SUN) in the generalized zero-shot learning and continual (fixed/dynamic) zero-shot learning settings. Extensive ablations and analyses demonstrate the efficacy of various components proposed.",https://openaccess.thecvf.com/content/WACV2024/html/Verma_Meta-Learned_Attribute_Self-Interaction_Network_for_Continual_and_Generalized_Zero-Shot_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Verma_Meta-Learned_Attribute_Self-Interaction_Network_for_Continual_and_Generalized_Zero-Shot_Learning_WACV_2024_paper.pdf,,,2312.01167,main,Poster,https://ieeexplore.ieee.org/document/10484442/,"['Training', 'Adaptation models', 'Protocols', 'Zero-shot learning', 'Computational modeling', 'Reservoirs', 'Data models']","['Zero-shot', 'Generalized Zero-shot Learning', 'Learning Settings', 'Incremental Learning', 'Unseen Classes', 'Catastrophic Forgetting', 'Visual Features', 'Classification Of Samples', 'Latent Space', 'Harmonic Mean', 'Sigmoid Activation', 'Current Task', 'Unseen Data', 'Sequential Task', 'Variational Autoencoder', 'Clear Connection', 'Visual Space', 'Previous Tasks', 'Future Tasks', 'Sample Reservoir', 'Polynomial Kernel', 'Semantic Space', 'Cycle Consistency Loss', 'Conditional Variational Autoencoder', 'Memory Size']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",2,"Zero-shot learning (ZSL) is a promising approach to generalizing a model to categories unseen during training by leveraging class attributes, but challenges remain. Recently, methods using generative models to combat bias towards classes seen during training have pushed state of the art, but these generative models can be slow or computationally expensive to train. Also, these generative models assume that the attribute vector of each unseen class is available a priori at training, which is not always practical. Additionally, while many previous ZSL methods assume a one-time adaptation to unseen classes, in reality, the world is always changing, necessitating a constant adjustment of deployed models. Models unprepared to handle a sequential stream of data are likely to experience catastrophic forgetting. We propose a Meta-learned Attribute self-Interaction Network (MAIN) for continual ZSL. By pairing attribute self-interaction trained using meta-learning with inverse regularization of the attribute encoder, we are able to outperform state-of-the-art results without leveraging the unseen class attributes while also being able to train our models substantially faster (> 100×) than expensive generative-based approaches. We demonstrate this with experiments on five standard ZSL datasets (CUB, aPY, AWA1, AWA2, and SUN) in the generalized zero-shot learning and continual (fixed/dynamic) zero-shot learning settings. Extensive ablations and analyses demonstrate the efficacy of various components proposed."
Meta-Learned Kernel for Blind Super-Resolution Kernel Estimation,"Royson Lee, Rui Li, Stylianos Venieris, Timothy Hospedales, Ferenc Huszár, Nicholas D. Lane","University of Cambridge, UK; Flower Labs; Samsung AI Center, Cambridge, UK; University of Cambridge, UK; Samsung AI Center, Cambridge, UK; University of Edinburgh, UK; University of Cambridge, UK; Samsung AI Center, Cambridge, UK",87.5,UK,12.5,USA,"Recent image degradation estimation methods have enabled single-image super-resolution (SR) approaches to better upsample real-world images. Among these methods, explicit kernel estimation approaches have demonstrated unprecedented performance at handling unknown degradations. Nonetheless, a number of limitations constrain their efficacy when used by downstream SR models. Specifically, this family of methods yields i) excessive inference time due to long per-image adaptation times and ii)inferior image fidelity due to kernel mismatch. In this work, we introduce a learning-to-learn approach that meta-learns from the information contained in a distribution of images, thereby enabling significantly faster adaptation to new images with substantially improved performance in both kernel estimation and image fidelity. Specifically, we meta-train a kernel-generating GAN, named MetaKernelGAN, on a range of tasks, such that when a new image is presented, the generator starts from an informed kernel estimate and the discriminator starts with a strong capability to distinguish between patch distributions. Compared with state-of-the-art methods, our experiments show that MetaKernelGAN better estimates the magnitude and covariance of the kernel, leading to state-of-the-art blind SR results within a similar computational regime when combined with a non-blind SR model. Through supervised learning of an unsupervised learner, our method maintains the generalizability of the unsupervised learner, improves the optimization stability of kernel estimation, and hence image adaptation, and leads to a faster inference with a speedup between 14.24 to 102.1x over existing methods.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Meta-Learned_Kernel_for_Blind_Super-Resolution_Kernel_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Meta-Learned_Kernel_for_Blind_Super-Resolution_Kernel_Estimation_WACV_2024_paper.pdf,,https://github.com/royson/metakernelgan,2212.07886,main,Poster,https://ieeexplore.ieee.org/document/10484233/,"['Metalearning', 'Degradation', 'Adaptation models', 'Costs', 'Superresolution', 'Estimation', 'Generators']","['Kernel Estimation', 'Supervised Learning', 'Unsupervised Learning', 'Estimation Approach', 'Inference Time', 'Distribution Of Images', 'Real-world Images', 'Explicit Estimates', 'Distribution Of Patches', 'Single Image Super-resolution', 'Image Fidelity', 'Convolutional Neural Network', 'Hyperparameters', 'Covariance Matrix', 'High-resolution Images', 'Adaptation Process', 'Degradation Process', 'Unsupervised Methods', 'Low-resolution Images', 'Joint Optimization', 'Blur Kernel', 'Noisy Images', 'Isotropic Kernel', 'Kernel Learning', 'Adaptive Step', 'Implicit Method', 'Pixel Spacing', 'Internal Information', 'Inference Cost', 'External Image']","['Algorithms', 'Low-level and physics-based vision']",4,"Recent image degradation estimation methods have enabled single-image super-resolution (SR) approaches to better upsample real-world images. Among these methods, explicit kernel estimation approaches have demonstrated unprecedented performance at handling unknown degradations. Nonetheless, a number of limitations constrain their efficacy when used by downstream SR models. Specifically, this family of methods yields i) excessive inference time due to long per-image adaptation times and ii) inferior image fidelity due to kernel mismatch. In this work, we introduce a learning-to-learn approach that meta-learns from the information contained in a distribution of images, thereby enabling significantly faster adaptation to new images with substantially improved performance in both kernel estimation and image fidelity. Specifically, we meta-train a kernelgenerating GAN, named MetaKernelGAN, on a range of tasks, such that when a new image is presented, the generator starts from an informed kernel estimate and the discriminator starts with a strong capability to distinguish between patch distributions. Compared with state-of-the-art methods, our experiments show that MetaKernelGAN better estimates the magnitude and covariance of the kernel, leading to state-of-the-art blind SR results within a similar computational regime when combined with a non-blind SR model. Through supervised learning of an unsupervised learner, our method maintains the generalizability of the unsupervised learner, improves the optimization stability of kernel estimation, and hence image adaptation, and leads to a faster inference with a speedup between 14.24 to 102.1× over existing methods.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sup>"
MetaSeg: MetaFormer-Based Global Contexts-Aware Network for Efficient Semantic Segmentation,"Beoungwoo Kang, Seunghun Moon, Yubin Cho, Hyunwoo Yu, Suk-Ju Kang","Sogang University, Republic of Korea",100.0,South Korea,0.0,,"Beyond the Transformer, it is important to explore how to exploit the capacity of the MetaFormer, an architecture that is fundamental to the performance improvements of the Transformer. Previous studies have exploited it only for the backbone network. Unlike previous studies, we explore the capacity of the Metaformer architecture more extensively in the semantic segmentation task. We propose a powerful semantic segmentation network, MetaSeg, which leverages the Metaformer architecture from the backbone to the decoder. Our MetaSeg shows that the MetaFormer architecture plays a significant role in capturing the useful contexts for the decoder as well as for the backbone. In addition, recent segmentation methods have shown that using a CNN-based backbone for extracting the spatial information and a decoder for extracting the global information is more effective than using a transformer-based backbone with a CNN-based decoder. This motivates us to adopt the CNN-based backbone using the MetaFormer block and design our MetaFormer-based decoder, which consists of a novel self-attention module to capture the global contexts. To consider both the global contexts extraction and the computational efficiency of the self-attention for semantic segmentation, we propose a Channel Reduction Attention (CRA) module that reduces the channel dimension of the query and key into the one dimension. In this way, our proposed MetaSeg outperforms the previous state-of-the-art methods with more efficient computational costs on popular semantic segmentation and a medical image segmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse.",https://openaccess.thecvf.com/content/WACV2024/html/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kang_MetaSeg_MetaFormer-Based_Global_Contexts-Aware_Network_for_Efficient_Semantic_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484438/,"['Computer vision', 'Semantic segmentation', 'Computer architecture', 'Transformers', 'Decoding', 'Computational efficiency', 'Data mining']","['Semantic Segmentation', 'Computational Cost', 'Synapse', 'Global Context', 'Global Information', 'Segmentation Method', 'Attention Module', 'Power Network', 'Segmentation Task', 'Backbone Network', 'Semantic Segmentation Task', 'Medical Image Segmentation', 'Self-attention Module', 'Channel Reduction', 'Local Information', 'Convolutional Layers', 'Data Augmentation', 'Encoder-decoder', 'Multi-scale Features', 'Segmentation Performance', 'Vision Transformer', 'Transformer-based Methods', 'Attention Operation', 'Transformer Block', 'Floating-point Operations', 'Encoding Stage', 'Low-level Information', 'Segmentation Dataset', 'Semantic Segmentation Datasets', 'Frames Per Second']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Autonomous Driving', 'Applications', 'Biomedical / healthcare / medicine']",4,"Beyond the Transformer, it is important to explore how to exploit the capacity of the MetaFormer, an architecture that is fundamental to the performance improvements of the Transformer. Previous studies have exploited it only for the backbone network. Unlike previous studies, we explore the capacity of the Metaformer architecture more extensively in the semantic segmentation task. We propose a powerful semantic segmentation network, MetaSeg, which leverages the Metaformer architecture from the backbone to the decoder. Our MetaSeg shows that the MetaFormer architecture plays a significant role in capturing the useful contexts for the decoder as well as for the backbone. In addition, recent segmentation methods have shown that using a CNN-based backbone for extracting the spatial information and a decoder for extracting the global information is more effective than using a transformer-based backbone with a CNN-based decoder. This motivates us to adopt the CNN-based backbone using the MetaFormer block and design our MetaFormer-based decoder, which consists of a novel self-attention module to capture the global contexts. To consider both the global contexts extraction and the computational efficiency of the self-attention for semantic segmentation, we propose a Channel Reduction Attention (CRA) module that reduces the channel dimension of the query and key into the one dimension. In this way, our proposed MetaSeg outperforms the previous state-of-the-art methods with more efficient computational costs on popular semantic segmentation and a medical image segmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse."
MetaVers: Meta-Learned Versatile Representations for Personalized Federated Learning,"Jin Hyuk Lim, SeungBum Ha, Sung Whan Yoon","POSCO N.EX.T Hub, Applied AI Research Cell, Republic of Korea; Graduate School of Artiﬁcial Intelligence, Ulsan National Institute of Science and Technology (UNIST), Republic of Korea",50.0,South Korea,50.0,South Korea,"One of the daunting challenges in federated learning (FL) is the heterogeneity across clients that hinders the successful federation of a global model. When the heterogeneity becomes worse, personalized federated learning (PFL) pursues to detour the hardship of capturing the commonality across clients by allowing the personalization of models built upon the federation. In the scope of PFL for visual models, on the contrary, the recent effort for aggregating an effective global representation rather than chasing further personalization draws great attention. Along the same lines, we aim to train a large-margin global representation with a strong generalization across clients by adopting the meta-learning framework and margin-based loss, which are widely accepted to be effective in handling multiple visual tasks. Our method called MetVers achieves state-of-the-art accuracies for the PFL benchmarks with the CIFAR-10, CIFAR-100, and CINIC-10 datasets while showing robustness against data reconstruction attacks. Noteworthy, the versatile representation of MetaVers exhibits a strong generalization when tested on new clients with novel classes.",https://openaccess.thecvf.com/content/WACV2024/html/Lim_MetaVers_Meta-Learned_Versatile_Representations_for_Personalized_Federated_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lim_MetaVers_Meta-Learned_Versatile_Representations_for_Personalized_Federated_Learning_WACV_2024_paper.pdf,,https://github.com/eepLearning/MetaVers,,main,Poster,https://ieeexplore.ieee.org/document/10484206/,"['Metalearning', 'Visualization', 'Computer vision', 'Codes', 'Federated learning', 'Aggregates', 'Benchmark testing']","['Federated Learning', 'Versatile Representations', 'Federation', 'Global Model', 'Global Representation', 'Strong Generalization', 'Model Parameters', 'Cross-entropy Loss', 'Distant Locations', 'Global Parameters', 'Local Gradient', 'Model Of Personality', 'Local Loss', 'Multi-task Learning', 'Marginal Value', 'Local Training', 'Dirichlet Allocation', 'Client-side', 'Negative Points', 'Triplet Loss', 'Shared Representation', 'Prototypical Network', 'Local Updates', 'Fine-tuning Step', 'Class Prototypes', 'Unseen Classes', 'Class Weights', 'Test Split', 'Model Aggregation', 'Max-pooling Layer']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"One of the daunting challenges in federated learning (FL) is the heterogeneity across clients that hinders the successful federation of a global model. When the heterogeneity becomes worse, personalized federated learning (PFL) pursues to detour the hardship of capturing the commonality across clients by allowing the personalization of models built upon the federation. In the scope of PFL for visual models, on the contrary, the recent effort for aggregating an effective global representation rather than chasing further personalization draws great attention. Along the same lines, we aim to train a large-margin global representation with a strong generalization across clients by adopting the meta-learning framework and margin-based loss, which are widely accepted to be effective in handling multiple visual tasks. Our method called MetaVers achieves state-of-the-art accuracies for the PFL benchmarks with the CIFAR-10, CIFAR-100, and CINIC-10 datasets while showing robustness against data reconstruction attacks. Noteworthy, the versatile representation of MetaVers exhibits a strong generalization when tested on new clients with novel classes. Code is available at https://github.com/eepLearning/MetaVers."
Mini but Mighty: Finetuning ViTs With Mini Adapters,"Imad Eddine Marouf, Enzo Tartaglione, Stéphane Lathuilière","LTCI, T´el´ecom-Paris, Institut Polytechnique de Paris, France",100.0,France,0.0,,"Vision Transformers (ViTs) have become one of the dominant architectures in computer vision, and pre-trained ViT models are commonly adapted to new tasks via fine-tuning. Recent works proposed several parameter-efficient transfer learning methods, such as adapters, to avoid the prohibitive training and storage cost of fine-tuning. In this work, we observe that adapters perform poorly when the dimension of adapters is small, and we propose MiMi, a training framework that addresses this issue. We start with large adapters which can reach high performance, and iteratively reduce the size of every adapter. We introduce a scoring function that compares neuron importance across layers and consequently allows automatic estimation of the hidden dimension of every adapter. Our method outperforms existing methods in finding the best trade-off between accuracy and trained parameters across the three dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets. We will release our code publicly upon acceptance.",https://openaccess.thecvf.com/content/WACV2024/html/Marouf_Mini_but_Mighty_Finetuning_ViTs_With_Mini_Adapters_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Marouf_Mini_but_Mighty_Finetuning_ViTs_With_Mini_Adapters_WACV_2024_paper.pdf,,https://github.com/IemProg/MiMi,,main,Poster,https://ieeexplore.ieee.org/document/10484291/,"['Training', 'Computer vision', 'Costs', 'Neurons', 'Transfer learning', 'Estimation', 'Computer architecture']","['Benchmark', 'Scoring Function', 'Trainable Parameters', 'Storage Cost', 'Hidden Dimension', 'Vision Transformer', 'Average Accuracy', 'Training Procedure', 'Latent Space', 'Fully-connected Layer', 'Vanilla', 'Vision Tasks', 'Performance Gap', 'Fewer Parameters', 'Linear Classifier', 'Importance Scores', 'Residual Connection', 'Final Cycle', 'Compression Rate', 'Natural Language Processing Tasks', 'Neural Architecture Search', 'Multi-head Self-attention', 'Hidden Space']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Vision Transformers (ViTs) have become one of the dominant architectures in computer vision, and pre-trained ViT models are commonly adapted to new tasks via finetuning. Recent works proposed several parameter-efficient transfer learning methods, such as adapters, to avoid the prohibitive training and storage cost of finetuning.In this work, we observe that adapters perform poorly when the dimension of adapters is small, and we propose MiMi, a training framework that addresses this issue. We start with large adapters which can reach high performance, and iteratively reduce their size. To enable automatic estimation of the hidden dimension of every adapter, we also introduce a new scoring function, specifically designed for adapters, that compares the neuron importance across layers. Our method outperforms existing methods in finding the best trade-off between accuracy and trained parameters across the three dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning,"M. Yashwanth, Gaurav Kumar Nayak, Harsh Rangwani, Arya Singh, R. Venkatesh Babu, Anirban Chakraborty","BITS Pilani; University of Central Florida; Indian Institute of Science, Bangalore",100.0,"India, USA",0.0,,"Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. %of the global model. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.",https://openaccess.thecvf.com/content/WACV2024/html/Yashwanth_Minimizing_Layerwise_Activation_Norm_Improves_Generalization_in_Federated_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yashwanth_Minimizing_Layerwise_Activation_Norm_Improves_Generalization_in_Federated_Learning_WACV_2024_paper.pdf,,https://github.com/vcl-iisc/fedMAN.git,,main,Poster,https://ieeexplore.ieee.org/document/10484492/,"['Training', 'Federated learning', 'Computational modeling', 'Training data', 'Lead', 'Eigenvalues and eigenfunctions', 'Data models']","['Federated Learning', 'Eigenvalues', 'Optimization Problem', 'Global Model', 'Normalization Layer', 'Training Loss', 'Generalization Performance Of The Model', 'Data Distribution', 'Gradient Descent', 'Convolutional Layers', 'Batch Size', 'Feature Maps', 'Fully-connected Layer', 'Forward Pass', 'Dirichlet Distribution', 'Gradient Ascent', 'Communication Rounds', 'Federated Learning Algorithm']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client’s training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a ‘sharp minimum’ thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a ‘flatness’ constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
, dubbed ‘MAN,’ which Minimizes Activation’s Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client’s loss, which in turn decreases the overall Hessian’s top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art."
Mining and Unifying Heterogeneous Contrastive Relations for Weakly-Supervised Actor-Action Segmentation,"Bin Duan, Hao Tang, Changchang Sun, Ye Zhu, Yan Yan",Princeton University; Carnegie Mellon University; Illinois Institute of Technology,100.0,USA,0.0,,"We introduce a novel weakly-supervised video actor-action segmentation (VAAS) framework, where only video-level tags are available. Previous VAAS methods follow a synthesize-and-refine scheme, i.e., they first synthesize the pseudo-segmentation and recursively refine the segmentation. However, this process requires significant time costs and heavily relies on the quality of the initial segmentation. Unlike existing works, our method hierarchically mines contrastive relations to supplement each other for learning a visually-plausible segmentation model. Specifically, three contrastive relations are abstracted from the pixel-level and frame-level, i.e., low-level edge-aware, class-activation map aware, and semantic tag-aware relations. Then, the discovered contrastive relations are unified into a universal objective for training the segmentation model, regardless of their heterogeneity. Moreover, we incorporate motion cues and unlabeled samples to increase the discriminative power and robustness of the segmentation model. Extensive experiments indicate that our proposed method produces reasonable segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Duan_Mining_and_Unifying_Heterogeneous_Contrastive_Relations_for_Weakly-Supervised_Actor-Action_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484340/,"['Training', 'Visualization', 'Computer vision', 'Costs', 'Motion segmentation', 'Semantics', 'Pipelines']","['Segmentation Model', 'Class Activation Maps', 'Motion Cues', 'Objects In The Universe', 'Classification Model', 'Objective Function', 'Common Practice', 'Positive Samples', 'Multilayer Perceptron', 'Video Clips', 'Representation Learning', 'Semantic Segmentation', 'Action Recognition', 'Video Sequences', 'Object Segmentation', 'Self-supervised Learning', 'Instance Segmentation', 'Joint Learning', 'One-hot Vector', 'Spherical Clusters', 'Edge Segmentation', 'Raw Frames', 'Pixel-level Annotations']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",,"We introduce a novel weakly-supervised video actor-action segmentation (VAAS) framework, where only video-level tags are available. Previous VAAS methods follow a synthesize-and-refine scheme, i.e., they first synthesize the pseudo-segmentation and recursively refine the segmentation. However, this process requires significant time costs and heavily relies on the quality of the initial segmentation. Unlike existing works, our method hierarchically mines contrastive relations to supplement each other for learning a visually-plausible segmentation model. Specifically, three contrastive relations are abstracted from the pixel-level and frame-level, i.e., low-level edge-aware, class-activation map aware, and semantic tag-aware relations. Then, the discovered contrastive relations are unified into a universal objective for training the segmentation model, regardless of their heterogeneity. Moreover, we incorporate motion cues and unlabeled samples to increase the discriminative power and robustness of the segmentation model. Extensive experiments indicate that our proposed method produces reasonable segmentation."
Missing Modality Robustness in Semi-Supervised Multi-Modal Semantic Segmentation,"Harsh Maheshwari, Yen-Cheng Liu, Zsolt Kira","Georgia Institute of Technology; Georgia Institute of Technology, Avataar.ai",100.0,USA,0.0,,"Using multiple spatial modalities has been proven helpful in improving semantic segmentation performance. However, there are several real-world challenges that have yet to be addressed: (a) improving label efficiency and (b) enhancing robustness in realistic scenarios where modalities are missing at the test time. To address these challenges, we first propose a simple yet efficient multi-modal fusion mechanism Linear Fusion, that performs better than the state-of-the-art multi-modal models even with limited supervision. Second, we propose M3L: Multi-modal Teacher for Masked Modality Learning, a semi-supervised framework that not only improves the multi-modal performance but also makes the model robust to the realistic missing modality scenario using unlabeled data. We create the first benchmark for semi-supervised multi-modal semantic segmentation and also report the robustness to missing modalities. Our proposal shows an absolute improvement of up to 5% on robust mIoU above the most competitive baselines. Our project page is at https://harshm121.github.io/projects/m3l.html",https://openaccess.thecvf.com/content/WACV2024/html/Maheshwari_Missing_Modality_Robustness_in_Semi-Supervised_Multi-Modal_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Maheshwari_Missing_Modality_Robustness_in_Semi-Supervised_Multi-Modal_Semantic_Segmentation_WACV_2024_paper.pdf,https://harshm121.github.io/projects/m3l.html,,2304.10756,main,Poster,https://ieeexplore.ieee.org/document/10483775/,"['Training', 'Computer vision', 'Semantic segmentation', 'Benchmark testing', 'Robustness', 'Data models', 'Proposals']","['Semantic Segmentation', 'Multimodal Segmentation', 'Multiple Modalities', 'Unlabeled Data', 'Labeling Efficiency', 'Segmentation Performance', 'Multimodal Model', 'Simple Fusion', 'Limited Supervision', 'Validation Set', 'Additional Modifications', 'Teacher Model', 'Segmentation Model', 'Mean Education', 'Semi-supervised Learning', 'Student Model', 'Auxiliary Information', 'Pseudo Labels', 'Fusion Weights', 'Improve Segmentation Performance']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",5,"Using multiple spatial modalities has been proven helpful in improving semantic segmentation performance. However, there are several real-world challenges that have yet to be addressed: (a) improving label efficiency and (b) enhancing robustness in realistic scenarios where modalities are missing at the test time. To address these challenges, we first propose a simple yet efficient multi-modal fusion mechanism Linear Fusion, that performs better than the state-of-the-art multi-modal models even with limited supervision. Second, we propose M3L: Multi-modal Teacher for Masked Modality Learning, a semi-supervised framework that not only improves the multi-modal performance but also makes the model robust to the realistic missing modality scenario using unlabeled data. We create the first benchmark for semi-supervised multi-modal semantic segmentation and also report the robustness to missing modalities. Our proposal shows an absolute improvement of up to 5% on robust mIoU above the most competitive baselines. Our project page is at https://harshm121.github.io/projects/m3l.html"
Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID,"Qilei Li, Shaogang Gong",Queen Mary University of London,100.0,UK,0.0,,"While deep learning has significantly improved ReID model accuracy under the independent and identical distribution (IID) assumption, it has also become clear that such models degrade notably when applied to an unseen novel domain due to unpredictable/unknown domain shift. Contemporary domain generalization (DG) ReID models struggle in learning domain-invariant representation through solely training on an instance classification objective. We consider that a deep learning model is heavily influenced therefore biased towards domain-specific characteristics, e.g., background clutter, scale and viewpoint variations, limiting the generalizability of the learned model, and hypothesize that the pedestrians are domain invariant owning they share the same structural characteristics. To enable ReID model to be less domain-specific from these pure pedestrians and domain-specific factors, we introduce a method that guides model learning of the primary ReID instance classification objective by a concurrent auxiliary learning objective on weakly labeled pedestrian saliency detection. To solve the problem of conflicting optimization criteria in the model parameter space between the two learning objectives, we introduce a Primary-Auxiliary Objectives Association (PAOA) mechanism to calibrate the loss gradients of the auxiliary task towards the primary learning task gradients. Benefited from the harmonious multitask learning design, our model can be extended with the recent test-time diagram to form the PAOA+, which performs on-the-fly optimization against the auxiliary objective in order to maximize the model's generative capacity in the test target domain. Experiments demonstrate the superiority of the proposed PAOA model.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Mitigate_Domain_Shift_by_Primary-Auxiliary_Objectives_Association_for_Generalizing_Person_WACV_2024_paper.pdf,,,2310.15913,main,Poster,https://ieeexplore.ieee.org/document/10484483/,"['Deep learning', 'Training', 'Computer vision', 'Pedestrians', 'Limiting', 'Noise measurement', 'Task analysis']","['Domain Shift', 'Mitigate Domain Shift', 'Deep Learning', 'Learning Models', 'Primary Objective', 'Learning Task', 'Deep Learning Models', 'Learning Objectives', 'Primary Task', 'Target Domain', 'Primary Categories', 'Multi-task Learning', 'Domain Generalization', 'Class Instances', 'Saliency Detection', 'Auxiliary Task', 'Model Parameter Space', 'Pedestrian Detection', 'Training Data', 'Test Data', 'Weak Labels', 'Source Domain', 'Salient Object Detection', 'Saliency Map', 'Training Objective', 'Model Discrimination', 'Cross-entropy Loss', 'Triplet Loss', 'Representation Of Identity', 'Pixel Level']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",,"While deep learning has significantly improved ReID model accuracy under the independent and identical distribution (IID) assumption, it has also become clear that such models degrade notably when applied to an unseen novel domain due to unpredictable/unknown domain shift. Contemporary domain generalization (DG) ReID models struggle in learning domain-invariant representation solely through training on an instance classification objective. We consider that a deep learning model is heavily influenced and therefore biased towards domain-specific characteristics, e.g., background clutter, scale and viewpoint variations, limiting the generalizability of the learned model, and hypothesize that the pedestrians are domain invariant owning they share the same structural characteristics. To enable the ReID model to be less domain-specific from these pure pedestrians, we introduce a method that guides model learning of the primary ReID instance classification objective by a concurrent auxiliary learning objective on weakly la-beled pedestrian saliency detection. To solve the problem of conflicting optimization criteria in the model parameter space between the two learning objectives, we introduce a Primary-Auxiliary Objectives Association (PAOA) mechanism to calibrate the loss gradients of the auxiliary task towards the primary learning task gradients. Benefiting from the harmonious multitask learning design, our model can be extended with the recent test-time diagram to form the PAOA+, which performs on-the-fly optimization against the auxiliary objective in order to maximize the model’s generative capacity in the test target domain. Experiments demonstrate the superiority of the proposed PAOA model."
Mixing Gradients in Neural Networks as a Strategy To Enhance Privacy in Federated Learning,"Shaltiel Eloul, Fran Silavong, Sanket Kamthe, Antonios Georgiadis, Sean J. Moran","Not provided in the text; CTO, JPMorgan Chase",0.0,,100.0,,"Federated learning reduces the risk of information leakage, but remains vulnerable to attack. We show that well-mixed gradients provide numerical resistance to gradient inversion in neural networks. For example, we can enhance mixing gradients in a batch by choosing an appropriate loss function and drawing identical labels, and we support this with an approximate solution of batch inversion for linear layers. These simple architecture choices show no degradation to classification performance as opposed to noise perturbation defense. To accurately assess data recovery, we propose to use a variation distance metric for information leakage in images, derived from total variation. In contrast to Mean Squared Error or Structural Similarity Index metrics, it provides a continuous metric for information recovery. Finally, our empirical results of information recovery from various inversion attacks and training performance supports our defense strategies. These simple architecture choices found to be also useful for practical size of convolutional neural networks but depends on their size. We hope this work will trigger further defense studies using gradient mixing, towards achieving a trustful federation policy.",https://openaccess.thecvf.com/content/WACV2024/html/Eloul_Mixing_Gradients_in_Neural_Networks_as_a_Strategy_To_Enhance_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Eloul_Mixing_Gradients_in_Neural_Networks_as_a_Strategy_To_Enhance_WACV_2024_paper.pdf,Not provided in the text,Not provided in the text,,main,Poster,https://ieeexplore.ieee.org/document/10484086/,"['Measurement', 'Training', 'Resistance', 'Privacy', 'Federated learning', 'Noise', 'Neural networks']","['Neural Network', 'Federated Learning', 'Loss Function', 'Mean Square Error', 'Convolutional Network', 'Convolutional Neural Network', 'Training Performance', 'Defense Strategy', 'Information Leakage', 'Linear Layer', 'Data Recovery', 'Simple Choice', 'Information Recovery', 'Variation Distance', 'Objective Function', 'Batch Size', 'Single Layer', 'Recovery Rate', 'Softmax', 'Cross-entropy', 'Dense Layer', 'MNIST Dataset', 'Input Vector', 'Numerical Optimization', 'Mixed Strategy', 'Differential Privacy', 'Number Of Filters', 'Set Of Equations', 'Structural Similarity Index Measure', 'Successful Recovery']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Datasets and evaluations']",1,"Federated learning reduces the risk of information leakage, but remains vulnerable to attack. We show that well-mixed gradients provide numerical resistance to gradient inversion in neural networks. For example, we can enhance mixing gradients in a batch by choosing an appropriate loss function and drawing identical labels, and we support this with an approximate solution of batch inversion for linear layers. These simple architecture choices show no degradation to classification performance as opposed to noise perturbation defense. To accurately assess data recovery, we propose to use a variation distance metric for information leakage in images, derived from total variation. In contrast to Mean Squared Error or Structural Similarity Index metrics, it provides a continuous metric for information recovery. Finally, our empirical results of information recovery from various inversion attacks and training performance supports our defense strategies. These simple architecture choices found to be also useful for practical size of convolutional neural networks but depends on their size. We hope this work will trigger further defense studies using gradient mixing, towards achieving a trustful federation policy."
MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters,"Chau Pham, Piotr Teterwak, Soren Nelson, Bryan A. Plummer",Physical Sciences Inc; Boston University,50.0,USA,50.0,USA,"Most deep neural networks are trained under fixed network architectures and require retraining when the architecture changes. If expanding the network's size is needed, it is necessary to retrain from scratch, which is expensive. To avoid this, one can grow from a small network by adding random weights over time to gradually achieve the target network size. However, this naive approach falls short in practice as it brings too much noise to the growing process. Prior work tackled this issue by leveraging the already learned weights and training data for generating new weights through conducting a computationally expensive analysis step. In this paper, we introduce MixtureGrowth, a new approach to growing networks that circumvents the initialization overhead in prior work. Before growing, each layer in our model is generated with a linear combination of parameter templates. Newly grown layer weights are generated by using a new linear combination of existing templates for a layer. On one hand, these templates are already trained for the task, providing a strong initialization. On the other, the new coefficients provide flexibility for the added layer weights to learn something new. We show that our approach boosts top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet datasets, while achieving comparable performance with fewer FLOPs to a larger network trained from scratch. Code is available at https://github.com/chaudatascience/mixturegrowth",https://openaccess.thecvf.com/content/WACV2024/html/Pham_MixtureGrowth_Growing_Neural_Networks_by_Recombining_Learned_Parameters_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pham_MixtureGrowth_Growing_Neural_Networks_by_Recombining_Learned_Parameters_WACV_2024_paper.pdf,,https://github.com/chaudatascience/mixturegrowth,2311.04251,main,Poster,https://ieeexplore.ieee.org/document/10483835/,"['Computer vision', 'Codes', 'Computational modeling', 'Noise', 'Training data', 'Computer architecture', 'Artificial neural networks']","['Neural Network', 'Deep Neural Network', 'Large Networks', 'Network Training', 'Weights Of Layer', 'Target Network', 'ImageNet Dataset', 'Top-1 Accuracy', 'CIFAR-100 Dataset', 'Task Performance', 'Computational Efficiency', 'Firefly', 'Linear Coefficient', 'Stochastic Gradient Descent', 'Trainable Parameters', 'Network Weights', 'Network Growth', 'Random Initialization', 'Teacher Network', 'Growth Step', 'Shared Parameters', 'Neural Architecture Search', 'Standard Neural Network', 'Traditional Neural Network', 'Width Of Layer']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Most deep neural networks are trained under fixed network architectures and require retraining when the architecture changes. If expanding the network’s size is needed, it is necessary to retrain from scratch, which is expensive. To avoid this, one can grow from a small network by adding random weights over time to gradually achieve the target network size. However, this naive approach falls short in practice as it brings too much noise to the growing process. Prior work tackled this issue by leveraging the already learned weights and training data for generating new weights through conducting a computationally expensive analysis step. In this paper, we introduce MixtureGrowth, a new approach to growing networks that circumvents the initialization overhead in prior work. Before growing, each layer in our model is generated with a linear combination of parameter templates. Newly grown layer weights are generated by using a new linear combination of existing templates for a layer. On one hand, these templates are already trained for the task, providing a strong initialization. On the other, the new coefficients provide flexibility for the added layer weights to learn something new. We show that our approach boosts top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet datasets, while achieving comparable performance with fewer FLOPs to a larger network trained from scratch. Code is available at https://github.com/chaudatascience/mixturegrowth."
MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning,"Julien Nicolas, Florent Chiaroni, Imtiaz Ziko, Ola Ahmad, Christian Desrosiers, Jose Dolz",,,,,,"Despite the recent progress in incremental learning, addressing catastrophic forgetting under distributional drift is still an open and important problem. Indeed, while state-of-the-art domain incremental learning (DIL) methods perform satisfactorily within known domains, their performance largely degrades in the presence of novel domains. This limitation hampers their generalizability, and restricts their scalability to more realistic settings where train and test data are drawn from different distributions. To address these limitations, we present a novel DIL approach based on a mixture of prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of S-Prompting to handle both in-distribution and out-of-distribution data at inference. In particular, at the training stage we model the features distribution of every class in each domain, learning individual text and visual prompts to adapt to a given domain. At inference, the learned distributions allow us to identify whether a given test sample belongs to a known domain, selecting the correct prompt for the classification task, or from an unseen domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical evaluation reveals the poor performance of existing DIL methods under domain shift, and suggests that the proposed MoP-CLIP performs competitively in the standard DIL settings while outperforming state-of-the-art methods in OOD scenarios. These results demonstrate the superiority of MoP-CLIP, offering a robust and general solution to the problem of domain incremental learning.",https://openaccess.thecvf.com/content/WACV2024/html/Nicolas_MoP-CLIP_A_Mixture_of_Prompt-Tuned_CLIP_Models_for_Domain_Incremental_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nicolas_MoP-CLIP_A_Mixture_of_Prompt-Tuned_CLIP_Models_for_Domain_Incremental_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483776/,"['Training', 'Adaptation models', 'Visualization', 'Scalability', 'Data models', 'Task analysis', 'Tuning']","['Mixture Model', 'Incremental Learning', 'Domain Incremental Learning', 'Training Data', 'Test Samples', 'Domain Shift', 'Catastrophic Forgetting', 'Unseen Domains', 'Average Accuracy', 'Average Performance', 'Recent Approaches', 'ImageNet', 'Real-world Scenarios', 'Target Domain', 'Sequential Manner', 'Domain Generalization', 'Learning Scenarios', 'Input Transformation', 'Vision Transformer']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",1,"Despite the recent progress in incremental learning, addressing catastrophic forgetting under distributional drift is still an open and important problem. Indeed, while state-of-the-art domain incremental learning (DIL) methods perform satisfactorily within known domains, their performance largely degrades in the presence of novel domains. This limitation hampers their generalizability, and restricts their scalability to more realistic settings where train and test data are drawn from different distributions. To address these limitations, we present a novel DIL approach based on a mixture of prompt-tuned CLIP models (MoPCLIP), which generalizes the paradigm of S-Prompting to handle both in-distribution and out-of-distribution data at inference. In particular, at the training stage we model the features distribution of every class in each domain, learning individual text and visual prompts to adapt to a given domain. At inference, the learned distributions allow us to identify whether a given test sample belongs to a known domain, selecting the correct prompt for the classification task, or from an unseen domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical evaluation reveals the poor performance of existing DIL methods under domain shift, and suggests that the proposed MoP-CLIP performs competitively in the standard DIL settings while outperforming state-of-the-art methods in OOD scenarios. These results demonstrate the superiority of MoP-CLIP, offering a robust and general solution to the problem of domain incremental learning."
MoRF: Mobile Realistic Fullbody Avatars From a Monocular Video,"Renat Bashirov, Alexey Larionov, Evgeniya Ustinova, Mikhail Sidorenko, David Svitov, Ilya Zakharkin, Victor Lempitsky",Cinemersive Labs; ZERO10; Samsung Research,0.0,,100.0,USA,"We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF avatars are rendered in real-time on mobile devices, learned from monocular videos, and have high realism. We use SMPL-X as a proxy geometry and render it with DNR (neural texture and image-2-image network). We improve on prior work, by overfitting per-frame warping fields in the neural texture space, allowing to better align the training signal between different frames. We also refine SMPL-X mesh fitting procedure to improve the overall avatar quality. In the comparisons to other monocular video-based avatar systems, MoRF avatars achieve higher image sharpness and temporal consistency. Participants of our user study also preferred avatars generated by MoRF.",https://openaccess.thecvf.com/content/WACV2024/html/Bashirov_MoRF_Mobile_Realistic_Fullbody_Avatars_From_a_Monocular_Video_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bashirov_MoRF_Mobile_Realistic_Fullbody_Avatars_From_a_Monocular_Video_WACV_2024_paper.pdf,,,2303.10275,main,Poster,https://ieeexplore.ieee.org/document/10483960/,"['Training', 'Geometry', 'Computer vision', 'Avatars', 'Fitting', 'Real-time systems', 'Mobile handsets']","['Monocular Video', 'Full-body Avatar', 'Mobile Devices', 'User Study', 'Temporal Consistency', 'Warp Field', 'Training Data', 'Mobile App', 'RGB Images', 'Temporal Stability', 'Camera View', 'Training Videos', 'Latent Vector', 'Camera Pose', 'Optimization-based Methods', 'Target Person', 'Body Pose', 'Training Frames']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"We present a system to create Mobile Realistic Fullbody (MoRF) avatars. MoRF avatars are rendered in real-time on mobile devices, learned from monocular videos, and have high realism. We use SMPL-X as a proxy geometry and render it with DNR (neural texture and image-2-image network). We improve on prior work, by overfitting perframe warping fields in the neural texture space, allowing to better align the training signal between different frames. We also refine SMPL-X mesh fitting procedure to improve the overall avatar quality. In the comparisons to other monocular video-based avatar systems, MoRF avatars achieve higher image sharpness and temporal consistency. Participants of our user study also preferred avatars generated by MoRF."
MobileNVC: Real-Time 1080p Neural Video Compression on a Mobile Device,"Ties van Rozendaal, Tushar Singhal, Hoang Le, Guillaume Sautiere, Amir Said, Krishna Buska, Anjuman Raha, Dimitris Kalatzis, Hitarth Mehta, Frank Mayer, Liang Zhang, Markus Nagel, Auke Wiggers",Qualcomm AI Research,0.0,,100.0,USA,"Neural video codecs have recently become competitive with standard codecs such as HEVC in the low-delay setting. However, most neural codecs are large floating-point networks that use pixel-dense warping operations for temporal modeling, making them too computationally expensive for deployment on mobile devices. Recent work has demonstrated that running a neural decoder in real time on mobile is feasible, but shows this only for 720p RGB video. This work presents the first neural video codec that decodes 1080p YUV420 video in real time on a mobile device. Our codec relies on two major contributions. First, we design an efficient codec that uses a block-based motion compensation algorithm available on the warping core of the mobile accelerator, and we show how to quantize this model to integer precision. Second, we implement a fast decoder pipeline that concurrently runs neural network components on the neural signal processor, parallel entropy coding on the mobile GPU, and warping on the warping core. Our codec outperforms the previous on-device codec by a large margin with up to 48 % BD-rate savings, while reducing the MAC count on the receiver side by 10x. We perform a careful ablation to demonstrate the effect of the introduced motion compensation scheme, and ablate the effect of model quantization.",https://openaccess.thecvf.com/content/WACV2024/html/van_Rozendaal_MobileNVC_Real-Time_1080p_Neural_Video_Compression_on_a_Mobile_Device_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/van_Rozendaal_MobileNVC_Real-Time_1080p_Neural_Video_Compression_on_a_Mobile_Device_WACV_2024_paper.pdf,,,2310.01258,main,Poster,https://ieeexplore.ieee.org/document/10484439/,"['Performance evaluation', 'Codecs', 'Quantization (signal)', 'Pipelines', 'Signal processing algorithms', 'Real-time systems', 'Motion compensation']","['Mobile Devices', 'Real-time Video', 'Video Compression', 'Neural Compression', 'Ablation', 'Decoding', 'Receiver Side', 'Quantification Model', 'Motion Compensation', 'Quantization Effects', 'Neural Decoding', 'Entropy Coding', 'Convolution', 'Step Size', 'Parallelization', 'Training Stage', 'Peak Signal-to-noise Ratio', 'Quantization Parameter', 'Least Significant Bit', 'Group Of Pictures', 'Motion Vector', 'Frames Per Second', 'High-definition Video', 'Inference Speed', 'Quantization Scheme', 'Image Compression', 'Pixel Block', 'Antirrhinum']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Datasets and evaluations']",6,"Neural video codecs have recently become competitive with standard codecs such as HEVC in the low-delay setting. However, most neural codecs are large floating-point networks that use pixel-dense warping operations for temporal modeling, making them too computationally expensive for deployment on mobile devices. Recent work has demonstrated that running a neural decoder in real time on mobile is feasible, but shows this only for 720p RGB video.This work presents the first neural video codec that decodes 1080p YUV420 video in real time on a mobile device. Our codec relies on two major contributions. First, we design an efficient codec that uses a block-based motion compensation algorithm available on the warping core of the mobile accelerator, and we show how to quantize this model to integer precision. Second, we implement a fast decoder pipeline that concurrently runs neural network components on the neural signal processor, parallel entropy coding on the mobile GPU, and warping on the warping core. Our codec outperforms the previous on-device codec by a large margin with up to 48 % BD-rate savings, while reducing the MAC count on the receiver side by 10×. We perform a careful ablation to demonstrate the effect of the introduced motion compensation scheme, and ablate the effect of model quantization."
Modality-Aware Representation Learning for Zero-Shot Sketch-Based Image Retrieval,"Eunyi Lyou, Doyeon Lee, Jooeun Kim, Joonseok Lee","Seoul National University; Seoul National University, Google Research",100.0,South Korea,0.0,,"Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection. Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples. We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs. With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space. From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings.",https://openaccess.thecvf.com/content/WACV2024/html/Lyou_Modality-Aware_Representation_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lyou_Modality-Aware_Representation_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483655/,"['Representation learning', 'Computer vision', 'Zero-shot learning', 'Computational modeling', 'Image retrieval', 'Semantics', 'Data collection']","['Sketch-based Image Retrieval', 'Zero-shot Sketch-based Image Retrieval', 'Paired Samples', 'Latent Space', 'Classifier Training', 'Wind Turbine', 'Training Examples', 'Common Space', 'Semantic Representations', 'Loss Term', 'Training Objective', 'Caption Text', 'Contrastive Loss', 'Direct Alignment', 'Multimodal Model', 'Image Encoder', 'Image Embedding', 'Hierarchical Graph', 'Unseen Classes', 'Advantage Of Datasets', 'Text Encoder', 'Catastrophic Forgetting', 'Projection Layer']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding']",3,"Zero-shot learning offers an efficient solution for a machine learning model to treat unseen categories, avoiding exhaustive data collection. Zero-shot Sketch-based Image Retrieval (ZS-SBIR) simulates real-world scenarios where it is hard and costly to collect paired sketch-photo samples. We propose a novel framework that indirectly aligns sketches and photos by contrasting them through texts, removing the necessity of access to sketch-photo pairs. With an explicit modality encoding learned from data, our approach disentangles modality-agnostic semantics from modality-specific information, bridging the modality gap and enabling effective cross-modal content retrieval within a joint latent space. From comprehensive experiments, we verify the efficacy of the proposed model on ZS-SBIR, and it can be also applied to generalized and fine-grained settings."
MonoProb: Self-Supervised Monocular Depth Estimation With Interpretable Uncertainty,"Rémi Marsal, Florian Chabot, Angélique Loesch, William Grolleau, Hichem Sahbi","Sorbonne University, CNRS, LIP6 F-75005, Paris, France; Universit ´e Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France",100.0,France,0.0,,"Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb",https://openaccess.thecvf.com/content/WACV2024/html/Marsal_MonoProb_Self-Supervised_Monocular_Depth_Estimation_With_Interpretable_Uncertainty_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Marsal_MonoProb_Self-Supervised_Monocular_Depth_Estimation_With_Interpretable_Uncertainty_WACV_2024_paper.pdf,,https://github.com/CEA-LIST/MonoProb,2311.06137,main,Poster,https://ieeexplore.ieee.org/document/10483762/,"['Training', 'Uncertainty', 'Measurement uncertainty', 'Neural networks', 'Estimation', 'Predictive models', 'Probabilistic logic']","['Depth Estimation', 'Monocular Depth Estimation', 'Self-supervised Monocular Depth Estimation', 'Unsupervised Methods', 'Prediction Uncertainty', 'Depth Prediction', 'Neural Network', 'Distribution Of Parameters', 'Unsupervised Learning', 'Empirical Approach', 'Ensemble Method', 'Depth Map', 'Source Images', 'Family Of Distributions', 'Uncertainty Quantification', 'Depth Distribution', 'Reconstruction Loss', 'Camera Calibration', 'Negative Log-likelihood', 'Stereo Images', 'Absolute Uncertainty', 'Law Of Total Probability', 'Uncertainty Error', 'Camera Motion', 'Depth Error', 'Laplace Distribution', 'Image Reconstruction', 'Kullback-Leibler', 'Student Model', 'Uncertainty Estimation']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb"
Monocular 3D Object Detection With LiDAR Guided Semi Supervised Active Learning,"Aral Hekimoglu, Michael Schmidt, Alvaro Marcos-Ramiro","BMW Group, Munich, Germany; Technical University Munich, Munich, Germany",50.0,Germany,50.0,Germany,"We propose a novel semi-supervised active learning framework for monocular 3D object detection with LiDAR guidance (MonoLiG), which leverages all modalities of collected data during model development. We utilize LiDAR to guide the data selection and training of monocular 3D detectors without introducing any overhead in the inference phase. During training, we leverage the LiDAR teacher, monocular student cross-modal framework from semi-supervised learning to distill information from unlabeled data as pseudo-labels. To handle the differences in sensor characteristics, we propose a data noise-based weighting mechanism to reduce the effect of propagating noise from LiDAR modality to monocular. For selecting which samples to label to improve the model performance, we propose a sensor consistency-based selection score that is also coherent with the training objective. Extensive experimental results on KITTI and Waymo datasets verify the effectiveness of our proposed framework. In particular, our selection strategy consistently outperforms state-of-the-art active learning baselines, yielding up to 17% better saving rate in labeling costs. Our training strategy attains the top place in KITTI 3D and bird's-eye-view (BEV) monocular object detection official benchmarks by improving the BEV Average Precision (AP) by 2.02. Code is shared at https://github.com/aralhekimoglu/monolig.",https://openaccess.thecvf.com/content/WACV2024/html/Hekimoglu_Monocular_3D_Object_Detection_With_LiDAR_Guided_Semi_Supervised_Active_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hekimoglu_Monocular_3D_Object_Detection_With_LiDAR_Guided_Semi_Supervised_Active_WACV_2024_paper.pdf,,https://github.com/aralhekimoglu/monolig,2307.08415,main,Poster,https://ieeexplore.ieee.org/document/10483936/,"['Training', 'Solid modeling', 'Laser radar', 'Three-dimensional displays', 'Uncertainty', 'Noise', 'Detectors']","['Active Learning', 'Object Detection', '3D Object Detection', 'Monocular 3D Object Detection', 'Selection Strategy', 'Training Strategy', 'Unlabeled Data', 'Semi-supervised Learning', 'Semi-supervised Learning Framework', 'Training Set', 'Convolutional Neural Network', 'Supervised Learning', 'Training Phase', 'Teacher Model', 'Intersection Over Union', 'Point Cloud', 'High Uncertainty', 'Bounding Box', 'Confidence Score', 'Random Initialization', 'Aleatoric Uncertainty', 'Selection Phase', 'Student Model', 'Epistemic Uncertainty', 'Low Uncertainty', 'Position Uncertainty', 'Acquisition Function', 'Occluded Objects', 'Negative Log-likelihood']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",4,"We propose a novel semi-supervised active learning framework for monocular 3D object detection with LiDAR guidance (MonoLiG), which leverages all modalities of collected data during model development. We utilize LiDAR to guide the data selection and training of monocular 3D detectors without introducing any overhead in the inference phase. During training, we leverage the LiDAR teacher, monocular student cross-modal framework from semi-supervised learning to distill information from unlabeled data as pseudo-labels. To handle the differences in sensor characteristics, we propose a data noise-based weighting mechanism to reduce the effect of propagating noise from LiDAR modality to monocular. For selecting which samples to label to improve the model performance, we propose a sensor consistency-based selection score that is also coherent with the training objective. Extensive experimental results on KITTI and Waymo datasets verify the effectiveness of our proposed framework. In particular, our selection strategy consistently outperforms state-of-the-art active learning baselines, yielding up to 17% better saving rate in labeling costs. Our training strategy attains the top place in KITTI 3D and bird’s-eye-view (BEV) monocular object detection official benchmarks by improving the BEV Average Precision (AP) by 2.02. Code is shared at https://github.com/aralhekimoglu/monolig."
Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement,"Akshay Paruchuri, Xin Liu, Yulu Pan, Shwetak Patel, Daniel McDuff, Soumyadip Sengupta",University of Washington; UNC Chapel Hill,100.0,USA,0.0,,"Machine learning models for camera-based physiological measurement can have weak generalization due to a lack of representative training data. Body motion is one of the most significant sources of noise when attempting to recover the subtle cardiac pulse from a video. We explore motion transfer as a form of data augmentation to introduce motion variation while preserving physiological changes of interest. We adapt a neural video synthesis approach to augment videos for the task of remote photoplethysmography (rPPG) and study the effects of motion augmentation with respect to 1) the magnitude and 2) the type of motion. After training on motion-augmented versions of publicly available datasets, the presented inter-dataset results on five benchmark datasets show improvements of up to 79% over existing inter-dataset results using TS-CAN, a neural rPPG estimation method. Additionally, we demonstrate a 47% improvement over existing results on the PURE dataset using various state-of-the-art methods. Our findings illustrate the usefulness of motion transfer as a data augmentation technique for improving the generalization of models for camera-based physiological sensing. We release our code for using motion transfer as a data augmentation technique on three publicly available datasets, UBFC-rPPG, PURE, and SCAMPS, and models pre-trained on motion-augmented data.",https://openaccess.thecvf.com/content/WACV2024/html/Paruchuri_Motion_Matters_Neural_Motion_Transfer_for_Better_Camera_Physiological_Measurement_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Paruchuri_Motion_Matters_Neural_Motion_Transfer_for_Better_Camera_Physiological_Measurement_WACV_2024_paper.pdf,,https://motion-matters.github.io/,2303.12059,main,Poster,https://ieeexplore.ieee.org/document/10484021/,"['Training', 'Training data', 'Data augmentation', 'Cameras', 'Physiology', 'Data models', 'Reflection']","['Motion Transfer', 'Neural Motion', 'Training Data', 'Machine Learning Models', 'Data Augmentation', 'Type Of Motion', 'Data Augmentation Techniques', 'Photoplethysmography', 'Facial Expressions', 'Learning Task', 'Neural Model', 'Generative Adversarial Networks', 'Head Motion', 'Video Frames', 'Source Images', 'Error Reduction', 'Rigid Transformation', 'Neural Algorithm', 'Standard Deviation Range', 'Large Motion', 'Non-rigid Motion', 'Source Video', 'Changes In Facial Expressions', 'Facial Action Units', 'Video Dataset', 'Case Of Motion', 'Natural Motion', 'Video Of A Person', 'Synthetic Generation', 'State Of The Art Methods']","['Applications', 'Biomedical / healthcare / medicine', 'Applications', 'Remote Sensing']",3,"Machine learning models for camera-based physiological measurement can have weak generalization due to a lack of representative training data. Body motion is one of the most significant sources of noise when attempting to recover the subtle cardiac pulse from a video. We explore motion transfer as a form of data augmentation to introduce motion variation while preserving physiological changes of interest. We adapt a neural video synthesis approach to augment videos for the task of remote photoplethysmography (rPPG) and study the effects of motion augmentation with respect to 1) the magnitude and 2) the type of motion. After training on motion-augmented versions of publicly available datasets, we demonstrate a 47% improvement over existing inter-dataset results using various state-of-the-art methods on the PURE dataset. We also present inter-dataset results on five benchmark datasets to show improvements of up to 79% using TS-CAN, a neural rPPG estimation method. Our findings illustrate the usefulness of motion transfer as a data augmentation technique for improving the generalization of models for camera-based physiological sensing. We release our code for using motion transfer as a data augmentation technique on three publicly available datasets, UBFC-rPPG, PURE, and SCAMPS, and models pre-trained on motion-augmented data here: https://motion-matters.github.io/"
MotionAGFormer: Enhancing 3D Human Pose Estimation With a Transformer-GCNFormer Network,"Soroush Mehraban, Vida Adeli, Babak Taati","KITE Research Institute, Department of Computer Science, University of Toronto; KITE Research Institute, Institute of Biomedical Engineering, Department of Computer Science, University of Toronto; KITE Research Institute, Institute of Biomedical Engineering, University of Toronto",100.0,Canada,0.0,,"Recent transformer-based approaches have demonstrated excellent performance in 3D human pose estimation. However, they have a holistic view and by encoding global relationships between all the joints, they do not capture the local dependencies precisely. In this paper, we present a novel Attention-GCNFormer (AGFormer) block that divides the number of channels by using two parallel transformer and GCNFormer streams. Our proposed GCNFormer module exploits the local relationship between adjacent joints, outputting a new representation that is complementary to the transformer output. By fusing these two representation in an adaptive way, AGFormer exhibits the ability to better learn the underlying 3D structure. By stacking multiple AGFormer blocks, we propose MotionAGFormer in four different variants, which can be chosen based on the speed-accuracy trade-off. We evaluate our model on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. MotionAGFormer-B achieves state-of-the-art results, with P1 errors of 38.4 mm and 16.2 mm, respectively. Remarkably, it uses a quarter of the parameters and is three times more computationally efficient than the previous leading model on Human3.6M dataset. Code and models are available at https://github.com/TaatiTeam/MotionAGFormer.",https://openaccess.thecvf.com/content/WACV2024/html/Mehraban_MotionAGFormer_Enhancing_3D_Human_Pose_Estimation_With_a_Transformer-GCNFormer_Network_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mehraban_MotionAGFormer_Enhancing_3D_Human_Pose_Estimation_With_a_Transformer-GCNFormer_Network_WACV_2024_paper.pdf,,https://github.com/TaatiTeam/MotionAGFormer,2310.16288,main,Poster,https://ieeexplore.ieee.org/document/10483869/,"['Adaptation models', 'Solid modeling', 'Three-dimensional displays', 'Computational modeling', 'Pose estimation', 'Stacking', 'Transformers']","['Pose Estimation', 'Human Pose Estimation', 'Computational Efficiency', 'Local Relations', 'Local Dependence', '3D Pose', 'Local Information', 'Recent Models', '3D Space', 'Multilayer Perceptron', 'Batch Normalization', 'Real-world Scenarios', 'Noisy Data', 'Action Recognition', 'Graph Convolutional Network', 'Human Motion', 'Residual Connection', 'Center Of Frame', 'Transformer Architecture', '2D Pose', 'Position Embedding', 'Multi-head Self-attention', 'Transformer-based Methods']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",20,"Recent transformer-based approaches have demonstrated excellent performance in 3D human pose estimation. However, they have a holistic view and by encoding global relationships between all the joints, they do not capture the local dependencies precisely. In this paper, we present a novel Attention-GCNFormer (AGFormer) block that divides the number of channels by using two parallel transformer and GCNFormer streams. Our proposed GCNFormer module exploits the local relationship between adjacent joints, outputting a new representation that is complementary to the transformer output. By fusing these two representation in an adaptive way, AGFormer exhibits the ability to better learn the underlying 3D structure. By stacking multiple AGFormer blocks, we propose MotionAGFormer in four different variants, which can be chosen based on the speed-accuracy trade-off. We evaluate our model on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. MotionAGFormer-B achieves state-of-the-art results, with P1 errors of 38.4 mm and 16.2 mm, respectively. Remarkably, it uses a quarter of the parameters and is three times more computationally efficient than the previous leading model on Human3.6M dataset. Code and models are available at https://github.com/TaatiTeam/MotionAGFormer."
MotionGPT: Human Motion Synthesis With Improved Diversity and Realism via GPT-3 Prompting,"Jose Ribeiro-Gomes, Tianhui Cai, Zoltán Á. Milacski, Chen Wu, Aayush Prakash, Shingo Takagi, Amaury Aubel, Daeil Kim, Alexandre Bernardino, Fernando De la Torre","Meta, USA; Instituto Superior Tecnico, Lisbon, Portugal; Carnegie Mellon University, USA",66.66666666666666,"Portugal, USA",33.33333333333334,USA,"There are numerous applications for human motion synthesis, including animation, gaming, robotics, or sports science. In recent years, human motion generation from natural language has emerged as a promising alternative to costly and labor-intensive data collection methods relying on motion capture or wearable sensors (e.g., suits). Despite this, generating human motion from textual descriptions remains a challenging and intricate task, primarily due to the scarcity of large-scale supervised datasets capable of capturing the full diversity of human activity. This study proposes a new approach, called MotionGPT, to address the limitations of previous text-based human motion generation methods by utilizing the extensive semantic information available in large language models (LLMs). We first pretrain a doubly text-conditional motion diffusion model on both coarse (""high-level"") and detailed (""low-level"") ground truth text data. Then during inference, we improve motion diversity and alignment with the training set, by zero-shot prompting GPT-3 for additional ""low-level"" details. Our method achieves new state-of-the-art quantitative results in terms of Frechet Inception Distance (FID) and motion diversity metrics, and improves all considered metrics. Furthermore, it has strong qualitative performance, producing natural results.",https://openaccess.thecvf.com/content/WACV2024/html/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ribeiro-Gomes_MotionGPT_Human_Motion_Synthesis_With_Improved_Diversity_and_Realism_via_WACV_2024_paper.pdf,,https://github.com/humansensinglab/MotionGPT,,main,Poster,https://ieeexplore.ieee.org/document/10484383/,"['Measurement', 'Training', 'Semantics', 'Natural languages', 'Data augmentation', 'Robot sensing systems', 'Motion capture']","['Human Motion', 'Human Motion Synthesis', 'Training Set', 'Quantitative Results', 'Natural Language', 'Diffusion Model', 'Diversity Metrics', 'Motion Capture', 'Language Model', 'Sport Science', 'Textual Descriptions', 'Wearable Sensors', 'Distance Metrics', 'Text For Details', 'Inception Distance', 'Fréchet Inception Distance', 'Motion Generation', 'Training Data', 'Training Dataset', 'Data Augmentation', 'Description Of Motion', 'Forward Process', 'Vector C', 'Average Similarity', 'Similar Examples', 'Input Text', 'Additional Input', 'Variational Autoencoder', 'Linear Projection', 'Text Annotation']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Applications', 'Arts / games / social media']",1,"There are numerous applications for human motion synthesis, including animation, gaming, robotics, or sports science. In recent years, human motion generation from natural language has emerged as a promising alternative to costly and labor-intensive data collection methods relying on motion capture or wearable sensors (e.g., suits). Despite this, generating human motion from textual descriptions remains a challenging and intricate task, primarily due to the scarcity of large-scale supervised datasets capable of capturing the full diversity of human activity.This study proposes a new approach, called MotionGPT, to address the limitations of previous text-based human motion generation methods by utilizing the extensive semantic information available in large language models (LLMs). We first pretrain a doubly text-conditional motion diffusion model on both coarse (""high-level"") and detailed (""low-level"") ground truth text data. Then during inference, we improve motion diversity and alignment with the training set, by zero-shot prompting GPT-3 for additional ""low-level"" details. Our method achieves new state-of-the-art quantitative results in terms of Fréchet Inception Distance (FID) and motion diversity metrics, and improves all considered metrics. Furthermore, it has strong qualitative performance, producing natural results. Code is available at https://github.com/humansensinglab/MotionGPT"
Movie Genre Classification by Language Augmentation and Shot Sampling,"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer, Xin Miao, Jiayi Liu, Huayan Wang",Kuaishou Technology; Boston University,50.0,USA,50.0,China,"Video-based movie genre classification has garnered considerable attention due to its various applications in recommendation systems. Prior work has typically addressed this task by adapting models from traditional video classification tasks, such as action recognition or event detection. However, these models often neglect language elements (e.g., narrations or conversations) present in videos, which can implicitly convey high-level semantics of movie genres, like storylines or background context. Additionally, existing approaches are primarily designed to encode the entire content of the input video, leading to inefficiencies in predicting movie genres. Movie genre prediction may require only a few shots to accurately determine the genres, rendering a comprehensive understanding of the entire video unnecessary. To address these challenges, we propose a Movie genre Classification method based on Language augmentatIon and shot samPling (Movie-CLIP). Movie-CLIP mainly consists of two parts: a language augmentation module to recognize language elements from the input audio, and a shot sampling module to select representative shots from the entire video. We evaluate our method on MovieNet and Condensed Movies datasets, achieving approximate 6-9% improvement in mean Average Precision (mAP) over the baselines. We also generalize Movie-CLIP to the scene boundary detection task, achieving 1.1% improvement in Average Precision (AP) over the state-of-the-art. We release our implementation at github.com/Zhongping-Zhang/Movie-CLIP.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Movie_Genre_Classification_by_Language_Augmentation_and_Shot_Sampling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Movie_Genre_Classification_by_Language_Augmentation_and_Shot_Sampling_WACV_2024_paper.pdf,,http://this.http.URL,2203.13281,main,Poster,https://ieeexplore.ieee.org/document/10483588/,"['Uniform resource locators', 'Adaptation models', 'Event detection', 'Computational modeling', 'Semantics', 'Oral communication', 'Motion pictures']","['Movie Genres', 'Genre Classification', 'Narrative', 'Event Detection', 'Video Analysis', 'Average Precision', 'Action Recognition', 'Recommender Systems', 'Mean Average Precision', 'Boundary Detection', 'Entire Video', 'Elements Of Language', 'Audio Input', 'Visual Representation', 'Sampling Frame', 'Prediction Score', 'Transcription Initiation', 'Pronouns', 'Visual Modality', 'Textual Descriptions', 'Automatic Speech Recognition System', 'Language Information', 'Music Genres', 'Movie Clips', 'Language Mode', 'Sound Effects', 'Part Of The Input', 'Multimodal Features', 'Language Representation', 'Whispering']","['Applications', 'Arts / games / social media', 'Algorithms', 'Video recognition and understanding']",3,"Video-based movie genre classification has garnered considerable attention due to its various applications in recommendation systems. Prior work has typically addressed this task by adapting models from traditional video classification tasks, such as action recognition or event detection. However, these models often neglect language elements (e.g., narrations or conversations) present in videos, which can implicitly convey high-level semantics of movie genres, like storylines or background context. Additionally, existing approaches are primarily designed to encode the entire content of the input video, leading to inefficiencies in predicting movie genres. Movie genre prediction may require only a few shots
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
 to accurately determine the genres, rendering a comprehensive understanding of the entire video unnecessary. To address these challenges, we propose a Movie genre Classification method based on Language augmentatIon and shot samPling (Movie-CLIP). MovieCLIP mainly consists of two parts: a language augmentation module to recognize language elements from the input audio, and a shot sampling module to select representative shots from the entire video. We evaluate our method on MovieNet and Condensed Movies datasets, achieving approximate 6 − 9% improvement in mean Average Precision (mAP) over the baselines. We also generalize Movie-CLIP to the scene boundary detection task, achieving 1.1% improvement in Average Precision (AP) over the state-of-the-art. We release our implementation at this http URL."
MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction and Novel View Synthesis,"Xuqian Ren, Wenjia Wang, Dingding Cai, Tuuli Tuominen, Juho Kannala, Esa Rahtu","Aalto University, Finland; The University of Hong Kong, China; Tampere University, Finland",100.0,"Finland, Hong Kong",0.0,,"Metaverse technologies demand accurate, real-time, and immersive modeling on consumer-grade hardware for both non-human perception (e.g., drone/robot/autonomous car navigation) and immersive technologies like AR/VR, requiring both structural accuracy and photorealism. However, there exists a knowledge gap in how to apply geometric reconstruction and photorealism modeling (novel view synthesis) in a unified framework. To address this gap and promote the development of robust and immersive modeling and rendering with consumer-grade devices, first, we propose a real-world Multi-Sensor Hybrid Room Dataset (MuSHRoom). Our dataset presents exciting challenges and requires state-of-the-art methods to be cost-effective, robust to noisy data and devices, and can jointly learn 3D reconstruction and novel view synthesis, instead of treating them as separate tasks, making them ideal for real-world applications. Second, we benchmark several famous pipelines on our dataset for joint 3D mesh reconstruction and novel view synthesis. Finally, in order to further improve the overall performance, we propose a new method that achieves a good trade-off between the two tasks. Our dataset and benchmark show great potential in promoting the improvements for fusing 3D reconstruction and high-quality rendering in a robust and computationally efficient end-to-end fashion. The dataset and code is available at the project webpate: https://xuqianren.github. io/publications/MuSHRoom/.",https://openaccess.thecvf.com/content/WACV2024/html/Ren_MuSHRoom_Multi-Sensor_Hybrid_Room_Dataset_for_Joint_3D_Reconstruction_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ren_MuSHRoom_Multi-Sensor_Hybrid_Room_Dataset_for_Joint_3D_Reconstruction_and_WACV_2024_paper.pdf,https://xuqianren.github.io/publications/MuSHRoom/,https://github.com/xuqianren,2311.02778,main,Poster,https://ieeexplore.ieee.org/document/10483726/,"['Training', 'Photorealism', 'Three-dimensional displays', 'Pipelines', 'Benchmark testing', 'Rendering (computer graphics)', 'Real-time systems']","['3D Reconstruction', 'View Synthesis', 'Benchmark', 'Real-world Datasets', '3D Mesh', 'Training Set', 'Data Augmentation', 'Color Images', 'Raw Images', 'Point Cloud', 'Multilayer Perceptron', 'Evaluation Of Strategies', 'RGB Images', 'Depth Images', 'Inertial Measurement Unit', 'Evaluation Protocol', 'Multiple Devices', 'Reconstruction Quality', 'Motion Blur', 'Mesh Model', 'Accurate Geometry', 'Simultaneous Localization And Mapping', 'Bundle Adjustment', 'RGB-D Images', 'Real-world Challenges', 'Point Cloud Reconstruction', 'Depth Camera', 'Camera Position', 'Reconstruction Task', 'Ground Truth Reference']","['Algorithms', 'Datasets and evaluations', 'Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Metaverse technologies demand accurate, real-time, and immersive modeling on consumer-grade hardware for both non-human perception (e.g., drone/robot/autonomous car navigation) and immersive technologies like AR/VR, requiring both structural accuracy and photorealism. However, there exists a knowledge gap in how to apply geometric reconstruction and photorealism modeling (novel view synthesis) in a unified framework. To address this gap and promote the development of robust and immersive modeling and rendering with consumer-grade devices, first, we propose a real-world Multi-Sensor Hybrid Room Dataset (MuSHRoom). Our dataset presents exciting challenges and requires state-of-the-art methods to be cost-effective, robust to noisy data and devices, and can jointly learn 3D reconstruction and novel view synthesis instead of treating them as separate tasks, making them ideal for realworld applications. Second, we benchmark several famous pipelines on our dataset for joint 3D mesh reconstruction and novel view synthesis. Finally, in order to further improve the overall performance, we propose a new method that achieves a good trade-off between the two tasks. Our dataset and benchmark show great potential in promoting the improvements for fusing 3D reconstruction and highquality rendering in a robust and computationally efficient end-to-end fashion. The dataset and code are available at the project website: https://xuqianren.github.io/publications/MuSHRoom/."
Multi-Class Segmentation From Aerial Views Using Recursive Noise Diffusion,"Benedikt Kolbeinsson, Krystian Mikolajczyk",Imperial College London,100.0,UK,0.0,,"Semantic segmentation from aerial views is a crucial task for autonomous drones, as they rely on precise and accurate segmentation to navigate safely and efficiently. However, aerial images present unique challenges such as diverse viewpoints, extreme scale variations, and high scene complexity. In this paper, we propose an end-to-end multi-class semantic segmentation diffusion model that addresses these challenges. We introduce recursive denoising to allow information to propagate through the denoising process, as well as a hierarchical multi-scale approach that complements the diffusion process. Our method achieves promising results on the UAVid dataset and state-of-the-art performance on the Vaihingen Building segmentation benchmark. Being the first iteration of this method, it shows great promise for future improvements. Our code and models are available at: https://github.com/benediktkol/recursive-noise-diffusion",https://openaccess.thecvf.com/content/WACV2024/html/Kolbeinsson_Multi-Class_Segmentation_From_Aerial_Views_Using_Recursive_Noise_Diffusion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kolbeinsson_Multi-Class_Segmentation_From_Aerial_Views_Using_Recursive_Noise_Diffusion_WACV_2024_paper.pdf,,https://github.com/benediktkol/recursive-noise-diffusion,2212.00787,main,Poster,https://ieeexplore.ieee.org/document/10484298/,"['Industries', 'Navigation', 'Semantic segmentation', 'Computational modeling', 'Noise reduction', 'Buildings', 'Noise']","['Multi-class Segmentation', 'Promising Results', 'Diffusion Process', 'Diffusion Model', 'Aerial Images', 'Semantic Segmentation', 'Autonomous Drone', 'Loss Function', 'Time Step', 'Convolutional Neural Network', 'Number Of Steps', 'Image Pixels', 'Intersection Over Union', 'Receptive Field', 'Generative Adversarial Networks', 'Unmanned Aerial Vehicles', 'Segmentation Model', 'Task Model', 'Segmentation Task', 'Multi-scale Features', 'Number Of Time Steps', 'Binary Segmentation', 'Forward Process', 'Image X', 'Auxiliary Loss', 'Vertical Flip', 'Central Building', 'Recursive Approach', 'Segmentation Map', 'Gaussian Noise']","['Applications', 'Robotics', 'Algorithms', 'Image recognition and understanding']",4,"Semantic segmentation from aerial views is a crucial task for autonomous drones, as they rely on precise and accurate segmentation to navigate safely and efficiently. However, aerial images present unique challenges such as diverse viewpoints, extreme scale variations, and high scene complexity. In this paper, we propose an end-to-end multiclass semantic segmentation diffusion model that addresses these challenges. We introduce recursive denoising to allow information to propagate through the denoising process, as well as a hierarchical multi-scale approach that complements the diffusion process. Our method achieves promising results on the UAVid dataset and state-of-the-art performance on the Vaihingen Building segmentation benchmark. Being the first iteration of this method, it shows great promise for future improvements. Our code and models are available at: https://github.com/benediktkol/recursive-noise-diffusion"
Multi-Level Attention Aggregation for Aesthetic Face Relighting,"Hemanth Pidaparthy, Abhay Chauhan, Pavan Sudheendra","Samsung R&D Institute Bangalore, India",100.0,India,0.0,,"Face relighting is the challenging task of estimating the illumination cast on portrait images by a light source varying in both position and intensity. As shadows are an important aspect of relighting, many prior works focus on estimating accurate shadows using either a shadow mask or face geometry. While these work well, the rendered images do not look aesthetic/photo-realistic. We propose a novel method that combines the features from attention maps at higher resolutions with the lighting information to estimate aesthetic relit images with accurate shadows. We created a new relighting dataset using a synthetic One-Light-At-a-Time (OLAT) lighting rig in Blender software that captures most of the variations encountered in face relighting. Through extensive experimental validation, we show that the performance of our model is better than the current state-of-art face relighting models despite training on a significantly smaller dataset of only synthetic images. We also demonstrate unsupervised domain adaptation from synthetic to real images. We show that our model is able to adapt very well to significantly different out-of-training light source positions.",https://openaccess.thecvf.com/content/WACV2024/html/Pidaparthy_Multi-Level_Attention_Aggregation_for_Aesthetic_Face_Relighting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pidaparthy_Multi-Level_Attention_Aggregation_for_Aesthetic_Face_Relighting_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484450/,"['Training', 'Geometry', 'Adaptation models', 'Image resolution', 'Statistical analysis', 'Lighting', 'Software']","['Light Source', 'Synthetic Images', 'Light Intensity', 'Global Scale', 'Stage 2', 'Input Image', 'Single Image', 'Local Scale', 'Generative Adversarial Networks', 'Variable Positions', 'RGB Images', 'Attention Module', 'Source Images', 'Local Loss', 'Style Transfer', 'Attention Block', 'Photo-realistic Images', 'Smooth L1 Loss', '3D Human Model', 'Generative Adversarial Network Framework', 'Intensity Of The Light Source', 'Light Position', 'Source Intensity', 'Skin Color', 'Light Color', 'Test Dataset', 'Ground Truth Image', 'Reference Image', 'Color Jittering']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Visualization']",,"Face relighting is the challenging task of estimating the illumination cast on portrait images by a light source varying in both position and intensity. As shadows are an important aspect of relighting, many prior works focus on estimating accurate shadows using either a shadow mask or face geometry. While these work well, the rendered images do not look aesthetic/photo-realistic. We propose a novel method that combines the features from attention maps at higher resolutions with the lighting information to estimate aesthetic relit images with accurate shadows. We created a new relighting dataset using a synthetic One-Light-At-a-Time (OLAT) lighting rig in Blender software that captures most of the variations encountered in face relighting. Through extensive experimental validation, we show that the performance of our model is better than the current state-of-art face relighting models despite training on a significantly smaller dataset of only synthetic images. We also demonstrate unsupervised domain adaptation from synthetic to real images. We show that our model is able to adapt very well to significantly different out-of-training light source positions."
Multi-Modal Gaze Following in Conversational Scenarios,"Yuqi Hou, Zhongqun Zhang, Nora Horanyi, Jaewon Moon, Yihua Cheng, Hyung Jin Chang","University of Birmingham, United Kingdom; KETI, Korea",50.0,UK,50.0,South Korea,"Gaze following estimates gaze targets of in-scene person by understanding human behavior and scene information. Existing methods usually analyze scene images for gaze following. However, compared with visual images, audio also provides crucial cues for determining human behavior.This suggests that we can further improve gaze following considering audio cues. In this paper, we explore gaze following tasks in conversational scenarios. We propose a novel multi-modal gaze following framework based on our observation ""audiences tend to focus on the speaker"". We first leverage the correlation between audio and lips, and classify speakers and listeners in a scene. We then use the identity information to enhance scene images and propose a gaze candidate estimation network. The network estimates gaze candidates from enhanced scene images and we use MLP to match subjects with candidates as classification tasks. Existing gaze following datasets focus on visual images while ignore audios.To evaluate our method, we collect a conversational dataset, VideoGazeSpeech (VGS), which is the first gaze following dataset including images and audio. Our method significantly outperforms existing methods in VGS datasets. The visualization result also prove the advantage of audio cues in gaze following tasks. Our work will inspire more researches in multi-modal gaze following estimation.",https://openaccess.thecvf.com/content/WACV2024/html/Hou_Multi-Modal_Gaze_Following_in_Conversational_Scenarios_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hou_Multi-Modal_Gaze_Following_in_Conversational_Scenarios_WACV_2024_paper.pdf,,,2311.05669,main,Poster,https://ieeexplore.ieee.org/document/10484112/,"['Visualization', 'Computer vision', 'Correlation', 'Lips', 'Estimation', 'Human-robot interaction', 'Gaze tracking']","['Conversion Scenarios', 'Identity Information', 'Multilayer Perceptron', 'Visual Images', 'Scene Images', 'Matched Subjects', 'Audio Cues', 'Computer Vision', 'Visual Information', 'Feature Maps', 'Visual Cues', 'Visual Features', 'Eye-tracking', 'Bounding Box', 'Human-robot Interaction', 'Identity Mapping', '3D Datasets', 'L1 Loss', 'Speaker Recognition', 'Value R', 'Audio Information', 'Computer Vision Research', 'Raw Frames', 'Object Detection Task', 'SOTA Methods', 'Multimodal Input', 'Multimodal Information', 'Video Dataset', 'Fully Convolutional Network', 'Smooth L1 Loss']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations']",2,"Gaze following estimates gaze targets of in-scene person by understanding human behavior and scene information. Existing methods usually analyze scene images for gaze following. However, compared with visual images, audio also provides crucial cues for determining human behavior. This suggests that we can further improve gaze following considering audio cues. In this paper, we explore gaze following tasks in conversational scenarios. We propose a novel multimodal gaze following framework based on our observation ""audiences tend to focus on the speaker"". We first leverage the correlation between audio and lips, and classify speakers and listeners in a scene. We then use the identity information to enhance scene images and propose a gaze candidate estimation network. The network estimates gaze candidates from enhanced scene images and we use MLP to match subjects with candidates as classification tasks. Existing gaze following datasets focus on visual images while ignore audios. To evaluate our method, we collect a conversational dataset, VideoGazeSpeech (VGS), which is the first gaze following dataset including images and audio. Our method significantly outperforms existing methods in VGS datasets. The visualization result also prove the advantage of audio cues in gaze following tasks. Our work will inspire more researches in multi-modal gaze following estimation."
Multi-Source Domain Adaptation for Object Detection With Prototype-Based Mean Teacher,"Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger","GAIA Montreal, Ericsson Canada; LIVIA, Dept. of Systems Engineering, ETS Montreal, Canada",50.0,Canada,50.0,Canada,"Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. Recent studies have shown that when the labeled dataset comes from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over blending these source domains and performing a UDA. For adaptation, existing MSDA methods learn domain-invariant and domain-specific parameters (for each source domain). However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly in proportion to the number of source domains. This paper proposes a novel MSDA method called Prototype-based Mean Teacher (PMT), which uses class prototypes instead of domain-specific subnets to encode domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across domains and separating different categories far apart. Given the use of prototypes, the number of parameters required for our PMT method does not increase significantly with the number of source domains, thus reducing memory issues and possible overfitting. Empirical studies indicate that PMT outperforms state-of-the-art MSDA methods on several challenging object detection datasets. Our code is available at https://github.com/imatif17/Prototype-Mean-Teacher",https://openaccess.thecvf.com/content/WACV2024/html/Belal_Multi-Source_Domain_Adaptation_for_Object_Detection_With_Prototype-Based_Mean_Teacher_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Belal_Multi-Source_Domain_Adaptation_for_Object_Detection_With_Prototype-Based_Mean_Teacher_WACV_2024_paper.pdf,,https://github.com/imatif17/Prototype-Mean-Teacher,2309.14950,main,Poster,https://ieeexplore.ieee.org/document/10483923/,"['Visualization', 'Computer vision', 'Codes', 'Memory management', 'Prototypes', 'Object detection', 'Detectors']","['Object Detection', 'Mean Education', 'Domain Adaptation', 'Multi-source Domain Adaptation', 'Number Of Domains', 'Target Domain', 'Source Domain', 'Contrastive Loss', 'Class Prototypes', 'Domain-specific Information', 'Data Sources', 'Training Data', 'Feature Space', 'Teacher Model', 'Bounding Box', 'Domain Shift', 'Teacher Network', 'Adversarial Training', 'Source Dataset', 'Prototypical Network', 'Domain-invariant Features', 'Domain Discrepancy', 'Domain Discriminator', 'Unlabeled Images', 'Source Domain Data', 'Alignment Loss', 'Domain-specific Features', 'Detection Head', 'Feature Alignment']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",3,"Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. Recent studies have shown that when the labeled dataset comes from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over blending these source domains and performing a UDA. For adaptation, existing MSDA methods learn domain-invariant and domain-specific parameters (for each source domain). However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly in proportion to the number of source domains. This paper proposes a novel MSDA method called Prototype-based Mean Teacher (PMT), which uses class prototypes instead of domain-specific subnets to encode domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across domains and separating different categories far apart. Given the use of prototypes, the number of parameters required for our PMT method does not increase significantly with the number of source domains, thus reducing memory issues and possible overfitting. Empirical studies indicate that PMT outperforms state-of-the-art MSDA methods on several challenging object detection datasets. Our code is available at https://github.com/imatif17/Prototype-Mean-Teacher"
Multi-View 3D Object Reconstruction and Uncertainty Modelling With Neural Shape Prior,"Ziwei Liao, Steven L. Waslander","Robotics Institute & Institute for Aerospace Study, University of Toronto",100.0,Canada,0.0,,"3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view observations. We evaluate the system in both synthetic and real datasets to demonstrate the effectiveness of uncertainty-based fusion to improve 3D object reconstruction accuracy.",https://openaccess.thecvf.com/content/WACV2024/html/Liao_Multi-View_3D_Object_Reconstruction_and_Uncertainty_Modelling_With_Neural_Shape_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liao_Multi-View_3D_Object_Reconstruction_and_Uncertainty_Modelling_With_Neural_Shape_WACV_2024_paper.pdf,,,2306.11739,main,Poster,https://ieeexplore.ieee.org/document/10484515/,"['Solid modeling', 'Uncertainty', 'Three-dimensional displays', 'Codes', 'Simultaneous localization and mapping', 'Shape', 'Streaming media']","['3D Reconstruction', 'Model Uncertainty', 'Object Reconstruction', 'Input Image', 'Latent Space', 'Neural Representations', 'Fusion Method', 'Reconstruction Accuracy', 'Object Shape', '3D Shape', 'Partial Representation', 'Detailed Shape', 'Latent Code', 'Signed Distance Function', 'Monocular Images', 'Neural Network', 'Covariance Matrix', 'Difficult Task', 'Object Detection', 'Intersection Over Union', 'Energy Score', 'Implicit Representation', 'Point Cloud', 'Bayesian Neural Network', 'Structure From Motion', 'Marching Cubes Algorithm', 'Negative Log-likelihood', 'Pose Estimation', 'Scene Reconstruction', 'Image Encoder']","['Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Image recognition and understanding']",4,"3D object reconstruction is important for semantic scene understanding. It is challenging to reconstruct detailed 3D shapes from monocular images directly due to a lack of depth information, occlusion and noise. Most current methods generate deterministic object models without any awareness of the uncertainty of the reconstruction. We tackle this problem by leveraging a neural object representation which learns an object shape distribution from large dataset of 3d object models and maps it into a latent space. We propose a method to model uncertainty as part of the representation and define an uncertainty-aware encoder which generates latent codes with uncertainty directly from individual input images. Further, we propose a method to propagate the uncertainty in the latent code to SDF values and generate a 3d object mesh with local uncertainty for each mesh component. Finally, we propose an incremental fusion method under a Bayesian framework to fuse the latent codes from multi-view observations. We evaluate the system in both synthetic and real datasets to demonstrate the effectiveness of uncertainty-based fusion to improve 3D object reconstruction accuracy."
Multi-View Classification Using Hybrid Fusion and Mutual Distillation,"Samuel Black, Richard Souvenir","Department of Computer and Information Sciences, Temple University, USA",100.0,USA,0.0,,"Multi-view classification problems are common in medical image analysis, forensics, and other domains where problem queries involve multi-image input. Existing multi-view classification methods are often tailored to a specific task. In this paper, we repurpose off-the-shelf Hybrid CNN-Transformer networks for multi-view classification with either structured or unstructured views. Our approach incorporates a novel fusion scheme, mutual distillation, and introduces minimal additional parameters. We demonstrate the effectiveness and generalization capability of our approach, MV-HFMD, on multiple multi-view classification tasks and show that it outperforms other multi-view approaches, even task-specific methods. Code is available at https://github.com/vidarlab/multi-view-hybrid.",https://openaccess.thecvf.com/content/WACV2024/html/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Black_Multi-View_Classification_Using_Hybrid_Fusion_and_Mutual_Distillation_WACV_2024_paper.pdf,,https://github.com/vidarlab/multi-view-hybrid,,main,Poster,https://ieeexplore.ieee.org/document/10483891/,"['Computer vision', 'Adaptation models', 'Image analysis', 'Codes', 'Forensics', 'Computational modeling', 'Computer architecture']","['Hybrid Fusion', 'Multi-view Classification', 'Mutual Distillation', 'Fusion Strategy', 'Medical Image Analysis', 'Loss Function', 'Feature Maps', 'Image Pairs', 'Activation Maps', 'Action Recognition', 'Ground Truth Labels', 'Training Examples', 'Image Collection', 'Classification Loss', 'Modern Approaches', 'Loss Term', 'Student Model', 'Test Split', 'Mutual Learning', 'Graph Convolution', 'Distillation Method', 'Late Fusion', 'Encoder Block']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Multi-view classification problems are common in medical image analysis, forensics, and other domains where problem queries involve multi-image input. Existing multi-view classification methods are often tailored to a specific task. In this paper, we repurpose off-the-shelf Hybrid CNN-Transformer networks for multi-view classification with either structured or unstructured views. Our approach incorporates a novel fusion scheme, mutual distillation, and minimal additional parameters. We demonstrate the effectiveness and generalization capability of our approach, MV-HFMD, on multiple multi-view classification tasks and show that it outperforms other multi-view approaches, even task-specific methods. Code is available at https://github.com/vidarlab/multi-view-hybrid."
Multimodal Channel-Mixing: Channel and Spatial Masked AutoEncoder on Facial Action Unit Detection,"Xiang Zhang, Huiyuan Yang, Taoyue Wang, Xiaotian Li, Lijun Yin",Rice University; State University of New York at Binghamton,100.0,USA,0.0,,"Recent studies have focused on utilizing multi-modal data to develop robust models for facial Action Unit (AU) detection. However, the heterogeneity of multi-modal data poses challenges in learning effective representations. One such challenge is extracting relevant features from multiple modalities using a single feature extractor. Moreover, previous studies have not fully explored the potential of multi-modal fusion strategies. In contrast to the extensive work on late fusion, there are limited investigations on early fusion for channel information exploration. This paper presents a novel multi-modal reconstruction network, named Multimodal Channel-Mixing (MCM), as a pre-trained model to learn robust representation for facilitating multi-modal fusion. The approach follows an early fusion setup, integrating a Channel-Mixing module, where two out of five channels are randomly dropped. The dropped channels then are reconstructed from the remaining channels using masked autoencoder. This module not only reduces channel redundancy, but also facilitates multi-modal learning and reconstruction capabilities, resulting in robust feature learning. The encoder is fine-tuned on a downstream task of automatic facial action unit detection. Pretraining experiments were conducted on BP4D+, followed by fine-tuning on BP4D and DISFA to assess the effectiveness and robustness of the proposed framework. The results demonstrate that our method meets and surpasses the performance of state-of-the-art baseline method.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Multimodal_Channel-Mixing_Channel_and_Spatial_Masked_AutoEncoder_on_Facial_Action_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Multimodal_Channel-Mixing_Channel_and_Spatial_Masked_AutoEncoder_on_Facial_Action_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483740/,"['Representation learning', 'Gold', 'Computer vision', 'Redundancy', 'Feature extraction', 'Robustness', 'Data models']","['Facial Action', 'Action Units', 'Facial Action Units', 'Action Unit Detection', 'Facial Action Unit Detection', 'Masked Autoencoder', 'Multiple Modalities', 'Baseline Methods', 'Multimodal Learning', 'Early Fusion', 'Late Fusion', 'Computer Vision', 'Infrared Imaging', 'Image Reconstruction', 'Bounding Box', 'Face Recognition', 'RGB Images', 'Multimodal Approach', 'Depth Images', 'Reconstruction Results', 'Facial Action Coding System', 'Pre-training Stage', 'Self-supervised Learning', 'Multimodal Methods', 'Facial Expression Recognition', 'Latent Features', 'Transformer Block', 'Fine-tuning Stage', 'Position Embedding', 'Reconstruction Task']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Image recognition and understanding']",3,"Recent studies have focused on utilizing multi-modal data to develop robust models for facial Action Unit (AU) detection. However, the heterogeneity of multi-modal data poses challenges in learning effective representations. One such challenge is extracting relevant features from multiple modalities using a single feature extractor. Moreover, previous studies have not fully explored the potential of multi-modal fusion strategies. In contrast to the extensive work on late fusion, there are limited investigations on early fusion for channel information exploration. This paper presents a novel multi-modal reconstruction network, named Multimodal Channel-Mixing (MCM), as a pre-trained model to learn robust representation for facilitating multi-modal fusion. The approach follows an early fusion setup, integrating a Channel-Mixing module, where two out of five channels are randomly dropped. The dropped channels then are reconstructed from the remaining channels using masked autoencoder. This module not only reduces channel redundancy, but also facilitates multi-modal learning and reconstruction capabilities, resulting in robust feature learning. The encoder is fine-tuned on a downstream task of automatic facial action unit detection. Pre-training experiments were conducted on BP4D+, followed by fine-tuning on BP4D and DISFA to assess the effectiveness and robustness of the proposed framework. The results demonstrate that our method meets and surpasses the performance of state-of-the-art baseline methods."
Multimodal Deep Learning for Remote Stress Estimation Using CCT-LSTM,"Sayyedjavad Ziaratnia, Tipporn Laohakangvalvit, Midori Sugaya, Peeraya Sripian","College of Engineering, Shibaura Institute of Technology, Tokyo, Japan",100.0,Japan,0.0,,"Stress estimation is key to the early detection and mitigation of health problems, enhancing driving safety through driver stress monitoring, and improving human-robot interaction efficiency by adapting to user's stress levels. In this paper, we present a novel method for video-based remote stress estimation and categorization, which involves two separate experiments: one for stress task classification and another for multilevel stress classification. The method combines two deep learning approaches, the Compact Convolutional Transformer (CCT) and Long Short-Term Memory (LSTM), to form a CCT-LSTM pipeline. For each modality (facial expression and rPPG), a CCT model is used to extract features, followed by an LSTM block for temporal pattern recognition. In stress task classification, T1, T2, and T3 tasks from the UBFC-Phys dataset are used, utilizing sevenfold cross-validation. The results indicated a mean accuracy of 83.2% and an F1 score of 83.4%. For multilevel stress classification, the control (lower stress) and test (higher stress) groups from the same dataset were used with fivefold cross-validation, achieving a mean accuracy of 80.5% and an F1 score of 80.3%. The results suggest that our proposed model surpasses existing stress estimation methods by effectively using multimodal deep learning and the CCT-LSTM pipeline for precise, non-invasive stress detection and categorization, with applications in health monitoring, safety, and interactive technologies.",https://openaccess.thecvf.com/content/WACV2024/html/Ziaratnia_Multimodal_Deep_Learning_for_Remote_Stress_Estimation_Using_CCT-LSTM_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ziaratnia_Multimodal_Deep_Learning_for_Remote_Stress_Estimation_Using_CCT-LSTM_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483750/,"['Deep learning', 'Pipelines', 'Estimation', 'Human factors', 'Benchmark testing', 'Feature extraction', 'Safety']","['Deep Learning', 'Multimodal Learning', 'Remote Stress', 'Stress Levels', 'Classification Task', 'Short-term Memory', 'Test Group', 'F1 Score', 'Facial Expressions', 'Low Stress', 'Long Short-term Memory', 'Five-fold Cross-validation', 'Human-robot Interaction', 'Stress Task', 'Stress Detection', 'Stress Monitoring', 'Sevenfold Cross-validation', 'Health Monitoring Applications', 'Convolutional Neural Network', 'Challenging Task', 'Heart Rate Variability', 'Vision Transformer', 'Highest F1 Score', 'Binary Classification', 'Class Level', 'Number Of Landmarks', 'Multimodal Approach', 'Time Series Data', 'Arithmetic Task', 'Computer Vision']","['Applications', 'Remote Sensing', 'Applications', 'Biomedical / healthcare / medicine']",,"Stress estimation is key to the early detection and mitigation of health problems, enhancing driving safety through driver stress monitoring, and improving human–robot interaction efficiency by adapting to user’s stress levels. In this paper, we present a novel method for video-based remote stress estimation and categorization, which involves two separate experiments: one for stress task classification and another for multilevel stress classification. The method combines two deep learning approaches, the Compact Convolutional Transformer (CCT) and Long Short-Term Memory (LSTM), to form a CCT-LSTM pipeline. For each modality (facial expression and rPPG), a CCT model is used to extract features, followed by an LSTM block for temporal pattern recognition. In stress task classification, T1, T2, and T3 tasks from the UBFC-Phys dataset are used, utilizing sevenfold cross-validation. The results indicated a mean accuracy of 83.2% and an F1 score of 83.4%. For multilevel stress classification, the control (lower stress) and test (higher stress) groups from the same dataset were used with fivefold cross-validation, achieving a mean accuracy of 80.5% and an F1 score of 80.3%. The results suggest that our proposed model surpasses existing stress estimation methods by effectively using multimodal deep learning and the CCT-LSTM pipeline for precise, non-invasive stress detection and categorization, with applications in health monitoring, safety, and interactive technologies."
Multimodality-Guided Image Style Transfer Using Cross-Modal GAN Inversion,"Hanyu Wang, Pengxiang Wu, Kevin Dela Rosa, Chen Wang, Abhinav Shrivastava","University of Maryland, College Park; Snap Inc.",50.0,USA,50.0,USA,"Image Style Transfer (IST) is an interdisciplinary topic of computer vision and art that continuously attracts researchers' interests. Different from traditional Image-guided Image Style Transfer (IIST) methods that require a style reference image as input to define the desired style, recent works start to tackle the problem in a text-guided manner, i.e., Text-guided Image Style Transfer (TIST). Compared to IIST, such approaches provide more flexibility with text-specified styles, which are useful in scenarios where the style is hard to define with reference images. Unfortunately, many TIST approaches produce undesirable artifacts in the transferred images. To address this issue, we present a novel method to achieve much improved style transfer based on text guidance. Meanwhile, to offer more flexibility than IIST and TIST, our method allows style inputs from multiple sources and modalities, enabling MultiModality-guided Image Style Transfer (MMIST). Specifically, we realize MMIST with a novel cross-modal GAN inversion method, which generates style representations consistent with specified styles. Such style representations facilitate style transfer and in principle generalize any IIST methods to MMIST. Large-scale experiments and user studies demonstrate that our method achieves state-of-the-art performance on TIST task. Furthermore, comprehensive qualitative results confirm the effectiveness of our method on MMIST task and cross-modal style interpolation.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Multimodality-Guided_Image_Style_Transfer_Using_Cross-Modal_GAN_Inversion_WACV_2024_paper.pdf,,,2312.01671,main,Poster,https://ieeexplore.ieee.org/document/10484159/,"['Computer vision', 'Interpolation', 'Art', 'Task analysis']","['Style Transfer', 'Style Image', 'Image Style Transfer', 'GAN Inversion', 'User Study', 'Multiple Modalities', 'Input Image', 'Use Of Imaging', 'Feed-forward Network', 'Latent Space', 'Multiple Inputs', 'Textual Descriptions', 'Loss Of Content', 'Single Pass', 'Multiple Reference', 'Intermediate Representation', 'Multiple Styles']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities']",4,"Image Style Transfer (IST) is an interdisciplinary topic of computer vision and art that continuously attracts researchers’ interests. Different from traditional Image-guided Image Style Transfer (IIST) methods that require a style reference image as input to define the desired style, recent works start to tackle the problem in a text-guided manner, i.e., Text-guided Image Style Transfer (TIST). Compared to IIST, such approaches provide more flexibility with text-specified styles, which are useful in scenarios where the style is hard to define with reference images. Unfortunately, many TIST approaches produce undesirable artifacts in the transferred images. To address this issue, we present a novel method to achieve much improved style transfer based on text guidance. Meanwhile, to offer more flexibility than IIST and TIST, our method allows style inputs from multiple sources and modalities, enabling Multi-Modality-guided Image Style Transfer (MMIST). Specifically, we realize MMIST with a novel cross-modal GAN inversion method, which generates style representations consistent with specified styles. Such style representations facilitate style transfer and in principle generalize any IIST methods to MMIST. Large-scale experiments and user studies demonstrate that our method achieves state-of-the-art performance on TIST task. Furthermore, comprehensive qualitative results confirm the effectiveness of our method on MMIST task and cross-modal style interpolation."
Multispectral Imaging for Differential Face Morphing Attack Detection: A Preliminary Study,"Raghavendra Ramachandra, Sushma Venkatesh, Naser Damer, Narayan Vetrekar, R. S. Gad","School of Physical and Applied Sciences, Goa University, Goa, India.; Fraunhofer Institute for Computer Graphics Research, Germany.; Norwegian University of Science and Technology (NTNU), Gjøvik, Norway.",100.0,"Germany, India, Norway",0.0,,"Face morphing attack detection is emerging as an increasingly challenging problem owing to advancements in high-quality and realistic morphing attack generation. Reliable detection of morphing attacks is essential because these attacks are targeted for border control applications. This paper presents a multispectral framework for differential morphing-attack detection (D-MAD). The D-MAD methods are based on using two facial images that are captured from the ePassport (also called the reference image) and the trusted device (for example, Automatic Border Control (ABC) gates) to detect whether the face image presented in ePassport is morphed. The proposed multispectral D-MAD framework introduce a multispectral image captured as a trusted capture to acquire seven different spectral bands to detect morphing attacks. Extensive experiments were conducted on the newly created Multispectral Morphed Datasets (MSMD) with 143 unique data subjects that were captured using both visible and multispectral cameras in multiple sessions. The results indicate the superior performance of the proposed multispectral framework compared to visible images.",https://openaccess.thecvf.com/content/WACV2024/html/Ramachandra_Multispectral_Imaging_for_Differential_Face_Morphing_Attack_Detection_A_Preliminary_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ramachandra_Multispectral_Imaging_for_Differential_Face_Morphing_Attack_Detection_A_Preliminary_WACV_2024_paper.pdf,,,2304.03510,main,Poster,https://ieeexplore.ieee.org/document/10483714/,"['Computer vision', 'Multispectral imaging', 'Image synthesis', 'Logic gates', 'Image capture', 'Feature extraction', 'Cameras']","['Multispectral Images', 'Morphed Faces', 'Morphing Attack', 'Cross-border', 'Spectral Bands', 'Extensive Experiments', 'Data Subject', 'Reliable Detection', 'Reference Image', 'Face Images', 'Image Capture', 'Detection Framework', 'Visible Images', 'Unifying Theme', 'Multispectral Camera', 'Training Set', 'Visible Light', 'Detection Accuracy', 'Live Imaging', 'Detection Performance', 'Multispectral Data', 'Facial Features', 'Visible Bands', 'Individual Bands', 'Face Recognition', 'Texture Features', 'Quantitative Performance', 'Technical Performance', 'Deep Features']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Datasets and evaluations', 'Applications', 'Smartphones / end user devices']",2,"Face morphing attack detection is emerging as an increasingly challenging problem owing to advancements in high-quality and realistic morphing attack generation. Reliable detection of morphing attacks is essential because these attacks are targeted for border control applications. This paper presents a multispectral framework for differential morphing-attack detection (D-MAD). The D-MAD methods are based on using two facial images that are captured from the ePassport (also called the reference image) and the trusted device (for example, Automatic Border Control (ABC) gates) to detect whether the face image presented in ePassport is morphed. The proposed multi-spectral D-MAD framework introduce a multispectral image captured as a trusted capture to acquire seven different spectral bands to detect morphing attacks. Extensive experiments were conducted on the newly created Multispectral Morphed Datasets (MSMD) with 143 unique data subjects that were captured using both visible and multispectral cameras in multiple sessions. The results indicate the superior performance of the proposed multispectral framework compared to visible images."
Multitask Vision-Language Prompt Tuning,"Sheng Shen, Shijia Yang, Tianjun Zhang, Bohan Zhai, Joseph E. Gonzalez, Kurt Keutzer, Trevor Darrell","University of California, Berkeley",100.0,USA,0.0,,"Prompt Tuning, conditioning on task-specific learned prompt vectors, has emerged as a data-efficient and parameter-efficient method for adapting large pretrained vision-language models to multiple downstream tasks. However, existing approaches usually consider learning prompt vectors for each task independently from scratch, thereby failing to exploit the rich shareable knowledge across different vision-language tasks. In this paper, we propose multitask vision-language prompt tuning (MVLPT), which incorporates cross-task knowledge into prompt tuning for vision-language models. Specifically, (i) we demonstrate the effectiveness of learning a single transferable prompt from multiple source tasks to initialize the prompt for each target task; (ii) we show many target tasks can benefit each other from sharing prompt vectors and thus can be jointly learned via multitask prompt tuning. We benchmark the proposed MVLPT using three representative prompt tuning methods, namely text prompt tuning, visual prompt tuning, and the unified vision-language prompt tuning. Results in 20 vision tasks demonstrate that the proposed approach outperforms all single-task baseline prompt tuning methods, setting the new state-of-the-art on the few-shot ELEVATER benchmarks and cross-task generalization benchmarks. To understand where the cross-task knowledge is most effective, we also conduct a large-scale study on task transferability with 20 vision tasks in 400 combinations for each prompt tuning method. It shows that the most performant MVLPT for each prompt tuning method prefers different task combinations and many tasks can benefit each other, depending on their visual similarity and label similarity.",https://openaccess.thecvf.com/content/WACV2024/html/Shen_Multitask_Vision-Language_Prompt_Tuning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shen_Multitask_Vision-Language_Prompt_Tuning_WACV_2024_paper.pdf,,,2211.11720,main,Poster,https://ieeexplore.ieee.org/document/10483599/,"['Learning systems', 'Visualization', 'Computer vision', 'Adaptation models', 'Benchmark testing', 'Vectors', 'Task analysis']","['Multiple Tasks', 'Target Task', 'Transfer Task', 'Source Task', 'Training Set', 'Multi-label', 'Image Recognition', 'Language Model', 'Random Initialization', 'Rich Literature', 'Multi-task Learning', 'Large-scale Models', 'Few-shot Learning', 'Image Recognition Tasks', 'Image Encoder', 'Visual Encoding', 'Text Encoder']","['Algorithms', 'Vision + language and/or other modalities']",5,"Prompt Tuning, conditioning on task-specific learned prompt vectors, has emerged as a data-efficient and parameter-efficient method for adapting large pretrained vision-language models to multiple downstream tasks. However, existing approaches usually consider learning prompt vectors for each task independently from scratch, thereby failing to exploit the rich shareable knowledge across different vision-language tasks. In this paper, we propose multitask vision-language prompt tuning (MVLPT), which incorporates cross-task knowledge into prompt tuning for vision-language models. Specifically, (i) we demonstrate the effectiveness of learning a single transferable prompt from multiple source tasks to initialize the prompt for each target task; (ii) we show many target tasks can benefit each other from sharing prompt vectors and thus can be jointly learned via multitask prompt tuning. We benchmark the proposed MVLPT using three representative prompt tuning methods, namely text prompt tuning, visual prompt tuning, and the unified vision-language prompt tuning. Results in 20 vision tasks demonstrate that the proposed approach outperforms all single-task baseline prompt tuning methods, setting the new state-of-the-art on the few-shot Elevater benchmarks and cross-task generalization benchmarks. To understand where the cross-task knowledge is most effective, we also conduct a large-scale study on task transferability with 20 vision tasks in 400 combinations for each prompt tuning method. It shows that the most performant MVLPT for each prompt tuning method prefers different task combinations and many tasks can benefit each other, depending on their visual similarity and label similarity."
NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations,"Sungmin Cha, Naeun Ko, Heewoong Choi, Youngjoon Yoo, Taesup Moon",New York University; ASRI / INMC / Seoul National University; NAVER Cloud / NAVER AI Lab; NAVER Cloud,50.0,"Canada, South Korea, USA",50.0,South Korea,"We propose a novel and effective purification-based adversarial defense method against pre-processor blind white- and black-box attacks, without requiring any adversarial training or retraining of the classification model. Based on the observation of the adversarial noise, we propose a simple iterative Gaussian Smoothing (GS) that smoothes out adversarial noise and achieves substantially high robust accuracy. To further improve the method, we propose Neural Contextual Iterative Smoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised manner to reconstruct the discriminative features of the smoothed original image. From the extensive experiments on the large-scale ImageNet, we show that our method achieves both competitive standard accuracy and state-of-the-art robust accuracy against most strong purifier-blind white- and black-box attacks. Also, we propose a new evaluation benchmark based on commercial image classification APIs, including AWS, Azure, Clarifai, and Google, and demonstrate that users can use our method to increase the adversarial robustness of APIs.",https://openaccess.thecvf.com/content/WACV2024/html/Cha_NCIS_Neural_Contextual_Iterative_Smoothing_for_Purifying_Adversarial_Perturbations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cha_NCIS_Neural_Contextual_Iterative_Smoothing_for_Purifying_Adversarial_Perturbations_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483882/,"['Training', 'Smoothing methods', 'Perturbation methods', 'Noise', 'Closed box', 'Robustness', 'Internet']","['Adversarial Perturbations', 'Iterative Smoothing', 'Classification Model', 'Image Classification', 'Discriminative Features', 'Gaussian Blur', 'Adversarial Training', 'Defense Methods', 'Self-supervised Manner', 'Adversarial Robustness', 'Black-box Attacks', 'Gaussian Kernel', 'Denoising', 'Input Image', 'Smooth Function', 'Training Methods', 'Convolution Operation', 'Purification Methods', 'Clear Image', 'Symmetric Distribution', 'Adversarial Examples', 'White-box Attack', 'Types Of Attacks', 'Adversarial Attacks', 'Symmetric Kernel', 'ImageNet Pre-trained Model', 'Image X', 'Inference Time', 'Set Of Attacks', 'Official Code']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Low-level and physics-based vision']",,"We propose a novel and effective purification-based adversarial defense method against pre-processor blind white-and black-box attacks, without requiring any adversarial training or retraining of the classification model. Based on the observation of the adversarial noise, we propose a simple iterative Gaussian Smoothing (GS) that smoothes out adversarial noise and achieves substantially high robust accuracy. To further improve the method, we propose Neural Contextual Iterative Smoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised manner to reconstruct the discriminative features of the smoothed original image. From the extensive experiments on the large-scale ImageNet, we show that our method achieves both competitive standard accuracy and state-of-the-art robust accuracy against most strong purifier-blind white- and black-box attacks. Also, we propose a new evaluation benchmark based on commercial image classification APIs, including AWS, Azure, Clarifai, and Google, and demonstrate that users can use our method to increase the adversarial robustness of APIs."
NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction,"Thorsten Hempel, Magnus Jung, Ahmed A. Abdelrahman, Ayoub Al-Hamadi","Neuro-Information Technology Group, Otto von Guericke University, Magdeburg, Germany",100.0,Germany,0.0,,"Eye contact is a crucial non-verbal interaction modality and plays an important role in our everyday social life. While humans are very sensitive to eye contact, the capabilities of machines to capture a person's gaze are still mediocre. We tackle this challenge and present NITEC, a hand-annotated eye contact dataset for ego-vision interaction. NITEC exceeds existing datasets for ego-vision eye contact in size and variety of demographics, social contexts, and lighting conditions, making it a valuable resource for advancing ego-vision-based eye contact research. Our extensive evaluations on NITEC demonstrate strong cross-dataset performance, emphasizing its effectiveness and adaptability in various scenarios, that allows seamless utilization to the fields of computer vision, human-computer interaction, and social robotics. We make our NITEC dataset publicly available to foster reproducibility and further exploration in the field of ego-vision interaction.",https://openaccess.thecvf.com/content/WACV2024/html/Hempel_NITEC_Versatile_Hand-Annotated_Eye_Contact_Dataset_for_Ego-Vision_Interaction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hempel_NITEC_Versatile_Hand-Annotated_Eye_Contact_Dataset_for_Ego-Vision_Interaction_WACV_2024_paper.pdf,,https://github.com/thohemp/nitec,2311.04505,main,Poster,https://ieeexplore.ieee.org/document/10484276/,"['Human computer interaction', 'Computer vision', 'Computational modeling', 'Lighting', 'Reproducibility of results']","['Eye Contact', 'Human-computer Interaction', 'Social Robots', 'Field Exploration', 'Non-verbal Interactions', 'Everyday Social Life', 'Training Set', 'Test Dataset', 'Pedestrian', 'Facial Features', 'Average Precision', 'Quality Of Dataset', 'Interaction Field', 'Gaze Direction', 'Human-robot Interaction', 'Humanoid Robot', 'ResNet-50 Model', 'Exemplary Images', 'Head Pose', 'ResNet-18 Model', 'Label Noise']","['Algorithms', 'Datasets and evaluations', 'Applications', 'Robotics']",1,"Eye contact is a crucial non-verbal interaction modality and plays an important role in our everyday social life. While humans are very sensitive to eye contact, the capabilities of machines to capture a person’s gaze are still mediocre. We tackle this challenge and present NITEC, a hand-annotated eye contact dataset for ego-vision interaction. NITEC exceeds existing datasets for ego-vision eye contact in size and variety of demographics, social contexts, and lighting conditions, making it a valuable resource for advancing ego-vision-based eye contact research. Our extensive evaluations on NITEC demonstrate strong cross-dataset performance, emphasizing its effectiveness and adaptability in various scenarios, that allows seamless utilization to the fields of computer vision, human-computer interaction, and social robotics. We make our NITEC dataset publicly available to foster reproducibility and further exploration in the field of ego-vision interaction
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
"NOMAD: A Natural, Occluded, Multi-Scale Aerial Dataset, for Emergency Response Scenarios","Arturo Miguel Russell Bernal, Walter Scheirer, Jane Cleland-Huang","Computer Science and Engineering Department, University of Notre Dame, Indiana, USA",100.0,USA,0.0,,"With the increasing reliance on small Unmanned Aerial Systems (sUAS) for Emergency Response Scenarios, such as Search and Rescue, the integration of computer vision capabilities has become a key factor in mission success. Nevertheless, computer vision performance for detecting humans severely degrades when shifting from ground to aerial views. Several aerial datasets have been created to mitigate this problem, however, none of them has specifically addressed the issue of occlusion, a critical component in Emergency Response Scenarios. Natural Occluded Multi-scale Aerial Dataset (NOMAD) presents a benchmark for human detection under occluded aerial views, with five different aerial distances and rich imagery variance. NOMAD is composed of 100 different Actors, all performing sequences of walking, laying and hiding. It includes 42,825 frames, extracted from 5.4k resolution videos, and manually annotated with a bounding box and a label describing 10 different visibility levels, categorized according to the percentage of the human body visible inside the bounding box. This allows computer vision models to be evaluated on their detection performance across different ranges of occlusion. NOMAD is designed to improve the effectiveness of aerial search and rescue and to enhance collaboration between sUAS and humans, by providing a new benchmark dataset for human detection under occluded aerial views.",https://openaccess.thecvf.com/content/WACV2024/html/Bernal_NOMAD_A_Natural_Occluded_Multi-Scale_Aerial_Dataset_for_Emergency_Response_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bernal_NOMAD_A_Natural_Occluded_Multi-Scale_Aerial_Dataset_for_Emergency_Response_WACV_2024_paper.pdf,,,2309.09518,main,Poster,https://ieeexplore.ieee.org/document/10483999/,"['Legged locomotion', 'Computer vision', 'Image resolution', 'Computational modeling', 'Biological system modeling', 'Collaboration', 'Benchmark testing']","['Emergency Response', 'Aerial Dataset', 'Emergency Response Scenarios', 'Computer Vision', 'Benchmark Datasets', 'Bounding Box', 'Video Resolution', 'Unmanned Aerial Systems', 'Body Parts', 'Local Variations', 'Less Than Or Equal', 'Data Collection Process', 'Object Detection', 'Diverse Environments', 'Detection Task', 'Natural Behavior', 'Mobile Robot', 'Closest Distance', 'IRB Protocol', 'Racial Distribution', 'Occlusion Level', 'Views Of Actors', 'HSV Color', 'Emergency Responders', 'Ground Sampling Distance', 'First-person View', 'Re-identification Task', 'Missing Persons']","['Applications', 'Social good', 'Algorithms', 'Datasets and evaluations', 'Applications', 'Robotics']",,"With the increasing reliance on small Unmanned Aerial Systems (sUAS) for Emergency Response Scenarios, such as Search and Rescue, the integration of computer vision capabilities has become a key factor in mission success. Nevertheless, computer vision performance for detecting humans severely degrades when shifting from ground to aerial views. Several aerial datasets have been created to mitigate this problem, however, none of them has specifically addressed the issue of occlusion, a critical component in Emergency Response Scenarios. Natural Occluded Multi-scale Aerial Dataset (NOMAD) presents a benchmark for human detection under occluded aerial views, with five different aerial distances and rich imagery variance. NOMAD is composed of 100 different Actors, all performing sequences of walking, laying and hiding. It includes 42,825 frames, extracted from 5.4k resolution videos, and manually annotated with a bounding box and a label describing 10 different visibility levels, categorized according to the percentage of the human body visible inside the bounding box. This allows computer vision models to be evaluated on their detection performance across different ranges of occlusion. NOMAD is designed to improve the effectiveness of aerial search and rescue and to enhance collaboration between sUAS and humans, by providing a new benchmark dataset for human detection under occluded aerial views."
NVAutoNet: Fast and Accurate 360deg 3D Visual Perception for Self Driving,"Trung Pham, Mehran Maghoumi, Wanli Jiang, Bala Siva Sashank Jujjavarapu, Mehdi Sajjadi, Xin Liu, Hsuan-Chu Lin, Bor-Jeng Chen, Giang Truong, Chao Fang, Junghyun Kwon, Minwoo Park",NVIDIA,0.0,,100.0,USA,"Achieving robust and real-time 3D perception is fundamental for autonomous vehicles. While most existing 3D perception methods prioritize detection accuracy, they often overlook critical aspects such as computational efficiency, onboard chip deployment friendliness, resilience to sensor mounting deviations, and adaptability to various vehicle types. To address these challenges, we present NVAutoNet: a specialized Bird's-Eye-View (BEV) perception network tailored explicitly for automated vehicles. NVAutoNet takes synchronized camera images as input and predicts 3D signals like obstacles, freespaces, and parking spaces. The core of NVAutoNet's architecture (image and BEV backbones) relies on efficient convolutional networks, optimized for high performance using TensorRT. Our image-to-BEV transformation employs simple linear layers and BEV look-up tables, ensuring rapid inference speed. Trained on an extensive proprietary dataset, NVAutoNet consistently achieves elevated perception accuracy, operating remarkably at 53 frames per second on the NVIDIA DRIVE Orin SoC. Notably, NVAutoNet demonstrates resilience to sensor mounting deviations arising from diverse car models. Moreover, NVAutoNet excels in adapting to varied vehicle types, facilitated by inexpensive model fine-tuning procedures that expedite compatibility adjustments.",https://openaccess.thecvf.com/content/WACV2024/html/Pham_NVAutoNet_Fast_and_Accurate_360deg_3D_Visual_Perception_for_Self_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pham_NVAutoNet_Fast_and_Accurate_360deg_3D_Visual_Perception_for_Self_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Natural Light Can Also Be Dangerous: Traffic Sign Misinterpretation Under Adversarial Natural Light Attacks,"Teng-Fang Hsiao, Bo-Lun Huang, Zi-Xiang Ni, Yan-Ting Lin, Hong-Han Shuai, Yung-Hui Li, Wen-Huang Cheng","Hon Hai Research Institute; Department of Electronics and Electrical Engineering, National Yang Ming Chiao Tung University, Taiwan; Information Engineering Graduate Institute of Taiwan University",100.0,Taiwan,0.0,,"Common illumination sources like sunlight or artificial light may introduce hidden vulnerabilities to AI systems. Our paper delves into these potential threats, offering a novel approach to simulate varying light conditions, including sunlight, headlights, and flashlight illuminations. Moreover, unlike typical physical adversarial attacks requiring conspicuous alterations, our method utilizes a model-agnostic black-box attack integrated with the Zeroth Order Optimization (ZOO) algorithm to identify deceptive patterns in a physically-applicable space. Consequently, attackers can recreate these simulated conditions, deceiving machine learning models with seemingly natural light. Empirical results demonstrate the efficacy of our method, misleading models trained on the GTSRB and LISA datasets under natural-like physical environments with an attack success rate exceeding 70% across all digital datasets, and remaining effective against all evaluated real-world traffic signs. Importantly, after adversarial training using samples generated from our approach, models showcase enhanced robustness, underscoring the dual value of our work in both identifying and mitigating potential threats.",https://openaccess.thecvf.com/content/WACV2024/html/Hsiao_Natural_Light_Can_Also_Be_Dangerous_Traffic_Sign_Misinterpretation_Under_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hsiao_Natural_Light_Can_Also_Be_Dangerous_Traffic_Sign_Misinterpretation_Under_WACV_2024_paper.pdf,,https://github.com/BlueDyee/natural-light-attack,,main,Poster,https://ieeexplore.ieee.org/document/10483842/,"['Training', 'Computer vision', 'Machine learning algorithms', 'Machine learning', 'Robustness', 'Object recognition', 'Task analysis']","['Daylight', 'Light Signal', 'Illumination', 'Sunlight', 'Light Conditions', 'Machine Learning Models', 'Artificial Light', 'Flashlight', 'Adversarial Training', 'Physical Attacks', 'Adversarial Attacks', 'Attack Success Rate', 'Black-box Attacks', 'Light Source', 'Support Vector Machine', 'Natural Conditions', 'Loss Value', 'Confidence Score', 'Autonomous Vehicles', 'Digital Domain', 'Adversarial Examples', 'Physical Source', 'Different Signs', 'Digital Sources', 'Digital Realm', 'CIELAB Color Space', 'Image X', 'Meta Learning', 'Physical Domain']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding']",2,"Common illumination sources like sunlight or artificial light may introduce hidden vulnerabilities to AI systems. Our paper delves into these potential threats, offering a novel approach to simulate varying light conditions, including sunlight, headlights, and flashlight illuminations. Moreover, unlike typical physical adversarial attacks requiring conspicuous alterations, our method utilizes a model-agnostic black-box attack integrated with the Zeroth Order Optimization (ZOO) algorithm to identify deceptive patterns in a physically-applicable space. Consequently, attackers can recreate these simulated conditions, deceiving machine learning models with seemingly natural light. Empirical results demonstrate the efficacy of our method, misleading models trained on the GTSRB and LISA datasets under natural-like physical environments with an attack success rate exceeding 70% across all digital datasets, and remaining effective against all evaluated real-world traffic signs. Importantly, after adversarial training using samples generated from our approach, models showcase enhanced robustness, underscoring the dual value of our work in both identifying and mitigating potential threats.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
NeRFEditor: Differentiable Style Decomposition for 3D Scene Editing,"Chunyi Sun, Yanbin Liu, Junlin Han, Stephen Gould",Australian National University,100.0,Australia,0.0,,"We present NeRFEditor, an efficient learning framework for 3D scene editing, which takes a video as input and outputs a high quality, identity-preserving stylized 3D scene. Our goal is to bridge the gap between 2D and 3D editing, catering to a wide array of creative modifications such as reference-guided alterations, text-based prompts, and user interactions. We achieve this by encouraging a pre-trained StyleGAN model and a NeRF model to learn mutually consistent renderings. Specifically, we use NeRF to generate numerous (image, camera pose)-pairs to train an adjustor module, which adapts the StyleGAN latent code for generating high fidelity stylized images from any given viewing angle. To extrapolate edits to novel views, i.e., those not seen by StyleGAN pre-training, while maintaining 360 degree consistency, we propose a second self-supervised module that maps these views into the hidden space of StyleGAN. Together these two modules produce sufficient guidance for NeRF to learn consistent stylization effects across the full range of views. Experiments show that NeRFEditor outperforms prior work on benchmark and real-world scenes with better editability, fidelity, and identity preservation.",https://openaccess.thecvf.com/content/WACV2024/html/Sun_NeRFEditor_Differentiable_Style_Decomposition_for_3D_Scene_Editing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sun_NeRFEditor_Differentiable_Style_Decomposition_for_3D_Scene_Editing_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484169/,"['Training', 'Adaptation models', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Benchmark testing', 'Rendering (computer graphics)']","['3D Scene', '3D Editing', 'Scene Editing', 'Learning Framework', 'Viewing Angle', 'Efficient Framework', 'Camera Pose', 'Latent Code', 'Identity Preservation', 'Fine-tuned', 'Image Quality', 'Latent Space', 'Human Faces', 'Identification Scores', 'Loss Term', 'Camera View', 'Ground Truth Image', 'Real Scenes', 'Latent Vector', 'Style Transfer', 'Target View', 'Single GPU', 'Orthogonal Basis', 'Frontal Images', 'Self-supervised Training', '2D Methods', 'Generation Layer']","['Applications', 'Arts / games / social media', 'Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"We present NeRFEditor, an efficient learning framework for 3D scene editing, which takes a video as input and outputs a high-quality, identity-preserving stylized 3D scene. Our goal is to bridge the gap between 2D and 3D editing, catering to a wide array of creative modifications such as reference-guided alterations, text-based prompts, and user interactions. We achieve this by encouraging a pre-trained StyleGAN model and a NeRF model to learn mutually consistent renderings. Specifically, we use NeRF to generate numerous (image, camera pose)-pairs to train an adjustor module, which adapts the StyleGAN latent code for generating high-fidelity stylized images from any given viewing angle. To extrapolate edits to novel views, i.e., those not seen by StyleGAN pre-training, while maintaining 360° consistency, we propose a second self-supervised module that maps these views into the hidden space of StyleGAN. Together these two modules produce sufficient guidance for NeRF to learn consistent stylization effects across the full range of views. Experiments show that NeRFEditor outperforms prior work on benchmark and real-world scenes with better editability, fidelity, and identity preservation."
Nested Diffusion Processes for Anytime Image Generation,"Noam Elata, Bahjat Kawar, Tomer Michaeli, Michael Elad","Department of CS, Technion – Israel Institute of Technology; Department of ECE, Technion – Israel Institute of Technology",100.0,Israel,0.0,,"Diffusion models are the current state-of-the-art in image generation, synthesizing high-quality images by breaking down the generation process into many fine-grained denoising steps. Despite their good performance, diffusion models are computationally expensive, requiring many neural function evaluations (NFEs). In this work, we propose an anytime diffusion-based method that can generate viable images when stopped at arbitrary times before completion. Using existing pretrained diffusion models, we show that the generation scheme can be recomposed as two nested diffusion processes, enabling fast iterative refinement of a generated image. In experiments on ImageNet and Stable Diffusion-based text-to-image generation, we show, both qualitatively and quantitatively, that our method's intermediate generation quality greatly exceeds that of the original diffusion model, while the final generation result remains comparable. We illustrate the applicability of Nested Diffusion in several settings, including for solving inverse problems, and for rapid text-based content creation by allowing user intervention throughout the sampling process.",https://openaccess.thecvf.com/content/WACV2024/html/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Elata_Nested_Diffusion_Processes_for_Anytime_Image_Generation_WACV_2024_paper.pdf,,https://github.com/noamelata/NestedDiffusion,2305.19066,main,Poster,https://ieeexplore.ieee.org/document/10484513/,"['Computer vision', 'Image synthesis', 'Inverse problems', 'Computational modeling', 'Noise reduction', 'Diffusion processes', 'Probabilistic logic']","['Diffusion Process', 'Image Generation', 'Sample Processing', 'Denoising', 'Generation Process', 'ImageNet', 'Diffusion Model', 'Inverse Problem', 'Content Creation', 'Iterative Refinement', 'Intermediate Quality', 'Hyperparameters', 'Decoding', 'Deep Neural Network', 'Number Of Steps', 'Gaussian Noise', 'Random Noise', 'Reversible Process', 'Latent Space', 'Delta Function', 'Intermediate Samples', 'Evidence Lower Bound', 'Image Editing', 'Deep Neural Network Model']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Diffusion models are the current state-of-the-art in image generation, synthesizing high-quality images by breaking down the generation process into many fine-grained denoising steps. Despite their good performance, diffusion models are computationally expensive, requiring many neural function evaluations (NFEs). In this work, we propose an anytime diffusion-based method that can generate viable images when stopped at arbitrary times before completion. Using existing pretrained diffusion models, we show that the generation scheme can be recomposed as two nested diffusion processes, enabling fast iterative refinement of a generated image. In experiments on ImageNet and Stable Diffusion-based text-to-image generation, we show, both qualitatively and quantitatively, that our method’s intermediate generation quality greatly exceeds that of the original diffusion model, while the final generation result remains comparable. We illustrate the applicability of Nested Diffusion in several settings, including for solving inverse problems, and for rapid text-based content creation by allowing user intervention throughout the sampling process. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields,"Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu","MIT; TU Vienna; TU Vienna, MIT; University of Waterloo",100.0,"Austria, Canada, USA",0.0,,"In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks.",https://openaccess.thecvf.com/content/WACV2024/html/Babaiee_Neural_Echos_Depthwise_Convolutional_Filters_Replicate_Biological_Receptive_Fields_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Babaiee_Neural_Echos_Depthwise_Convolutional_Filters_Replicate_Biological_Receptive_Fields_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484263/,"['Analytical models', 'Computer vision', 'Filters', 'Biological system modeling', 'Computational modeling', 'Machine vision', 'Computer architecture']","['Receptive Field', 'Field Of Biology', 'Depthwise Convolution', 'Accuracy Of Model', 'Convolutional Network', 'Convolutional Neural Network', 'Convolution Kernel', 'Convolutional Neural Network Architecture', 'Intriguing Finding', 'ImageNet Dataset', 'Accuracy Enhancement', 'Neural Network', 'Gaussian Kernel', 'Artificial Neural Network', 'Deep Neural Network', 'Convolutional Layers', 'Visual System', 'Clustering Algorithm', 'Visual Cortex', 'Ganglion Cells', 'Difference Of Gaussian', 'Advances In Neuroscience', 'Biological Counterparts', 'Larger Kernel Size', 'Pointwise Convolution', 'Central Type', 'Edge Detection', 'Vision Transformer', 'K-means Algorithm', 'Input Channels']","['Applications', 'Psychology and cognitive science', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"In this study, we present evidence suggesting that depthwise convolutional kernels are effectively replicating the structural intricacies of the biological receptive fields observed in the mammalian retina. We provide analytics of trained kernels from various state-of-the-art models substantiating this evidence. Inspired by this intriguing discovery, we propose an initialization scheme that draws inspiration from the biological receptive fields. Experimental analysis of the ImageNet dataset with multiple CNN architectures featuring depthwise convolutions reveals a marked enhancement in the accuracy of the learned model when initialized with biologically derived weights. This underlies the potential for biologically inspired computational models to further our understanding of vision processing systems and to improve the efficacy of convolutional networks."
Neural Image Compression Using Masked Sparse Visual Representation,"Wei Jiang, Wei Wang, Yue Chen","Futurewei Technologies Inc., Santa Clara, CA",100.0,USA,0.0,,"We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent feature subspace to balance bitrate and reconstruction quality. A set of semantic-class-dependent basis codebooks are learned, which are weighted combined to generate a rich latent feature for high-quality reconstruction. The combining weights are adaptively derived from each input image, providing fidelity information with additional transmission costs. By masking out unimportant weights in the encoder and recovering them in the decoder, we can trade off reconstruction quality for transmission bits, and the masking rate controls the balance between bitrate and distortion. Experiments over the standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode approach.",https://openaccess.thecvf.com/content/WACV2024/html/Jiang_Neural_Image_Compression_Using_Masked_Sparse_Visual_Representation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jiang_Neural_Image_Compression_Using_Masked_Sparse_Visual_Representation_WACV_2024_paper.pdf,,,2309.11661,main,Poster,https://ieeexplore.ieee.org/document/10483899/,"['Visualization', 'Computer vision', 'Image coding', 'Costs', 'Bit rate', 'Rate-distortion', 'Distortion']","['Sparse Representation', 'Image Compression', 'Neural Compression', 'Neural Image Compression', 'Input Image', 'Latent Space', 'Reconstruction Quality', 'Rich Features', 'Latent Features', 'Transmission Cost', 'High-quality Reconstruction', 'Neural Network', 'General Method', 'Generative Adversarial Networks', 'Perception Of Quality', 'Representation Learning', 'Redundant Information', 'Software Platform', 'Human Faces', 'Weight Map', 'Rich Details', 'Entropy Model', 'Latent Representation', 'Hyperprior', 'Variational Autoencoder', 'Network Reconstruction', 'Visual Content', 'Least Significant Bit', 'Reconstruction Performance']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",2,"We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent feature subspace to balance bitrate and reconstruction quality. A set of semantic-class-dependent basis codebooks are learned, which are weighted combined to generate a rich latent feature for high-quality reconstruction. The combining weights are adaptively derived from each input image, providing fidelity information with additional transmission costs. By masking out unimportant weights in the encoder and recovering them in the decoder, we can trade off reconstruction quality for transmission bits, and the masking rate controls the balance between bitrate and distortion. Experiments over the standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode approach."
Neural Style Protection: Counteracting Unauthorized Neural Style Transfer,"Yaxin Li, Jie Ren, Han Xu, Hui Liu",Michigan State University,100.0,USA,0.0,,"Arbitrary neural style transfer is an advanced AI technique that can effectively synthesize pictures with an artistic style similar to a given source picture. However, if such an AI technique is leveraged by unauthorized individuals, it can significantly infringe upon the copyright of the source picture's owner. In this paper, we study how to protect the artistic style of source images against unauthorized style transfer by adding imperceptible perturbations to the original source pictures. In particular, our goal is to disable the neural style transfer models from producing high-quality pictures with a similar style to the source pictures with slight manipulating the source images. We introduce Neural Style Protection (NSP), which provides protection for source images against various neural style transfer models. Through extensive experiments, we demonstrate the effectiveness and generalizability of the proposed style protection algorithm across numerous style transfer models using varied metrics.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Neural_Style_Protection_Counteracting_Unauthorized_Neural_Style_Transfer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Neural_Style_Protection_Counteracting_Unauthorized_Neural_Style_Transfer_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483922/,"['Measurement', 'Computer vision', 'Perturbation methods', 'Computational modeling', 'Decoding', 'Protection', 'Artificial intelligence']","['Style Transfer', 'Neural Style Transfer', 'Source Images', 'Style Image', 'Imitation', 'Gaussian Noise', 'Random Noise', 'Ensemble Model', 'Dominant Model', 'Baseline Methods', 'Content Features', 'Update Step', 'Encoder Layer', 'Intermediate Representation', 'Unknown Model', 'Style Features', 'Worst Score', 'Adversarial Examples', 'Decoder Part', 'Uniform Noise', 'Pre-trained Encoder', 'Pre-trained VGG-19', 'Adversarial Perturbations']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",1,"Arbitrary neural style transfer is an advanced AI technique that can effectively synthesize pictures with an artistic style similar to a given source picture. However, if such an AI technique is leveraged by unauthorized individuals, it can significantly infringe upon the copyright of the source picture’s owner. In this paper, we study how to protect the artistic style of source images against unauthorized style transfer by adding imperceptible perturbations to the original source pictures. In particular, our goal is to disable the neural style transfer models from producing high-quality pictures with a similar style to the source pictures with slight manipulating the source images. We introduce Neural Style Protection (NSP), which provides protection for source images against various neural style transfer models. Through extensive experiments, we demonstrate the effectiveness and generalizability of the proposed style protection algorithm across numerous style transfer models using varied metrics."
Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis,"Angtian Wang, Wufei Ma, Alan Yuille, Adam Kortylewski","University of Freiburg, Max-Planck-Institute for Informatics; Johns Hopkins University",100.0,"Germany, USA",0.0,,"Human vision demonstrates higher robustness than current AI algorithms under out-of-distribution scenarios. It has been conjectured such robustness benefits from performing analysis-by-synthesis. Our paper formulates triple vision tasks in a consistent manner using approximate analysis-by-synthesis by render-and-compare algorithms on neural features. In this work, we introduce Neural Textured Deformable Meshes, which involve the object model with deformable geometry that allows optimization on both camera parameters and object geometries. The deformable mesh is parameterized as a neural field, and covered by whole-surface neural texture maps, which are trained to have spatial discriminability. During inference, we extract the feature map of the test image and subsequently optimize the 3D pose and shape parameters of our model using differentiable rendering to best reconstruct the target feature map. We show that our analysis-by-synthesis is much more robust than conventional neural networks when evaluated on real-world images and even in challenging out-of-distribution scenarios, such as occlusion and domain shift. Our algorithms are competitive with standard algorithms when tested on conventional performance measures.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Neural_Textured_Deformable_Meshes_for_Robust_Analysis-by-Synthesis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Neural_Textured_Deformable_Meshes_for_Robust_Analysis-by-Synthesis_WACV_2024_paper.pdf,,,2306.00118,main,Poster,https://ieeexplore.ieee.org/document/10484078,"['Geometry', 'Visualization', 'Three-dimensional displays', 'Shape', 'Feature extraction', 'Cameras', 'Approximation algorithms']","['Mesh Deformation', 'Neural Network', 'Feature Maps', 'Domain Shift', 'Vision Tasks', 'Consistent Manner', 'Object Geometry', '3D Pose', 'Spatial Discrimination', 'Deep Neural Network', 'Probabilistic Model', 'Image Classification', 'Multilayer Perceptron', 'Object Classification', '3D Mesh', 'Pose Estimation', '3D Representation', 'Reconstruction Loss', 'Feature Distance', 'CAD Model', 'Human Pose Estimation', 'Camera Pose', 'Object Pose', 'Surface Normals', 'Rotation Vector', 'Partial Occlusion', 'Multi-task Model', 'Mesh Vertices', 'In-plane Rotation', 'Shape Optimization']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",1,"Human vision demonstrates higher robustness than current AI algorithms under out-of-distribution scenarios. It has been conjectured such robustness benefits from performing analysis-by-synthesis. Our paper formulates triple vision tasks in a consistent manner using approximate analysis-by-synthesis by render-and-compare algorithms on neural features. In this work, we introduce Neural Textured Deformable Meshes (NTDM), which involve the object model with deformable geometry that allows optimization on both camera parameters and object geometries. The deformable mesh is parameterized as a neural field, and covered by whole-surface neural texture maps, which are trained to have spatial discriminability. During inference, we extract the feature map of the test image and subsequently optimize the 3D pose and shape parameters of our model using differentiable rendering to best reconstruct the target feature map. We show that our analysis-by-synthesis is much more robust than conventional neural networks when evaluated on real-world images and even in challenging out-of-distribution scenarios, such as occlusion and domain shift. Our algorithms are competitive with standard algorithms when tested on conventional performance measures."
OE-CTST: Outlier-Embedded Cross Temporal Scale Transformer for Weakly-Supervised Video Anomaly Detection,"Snehashis Majhi, Rui Dai, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, François Brémond","Toyota Motor Europe; Woven by Toyota; INRIA, Cˆote d’Azur University",33.33333333333333,France,66.66666666666667,Belgium,"Video anomaly detection in real-world scenarios is challenging due to the complex temporal blending of long and short-length anomalies with normal ones. Further, it is more difficult to detect those due to : (i) Distinctive features characterizing the short and long anomalies with sharp and progressive temporal cues respectively; (ii) Lack of precise temporal information (i.e. weak-supervision) limits the temporal dynamics modeling of anomalies from normal events. In this paper, we propose a novel 'temporal transformer' framework for weakly-supervised anomaly detection: OE-CTST. The proposed framework has two major components: (i) Outlier Embedder (OE) and (ii) Cross Temporal Scale Transformer (CTST). First, OE generates anomaly-aware temporal position encoding to allow the transformer to effectively model the temporal dynamics among the anomalies and normal events. Second, CTST encodes the cross-correlation between multi-temporal scale features to benefit short and long length anomalies by modeling the global temporal relations. The proposed OE-CTST is validated on three publicly available datasets i.e. UCF-Crime, XD-Violence, and IITB-Corridor, outperforming recently reported state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Majhi_OE-CTST_Outlier-Embedded_Cross_Temporal_Scale_Transformer_for_Weakly-Supervised_Video_Anomaly_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Majhi_OE-CTST_Outlier-Embedded_Cross_Temporal_Scale_Transformer_for_Weakly-Supervised_Video_Anomaly_WACV_2024_paper.pdf,,https://github.com/snehashismajhi/OECTST,,main,Poster,https://ieeexplore.ieee.org/document/10484111/,"['Computer vision', 'Correlation', 'Transformers', 'Feature extraction', 'Encoding', 'Anomaly detection']","['Temporal Scales', 'Anomaly Detection', 'Temporal Transformer', 'Video Anomaly', 'Video Anomaly Detection', 'Temporal Relationship', 'Temporal Information', 'Normal Events', 'Temporal Model', 'Temporal Localization', 'Positional Encoding', 'Weak Supervision', 'Large Variation', 'Unsupervised Learning', 'Feature Dimension', 'Temporal Features', 'Unsupervised Methods', 'Performance Gain', 'Sharp Change', 'Temporal Consistency', 'Pseudo Labels', 'Visual Encoding', 'Multiple Instance Learning', 'Position Embedding', 'Normal Segments', 'Consecutive Segments', 'Transformer Block', 'Motion Cues', 'Normal Scenario', 'Inductive Bias']","['Applications', 'Social good', 'Applications', 'Autonomous Driving']",2,"Video anomaly detection in real-world scenarios is challenging due to the complex temporal blending of long and short-length anomalies with normal ones. Further, it is more difficult to detect those due to : (i) Distinctive features characterizing the short and long anomalies with sharp and progressive temporal cues respectively; (ii) Lack of precise temporal information (i.e. weak-supervision) limits the temporal dynamics modeling of anomalies from normal events. In this paper, we propose a novel ‘temporal transformer’ framework for weakly-supervised anomaly detection: OE-CTST
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">†</sup>
. The proposed framework has two major components: (i) Outlier Embedder (OE) and (ii) Cross Temporal Scale Transformer (CTST). First, OE generates anomaly-aware temporal position encoding to allow the transformer to effectively model the temporal dynamics among the anomalies and normal events. Second, CTST encodes the cross-correlation between multi-temporal scale features to benefit short and long length anomalies by modeling the global temporal relations. The proposed OE-CTST is validated on three publicly available datasets i.e. UCF-Crime, XD-Violence, and IITB-Corridor, outperforming recently reported state-of-the-art approaches."
OOD Aware Supervised Contrastive Learning,"Soroush Seifi, Daniel Olmeda Reino, Nikolay Chumerin, Rahaf Aljundi",Toyota Motor Europe,0.0,,100.0,Belgium,"Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our solution is simple and efficient and acts as a natural extension of the closed-set supervised contrastive representation learning. We compare against different OOD detection methods on the common benchmarks and show state-of-the-art results.",https://openaccess.thecvf.com/content/WACV2024/html/Seifi_OOD_Aware_Supervised_Contrastive_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Seifi_OOD_Aware_Supervised_Contrastive_Learning_WACV_2024_paper.pdf,,,2310.01942,main,Poster,https://ieeexplore.ieee.org/document/10484427/,"['Training', 'Representation learning', 'Computer vision', 'Prototypes', 'Self-supervised learning', 'Detectors', 'Benchmark testing']","['Self-supervised Learning', 'Representation Learning', 'Auxiliary Data', 'Class Prototypes', 'False Positive Rate', 'Scoring Function', 'Cross-entropy Loss', 'Classification Of Samples', 'Training Strategy', 'Latent Space', 'Linear Classifier', 'Loss Term', 'Contrastive Loss', 'Feature Encoder', 'Characteristics Of Head', 'Auxiliary Loss', 'Safety-critical Applications', 'Penultimate Layer', 'CIFAR-100 Dataset', 'Pseudo Data']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our solution is simple and efficient and acts as a natural extension of the closed-set supervised contrastive representation learning. We compare against different OOD detection methods on the common benchmarks and show state-of-the-art results."
OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation,"Yuerong Li, Zhengrong Xue, Huazhe Xu",Tsinghua University; Shanghai Qi Zhi Institute; Zhejiang University,100.0,China,0.0,,"Temporal action segmentation is typically achieved by discovering the dramatic variances in global visual descriptors. In this paper, we explore the merits of local features by proposing the unsupervised framework of Object-centric Temporal Action Segmentation (OTAS). Broadly speaking, OTAS consists of self-supervised global and local feature extraction modules as well as a boundary selection module that fuses the features and detects salient boundaries for action segmentation. As a second contribution, we discuss the pros and cons of existing frame-level and boundary-level evaluation metrics. Through extensive experiments, we find OTAS is superior to the previous state-of-the-art method by 41% on average in terms of our recommended F1 score. Surprisingly, OTAS even outperforms the ground-truth human annotations in the user study. Moreover, OTAS is efficient enough to allow real-time inference",https://openaccess.thecvf.com/content/WACV2024/html/Li_OTAS_Unsupervised_Boundary_Detection_for_Object-Centric_Temporal_Action_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_OTAS_Unsupervised_Boundary_Detection_for_Object-Centric_Temporal_Action_Segmentation_WACV_2024_paper.pdf,,https://github.com/yl596/OTAS,2309.06276,main,Poster,https://ieeexplore.ieee.org/document/10484161/,"['Measurement', 'Visualization', 'Computer vision', 'Fuses', 'Annotations', 'Feature extraction', 'Real-time systems']","['Boundary Detection', 'Action Segmentation', 'Temporal Action Segmentation', 'Local Features', 'F1 Score', 'User Study', 'Global Features', 'Global Module', 'Global Feature Extraction', 'Computational Cost', 'Scoring Function', 'Visual Features', 'Intersection Over Union', 'Video Clips', 'Final Prediction', 'Lookup Table', 'Sequence Patterns', 'Object Relations', 'Qualitative Case Study', 'Graph Neural Networks', 'Hidden Representation', 'Graph Attention Network', 'Ground Truth Annotations', 'Clustering-based Methods', 'Final Boundary', 'Frame Prediction', 'COCO Dataset', 'Graph Construction', 'Sequence Embedding', 'Ranking Score']","['Algorithms', 'Video recognition and understanding']",,"Temporal action segmentation is typically achieved by discovering the dramatic variances in global visual descriptors. In this paper, we explore the merits of local features by proposing the unsupervised framework of Object-centric Temporal Action Segmentation (OTAS). Broadly speaking, OTAS consists of self-supervised global and local feature extraction modules as well as a boundary selection module that fuses the features and detects salient boundaries for action segmentation. As a second contribution, we discuss the pros and cons of existing frame-level and boundary-level evaluation metrics. Through extensive experiments, we find OTAS is superior to the previous state-of-the-art method by 41% on average in terms of our recommended F1 score. Surprisingly, OTAS even outperforms the ground-truth human annotations in the user study. Moreover, OTAS is efficient enough to allow real-time inference."
OVeNet: Offset Vector Network for Semantic Segmentation,"Stamatis Alexandropoulos, Christos Sakaridis, Petros Maragos",ETH Zürich; Princeton University; National Technical University of Athens,100.0,"Greece, Switzerland, USA",0.0,,"Semantic segmentation is a fundamental task in visual scene understanding. We focus on the supervised setting, where ground-truth semantic annotations are available. Based on knowledge about the high regularity of real-world scenes, we propose a method for improving class predictions by learning to selectively exploit information from neighboring pixels. In particular, our method is based on the prior that for each pixel, there is a seed pixel in its close neighborhood sharing the same prediction with the former. Motivated by this prior, we design a novel two-head network, named Offset Vector Network (OVeNet), which generates both standard semantic predictions and a dense 2D offset vector field indicating the offset from each pixel to the respective seed pixel, which is used to compute an alternative, seed-based semantic prediction. The two predictions are adaptively fused at each pixel using a learnt dense confidence map for the predicted offset vector field. We supervise offset vectors indirectly via optimizing the seed-based prediction and via a novel loss on the confidence map. Compared to the baseline state-of-the-art architectures HRNet and HRNet+OCR on which OVeNet is built, the latter achieves significant performance gains on three prominent benchmarks for semantic segmentation, namely Cityscapes, ACDC and ADE20K. Code is available at https://github.com/stamatisalex/OVeNet.",https://openaccess.thecvf.com/content/WACV2024/html/Alexandropoulos_OVeNet_Offset_Vector_Network_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Alexandropoulos_OVeNet_Offset_Vector_Network_for_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/stamatisalex/OVeNet,2303.14516,main,Poster,https://ieeexplore.ieee.org/document/10484119/,"['Visualization', 'Shape', 'Semantic segmentation', 'Semantics', 'Benchmark testing', 'Predictive models', 'Performance gain']","['Semantic Segmentation', 'Semantic Segmentation Network', 'Offset Vector', 'Class Prediction', 'Confidence Map', 'Semantic Prediction', 'Neural Network', 'Training Set', 'Convolutional Neural Network', 'Qualitative Results', 'Intersection Over Union', 'Final Prediction', 'Segmentation Results', 'Depth Estimation', 'Confidence Value', 'False Predictions', 'Pixel Coordinates', 'Incorrect Predictions', 'Loss Of Confidence', 'Pre-trained Weights', 'Blue Frame', 'Red Frame', 'Simple Loss', 'Head Network', 'Outdoor Scenes', 'Semantic Segmentation Task', 'Transformer', 'Yield Prediction', 'Image Analysis Tasks']","['Applications', 'Autonomous Driving']",,"Semantic segmentation is a fundamental task in visual scene understanding. We focus on the supervised setting, where ground-truth semantic annotations are available. Based on knowledge about the high regularity of real-world scenes, we propose a method for improving class predictions by learning to selectively exploit information from neighboring pixels. In particular, our method is based on the prior that for each pixel, there is a seed pixel in its close neighborhood sharing the same prediction with the former. Motivated by this prior, we design a novel two-head network, named Offset Vector Network (OVeNet), which generates both standard semantic predictions and a dense 2D offset vector field indicating the offset from each pixel to the respective seed pixel, which is used to compute an alternative, seed-based semantic prediction. The two predictions are adaptively fused at each pixel using a learnt dense confidence map for the predicted offset vector field. We supervise offset vectors indirectly via optimizing the seed-based prediction and via a novel loss on the confidence map. Compared to the baseline state-of-the-art architectures HRNet and HRNet+OCR on which OVeNet is built, the latter achieves significant performance gains on three prominent benchmarks for semantic segmentation, namely Cityscapes, ACDC and ADE20K. Code is available at https://github.com/stamatisalex/OVeNet."
Object Aware Contrastive Prior for Interactive Image Segmentation,"Praful Mathur, Shashi Kumar Parwani, Mrinmoy Sen, Roopa Sheshadri, Aman Sharma",Samsung R&D Institute India - Bangalore,100.0,India,0.0,,"Interactive Image Segmentation is a process of separating a user selected object from the background. This task requires building an effective class-agnostic segmentation model that performs well even on unseen categories. To achieve good accuracy with limited training dataset, it is important that the model has robust prior understanding of features of similar class objects. The model should also have good distinguishing capabilities of foreground objects with the background. In this paper, we propose Object Aware Click Embeddings (OACE) that represents user click aware foreground object features. OACE is obtained based on a prior network trained using the Contrastive Learning paradigm. The single-click object selection accuracy of our base interactive segmentation network is vastly improved with the OACE input. Additionally, we propose a Multi-Stage fusion approach to better utilize user click information. With the proposed method, we outperform existing state-of-the-art approaches by 21% on publicly available test-sets for click-based Interactive Image Segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mathur_Object_Aware_Contrastive_Prior_for_Interactive_Image_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484042/,"['Training', 'Performance evaluation', 'Image segmentation', 'Computer vision', 'Buildings', 'Training data', 'Self-supervised learning']","['Image Segmentation', 'Interactive Segmentation', 'Interactive Image Segmentation', 'Training Dataset', 'Interaction Network', 'Segmentation Model', 'Object Features', 'Self-supervised Learning', 'Foreground Objects', 'Prior Network', 'Multi-feature Fusion', 'Input Image', 'Intersection Over Union', 'Background Characteristics', 'Latent Space', 'Segmentation Accuracy', 'User Input', 'Background Regions', 'Distance Map', 'Convolutional Block', 'Single Click', 'Forward Pass', 'Segmentation Output', 'Representational Similarity', 'Late Fusion', 'COCO Dataset', 'Touchpoints', 'Foundation Model', 'Masked Images', 'Object Segmentation']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Interactive Image Segmentation is a process of separating a user selected object from the background. This task requires building an effective class-agnostic segmentation model that performs well even on unseen categories. To achieve good accuracy with limited training dataset, it is important that the model has robust prior understanding of features of similar class objects. The model should also have good distinguishing capabilities of foreground objects with the background. In this paper, we propose Object Aware Click Embeddings (OACE) that represents user click aware foreground object features. OACE is obtained based on a prior network trained using the Contrastive Learning paradigm. The single-click object selection accuracy of our base interactive segmentation network is vastly improved with the OACE input. Additionally, we propose a Multi-Stage fusion approach to better utilize user click information. With the proposed method, we outperform existing state-of-the-art approaches by 21% on publicly available test-sets for click-based Interactive Image Segmentation."
Object Re-Identification From Point Clouds,"Benjamin Thérien, Chengjie Huang, Adrian Chow, Krzysztof Czarnecki",University of Waterloo,100.0,Canada,0.0,,"Object re-identification (ReID) from images plays a critical role in application domains of image retrieval (surveillance, retail analytics, etc.) and multi-object tracking (autonomous driving, robotics, etc.). However, systems that additionally or exclusively perceive the world from depth sensors are becoming more commonplace without any corresponding methods for object ReID. In this work, we fill the gap by providing the first large-scale study of object ReID from point clouds and establishing its performance relative to image ReID. To enable such a study, we create two large-scale ReID datasets with paired image and LiDAR observations and propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in Siamese style, our proposed point cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Our strongest network trained at the largest scale achieves ReID accuracy exceeding 90% for rigid objects and 85% for deformable objects (without any explicit skeleton normalization). To our knowledge, we are the first to study object re-identification from real point cloud observations.",https://openaccess.thecvf.com/content/WACV2024/html/Therien_Object_Re-Identification_From_Point_Clouds_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Therien_Object_Re-Identification_From_Point_Clouds_WACV_2024_paper.pdf,,https://github.com/bentherien/point-cloud-reid,,main,Poster,https://ieeexplore.ieee.org/document/10483948/,"['Point cloud compression', 'Image sensors', 'Training', 'Laser radar', 'Image resolution', 'Head', 'Surveillance']","['Point Cloud', 'Object Re-identification', 'Large-scale Datasets', 'Depth Camera', 'Sensor Resolution', 'Deformable Objects', 'Multi-object Tracking', 'Model Performance', 'Training Set', 'True Positive', 'Object Detection', 'Pedestrian', 'Intersection Over Union', 'Bounding Box', 'Change In Slope', 'Image Sensor', 'Large Objects', 'Random Initialization', 'Can Approach', 'Input Point', 'LiDAR Sensor', '3D Bounding Box', 'LiDAR Scans', 'High Point Density', '3D Object Detection', 'Search Area', 'Large Number Of Comparisons', 'Training Objective', 'Empirical Evaluation', 'Blind Spot']","['Applications', 'Robotics', 'Algorithms', '3D computer vision', 'Algorithms', 'Video recognition and understanding']",,"Object re-identification (ReID) from images plays a critical role in application domains of image retrieval (surveillance, retail analytics, etc.) and multi-object tracking (autonomous driving, robotics, etc.). However, systems that additionally or exclusively perceive the world from depth sensors are becoming more commonplace without any corresponding methods for object ReID. In this work, we fill the gap by providing the first large-scale study of object ReID from point clouds and establishing its performance relative to image ReID. To enable such a study, we create two large-scale ReID datasets with paired image and LiDAR observations and propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in Siamese style, our proposed point cloud ReID networks can make thousands of pairwise comparisons in real-time (10 Hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Our strongest network trained at the largest scale achieves ReID accuracy exceeding 90% for rigid objects and 85% for deformable objects (without any explicit skeleton normalization). To our knowledge, we are the first to study object re-identification from real point cloud observations. Our code is available at https://github.com/bentherien/point-cloud-reid."
Object-Centric Video Representation for Long-Term Action Anticipation,"Ce Zhang, Changcheng Fu, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun",Honda Research Institute USA; Samsung; Brown University,66.66666666666666,"Japan, USA",33.33333333333334,South Korea,"This paper focuses on building object-centric representations for long-term action anticipation in videos. Our key motivation is that objects provide important cues to recognize and predict human-object interactions, especially when the predictions are longer term, as an observed ""background"" object could be used by the human actor in the future. We observe that existing object-based video recognition frameworks either assume the existence of in-domain supervised object detectors or follow a fully weakly-supervised pipeline to infer object locations from action labels. We propose to build object-centric video representations by leveraging visual-language pretrained models. This is achieved by ""object prompts"", an approach to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. To recognize and predict human-object interactions, we use a Transformer-based neural architecture which allows the ""retrieval"" of relevant objects for action anticipation at various time scales. We conduct extensive evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both quantitative and qualitative results confirm the effectiveness of our proposed method.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Object-Centric_Video_Representation_for_Long-Term_Action_Anticipation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Object-Centric_Video_Representation_for_Long-Term_Action_Anticipation_WACV_2024_paper.pdf,,github.com/brown-palm/ObjectPrompt,2311.00180,main,Poster,https://ieeexplore.ieee.org/document/10484396/,"['Computer vision', 'Computational modeling', 'Pipelines', 'Detectors', 'Computer architecture', 'Benchmark testing', 'Predictive models']","['Activity Prediction', 'Benchmark', 'Object Detection', 'Object Location', 'Action Labels', 'Cognitive Domains', 'Recent Models', 'Future Actions', 'Bounding Box', 'Object Features', 'Object Of Interest', 'Stopping Rule', 'Motion Information', 'Image Descriptors', 'Test Split', 'Target Dataset', 'Edit Distance', 'Video Segments', 'Object Pairs', 'COCO Dataset', 'Video Encoding', 'Transformer Encoder', 'List Of Objects', 'Object Proposals', 'Top-1 Accuracy', 'Temporal Model', 'Category Information', 'Video Frames', 'Linear Classifier', 'Bounding Box Annotations']","['Algorithms', 'Video recognition and understanding']",5,"This paper focuses on building object-centric representations for long-term action anticipation in videos. Our key motivation is that objects provide important cues to recognize and predict human-object interactions, especially when the predictions are longer term, as an observed ""background"" object could be used by the human actor in the future. We observe that existing object-based video recognition frameworks either assume the existence of in-domain supervised object detectors or follow a fully weakly-supervised pipeline to infer object locations from action labels. We propose to build object-centric video representations by leveraging visual-language pretrained models. This is achieved by ""object prompts"", an approach to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. To recognize and predict human-object interactions, we use a Transformer-based neural architecture which allows the ""retrieval"" of relevant objects for action anticipation at various time scales. We conduct extensive evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both quantitative and qualitative results confirm the effectiveness of our proposed method. Our code is available at github.com/brown-palm/ObjectPrompt."
Occlusion Sensitivity Analysis With Augmentation Subspace Perturbation in Deep Feature Space,"Pedro H. V. Valois, Koichiro Niinuma, Kazuhiro Fukui",University of Tsukuba; Fujitsu Research of America,50.0,Japan,50.0,USA,"Deep Learning of neural networks has gained prominence in multiple life-critical applications like medical diagnoses and autonomous vehicle accident investigations. However, concerns about model transparency and biases persist. Explainable methods are viewed as the solution to address these challenges. In this study, we introduce the Occlusion Sensitivity Analysis with Deep Feature Augmentation Subspace (OSA-DAS), a novel perturbation-based interpretability approach for computer vision. While traditional perturbation methods make only use of occlusions to explain the model predictions, OSA-DAS extends standard occlusion sensitivity analysis by enabling the integration with diverse image augmentations. Distinctly, our method utilizes the output vector of a DNN to build low-dimensional subspaces within the deep feature vector space, offering a more precise explanation of the model prediction. The structural similarity between these subspaces encompasses the influence of diverse augmentations and occlusions. We test extensively on the ImageNet-1k, and our class- and model-agnostic approach outperforms commonly used interpreters, setting it apart in the realm of explainable AI.",https://openaccess.thecvf.com/content/WACV2024/html/Valois_Occlusion_Sensitivity_Analysis_With_Augmentation_Subspace_Perturbation_in_Deep_Feature_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Valois_Occlusion_Sensitivity_Analysis_With_Augmentation_Subspace_Perturbation_in_Deep_Feature_WACV_2024_paper.pdf,,,2311.15022,main,Poster,https://ieeexplore.ieee.org/document/10483645/,"['Analytical models', 'Computer vision', 'Sensitivity analysis', 'Computational modeling', 'Perturbation methods', 'Neural networks', 'Predictive models']","['Deep Features', 'Deep Feature Space', 'Deep Learning', 'Deep Neural Network', 'Autonomous Vehicles', 'Image Augmentation', 'Explainable Artificial Intelligence', 'Heatmap', 'Minimum Size', 'Machine Learning Models', 'Input Image', 'Data Augmentation', 'Singular Value Decomposition', 'Insert Size', 'Natural Model', 'Prediction Probability', 'Orthonormal Basis', 'Augmentation Techniques', 'Image X', 'Actual Cause', 'Subspace Representation', 'Robust Explanation', 'Size Metrics', 'Largest Singular Value', 'Classification Head']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Visualization']",,"Deep Learning of neural networks has gained prominence in multiple life-critical applications like medical diagnoses and autonomous vehicle accident investigations. However, concerns about model transparency and biases persist. Explainable methods are viewed as the solution to address these challenges. In this study, we introduce the Occlusion Sensitivity Analysis with Deep Feature Augmentation Subspace (OSA-DAS), a novel perturbation-based interpretability approach for computer vision. While traditional perturbation methods make only use of occlusions to explain the model predictions, OSA-DAS extends standard occlusion sensitivity analysis by enabling the integration with diverse image augmentations. Distinctly, our method utilizes the output vector of a DNN to build low-dimensional subspaces within the deep feature vector space, offering a more precise explanation of the model prediction. The structural similarity between these subspaces encompasses the influence of diverse augmentations and occlusions. We test extensively on the ImageNet-1k, and our class- and model-agnostic approach outperforms commonly used interpreters, setting it apart in the realm of explainable AI."
Offline-to-Online Knowledge Distillation for Video Instance Segmentation,"Hojin Kim, Seunghun Lee, Hyeon Kang, Sunghoon Im","Department of Electrical Engineering & Computer Science, DGIST, Daegu, Korea; SI Analytics, Daejeon, Korea",50.0,South Korea,50.0,South Korea,"In this paper, we present offline-to-online knowledge distillation (OOKD) for video instance segmentation (VIS), which transfers a wealth of video knowledge from an offline model to an online model for consistent prediction. Unlike previous methods that have adopted either an online or offline model, our single online model takes advantage of both models by distilling offline knowledge. To transfer knowledge correctly, we propose query filtering and association (QFA), which filters irrelevant queries to exact instances. Our KD with QFA increases the robustness of feature matching by encoding object-centric features from a single frame supplemented by long-range global information. We also propose a simple data augmentation scheme for knowledge distillation in the VIS task that fairly transfers the knowledge of all classes into the online model. Extensive experiments show that our method significantly improves the performance in video instance segmentation, especially for challenging datasets, including long, dynamic sequences. Our method also achieves state-of-the-art performance on YTVIS-21, YTVIS-22, and OVIS datasets, with mAP scores of 46.1%, 43.6%, and 31.1%, respectively.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Offline-to-Online_Knowledge_Distillation_for_Video_Instance_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Offline-to-Online_Knowledge_Distillation_for_Video_Instance_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484152,"['Instance segmentation', 'Computer vision', 'Filters', 'Predictive models', 'Data augmentation', 'Robustness', 'Encoding']","['Instance Segmentation', 'Data Augmentation', 'Single Frame', 'Feature Matching', 'Challenging Dataset', 'Augmentation Strategy', 'Knowledge Transfer', 'Teacher Model', 'Bounding Box', 'Video Clips', 'Video Frames', 'Short Video', 'Squirrels', 'Video Sequences', 'Minority Class', 'Online Methods', 'Student Model', 'Competitive Methods', 'Video Dataset', 'Student Network', 'Distillation Method', 'Action Detection', 'Memory Bank', 'Aggregation Module', 'Instance Labels', 'Dynamic Video', 'Intersection Over Union', 'Entire Video', 'Class Instances', 'High Similarity']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding']",2,"In this paper, we present offline-to-online knowledge distillation (OOKD) for video instance segmentation (VIS), which transfers a wealth of video knowledge from an offline model to an online model for consistent prediction. Unlike previous methods that have adopted either an online or offline model, our single online model takes advantage of both models by distilling offline knowledge. To transfer knowledge correctly, we propose query filtering and association (QFA), which filters irrelevant queries to exact instances. Our KD with QFA increases the robustness of feature matching by encoding object-centric features from a single frame supplemented by long-range global information. We also propose a simple data augmentation scheme for knowledge distillation in the VIS task that fairly transfers the knowledge of all classes into the online model. Extensive experiments show that our method significantly improves the performance in video instance segmentation, especially for challenging datasets, including long, dynamic sequences. Our method also achieves state-of-the-art performance on YTVIS-21, YTVIS-22, and OVIS datasets, with mAP scores of 46.1%, 43.6%, and 31.1%, respectively."
OmniVec: Learning Robust Representations With Cross Modal Sharing,"Siddharth Srivastava, Gaurav Sharma",TensorTour Inc.,0.0,,100.0,USA,"Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g. visual, audio, text and 3D, and report results on 22 diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.",https://openaccess.thecvf.com/content/WACV2024/html/Srivastava_OmniVec_Learning_Robust_Representations_With_Cross_Modal_Sharing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Srivastava_OmniVec_Learning_Robust_Representations_With_Cross_Modal_Sharing_WACV_2024_paper.pdf,,,2311.05709,main,Poster,https://ieeexplore.ieee.org/document/10483855/,"['Training', 'Learning systems', 'Visualization', 'Computer vision', 'Three-dimensional displays', 'Information sharing', 'Computer architecture']","['Cross-modal', 'Learning Robust Representations', 'Network Training', 'Multiple Modalities', 'Multiple Tasks', 'Task Network', 'Convolutional Neural Network', 'Text Data', 'Simple Task', 'Point Cloud', 'Knowledge Sharing', 'Depth Map', 'Backbone Network', 'Shared Space', 'Multi-task Learning', '3D Point Cloud', 'Input Representation', 'Embedding Learning', 'Auditory Domain', 'Input Patch', 'Common Backbone', 'Single Architecture', 'Depth Prediction', 'Projection Layer', 'Vision Transformer', 'Video Summarization', 'Fine-tuned', 'Latent Space', 'Omnivorous', 'Unique Network']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",22,"Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g. visual, audio, text and 3D, and report results on 22 diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks."
On Manipulating Scene Text in the Wild With Diffusion Models,"Joshua Santoso, Christian Simon, Williem",The Australian National University; Independent Researcher,50.0,Australia,50.0,,"Diffusion models have gained attention for image editing yielding impressive results in text-to-image tasks. On the downside, one might notice that generated images of stable diffusion models suffer from deteriorated details. This pitfall impacts image editing tasks that require information preservation e.g., scene text editing. As a desired result, the model must show the capability to replace the text on the source image to the target text while preserving the details e.g., color, font size, and background. To leverage the potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene Text manipulation network so-called DBEST. Specifically, we design two adaptation strategies, namely one-shot style adaptation and text-recognition guidance. In experiments, we thoroughly assess and compare our proposed method against state-of-the-arts on various scene text datasets, then provide extensive ablation studies for each granularity to analyze our performance gain. Also, we demonstrate the effectiveness of our proposed method to synthesize scene text indicated by competitive Optical Character Recognition (OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and ICDAR2013 datasets for character-level evaluation.",https://openaccess.thecvf.com/content/WACV2024/html/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Santoso_On_Manipulating_Scene_Text_in_the_Wild_With_Diffusion_Models_WACV_2024_paper.pdf,,,2311.00734,main,Poster,https://ieeexplore.ieee.org/document/10484533/,"['Adaptation models', 'Computer vision', 'Text recognition', 'Image color analysis', 'Optical character recognition', 'Performance gain', 'Real-time systems']","['Diffusion Model', 'Scene Text', 'Source Images', 'Font Size', 'Optical Character Recognition', 'Use Of Text', 'Image Editing', 'Target Text', 'Text Editing', 'Denoising', 'Input Image', 'Generative Adversarial Networks', 'Latent Space', 'Language Model', 'Recognition Model', 'Source Text', 'Prior Methods', 'Noisy Images', 'State Of The Art Methods', 'Style Transfer', 'Text Generation', 'Text Length', 'Forward Step', 'Style Image']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities']",1,"Diffusion models have gained attention for image editing yielding impressive results in text-to-image tasks. On the downside, one might notice that generated images of stable diffusion models suffer from deteriorated details. This pitfall impacts image editing tasks that require information preservation e.g., scene text editing. As a desired result, the model must show the capability to replace the text on the source image to the target text while preserving the details e.g., color, font size, and background. To leverage the potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene Text manipulation Network so-called DBEST. Specifically, we design two adaptation strategies, namely one-shot style adaptation and text-recognition guidance. In experiments, we thoroughly assess and compare our proposed method against state-of-the-arts on various scene text datasets, then provide extensive ablation studies for each granularity to analyze our performance gain. Also, we demonstrate the effectiveness of our proposed method to synthesize scene text indicated by competitive Optical Character Recognition (OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and ICDAR2013 datasets for character-level evaluation."
On the Fly Neural Style Smoothing for Risk-Averse Domain Generalization,"Akshay Mehra, Yunbei Zhang, Bhavya Kailkhura, Jihun Hamm",Tulane University; Lawrence Livermore National Laboratory,100.0,USA,0.0,,"Achieving high accuracy on data from domains unseen during training is a fundamental challenge in domain generalization (DG). While state-of-the-art DG classifiers have demonstrated impressive performance across various tasks, they have shown a bias towards domain-dependent information, such as image styles, rather than domain-invariant information, such as image content. This bias renders them unreliable for deployment in risk-sensitive scenarios such as autonomous driving where a misclassification could lead to catastrophic consequences. To enable risk-averse predictions from a DG classifier, we propose a novel inference procedure, Test-Time Neural Style Smoothing (TT-NSS), that uses a ""style-smoothed"" version of the DG classifier for prediction at test time. Specifically, the style-smoothed classifier classifies a test image as the most probable class predicted by the DG classifier on random re-stylizations of the test image. TT-NSS uses a neural style transfer module to stylize a test image on the fly, requires only black-box access to the DG classifier, and crucially, abstains when predictions of the DG classifier on the stylized test images lack consensus. Additionally, we propose a neural style smoothing (NSS) based training procedure that can be seamlessly integrated with existing DG methods. This procedure enhances prediction consistency, improving the performance of TT-NSS on non-abstained samples. Our empirical results demonstrate the effectiveness of TT-NSS and NSS at producing and improving risk-averse predictions on unseen domains from DG classifiers trained with SOTA training methods on various benchmark datasets and their variations.",https://openaccess.thecvf.com/content/WACV2024/html/Mehra_On_the_Fly_Neural_Style_Smoothing_for_Risk-Averse_Domain_Generalization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mehra_On_the_Fly_Neural_Style_Smoothing_for_Risk-Averse_Domain_Generalization_WACV_2024_paper.pdf,,,2307.08551,main,Poster,https://ieeexplore.ieee.org/document/10483661/,"['Training', 'Computer vision', 'Smoothing methods', 'Closed box', 'Benchmark testing', 'Task analysis', 'Autonomous vehicles']","['Domain Generalization', 'Training Methods', 'Training Procedure', 'Class Prediction', 'Benchmark Datasets', 'Inference Procedure', 'Style Transfer', 'Style Image', 'Unseen Domains', 'Neural Network', 'Test Samples', 'Classification Accuracy', 'Classification Performance', 'Network Layer', 'Single Domain', 'Kullback-Leibler', 'Base Classifiers', 'Defocus', 'Source Domain', 'Empirical Risk Minimization', 'Classification Parameters', 'Source Domain Data', 'Image X', 'Style Changes', 'Incorrect Predictions', 'Reliable Classification']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Achieving high accuracy on data from domains unseen during training is a fundamental challenge in domain generalization (DG). While state-of-the-art (SOTA) DG classifiers have demonstrated impressive performance across various tasks, they have shown a bias towards domain-dependent information, such as image styles, rather than domain-invariant information, such as image content. This bias renders them unreliable for deployment in risk-sensitive scenarios such as autonomous driving where a misclassification could have catastrophic consequences. To enable risk-averse predictions from a DG classifier, we propose a novel inference procedure, Test-Time Neural Style Smoothing (TT-NSS), that uses a ""style-smoothed"" version of the DG classifier for prediction at test time. Specifically, the style-smoothed classifier classifies a test image as the most probable class predicted by the DG classifier on random re-stylizations of the test image. TT-NSS uses a neural style transfer module to stylize a test image on the fly, requires only black-box access to the DG classifier, and crucially, abstains when predictions of the DG classifier on the stylized test images lack consensus. Additionally, we propose a neural style smoothing (NSS) based training procedure that can be seamlessly integrated with existing DG methods. This procedure enhances the prediction consistency of DG classifiers, improving the performance of TT-NSS on non-abstained samples. Our empirical results demonstrate the effectiveness of TT-NSS and NSS at producing and improving risk-averse predictions on unseen domains from DG classifiers trained with SOTA training methods on various benchmark datasets and their variations."
On the Importance of Large Objects in CNN Based Object Detection Algorithms,"Ahmed Ben Saad, Gabriele Facciolo, Axel Davy","Universit ´e Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, 91190, Gif-sur-Yvette, France",100.0,France,0.0,,"Object detection models, a prominent class of machine learning algorithms, aim to identify and precisely locate objects in images or videos. However, the task of accurately localizing objects within images yields uneven performances sometimes caused by the objects sizes and the quality of the images and labels. In this paper, we highlight the importance of large objects in learning features that are critical for all sizes. Given these findings, we propose to address this by introducing a weighting term into the loss during training. This term is a function of the object area size. We show that giving more weight to large objects leads to improvement in detection scores across all sizes and so an overall improvement in Object Detectors performances (+2% mAP on small objects, +2% on medium and +4% on large on COCO val 2017 with InternImage-T). Additional experiments and ablation studies with different models and on different dataset further confirm the robustness of our findings.",https://openaccess.thecvf.com/content/WACV2024/html/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Saad_On_the_Importance_of_Large_Objects_in_CNN_Based_Object_WACV_2024_paper.pdf,,,2311.11714,main,Poster,https://ieeexplore.ieee.org/document/10484501/,"['Training', 'Computer vision', 'Machine learning algorithms', 'Object detection', 'Detectors', 'Feature extraction', 'Robustness']","['Convolutional Neural Network', 'Object Detection', 'Important Objective', 'Large Objects', 'Additional Experiments', 'Feature Learning', 'Small Objects', 'Object Size', 'Training Loss', 'Object Detection Model', 'Detection Performance', 'Data Augmentation', 'Bounding Box', 'Weight Function', 'Performance Gain', 'Target Object', 'Low-level Features', 'Loss Term', 'Local Loss', 'COCO Dataset', 'You Only Look Once', 'Object Dataset', 'Weighting Scheme', 'Feature Pyramid Network', 'Beginning Of Training', 'Center Of The Bounding Box', 'Small Ones', 'Mask R-CNN', 'Hard Examples']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations']",2,"Object detection models, a prominent class of machine learning algorithms, aim to identify and precisely locate objects in images or videos. However, this task might yield uneven performances sometimes caused by the objects sizes and the quality of the images and labels used for training. In this paper, we highlight the importance of large objects in learning features that are critical for all sizes. Given these findings, we propose to introduce a weighting term into the training loss. This term is a function of the object area size. We show that giving more weight to large objects leads to improved detection scores across all object sizes and so an overall improvement in Object Detectors performances (+2 p.p. of mAP on small objects, +2 p.p. on medium and +4 p.p. on large on COCO val 2017 with InternImage-T). Additional experiments and ablation studies with different models and on a different dataset further confirm the robustness of our findings."
On the Quantification of Image Reconstruction Uncertainty Without Training Data,"Jiaxin Zhang, Sirui Bi, Victor Fung",Walmart Global Tech; Intuit AI Research; Georgia Institute of Technology,66.66666666666666,USA,33.33333333333334,USA,"Computational imaging plays a pivotal role in determining hidden information from sparse measurements. A robust inverse solver is crucial to fully characterize the uncertainty induced by these measurements, as it allows for the estimation of the complete posterior of unrecoverable targets. This, in turn, facilitates a probabilistic interpretation of observational data for decision-making. In this study, we propose a deep variational framework that leverages a deep generative model to learn an approximate posterior distribution to effectively quantify image reconstruction uncertainty without the need for training data. We parameterize the target posterior using a flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve accurate uncertainty estimation. To bolster stability, we introduce a robust flow-based model with bi-directional regularization and enhance expressivity through gradient boosting. Additionally, we incorporate a space-filling design to achieve substantial variance reduction on both latent prior space and target posterior space. We validate our method on several benchmark tasks and two real-world applications, namely fastMRI and black hole image reconstruction. Our results indicate that our method provides reliable and high-quality image reconstruction with robust uncertainty estimation.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_On_the_Quantification_of_Image_Reconstruction_Uncertainty_Without_Training_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_On_the_Quantification_of_Image_Reconstruction_Uncertainty_Without_Training_Data_WACV_2024_paper.pdf,,,2311.09639,main,Poster,https://ieeexplore.ieee.org/document/10484250/,"['Uncertainty', 'Estimation', 'Training data', 'Imaging', 'Benchmark testing', 'Probabilistic logic', 'Stability analysis']","['Image Reconstruction', 'Uncertainty Quantification', 'Posterior Probability', 'Deep Models', 'Uncertainty Estimation', 'Latent Space', 'Variance Reduction', 'Black Hole', 'Computer Image', 'Gradient Boosting', 'Deep Generative Models', 'High-quality Reconstruction', 'Sparse Measurements', 'Variational Framework', 'Accurate Uncertainty', 'Loss Function', 'Markov Chain Monte Carlo', 'Inverse Problem', 'Simple Random Sampling', 'Variable Approach', 'Interferometric Imaging', 'Fréchet Inception Distance', 'Latin Hypercube Sampling', 'Inverse Solution', 'Preimage', 'Inverse Mapping', 'Normal Flow', 'Forward Mapping', 'Variational Inference']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",,"Computational imaging plays a pivotal role in determining hidden information from sparse measurements. A robust inverse solver is crucial to fully characterize the uncertainty induced by these measurements, as it allows for the estimation of the complete posterior of unrecoverable targets. This, in turn, facilitates a probabilistic interpretation of observational data for decision-making. In this study, we propose a deep variational framework that leverages a deep generative model to learn an approximate posterior distribution to effectively quantify image reconstruction uncertainty without the need for training data. We parameterize the target posterior using a flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve accurate uncertainty estimation. To bolster stability, we introduce a robust flow-based model with bi-directional regularization and enhance expressivity through gradient boosting. Additionally, we incorporate a space-filling design to achieve substantial variance reduction on both latent prior space and target posterior space. We validate our method on several benchmark tasks and two real-world applications, namely fastMRI and black hole image reconstruction. Our results indicate that our method provides reliable and high-quality image reconstruction with robust uncertainty estimation."
One Style Is All You Need To Generate a Video,"Sandeep Manandhar, Auguste Genovesio","IBENS, Ecole Normale Supérieure, 75005 Paris, France",100.0,France,0.0,,"In this paper, we propose a style-based conditional video generative model. We introduce a novel temporal generator based on a set of learned sinusoidal bases. Our method learns dynamic representations of various actions that are independent of image content and can be transferred between different actors. Beyond the significant enhancement of video quality compared to prevalent methods, we demonstrate that the disentangled dynamic and content permit their independent manipulation, as well as temporal GAN-inversion to retrieve and transfer a video motion from one content or identity to another without further preprocessing such as landmark points.",https://openaccess.thecvf.com/content/WACV2024/html/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Manandhar_One_Style_Is_All_You_Need_To_Generate_a_Video_WACV_2024_paper.pdf,,,2310.17835,main,Poster,https://ieeexplore.ieee.org/document/10483577/,"['Measurement', 'Computer vision', 'Dynamics', 'Web pages', 'Training data', 'Generators', 'Vectors']","['Video Quality', 'Training Set', 'Convolutional Neural Network', 'Recurrent Neural Network', 'Latent Space', 'Random Vector', 'Video Frames', 'Action Recognition', 'Optical Flow', 'Single Vector', 'Consecutive Frames', 'Latent Vector', 'Temporal Coherence', 'Pose Information', 'Fourier Basis', 'Real Videos', 'Content Consistency', 'Ramp Function', 'Arbitrary Rate', 'Synthesis Network', 'Temporal Vector', 'Dot Product', 'Temporal Representation', 'Diffusion Model', 'Image Generation', 'Linear Term', 'Semantic', 'Video Clips']","['conditional video generation', 'temporal style', 'dynamics transfer']",,"In this paper, we propose a style-based conditional video generative model. We introduce a novel temporal generator based on a set of learned sinusoidal bases. Our method learns dynamic representations of various actions that are independent of image content and can be transferred between different actors. Beyond the significant enhancement of video quality compared to prevalent methods, we demonstrate that the disentangled dynamic and content permit their independent manipulation, as well as temporal GAN-inversion to retrieve and transfer a video motion from one content or identity to another without further preprocessing such as landmark points."
Online Class-Incremental Learning for Real-World Food Image Classification,"Siddeshwar Raghavan, Jiangpeng He, Fengqing Zhu","School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana USA",100.0,USA,0.0,,"Food image classification is essential for monitoring health and tracking dietary in image-based dietary assessment methods. However, conventional systems often rely on static datasets with fixed classes and uniform distribution. In contrast, real-world food consumption patterns, shaped by cultural, economic, and personal influences, involve dynamic and evolving data. Thus, require the classification system to cope with continuously evolving data. Online Class Incremental Learning (OCIL) addresses the challenge of learning continuously from a single-pass data stream while adapting to the new knowledge and reducing catastrophic forgetting. Experience Replay (ER) based OCIL methods store a small portion of previous data and have shown encouraging performance. However, most existing OCIL works assume that the distribution of encountered data is perfectly balanced, which rarely happens in real-world scenarios. In this work, we explore OCIL for real-world food image classification by first introducing a probabilistic framework to simulate realistic food consumption scenarios. Subsequently, we present an attachable Dynamic Model Update (DMU) module designed for existing ER methods, which enables the selection of relevant images for model training, addressing challenges arising from data repetition and imbalanced sample occurrences inherent in realistic food consumption patterns within the OCIL framework. Our performance evaluation demonstrates significant enhancements compared to established ER methods, showing great potential for lifelong learning in real-world food image classification scenarios. The code of our method is publicly accessible at https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification",https://openaccess.thecvf.com/content/WACV2024/html/Raghavan_Online_Class-Incremental_Learning_for_Real-World_Food_Image_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Raghavan_Online_Class-Incremental_Learning_for_Real-World_Food_Image_Classification_WACV_2024_paper.pdf,,https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification,,main,Poster,https://ieeexplore.ieee.org/document/10484236/,"['Training', 'Performance evaluation', 'Economics', 'Computer vision', 'Pipelines', 'Probabilistic logic', 'Data models']","['Real-world Images', 'Food Images', 'Class-incremental Learning', 'Real-world Food', 'Real-world Image Classification', 'Feeding', 'Uniform Distribution', 'Real Scenarios', 'Training Images', 'Real-world Scenarios', 'Incremental Learning', 'Dataset Statistics', 'Food Consumption Patterns', 'Realistic Patterns', 'Experience Replay', 'Dynamic Update', 'Real-world Patterns', 'Catastrophic Forgetting', 'Data Distribution', 'Hyperparameters', 'Continuous Learning Process', 'Characteristic Zero', 'Buffer Size', 'Food Patterns', 'Input Image', 'Real Distribution', 'Food Categories', 'Batch Size']","['Applications', 'Food science and nutrition', 'Applications', 'Biomedical / healthcare / medicine']",3,"Food image classification is essential for monitoring health and tracking dietary in image-based dietary assessment methods. However, conventional systems often rely on static datasets with fixed classes and uniform distribution. In contrast, real-world food consumption patterns, shaped by cultural, economic, and personal influences, involve dynamic and evolving data. Thus, require the classification system to cope with continuously evolving data. Online Class Incremental Learning (OCIL) addresses the challenge of learning continuously from a single-pass data stream while adapting to the new knowledge and reducing catastrophic forgetting. Experience Replay (ER) based OCIL methods store a small portion of previous data and have shown encouraging performance. However, most existing OCIL works assume that the distribution of encountered data is perfectly balanced, which rarely happens in real-world scenarios. In this work, we explore OCIL for real-world food image classification by first introducing a probabilistic framework to simulate realistic food consumption scenarios. Subsequently, we present an attachable Dynamic Model Update (DMU) module designed for existing ER methods, which enables the selection of relevant images for model training, addressing challenges arising from data repetition and imbalanced sample occurrences inherent in realistic food consumption patterns within the OCIL framework. Our performance evaluation demonstrates significant enhancements compared to established ER methods, showing great potential for lifelong learning in real-world food image classification scenarios. The code of our method is publicly accessible at https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification"
Open-NeRF: Towards Open Vocabulary NeRF Decomposition,"Hao Zhang, Fang Li, Narendra Ahuja",University of Illinois Urbana-Champaign,100.0,USA,0.0,,"In this paper, we address the challenge of decomposing Neural Radiance Fields (NeRF) into objects from an open vocabulary, a critical task for object manipulation in 3D reconstruction and view synthesis. Current techniques for NeRF decomposition involve a trade-off between the flexibility of processing open-vocabulary queries and the accuracy of 3D segmentation. We present, Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage large-scale, off-the-shelf, segmentation models like the Segment Anything Model (SAM) and introduce an integrate-and-distill paradigm with hierarchical embeddings to achieve both the flexibility of open-vocabulary querying and 3D segmentation accuracy. Open-NeRF first utilizes large-scale foundation models to generate hierarchical 2D mask proposals from varying viewpoints. These proposals are then aligned via tracking approaches and integrated within the 3D space and subsequently distilled into the 3D field. This process ensures consistent recognition and granularity of objects from different viewpoints, even in challenging scenarios involving occlusion and indistinct features. Our experimental results show that the proposed Open-NeRF outperforms state-of-the-art methods such as LERF and FFD in open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF decomposition, guided by open-vocabulary queries, enabling novel applications in robotics and vision-language interaction in open-world 3D scenes. Please find the code at https://github.com/haoz19/Open-NeRF",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Open-NeRF_Towards_Open_Vocabulary_NeRF_Decomposition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Open-NeRF_Towards_Open_Vocabulary_NeRF_Decomposition_WACV_2024_paper.pdf,,https://github.com/haoz19/Open-NeRF,,main,Poster,https://ieeexplore.ieee.org/document/10484306/,"['Vocabulary', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Proposals', 'Task analysis']","['Neural Radiance Fields', 'Open Vocabulary', '3D Space', 'Segmentation Model', 'Segmentation Accuracy', '3D Segmentation', '3D Scene', '3D Field', 'Foundation Model', 'View Synthesis', 'Qualitative Results', '2D Images', 'Multilayer Perceptron', 'Viewing Angle', 'Dot Product', 'Segmentation Results', '3D Coordinates', 'Knowledge Integration', 'Background Regions', 'Object Parts', 'Image Embedding', 'Relevance Score', 'Image Encoder', 'COCO Dataset', 'Region Proposal', 'Track Model', '2D Segmentation', 'Integration Procedure', 'Camera Position']","['Algorithms', '3D computer vision']",,"In this paper, we address the challenge of decomposing Neural Radiance Fields (NeRF) into objects from an open vocabulary, a critical task for object manipulation in 3D reconstruction and view synthesis. Current techniques for NeRF decomposition involve a trade-off between the flexibility of processing open-vocabulary queries and the accuracy of 3D segmentation. We present, Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage large-scale, off-the-shelf, segmentation models like the Segment Anything Model (SAM) and introduce an integrate-and-distill paradigm with hierarchical embeddings to achieve both the flexibility of open-vocabulary querying and 3D segmentation accuracy. Open-NeRF first utilizes large-scale foundation models to generate hierarchical 2D mask proposals from varying viewpoints. These proposals are then aligned via tracking approaches and integrated within the 3D space and subsequently distilled into the 3D field. This process ensures consistent recognition and granularity of objects from different viewpoints, even in challenging scenarios involving occlusion and indistinct features. Our experimental results show that the proposed Open-NeRF
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
 outperforms state-of-the-art methods such as LERF [16] and FFD [18] in open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF decomposition, guided by open-vocabulary queries, enabling novel applications in robotics and vision-language interaction in open-world 3D scenes. Please find the code at https://github.com/haoz19/Open-NeRF."
Open-Set Object Detection by Aligning Known Class Representations,"Hiran Sarkar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik, Vineeth N. Balasubramanian",Sony Research India; Indian Institute of Technology Hyderabad,50.0,India,50.0,India,"Open Set Object Detection (OSOD) has emerged as a contemporary research direction to address the detection of unknown objects. Recently, few works have achieved remarkable performance in the OSOD task by employing contrastive clustering to separate unknown classes. In contrast, we propose a new semantic clustering-based approach to facilitate a meaningful alignment of clusters in semantic space and introduce a class decorrelation module to enhance inter-cluster separation. Our approach further incorporates an object focus module to predict objectness scores, which enhances the detection of unknown objects. Further, we employ i) an evaluation technique that penalizes low-confidence outputs to mitigate the risk of misclassification of the unknown objects and ii) a new metric called HMP that combines known and unknown precision using harmonic mean. Our extensive experiments demonstrate that the proposed model achieves significant improvement on the MS-COCO & PASCAL VOC dataset for the OSOD task.",https://openaccess.thecvf.com/content/WACV2024/html/Sarkar_Open-Set_Object_Detection_by_Aligning_Known_Class_Representations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sarkar_Open-Set_Object_Detection_by_Aligning_Known_Class_Representations_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484502/,"['Measurement', 'Computer vision', 'Computational modeling', 'Semantics', 'Object detection', 'Harmonic analysis', 'Decorrelation']","['Object Detection', 'Harmonic Mean', 'Evaluation Techniques', 'Clusters In Space', 'Semantic Space', 'Unknown Objects', 'PASCAL VOC Dataset', 'Training Dataset', 'Convolutional Layers', 'Detection Performance', 'Bounding Box', 'Semantic Similarity', 'Weight Coefficient', 'Decision Boundary', 'Combined Loss', 'Faster R-CNN', 'Single GPU', 'Region Proposal Network', 'ResNet-50 Backbone', 'Semantic Clustering', 'Object Proposals', 'Unseen Objects', 'Orthogonality Constraint', 'Training Categories', 'Unknown Probability', 'Unseen Classes', 'Geometric Mean', 'Ground-truth Bounding Box']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Open-Set Object Detection (OSOD) has emerged as a contemporary research direction to address the detection of unknown objects. Recently, few works have achieved remarkable performance in the OSOD task by employing contrastive clustering to separate unknown classes. In contrast, we propose a new semantic clustering-based approach to facilitate a meaningful alignment of clusters in semantic space and introduce a class decorrelation module to enhance inter-cluster separation. Our approach further incorporates an object focus module to predict objectness scores, which enhances the detection of unknown objects. Further, we employ i) an evaluation technique that penalizes low-confidence outputs to mitigate the risk of misclassification of the unknown objects and ii) a new metric called HMP that combines known and unknown precision using harmonic mean. Our extensive experiments demonstrate that the proposed model achieves significant improvement on the MS-COCO & PASCAL VOC dataset for the OSOD task."
Opinion Unaware Image Quality Assessment via Adversarial Convolutional Variational Autoencoder,"Ankit Shukla, Avinash Upadhyay, Swati Bhugra, Manoj Sharma","Bennett University, Greater Noida, India; Indian Institute of Technology Delhi, New Delhi, India",100.0,India,0.0,,"Image quality assessment is a challenging computer vision task due to the lack of corresponding reference (pristine) images. This no-reference bottleneck has been tackled with the utilisation of subjective mean opinion scores (MOS) termed as supervised blind image quality assessment (BIQA) methods. However, inaccessible opinion score scenarios limits their applicability. To relieve these limitations, we propose to employ reconstruction based learning trained only on pristine images. This permits an implicit distribution learning of pristine images and the deviation from this learned feature distribution is subsequently utilised for unsupervised image quality assessment. Specifically, an adversarial convolutional variational auto-encoder framework is employed with KL divergence, perceptual and discriminator loss. With state-of-the-art results on four benchmark datasets, we demonstrate the effectiveness of our proposed framework. An ablation study has also been conducted to highlight the contribution of each module i.e. loss and quality metric for an efficient unsupervised BIQA.",https://openaccess.thecvf.com/content/WACV2024/html/Shukla_Opinion_Unaware_Image_Quality_Assessment_via_Adversarial_Convolutional_Variational_Autoencoder_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shukla_Opinion_Unaware_Image_Quality_Assessment_via_Adversarial_Convolutional_Variational_Autoencoder_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484040/,"['Measurement', 'Image quality', 'Training', 'Representation learning', 'Computer vision', 'Benchmark testing', 'Distortion']","['Image Quality', 'Variational Autoencoder', 'Convolutional Variational Autoencoder', 'Distribution Characteristics', 'Feature Learning', 'Benchmark Datasets', 'Kullback-Leibler', 'Reference Image', 'Subjective Scores', 'Distributed Learning', 'Discriminator Loss', 'Quality Assessment Methods', 'Opinion Score', 'Mean Opinion Score', 'Convolutional Neural Network', 'Image Dataset', 'Statistical Features', 'Generative Adversarial Networks', 'Natural Images', 'Latent Space', 'Latent Representation', 'Distortion Types', 'Natural Scene Statistics', 'High-quality Images', 'JPEG Compression', 'Quality Metrics', 'Distribution Of Richness', 'Discriminator Network', 'Inference Stage', 'Gaussian Blur']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Image quality assessment is a challenging computer vision task due to the lack of corresponding reference (pristine) images. This no-reference bottleneck has been tackled with the utilisation of subjective mean opinion scores (MOS) termed as supervised blind image quality assessment (BIQA) methods. However, inaccessible opinion score scenarios limits their applicability. To relieve these limitations, we propose to employ reconstruction based learning trained only on pristine images. This permits an implicit distribution learning of pristine images and the deviation from this learned feature distribution is subsequently utilised for unsupervised image quality assessment. Specifically, an adversarial convolutional variational auto-encoder framework is employed with KL divergence, perceptual and discriminator loss. With state-of-the-art results on four benchmark datasets, we demonstrate the effectiveness of our proposed framework. An ablation study has also been conducted to highlight the contribution of each module i.e. loss and quality metric for an efficient unsupervised BIQA."
OptFlow: Fast Optimization-Based Scene Flow Estimation Without Supervision,"Rahul Ahuja, Chris Baker, Wilko Schwarting",ISEE AI,0.0,,100.0,USA,"Scene flow estimation is a crucial component in the development of autonomous driving and 3D robotics, providing valuable information for environment perception and navigation. Despite the advantages of learning-based scene flow estimation techniques, their domain specificity and limited generalizability across varied scenarios pose challenges. In contrast, non-learning optimization-based methods, incorporating robust priors or regularization, offer competitive scene flow estimation performance, require no training, and show extensive applicability across datasets, but suffer from lengthy inference times. In this paper, we present OptFlow, a fast optimization-based scene flow estimation method. Without relying on learning or any labeled datasets, OptFlow achieves state-of-the-art performance for scene flow estimation on popular autonomous driving benchmarks. It integrates a local correlation weight matrix for correspondence matching, an adaptive correspondence threshold limit for nearest-neighbor search, and graph prior rigidity constraints, resulting in expedited convergence and improved point correspondence identification. Moreover, we demonstrate how integrating a point cloud registration function within our objective function bolsters accuracy and differentiates between static and dynamic points without relying on external odometry data. Consequently, OptFlow outperforms the baseline graph-prior method by approximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy, all while offering the fastest inference time among all non-learning scene flow estimation methods.",https://openaccess.thecvf.com/content/WACV2024/html/Ahuja_OptFlow_Fast_Optimization-Based_Scene_Flow_Estimation_Without_Supervision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ahuja_OptFlow_Fast_Optimization-Based_Scene_Flow_Estimation_Without_Supervision_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484394/,"['Point cloud compression', 'Training', 'Correlation', 'Three-dimensional displays', 'Estimation', 'Nearest neighbor methods', 'Rigidity']","['Scene Flow', 'Scene Flow Estimation', 'Objective Function', 'Point Cloud', 'Corresponding Points', 'Inference Time', 'Adaptive Threshold', 'Fastest Time', 'Optimization-based Methods', 'Dynamic Point', 'Local Weights', 'Point Cloud Registration', 'Rigid Constraints', 'Training Data', 'Flow Field', 'Point Source', 'Learning-based Methods', 'Real-world Scenarios', 'Distance Threshold', 'Transformation Function', 'Flow Vector', 'Iterative Closest Point', 'KITTI Dataset', 'Dynamic Objects', '3D Motion', 'Final Objective Function', 'Pair Of Points', 'Graph Laplacian', 'Dense Dataset', 'Motion Estimation']","['Algorithms', '3D computer vision', 'Applications', 'Autonomous Driving', 'Applications', 'Robotics']",1,"Scene flow estimation is a crucial component in the development of autonomous driving and 3D robotics, providing valuable information for environment perception and navigation. Despite the advantages of learning-based scene flow estimation techniques, their domain specificity and limited generalizability across varied scenarios pose challenges. In contrast, non-learning optimization-based methods, incorporating robust priors or regularization, offer competitive scene flow estimation performance, require no training, and show extensive applicability across datasets, but suffer from lengthy inference times.In this paper, we present OptFlow, a fast optimization-based scene flow estimation method. Without relying on learning or any labeled datasets, OptFlow achieves state-of-the-art performance for scene flow estimation on popular autonomous driving benchmarks. It integrates a local correlation weight matrix for correspondence matching, an adaptive correspondence threshold limit for nearest-neighbor search, and graph prior rigidity constraints, resulting in expedited convergence and improved point correspondence identification. Moreover, we demonstrate how integrating a point cloud registration function within our objective function bolsters accuracy and differentiates between static and dynamic points without relying on external odometry data. Consequently, OptFlow outperforms the baseline graph-prior method by approximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy, all while offering the fastest inference time among all nonlearning scene flow estimation methods."
Optical Flow Domain Adaptation via Target Style Transfer,"Jeongbeen Yoon, Sanghyun Kim, Suha Kwak, Minsu Cho","Pohang University of Science and Technology (POSTECH), South Korea",100.0,South Korea,0.0,,"Optical flows play an integral role for a variety of motion-related tasks such as action recognition, object segmentation, and tracking in videos. While state-of-the-art optical flow methods heavily rely on learning, the learned optical flow methods significantly degrade when applied to different domains, and the training datasets are very limited due to the extreme cost of flow-level annotation. To tackle the issue, we introduce a domain adaptation technique for optical flow estimation. Our method extracts diverse style statistics of the target domain and use them in training to generate synthetic features from the source features, which contain the contents of the source but the style of the target. We also impose motion consistency between the synthetic target and the source and deploy adversarial learning at the flow prediction to encourage domain-invariant features. Experimental results show that the proposed method achieves substantial and consistent improvements in different domain adaptation scenarios on VKITTI 2, Sintel, and KITTI 2015 benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Yoon_Optical_Flow_Domain_Adaptation_via_Target_Style_Transfer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yoon_Optical_Flow_Domain_Adaptation_via_Target_Style_Transfer_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484458/,"['Training', 'Computer vision', 'Computational modeling', 'Ultraviolet sources', 'Estimation', 'Feature extraction', 'Adversarial machine learning']","['Optical Flow', 'Domain Adaptation', 'Style Transfer', 'Optical Domain', 'Target Style', 'Generative Adversarial Networks', 'Action Recognition', 'Target Domain', 'Source Characteristics', 'Object Segmentation', 'Flow Estimation', 'Flow Prediction', 'Synthetic Target', 'Optical Flow Estimation', 'Domain-invariant Features', 'Optical Flow Method', 'Domain Adaptation Techniques', 'Convolutional Layers', 'Extreme Weather', 'Aforementioned Methods', 'Source Domain', 'Flow Map', 'Source Concentration', 'Domain Gap', 'Random Permutations', 'Refinement Network', 'Optical Model', 'Transfer Module', 'Network Flow', 'Semantic Segmentation']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Optical flows play an integral role for a variety of motion-related tasks such as action recognition, object segmentation, and tracking in videos. While state-of-the-art optical flow methods heavily rely on learning, the learned optical flow methods significantly degrade when applied to different domains, and the training datasets are very limited due to the extreme cost of flow-level annotation. To tackle the issue, we introduce a domain adaptation technique for optical flow estimation. Our method extracts diverse style statistics of the target domain and use them in training to generate synthetic features from the source features, which contain the contents of the source but the style of the target. We also impose motion consistency between the synthetic target and the source and deploy adversarial learning at the flow prediction to encourage domain-invariant features. Experimental results show that the proposed method achieves substantial and consistent improvements in different domain adaptation scenarios on VKITTI 2, Sintel, and KITTI 2015 benchmarks."
Optimizing Long-Term Robot Tracking With Multi-Platform Sensor Fusion,"Giuliano Albanese, Arka Mitra, Jan-Nico Zaech, Yupeng Zhao, Ajad Chhatkuli, Luc Van Gool","ETH Zurich, Zurich, Switzerland; KU Leuven, Leuven, Belgium; INSAIT, Sofia, Bulgaria; ETH Zurich, Zurich, Switzerland",100.0,"Belgium, Bulgaria, Switzerland",0.0,,"Monitoring a fleet of robots requires stable long-term tracking with re-identification, which is yet an unsolved challenge in many scenarios. One application of this is the analysis of autonomous robotic soccer games at RoboCup. Tracking in these games requires handling of identically looking players, strong occlusions, and non-professional video recordings, but also offers state information estimated by the robots. In order to make effective use of the information coming from the robot sensors, we propose a robust tracking and identification pipeline. It fuses external non-calibrated camera data with the robots' internal states using quadratic optimization for tracklet matching. The approach is validated using game recordings from previous RoboCup World Cup tournaments.",https://openaccess.thecvf.com/content/WACV2024/html/Albanese_Optimizing_Long-Term_Robot_Tracking_With_Multi-Platform_Sensor_Fusion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Albanese_Optimizing_Long-Term_Robot_Tracking_With_Multi-Platform_Sensor_Fusion_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483912/,"['Visualization', 'Video sequences', 'Games', 'Sensor fusion', 'Robot sensing systems', 'Sensors', 'Robots']","['Long-term Tracking', 'Use Of Information', 'Internal State', 'Robust Tracking', 'Quadratic Optimization', 'External Camera', 'Optimization Problem', 'Bounding Box', 'Particle Swarm Optimization', 'Pose Estimation', 'Field Lines', 'Tracking Problem', 'Humanoid Robot', 'Camera Calibration', 'Camera Pose', 'Tracking Approach', 'Cost Term', 'Soccer Match', 'Cost Weight', 'Game Controller', 'Camera Pose Estimation', 'Multi-object Tracking', 'Joint Detection', 'Constant Velocity Model', 'Identification Of Players', 'Binary Optimization', 'Game Conditions', 'Pedestrian', 'Straight Line', 'Feature Pyramid Network']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Robotics']",,"Monitoring a fleet of robots requires stable long-term tracking with re-identification, which is yet an unsolved challenge in many scenarios. One application of this is the analysis of autonomous robotic soccer games at RoboCup. Tracking in these games requires handling of identically looking players, strong occlusions, and non-professional video recordings, but also offers state information estimated by the robots. In order to make effective use of the information coming from the robot sensors, we propose a robust tracking and identification pipeline. It fuses external non-calibrated camera data with the robots’ internal states using quadratic optimization for tracklet matching. The approach is validated using game recordings from previous RoboCup World Cup tournaments."
Ordinal Classification With Distance Regularization for Robust Brain Age Prediction,"Jay Shah, Md Mahfuzur Rahman Siddiquee, Yi Su, Teresa Wu, Baoxin Li","Arizona State University, ASU-Mayo Center for Innovative Imaging; Banner Alzheimer’s Institute, ASU-Mayo Center for Innovative Imaging",100.0,USA,0.0,,"Age is one of the major known risk factors for Alzheimer's Disease (AD). Detecting AD early is crucial for effective treatment and preventing irreversible brain damage. Brain age, a measure derived from brain imaging reflecting structural changes due to aging, may have the potential to identify AD onset, assess disease risk, and plan targeted interventions. Deep learning-based regression techniques to predict brain age from magnetic resonance imaging (MRI) scans have shown great accuracy recently. However, these methods are subject to an inherent regression to the mean effect, which causes a systematic bias resulting in an overestimation of brain age in young subjects and underestimation in old subjects. This weakens the reliability of predicted brain age as a valid biomarker for downstream clinical applications. Here, we reformulate the brain age prediction task from regression to classification to address the issue of systematic bias. Recognizing the importance of preserving ordinal information from ages to understand aging trajectory and monitor aging longitudinally, we propose a novel ORdinal Distance Encoded Regularization (ORDER) loss that incorporates the order of age labels, enhancing the model's ability to capture age-related patterns. Extensive experiments and ablation studies demonstrate that this framework reduces systematic bias, outperforms state-of-art methods by statistically significant margins, and can better capture subtle differences between clinical groups in an independent AD dataset. Our implementation is publicly available at https://github.com/jaygshah/Robust-Brain-Age-Prediction.",https://openaccess.thecvf.com/content/WACV2024/html/Shah_Ordinal_Classification_With_Distance_Regularization_for_Robust_Brain_Age_Prediction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shah_Ordinal_Classification_With_Distance_Regularization_for_Robust_Brain_Age_Prediction_WACV_2024_paper.pdf,,https://github.com/jaygshah/Robust-Brain-Age-Prediction,,main,Poster,https://ieeexplore.ieee.org/document/10483799/,"['Systematics', 'Magnetic resonance imaging', 'Biological system modeling', 'Aging', 'Predictive models', 'Brain modeling', 'Entropy']","['Brain Aging', 'Ordinal Categories', 'Brain Age Prediction', 'Magnetic Resonance Imaging', 'Alzheimer’s Disease', 'Magnetic Resonance Imaging Scans', 'Systematic Bias', 'Clinical Groups', 'Young Subjects', 'Risk Factor For Alzheimer', 'Regularization Loss', 'Irreversible Brain Damage', 'Deep Learning', 'Classification Model', 'Ordination', 'Healthy Aging', 'Mild Cognitive Impairment', 'Feature Representation', 'Deep Learning Models', 'Alzheimer’s Disease Patients', 'Mean Square Error Loss', 'Prediction Bias', 'Cross-entropy Loss', 'Manhattan Distance', 'Multi-label', 'Chronological Age', 'Distance Metrics', 'Healthy Cohort', 'Public Sources', 'Class Labels']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",1,"Age is one of the major known risk factors for Alzheimer’s Disease (AD). Detecting AD early is crucial for effective treatment and preventing irreversible brain damage. Brain age, a measure derived from brain imaging reflecting structural changes due to aging, may have the potential to identify AD onset, assess disease risk, and plan targeted interventions. Deep learning-based regression techniques to predict brain age from magnetic resonance imaging (MRI) scans have shown great accuracy recently. However, these methods are subject to an inherent regression to the mean effect, which causes a systematic bias resulting in an overestimation of brain age in young subjects and underestimation in old subjects. This weakens the reliability of predicted brain age as a valid biomarker for downstream clinical applications. Here, we reformulate the brain age prediction task from regression to classification to address the issue of systematic bias. Recognizing the importance of preserving ordinal information from ages to understand aging trajectory and monitor aging longitudinally, we propose a novel ORdinal Distance Encoded Regularization (ORDER) loss that incorporates the order of age labels, enhancing the model’s ability to capture age-related patterns. Extensive experiments and ablation studies demonstrate that this framework reduces systematic bias, outperforms state-of-art methods by statistically significant margins, and can better capture subtle differences between clinical groups in an independent AD dataset. Our implementation is publicly available at https://github.com/jaygshah/Robust-Brain-Age-Prediction."
Out-of-Distribution Detection With Logical Reasoning,"Konstantin Kirchheim, Tim Gonschorek, Frank Ortmeier","Department of Computer Science, Otto von Guericke University Magdeburg, Germany",100.0,Germany,0.0,,"Machine Learning models often only generalize reliably to samples from the training distribution. Consequentially, detecting when input data is out-of-distribution (OOD) is crucial, especially in safety-critical applications. Current OOD detection methods, however, tend to be domain agnostic and often fail to incorporate valuable prior knowledge about the structure of the training distribution. To address this limitation, we introduce a novel, hybrid OOD detection algorithm that combines a deep learning-based perception system with a first-order logic-based knowledge representation. A logical reasoning system uses this knowledge base at run-time to infer whether inputs are consistent with prior knowledge about the training distribution. In contrast to purely neural systems, the structured knowledge representation allows humans to inspect and modify the rules that govern the OOD detectors' behavior. This not only enhances performance but also fosters a level of explainability that is particularly beneficial in safety-critical contexts. We demonstrate the effectiveness of our method through experiments on several datasets and discuss advantages and limitations. Our code is available online.",https://openaccess.thecvf.com/content/WACV2024/html/Kirchheim_Out-of-Distribution_Detection_With_Logical_Reasoning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kirchheim_Out-of-Distribution_Detection_With_Logical_Reasoning_WACV_2024_paper.pdf,,https://github.com/kkirchheim/logic-ood,,main,Poster,https://ieeexplore.ieee.org/document/10483945/,"['Training', 'Computer vision', 'Codes', 'Knowledge based systems', 'Knowledge representation', 'Machine learning', 'Cognition']","['Logical Reasoning', 'Knowledge Base', 'Perceptual System', 'Training Distribution', 'Safety-critical Applications', 'Primates', 'Posterior Probability', 'Deep Neural Network', 'Cognitive Domains', 'State Space', 'Light Signal', 'Root Node', 'Formal Rules', 'Gibbons', 'Input Space', 'Red Square', 'Semantic Knowledge', 'Form Of Constraints', 'Additional Labels', 'WordNet']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Machine Learning models often only generalize reliably to samples from the training distribution. Consequentially, detecting when input data is out-of-distribution (OOD) is crucial, especially in safety-critical applications. Current OOD detection methods, however, tend to be domain agnostic and often fail to incorporate valuable prior knowledge about the structure of the training distribution. To address this limitation, we introduce a novel, hybrid OOD detection algorithm that combines a deep learning-based perception system with a first-order logic-based knowledge representation. A logical reasoning system uses this knowledge base at run-time to infer whether inputs are consistent with prior knowledge about the training distribution. In contrast to purely neural systems, the structured knowledge representation allows humans to inspect and modify the rules that govern the OOD detectors’ behavior. This not only enhances performance but also fosters a level of explainability that is particularly beneficial in safety-critical contexts. We demonstrate the effectiveness of our method through experiments on several datasets and discuss advantages and limitations. Our code is available online.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Overcoming Catastrophic Forgetting for Multi-Label Class-Incremental Learning,"Xiang Song, Kuang Shu, Songlin Dong, Jie Cheng, Xing Wei, Yihong Gong","Central Southern China Electric Power Design Institute Co., ltd. of China Power Engineering Consulting Group, Wuhan, China; Huawei Base, Bantian, Shenzhen, China; School of Software Engineering, Xi’an Jiaotong Univerisity, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China",75.0,China,25.0,China,"Despite the recent progress of class-incremental learning (CIL) methods, their capabilities in real-world scenarios such as multi-label settings remain unexplored. This paper focuses on a more practical CIL problem named multi-label class-incremental learning (MLCIL). MLCIL requires the vision models to overcome catastrophic forgetting of old knowledge while learning new classes from multi-label samples. Direct application of existing CIL methods to MLCIL leads to label absence, representative sample selection, and feature dilution problems. To address these problems, we present a novel AdaPtive Pseudo-Label-drivEn (APPLE) framework consisting of three components. First, the adaptive pseudo-label strategy is proposed to solve the label absence problem, which leverages the old model to annotate old classes for new samples. Second, a cluster sampling strategy is proposed to obtain more diverse samples to alleviate catastrophic forgetting under the MLCIL setting better. Finally, a class attention decoder is designed to mitigate the object feature dilution problem in multi-label samples. The extensive experiments on PASCAL VOC 2007 and MS-COCO demonstrate that our proposed method significantly outperforms other representative state-of-the-art CIL methods.",https://openaccess.thecvf.com/content/WACV2024/html/Song_Overcoming_Catastrophic_Forgetting_for_Multi-Label_Class-Incremental_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Song_Overcoming_Catastrophic_Forgetting_for_Multi-Label_Class-Incremental_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484278/,"['Adaptation models', 'Computer vision', 'Decoding']","['Catastrophic Forgetting', 'Multi-label Learning', 'Class-incremental Learning', 'Representative Sample', 'Cluster Sampling', 'Diverse Sample', 'Extensive Experiments', 'Object Features', 'Cluster Sampling Strategy', 'Training Set', 'Training Data', 'Spatial Information', 'Feature Maps', 'Multilayer Perceptron', 'Incremental Learning', 'Joint Training', 'Attention Heads', 'MS COCO Dataset', 'Distillation Loss', 'ResNet-101 Backbone', 'Multi-label Classification Task', 'Second-best Method', 'Spatial Feature Information']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",2,"Despite the recent progress of class-incremental learning (CIL) methods, their capabilities in real-world scenarios such as multi-label settings remain unexplored. This paper focuses on a more practical CIL problem named multi-label class-incremental learning (MLCIL). MLCIL requires the vision models to overcome catastrophic forgetting of old knowledge while learning new classes from multi-label samples. Direct application of existing CIL methods to MLCIL leads to label absence, representative sample selection, and feature dilution problems. To address these problems, we present a novel AdaPtive Pseudo-Label-drivEn (APPLE) framework consisting of three components. First, the adaptive pseudo-label strategy is proposed to solve the label absence problem, which leverages the old model to annotate old classes for new samples. Second, a cluster sampling strategy is proposed to obtain more diverse samples to alleviate catastrophic forgetting under the MLCIL setting better. Finally, a class attention decoder is designed to mitigate the object feature dilution problem in multi-label samples. The extensive experiments on PASCAL VOC 2007 and MS-COCO demonstrate that our proposed method significantly outperforms other representative state-of-the-art CIL methods."
P-Age: Pexels Dataset for Robust Spatio-Temporal Apparent Age Classification,"Abid Ali, Ashish Marisetty, François Brémond","Inria, France; Universit ´e Cote d’Azur, France; IIIT Naya Raipur, India",100.0,"France, India",0.0,,"Age estimation is a challenging task that has numerous applications. In this paper, we propose a new direction for age classification that utilizes a video-based model to address challenges such as occlusions, low-resolution, and lighting conditions. To address these challenges, we propose AgeFormer which utilizes spatio-temporal information on the dynamics of the entire body dominating face-based methods for age classification. Our novel two-stream architecture uses TimeSformer and EfficientNet as backbones, to effectively capture both facial and body dynamics information for efficient and accurate age estimation in videos. Furthermore, to fill the gap in predicting age in real-world situations from videos, we construct a video dataset called Pexels Age (P-Age) for age classification. The proposed method achieves superior results compared to existing face-based age estimation methods and is evaluated in situations where the face is highly occluded, blurred, or masked. The method is also cross-tested on a variety of challenging video datasets such as Charades, Smarthome, and Thumos-14.",https://openaccess.thecvf.com/content/WACV2024/html/Ali_P-Age_Pexels_Dataset_for_Robust_Spatio-Temporal_Apparent_Age_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ali_P-Age_Pexels_Dataset_for_Robust_Spatio-Temporal_Apparent_Age_Classification_WACV_2024_paper.pdf,,https://github.com/Ashish013/AgeFormer,,main,Poster,https://ieeexplore.ieee.org/document/10483887/,"['Privacy', 'Computer vision', 'Computational modeling', 'Estimation', 'Lighting', 'Computer architecture', 'Predictive models']","['Age Categories', 'Robust Classification', 'Light Conditions', 'Challenging Task', 'Age Estimation', 'Real-world Situations', 'Challenging Dataset', 'Smart Home', 'Video Dataset', 'Body Dynamics', 'Accuracy Of Age Estimation', 'Age Groups', 'Ethnic Groups', 'Transformer', 'Convolutional Neural Network', 'Attention Mechanism', 'Upper Body', 'Elderly Persons', 'Facial Features', 'Video Analysis', 'Action Recognition', 'Privacy Preservation', 'Age Prediction']","['Applications', 'Social good', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",1,"Age estimation is a challenging task that has numerous applications. In this paper, we propose a new direction for age classification that utilizes a video-based model to address challenges such as occlusions, low-resolution, and lighting conditions. To address these challenges, we propose AgeFormer which utilizes spatio-temporal information on the dynamics of the entire body dominating facebased methods for age classification. Our novel two-stream architecture uses TimeSformer and EfficientNet as backbones, to effectively capture both facial and body dynamics information for efficient and accurate age estimation in videos. Furthermore, to fill the gap in predicting age in real-world situations from videos, we construct a video dataset called Pexels Age (P-Age) for age classification. The proposed method achieves superior results compared to existing face-based age estimation methods and is evaluated in situations where the face is highly occluded, blurred, or masked. The method is also cross-tested on a variety of challenging video datasets such as Charades, Smarthome, and Thumos-14. The code and dataset is available at https://github.com/Ashish013/AgeFormer."
P2D: Plug and Play Discriminator for Accelerating GAN Frameworks,"Min Jin Chong, Krishna Kumar Singh, Yijun Li, Jingwan Lu, David Forsyth",ByteDance Inc.; UIUC; Adobe Research,33.33333333333333,USA,66.66666666666667,China,"Most image classification tasks benefit from using pretrained feature stacks. In contrast, the discriminator for adversarial losses is trained at the same time as the model because using a pretrained feature stack yields a very poor model. Recent work has shown that an implicit regularization scheme allows using pretrained feature stacks to construct a discriminator, which improves both speed of training and quality of results. However, we observe that changes in hyperparameters can result in substantial changes in generator behavior. We show that using a modified version of the R1 regularization scheme that regularizes in the feature space instead of the image space results in a plug-and-play discriminator -- P2D. Our scheme results in a method that is highly stable across changes in architecture and framework; that significantly speeds up training; and that produces models that reliably beat SOTA in quality. The huge reduction in training resources required means that P2D could make training powerful generative models over specific datasets accessible to most researchers.",https://openaccess.thecvf.com/content/WACV2024/html/Chong_P2D_Plug_and_Play_Discriminator_for_Accelerating_GAN_Frameworks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chong_P2D_Plug_and_Play_Discriminator_for_Accelerating_GAN_Frameworks_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483836/,"['Training', 'Computer vision', 'Computer architecture', 'Generators', 'Reliability', 'Task analysis', 'Plugs']","['Feature Space', 'Training Speed', 'Pre-trained Feature', 'Image Quality', 'Batch Size', 'Foundation Model', 'Wall-clock', 'Large Batch Size', 'Small Batch Size', 'Fréchet Inception Distance', 'Multiple Feature Extraction', 'Coded Based']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Most image classification tasks benefit from using pre-trained feature stacks. In contrast, the discriminator for adversarial losses is trained at the same time as the model because using a pretrained feature stack yields a very poor model. Recent work has shown that an implicit regularization scheme allows using pretrained feature stacks to construct a discriminator, which improves both speed of training and quality of results. However, we observe that changes in hyperparameters can result in substantial changes in generator behavior.We show that using a modified version of the R1 regularization scheme that regularizes in the feature space instead of the image space results in a plug-and-play discriminator– P2D. Our scheme results in a method that is highly stable across changes in architecture and framework; that significantly speeds up training; and that produces models that reliably beat SOTA in quality. The huge reduction in training resources required means that P2D could make training powerful generative models over specific datasets accessible to most researchers."
PAIR: Perception Aided Image Restoration for Natural Driving Conditions,"Pranjay Shyam, HyunJin Yoo","Faurecia IRYStec Inc., Montreal, Canada",0.0,,100.0,Canada,"We present a two-stage mechanism for generic image restoration in natural driving conditions, where multiple non-linear degradations simultaneously impact perception for humans and driving assistance systems. Our approach overcomes the limitations of utilizing a single neural network that incurs excessive computational overhead and yields sub-optimal recovery. The proposed first stage comprises computationally inexpensive image processing operations applied at a patch level using a lightweight convolutional neural network (CNN) that determines their intensity of operation. This patch size is guided by the receptive field of the CNN, allowing for dynamic restoration of non-linear and non-homogeneous degradation profiles. The second stage leverages a lightweight end-to-end neural network functioning as an inpainting network. It identifies inadequately restored regions and leverages global semantic and structural information to fill the affected areas. This approach enhances the restoration process by considering the entire image and addresses the remainder of localized deficiencies. In addition, we integrate dense perception tasks such as semantic and depth estimation during the optimization cycle to ensure restored images that are perceptually pleasing and conducive for downstream perception tasks. Since datasets covering diverse degradation scenarios for high- and low-level perception tasks are lacking, we utilize a synthetic data augmentation technique to generate non-homogeneous non-linear degradation profiles. Experiments on images captured in adverse weather conditions demonstrate the efficacy of our approach, yielding higher perceptual quality in restored images and improved performance in downstream perception tasks under adverse driving conditions. Importantly, our method offers computational efficiency compared to end-to-end image restoration algorithms, making it suitable for real-time applications.",https://openaccess.thecvf.com/content/WACV2024/html/Shyam_PAIR_Perception_Aided_Image_Restoration_for_Natural_Driving_Conditions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shyam_PAIR_Perception_Aided_Image_Restoration_for_Natural_Driving_Conditions_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks,"Shiwei Ding, Lan Zhang, Miao Pan, Xiaoyong Yuan",Michigan Technological University; University of Houston,100.0,USA,0.0,,"Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs). In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference. However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference. Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference. This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference. PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features. Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation. To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively. On a real-world collaborative inference task, vehicle re-identification, we demonstrate the superior performance of PATROL in terms of against MIAs.",https://openaccess.thecvf.com/content/WACV2024/html/Ding_PATROL_Privacy-Oriented_Pruning_for_Collaborative_Inference_Against_Model_Inversion_Attacks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ding_PATROL_Privacy-Oriented_Pruning_for_Collaborative_Inference_Against_Model_Inversion_Attacks_WACV_2024_paper.pdf,,,2307.10981,main,Poster,https://ieeexplore.ieee.org/document/10483973/,"['Training', 'Performance evaluation', 'Privacy', 'Perturbation methods', 'Collaboration', 'Artificial neural networks', 'Feature extraction']","['Inverse Model', 'Model Inversion Attack', 'Collaborative Inference', 'Neural Network', 'Deep Neural Network', 'Intermediate Results', 'Regular Training', 'Adversarial Training', 'Edge Devices', 'Real-world Tasks', 'Collaborative Tasks', 'Efficient Inference', 'Perturbation Technique', 'Task-specific Features', 'Pruning Techniques', 'Prediction Accuracy', 'Cloud Computing', 'Image Reconstruction', 'Generative Adversarial Networks', 'Privacy Protection', 'Defense Methods', 'Target Model', 'Black-box Attacks', 'White-box Attack', 'Intermediate Output', 'Privacy Risks', 'Multi-party Computation', 'ResNet Block', 'Lipschitz Continuous', 'Raw Input']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs). In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference. However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference. Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference. This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference. PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features. Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation. To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively. On a real-world collaborative inference task, vehicle re-identification, we demonstrate the superior performance of PATROL in terms of against MIAs."
PDA-RWSR: Pixel-Wise Degradation Adaptive Real-World Super-Resolution,"Andreas Aakerberg, Majed El Helou, Kamal Nasrollahi, Thomas Moeslund","ETH Zürich, Switzerland; Aalborg University, Denmark",100.0,"Denmark, Switzerland",0.0,,"While many methods have been proposed to solve the Super-Resolution (SR) problem of Low-Resolution (LR) images with complex unknown degradations, their performance still drops significantly when evaluated on images with challenging real-world degradations. One often overlooked factor contributing to this, is the presence of spatially varying degradations in real LR images. To address this issue, we propose a novel degradation pipeline capable of generating paired LR/High-Resolution (HR) images with spatially varying noise, a key contributor to reducedimage quality. Furthermore, to fully leverage such training data, we novelly propose a Pixel-Wise Degradation Adaptive Real-World Super-Resolution (PDA-RWSR) framework. Specifically, we design a new Restormer-based Real-World Super-Resolution (RWSR) model capable of adapting the reconstruction process based on pixel-wise degradation features extracted by a new supervised degradation estimation model. Along with our proposed method, we also introduce a new challenging real-world Spatially Variant Super-Resolution (SVSR) benchmarking dataset, where the images are degraded by complex noise of varying intensity and type, to evaluate the robustness of existing RWSR methods. Comprehensive experiments on synthetic and the proposed challenging real dataset demonstrates the superiority of our method over the current State-of-The-Art (SoTA).",https://openaccess.thecvf.com/content/WACV2024/html/Aakerberg_PDA-RWSR_Pixel-Wise_Degradation_Adaptive_Real-World_Super-Resolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Aakerberg_PDA-RWSR_Pixel-Wise_Degradation_Adaptive_Real-World_Super-Resolution_WACV_2024_paper.pdf,https://doi.org/10.5281/zenodo.10044260,,,main,Poster,https://ieeexplore.ieee.org/document/10483938/,"['Degradation', 'Training', 'Adaptation models', 'Superresolution', 'Pipelines', 'Noise', 'Training data']","['Training Data', 'Image Quality', 'Benchmark Datasets', 'Low-resolution Images', 'Degradation Characteristics', 'Super-resolution Model', 'Convolutional Neural Network', 'High-resolution Images', 'Deep Neural Network', 'Convolutional Layers', 'Gaussian Noise', 'Feature Maps', 'Additive Noise', 'Image Pairs', 'Generative Adversarial Networks', 'Peak Signal-to-noise Ratio', 'Types Of Noise', 'Shot Noise', 'Noisy Images', 'Super-resolution Network', 'Blur Kernel', 'Real-world Images', 'JPEG Compression', 'Compression Artifacts', 'Zoom Lens', 'Elaborate Models', 'Transformer', 'Feature Module', 'Quality Assessment Metrics']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Datasets and evaluations']",,"While many methods have been proposed to solve the Super-Resolution (SR) problem of Low-Resolution (LR) images with complex unknown degradations, their performance still drops significantly when evaluated on images with challenging real-world degradations. One often overlooked factor contributing to this, is the presence of spatially varying degradations in real LR images. To address this issue, we propose a novel degradation pipeline capable of generating paired LR/High-Resolution (HR) images with spatially varying noise, a key contributor to reduced image quality. Furthermore, to fully leverage such training data, we novelly propose a Pixel-Wise Degradation Adaptive Real-World Super-Resolution (PDA-RWSR) framework. Specifically, we design a new Restormer-based Real-World Super-Resolution (RWSR) model capable of adapting the reconstruction process based on pixel-wise degradation features extracted by a new supervised degradation estimation model. Along with our proposed method, we also introduce a new challenging real-world Spatially Variant Super-Resolution (SVSR) benchmarking dataset, where the images are degraded by complex noise of varying intensity and type, to evaluate the robustness of existing RWSR methods. Comprehensive experiments on synthetic and the proposed challenging real dataset demonstrates the superiority of our method over the current State-of-The-Art (SoTA). The SVSR dataset is available at https://doi.org/10.5281/zenodo.10044260."
PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment,"Amirhossein Dadashzadeh, Shuchao Duan, Alan Whone, Majid Mirmehdi","Translational Health Sciences, University of Bristol, UK; School of Computer Science, University of Bristol, UK",100.0,UK,0.0,,"The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules' parameters are updated. We demonstrate PECoP's ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (| 6.0%), MTL-AQA (| 0.99%), and FineDiving (| 2.54%). We also present a new Parkinson's Disease dataset, PD4T, of real patients performing four various actions, where we surpass (| 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.",https://openaccess.thecvf.com/content/WACV2024/html/Dadashzadeh_PECoP_Parameter_Efficient_Continual_Pretraining_for_Action_Quality_Assessment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dadashzadeh_PECoP_Parameter_Efficient_Continual_Pretraining_for_Action_Quality_Assessment_WACV_2024_paper.pdf,,https://github.com/Plrbear/PECoP,2311.07603,main,Poster,https://ieeexplore.ieee.org/document/10484297/,"['Training', 'Computer vision', 'Costs', 'Self-supervised learning', 'Benchmark testing', 'Data models', 'Quality assessment']","['Action Quality Assessment', 'Benchmark Datasets', 'Domain Shift', 'Self-supervised Learning', 'Adaptive Modulation', 'Disease Dataset', 'Pre-training Stage', 'Ablation', 'Training Data', 'Small Datasets', 'Transfer Learning', 'ImageNet', 'Video Clips', 'Trainable Parameters', 'Action Recognition', 'Target Domain', 'Spatiotemporal Characteristics', 'Learned Weights', 'Source Domain', 'Target Task', 'Domain-specific Knowledge', 'Self-supervised Learning Methods', 'Video Dataset', 'Inception Module', '3D Convolution', 'Affine Parameter', 'Pretext Task']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Video recognition and understanding']",5,"The limited availability of labelled data in Action Quality Assessment (AQA), has forced previous works to fine-tune their models pretrained on large-scale domain-general datasets. This common approach results in weak generalisation, particularly when there is a significant domain shift. We propose a novel, parameter efficient, continual pretraining framework, PECoP, to reduce such domain shift via an additional pretraining stage. In PECoP, we introduce 3D-Adapters, inserted into the pretrained model, to learn spatiotemporal, in-domain information via self-supervised learning where only the adapter modules’ parameters are updated. We demonstrate PECoP’s ability to enhance the performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS (↑ 6.0%), MTL-AQA (↑ 0.99%), and FineDiving (↑ 2.54%). We also present a new Parkinson’s Disease dataset, PD4T, of real patients performing four various actions, where we surpass (↑ 3.56%) the state-of-the-art in comparison. Our code, pretrained models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP."
PETIT-GAN: Physically Enhanced Thermal Image-Translating Generative Adversarial Network,"Omri Berman, Navot Oz, David Mendlovic, Nir Sochen, Yafit Cohen, Iftach Klapp","Agricultural Research Organization - Volcani Institute, Rishon LeZion, Israel; Tel Aviv University, Tel Aviv, Israel",100.0,Israel,0.0,,"Thermal multispectral imagery is imperative for a plethora of environmental applications. Unfortunately, there are no publicly-available datasets of thermal multispectral images with a high spatial resolution that would enable the development of algorithms and systems in this field. However, image-to-image (I2I) translation could be used to artificially synthesize such data by transforming largely-available datasets of other visual modalities. In most cases, pairs of content-wise-aligned input-target images are not available, making it harder to train and converge to a satisfying solution. Nevertheless, some data domains, and particularly the thermal domain, have unique properties that tie the input to the output that could help mitigate those weaknesses. We propose PETIT-GAN, a physically enhanced thermal image-translating generative adversarial network to transform between different thermal modalities - a step toward synthesizing a complete thermal multispectral dataset. Our novel approach embeds physically modeled prior information in an UI2I translation to produce outputs with greater fidelity to the target modality. We further show that our solution outperforms the current state-of-the-art architectures at thermal UI2I translation by approximately 50% with respect to the standard perceptual metrics, and enjoys a more robust training procedure. The code and data used for the development and analysis of our method are publicly available and can be accessed through our project's website: https://bermanz.github.io/PETIT",https://openaccess.thecvf.com/content/WACV2024/html/Berman_PETIT-GAN_Physically_Enhanced_Thermal_Image-Translating_Generative_Adversarial_Network_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Berman_PETIT-GAN_Physically_Enhanced_Thermal_Image-Translating_Generative_Adversarial_Network_WACV_2024_paper.pdf,https://bermanz.github.io/PETIT,https://bermanz.github.io/PETIT,,main,Poster,https://ieeexplore.ieee.org/document/10483608/,"['Training', 'Measurement', 'Visualization', 'Computer vision', 'Transforms', 'Computer architecture', 'Generative adversarial networks']","['Generative Adversarial Networks', 'Infrared Imaging', 'Multispectral Images', 'Thermal Mode', 'Target Modality', 'Input Image', 'Grayscale Images', 'Blackbody', 'Affine Transformation', 'Emissivity', 'Random Initialization', 'Incident Power', 'Weight Initialization', 'Translation Task', 'Panchromatic Image', 'Input Domain', 'Thermal Spectrum', 'End Of Epoch', 'Cycle Consistency', 'Fréchet Inception Distance', 'Ideal Object', 'Output Domain', 'Preservation Of Content', 'Calibration Setup', 'Aerial Image Dataset', 'Intensity Levels', 'Camera Lens', 'Monomial', 'Conditional Generative Adversarial Network']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",1,"Thermal multispectral imagery is imperative for a plethora of environmental applications. Unfortunately, there are no publicly-available datasets of thermal multi-spectral images with a high spatial resolution that would enable the development of algorithms and systems in this field. However, image-to-image (I2I) translation could be used to artificially synthesize such data by transforming largely-available datasets of other visual modalities. In most cases, pairs of content-wise-aligned input-target images are not available, making it harder to train and converge to a satisfying solution. Nevertheless, some data domains, and particularly the thermal domain, have unique properties that tie the input to the output that could help mitigate those weaknesses. We propose PETIT-GAN, a physically enhanced thermal image-translating generative adversarial network to transform between different thermal modalities - a step toward synthesizing a complete thermal multispectral dataset. Our novel approach embeds physically modeled prior information in an UI2I translation to produce outputs with greater fidelity to the target modality. We further show that our solution outperforms the current state-of-the-art architectures at thermal UI2I translation by approximately 50% with respect to the standard perceptual metrics, and enjoys a more robust training procedure. The code and data used for the development and analysis of our method are publicly available and can be accessed through our project’s website: https://bermanz.github.io/PETIT"
PGVT: Pose-Guided Video Transformer for Fine-Grained Action Recognition,"Haosong Zhang, Mei Chee Leong, Liyuan Li, Weisi Lin","Institute for Infocomm Research (I2R), A*STAR, Singapore; Nanyang Technological University",100.0,Singapore,0.0,,"Based on recent advancements in transformer-based video models and multi-modal joint learning, we propose a novel model, named Pose-Guided Video Transformer (PGVT), to incorporate sparse high-level body joints locations and dense low-level visual pixels for effective learning and accurate recognition of human actions. PGVT leverages the pre-trained image models by freezing their parameters and introducing trainable adapters to effectively integrate two input modalities, i.e., human poses and video frames, to learn a pose-focused spatiotemporal representation of human actions. We design two novel core modules, i.e., Pose Temporal Attention and Pose-Video Spatial Attention, to facilitate interaction between body joint locations and uniform video tokens, enriching each modality with contextualized information from the other. We evaluate PGVT model on four action recognition datasets: Diving48, Gym99, and Gym288 for fine-grained action recognition, and Kinetics400 for coarse-grained action recognition. Our model achieves new SOTA performance on the three fine-grained human action recognition datasets and comparable performance on Kinetics400 with a small number of tunable parameters compared with SOTA methods. The PGVT model exploits effective multi-modality learning by explicitly modeling human body joints and leveraging their contextualized interactions with video clips.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_PGVT_Pose-Guided_Video_Transformer_for_Fine-Grained_Action_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_PGVT_Pose-Guided_Video_Transformer_for_Fine-Grained_Action_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484026/,"['Training', 'Adaptation models', 'Visualization', 'Computer vision', 'Image recognition', 'Costs', 'Semantics']","['Action Recognition', 'Fine-grained Action', 'Fine-grained Action Recognition', 'Human Activities', 'Performance Comparison', 'Video Frames', 'Spatial Attention', 'Input Modalities', 'Joint Learning', 'Body Joints', 'Human Activity Recognition', 'Temporal Attention', 'Video Modeling', 'Spatiotemporal Representation', 'SOTA Methods', 'Action Recognition Datasets', 'Semantic', 'Computational Cost', 'Temporal Dimension', 'Temporal Dynamics', 'Vision Transformer', 'Pose Information', 'Video Action Recognition', 'Temporal Modulation', 'Number Of Joints', '2D Pose', 'Self-attention Layer', 'Video Features', 'Baseline Architecture', 'Transformer Model']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",,"Based on recent advancements in transformer-based video models and multi-modal joint learning, we propose a novel model, named Pose-Guided Video Transformer (PGVT), to incorporate sparse high-level body joints locations and dense low-level visual pixels for effective learning and accurate recognition of human actions. PGVT leverages the pre-trained image models by freezing their parameters and introducing trainable adapters to effectively integrate two input modalities, i.e., human poses and video frames, to learn a pose-focused spatiotemporal representation of human actions. We design two novel core modules, i.e., Pose Temporal Attention and Pose-Video Spatial Attention, to facilitate interaction between body joint locations and uniform video tokens, enriching each modality with contextualized information from the other. We evaluate PGVT model on four action recognition datasets: Diving48, Gym99, and Gym288 for fine-grained action recognition, and Kinetics400 for coarse-grained action recognition. Our model achieves new SOTA performance on the three fine-grained human action recognition datasets and comparable performance on Kinetics400 with a small number of tunable parameters compared with SOTA methods. Various ablation studies are performed which verify the benefits of our new designs."
PHG-Net: Persistent Homology Guided Medical Image Classification,"Yaopeng Peng, Hongxiao Wang, Milan Sonka, Danny Z. Chen",University of Notre Dame; University of Iowa,100.0,USA,0.0,,"Modern deep neural networks have achieved great successes in medical image analysis. However, the features captured by convolutional neural networks (CNNs) or Transformers tend to be optimized for pixel intensities and neglect key anatomical structures such as connected components and loops. In this paper, we propose a persistent homology guided approach (PHG-Net) that explores topological features of objects for medical image classification. For an input image, we first compute its cubical persistence diagram and extract topological features into a vector representation using a small neural network (called the PH module). The extracted topological features are then incorporated into the feature map generated by CNN or Transformer for feature fusion. The PH module is lightweight and capable of integrating topological features into any CNN or Transformer architectures in an end-to-end fashion. We evaluate our PHG-Net on three public datasets and demonstrate its considerable improvements on the target classification tasks over state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Peng_PHG-Net_Persistent_Homology_Guided_Medical_Image_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Peng_PHG-Net_Persistent_Homology_Guided_Medical_Image_Classification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484262/,"['Costs', 'Network topology', 'Feature extraction', 'Transformers', 'Vectors', 'Topology', 'Convolutional neural networks']","['Medical Imaging', 'Medical Classification', 'Medical Image Classification', 'Neural Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Feature Maps', 'Topological Features', 'Vector Representation', 'Medical Image Analysis', 'Contralateral', 'Prostate Cancer', 'Deep Learning Models', 'Multiple Scales', 'Multilayer Perceptron', 'Max-pooling', 'Topological Structure', 'Average Pooling', 'Convolutional Neural Network Layers', 'Digital Pathology', 'Convolutional Neural Network Features', 'Prostate Cancer Dataset', 'Topological Data Analysis', 'Vision Transformer', 'Homology Groups', 'Transformer Block', 'Floating-point Operations', 'Convolutional Neural Networks Backbone', 'Topological Information', 'Encoder Output']","['Applications', 'Biomedical / healthcare / medicine']",,"Modern deep neural networks have achieved great successes in medical image analysis. However, the features captured by convolutional neural networks (CNNs) or Transformers tend to be optimized for pixel intensities and neglect key anatomical structures such as connected components and loops. In this paper, we propose a persistent homology guided approach (PHG-Net) that explores topological features of objects for medical image classification. For an input image, we first compute its cubical persistence diagram and extract topological features into a vector representation using a small neural network (called the PH module). The extracted topological features are then incorporated into the feature map generated by CNN or Transformer for feature fusion. The PH module is lightweight and capable of integrating topological features into any CNN or Transformer architectures in an end-to-end fashion. We evaluate our PHG-Net on three public datasets and demonstrate its considerable improvements on the target classification tasks over state-of-the-art methods."
PIDiffu: Pixel-Aligned Diffusion Model for High-Fidelity Clothed Human Reconstruction,"Jungeun Lee, Sanghun Kim, Hansol Lee, Tserendorj Adiya, Hwasup Lim",Korea Institute of Science and Technology (KIST),100.0,South Korea,0.0,,"This paper presents the Pixel-aligned Diffusion Model (PIDiffu), a new framework for reconstructing high-fidelity clothed 3D human models from a single image. While existing PIFu variants have made significant advances using more complicated 2D and 3D feature extractions, these methods still suffer from floating artifacts and body part duplication due to their reliance on point-wise occupancy field estimations. PIDiffu employs a diffusion-based strategy for line-wise estimation along the ray direction, conditioned by pixel-aligned features with a guided attention. This approach improves the local details and structural accuracy of the reconstructed body shape and is robust to unfamiliar and complex image features. Moreover, PIDiffu can be easily integrated with existing PIFu-based methods to leverage their advantages. The paper demonstrates that PIDiffu outperforms state-of-the-art methods that do not rely on parametric 3D body models. Especially, our method is superior in handling 'in-the-wild' images, such as those with complex patterned clothes unseen in the training data.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PIDiffu_Pixel-Aligned_Diffusion_Model_for_High-Fidelity_Clothed_Human_Reconstruction_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483570/,"['Geometry', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Estimation', 'Training data']","['Diffusion Model', 'Human Reconstruction', 'Clothed Human', 'Image Features', 'Single Image', 'Local Details', '3D Human Model', 'Human 3D', 'Neural Network', 'Quantitative Results', 'Input Image', 'Attention Mechanism', 'Multilayer Perceptron', 'Reversible Process', 'Depth Map', '3D Scanning', '3D Mesh', 'Implicit Function', '3D Geometry', '3D Distribution', '3D U-Net', 'Training Distribution', 'Occupancy Estimates', 'Normal Map', '3D Convolution', 'Distribution Of Training Data', 'Ambiguity Issue', '3D Convolutional Network']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', '3D computer vision']",1,"This paper presents the Pixel-aligned Diffusion Model (PIDiffu), a new framework for reconstructing high-fidelity clothed 3D human models from a single image. While existing PIFu variants have made significant advances using more complicated 2D and 3D feature extractions, these methods still suffer from floating artifacts and body part du-plication due to their reliance on point-wise occupancy field estimations. PIDiffu employs a diffusion-based strategy for line-wise estimation along the ray direction, conditioned by pixel-aligned features with a guided attention. This approach improves the local details and structural accuracy of the reconstructed body shape and is robust to unfamiliar and complex image features. Moreover, PIDiffu can be easily integrated with existing PIFu-based methods to leverage their advantages. The paper demonstrates that PIDiffu outperforms state-of-the-art methods that do not rely on parametric 3D body models. Especially, our method is superior in handling ’in-the-wild’ images, such as those with complex patterned clothes unseen in the training data."
PMI Sampler: Patch Similarity Guided Frame Selection for Aerial Action Recognition,"Ruiqi Xian, Xijun Wang, Divya Kothandaraman, Dinesh Manocha",University of Maryland - College Park,100.0,USA,0.0,,"We present a new algorithm for the selection of informative frames in video action recognition. Our approach is designed for aerial videos captured using a moving camera where human actors occupy a small spatial resolution of video frames. Our algorithm utilizes the motion bias within aerial videos, which enables the selection of motion-salient frames. We introduce the concept of patch mutual information (PMI) score to quantify the motion bias between adjacent frames, by measuring the similarity of patches. We use this score to assess the amount of discriminative motion information contained in one frame relative to another. We present an adaptive frame selection strategy using shifted leaky ReLu and cumulative distribution function, which ensures that the sampled frames comprehensively cover all the essential segments with high motion salience. Our approach can be integrated with any action recognition model to enhance its accuracy. In practice, our method achieves a relative improvement of 2.2 - 13.8% in top-1 accuracy on UAV-Human, 6.8% on NEC Drone, and 9.0% on Diving48 datasets. The code is available at https://github.com/Ricky- Xian/PMI-Sampler.",https://openaccess.thecvf.com/content/WACV2024/html/Xian_PMI_Sampler_Patch_Similarity_Guided_Frame_Selection_for_Aerial_Action_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xian_PMI_Sampler_Patch_Similarity_Guided_Frame_Selection_for_Aerial_Action_WACV_2024_paper.pdf,,https://github.com/Ricky-Xian/PMI-Sampler,2304.06866,main,Poster,https://ieeexplore.ieee.org/document/10483784/,"['Computer vision', 'Adaptation models', 'Codes', 'Motion segmentation', 'Cameras', 'Motion measurement', 'Spatial resolution']","['Action Recognition', 'Frame Selection', 'Patch Similarity', 'Cumulative Distribution', 'Mutual Information', 'Video Frames', 'Recognition Model', 'Adaptive Selection', 'Video Capture', 'Motion Information', 'Adjacent Frames', 'ReLU Function', 'Top-1 Accuracy', 'Frame Information', 'Small Resolution', 'Mutual Information Score', 'Video Action Recognition', 'Action Recognition Model', 'Convolutional Neural Network', 'Light Conditions', 'Unmanned Aerial Vehicles', 'Mutual Information Estimation', 'Deep Learning-based Methods', 'Joint Entropy', 'Learning-based Methods', 'High Altitude', 'Oblique Angle', 'Video Dataset', 'Patch Size', 'Similarity Measure']","['Algorithms', 'Video recognition and understanding', 'Applications', 'Robotics']",4,"We present a new algorithm for the selection of informative frames in video action recognition. Our approach is designed for aerial videos captured using a moving camera where human actors occupy a small spatial resolution of video frames. Our algorithm utilizes the motion bias within aerial videos, which enables the selection of motion-salient frames. We introduce the concept of patch mutual information (PMI) score to quantify the motion bias between adjacent frames, by measuring the similarity of patches. We use this score to assess the amount of discriminative motion information contained in one frame relative to another. We present an adaptive frame selection strategy using shifted leaky ReLu and cumulative distribution function, which ensures that the sampled frames comprehensively cover all the essential segments with high motion salience. Our approach can be integrated with any action recognition model to enhance its accuracy. In practice, our method achieves a relative improvement of 2.2 - 13.8% in top-1 accuracy on UAV-Human, 6.8% on NEC Drone, and 9.0% on Diving48 datasets. The code is available at https://github.com/Ricky-Xian/PMI-Sampler."
PMVC: Promoting Multi-View Consistency for 3D Scene Reconstruction,"Chushan Zhang, Jinguang Tong, Tao Jun Lin, Chuong Nguyen, Hongdong Li","Data61, CSIRO; The Australian National University, Data61, CSIRO; The Australian National University",100.0,Australia,0.0,,"Reconstructing the geometry of a 3D scene from its multi-view 2D observations has been a central task of 3D computer vision. Recent methods based on neural rendering that use implicit shape representations, such as the neural Signed Distance Function(SDF), have shown impressive performance. However, they fall short in recovering fine details in the scene, especially when employing an MLP as the interpolation function for the SDF representation. Per-frame image normal or depth-map prediction have been utilized to tackle this issue, but these learning-based depth/normal predictions are based on a single image frame only, hence overlooking the underlying multiview consistency of the scene, leading to inconsistent erroneous 3D reconstruction. To mitigate this problem, we propose to leverage multi-view deep features computed on the images. In addition, we employ an adaptive sampling strategy that assesses the fidelity of the multi-view image consistency. Our approach outperforms current state-of-the-art methods, delivering an accurate and robust scene representation with particularly enhanced details in those thin or textureless regions. The effectiveness of our proposed approach is evaluated by extensive experiments conducted on the ScanNet and Replica datasets, showing superior performance than the current state-of-the-art.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_PMVC_Promoting_Multi-View_Consistency_for_3D_Scene_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_PMVC_Promoting_Multi-View_Consistency_for_3D_Scene_Reconstruction_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484473/,"['Geometry', 'Computer vision', 'Interpolation', 'Three-dimensional displays', 'Shape', 'Predictive models', 'Multilayer perceptrons']","['3D Reconstruction', '3D Scene', 'Scene Reconstruction', 'Multi-view Consistency', 'Multilayer Perceptron', 'Adaptive Sampling', 'Scene Representation', 'Signed Distance Function', 'Scene Details', 'Single-frame Images', 'Surface Characteristics', 'Point Cloud', 'Bounding Box', 'Depth Map', 'Monocular', '3D Coordinates', 'Reconstruction Results', 'Surface Points', 'L1 Loss', 'Reprojection', 'Normal Map', 'Implicit Representation', 'Scene Geometry', 'Volume Rendering', 'Camera Pose', 'Structure From Motion', 'Multi-view Stereo', 'Surface Normals', 'Adjacent Frames', 'Scene Understanding']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Virtual / augmented reality']",1,"Reconstructing the geometry of a 3D scene from its multi-view 2D observations has been a central task of 3D computer vision. Recent methods based on neural rendering that use implicit shape representations such as the neural Signed Distance Function (SDF), have shown impressive performance. However, they fall short in recovering fine details in the scene, especially when employing a multilayer perceptron (MLP) as the interpolation function for the SDF representation. Per-frame image normal or depth-map prediction have been utilized to tackle this issue, but these learning-based depth/normal predictions are based on a single image frame only, hence overlooking the underlying multiview consistency of the scene, leading to inconsistent erroneous 3D reconstruction. To mitigate this problem, we propose to leverage multi-view deep features computed on the images. In addition, we employ an adaptive sampling strategy that assesses the fidelity of the multi-view image consistency. Our approach outperforms current state-of-the-art methods, delivering an accurate and robust scene representation with particularly enhanced details. The effectiveness of our proposed approach is evaluated by extensive experiments conducted on the ScanNet and Replica datasets, showing superior performance than the current state-of-the-art."
POISE: Pose Guided Human Silhouette Extraction Under Occlusions,"Arindam Dutta, Rohit Lal, Dripta S. Raychaudhuri, Calvin-Khang Ta, Amit K. Roy-Chowdhury","University of California, Riverside, AWS AI Labs; University of California, Riverside",100.0,USA,0.0,,"Human silhouette extraction is a fundamental task in computer vision with applications in various downstream tasks. However, occlusions pose a significant challenge, leading to distorted silhouettes. To address this challenge, we introduce POISE : Pose Guided Human Silhouette Extraction under Occlusions, a fusion framework that enhances accuracy and robustness in human silhouette prediction. By combining initial silhouette estimates from a segmentation model with human joint predictions from a 2D pose estimation model, POISE leverages the complementary strengths of both approaches, effectively integrating precise body shape information and spatial information to tackle occlusions. Furthermore, the unsupervised nature of POISE eliminates the need for costly annotations, making it scalable and practical. Extensive experimental results demonstrate its superiority in improving silhouette extraction under occlusions, with promising results in downstream tasks such as gait recognition.",https://openaccess.thecvf.com/content/WACV2024/html/Dutta_POISE_Pose_Guided_Human_Silhouette_Extraction_Under_Occlusions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dutta_POISE_Pose_Guided_Human_Silhouette_Extraction_Under_Occlusions_WACV_2024_paper.pdf,,https://github.com/take2rohit/poise,2311.05077,main,Poster,https://ieeexplore.ieee.org/document/10483905/,"['Computer vision', 'Codes', 'Shape', 'Annotations', 'Pose estimation', 'Self-supervised learning', 'Predictive models']","['Human Silhouette', 'Silhouette Extraction', 'Computer Vision', 'Body Shape', 'Segmentation Model', 'Pose Estimation', 'Fundamental Task', 'Fusion Framework', 'Need For Annotation', 'Additional Details', 'Image Dataset', 'Qualitative Results', 'Adam Optimizer', 'Adaptive Algorithm', 'Teacher Model', 'Large-scale Datasets', 'Semantic Segmentation', 'Domain Adaptation', 'Self-supervised Learning', 'Student Model', 'Human Pose Estimation', 'Learning Rate Of 1e', 'Atmospheric Turbulence', 'Video Dataset', 'Presence Of Occlusion', 'Target Dataset', 'Source Dataset', 'Human Pose', 'Pose Prediction', '2D Keypoints']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"Human silhouette extraction is a fundamental task in computer vision with applications in various downstream tasks. However, occlusions pose a significant challenge, leading to incomplete and distorted silhouettes. To address this challenge, we introduce POISE: Pose Guided Human Silhouette Extraction under Occlusions, a novel self-supervised fusion framework that enhances accuracy and robustness in human silhouette prediction. By combining initial silhouette estimates from a segmentation model with human joint predictions from a 2D pose estimation model, POISE leverages the complementary strengths of both approaches, effectively integrating precise body shape information and spatial information to tackle occlusions. Furthermore, the self-supervised nature of POISE eliminates the need for costly annotations, making it scalable and practical. Extensive experimental results demonstrate its superiority in improving silhouette extraction under occlusions, with promising results in downstream tasks such as gait recognition. The code for our method is available https://github.com/take2rohit/poise."
"POP-VQA - Privacy Preserving, On-Device, Personalized Visual Question Answering","Pragya Paramita Sahu, Abhishek Raut, Jagdish Singh Samant, Mahesh Gorijala, Vignesh Lakshminarayanan, Pinaki Bhaskar","Samsung Research Institute, Bangalore, India",100.0,India,0.0,,"The next generation of device smartness needs to go beyond being able to understand basic user commands. As our systems become more efficient, they need to be taught to understand user interactions and intents from all possible input modalities. This is where the recent advent of large scale multi-modal models can form the foundation for next-gen technologies. However, the true power of such interactive systems can only be realized with privacy conserving personalization. In this paper, we propose an on-device visual question answering system that generates personalized answers using on-device user knowledge graph. These systems have the potential to serve as a fundamental groundwork for the development of genuinely intelligent and tailored assistants, targeted specifically to the needs and preferences of each individual. We validate our model performance on both in-realm, public datasets and personal user data. Our results show consistent performance increase across both tasks, with an absolute improvement of  36% with KVQA data-set on 1-hop inferences and  6% improvement on user personal data. We also conduct and showcase user-study results to validate our hypothesis of the need and relevance of proposed system.",https://openaccess.thecvf.com/content/WACV2024/html/Sahu_POP-VQA_-_Privacy_Preserving_On-Device_Personalized_Visual_Question_Answering_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sahu_POP-VQA_-_Privacy_Preserving_On-Device_Personalized_Visual_Question_Answering_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Painterly Image Harmonization via Adversarial Residual Learning,"Xudong Wang, Li Niu, Junyan Cao, Yan Hong, Liqing Zhang","Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University",100.0,China,0.0,,"Image compositing plays a vital role in photo editing. After inserting a foreground object into another background image, the composite image may look unnatural and inharmonious. When the foreground is photorealistic and the background is an artistic painting, painterly image harmonization aims to transfer the style of background painting to the foreground object, which is a challenging task due to the large domain gap between foreground and background. In this work, we employ adversarial learning to bridge the domain gap between foreground feature map and background feature map. Specifically, we design a dual-encoder generator, in which the residual encoder produces the residual features added to the foreground feature map from main encoder. Then, a pixel-wise discriminator plays against the generator, encouraging the refined foreground feature map to be indistinguishable from background feature map. Extensive experiments demonstrate that our method could achieve more harmonious and visually appealing results than previous methods.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Painterly_Image_Harmonization_via_Adversarial_Residual_Learning_WACV_2024_paper.pdf,,,2311.08646,main,Poster,https://ieeexplore.ieee.org/document/10483968/,"['Bridges', 'Computer vision', 'Generators', 'Adversarial machine learning', 'Task analysis', 'Painting']","['Image Harmonization', 'Feature Maps', 'Generative Adversarial Networks', 'Background Image', 'Composite Image', 'Residual Feature', 'Domain Gap', 'Foreground Objects', 'Convolutional Layers', 'User Study', 'Large-scale Datasets', 'Group Method', 'Baseline Group', 'Residual Block', 'Photographic Images', 'Loss Of Content', 'Output Image', 'Batch Normalization Layer', 'Composite Mapping', 'Background Pixels', 'COCO Dataset', 'Foreground Regions', 'Style Transfer', 'Foreground Pixels', 'Iterative Optimization Process', 'Optimization-based Methods', 'Style Image', 'Optimization-based Approach', 'Deep Learning', 'Image Pairs']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc']",1,"Image compositing plays a vital role in photo editing. After inserting a foreground object into another background image, the composite image may look unnatural and inharmonious. When the foreground is photorealistic and the background is an artistic painting, painterly image harmonization aims to transfer the style of background painting to the foreground object, which is a challenging task due to the large domain gap between foreground and background. In this work, we employ adversarial learning to bridge the domain gap between foreground feature map and background feature map. Specifically, we design a dual-encoder generator, in which the residual encoder produces the residual features added to the foreground feature map from main encoder. Then, a pixel-wise discriminator plays against the generator, encouraging the refined foreground feature map to be indistinguishable from background feature map. Extensive experiments demonstrate that our method could achieve more harmonious and visually appealing results than previous methods."
Panelformer: Sewing Pattern Reconstruction From 2D Garment Images,"Cheng-Hsiu Chen, Jheng-Wei Su, Min-Chun Hu, Chih-Yuan Yao, Hung-Kuo Chu",National Taiwan University of Science and Technology; National Tsing Hua University,100.0,Taiwan,0.0,,"In this paper, we present a novel approach for reconstructing garment sewing patterns from 2D garment images. Our method addresses the challenge of handling occlusion in 2D images by leveraging the symmetric and correlated nature of garment panels. We introduce a transformer-based deep neural network called Panelformer that learns the parametric space of garment sewing patterns. The network comprises two components: the panel transformer and the stitch predictor. The panel transformer estimates the parametric panel shapes, including the occluded panels, by learning from the visible ones. The stitch predictor determines the stitching information among the predicted panels, enabling the reconstruction of the complete garment. To mitigate the overfitting problem caused by strong panel correlations, we propose two tailor-made data augmentation techniques: panel masking and garment mixing. These techniques generate a wider variety of panel combinations, enhancing the model's robustness and generalization capability. We evaluate the effectiveness of Panelformer using a synthetic dataset with diverse garment types. The experimental results demonstrate that our method outperforms competing baselines and achieves comparable performance to NeuralTailor, which operates on 3D point cloud data. This validates the efficacy of our approach in the context of garment sewing pattern reconstruction. By utilizing 2D images as input, our method expands the potential applications of garment modeling and offers easy accessibility to end users. Our code is available online.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Panelformer_Sewing_Pattern_Reconstruction_From_2D_Garment_Images_WACV_2024_paper.pdf,,https://ericsujw.github.io/Panelformer/,,main,Poster,https://ieeexplore.ieee.org/document/10483920/,"['Point cloud compression', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Clothing', 'Transformers', 'Data augmentation']","['Garment Images', 'Sewing Pattern', 'Data Augmentation', '2D Images', 'Point Cloud', '3D Data', 'Augmentation Techniques', '3D Point Cloud', 'Data Augmentation Techniques', '3D Point Cloud Data', 'Training Set', 'Deep Learning', 'Learning Rate', 'Input Image', 'Feature Maps', 'Object Detection', 'Attention Mechanism', 'Bounding Box', 'Viewing Angle', 'Implicit Function', 'Edge Points', 'Positional Encoding', 'Pair Of Edges', 'Transformer Decoder', 'Fine-tuning Step', 'Computer Graphics', 'Input Point Cloud']","['Algorithms', 'Image recognition and understanding', 'Algorithms', '3D computer vision']",1,"In this paper, we present a novel approach for reconstructing garment sewing patterns from 2D garment images. Our method addresses the challenge of handling occlusion in 2D images by leveraging the symmetric and correlated nature of garment panels. We introduce a transformer-based deep neural network called Panelformer that learns the parametric space of garment sewing patterns. The network comprises two components: the panel transformer and the stitch predictor. The panel transformer estimates the parametric panel shapes, including the occluded panels, by learning from the visible ones. The stitch predictor determines the stitching information among the predicted panels, enabling the reconstruction of the complete garment. To mitigate the overfitting problem caused by strong panel correlations, we propose two tailor-made data augmentation techniques: panel masking and garment mixing. These techniques generate a wider variety of panel combinations, enhancing the model’s robustness and generalization capability. We evaluate the effectiveness of Panelformer using a synthetic dataset with diverse garment types. The experimental results demonstrate that our method outperforms competing baselines and achieves comparable performance to NeuralTailor, which operates on 3D point cloud data. This validates the efficacy of our approach in the context of garment sewing pattern reconstruction. By utilizing 2D images as input, our method expands the potential applications of garment modeling and offers easy accessibility to end users. Our code is available online
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Partial Binarization of Neural Networks for Budget-Aware Efficient Learning,"Udbhav Bamba, Neeraj Anand, Saksham Aggarwal, Dilip K. Prasad, Deepak K. Gupta","UiT The Arctic University of Norway, Norway; Nyun AI, India; Transmute AI Lab, India",66.66666666666666,"India, Norway",33.33333333333334,India,"Binarization is a powerful compression technique for neural networks, significantly reducing FLOPs, but often results in a significant drop in model performance. To address this issue, partial binarization techniques have been developed, but a systematic approach to mixing binary and full-precision parameters in a single network is still lacking. In this paper, we propose a controlled approach to partial binarization, creating a budgeted binary neural network (B2NN) with our MixBin strategy. This method optimizes the mixing of binary and full-precision components, allowing for explicit selection of the fraction of the network to remain binary. Our experiments show that B2NNs created using MixBin outperform those from random or iterative searches and state-of-the-art layer selection methods by up to 3% on the ImageNet-1K dataset. We also show that B2NNs outperform the structured pruning baseline by approximately 23% at the extreme FLOP budget of 15%, and perform well in object tracking, with up to a 12.4% relative improvement over other baselines. Additionally, we demonstrate that B2NNs developed by MixBin can be transferred across datasets, with some cases showing improved performance over directly applying MixBin on the downstream data.",https://openaccess.thecvf.com/content/WACV2024/html/Bamba_Partial_Binarization_of_Neural_Networks_for_Budget-Aware_Efficient_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bamba_Partial_Binarization_of_Neural_Networks_for_Budget-Aware_Efficient_Learning_WACV_2024_paper.pdf,,https://github.com/transmuteAI/trailmet,2211.06739,main,Poster,https://ieeexplore.ieee.org/document/10483283/,"['Computer vision', 'Systematics', 'Neural networks', 'Object tracking', 'Iterative methods']","['Neural Network', 'Model Performance', 'Object Tracking', 'Floating-point Operations', 'Iterative Search', 'Binary Network', 'Binary Components', 'Convolutional Neural Network', 'Performance Of Method', 'Selection Strategy', 'Multiple Layers', 'Random Selection', 'Part Of Network', 'Network Layer', 'Network Performance', 'Convolution Operation', 'Boolean Operators', 'Rest Of The Network', 'Set Of Layers', 'Gradient Loss', 'Iterative Selection', 'Ith Layer', 'Greedy Selection', 'Compression Level']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Binarization is a powerful compression technique for neural networks, significantly reducing FLOPs, but often results in a significant drop in model performance. To address this issue, partial binarization techniques have been developed, but a systematic approach to mixing binary and full-precision parameters in a single network is still lacking. In this paper, we propose a controlled approach to partial binarization, creating a budgeted binary neural network (B2NN) with our MixBin strategy. This method optimizes the mixing of binary and full-precision components, allowing for explicit selection of the fraction of the network to remain binary. Our experiments show that B2NNs created using MixBin outperform those from random or iterative searches and state-of-the-art layer selection methods by up to 3% on the ImageNet-1K dataset. We also show that B2NNs outperform the structured pruning baseline by approximately 23% at the extreme FLOP budget of 15%, and perform well in object tracking, with up to a 12.4% relative improvement over other baselines. Additionally, we demonstrate that B2NNs developed by MixBin can be transferred across datasets, with some cases showing improved performance over directly applying MixBin on the downstream data.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance Fields,"Jad Abou-Chakra, Feras Dayoub, Niko Sünderhauf",University of Adelaide; Queensland University of Technology,100.0,Australia,0.0,,"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline methods with an emphasis on visual fidelity, our paper addresses the online use case that prioritises real-time adaptability. We present ParticleNeRF, a new approach that dynamically adapts to changes in the scene geometry by learning an up-to-date representation online, every 200ms. ParticleNeRF achieves this using a novel particle-based parametric encoding. We couple features to particles in space and backpropagate the photometric reconstruction loss into the particles' position gradients, which are then interpreted as velocity vectors. Governed by a lightweight physics system to handle collisions, this lets the features move freely with the changing scene geometry. We demonstrate ParticleNeRF on various dynamic scenes containing translating, rotating, articulated, and deformable objects. ParticleNeRF is the first online dynamic NeRF and achieves fast adaptability with better visual fidelity than brute-force online InstantNGP and other baseline approaches on dynamic scenes with online constraints.",https://openaccess.thecvf.com/content/WACV2024/html/Abou-Chakra_ParticleNeRF_A_Particle-Based_Encoding_for_Online_Neural_Radiance_Fields_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Abou-Chakra_ParticleNeRF_A_Particle-Based_Encoding_for_Online_Neural_Radiance_Fields_WACV_2024_paper.pdf,https://sites.google.com/view/particlenerf,,,main,Poster,https://ieeexplore.ieee.org/document/10483733/,"['Geometry', 'Visualization', 'Computer vision', 'Encoding', 'Vectors', 'Real-time systems', 'Physics']","['Neural Radiance Fields', 'Physical System', 'Velocity Vector', 'Dynamic Scenes', 'Deformable Objects', 'Scene Geometry', 'Unit Vector', 'Multilayer Perceptron', 'Radial Basis Function', 'Latent Space', 'Incremental Learning', 'Training Step', 'Particle Position', 'Training Iterations', 'Reconstruction Quality', 'Nearest Neighbor Search', 'Memory Characteristics', 'Particles In Region', 'Static Scenes', 'Dynamic Datasets', 'End Of Frame', 'Unit Cube', 'Query Point', 'Point Xi']","['Applications', 'Robotics', 'Algorithms', '3D computer vision']",4,"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline methods with an emphasis on visual fidelity, our paper addresses the online use case that prioritises real-time adaptability. We present ParticleNeRF, a new approach that dynamically adapts to changes in the scene geometry by learning an up-to-date representation online, every 200 ms. ParticleNeRF achieves this using a novel particle-based parametric encoding. We couple features to particles in space and backpropagate the photometric reconstruction loss into the particles’ position gradients, which are then interpreted as velocity vectors. Governed by a lightweight physics system to handle collisions, this lets the features move freely with the changing scene geometry. We demonstrate ParticleNeRF on various dynamic scenes containing translating, rotating, articulated, and deformable objects. ParticleNeRF is the first online dynamic NeRF and achieves fast adaptability with better visual fidelity than brute-force online InstantNGP and other base-line approaches on dynamic scenes with online constraints. Videos of our system can be found at the anonymous project website https://sites.google.com/view/particlenerf."
Patch-Based Selection and Refinement for Early Object Detection,"Tianyi Zhang, Kishore Kasichainula, Yaoxin Zhuo, Baoxin Li, Jae-Sun Seo, Yu Cao",Arizona State University; Cornell Tech; University of Minnesota,100.0,USA,0.0,,"Early object detection (OD) is a crucial task for the safety of many dynamic systems. Current OD algorithms have limited success for small objects at a long distance. To improve the accuracy and efficiency of such a task, we propose a novel set of algorithms that divide the image into patches, select patches with objects at various scales, elaborate the details of a small object, and detect it as early as possible. Our approach is built upon a transformer-based network and integrates the diffusion model to improve the detection accuracy. As demonstrated on BDD100K, our algorithms enhance the mAP for small objects from 1.03 to 8.93, and reduce the data volume in computation by more than 77%.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Patch-Based_Selection_and_Refinement_for_Early_Object_Detection_WACV_2024_paper.pdf,,,2311.02274,main,Poster,https://ieeexplore.ieee.org/document/10484002/,"['Computer vision', 'Heuristic algorithms', 'Computational modeling', 'Object detection', 'Transformers', 'Safety', 'Task analysis']","['Early Detection', 'Object Detection', 'Diffusion Model', 'Small Objects', 'Sample Processing', 'Hierarchical Structure', 'Real-world Applications', 'Super-resolution', 'Multilayer Perceptron', 'Reversible Process', 'Generative Adversarial Networks', 'Image Generation', 'Entire Image', 'Peak Signal-to-noise Ratio', 'Bilinear Interpolation', 'Faster R-CNN', 'Noisy Images', 'Square Pixels', 'Forward Process', 'Corrupted Data', 'Positive Patch', 'Fréchet Inception Distance', 'Patch Selection', 'Transformer Layers', 'Computer Vision', 'Image Patches', 'Convolutional Layers', 'Subsequent Task']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",1,"Early object detection (OD) is a crucial task for the safety of many dynamic systems. Current OD algorithms have limited success for small objects at a long distance. To improve the accuracy and efficiency of such a task, we propose a novel set of algorithms that divide the image into patches, select patches with objects at various scales, elaborate the details of a small object, and detect it as early as possible. Our approach is built upon a transformer-based network and integrates the diffusion model to improve the detection accuracy. As demonstrated on BDD100K, our algorithms enhance the mAP for small objects from 1.03 to 8.93, and reduce the data volume in computation by more than 77%."
PatchRefineNet: Improving Binary Segmentation by Incorporating Signals From Optimal Patch-Wise Binarization,"Savinay Nagendra, Daniel Kifer","Department of Computer Science, The Pennsylvania State University, University Park",100.0,USA,0.0,,"The purpose of binary segmentation models is to determine which pixels belong to an object of interest (e.g., which pixels in an image are part of roads). The models assign a logit score (i.e., probability) to each pixel and these are converted into predictions by thresholding (i.e., each pixel with logit score >= t is predicted to be part of a road). However, a common phenomenon in current and former state-of-the-art segmentation models is spatial bias -- in some patches, the logit scores are consistently biased upwards and in others they are consistently biased downwards. These biases cause false positives and false negatives in the final predictions. In this paper, we propose PatchRefineNet (PRN), a small network that sits on top of a base segmentation model and learns to correct its patch-specific biases. Across a wide variety of base models, PRN consistently helps them improve mIoU by 2-3%. One of the key ideas behind PRN is the addition of a novel supervision signal during training. Given the logit scores produced by the base segmentation model, each pixel is given a pseudo-label that is obtained by optimally thresholding the logit scores in each image patch. Incorporating these pseudo-labels into the loss function of PRN helps correct systematic biases and reduce false positives/negatives. Although we mainly focus on binary segmentation, we also show how PRN can be extended to saliency detection and few-shot segmentation. We also discuss how the ideas can be extended to multi-class segmentation. Source code is available at https://github.com/savinay95n/PatchRefineNet.",https://openaccess.thecvf.com/content/WACV2024/html/Nagendra_PatchRefineNet_Improving_Binary_Segmentation_by_Incorporating_Signals_From_Optimal_Patch-Wise_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nagendra_PatchRefineNet_Improving_Binary_Segmentation_by_Incorporating_Signals_From_Optimal_Patch-Wise_WACV_2024_paper.pdf,,https://github.com/savinay95n/PatchRefineNet,2211.06560,main,Poster,https://ieeexplore.ieee.org/document/10484405/,"['Training', 'Image segmentation', 'Computer vision', 'Systematics', 'Roads', 'Computational modeling', 'Source coding']","['Binary Segmentation', 'Loss Function', 'False Negative', 'Final Prediction', 'Segmentation Model', 'Image Patches', 'Spatial Bias', 'Upward Bias', 'Saliency Detection', 'Part Of The Road', 'Semantic', 'Training Set', 'Training Data', 'Validation Data', 'Validation Set', 'Feature Maps', 'Deeper Layers', 'Training Images', 'Semantic Segmentation', 'Patch Size', 'Focal Loss', 'Skip Connections', 'Encoder Output', 'Conditional Random Field', 'Pixel Clusters', 'Pyramid Pooling', 'Post-processing Methods', 'Post-processing Techniques', 'Yellow Box', 'Binary Network']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",2,"The purpose of binary segmentation models is to determine which pixels belong to an object of interest (e.g., which pixels in an image are part of roads). The models assign a logit score (i.e., probability) to each pixel and these are converted into predictions by thresholding (i.e., each pixel with logit score ≥ τ is predicted to be part of a road). However, a common phenomenon in current and former state-of-the-art segmentation models is spatial bias – in some patches, the logit scores are consistently biased upwards and in others they are consistently biased downwards. These biases cause false positives and false negatives in the final predictions. In this paper, we propose PatchRefineNet (PRN), a small network that sits on top of a base segmentation model and learns to correct its patch-specific biases. Across a wide variety of base models, PRN consistently helps them improve mIoU by 2-3%. One of the key ideas behind PRN is the addition of a novel supervision signal during training. Given the logit scores produced by the base segmentation model, each pixel is given a pseudo-label that is obtained by optimally thresholding the logit scores in each image patch. Incorporating these pseudo-labels into the loss function of PRN helps correct systematic biases and reduce false positives/negatives. Although we mainly focus on binary segmentation, we also show how PRN can be extended to saliency detection and few-shot segmentation. We also discuss how the ideas can be extended to multiclass segmentation. Source code is available at https://github.com/savinay95n/PatchRefineNet."
PathLDM: Text Conditioned Latent Diffusion Model for Histopathology,"Srikar Yellapragada, Alexandros Graikos, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Dimitris Samaras",Stony Brook University,100.0,USA,0.0,,"To achieve high-quality results, diffusion models must be trained on large datasets. This can be notably prohibitive for models in specialized domains, such as computational pathology. Conditioning on labeled data is known to help in data-efficient model training. Therefore, histopathology reports, which are rich in valuable clinical information, are an ideal choice as guidance for a histopathology generative model. In this paper, we introduce PathLDM, the first text-conditioned Latent Diffusion Model tailored for generating high-quality histopathology images. Leveraging the rich contextual information provided by pathology text reports, our approach fuses image and textual data to enhance the generation process. By utilizing GPT's capabilities to distill and summarize complex text reports, we establish an effective conditioning mechanism. Through strategic conditioning and necessary architectural enhancements, we achieved a SoTA FID score of 7.64 for text-to-image generation on the TCGA-BRCA dataset, significantly outperforming the closest text-conditioned competitor with FID 30.1.",https://openaccess.thecvf.com/content/WACV2024/html/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yellapragada_PathLDM_Text_Conditioned_Latent_Diffusion_Model_for_Histopathology_WACV_2024_paper.pdf,,https://github.com/cvlab-stonybrook/PathLDM,2309.00748,main,Poster,https://ieeexplore.ieee.org/document/10483856/,"['Training', 'Visualization', 'Computer vision', 'Histopathology', 'Fuses', 'Computational modeling', 'Buildings']","['Diffusion Model', 'Latent Model', 'Histopathological Images', 'Fréchet Inception Distance', 'Valuable Clinical Information', 'High Tumor', 'Class Labels', 'ImageNet', 'Latent Space', 'Image Generation', 'Tumor-infiltrating Lymphocytes', 'Imaging Model', 'Low Tumor', 'Reconstruction Quality', 'Variational Autoencoder', 'Slide Images', 'Digital Pathology', 'Latent Vector', 'Pathological Images', 'Concise Summary', 'Text Summary', 'Text Encoder', 'High Tumor-infiltrating Lymphocytes', 'Downsampling Factor', 'Inception Distance', 'Language Model', 'Ablation', 'Caption Text', 'Optical Character Recognition', 'Satellite Imagery']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Biomedical / healthcare / medicine']",7,"To achieve high-quality results, diffusion models must be trained on large datasets. This can be notably prohibitive for models in specialized domains, such as computational pathology. Conditioning on labeled data is known to help in data-efficient model training. Therefore, histopathology reports, which are rich in valuable clinical information, are an ideal choice as guidance for a histopathology generative model. In this paper, we introduce PathLDM, the first text-conditioned Latent Diffusion Model tailored for generating high-quality histopathology images. Leveraging the rich contextual information provided by pathology text reports, our approach fuses image and textual data to enhance the generation process. By utilizing GPT’s capabilities to distill and summarize complex text reports, we establish an effective conditioning mechanism. Through strategic conditioning and necessary architectural enhancements, we achieved a SoTA FID score of 7.64 for text-to-image generation on the TCGA-BRCA dataset, significantly outperforming the closest text-conditioned competitor with FID 30.1. 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Permutation-Aware Activity Segmentation via Unsupervised Frame-To-Segment Alignment,"Quoc-Huy Tran, Ahmed Mehmood, Muhammad Ahmed, Muhammad Naufil, Anas Zafar, Andrey Konin, Zeeshan Zia","Retrocausal, Inc., Redmond, WA",0.0,,100.0,USA,"This paper presents an unsupervised transformer-based framework for temporal activity segmentation which leverages not only frame-level cues but also segment-level cues. This is in contrast with previous methods which often rely on frame-level information only. Our approach begins with a frame-level prediction module which estimates framewise action classes via a transformer encoder. The frame-level prediction module is trained in an unsupervised manner via temporal optimal transport. To exploit segment-level information, we utilize a segment-level prediction module and a frame-to-segment alignment module. The former includes a transformer decoder for estimating video transcripts, while the latter matches frame-level features with segment-level features, yielding permutation-aware segmentation results. Moreover, inspired by temporal optimal transport, we introduce simple-yet-effective pseudo labels for unsupervised training of the above modules. Our experiments on four public datasets, i.e., 50 Salads, YouTube Instructions, Breakfast, and Desktop Assembly show that our approach achieves comparable or better performance than previous methods in unsupervised activity segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Tran_Permutation-Aware_Activity_Segmentation_via_Unsupervised_Frame-To-Segment_Alignment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tran_Permutation-Aware_Activity_Segmentation_via_Unsupervised_Frame-To-Segment_Alignment_WACV_2024_paper.pdf,www.retrocausal.ai,,,main,Poster,,,,,,
Personalized Face Inpainting With Diffusion Models by Parallel Visual Attention,"Jianjin Xu, Saman Motamed, Praneetha Vaddamanu, Chen Henry Wu, Christian Haene, Jean-Charles Bazin, Fernando De la Torre",Carnegie Mellon University; Independent Researcher,50.0,USA,50.0,,"Face inpainting is important in various applications, such as photo restoration, image editing, and virtual reality. Despite the significant advances in face generative models, ensuring that a person's unique facial identity is maintained during the inpainting process is still an elusive goal. Current state-of-the-art techniques, exemplified by MyStyle, necessitate resource-intensive fine-tuning and a substantial number of images for each new identity. Furthermore, existing methods often fall short in accommodating user-specified semantic attributes, such as beard or expression. To improve inpainting results, and reduce the computational complexity during inference, this paper proposes the use of Parallel Visual Attention (PVA) in conjunction with diffusion models. Specifically, we insert parallel attention matrices to each cross-attention module in the denoising network, which attends to features extracted from reference images by an identity encoder. We train the added attention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for identity-preserving face inpainting. Experiments demonstrate that PVA attains unparalleled identity resemblance in both face inpainting and face inpainting with language guidance tasks, in comparison to various benchmarks, including MyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA ensures good identity preservation while offering effective language-controllability. Additionally, in contrast to Custom Diffusion, PVA requires just 40 fine-tuning steps for each new identity, which translates to a significant speed increase of over 20 times.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_Personalized_Face_Inpainting_With_Diffusion_Models_by_Parallel_Visual_Attention_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_Personalized_Face_Inpainting_With_Diffusion_Models_by_Parallel_Visual_Attention_WACV_2024_paper.pdf,,,2312.03556,main,Poster,https://ieeexplore.ieee.org/document/10484154/,"['Solid modeling', 'Visualization', 'Semantics', 'Noise reduction', 'Virtual reality', 'Benchmark testing', 'Feature extraction']","['Diffusion Model', 'Denoising', 'Number Of Images', 'Reference Image', 'Image Editing', 'Identity Preservation', 'Ablation', 'Fine-tuned', 'Image Quality', 'Gaussian Noise', 'Visual Features', 'Generative Adversarial Networks', 'Pre-trained Network', 'Textual Features', 'Variational Autoencoder', 'Identical Images', 'Ground Truth Image', 'Masked Images', 'Description Language', 'Single GPU', 'Fréchet Inception Distance', 'Language Control', 'Image Inpainting', 'StyleGAN', 'Inception Distance', 'Fine-tuning Process', 'Person Image']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc']",3,"Face inpainting is important in various applications, such as photo restoration, image editing, and virtual reality. Despite the significant advances in face generative models, ensuring that a person’s unique facial identity is maintained during the inpainting process is still an elusive goal. Current state-of-the-art techniques, exemplified by MyStyle, necessitate resource-intensive fine-tuning and a substantial number of images for each new identity. Furthermore, existing methods often fall short in accommodating user-specified semantic attributes, such as beard or expression.To improve inpainting results, and reduce the computational complexity during inference, this paper proposes the use of Parallel Visual Attention (PVA) in conjunction with diffusion models. Specifically, we insert parallel attention matrices to each cross-attention module in the denoising network, which attends to features extracted from reference images by an identity encoder. We train the added attention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for identity-preserving face inpainting. Experiments demonstrate that PVA attains unparalleled identity resemblance in both face inpainting and face inpainting with language guidance tasks, in comparison to various benchmarks, including MyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA ensures good identity preservation while offering effective language-controllability. Additionally, in contrast to Custom Diffusion, PVA requires just 40 fine-tuning steps for each new identity, which translates to a significant speed increase of over 20 times."
PhISH-Net: Physics Inspired System for High Resolution Underwater Image Enhancement,"Aditya Chandrasekar, Manogna Sreenivas, Soma Biswas","Indian Institute of Science, Bangalore",100.0,India,0.0,,"Underwater imaging presents numerous challenges due to refraction, light absorption, and scattering, resulting in color degradation, low contrast, and blurriness. Enhancing underwater images is crucial for high-level computer vision tasks, but existing methods either neglect the physics-based image formation process or require expensive computations. In this paper, we propose an effective framework that combines a physics-based Underwater Image Formation Model (UIFM) with a deep image enhancement approach based on the retinex model. Firstly, we remove backscatter by estimating attenuation coefficients using depth information. Then, we employ a retinex model-based deep image enhancement module to enhance the images. To ensure adherence to the UIFM, we introduce a novel Wideband Attenuation prior. The proposed PhISH-Net framework achieves real-time processing of high-resolution underwater images using a lightweight neural network and a bilateral-grid-based upsampler. Extensive experiments on two underwater image datasets demonstrate the superior performance of our method compared to state-of-the-art techniques. Additionally, qualitative evaluation on a cross-dataset scenario confirms its generalization capability. Our contributions lie in combining the physics-based UIFM with deep image enhancement methods, introducing the wideband attenuation prior, and achieving superior performance and efficiency.",https://openaccess.thecvf.com/content/WACV2024/html/Chandrasekar_PhISH-Net_Physics_Inspired_System_for_High_Resolution_Underwater_Image_Enhancement_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chandrasekar_PhISH-Net_Physics_Inspired_System_for_High_Resolution_Underwater_Image_Enhancement_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484357/,"['Water', 'Computer vision', 'Image color analysis', 'Computational modeling', 'Neural networks', 'Attenuation', 'Real-time systems']","['Image Enhancement', 'Underwater Image', 'Underwater Image Enhancement', 'Neural Network', 'Image Processing', 'High-resolution Images', 'Extensive Experiments', 'Image Formation', 'Low Contrast', 'Generalization Capability', 'Depth Information', 'Attenuation Coefficient', 'Physics-based Models', 'Image Enhancement Methods', 'Image Formation Process', 'Color Degradation', 'Image Quality', 'Deep Neural Network', 'Local Features', 'Number Of Images', 'Low-light Image', 'Structural Similarity Index Measure', 'Depth Map', 'Peak Signal-to-noise Ratio', 'Direct Signal', 'Smoothness Loss', 'Blue Channel', 'Depth Estimation', 'White Balance', 'Global Features']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",3,"Underwater imaging presents numerous challenges due to refraction, light absorption, and scattering, resulting in color degradation, low contrast, and blurriness. Enhancing underwater images is crucial for high-level computer vision tasks, but existing methods either neglect the physics-based image formation process or require expensive computations. In this paper, we propose an effective framework that combines a physics-based Underwater Image Formation Model (UIFM) with a deep image enhancement approach based on the retinex model. Firstly, we remove backscatter by estimating attenuation coefficients using depth information. Then, we employ a retinex model-based deep image enhancement module to enhance the images. To ensure adherence to the UIFM, we introduce a novel Wideband Attenuation prior. The proposed PhISH-Net framework achieves real-time processing of high-resolution underwater images using a lightweight neural network and a bilateral-grid-based upsampler. Extensive experiments on two underwater image datasets demonstrate the superior performance of our method compared to state-of-the-art techniques. Additionally, qualitative evaluation on a cross-dataset scenario confirms its generalization capability. Our contributions lie in combining the physics-based UIFM with deep image enhancement methods, introducing the wideband attenuation prior, and achieving superior performance and efficiency."
Physical-Space Multi-Body Mesh Detection Achieved by Local Alignment and Global Dense Learning,"Haoye Dong, Tiange Xiang, Sravan Chittupalli, Jun Liu, Dong Huang",Carnegie Mellon University; Stanford University,100.0,USA,0.0,,"From monocular RGB images captured in the wild, detecting multi-body 3D meshes in physical sizes and locations is notoriously difficult due to the diverse visual ambiguity and lack of explicit depth measurement. Modern DNN approaches made numerous advances based on either two-stage Region-of-Interests(RoI)-Align or single-stage fixed Field-of-View (FoV) detector frameworks for two main subtasks: local pelvis-centered mesh regression and global body-to-camera translation regression. However, sub-meter-level physical-space monocular mesh detection is still out of reach by existing solutions. In this paper, we recognize two common drawbacks: (1) The local meshes are usually estimated without explicitly aligning body features under image-space scaling, occlusion, and truncation; (2) The global translations are estimated based on a weak-perspective assumption, which tricks the network into prioritizing image-space (front-view) mesh alignment and leads to inaccurate mesh depth. We introduce Physical-space Multi-body Mesh Detection (PMMD), in which (1) Locally, we preserve the body aspect ratio, align the body-to-RoI layout, and densely refine the person-wise RoI features for robustness; (2) Globally, we learn dense-depth-guided features to amend the body-wise local feature for physical depth estimation. With the cleaned local features and explicit local-global associations, PMMD achieves the best centimeter-level local mesh metrics and the first sub-meter-level global mesh metrics from monocular images in 3DPW and AGORA datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Dong_Physical-Space_Multi-Body_Mesh_Detection_Achieved_by_Local_Alignment_and_Global_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dong_Physical-Space_Multi-Body_Mesh_Detection_Achieved_by_Local_Alignment_and_Global_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483986/,"['Visualization', 'Computer vision', 'Three-dimensional displays', 'Image recognition', 'Layout', 'Estimation', 'Detectors']","['Local Alignment', 'Global Learning', 'Aspect Ratio', 'Local Features', 'RGB Images', 'Physical Size', '3D Mesh', 'Depth Estimation', 'Global Translation', 'Global Metrics', 'Common Drawback', 'Local Metrics', 'Monocular Images', 'Local Mesh', 'Global Features', 'Physical Space', '3D Space', 'Bounding Box', '3D Coordinates', 'Global 3D', 'Global Task', 'Global Localization', 'Camera Pose', 'Learning Rate Of 1e', 'Local Head', 'Global Regression', 'Translation Vector', 'Dense Depth', 'Local Translation']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"From monocular RGB images captured in the wild, detecting multi-body 3D meshes in physical sizes and locations is notoriously difficult due to the diverse visual ambiguity and lack of explicit depth measurement. Modern DNN approaches made numerous advances based on either two-stage Region-of-Interests(RoI)-Align or single-stage fixed Field-of-View (FoV) detector frameworks for two main subtasks: local pelvis-centered mesh regression and global body-to-camera translation regression. However, sub-meter-level physical-space monocular mesh detection is still out of reach by existing solutions. In this paper, we recognize two common drawbacks: (1) The local meshes are usually estimated without explicitly aligning body features under image-space scaling, occlusion, and truncation; (2) The global translations are estimated based on a weak-perspective assumption, which tricks the network into prioritizing image-space (front-view) mesh alignment and leads to inaccurate mesh depth. We introduce Physical-space Multi-body Mesh Detection (PMMD), in which (1) Locally, we preserve the body aspect ratio, align the body-to-RoI layout, and densely refine the person-wise RoI features for robustness; (2) Globally, we learn dense-depth-guided features to amend the body-wise local feature for physical depth estimation. With the cleaned local features and explicit local-global associations, PMMD achieves the best centimeter-level local mesh metrics and the first sub-meter-level global mesh metrics from monocular images in 3DPW and AGORA datasets."
Pixel Matching Network for Cross-Domain Few-Shot Segmentation,"Hao Chen, Yonghan Dong, Zheming Lu, Yunlong Yu, Jungong Han",University of Sheffield; Huawei Technologies Ltd.; Zhejiang University,66.66666666666666,"China, UK",33.33333333333334,China,"Few-Shot Segmentation (FSS) aims to segment the novel class images with a few annotated samples. In the past, numerous studies have concentrated on cross-category tasks, where the training and testing sets are derived from the same dataset, while these methods face significant difficulties in domain-shift scenarios. To better tackle the cross-domain tasks, we propose a pixel matching network (PMNet) to extract the domain-agnostic pixel-level affinity matching with a frozen backbone and capture both the pixel-to-pixel and pixel-to-patch relations in each support-query pair with the bidirectional 3D convolutions. Different from the existing methods that remove the support background, we design a hysteretic spatial filtering module (HSFM) to filter the background-related query features and retain the foreground-related query features with the assistance of the support background, which is beneficial for eliminating interference objects in the query background. We comprehensively evaluate our PMNet on ten benchmarks under cross-category, cross-dataset, and cross-domain FSS tasks. Experimental results demonstrate that PMNet performs very competitively under different settings with only 0.68M parameters, especially under cross-domain FSS tasks, showing its effectiveness and efficiency.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Pixel_Matching_Network_for_Cross-Domain_Few-Shot_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Pixel_Matching_Network_for_Cross-Domain_Few-Shot_Segmentation_WACV_2024_paper.pdf,,https://github.com/chenhao-zju/PMNet,,main,Poster,https://ieeexplore.ieee.org/document/10484033/,"['Training', 'Image segmentation', 'Matched filters', 'Three-dimensional displays', 'Filtering', 'Interference', 'Benchmark testing']","['Matching Pixels', 'Few-shot Segmentation', 'Spatial Module', 'Spatial Filter', '3D Convolution', 'Annotated Samples', 'Query Features', 'Training Dataset', 'Convolutional Layers', 'Spatial Information', 'Chest X-ray', 'Feature Maps', 'Convolution Operation', 'Semantic Segmentation', 'Fewer Parameters', 'Segmentation Task', 'Base Classes', 'Affinity Matrix', 'Query Image', 'Density Correlation', 'Query Sample', 'ResNet-50 Backbone', '3D Convolutional Network', 'ResNet-101 Backbone', 'Classification Head', 'Dermoscopy', 'Background Objects']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",8,"Few-Shot Segmentation (FSS) aims to segment the novel class images with a few annotated samples. In the past, numerous studies have concentrated on cross-category tasks, where the training and testing sets are derived from the same dataset, while these methods face significant difficulties in domain-shift scenarios. To better tackle the cross-domain tasks, we propose a pixel matching network (PMNet) to extract the domain-agnostic pixel-level affinity matching with a frozen backbone and capture both the pixel-to-pixel and pixel-to-patch relations in each support-query pair with the bidirectional 3D convolutions. Different from the existing methods that remove the support background, we design a hysteretic spatial filtering module (HSFM) to filter the background-related query features and retain the foreground-related query features with the assistance of the support background, which is beneficial for eliminating interference objects in the query background. We comprehensively evaluate our PMNet on ten benchmarks under cross-category, cross-dataset, and cross-domain FSS tasks. Experimental results demonstrate that PMNet performs very competitively under different settings with only 0.68M parameters, especially under cross-domain FSS tasks, showing its effectiveness and efficiency. Code will be released at: https://github.com/chenhao-zju/PMNet"
Pixel-Grounded Prototypical Part Networks,"Zachariah Carmichael, Suhas Lohit, Anoop Cherian, Michael J. Jones, Walter J. Scheirer",University of Notre Dame; Mitsubishi Electric Research Laboratories,100.0,USA,0.0,,"Prototypical part neural networks (ProtoPartNNs), namely ProtoPNet and its derivatives, are an intrinsically interpretable approach to machine learning. Their prototype learning scheme enables intuitive explanations of the form, this (prototype) looks like that (testing image patch). But, does this actually look like that? In this work, we delve into why object part localization and associated heat maps in past work are misleading. Rather than localizing to object parts, existing ProtoPartNNs localize to the entire image, contrary to generated explanatory visualizations. We argue that detraction from these underlying issues is due to the alluring nature of visualizations and an over-reliance on intuition. To alleviate these issues, we devise new receptive field-based architectural constraints for meaningful localization and a principled pixel space mapping for ProtoPartNNs. To improve interpretability, we propose additional architectural improvements, including a simplified classification head. We also make additional corrections to ProtoPNet and its derivatives, such as the use of a validation set, rather than a test set, to evaluate generalization during training. Our approach, PixPNet (Pixel-grounded Prototypical part Network), is the only ProtoPartNN that truly learns and localizes to prototypical object parts. We demonstrate that PixPNet achieves quantifiably improved interpretability without sacrificing accuracy.",https://openaccess.thecvf.com/content/WACV2024/html/Carmichael_Pixel-Grounded_Prototypical_Part_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Carmichael_Pixel-Grounded_Prototypical_Part_Networks_WACV_2024_paper.pdf,,https://github.com/merlresearch/PixPNet,2309.14531,main,Poster,https://ieeexplore.ieee.org/document/10484503/,"['Location awareness', 'Training', 'Visualization', 'Neural networks', 'Prototypes', 'Space mapping', 'Machine learning']","['Prototypical Network', 'Prototype Parts', 'Neural Network', 'Heatmap', 'Validation Set', 'Image Patches', 'Object Parts', 'Pixel Spacing', 'Classification Head', 'Black Box', 'Similarity Score', 'Receptive Field', 'Bounding Box', 'Latent Space', 'Semantic Similarity', 'Original Space', 'Input Space', 'Original Approach', 'Original Score', 'Original Map', 'Case-based Reasoning', 'Explainable Artificial Intelligence', 'Sample Xi', 'Vision Transformer', 'AI Systems', 'Heatmap Visualization', 'Bicubic Interpolation', 'Class Boundaries', 'Similarity Map']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Prototypical part neural networks (ProtoPartNNs), namely ProtoPNet and its derivatives, are an intrinsically interpretable approach to machine learning. Their prototype learning scheme enables intuitive explanations of the form, this (prototype) looks like that (testing image patch). But, does this actually look like that? In this work, we delve into why object part localization and associated heat maps in past work are misleading. Rather than localizing to object parts, existing ProtoPartNNs localize to the entire image, contrary to generated explanatory visualizations. We argue that detraction from these underlying issues is due to the alluring nature of visualizations and an over-reliance on intuition. To alleviate these issues, we devise new receptive field-based architectural constraints for meaningful localization and a principled pixel space mapping for ProtoPartNNs. To improve interpretability, we propose additional architectural improvements, including a simplified classification head. We also make additional corrections to ProtoPNet and its derivatives, such as the use of a validation set, rather than a test set, to evaluate generalization during training. Our approach, PixPNet (Pixel-grounded Prototypical part Network), is the only ProtoPartNN that truly learns and localizes to prototypical object parts. We demonstrate that PixPNet achieves quantifiably improved interpretability without sacrificing accuracy
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
PlantPlotGAN: A Physics-Informed Generative Adversarial Network for Plant Disease Prediction,"Felipe A. Lopes, Vasit Sagan, Flavio Esposito",,,,,,"Monitoring plantations is crucial for crop management and producing healthy harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect multispectral images that aid in this monitoring. However, given the number of hectares to be monitored and the limitations of flight, plant disease signals become visually clear only in the later stages of plant growth and only if the disease has spread throughout a significant portion of the plantation. This limited amount of relevant data hampers the prediction models, as the algorithms struggle to generalize patterns with unbalanced or unrealistic augmented datasets effectively. To address this issue, we propose PlantPlotGAN, a physics-informed generative model capable of reproducing synthetic multispectral plot images with realistic vegetation indices. These indices served as a proxy for early disease detection and were used to evaluate if our model could help increase the accuracy of prediction models. The results demonstrate that the synthetic imagery generated from PlantPlotGAN outperforms state-of-the-art methods regarding the Frechet inception distance. Moreover, prediction models achieve higher accuracy metrics when trained with synthetic and original imagery for earlier plant disease detection compared to the training processes based solely on real imagery.",https://openaccess.thecvf.com/content/WACV2024/html/Lopes_PlantPlotGAN_A_Physics-Informed_Generative_Adversarial_Network_for_Plant_Disease_Prediction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lopes_PlantPlotGAN_A_Physics-Informed_Generative_Adversarial_Network_for_Plant_Disease_Prediction_WACV_2024_paper.pdf,,,2310.18268,main,Poster,https://ieeexplore.ieee.org/document/10484018/,"['Training', 'Plant diseases', 'Plantations', 'Vegetation mapping', 'Computer architecture', 'Predictive models', 'Generative adversarial networks']","['Plant Disease', 'Generative Adversarial Networks', 'Prediction Model', 'Plant Growth', 'Growth Stages', 'Vegetation Index', 'Unmanned Aerial Vehicles', 'Multispectral Images', 'Synthetic Images', 'Accuracy Metrics', 'Plant Growth Stages', 'Unbalanced Dataset', 'Inception Distance', 'Learning Algorithms', 'Convolutional Layers', 'Spectral Bands', 'Data Augmentation', 'Real Samples', 'Accuracy Scores', 'Normalized Difference Vegetation Index', 'Generative Adversarial Networks Model', 'Fréchet Inception Distance', 'Yellow Rust', 'Latent Space', 'Bhattacharyya Distance', 'Red Edge', 'Synthetic Data Generation', 'Generative Adversarial Network Architecture', 'Spectral Profiles', 'Plants In Each Plot']","['Applications', 'Agriculture', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Remote Sensing']",,"Monitoring plantations is crucial for crop management and producing healthy harvests. Unmanned Aerial Vehicles (UAVs) have been used to collect multispectral images that aid in this monitoring. However, given the number of hectares to be monitored and the limitations of flight, plant disease signals become visually clear only in the later stages of plant growth and only if the disease has spread throughout a significant portion of the plantation. This limited amount of relevant data hampers the prediction models, as the algorithms struggle to generalize patterns with unbalanced or unrealistic augmented datasets effectively. To address this issue, we propose PlantPlotGAN, a physics-informed generative model capable of creating synthetic multispectral plot images with realistic vegetation indices. These indices served as a proxy for disease detection and were used to evaluate if our model could help increase the accuracy of prediction models. The results demonstrate that the synthetic imagery generated from PlantPlotGAN outperforms state-of-the-art methods regarding the Frichet inception distance. Moreover, prediction models achieve higher accuracy metrics when trained with synthetic and original imagery for earlier plant disease detection compared to the training processes based solely on real imagery."
Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning,"Alex Gomez-Villa, Bartlomiej Twardowski, Kai Wang, Joost van de Weijer","Computer Vision Center, Barcelona, Spain; Universitat Autonoma de Barcelona, Barcelona, Spain; IDEAS NCBR, Warsaw, Poland; Computer Vision Center, Barcelona, Spain; Universitat Autonoma de Barcelona, Barcelona, Spain",80.0,Spain,20.0,Poland,"Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and initialize a new expert with the knowledge of the old network. We perform several experiments showing that our proposed approach outperforms other CURL exemplar-free methods in few- and many-task split settings. Furthermore, we show how to adapt our approach to semi-supervised continual learning (Semi-SCL) and show that we surpass the accuracy of other exemplar-free Semi-SCL methods and reach the results of some others that use exemplars.",https://openaccess.thecvf.com/content/WACV2024/html/Gomez-Villa_Plasticity-Optimized_Complementary_Networks_for_Unsupervised_Continual_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gomez-Villa_Plasticity-Optimized_Complementary_Networks_for_Unsupervised_Continual_Learning_WACV_2024_paper.pdf,,,2309.06086,main,Poster,https://ieeexplore.ieee.org/document/10484282/,"['Knowledge engineering', 'Representation learning', 'Computer vision', 'Self-supervised learning', 'Task analysis']","['Unsupervised Learning', 'Incremental Learning', 'Representation Learning', 'Data Streams', 'Semi-supervised Learning', 'Self-supervised Learning', 'Network Of Experts', 'Low Plasticity', 'Learning Algorithms', 'Current Data', 'Stage 2', 'Feature Representation', 'Data Augmentation', 'Training Procedure', 'Systems Theory', 'Projector', 'Latent Space', 'Unlabeled Data', 'Current Task', 'Knowledge Integration', 'Pretext Task', 'Edge Devices', 'Experience Replay', 'Cross-correlation Matrix', 'Previous Tasks', 'Catastrophic Forgetting']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Continuous unsupervised representation learning (CURL) research has greatly benefited from improvements in self-supervised learning (SSL) techniques. As a result, existing CURL methods using SSL can learn high-quality representations without any labels, but with a notable performance drop when learning on a many-tasks data stream. We hypothesize that this is caused by the regularization losses that are imposed to prevent forgetting, leading to a suboptimal plasticity-stability trade-off: they either do not adapt fully to the incoming data (low plasticity), or incur significant forgetting when allowed to fully adapt to a new SSL pretext-task (low stability). In this work, we propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting and initialize a new expert with the knowledge of the old network. We perform several experiments showing that our proposed approach outperforms other CURL exemplar-free methods in few- and many-task split settings. Furthermore, we show how to adapt our approach to semi-supervised continual learning (Semi-SCL) and show that we surpass the accuracy of other exemplar-free Semi-SCL methods and reach the results of some others that use exemplars."
Point-DynRF: Point-Based Dynamic Radiance Fields From a Monocular Video,"Byeongjun Park, Changick Kim",Korea Advanced Institute of Science and Technology (KAIST),100.0,South Korea,0.0,,"Dynamic radiance fields have emerged as a promising approach for generating novel views from a monocular video. However, previous methods enforce the geometric consistency to dynamic radiance fields only between adjacent input frames, making it difficult to represent the global scene geometry and degenerates at the viewpoint that is spatio-temporally distant from the input camera trajectory. To solve this problem, we introduce point-based dynamic radiance fields (Point-DynRF), a novel framework where the geometric information and the volume rendering process are trained by neural point clouds and dynamic radiance fields, respectively. Specifically, we reconstruct neural point clouds directly from geometric proxies and optimize both radiance fields and the geometric proxies using our proposed losses, allowing them to complement each other. We validate the effectiveness of our method with experiments on the NVIDIA Dynamic Scenes Dataset and several causally captured monocular video clips.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Point-DynRF_Point-Based_Dynamic_Radiance_Fields_From_a_Monocular_Video_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Point-DynRF_Point-Based_Dynamic_Radiance_Fields_From_a_Monocular_Video_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483679/,"['Point cloud compression', 'Geometry', 'Computer vision', 'Three-dimensional displays', 'Limiting', 'Rendering (computer graphics)', 'Cameras']","['Monocular Video', 'Radiance Field', 'Point Cloud', 'Dynamic Field', 'Adjacent Frames', 'Dynamic Scenes', 'Dynamic Datasets', 'Scene Geometry', 'Neural Field', 'Global Geometry', 'Active Surface', 'Depth Map', 'Neural Representations', 'Volume Density', 'Regional Dynamics', 'Optical Flow', 'Object Position', 'Joint Optimization', 'Depth Estimation', '3D Point Cloud', 'Flow-based Methods', 'Static Regions', 'View Synthesis', 'Dynamic Point', 'Dynamic Objects', 'Scene Representation', 'Time-dependent Field', 'Long-term View', 'Reconstruction Loss', 'Direct Regression']","['Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",3,"Dynamic radiance fields have emerged as a promising approach for generating novel views from a monocular video. However, previous methods enforce the geometric consistency to dynamic radiance fields only between adjacent input frames, making it difficult to represent the global scene geometry and degenerates at the viewpoint that is spatio-temporally distant from the input camera trajectory. To solve this problem, we introduce point-based dynamic radiance fields (Point-DynRF), a novel framework where the global geometric information and the volume rendering process are trained by neural point clouds and dynamic radiance fields, respectively. Specifically, we reconstruct neural point clouds directly from geometric proxies and optimize both radiance fields and the geometric proxies using our proposed losses, allowing them to complement each other. We validate the effectiveness of our method with experiments on the NVIDIA Dynamic Scenes Dataset and several causally captured monocular video clips."
PointCT: Point Central Transformer Network for Weakly-Supervised Point Cloud Semantic Segmentation,"Anh-Thuan Tran, Hoanh-Su Le, Suk-Hwan Lee, Ki-Ryong Kwon","Faculty of Information Systems, University of Economics and Law, Ho Chi Minh City, Vietnam; Department of Computer Engineering, Dong-A University, South Korea; Department of Artificial Intelligence Convergence, Pukyong National University, South Korea",100.0,"South Korea, Vietnam",0.0,,"Although point cloud segmentation has a principal role in 3D understanding, annotating fully large-scale scenes for this task can be costly and time-consuming. To resolve this issue, we propose Point Central Transformer (PointCT), a novel end-to-end trainable transformer network for weakly-supervised point cloud semantic segmentation. Divergent from prior approaches, our method addresses limited point annotation challenges exclusively based on 3D points through central-based attention. By employing two embedding processes, our attention mechanism integrates global features across neighborhoods, thereby effectively enhancing unlabeled point representations. Simultaneously, the interconnections between central points and their distinct neighborhoods are bidirectional cohered. Position encoding is further applied to enforce geometric features and improve overall performance. Notably, PointCT achieves outstanding performance under various labeled point settings without additional supervision. Extensive experiments on public datasets S3DIS, ScanNet-V2, and STPLS3D demonstrate the superiority of our proposed approach over other state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Tran_PointCT_Point_Central_Transformer_Network_for_Weakly-Supervised_Point_Cloud_Semantic_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tran_PointCT_Point_Central_Transformer_Network_for_Weakly-Supervised_Point_Cloud_Semantic_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484230/,"['Point cloud compression', 'Computer vision', 'Three-dimensional displays', 'Annotations', 'Semantic segmentation', 'Noise', 'Transformers']","['Point Cloud', 'Semantic Segmentation', 'Semantic Segmentation Network', 'Point Cloud Semantic Segmentation', 'Central Point', 'Attention Mechanism', 'Global Features', 'Geometric Features', 'Outstanding Performance', '3D Point', 'Representative Points', 'Positional Encoding', 'Point Cloud Segmentation', 'Limited Annotation', 'Spatial Features', 'K-nearest Neighbor', 'Position Of Point', 'Feature Points', 'Corresponding Points', 'Standard Light', 'Self-supervised Learning', 'Neighboring Points', 'Transformer Block', 'Weak Supervision', 'Point Cloud Dataset', 'Siamese Network', 'Attention Weights', 'Embedding Process', 'Downsampling Layer', 'Geometric Information']","['Algorithms', '3D computer vision']",,"Although point cloud segmentation has a principal role in 3D understanding, annotating fully large-scale scenes for this task can be costly and time-consuming. To resolve this issue, we propose Point Central Transformer (PointCT), a novel end-to-end trainable transformer network for weakly-supervised point cloud semantic segmentation. Divergent from prior approaches, our method addresses limited point annotation challenges exclusively based on 3D points through central-based attention. By employing two embedding processes, our attention mechanism integrates global features across neighborhoods, thereby effectively enhancing unlabeled point representations. Simultaneously, the interconnections between central points and their distinct neighborhoods are bidirectional cohered. Position encoding is further applied to enforce geometric features and improve overall performance. Notably, PointCT achieves outstanding performance under various labeled point settings without additional supervision. Extensive experiments on public datasets S3DIS, ScanNet-V2, and STPLS3D demonstrate the superiority of our proposed approach over other state-of-the-art methods."
Polarimetric PatchMatch Multi-View Stereo,"Jinyu Zhao, Jumpei Oishi, Yusuke Monno, Masatoshi Okutomi",Tokyo Institute of Technology,100.0,Japan,0.0,,"PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS approaches, owing to its balanced accuracy and efficiency. In this paper, we propose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the first method exploiting polarization cues to PatchMatch MVS. The key of PatchMatch MVS is to generate depth and normal hypotheses, which form local 3D planes and slanted stereo matching windows, and efficiently search for the best hypothesis based on the consistency among multi-view images. In addition to standard photometric consistency, our PolarPMS evaluates polarimetric consistency to assess the validness of a depth and normal hypothesis, motivated by the physical property that the polarimetric information is related to the object's surface normal. Experimental results demonstrate that our PolarPMS can improve the accuracy and the completeness of reconstructed 3D models, especially for texture-less surfaces, compared with state-of-the-art PatchMatch MVS methods.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Polarimetric_PatchMatch_Multi-View_Stereo_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_Polarimetric_PatchMatch_Multi-View_Stereo_WACV_2024_paper.pdf,,,2311.07600,main,Poster,https://ieeexplore.ieee.org/document/10484006/,"['Surface reconstruction', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Estimation', 'Reflection', 'Data models']","['Patch Matching', 'Local Plane', 'Hypothesis Of Normality', 'Multi-view Images', 'Stereo Matching', 'Surface Normals', '3D Plane', 'Polarimetric Information', 'Cost Function', 'Point Cloud', 'Azimuth Angle', 'Reference Image', 'Depth Map', 'Source Images', '3D Point', 'Minimum Angle', 'Normal Approximation', 'Similar Color', 'Depth Estimation', 'Armadillo', 'Geometric Consistency', 'Camera Pose', 'Polarization Imaging', 'Normal Map', 'Polarization Information', 'Rich Texture', 'Proportion Of Pixels', 'Windshield', 'Dense Point Cloud', '3D Point Cloud']","['Algorithms', '3D computer vision', 'Algorithms', 'Low-level and physics-based vision']",1,"PatchMatch Multi-View Stereo (PatchMatch MVS) is one of the popular MVS approaches, owing to its balanced accuracy and efficiency. In this paper, we propose Polarimetric PatchMatch multi-view Stereo (PolarPMS), which is the first method exploiting polarization cues to PatchMatch MVS. The key of PatchMatch MVS is to generate depth and normal hypotheses, which form local 3D planes and slanted stereo matching windows, and efficiently search for the best hypothesis based on the consistency among multi-view images. In addition to standard photometric consistency, our PolarPMS evaluates polarimetric consistency to assess the validness of a depth and normal hypothesis, motivated by the physical property that the polarimetric information is related to the object’s surface normal. Experimental results demonstrate that our PolarPMS can improve the accuracy and the completeness of reconstructed 3D models, especially for texture-less surfaces, compared with state-of-the-art PatchMatch MVS methods."
PolyMaX: General Dense Prediction With Mask Transformer,"Xuan Yang, Liangzhe Yuan, Kimberly Wilber, Astuti Sharma, Xiuye Gu, Siyuan Qiao, Stephanie Debats, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Liang-Chieh Chen","Google Research, ByteDance; Google Research",0.0,,100.0,USA,"Dense prediction tasks, such as semantic segmentation, depth estimation, and surface normal prediction, can be easily formulated as per-pixel classification (discrete outputs) or regression (continuous outputs). This per-pixel prediction paradigm has remained popular due to the prevalence of fully convolutional networks. However, on the recent frontier of segmentation task, the community has been witnessing a shift of paradigm from per-pixel prediction to cluster-prediction with the emergence of transformer architectures, particularly the mask transformers, which directly predicts a label for a mask instead of a pixel. Despite this shift, methods based on the per-pixel prediction paradigm still dominate the benchmarks on the other dense prediction tasks that require continuous outputs, such as depth estimation and surface normal prediction. Motivated by the success of DORN and AdaBins in depth estimation, achieved by discretizing the continuous output space, we propose to generalize the cluster-prediction based method to general dense prediction tasks. This allows us to unify dense prediction tasks with the mask transformer framework. Remarkably, the resulting model PolyMaX demonstrates state-of-the-art performance on three benchmarks of NYUD-v2 dataset. We hope our simple yet effective design can inspire more research on exploiting mask transformers for more dense prediction tasks. Code and model will be made available.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_PolyMaX_General_Dense_Prediction_With_Mask_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_PolyMaX_General_Dense_Prediction_With_Mask_Transformer_WACV_2024_paper.pdf,,https://github.com/google-research/deeplab2,2311.05770,main,Poster,https://ieeexplore.ieee.org/document/10484175/,"['Computer vision', 'Codes', 'Image synthesis', 'Semantic segmentation', 'Estimation', 'Computer architecture', 'Benchmark testing']","['Transformer', 'Dense Prediction', 'Benchmark', 'Discretion', 'Semantic Segmentation', 'Segmentation Task', 'Depth Estimation', 'Output Space', 'Continuous Output', 'Segmentation Prediction', 'Surface Normals', 'Discrete Output', 'Error Of The Mean', 'Linear Combination', 'Image Segmentation', 'Irregular Shape', 'Multilayer Perceptron', 'Cluster Centers', 'Depth Range', 'Continuous Domain', 'Central Bin', 'Pseudo Labels', 'Pixel Features', 'Pre-training Data', 'Intermediate Representation', 'Depth Values', 'Prediction Problem', 'Continuous Range']","['Algorithms', 'Image recognition and understanding']",4,"Dense prediction tasks, such as semantic segmentation, depth estimation, and surface normal prediction, can be easily formulated as per-pixel classification (discrete outputs) or regression (continuous outputs). This per-pixel prediction paradigm has remained popular due to the prevalence of fully convolutional networks. However, on the recent frontier of segmentation task, the community has been witnessing a shift of paradigm from per-pixel prediction to cluster-prediction with the emergence of transformer architectures, particularly the mask transformers, which directly predicts a label for a mask instead of a pixel. Despite this shift, methods based on the per-pixel prediction paradigm still dominate the benchmarks on the other dense prediction tasks that require continuous outputs, such as depth estimation and surface normal prediction. Motivated by the success of DORN and AdaBins in depth estimation, achieved by discretizing the continuous output space, we propose to generalize the cluster-prediction based method to general dense prediction tasks. This allows us to unify dense prediction tasks with the mask transformer framework. Remarkably, the resulting model PolyMaX demonstrates state-of-the-art performance on three benchmarks of NYUD-v2 dataset. We hope our simple yet effective design can inspire more research on exploiting mask transformers for more dense prediction tasks. Code and model will be made available 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
PoseDiff: Pose-Conditioned Multimodal Diffusion Model for Unbounded Scene Synthesis From Sparse Inputs,"Seoyoung Lee, Joonseok Lee","Seoul National University, Google Research; The University of Texas at Austin",100.0,"South Korea, USA",0.0,,"Novel view synthesis has been heavily driven by NeRF-based models, but these models often hold limitations with the requirement of dense coverage of input views and expensive computations. NeRF models designed for scenarios with a few sparse input views face difficulty in being generalizable to complex or unbounded scenes, where multiple scene content can be at any distance from a multi-directional camera, and thus generate unnatural and low quality images with blurry or floating artifacts. To accommodate the lack of dense information in sparse view scenarios and the computational burden of NeRF-based models in novel view synthesis, our approach adopts diffusion models. In this paper, we present PoseDiff, which combines the fast and plausible generation ability of diffusion models and 3D-aware view consistency of pose parameters from NeRF-based models. Specifically, PoseDiff is a multimodal pose-conditioned diffusion model applicable for novel view synthesis of unbounded scenes as well as bounded or forward-facing scenes with sparse views. PoseDiff renders plausible novel views for given pose parameters while maintaining high-frequency geometric details in significantly less time than conventional NeRF-based methods.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_PoseDiff_Pose-Conditioned_Multimodal_Diffusion_Model_for_Unbounded_Scene_Synthesis_From_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484084/,"['Computer vision', 'Image color analysis', 'Computational modeling', 'Scalability', 'Cameras', 'Tuning', 'Faces']","['Diffusion Model', 'Multimodal Model', 'Sparse Input', 'Lack Of Information', 'Dense Cover', 'Computational Expense', 'Geometric Details', 'Scene Content', 'View Synthesis', 'Pose Parameters', 'Large Datasets', 'Computation Time', 'Denoising', 'Computer Vision', 'Training Time', 'Multilayer Perceptron', 'Latent Space', 'Image Generation', 'Single Object', 'Image Noise', 'Camera Pose', 'Special Token', 'Inductive Bias', 'Realistic View', 'Training Costs', 'Variational Autoencoder', 'Ground Truth Image', 'Scene Details', 'Volumetric Density', 'Regularization Techniques']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', '3D computer vision', 'Algorithms', 'Vision + language and/or other modalities']",1,"Novel view synthesis has been heavily driven by NeRF-based models, but these models often hold limitations with the requirement of dense coverage of input views and expensive computations. NeRF models designed for scenarios with a few sparse input views face difficulty in being generalizable to complex or unbounded scenes, where multiple scene content can be at any distance from a multi-directional camera, and thus generate unnatural and low quality images with blurry or floating artifacts. To accommodate the lack of dense information in sparse view scenarios and the computational burden of NeRF-based models in novel view synthesis, our approach adopts diffusion models. In this paper, we present PoseDiff, which combines the fast and plausible generation ability of diffusion models and 3D-aware view consistency of pose parameters from NeRF-based models. Specifically, PoseDiff is a multimodal pose-conditioned diffusion model applicable for novel view synthesis of unbounded scenes as well as bounded or forward-facing scenes with sparse views. PoseDiff renders plausible novel views for given pose parameters while maintaining high-frequency geometric details in significantly less time than conventional NeRF-based methods."
PreciseDebias: An Automatic Prompt Engineering Approach for Generative AI To Mitigate Image Demographic Biases,"Colton Clemmer, Junhua Ding, Yunhe Feng","University of North Texas, Denton, TX, USA",100.0,USA,0.0,,"Recent years have witnessed growing concerns over demographic biases in image-centric applications, including image search engines and generative systems. While the advent of generative AI offers a pathway to mitigate these biases by producing underrepresented images, existing solutions still fail to precisely generate images that reflect specified demographic distributions. In this paper, we propose PreciseDebias, a comprehensive end-to-end framework that can rectify demographic bias in image generation. By leveraging fine-tuned Large Language Models (LLMs) coupled with text-to-image generative models, PreciseDebias transforms generic text prompts to produce images in line with specified demographic distributions. The core component of PreciseDebias is our novel instruction-following LLM, meticulously designed with an emphasis on model bias assessment and balanced model training. Extensive experiments demonstrate the effectiveness of PreciseDebias in rectifying biases pertaining to both ethnicity and gender in images. Furthermore, when compared with two baselines, PreciseDebias illustrates its robustness and capability to capture demographic intricacies. The generalization of PreciseDebias is further illuminated by the diverse images it produces across multiple professions and demographic attributes. To ensure reproducibility, we will make PreciseDebias openly accessible to the broader research community by releasing all models and code.",https://openaccess.thecvf.com/content/WACV2024/html/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_To_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Clemmer_PreciseDebias_An_Automatic_Prompt_Engineering_Approach_for_Generative_AI_To_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483834/,"['Training', 'Computer vision', 'Codes', 'Image synthesis', 'Computational modeling', 'Transforms', 'Search engines']","['Demographic Bias', 'Prompt Engineering', 'Image Generation', 'Language Model', 'Model Bias', 'Image Retrieval', 'Demographic Distribution', 'Demographic Attributes', 'Fine-tuned Model', 'Demographic Characteristics', 'Training Dataset', 'Learning Rate', 'Machine Learning Models', 'Neonatal Intensive Care Unit', 'Bias Analysis', 'Set Membership', 'Regular Expressions', 'Named Entity Recognition', 'Ethnicity Information', 'Specific Ethnicity', 'Constant Learning Rate']","['Applications', 'Social good']",2,"Recent years have witnessed growing concerns over demographic biases in image-centric applications, including image search engines and generative systems. While the advent of generative AI offers a pathway to mitigate these biases by producing underrepresented images, existing solutions still fail to precisely generate images that reflect specified demographic distributions. In this paper, we propose PreciseDebias, a comprehensive end-to-end framework that can rectify demographic bias in image generation. By leveraging fine-tuned Large Language Models (LLMs) coupled with text-to-image generative models, PreciseDebias transforms generic text prompts to produce images in line with specified demographic distributions. The core component of PreciseDebias is our novel instruction-following LLM, meticulously designed with an emphasis on model bias assessment and balanced model training. Extensive experiments demonstrate the effectiveness of PreciseDebias in rectifying biases pertaining to both ethnicity and gender in images. Furthermore, when compared with two baselines, PreciseDebias illustrates its robustness and capability to capture demographic intricacies. The generalization of PreciseDebias is further illuminated by the diverse images it produces across multiple professions and demographic attributes. To ensure reproducibility, we will make PreciseDebias openly accessible to the broader research community by releasing all models and code."
Preserving Image Properties Through Initializations in Diffusion Models,"Jeffrey Zhang, Shao-Yu Chang, Kedan Li, David Forsyth",Revery AI; University of Illinois at Urbana-Champaign,50.0,USA,50.0,USA,"Retail photography imposes specific requirements on images. For instance, images may need uniform background colors, consistent model poses, centered products, and consistent lighting. Minor deviations from these standards impact a site's aesthetic appeal, making the images unsuitable for use. We show that Stable Diffusion methods, as currently applied, do not respect these requirements. The usual practice of training the denoiser with a very noisy image and starting inference with a sample of pure noise leads to inconsistent generated images during inference. This inconsistency occurs because it is easy to tell the difference between samples of the training and inference distributions. As a result, a network trained with centered retail product images with uniform backgrounds generates images with erratic backgrounds. The problem is easily fixed by initializing inference with samples from an approximation of noisy images. However, in using such an approximation, the joint distribution of text and noisy image at inference time still slightly differs from that at training time. This discrepancy is corrected by training the network with samples from the approximate noisy image distribution. Extensive experiments on real application data show significant qualitative and quantitative improvements in performance from adopting these procedures. Finally, our procedure can interact well with other control-based methods to further enhance the controllability of diffusion-based methods.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Preserving_Image_Properties_Through_Initializations_in_Diffusion_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483585/,"['Training', 'Photography', 'Computer vision', 'Image synthesis', 'Image color analysis', 'Noise', 'Lighting']","['Diffusion Model', 'Image Properties', 'Training Time', 'Approximate Distribution', 'Inference Time', 'Noisy Images', 'Pure Noise', 'Denoising', 'Random Noise', 'Training Procedure', 'Intermediate Step', 'Reference Image', 'White Background', 'Standard Training', 'Inference Procedure', 'Ground Truth Image', 'Training Distribution', 'Similar Pose', 'Neutral Background', 'Desirable Image', 'Text Encoder']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Commercial / retail']",1,"Retail photography imposes specific requirements on images. For instance, images may need uniform background colors, consistent model poses, centered products, and consistent lighting. Minor deviations from these standards impact a site’s aesthetic appeal, making the images unsuitable for use. We show that Stable Diffusion methods, as currently applied, do not respect these requirements. The usual practice of training the denoiser with a very noisy image and starting inference with a sample of pure noise leads to inconsistent generated images during inference. This inconsistency occurs because it is easy to tell the difference between samples of the training and inference distributions. As a result, a network trained with centered retail product images with uniform backgrounds generates images with erratic backgrounds. The problem is easily fixed by initializing inference with samples from an approximation of noisy images. However, in using such an approximation, the joint distribution of text and noisy image at inference time still slightly differs from that at training time. This discrepancy is corrected by training the network with samples from the approximate noisy image distribution. Extensive experiments on real application data show significant qualitative and quantitative improvements in performance from adopting these procedures. Finally, our procedure can interact well with other control-based methods to further enhance the controllability of diffusion-based methods."
PressureVision++: Estimating Fingertip Pressure From Diverse RGB Images,"Patrick Grady, Jeremy A. Collins, Chengcheng Tang, Christopher D. Twigg, Kunal Aneja, James Hays, Charles C. Kemp",Meta Reality Labs; Georgia Institute of Technology,50.0,USA,50.0,USA,"Touch plays a fundamental role in manipulation for humans; however, machine perception of contact and pressure typically requires invasive sensors. Recent research has shown that deep models can estimate hand pressure based on a single RGB image. However, evaluations have been limited to controlled settings since collecting diverse data with ground-truth pressure measurements is difficult. We present a novel approach that enables diverse data to be captured with only an RGB camera and a cooperative participant. Our key insight is that people can be prompted to apply pressure in a certain way, and this prompt can serve as a weak label to supervise models to perform well under varied conditions. We collect a novel dataset with 51 participants making fingertip contact with diverse objects. Our network, PressureVision++, outperforms human annotators and prior work. We also demonstrate an application of PressureVision++ to mixed reality where pressure estimation allows everyday surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and models are available online.",https://openaccess.thecvf.com/content/WACV2024/html/Grady_PressureVision_Estimating_Fingertip_Pressure_From_Diverse_RGB_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Grady_PressureVision_Estimating_Fingertip_Pressure_From_Diverse_RGB_Images_WACV_2024_paper.pdf,,https://pressurevision.github.io/,,main,Poster,https://ieeexplore.ieee.org/document/10484532/,"['Training', 'Computer vision', 'Codes', 'Mixed reality', 'Estimation', 'Tactile sensors', 'Cameras']","['RGB Images', 'Variety Of Objects', 'RGB Camera', 'Mixed Reality', 'Weak Labels', 'Perception Of Pressure', 'Training Data', 'Pressure Sensor', 'High Force', 'Index Finger', 'Dexterity', 'Lower Force', 'Depth Camera', 'Physical Sensations', 'Pose Estimation', 'Domain Adaptation', 'Natural Objects', 'Flexible Sensors', 'Human Pose Estimation', 'Part Of The Hand', 'Hand Tracking', '3D Pose', 'Monochrome Camera', 'Loss Of Domain', 'Magnitude Of Pressure', 'Domain Discriminator']","['Applications', 'Virtual / augmented reality', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"Touch plays a fundamental role in manipulation for humans; however, machine perception of contact and pressure typically requires invasive sensors. Recent research has shown that deep models can estimate hand pressure based on a single RGB image. However, evaluations have been limited to controlled settings since collecting diverse data with ground-truth pressure measurements is difficult. We present a novel approach that enables diverse data to be captured with only an RGB camera and a cooperative participant. Our key insight is that people can be prompted to apply pressure in a certain way, and this prompt can serve as a weak label to supervise models to perform well under varied conditions. We collect a novel dataset with 51 participants making fingertip contact with diverse objects. Our network, PressureVision++, outperforms human annotators and prior work. We also demonstrate an application of PressureVision++ to mixed reality where pressure estimation allows everyday surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and models are available online . 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
PrivObfNet: A Weakly Supervised Semantic Segmentation Model for Data Protection,"ChiatPin Tay, Vigneshwaran Subbaraju, Thivya Kandappu","Institue of High Performance Computing, A*STAR, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore",100.0,Singapore,0.0,,"The use of social media has made it easy to communicate and share information over the internet. However, it also brings issues such as data privacy leakage, which can be exploited by recipients with malicious intentions to harm the sender. In this paper, we propose a deep neural network that analyzes the user's image for privacy sensitive content and automatically locates sensitive regions for obfuscation. Our approach relies solely on image level annotations and learns to (a) predict an overall privacy score, (b) detect sensitive attributes and (c) demarcate the sensitive regions for obfuscation, in a given input image. We validated the performance of our proposed method on three large datasets, VISPR, PASCAL VOC 2012 and MS COCO 2014, in terms of privacy score, attribute prediction and obfuscation performance. On the VISPR dataset, we achieved a Pearson correlation of 0.88 and a Spearman correlation of 0.86, outperforming previous methods. On PASCAL VOC 2012 and MS COCO 2014, our model achieved a mean IOU of 71.5% and 43.9% respectively, and is among the state-of-the-art techniques using weakly supervised semantic segmentation learning.",https://openaccess.thecvf.com/content/WACV2024/html/Tay_PrivObfNet_A_Weakly_Supervised_Semantic_Segmentation_Model_for_Data_Protection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tay_PrivObfNet_A_Weakly_Supervised_Semantic_Segmentation_Model_for_Data_Protection_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484314/,"['Privacy', 'Correlation', 'Social networking (online)', 'Semantic segmentation', 'Computational modeling', 'Urban areas', 'Data protection']","['Semantic Segmentation', 'Pearson Correlation', 'Social Media', 'Input Image', 'Data Privacy', 'Sensitive Attributes', 'Local Features', 'Feature Maps', 'Personal Data', 'Deep Learning Models', 'Unsupervised Learning', 'Private Property', 'Prediction Score', 'Global Features', 'Object Classification', 'Multiple Tasks', 'Attention Task', 'Transformer Model', 'Sensitive Areas', 'Graph Convolutional Network', 'Attention Map', 'Saliency Map', 'Salient Regions', 'Loss Of Privacy', 'Self-supervised Learning', 'Cropped Images', 'COCO Dataset', 'Computer Vision', 'Deep Learning', 'Training Images']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",,"The use of social media has made it easy to communicate and share information over the internet. However, it also brings issues such as data privacy leakage, which can be exploited by recipients with malicious intentions to harm the sender. In this paper, we propose a deep neural network that analyzes user’s image for privacy sensitive content and automatically locates sensitive regions for obfuscation. Our approach relies solely on image level annotations and learns to (a) predict an overall privacy score, (b) detect sensitive attributes and (c) demarcate the sensitive regions for obfuscation, in a given input image. We validated the performance of our proposed method on three large datasets, VISPR, PASCAL VOC 2012 and MS COCO 2014, in terms of privacy score, attribute prediction and obfuscation performance. On the VISPR dataset, we achieved a Pearson correlation of 0.88 and a Spearman correlation of 0.86, outperforming previous methods. On PASCAL VOC 2012 and MS COCO 2014, our model achieved a mean IOU of 71.5% and 43.9% respectively, and is among the state-of-the-art techniques using weakly supervised semantic segmentation learning."
Privacy-Enhancing Person Re-Identification Framework - A Dual-Stage Approach,"Kajal Kansal, Yongkang Wong, Mohan Kankanhalli","School of Computing, National University of Singapore",100.0,Singapore,0.0,,"In this work, we show that deep learning-based re-identification (Re-ID) models, albeit trained only with a Re-ID objective (i.e. if two samples belong to the same identity), encode personally identifiable information (PII) in the learned features that may lead to serious privacy concerns. In cognizance of the modern privacy regulations on protecting PII, we propose a novel dual-stage person Re-ID framework that (1) suppresses the PII from the discriminative features, and (2) introduces a controllable privacy mechanism through differential privacy. The former is achieved with a self-supervised de-identification (De-ID) decoder and an adversarial-identity (Adv-ID) module, whereas the latter mechanism leverages a controllable privacy budget to generate a privacy-protected gallery with a Gaussian noise generator. Furthermore, we introduce the notion of a privacy metric to quantify the privacy leakage in Re-ID features which is not explicitly examined in prior work. We demonstrate the feasibility of our approach in achieving a better trade-off between utility and privacy through rigorous experiments on person Re-ID benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Kansal_Privacy-Enhancing_Person_Re-Identification_Framework_-_A_Dual-Stage_Approach_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kansal_Privacy-Enhancing_Person_Re-Identification_Framework_-_A_Dual-Stage_Approach_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
ProS: Facial Omni-Representation Learning via Prototype-Based Self-Distillation,"Xing Di, Yiyu Zheng, Xiaoming Liu, Yu Cheng",Michigan State University; The Chinese University of Hong Kong; ProtagoLabs Inc.,66.66666666666666,"Hong Kong, USA",33.33333333333334,USA,"This paper presents a novel approach, called Prototype-based Self-Distillation (ProS), for unsupervised face representation learning. The existing supervised methods heavily rely on a large amount of annotated training facial data, which poses challenges in terms of data collection and privacy concerns. To address these issues, we propose ProS, which leverages a vast collection of unlabeled face images to learn a comprehensive facial omni-representation. In particular, ProS consists of two vision-transformers (teacher and student models) that are trained with different augmented images (cropping, blurring, coloring, etc.). Besides, we build a face-aware retrieval system along with augmentations to obtain the curated images comprising predominantly facial areas. To enhance the discrimination of learned features, we introduce a prototype-based matching loss that aligns the similarity distributions between features (teacher or student) and a set of learnable prototypes. After pre-training, the teacher vision transformer serves as a backbone for downstream tasks, including attribute estimation, expression recognition, and landmark alignment, achieved through simple fine-tuning with additional layers. Extensive experiments demonstrate that our method achieves state-of-the-art performance on various tasks, both in full and few-shot settings. Furthermore, we investigate pre-training with synthetic face images, and ProS exhibits promising performance in this scenario as well.",https://openaccess.thecvf.com/content/WACV2024/html/Di_ProS_Facial_Omni-Representation_Learning_via_Prototype-Based_Self-Distillation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Di_ProS_Facial_Omni-Representation_Learning_via_Prototype-Based_Self-Distillation_WACV_2024_paper.pdf,,,2311.01929,main,Poster,https://ieeexplore.ieee.org/document/10484277/,"['Training', 'Representation learning', 'Data privacy', 'Computer vision', 'Face recognition', 'Estimation', 'Prototypes']","['Unsupervised Learning', 'Feature Learning', 'Teacher Model', 'Representation Learning', 'Face Images', 'Synthetic Images', 'Student Model', 'Image Augmentation', 'Face Representation', 'Learning Rate', 'Input Image', 'Facial Expressions', 'Network Parameters', 'Weight Decay', 'Privacy Issues', 'Training Images', 'Face Recognition', 'Local Image', 'Self-supervised Learning', 'Masked Images', 'Face Alignment', 'Global Image', 'State Of The Art Methods', 'Facial Expression Recognition', 'Large-scale Image', 'Self-supervised Training', 'Student Network', 'Face Dataset', 'Local View', 'Similarity Score']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"This paper presents a novel approach, called Prototype-based Self-Distillation (ProS), for unsupervised face representation learning. The existing supervised methods heavily rely on a large amount of annotated training facial data, which poses challenges in terms of data collection and privacy concerns. To address these issues, we propose ProS, which leverages a vast collection of unlabeled face images to learn a comprehensive facial omni-representation. In particular, ProS consists of two vision-transformers (teacher and student models) that are trained with different augmented images (cropping, blurring, coloring, etc.). Besides, we build a face-aware retrieval system along with augmentations to obtain the curated images comprising predominantly facial areas. To enhance the discrimination of learned features, we introduce a prototype-based matching loss that aligns the similarity distributions between features (teacher or student) and a set of learnable prototypes. After pre-training, the teacher vision transformer serves as a backbone for downstream tasks, including attribute estimation, expression recognition, and landmark alignment, achieved through simple fine-tuning with additional layers. Extensive experiments demonstrate that our method achieves state-of-the-art performance on various tasks, both in full and few-shot settings. Further, we investigate pre-training with synthetic face images, and ProS exhibits promising performance in this scenario as well."
ProcSim: Proxy-Based Confidence for Robust Similarity Learning,"Oriol Barbany, Xiaofan Lin, Muhammet Bastan, Arnab Dhua","Visual Search & AR, Amazon; Institut de Rob `otica i Inform `atica Industrial, CSIC-UPC",50.0,Spain,50.0,USA,"Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs. Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them. Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes. To train robust DML models, we propose ProcSim, a simple framework that assigns a confidence score to each sample using the normalized distance to its class representative. The experimental results show that the proposed method achieves state-of-the-art performance on the DML benchmark datasets injected with uniform and the proposed semantically coherent noise.",https://openaccess.thecvf.com/content/WACV2024/html/Barbany_ProcSim_Proxy-Based_Confidence_for_Robust_Similarity_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Barbany_ProcSim_Proxy-Based_Confidence_for_Robust_Similarity_Learning_WACV_2024_paper.pdf,,,2311.00668,main,Poster,https://ieeexplore.ieee.org/document/10483631/,"['Training', 'Visualization', 'Computer vision', 'Noise', 'Semantics', 'Benchmark testing', 'Ontologies']","['Similar Learning', 'Benchmark Datasets', 'Confidence Score', 'Semantic Similarity', 'Classification Datasets', 'Metric Learning', 'Deep Metric Learning', 'Hyperparameters', 'Class Labels', 'Loss Value', 'Classifier Training', 'Noise Model', 'Clean Samples', 'Image Retrieval', 'Correct Label', 'Curriculum Learning', 'Otsu’s Method', 'Symmetric Model', 'Visual Similarity', 'WordNet', 'Incorrect Labels', 'Uniform Noise', 'Noisy Labels', 'Zero-shot', 'Labeling Errors', 'Label Noise', 'Interclass Similarity', 'Contrastive Loss', 'Hypernym', 'Class Hierarchy']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs. Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them. Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes. To train robust DML models, we propose ProcSim, a simple framework that assigns a confidence score to each sample using the normalized distance to its class representative. The experimental results show that the proposed method achieves state-of-the-art performance on the DML benchmark datasets injected with uniform and the proposed semantically coherent noise."
Progressive Hypothesis Transformer for 3D Human Mesh Recovery,"Huang-Ru Liao, Jen-Chun Lin, Chun-Yi Lee","†Academia Sinica, Taipei, Taiwan; ‡Elsa Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; †Academia Sinica, Taipei, Taiwan; ‡Elsa Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan",100.0,Taiwan,0.0,,"Recent advancements in Transformer-based human mesh reconstruction (HMR) are commendable. However, these models often lift 2D images directly to 3D vertices without explicit intermediate guidance. In addition, the global attention mechanism tends to spread attention across larger body areas and even unrelated background regions during human mesh estimation, rather than focusing on critical local regions such as human body joints. This tendency leads to inaccurate and unrealistic results for complex activities. To address these challenges, we introduce the Progressive Hypotheses Transformer, which employs 2D and 3D pose predictions to progressively guide our model. Moreover, we propose a mechanism that generates multiple plausible hypotheses for both 2D and 3D poses to mitigate potential inaccuracies arising from intermediate pose estimations. Our model also incorporates inter-intra attention to capture correlations between joints and hypotheses. Experimental results demonstrate that our method surpasses existing imagebased approaches on Human3.6M [13] and 3DPW [36] with fewer parameters and relatively lower computational costs.",https://openaccess.thecvf.com/content/WACV2024/html/Liao_Progressive_Hypothesis_Transformer_for_3D_Human_Mesh_Recovery_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liao_Progressive_Hypothesis_Transformer_for_3D_Human_Mesh_Recovery_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484325/,"['Solid modeling', 'Three-dimensional displays', 'Correlation', 'Pose estimation', 'Focusing', 'Transforms', 'Predictive models']","['Human Mesh', '3D Human Mesh', 'Attention Mechanism', '2D Images', 'Multiple Hypothesis', 'Fewer Parameters', 'Pose Estimation', 'Global Attention', 'Plausible Hypothesis', 'Body Joints', '3D Pose', 'Pose Prediction', 'Unrealistic Results', 'Intermediate Estimates', 'Shape Parameter', 'Fully-connected Layer', 'Dot Product', '3D Coordinates', '3D Mesh', 'Training Objective', '2D Pose', 'Intermediate Representation', 'Human Pose Estimation', 'Image-based Methods', 'Number Of Hypotheses', 'Latent Features', 'Progressive Learning', 'Dot Product Operation', '3D Counterparts', 'Transformer-based Methods']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', '3D computer vision']",,"Recent advancements in Transformer-based human mesh reconstruction (HMR) are commendable. However, these models often lift 2D images directly to 3D vertices without explicit intermediate guidance. In addition, the global attention mechanism tends to spread attention across larger body areas and even unrelated background regions during human mesh estimation, rather than focusing on critical local regions such as human body joints. This tendency leads to inaccurate and unrealistic results for complex activities. To address these challenges, we introduce the Progressive Hypothesis Transformer, which employs 2D and 3D pose predictions to progressively guide our model. Moreover, we propose a mechanism that generates multiple plausible hypotheses for both 2D and 3D poses to mitigate potential inaccuracies arising from intermediate pose estimations. Our model also incorporates inter-intra attention to capture correlations between joints and hypotheses. Experimental results demonstrate that our method surpasses existing image-based approaches on Human3.6M [13] and 3DPW [36] with fewer parameters and relatively lower computational costs."
PromptAD: Zero-Shot Anomaly Detection Using Text Prompts,"Yiting Li, Adam Goodge, Fayao Liu, Chuan-Sheng Foo","Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore and Centre for Frontier AI Research (CFAR), A*STAR, Singapore",100.0,Singapore,0.0,,"We target the problem of zero-shot anomaly detection, in which a model is pre-trained on a set of seen classes and expected to detect anomalies in other unseen classes at test time. Although providing exceptional results for many anomaly detection (AD) tasks, state-of-the-art AD algorithms catastrophically struggle in zero-shot scenarios. However, if knowledge of additional modalities exist (e.g. text), we can compensate for the lack of visual information and improve the AD performance. In this work, we propose a knowledge-guided learning framework, namely PromptAD, which achieves the compatibility of a abnormality view and a normality view through a dual-branch vision-language decoding network. Concretely, the normality branch establishes a normality profile to exclude anomalies. Meanwhile, the abnormality branch directly models anomaly behaviors provided by natural language. As the two views capture complementary information, we naturally think of the compatibility of them for achieving better performance. Therefore, a cross-view contrastive learning (CCL) s proposed to regularize the intra-view training with additional reference information from the other complementary view, and a cross-view mutual interaction (CMI) strategy further promotes the mutual exploration of useful knowledge from each branch.",https://openaccess.thecvf.com/content/WACV2024/html/Li_PromptAD_Zero-Shot_Anomaly_Detection_Using_Text_Prompts_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_PromptAD_Zero-Shot_Anomaly_Detection_Using_Text_Prompts_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484043/,"['Training', 'Visualization', 'Computer vision', 'Natural languages', 'Semantics', 'Self-supervised learning', 'Benchmark testing']","['Anomaly Detection', 'Text Prompts', 'Natural Language', 'Benchmark Datasets', 'Lack Of Examples', 'Unseen Classes', 'Normal Samples', 'Rich Information', 'Semantic Information', 'Representation Learning', 'Real-world Scenarios', 'Semantic Knowledge', 'Base Classes', 'Attention Map', 'Class Boundaries', 'Score Map', 'Query Image', 'Complementary View', 'Types Of Anomalies', 'Anomaly Score', 'Symmetric Loss', 'Query Features', 'Rich Semantic Information']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",4,"We consider the problem of zero-shot anomaly detection in which a model is pre-trained to detect anomalies in images belonging to seen classes, and expected to detect anomalies from unseen classes at test time. State-of-the-art anomaly detection (AD) methods can often achieve exceptional results when training images are abundant, but they catastrophically fail in zero-shot scenarios with a lack of real examples. However, with the emergence of multi-modal models such as CLIP, it is possible to use knowledge from other modalities (e.g. text) to compensate for the lack of visual information and improve AD performance. In this work, we propose PromptAD, a dual-branch framework which uses prior knowledge about both normal and abnormal behaviours in the form of text prompts to detect anomalies even in unseen classes. More specifically, it uses CLIP as a backbone encoder network and an additional dual-branch vision-language decoding network for both normality and abnormality information. The normality branch establishes a profile of normality, while the abnormality branch models anomalous behaviors, guided by natural language text prompts. As the two branches capture complementary information or ‘views’, we propose a ‘cross-view contrastive learning’ (CCL) component which regularizes each view with additional reference information from the other view. We further propose a cross-view mutual interaction (CMI) strategy to promote the mutual exploration of useful knowledge from each branch. We show that PromptAD outperforms existing baselines in zero-shot anomaly detection on key benchmark datasets and analyse the role of each component in ablation studies."
Prompting Classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation,"Balamurali Murugesan, Rukhshanda Hussain, Rajarshi Bhattacharya, Ismail Ben Ayed, Jose Dolz",ETS Montreal; Jadavpur University,100.0,"Canada, India",0.0,,"Recently, CLIP-based approaches have exhibited remarkable performance on generalization and few-shot learning tasks, fueled by the power of contrastive language-vision pre-training. In particular, prompt tuning has emerged as an effective strategy to adapt the pre-trained language-vision models to downstream tasks by employing task-related textual tokens. Motivated by this progress, in this work we question whether other fundamental problems, such as weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning. Our findings reveal two interesting observations that shed light on the impact of prompt tuning on WSSS. First, modifying only the class token of the text prompt results in a greater impact on the Class Activation Map (CAM), compared to arguably more complex strategies that optimize the context. And second, the class token associated with the image ground truth does not necessarily correspond to the category that yields the best CAM. Motivated by these observations, we introduce a novel approach based on a PrOmpt cLass lEarning (POLE) strategy. Through extensive experiments we demonstrate that our simple, yet efficient approach achieves SOTA performance in a well-known WSSS benchmark. These results highlight not only the benefits of language-vision models in WSSS but also the potential of prompt learning for this problem. The code is available at https://anonymous.4open.science/r/WSS_POLE-DB45/README.md",https://openaccess.thecvf.com/content/WACV2024/html/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Murugesan_Prompting_Classes_Exploring_the_Power_of_Prompt_Class_Learning_in_WACV_2024_paper.pdf,,code link,2307.00097,main,Poster,https://ieeexplore.ieee.org/document/10484182/,"['Adaptation models', 'Computer vision', 'Codes', 'Semantic segmentation', 'Focusing', 'Benchmark testing', 'Performance gain']","['Semantic Segmentation', 'Weakly Supervised Semantic Segmentation', 'Learning Strategies', 'Activation Maps', 'Complex Strategies', 'Few-shot Learning', 'Class Activation Maps', 'Neural Network', 'Model Performance', 'Training Set', 'Classification Task', 'Input Image', 'Target Object', 'Segmentation Performance', 'Saliency Map', 'Input Text', 'Image X', 'Category Names', 'Global Average Pooling Layer', 'Continuous Vector', 'Text Encoder', 'Image-level Labels', 'Vision Transformer']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Vision + language and/or other modalities']",,"Recently, CLIP-based approaches have exhibited remarkable performance on generalization and few-shot learning tasks, fueled by the power of contrastive language-vision pre-training. In particular, prompt tuning has emerged as an effective strategy to adapt the pre-trained language-vision models to downstream tasks by employing task-related textual tokens. Motivated by this progress, in this work we question whether other fundamental problems, such as weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning. Our findings reveal two interesting observations that shed light on the impact of prompt tuning on WSSS. First, modifying only the class token of the text prompt results in a greater impact on the Class Activation Map (CAM), compared to arguably more complex strategies that optimize the context. And second, the class token associated with the image ground truth does not necessarily correspond to the category that yields the best CAM. Motivated by these observations, we introduce a novel approach based on a PrOmpt cLass lEarning (POLE) strategy. Through extensive experiments we demonstrate that our simple, yet efficient approach achieves SOTA performance in a well-known WSSS benchmark. These results highlight not only the benefits of language-vision models in WSSS but also the potential of prompt learning for this problem. The code is available at code link"
PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers Using Synthetic Scene Data,"Roei Herzig, Ofir Abramovich, Elad Ben Avraham, Assaf Arbelle, Leonid Karlinsky, Ariel Shamir, Trevor Darrell, Amir Globerson",Reichman University; Tel-Aviv University; IBM Research; MIT-IBM Watson AI Lab; UC Berkeley,80.0,"Israel, USA",20.0,USA,"Action recognition models have achieved impressive results by incorporating scene-level annotations, such as objects, their relations, 3D structure, and more. However, obtaining annotations of scene structure for videos requires a significant amount of effort to gather and annotate, making these methods expensive to train. In contrast, synthetic datasets generated by graphics engines provide powerful alternatives for generating scene-level annotations across multiple tasks. In this work, we propose an approach to leverage synthetic scene data for improving video understanding. We present a multi-task prompt learning approach for video transformers, where a shared video transformer backbone is enhanced by a small set of specialized parameters for each task. Specifically, we add a set of ""task prompts"", each corresponding to a different task, and let each prompt predict task-related annotations. This design allows the model to capture information shared among synthetic scene tasks as well as information shared between synthetic scene tasks and a real video downstream task throughout the entire network. We refer to this approach as ""Promptonomy"", since the prompts model task-related structure. We propose the PromptonomyViT model (PViT), a video transformer that incorporates various types of scene-level information from synthetic data using the ""Promptonomy"" approach. PViT shows strong performance improvements on multiple video understanding tasks and datasets. Project page: https://ofir1080.github.io/PromptonomyViT/",https://openaccess.thecvf.com/content/WACV2024/html/Herzig_PromptonomyViT_Multi-Task_Prompt_Learning_Improves_Video_Transformers_Using_Synthetic_Scene_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Herzig_PromptonomyViT_Multi-Task_Prompt_Learning_Improves_Video_Transformers_Using_Synthetic_Scene_WACV_2024_paper.pdf,https://ofir1080.github.io/PromptonomyViT,,,main,Poster,https://ieeexplore.ieee.org/document/10483731/,"['Graphics', 'Solid modeling', 'Three-dimensional displays', 'Annotations', 'Transformers', 'Multitasking', 'Task analysis']","['Multi-task Learning', 'Synthetic Scenes', 'Video Transformer', 'Multiple Tasks', 'Action Recognition', 'Image Synthesis', 'Real Task', 'Real Videos', 'Video Understanding', 'Action Recognition Model', 'Natural Language', 'Real-world Data', 'Semantic Segmentation', 'Depth Map', 'Individual Tasks', 'Segmentation Map', 'Depth Estimation', 'Mean Average Precision', 'Machine Vision', 'Video Dataset', 'Auxiliary Task', 'Action Detection', 'Prediction Head', 'Vision Transformer', 'Action Recognition Task', '3D Joint', 'Video Modeling', 'Number Of Videos', 'Domain Gap', 'Real Domain']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding']",2,"Action recognition models have achieved impressive results by incorporating scene-level annotations, such as objects, their relations, 3D structure, and more. However, obtaining annotations of scene structure for videos requires a significant amount of effort to gather and annotate, making these methods expensive to train. In contrast, synthetic datasets generated by graphics engines provide powerful alternatives for generating scene-level annotations across multiple tasks. In this work, we propose an approach to leverage synthetic scene data for improving video understanding. We present a multi-task prompt learning approach for video transformers, where a shared video transformer backbone is enhanced by a small set of specialized parameters for each task. Specifically, we add a set of ""task prompts"", each corresponding to a different task, and let each prompt predict task-related annotations. This design allows the model to capture information shared among synthetic scene tasks as well as information shared between synthetic scene tasks and a real video downstream task throughout the entire network. We refer to this approach as ""Promptonomy"", since the prompts model task-related structure. We propose the PromptonomyViT model (PViT), a video transformer that incorporates various types of scene-level information from synthetic data using the ""Promptonomy"" approach. PViT shows strong performance improvements on multiple video understanding tasks and datasets. Project page: https://ofir1080.github.io/PromptonomyViT"
Prototype Learning for Explainable Brain Age Prediction,"Linde S. Hesse, Nicola K. Dinsdale, Ana I. L. Namburete","OMNI Lab, Department of Computer Science, University of Oxford, UK; Wellcome Centre for Integrative Neuroimaging, FMRIB, University of Oxford, UK; OMNI Lab, Department of Computer Science, University of Oxford, UK",100.0,UK,0.0,,"The lack of explainability of deep learning models limits the adoption of such models in clinical practice. Prototype-based models can provide inherent explainable predictions, but these have predominantly been designed for classification tasks, despite many important tasks in medical imaging being continuous regression problems. Therefore, in this work, we present ExPeRT: an explainable prototype-based model specifically designed for regression tasks. Our proposed model makes a sample prediction from the distances to a set of learned prototypes in latent space, using a weighted mean of prototype labels. The distances in latent space are regularized to be relative to label differences, and each of the prototypes can be visualized as a sample from the training set. The image-level distances are further constructed from patch-level distances, in which the patches of both images are structurally matched using optimal transport. This thus provides an example-based explanation with patch-level detail at inference time. We demonstrate our proposed model for brain age prediction on two imaging datasets: adult MR and fetal ultrasound. Our approach achieved state-of-the-art prediction performance while providing insight into the model's reasoning process.",https://openaccess.thecvf.com/content/WACV2024/html/Hesse_Prototype_Learning_for_Explainable_Brain_Age_Prediction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hesse_Prototype_Learning_for_Explainable_Brain_Age_Prediction_WACV_2024_paper.pdf,,,2306.09858,main,Poster,https://ieeexplore.ieee.org/document/10484288/,"['Training', 'Measurement', 'Visualization', 'Ultrasonic imaging', 'Magnetic resonance imaging', 'Prototypes', 'Predictive models']","['Brain Aging', 'Age Prediction', 'Prototype Learning', 'Brain Age Prediction', 'Training Set', 'Deep Learning', 'Medical Imaging', 'Predictive Performance', 'Important Task', 'Latent Space', 'Image Patches', 'Prenatal Ultrasound', 'Optimal Transport', 'Reasoning Process', 'Model Explainability', 'Magnetic Resonance Imaging', 'Feature Space', 'Negative Samples', 'Top Panel', 'Confusion Matrix', 'Consistency Loss', 'Metric Learning', 'Ultrasound Imaging', 'Contrastive Loss', 'Triplet Loss', 'Label Space', 'Geodesic Distance', 'Image Representation', 'Cost Matrix', 'Geometric Transformation']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",1,"The lack of explainability of deep learning models limits the adoption of such models in clinical practice. Prototype-based models can provide inherent explainable predictions, but these have predominantly been designed for classification tasks, despite many important tasks in medical imaging being continuous regression problems. Therefore, in this work, we present ExPeRT: an explainable prototype-based model specifically designed for regression tasks. Our proposed model makes a sample prediction from the distances to a set of learned prototypes in latent space, using a weighted mean of prototype labels. The distances in latent space are regularized to be relative to label differences, and each of the prototypes can be visualized as a sample from the training set. The image-level distances are further constructed from patch-level distances, in which the patches of both images are structurally matched using optimal transport. This thus provides an example-based explanation with patch-level detail at inference time. We demonstrate our proposed model for brain age prediction on two imaging datasets: adult MR and fetal ultrasound. Our approach achieved state-of-the-art prediction performance while providing insight into the model’s reasoning process."
Prototypical Contrastive Network for Imbalanced Aerial Image Segmentation,"Keiller Nogueira, Mayara Maezano Faita-Pinheiro, Ana Paula Marques Ramos, Wesley Nunes Gonçalves, José Marcato Junior, Jefersson A. dos Santos","University of Sheffield, Sheffield, S10 2TN, England, UK; University of Stirling, Stirling, FK9 4LA, Scotland, UK; Federal University of Mato Grosso do Sul (UFMS), Campo Grande, Mato Grosso do Sul, Brazil; University of Western São Paulo (UNOESTE), Presidente Prudente, São Paulo, Brazil; São Paulo State University (UNESP), Presidente Prudente, São Paulo, Brazil",100.0,"Brazil, UK",0.0,,"Binary segmentation is the main task underpinning several remote sensing applications, which are particularly interested in identifying and monitoring a specific category/object. Although extremely important, such a task has several challenges, including huge intra-class variance for the background and data imbalance. Furthermore, most works tackling this task partially or completely ignore one or both of these challenges and their developments. In this paper, we propose a novel method to perform imbalanced binary segmentation of remote sensing images based on deep networks, prototypes, and contrastive loss. The proposed approach allows the model to focus on learning the foreground class while alleviating the class imbalance problem by allowing it to concentrate on the most difficult background examples. The results demonstrate that the proposed method outperforms state-of-the-art techniques for imbalanced binary segmentation of remote sensing images while taking much less training time.",https://openaccess.thecvf.com/content/WACV2024/html/Nogueira_Prototypical_Contrastive_Network_for_Imbalanced_Aerial_Image_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nogueira_Prototypical_Contrastive_Network_for_Imbalanced_Aerial_Image_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484079/,"['Training', 'Representation learning', 'Image segmentation', 'Semantics', 'Prototypes', 'Stars', 'Transformers']","['Deep Network', 'Training Time', 'Remote Sensing', 'Huge Variety', 'Class Imbalance', 'Imbalanced Data', 'Remote Sensing Images', 'Contrastive Loss', 'Binary Segmentation', 'Class Imbalance Problem', 'False Positive', 'Analytical Results', 'False Negative', 'Cost Function', 'Negative Samples', 'Classification Of Samples', 'Precision And Recall', 'Representation Learning', 'Latent Space', 'GB Memory', 'Self-supervised Learning', 'Hard Examples', 'Positive Class', 'Experiments In This Work', 'Focal Loss', 'Semantic Understanding', 'Imbalance Issue', 'Relative Gain', 'Unbalanced Data', 'Mining Techniques']","['Applications', 'Remote Sensing', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Binary segmentation is the main task underpinning several remote sensing applications, which are particularly interested in identifying and monitoring a specific category/object. Although extremely important, such a task has several challenges, including huge intra-class variance for the background and data imbalance. Furthermore, most works tackling this task partially or completely ignore one or both of these challenges and their developments. In this paper, we propose a novel method to perform imbalanced binary segmentation of remote sensing images based on deep networks, prototypes, and contrastive loss. The proposed approach allows the model to focus on learning the foreground class while alleviating the class imbalance problem by allowing it to concentrate on the most difficult background examples. The results demonstrate that the proposed method outperforms state-of-the-art techniques for imbalanced binary segmentation of remote sensing images while taking much less training time."
ProxEdit: Improving Tuning-Free Real Image Editing With Proximal Guidance,"Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Anastasis Stathopoulos, Xiaoxiao He, Yuxiao Chen, Di Liu, Qilong Zhangli, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, Dimitris Metaxas",MIT-IBM AI Lab; UT Austin; Laval University; New York University; Rutgers University,100.0,"Canada, USA",0.0,,"DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose proximal guidance and incorporate it to NPI with cross-attention control. We enhance NPI with a regularization term and inversion guidance, which reduces artifacts while capitalizing on its training-free nature. Additionally, we extend the concepts to incorporate mutual self-attention control, enabling geometry and layout alterations in the editing process. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead.",https://openaccess.thecvf.com/content/WACV2024/html/Han_ProxEdit_Improving_Tuning-Free_Real_Image_Editing_With_Proximal_Guidance_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Han_ProxEdit_Improving_Tuning-Free_Real_Image_Editing_With_Proximal_Guidance_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483814/,"['Geometry', 'Computer vision', 'Closed-form solutions', 'Layout', 'Process control', 'Trajectory', 'Computational efficiency']","['Image Editing', 'Real Image Editing', 'Editing Process', 'Gradient Descent', 'Ordinary Differential Equations', 'Exact Solution', 'Diffusion Model', 'Source Images', 'Peak Signal-to-noise Ratio', 'Stochastic Differential Equations', 'Gradient Descent Step']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",9,"DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose proximal guidance and incorporate it to NPI with cross-attention control. We enhance NPI with a regularization term and inversion guidance, which reduces artifacts while capitalizing on its training-free nature. Additionally, we extend the concepts to incorporate mutual self-attention control, enabling geometry and layout alterations in the editing process. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead."
Pruning From Scratch via Shared Pruning Module and Nuclear Norm-Based Regularization,"Donghyeon Lee, Eunho Lee, Youngbae Hwang","Dept. of Intelligent Systems and Robotics, Chungbuk National University",100.0,South Korea,0.0,,"Most pruning methods focus on determining redundant channels from the pre-trained model. However, they overlook the cost of training large networks and the significance of selecting channels for effective reconfiguration. In this paper, we present a ""pruning from scratch"" framework that considers reconfiguration and expression capacity. Our Shared Pruning Module (SPM) handles a channel alignment problem in residual blocks for lossless reconfiguration after pruning. Moreover, we introduce nuclear norm-based regularization to preserve the representability of large networks during the pruning process. By combining it with MACs-based regularization, we achieve an efficient and powerful pruned network while compressing towards target MACs. The experimental results demonstrate that our method prunes redundant channels effectively to enhance representation capacity of the network. Our approach compresses ResNet50 on ImageNet without requiring additional resources, achieving a top-1 accuracy of 75.25% with only 41% of the original model's MACs. Code is available at https://github.com/jsleeg98/NuSPM.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Pruning_From_Scratch_via_Shared_Pruning_Module_and_Nuclear_Norm-Based_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Pruning_From_Scratch_via_Shared_Pruning_Module_and_Nuclear_Norm-Based_WACV_2024_paper.pdf,,https://github.com/jsleeg98/NuSPM,,main,Poster,https://ieeexplore.ieee.org/document/10483576/,"['Training', 'Computer vision', 'Image coding', 'Costs', 'Codes', 'Computational modeling', 'Complexity theory']","['Pruning Module', 'Large Networks', 'ImageNet', 'Residual Block', 'Top-1 Accuracy', 'Pruning Process', 'Pruning Method', 'Convolutional Neural Network', 'Convolutional Layers', 'Outer Layer', 'Feature Maps', 'Inner Layer', 'Performance Degradation', 'Network Efficiency', 'Original Network', 'Output Channels', 'Addition Operations', 'Pre-trained Network', 'L1-norm', 'Output Feature Map', 'Nuclear Norm', 'Shortcut Connection', 'Network Pruning', 'Regularization Loss', 'Reconfiguration Process', 'Fewer Channels', 'Blocking Layer', 'Network Size', 'Nuclear Loss', 'Computational Complexity']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Embedded sensing / real-time techniques']",,"Most pruning methods focus on determining redundant channels from the pre-trained model. However, they overlook the cost of training large networks and the significance of selecting channels for effective reconfiguration. In this paper, we present a ""pruning from scratch"" framework that considers reconfiguration and expression capacity. Our Shared Pruning Module (SPM) handles a channel alignment problem in residual blocks for lossless reconfiguration after pruning. Moreover, we introduce nuclear norm-based regularization to preserve the representability of large networks during the pruning process. By combining it with MACs-based regularization, we achieve an efficient and powerful pruned network while compressing towards target MACs. The experimental results demonstrate that our method prunes redundant channels effectively to enhance representation capacity of the network. Our approach compresses ResNet50 on ImageNet without requiring additional resources, achieving a top-1 accuracy of 75.25% with only 41% of the original model’s MACs. Code is available at https://github.com/jsleeg98/NuSPM."
PsyMo: A Dataset for Estimating Self-Reported Psychological Traits From Gait,"Adrian Cosma, Emilian Radoi","University Politehnica of Bucharest, Bucharest, Romania",100.0,Romania,0.0,,"Psychological trait estimation from external factors such as movement and appearance is a challenging and long-standing problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totaling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.",https://openaccess.thecvf.com/content/WACV2024/html/Cosma_PsyMo_A_Dataset_for_Estimating_Self-Reported_Psychological_Traits_From_Gait_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cosma_PsyMo_A_Dataset_for_Estimating_Self-Reported_Psychological_Traits_From_Gait_WACV_2024_paper.pdf,,,2308.10631,main,Poster,https://ieeexplore.ieee.org/document/10484507/,"['Legged locomotion', 'Three-dimensional displays', 'Biological system modeling', 'Psychology', 'Estimation', 'Benchmark testing', 'Fatigue']","['Psychological Traits', 'Mental Health', '3D Mesh', 'Walking Pattern', 'Estimates For Traits', 'Psychological Questionnaires', 'Embodiment Theory', 'Drop-in Replacement', 'Human Behavior', 'Personality Traits', 'Neuroticism', 'Walking Speed', 'Extraversion', '4-point Likert Scale', 'Five-factor Model', 'Pose Estimation', 'Signs Of Distress', 'Gait Analysis', 'Big Five Personality', 'General Health Questionnaire', 'Ordinal Categories', 'Verbal Aggression', 'Romanian Population', 'Psychological Help', 'Acute Fatigue', 'Normal Walking', 'Rosenberg Self-Esteem Scale', 'Kinect Sensor', 'Factor Scores', 'Gait Characteristics']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Applications', 'Psychology and cognitive science']",4,"Psychological trait estimation from external factors such as movement and appearance is a challenging and longstanding problem in psychology, and is principally based on the psychological theory of embodiment. To date, attempts to tackle this problem have utilized private small-scale datasets with intrusive body-attached sensors. Potential applications of an automated system for psychological trait estimation include estimation of occupational fatigue and psychology, and marketing and advertisement. In this work, we propose PsyMo (Psychological traits from Motion), a novel, multi-purpose and multi-modal dataset for exploring psychological cues manifested in walking patterns. We gathered walking sequences from 312 subjects in 7 different walking variations and 6 camera angles. In conjunction with walking sequences, participants filled in 6 psychological questionnaires, totaling 17 psychometric attributes related to personality, self-esteem, fatigue, aggressiveness and mental health. We propose two evaluation protocols for psychological trait estimation. Alongside the estimation of self-reported psychological traits from gait, the dataset can be used as a drop-in replacement to benchmark methods for gait recognition. We anonymize all cues related to the identity of the subjects and publicly release only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes."
Query-Guided Attention in Vision Transformers for Localizing Objects Using a Single Sketch,"Aditay Tripathi, Anand Mishra, Anirban Chakraborty",Indian Institute of Science; Indian Institute of Technology Jodhpur,100.0,India,0.0,,"In this study, we explore sketch-based object localization on natural images. Given a crude hand-drawn object sketch, the task is to locate all instances of that object in the target image. This problem proves difficult due to the abstract nature of hand-drawn sketches, variations in the style and quality of sketches, and the large domain gap between the sketches and the natural images. Existing solutions address this using attention-based frameworks to merge query information into image features. Yet, these methods often integrate query features after independently learning image features, causing inadequate alignment and as a result incorrect localization. In contrast, we propose a novel sketch-guided vision transformer encoder that uses cross-attention after each block of the transformer-based image encoder to learn query-conditioned image features, leading to stronger alignment with the query sketch. Further, at the decoder's output, object and sketch features are refined better to align the representation of objects with the sketch query, thereby improving localization. The proposed model also generalizes to the object categories not seen during training, as the target image features learned by the proposed model are query-aware. Our framework can utilize multiple sketch queries via a trainable novel sketch fusion strategy. The model is evaluated on the images from the public benchmark, MS-COCO, using the sketch queries from QuickDraw! and Sketchy datasets. Compared with existing localization methods, the proposed approach gives a 6.6% and 8.0% improvement in mAP for seen objects using sketch queries from QuickDraw! and Sketchy datasets, respectively, and a 12.2% improvement in AP@50 for large objects that are 'unseen' during training.",https://openaccess.thecvf.com/content/WACV2024/html/Tripathi_Query-Guided_Attention_in_Vision_Transformers_for_Localizing_Objects_Using_a_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tripathi_Query-Guided_Attention_in_Vision_Transformers_for_Localizing_Objects_Using_a_WACV_2024_paper.pdf,,https://vcl-iisc.github.io/locformer/,2303.08784,main,Poster,https://ieeexplore.ieee.org/document/10484208/,"['Location awareness', 'Training', 'Computer vision', 'Codes', 'Computational modeling', 'Benchmark testing', 'Transformers']","['Object Location', 'Vision Transformer', 'Single Sketch', 'Image Features', 'Image Object', 'Natural Images', 'Target Image', 'Object Features', 'Fusion Strategy', 'Variety Of Styles', 'Domain Gap', 'Image Encoder', 'Multiple Queries', 'Query Features', 'mAP Improvement', 'Localization Accuracy', 'Object Detection', 'Fluidic', 'Attention Mechanism', 'Bounding Box', 'Decoder Output', 'Feature Alignment', 'Region Proposal Network', 'Object Instances', 'MS COCO Dataset', 'Representation Learning', 'Encoder-decoder Model', 'Region Proposal', 'Computer Vision Research', 'Localization Performance']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",1,"In this study, we explore sketch-based object localization on natural images. Given a crude hand-drawn object sketch, the task is to locate all instances of that object in the target image. This problem proves difficult due to the abstract nature of hand-drawn sketches, variations in the style and quality of sketches, and the large domain gap between the sketches and the natural images. Existing solutions address this using attention-based frameworks to merge query information into image features. Yet, these methods often integrate query features after independently learning image features, causing inadequate alignment and as a result incorrect localization. In contrast, we propose a novel sketch-guided vision transformer encoder that uses cross-attention after each block of the transformer-based image encoder to learn query-conditioned image features, leading to stronger alignment with the query sketch. Further, at the decoder’s output, object and sketch features are refined better to align the representation of objects with the sketch query, thereby improving localization. The proposed model also generalizes to the object categories not seen during training, as the target image features learned by the proposed model are query-aware. Our framework can utilize multiple sketch queries via a trainable novel sketch fusion strategy. The model is evaluated on the images from the public benchmark, MS-COCO, using the sketch queries from QuickDraw! and Sketchy datasets. Compared with existing localization methods, the proposed approach gives a 6.6% and 8.0% improvement in mAP for seen objects using sketch queries from QuickDraw! and Sketchy datasets, respectively, and a 12.2% improvement in AP@50 for large objects that are ‘unseen’ during training. The code is available at https://vcl-iisc.github.io/locformer/."
RADIO: Reference-Agnostic Dubbing Video Synthesis,"Dongyeun Lee, Chaewon Kim, Sangjoon Yu, Jaejun Yoo, Gyeong-Moon Park",Klleon AI Research; UNIST; Kyung Hee University,66.66666666666666,South Korea,33.33333333333334,USA,"One of the most challenging problems in audio-driven talking head generation is achieving high-fidelity detail while ensuring precise synchronization. Given only a single reference image, extracting meaningful identity attributes becomes even more challenging, often causing the network to mirror the facial and lip structures too closely. To address these issues, we introduce RADIO, a framework engineered to yield high-quality dubbed videos regardless of the pose or expression in reference images. The key is to modulate the decoder layers using latent space composed of audio and reference features. Additionally, we incorporate ViT blocks into the decoder to emphasize high-fidelity details, especially in the lip region. Our experimental results demonstrate that RADIO displays high synchronization without the loss of fidelity. Especially in harsh scenarios where the reference frame deviates significantly from the ground truth, our method outperforms state-of-the-art methods, highlighting its robustness.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_RADIO_Reference-Agnostic_Dubbing_Video_Synthesis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_RADIO_Reference-Agnostic_Dubbing_Video_Synthesis_WACV_2024_paper.pdf,,,2309.01950,main,Poster,https://ieeexplore.ieee.org/document/10484196/,"['Shape', 'Lips', 'Modulation', 'Robustness', 'Decoding', 'Synchronization', 'Mirrors']","['Reference Frame', 'Single Image', 'Latent Space', 'Reference Image', 'Decoder Layer', 'High Synchrony', 'High-quality Video', 'Vision Transformer', 'Precise Synchronization', 'Loss Of Fidelity', 'Lip Region', 'Raw Images', 'Target Image', 'Source Images', 'Consecutive Frames', 'Content Features', 'Intermediate Features', 'Attention Map', 'Attention Layer', 'Target Frame', 'Style Features', 'Lip-sync', 'Lip Movements', 'Face Alignment', 'Audio Clips', 'Decoder Structure', 'Half Of The Face', 'Head Pose']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"One of the most challenging problems in audio-driven talking head generation is achieving high-fidelity detail while ensuring precise synchronization. Given only a single reference image, extracting meaningful identity attributes becomes even more challenging, often causing the network to mirror the facial and lip structures too closely. To address these issues, we introduce RADIO, a framework engineered to yield high-quality dubbed videos regardless of the pose or expression in reference images. The key is to modulate the decoder layers using latent space composed of audio and reference features. Additionally, we incorporate ViT blocks into the decoder to emphasize high-fidelity details, especially in the lip region. Our experimental results demonstrate that RADIO displays high synchronization without the loss of fidelity. Especially in harsh scenarios where the reference frame deviates significantly from the ground truth, our method outperforms state-of-the-art methods, highlighting its robustness."
REALM: Robust Entropy Adaptive Loss Minimization for Improved Single-Sample Test-Time Adaptation,"Skyler Seto, Barry-John Theobald, Federico Danieli, Navdeep Jaitly, Dan Busbridge",Apple,0.0,,100.0,USA,"Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating its effectiveness.",https://openaccess.thecvf.com/content/WACV2024/html/Seto_REALM_Robust_Entropy_Adaptive_Loss_Minimization_for_Improved_Single-Sample_Test-Time_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Seto_REALM_Robust_Entropy_Adaptive_Loss_Minimization_for_Improved_Single-Sample_Test-Time_WACV_2024_paper.pdf,,,2309.03964,main,Poster,https://ieeexplore.ieee.org/document/10483927/,"['Training', 'Adaptation models', 'Computer vision', 'Limiting', 'Training data', 'Minimization', 'Entropy']","['Test-time Adaptation', 'Loss Function', 'Training Data', 'Domain Shift', 'Minimum Entropy', 'Update Procedure', 'Slow Adaptation', 'Self-paced Learning', 'Data Sources', 'Hyperparameters', 'Deep Neural Network', 'Additional Details', 'Gaussian Noise', 'Part Of Network', 'Batch Of Samples', 'Unlabeled Data', 'High Entropy', 'Domain Adaptation', 'Target Distribution', 'Stochastic Differential Equations', 'Number Of Updates', 'Impulsive Noise', 'Gradient Update', 'Adaptive Step', 'Start Of Training', 'Loss Threshold', 'Adaptive Sampling', 'Domain Generalization', 'Validation Set', 'Source Model']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Fully-test-time adaptation (F-TTA) can mitigate performance loss due to distribution shifts between train and test data (1) without access to the training data, and (2) without knowledge of the model training procedure. In online F-TTA, a pre-trained model is adapted using a stream of test samples by minimizing a self-supervised objective, such as entropy minimization. However, models adapted with online using entropy minimization, are unstable especially in single sample settings, leading to degenerate solutions, and limiting the adoption of TTA inference strategies. Prior works identify noisy, or unreliable, samples as a cause of failure in online F-TTA. One solution is to ignore these samples, which can lead to bias in the update procedure, slow adaptation, and poor generalization. In this work, we present a general framework for improving robustness of F-TTA to these noisy samples, inspired by self-paced learning and robust loss functions. Our proposed approach, Robust Entropy Adaptive Loss Minimization (REALM), achieves better adaptation accuracy than previous approaches throughout the adaptation process on corruptions of CIFAR-10 and ImageNet-1K, demonstrating its effectiveness."
RGB-D Mapping and Tracking in a Plenoxel Radiance Field,"Andreas L. Teigen, Yeonsoo Park, Annette Stahl, Rudolf Mester","Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Mobiltech, Seoul, Republic of Korea",50.0,Norway,50.0,South Korea,"The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git",https://openaccess.thecvf.com/content/WACV2024/html/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.pdf,,https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git,,main,Poster,https://ieeexplore.ieee.org/document/10484344/,"['Solid modeling', 'Visualization', 'Three-dimensional displays', 'Simultaneous localization and mapping', 'Rendering (computer graphics)', 'Mathematical models', 'Data models']","['Radiance Field', 'RGB-D Mapping', 'Neural Network', '3D Reconstruction', 'Density Map', 'Depth Camera', 'Tracking Task', 'Map Tasks', '3D Model Reconstruction', 'View Synthesis', 'RGB-D Data', 'Root Mean Square Error', 'Partial Differential', 'Image Information', 'RGB Images', 'Mapping Algorithm', 'Tracking Accuracy', 'Tracking Algorithm', 'Color Values', 'Color Gradient', 'Simultaneous Localization And Mapping', 'Camera Pose', 'RGB-D Sensor', 'Geometric Loss', 'Voxel Grid', 'Volume Rendering', 'Catastrophic Forgetting', 'Trilinear Interpolation', 'Relative Pose', 'Camera Center']","['Algorithms', '3D computer vision', 'Applications', 'Robotics', 'Applications', 'Virtual / augmented reality']",3,"The widespread adoption of Neural Radiance Fields (NeRFs) have ensured significant advances in the domain of novel view synthesis in recent years. These models capture a volumetric radiance field of a scene, creating highly convincing, dense, photorealistic models through the use of simple, differentiable rendering equations. Despite their popularity, these algorithms suffer from severe ambiguities in visual data inherent to the RGB sensor, which means that although images generated with view synthesis can visually appear very believable, the underlying 3D model will often be wrong. This considerably limits the usefulness of these models in practical applications like Robotics and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise would be of significant value. In this paper, we present the vital differences between view synthesis models and 3D reconstruction models. We also comment on why a depth sensor is essential for modeling accurate geometry in general outward-facing scenes using the current paradigm of novel view synthesis methods. Focusing on the structure-from-motion task, we practically demonstrate this need by extending the Plenoxel radiance field model: Presenting an analytical differential approach for dense mapping and tracking with radiance fields based on RGB-D data without a neural network. Our method achieves state-of-the-art results in both mapping and tracking tasks, while also being faster than competing neural network-based approaches. The code is available at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git."
RGB-X Object Detection via Scene-Specific Fusion Modules,"Sri Aditya Deevi, Connor Lee, Lu Gan, Sushruth Nagesh, Gaurav Pandey, Soon-Jo Chung",California Institute of Technology; Ford Motor Company,50.0,USA,50.0,USA,"Multimodal deep sensor fusion has the potential to enable autonomous vehicles to visually understand their surrounding environments in all weather conditions. However, existing deep sensor fusion methods usually employ convoluted architectures with intermingled multimodal features, requiring large coregistered multimodal datasets for training. In this work, we present an efficient and modular RGB-X fusion network that can leverage and fuse pretrained single-modal models via scene-specific fusion modules, thereby enabling joint input-adaptive network architectures to be created using small, coregistered multimodal datasets. Our experiments demonstrate the superiority of our method compared to existing works on RGB-thermal and RGB-gated datasets, performing fusion using only a small amount of additional parameters. Our code is available at https://github.com/dsriaditya999/RGBXFusion.",https://openaccess.thecvf.com/content/WACV2024/html/Deevi_RGB-X_Object_Detection_via_Scene-Specific_Fusion_Modules_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Deevi_RGB-X_Object_Detection_via_Scene-Specific_Fusion_Modules_WACV_2024_paper.pdf,,https://github.com/dsriaditya999/RGBXFusion,,main,Poster,https://ieeexplore.ieee.org/document/10484456/,"['Training', 'Computer vision', 'Fuses', 'Lighting', 'Object detection', 'Sensor fusion', 'Network architecture']","['Object Detection', 'Autonomous Vehicles', 'Fusion Method', 'Fusion Network', 'Network Modularity', 'Training Data', 'Convolutional Neural Network', 'Online Learning', 'Bounding Box', 'Image Pairs', 'RGB Images', 'Thermal Characteristics', 'Attention Module', 'Spatial Attention', 'Pre-trained Network', 'Self-driving', 'Channel Attention', 'Scene Classification', 'Class Activation Maps', 'Overcast', 'RGB Features', 'Neural Architecture Search', 'Sensor Modalities', 'Detection Head', 'Architecture For Detection', 'Object Detection Results', 'Feature Maps', 'Inference Time', 'Object Detection Dataset', 'Single Shot Detector']","['Applications', 'Autonomous Driving', 'Applications', 'Robotics']",3,"Multimodal deep sensor fusion has the potential to enable autonomous vehicles to visually understand their surrounding environments in all weather conditions. However, existing deep sensor fusion methods usually employ convoluted architectures with intermingled multimodal features, requiring large coregistered multimodal datasets for training. In this work, we present an efficient and modular RGB-X fusion network that can leverage and fuse pre-trained single-modal models via scene-specific fusion modules, thereby enabling joint input-adaptive network architectures to be created using small, coregistered multimodal datasets. Our experiments demonstrate the superiority of our method compared to existing works on RGB-thermal and RGB-gated datasets, performing fusion using only a small amount of additional parameters. Our code is available at https://github.com/dsriaditya999/RGBXFusion."
RGBT-Dog: A Parametric Model and Pose Prior for Canine Body Analysis Data Creation,"Jake Deane, Sinéad Kearney, Kwang In Kim, Darren Cosker",POSTECH; University of Bath,100.0,"South Korea, UK",0.0,,"While there exists a great deal of labeled in-the-wild human data, the same is not true for animals. Manually creating new labels for the full range of animal species would take years of effort from the community. We are also now seeing the emerging potential for computer vision methods in areas like animal conservation, which is an additional motivation for this direction of research. Key to our approach is the ability to easily generate as many labeled training images as we desire across a range of different modalities. To achieve this, we present a new large scale canine motion capture dataset and parametric canine body and texture model. These are used to produce the first large scale, multi-domain, multi-task dataset for canine body analysis comprising of detailed synthetic labels on both real images and fully synthetic images in a range of realistic poses. We also introduce the first pose prior for animals in the form of a variational pose prior for canines which is used to fit the parametric model to images of canines. We demonstrate the effectiveness of our labels for training computer vision models on tasks such as parts-based segmentation and pose estimation and show such models can generalise to other animal species without additional training.",https://openaccess.thecvf.com/content/WACV2024/html/Deane_RGBT-Dog_A_Parametric_Model_and_Pose_Prior_for_Canine_Body_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Deane_RGBT-Dog_A_Parametric_Model_and_Pose_Prior_for_Canine_Body_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484497/,"['Training', 'Computer vision', 'Animals', 'Computational modeling', 'Multitasking', 'Motion capture', 'Data models']","['Model Parameters', 'Pose Priors', 'Animal Species', 'Computer Vision', 'Motion Capture', 'Pose Estimation', 'Synthetic Images', 'Body Model', 'Large Motion', 'Range Of Animal Species', 'Animal Conservation', 'Intersection Over Union', 'Shape Parameter', 'Body Shape', 'Latent Space', 'Labeled Data', '3D Shape', 'Domain Adaptation', 'Fitting Process', 'Reconstruction Loss', '3D Pose', 'Pose Parameters', 'Part Segmentation', 'Shape Space', '2D Keypoints', 'Texture Map', 'Human Pose Estimation', 'Set Of Shapes', 'Axis Angle', 'Texture Parameters']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Animals / Insects']",,"While there exists a great deal of labeled in-the-wild human data, the same is not true for animals. Manually creating new labels for the full range of animal species would take years of effort from the community. We are also now seeing the emerging potential for computer vision methods in areas like animal conservation, which is an additional motivation for this direction of research. Key to our approach is the ability to easily generate as many labeled training images as we desire across a range of different modalities. To achieve this, we present a new large scale canine motion capture dataset and parametric canine body and texture model. These are used to produce the first large scale, multi-domain, multi-task dataset for canine body analysis comprising of detailed synthetic labels on both real images and fully synthetic images in a range of realistic poses. We also introduce the first pose prior for animals in the form of a variational pose prior for canines which is used to fit the parametric model to images of canines. We demonstrate the effectiveness of our labels for training computer vision models on tasks such as parts-based segmentation and pose estimation and show such models can generalise to other animal species without additional training."
RIMeshGNN: A Rotation-Invariant Graph Neural Network for Mesh Classification,"Bahareh Shakibajahromi, Edward Kim, David E. Breen","Department of Computer Science, Drexel University, Philadelphia, PA, USA",100.0,USA,0.0,,"Shape analysis tasks, including mesh classification, segmentation, and retrieval demonstrate symmetries in Euclidean space and should be invariant to geometric transformations such as rotation and translation. However, existing methods in mesh analysis often rely on extensive data augmentation and more complex analysis models to handle 3D rotations. Despite these efforts, rotation invariance is not guaranteed, which can significantly reduce accuracy when test samples undergo arbitrary rotations, because the analysis method struggles to generalize to the unknown orientations of the test samples. To address these challenges, our work presents a novel approach that employs graph neural networks (GNNs) to analyze mesh-structured data. Our proposed GNN layer, aggregation function, and local pooling layer are equivariant to the rotation, reflection and translation of 3D shapes, making them suitable building blocks for our proposed rotation-invariant network for the classification of mesh models. Therefore, our proposed approach does not need rotation augmentation, and we can maintain accuracy even when test samples undergo arbitrary rotations. Extensive experiments on various datasets demonstrate that our methods achieve state-of-the-art performance.",https://openaccess.thecvf.com/content/WACV2024/html/Shakibajahromi_RIMeshGNN_A_Rotation-Invariant_Graph_Neural_Network_for_Mesh_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shakibajahromi_RIMeshGNN_A_Rotation-Invariant_Graph_Neural_Network_for_Mesh_Classification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483674/,"['Representation learning', 'Training', 'Solid modeling', 'Analytical models', 'Three-dimensional displays', 'Shape', 'Graph neural networks']","['Neural Network', 'Classification Network', 'Neural Network Classifier', 'Graph Neural Networks', 'Test Samples', 'Data Augmentation', 'Pooling Layer', 'Euclidean Space', 'Shape Analysis', '3D Shape', 'Aggregation Function', 'Rotation Invariance', 'Mesh Model', 'Geometric Transformation', '3D Rotation', 'Arbitrary Rotation', 'Deep Learning', 'Convolutional Neural Network', 'Normal Vector', 'Point Cloud', 'Dihedral Angle', 'Edge Features', 'Invariant Features', 'Node Features', '3D Mesh', 'Graph Attention Network', 'Node Embeddings', 'Grid Graph', 'Equivalent Properties', 'Entire Graph']","['Algorithms', '3D computer vision']",,"Shape analysis tasks, including mesh classification, segmentation, and retrieval demonstrate symmetries in Euclidean space and should be invariant to geometric transformations such as rotation and translation. However, existing methods in mesh analysis often rely on extensive data augmentation and more complex analysis models to handle 3D rotations. Despite these efforts, rotation invariance is not guaranteed, which can significantly reduce accuracy when test samples undergo arbitrary rotations, because the analysis method struggles to generalize to the unknown orientations of the test samples. To address these challenges, our work presents a novel approach that employs graph neural networks (GNNs) to analyze mesh-structured data. Our proposed GNN layer, aggregation function, and local pooling layer are equivariant to the rotation, reflection and translation of 3D shapes, making them suitable building blocks for our proposed rotation-invariant network for the classification of mesh models. Therefore, our proposed approach does not need rotation augmentation, and we can maintain accuracy even when test samples undergo arbitrary rotations. Extensive experiments on various datasets demonstrate that our methods achieve state-of-the-art performance."
RMFER: Semi-Supervised Contrastive Learning for Facial Expression Recognition With Reaction Mashup Video,"Yunseong Cho, Chanwoo Kim, Hoseong Cho, Yunhoe Ku, Eunseo Kim, Muhammadjon Boboev, Joonseok Lee, Seungryul Baek","UNIST, SNOW Corp.; UNIST; Seoul National University",100.0,South Korea,0.0,,"Facial expression recognition (FER) has greatly benefited from deep learning but still faces challenges in dataset collection due to the nuanced nature of facial expressions. In this study, we present a novel unlabeled dataset and semi-supervised contrastive learning framework that utilizes Reaction Mashup (RM) videos, a video that includes multiple individuals reacting to the same film. We created a Reaction Mashup dataset (RMset) from these videos. Our framework integrates three distinct modules: A classification module for supervised facial expression categorization, an attention module for inter-sample attention learning, and a contrastive module for attention-based contrastive learning using RMset. We utilize both the classification and attention modules for the initial training, subsequently incorporating the contrastive module to enhance the learning process. Our experiments demonstrate that our method improves feature learning and outperforms state-of-the-art models on three benchmark FER datasets. Codes are available at https://github.com/yunseongcho/RMFER.",https://openaccess.thecvf.com/content/WACV2024/html/Cho_RMFER_Semi-Supervised_Contrastive_Learning_for_Facial_Expression_Recognition_With_Reaction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cho_RMFER_Semi-Supervised_Contrastive_Learning_for_Facial_Expression_Recognition_With_Reaction_WACV_2024_paper.pdf,,https://github.com/yunseongcho/RMFER,,main,Poster,https://ieeexplore.ieee.org/document/10484312/,"['Training', 'Representation learning', 'Mashups', 'Face recognition', 'Supervised learning', 'Self-supervised learning', 'Benchmark testing']","['Facial Expressions', 'Face Recognition', 'Semi-supervised Learning', 'Self-supervised Learning', 'Facial Expression Recognition', 'Mash-up Videos', 'Deep Learning', 'Feature Learning', 'Attention Module', 'Contrastive Module', 'Semi-supervised Learning Framework', 'Training Set', 'Positive Samples', 'Validation Set', 'Softmax', 'Average Accuracy', 'Disgust', 'Benchmark Datasets', 'Bounding Box', 'Unlabeled Data', 'Image X', 'Batch Of Samples', 'Central Loss', 'Face Identity', 'Basic Emotions', 'Inter-class Separability', 'Attention Values', 'Frame Index', 'Face Detection']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Datasets and evaluations']",,"Facial expression recognition (FER) has greatly benefited from deep learning but still faces challenges in dataset collection due to the nuanced nature of facial expressions. In this study, we present a novel unlabeled dataset and semi-supervised contrastive learning framework that utilizes Reaction Mashup (RM) videos, a video that includes multiple individuals reacting to the same film. We created a Reaction Mashup dataset (RMset) from these videos. Our framework integrates three distinct modules: A classification module for supervised facial expression categorization, an attention module for inter-sample attention learning, and a contrastive module for attention-based contrastive learning using RMset. We utilize both the classification and attention modules for the initial training, subsequently incorporating the contrastive module to enhance the learning process. Our experiments demonstrate that our method improves feature learning and outperforms state-of-the-art models on three benchmark FER datasets. Codes are available at https://github.com/yunseongcho/RMFER."
RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection,"Fengyi Wu, Tianfang Zhang, Lei Li, Yian Huang, Zhenming Peng","University of Electronic Science and Technology of China, Chengdu, China; University of Copenhagen, Denmark",100.0,"China, Denmark",0.0,,"Deep learning (DL) networks have achieved remarkable performance in infrared small target detection (ISTD). However, these structures exhibit a deficiency in interpretability and are widely regarded as black boxes, as they disregard domain knowledge in ISTD. To alleviate this issue, this work proposes an interpretable deep network for detecting infrared dim targets, dubbed RPCANet. Specifically, our approach formulates the ISTD task as sparse target extraction, low-rank background estimation, and image reconstruction in a relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the iterative optimization updating steps into a deep-learning framework, time-consuming and complex matrix calculations are replaced by theory-guided neural networks. RPCANet detects targets with clear interpretability and preserves the intrinsic image feature, instead of directly transforming the detection task into a matrix decomposition problem. Extensive experiments substantiate the effectiveness of our deep unfolding framework and demonstrate its trustworthy results, surpassing baseline methods in both qualitative and quantitative evaluations.",https://openaccess.thecvf.com/content/WACV2024/html/Wu_RPCANet_Deep_Unfolding_RPCA_Based_Infrared_Small_Target_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wu_RPCANet_Deep_Unfolding_RPCA_Based_Infrared_Small_Target_Detection_WACV_2024_paper.pdf,,https://github.com/fengyiwu98/RPCANet,2311.00917,main,Poster,https://ieeexplore.ieee.org/document/10483614/,"['Source coding', 'Neural networks', 'Object detection', 'Feature extraction', 'Matrix decomposition', 'Sparse matrices', 'Iterative methods']","['Small Target', 'Neural Network', 'Deep Network', 'Black Box', 'Image Reconstruction', 'Convolutional Neural Network', 'Convolutional Layers', 'Iterative Algorithm', 'Singular Value', 'False Alarm', 'Data-driven Methods', 'Residual Block', 'Lipschitz Continuous', 'False Alarm Rate', 'Stages Of Decomposition', 'Soft Threshold', 'Neural Layers', 'Alternating Direction Method Of Multipliers', 'Nuclear Norm', 'Optimization-based Methods', 'Proximal Operator', 'Reconstruction Module', 'Reconstruction Stage', 'Ranking Function', 'Iterative Solver', 'Target Shape', 'Artificial Neural Network', 'L1-norm', 'F1 Score', 'Fewer Parameters']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Image recognition and understanding']",12,"Deep learning (DL) networks have achieved remarkable performance in infrared small target detection (ISTD). However, these structures exhibit a deficiency in interpretability and are widely regarded as black boxes, as they disregard domain knowledge in ISTD. To alleviate this issue, this work proposes an interpretable deep network for detecting infrared dim targets, dubbed RPCANet. Specifically, our approach formulates the ISTD task as sparse target extraction, low-rank background estimation, and image reconstruction in a relaxed Robust Principle Component Analysis (RPCA) model. By unfolding the iterative optimization updating steps into a deep-learning framework, time-consuming and complex matrix calculations are replaced by theory-guided neural networks. RPCANet detects targets with clear interpretability and preserves the intrinsic image feature, instead of directly transforming the detection task into a matrix decomposition problem. Extensive experiments substantiate the effectiveness of our deep unfolding framework and demonstrate its trustworthy results, surpassing baseline methods in both qualitative and quantitative evaluations. Our source code is available at https://github.com/fengyiwu98/RPCANet."
RS2G: Data-Driven Scene-Graph Extraction and Embedding for Robust Autonomous Perception and Scenario Understanding,"Junyao Wang, Arnav Vaibhav Malawade, Junhong Zhou, Shih-Yuan Yu, Mohammad Abdullah Al Faruque","University of California, Irvine, Irvine, United States, 92697",100.0,USA,0.0,,"Effectively capturing intricate interactions among road users is of critical importance to achieving safe navigation for autonomous vehicles. While graph learning (GL) has emerged as a promising approach to tackle this challenge, existing GL models rely on predefined domain-specific graph extraction rules that often fail in real-world drastically changing scenarios. Additionally, these graph extraction rules severely impede the capability of existing GL methods to generalize knowledge across domains. To address this issue, we propose RoadScene2Graph (RS2G), an innovative autonomous scenario understanding framework with a novel data-driven graph extraction and modeling approach that dynamically captures the diverse relations among road users. Our evaluations demonstrate that on average RS2G outperforms the state-of-the-art (SOTA) rule-based graph extraction method by 4.47% and the SOTA deep learning model by 22.19% in subjective risk assessment. More importantly, RS2G delivers notably better performance in transferring knowledge gained from simulation environments to unseen real-world scenarios.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_RS2G_Data-Driven_Scene-Graph_Extraction_and_Embedding_for_Robust_Autonomous_Perception_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_RS2G_Data-Driven_Scene-Graph_Extraction_and_Embedding_for_Robust_Autonomous_Perception_WACV_2024_paper.pdf,,,2304.08600,main,Poster,https://ieeexplore.ieee.org/document/10483691/,"['Training', 'Deep learning', 'Navigation', 'Roads', 'Computational modeling', 'Transfer learning', 'Transformers']","['Risk Assessment', 'Learning Models', 'Knowledge Transfer', 'Subjective Assessment', 'Graphical Model', 'Autonomous Vehicles', 'Real-world Scenarios', 'Extraction Approach', 'Road Users', 'Rule-based Methods', 'Graph Learning', 'Transformer', 'Convolutional Neural Network', 'Graphical Representation', 'Long Short-term Memory', 'Attention Mechanism', 'Transfer Learning', 'Spatial Model', 'Multilayer Perceptron', 'Real-world Datasets', 'Matthews Correlation Coefficient', 'Node Embeddings', 'Performance In Scenarios', 'Graph Convolutional Network', 'Variational Autoencoder', 'Temporal Model', 'Source Domain', 'Scene Representation', 'Objects In The Scene', 'Binary Classification Task']","['Applications', 'Autonomous Driving', 'Applications', 'Embedded sensing / real-time techniques']",2,"Effectively capturing intricate interactions among road users plays a critical role in achieving safe navigation for autonomous vehicles. While graph learning (GL) has emerged as a promising approach to tackle this challenge, existing GL models rely on predefined domain-specific graph extraction rules and often fail in real-world dynamic scenarios. Additionally, these graph extraction rules severely impede the capability of existing GL methods to generalize knowledge across domains. To address this issue, we propose RoadScene2Graph (RS2G), an innovative autonomous scenario understanding framework with a novel data-driven graph extraction and modeling approach that dynamically captures the diverse relations among road users. Our evaluations show that on average RS2G outperforms the state-of-the-art (SOTA) rule-based graph extraction method by 4.47% and the SOTA deep learning model by 22.19% in subjective risk assessment. RS2G also delivers notably better performance in transferring knowledge gained from simulations to unseen real-world scenarios."
RSMPNet: Relationship Guided Semantic Map Prediction,"Jingwen Sun, Jing Wu, Ze Ji, Yu-Kun Lai","Cardiff University, Cardiff, UK",100.0,UK,0.0,,"In semantic navigation, a top-down map with accurate and complete semantic information is vital to subsequent decision-making. However, due to occlusions and limitations of the robot's field of view (FOV), there are often unobserved areas in the top-down maps. To address this problem, recent works have studied semantic map prediction to complete the top-down maps. In this work, we propose to improve map prediction by integrating relational information. We propose RSMPNet, a relationship-guided semantic map prediction network, which makes use of semantic and spatial relationships to predict unobserved areas from accumulated semantic maps. Specifically, we propose a Relationship Reasoning Layer that includes two modules, namely 1) the Semantic Relationship Graph Reasoning Module (SeGRM) to capture the semantic relationship and 2) the Spatial Relationship Graph Reasoning Module (SpGRM) to utilize the spatial relationship. We also design a semantic relationship enhanced loss to enhance our model to learn semantic relationship information. Experiments show the effectiveness of our proposed network which achieves state-of-the-art performance in semantic map prediction. Our code and datasets are publicly available at https://github.com/jws39/semantic-mapprediction",https://openaccess.thecvf.com/content/WACV2024/html/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sun_RSMPNet_Relationship_Guided_Semantic_Map_Prediction_WACV_2024_paper.pdf,,https://github.com/jws39/semantic-map-prediction,,main,Poster,https://ieeexplore.ieee.org/document/10483910/,"['Computer vision', 'Codes', 'Navigation', 'Aggregates', 'Semantics', 'Decision making', 'Prediction methods']","['Prediction Map', 'Semantic Map', 'Semantic Map Prediction', 'Field Of View', 'Semantic Information', 'Semantic Similarity', 'Semantic Graph', 'Neural Network', 'Feature Maps', 'Depth Images', 'Complete Function', 'Current Observations', 'Graph Convolutional Network', 'Node Features', 'Encoder Layer', 'Part Of The Map', 'U-Net Model', 'Decoder Block']","['Algorithms', 'Image recognition and understanding']",,"In semantic navigation, a top-down map with accurate and complete semantic information is vital to subsequent decision-making. However, due to occlusions and limitations of the robot’s field of view (FOV), there are often unobserved areas in the top-down maps. To address this problem, recent works have studied semantic map prediction to complete the top-down maps. In this work, we propose to improve map prediction by integrating relational information. We propose RSMPNet, a relationship-guided semantic map prediction network, which makes use of semantic and spatial relationships to predict unobserved areas from accumulated semantic maps. Specifically, we propose a Relationship Reasoning Layer that includes two modules, namely 1) the Semantic Relationship Graph Reasoning Module (SeGRM) to capture the semantic relationship and 2) the Spatial Relationship Graph Reasoning Module (SpGRM) to utilize the spatial relationship. We also design a semantic relationship enhanced loss to enhance our model to learn semantic relationship information. Experiments show the effectiveness of our proposed network which achieves state-of-the-art performance in semantic map prediction. Our code and dataset are publicly available at https://github.com/jws39/semantic-map-prediction"
Random Walks for Temporal Action Segmentation With Timestamp Supervision,"Roy Hirsch, Regev Cohen, Tomer Golany, Daniel Freedman, Ehud Rivlin","Unknown; Verily AI, Israel",0.0,,100.0,,"Temporal action segmentation relates to high-level video understanding, commonly formulated as frame-wise classification of untrimmed videos into predefined actions. Fully-supervised deep-learning approaches require dense video annotations which are time and money consuming. Furthermore, the temporal boundaries between consecutive actions typically are not well-defined, leading to inherent ambiguity and inter-rater disagreement. A promising approach to remedy these limitations is timestamp supervision, requiring only one labeled frame per action instance in a training video. In this work, we reformulate the task of temporal segmentation as a graph segmentation problem with weakly-labeled vertices. We introduce an efficient segmentation method based on random walks on graphs, obtained by solving a sparse system of linear equations. Furthermore, the proposed technique can be employed in any one or combination of the following forms: (1) as a standalone solution for generating dense pseudo-labels from timestamps; (2) as a training loss; (3) as a smoothing mechanism given intermediate predictions. Extensive experiments with three datasets (50Salads, Breakfast, GTEA) show that our method competes with state-of-the-art, and allows the identification of regions of uncertainty around action boundaries.",https://openaccess.thecvf.com/content/WACV2024/html/Hirsch_Random_Walks_for_Temporal_Action_Segmentation_With_Timestamp_Supervision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hirsch_Random_Walks_for_Temporal_Action_Segmentation_With_Timestamp_Supervision_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483652/,"['Training', 'Computer vision', 'Adaptation models', 'Uncertainty', 'Smoothing methods', 'Image annotation', 'Predictive models']","['Random Walk', 'Action Segmentation', 'Temporal Action Segmentation', 'System Of Equations', 'Breakfast', 'Segmentation Method', 'Training Loss', 'Segmentation Task', 'System Of Linear Equations', 'Training Videos', 'Model Parameters', 'Hidden Markov Model', 'Recurrent Neural Network', 'Intersection Over Union', 'Segmentation Model', 'Video Frames', 'Graph Convolutional Network', 'Temporal Model', 'Temporal Localization', 'Graph Laplacian', 'Random Walk Algorithm', 'Frame Features', 'Random Segments', 'Weak Supervision', 'Temporal Convolutional Network', 'Random Solution', 'Edit Distance', 'Action Labels', 'Loss Of Confidence', 'Form Of Supervision']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Temporal action segmentation relates to high-level video understanding, commonly formulated as frame-wise classification of untrimmed videos into predefined actions. Fully-supervised deep-learning approaches require dense video annotations which are time and money consuming. Furthermore, the temporal boundaries between consecutive actions typically are not well-defined, leading to inherent ambiguity and interrater disagreement. A promising approach to remedy these limitations is timestamp supervision, requiring only one labeled frame per action instance in a training video. In this work, we reformulate the task of temporal segmentation as a graph segmentation problem with weakly-labeled vertices. We introduce an efficient segmentation method based on random walks on graphs, obtained by solving a sparse system of linear equations. Furthermore, the proposed technique can be employed in any one or combination of the following forms: (1) as a standalone solution for generating dense pseudo-labels from timestamps; (2) as a training loss; (3) as a smoothing mechanism given intermediate predictions. Extensive experiments with three datasets (50Salads, Breakfast, GTEA) show that our method competes with state-of-the-art, and allows the identification of regions of uncertainty around action boundaries."
Randomized Adversarial Style Perturbations for Domain Generalization,"Taehoon Kim, Bohyung Han","ECE1 & IPAI2, Seoul National University",100.0,South Korea,0.0,,"We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to its style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class, and prevents the model from being misled by the unexpected styles observed in unseen target domains. While RASP is effective for handling domain shifts, its naive integration into the training procedure is prone to degrade the capability of learning knowledge from source domains due to the feature distortions caused by style perturbation. This challenge is alleviated by Normalized Feature Mixup (NFM) during training, which facilitates learning the original features while achieving robustness to perturbed representations. We evaluate the proposed algorithm via extensive experiments on various benchmarks and show that our approach improves domain generalization performance, especially in large-scale benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Randomized_Adversarial_Style_Perturbations_for_Domain_Generalization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Randomized_Adversarial_Style_Perturbations_for_Domain_Generalization_WACV_2024_paper.pdf,,,2304.01959,main,Poster,https://ieeexplore.ieee.org/document/10483707/,"['Training', 'Computer vision', 'Perturbation methods', 'Benchmark testing', 'Feature extraction', 'Distortion', 'Robustness']","['Domain Generalization', 'Adversarial Perturbations', 'Training Procedure', 'Domain Shift', 'Domain Features', 'Target Domain', 'Source Domain', 'Unseen Domains', 'Neural Network', 'Step Size', 'Computational Complexity', 'Deep Neural Network', 'Generalization Ability', 'Data Augmentation', 'Batch Normalization', 'Real-world Scenarios', 'Residual Block', 'Ground Truth Labels', 'Domain Classifier', 'Augmentation Techniques', 'Adversarial Attacks', 'Instance Normalization', 'Empirical Risk Minimization', 'Standard Cross-entropy Loss', 'Standard Benchmark', 'Augmentation Process', 'Affine Parameter', 'Perturbation Magnitude', 'Previous Iteration', 'Image Space']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",1,"We propose a novel domain generalization technique, referred to as Randomized Adversarial Style Perturbation (RASP), which is motivated by the observation that the characteristics of each domain are captured by the feature statistics corresponding to its style. The proposed algorithm perturbs the style of a feature in an adversarial direction towards a randomly selected class. By incorporating the perturbed styles into training, we prevent the model from being misled by the unexpected styles observed in unseen target domains. While RASP is effective for handling domain shifts, its naïve integration into the training procedure is prone to degrade the capability of learning knowledge from source domains due to the feature distortions caused by style perturbation. This challenge is alleviated by Normalized Feature Mixup (NFM) during training, which facilitates learning the original features while achieving robustness to perturbed representations. We evaluate the proposed algorithm via extensive experiments on various benchmarks and show that our approach improves domain generalization performance, especially in large-scale benchmarks."
Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning,"Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Mykel Kochenderfer, Chiho Choi, Behzad Dariush",Honda Research Institute USA; Stanford University,100.0,"Japan, USA",0.0,,"The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Furthermore, we introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset and demonstrate performance with quantitative evaluations.",https://openaccess.thecvf.com/content/WACV2024/html/Sachdeva_Rank2Tell_A_Multimodal_Driving_Dataset_for_Joint_Importance_Ranking_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sachdeva_Rank2Tell_A_Multimodal_Driving_Dataset_for_Joint_Importance_Ranking_and_WACV_2024_paper.pdf,https://usa.honda-ri.com/rank2tell,,2309.06597,main,Poster,https://ieeexplore.ieee.org/document/10483594/,"['Visualization', 'Computer vision', 'Annotations', 'Semantics', 'Natural languages', 'Closed box', 'Benchmark testing']","['Importance Ranking', 'Multimodal Dataset', 'DRIVE Dataset', 'Semantic', 'Natural Language', 'Autonomic System', 'Question Answering', 'Autonomous Vehicles', 'Joint Model', 'Visual Scene', 'Scene Understanding', 'Traffic Scenarios', 'Advanced Driver Assistance Systems', 'Visual Question Answering', 'Visual Understanding', 'Related Features', 'Global Features', 'Point Cloud', 'Bounding Box', 'Object Features', 'Objects In The Scene', 'Controller Area Network', '2D Feature', '3D Features', 'Important Agents', 'Scene Graph', 'Situational Awareness', 'Graph Convolutional Network', '3D Point Cloud', 'Urban Scenarios']","['Applications', 'Autonomous Driving', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",5,"The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Furthermore, we introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset and demonstrate performance with quantitative evaluations."
RankDVQA: Deep VQA Based on Ranking-Inspired Hybrid Training,"Chen Feng, Duolikun Danier, Fan Zhang, David Bull","Visual Information Laboratory, University of Bristol, Bristol, UK, BS1 5DD",100.0,UK,0.0,,"In recent years, deep learning techniques have shown significant potential for improving video quality assessment (VQA), achieving higher correlation with subjective opinions compared to conventional approaches. However, the development of deep VQA methods has been constrained by the limited availability of large-scale training databases and ineffective training methodologies. As a result, it is difficult for deep VQA approaches to achieve consistently superior performance and model generalization. In this context, this paper proposes new VQA methods based on a two-stage training methodology which motivates us to develop a large-scale VQA training database without employing human subjects to provide ground truth labels. This method was used to train a new transformer-based network architecture, exploiting quality ranking of different distorted sequences rather than minimizing the difference from the ground-truth quality labels. The resulting deep VQA methods (for both full reference and no reference scenarios), FR- and NR-RankDVQA, exhibit consistently higher correlation with perceptual quality compared to the state-of-the-art conventional and deep VQA methods, with average SROCC values of 0.8972 (FR) and 0.7791 (NR) over eight test sets without performing cross-validation. The source code of the proposed quality metrics and the large training database are available at https://chenfeng-bristol.github.io/RankDVQA.",https://openaccess.thecvf.com/content/WACV2024/html/Feng_RankDVQA_Deep_VQA_Based_on_Ranking-Inspired_Hybrid_Training_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Feng_RankDVQA_Deep_VQA_Based_on_Ranking-Inspired_Hybrid_Training_WACV_2024_paper.pdf,,https://chenfeng-bristol.github.io/RankDVQA,2202.08595,main,Poster,https://ieeexplore.ieee.org/document/10484488/,"['Training', 'Measurement', 'Deep learning', 'Correlation', 'Databases', 'Source coding', 'Network architecture']","['Video Quality Assessment', 'Hybrid Training', 'Distortion', 'Deep Learning', 'Quality Assessment', 'Quality Metrics', 'Ground Truth Labels', 'Video Quality', 'Training Methodology', 'Subjective Opinions', 'Root Mean Square Error', 'Stage 2', 'Feature Maps', 'Network Output', 'Learning-based Methods', 'Subjective Scores', 'Multiple Databases', 'Training Materials', 'Deep Learning-based Methods', 'Small Database', 'Quality Assessment Methods', 'Spatiotemporal Modulation', 'Inconsistent Performance', 'Learning To Rank', 'Aggregation Network', 'Video Compression', 'Patch Quality', '3D Tensor', 'Quantization Levels', 'Patch Level']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Datasets and evaluations']",6,"In recent years, deep learning techniques have shown significant potential for improving video quality assessment (VQA), achieving higher correlation with subjective opinions compared to conventional approaches. However, the development of deep VQA methods has been constrained by the limited availability of large-scale training databases and ineffective training methodologies. As a result, it is difficult for deep VQA approaches to achieve consistently superior performance and model generalization. In this context, this paper proposes new VQA methods based on a two-stage training methodology which motivates us to develop a large-scale VQA training database without employing human subjects to provide ground truth labels. This method was used to train a new transformer-based network architecture, exploiting quality ranking of different distorted sequences rather than minimizing the difference from the ground-truth quality labels. The resulting deep VQA methods (for both full reference and no reference scenarios), FR- and NR-RankDVQA, exhibit consistently higher correlation with perceptual quality compared to the state-of-the-art conventional and deep VQA methods, with average SROCC values of 0.8972 (FR) and 0.7791 (NR) over eight test sets without performing cross-validation. The source code of the proposed quality metrics and the large training database are available at https://chenfeng-bristol.github.io/RankDVQA."
Ray Deformation Networks for Novel View Synthesis of Refractive Objects,"Weijian Deng, Dylan Campbell, Chunyi Sun, Shubham Kanitkar, Matthew Shaffer, Stephen Gould",Rios Intelligent Machines; The Australian National University,50.0,Australia,50.0,USA,"Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in creating photorealistic novel views using volume rendering on a radiance field. However, the intrinsic assumption of straight light rays within NeRF becomes a limitation when dealing with transparent or translucent objects that exhibit refraction, and therefore have curved light paths. This hampers the ability of these approaches to accurately model the appearance of refractive objects, resulting in suboptimal novel view synthesis and geometry estimates. To address this issue, we propose an innovative solution using deformable networks to learn a tailored deformation field for refractive objects. Our approach predicts position and direction offsets, allowing NeRF to model the curved light paths caused by refraction and therefore the complex and highly view-dependent appearances of refractive objects. We also introduce a regularization strategy that encourages piece-wise linear light paths, since most physical systems can be approximated with a piece-wise constant index of refraction. By seamlessly integrating our deformation networks into the NeRF framework, our method achieves significant improvements in rendering refractive objects from novel views.",https://openaccess.thecvf.com/content/WACV2024/html/Deng_Ray_Deformation_Networks_for_Novel_View_Synthesis_of_Refractive_Objects_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Deng_Ray_Deformation_Networks_for_Novel_View_Synthesis_of_Refractive_Objects_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483621/,"['Geometry', 'Deformable models', 'Computer vision', 'Deformation', 'Computational modeling', 'Refractive index', 'Estimation']","['View Synthesis', 'Refractive Objects', 'Refractive Index', 'Step Function', 'Light Path', 'Light Rays', 'Deformation Field', 'Regularization Scheme', 'Object Appearance', 'Piecewise Linear', 'Geometry Estimation', 'Offset Position', 'Illumination', 'Training Set', 'Validation Set', 'Multilayer Perceptron', 'Internal Reflection', 'Visual Quality', 'Peak Signal-to-noise Ratio', 'Scene Images', 'Geometric Representation', 'Snell’s Law', 'Camera Pose', 'View Direction', 'Dynamic Scenes', 'Light Refraction', 'Object Geometry']","['Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in creating photorealistic novel views using volume rendering on a radiance field. However, the intrinsic assumption of straight light rays within NeRF becomes a limitation when dealing with transparent or translucent objects that exhibit refraction, and therefore have curved light paths. This hampers the ability of these approaches to accurately model the appearance of refractive objects, resulting in suboptimal novel view synthesis and geometry estimates. To address this issue, we propose an innovative solution using deformable networks to learn a tailored deformation field for refractive objects. Our approach predicts position and direction offsets, allowing NeRF to model the curved light paths caused by refraction and therefore the complex and highly view-dependent appearances of refractive objects. We also introduce a regularization strategy that encourages piece-wise linear light paths, since most physical systems can be approximated with a piece-wise constant index of refraction. By seamlessly integrating our deformation networks into the NeRF framework, our method significantly improves rendering refractive objects from novel views."
Re-Evaluating LiDAR Scene Flow,"Nathaniel Chodosh, Deva Ramanan, Simon Lucey",University of Adelaide; Carnegie Mellon University,100.0,"Australia, USA",0.0,,"Popular benchmarks for self-supervised LiDAR scene flow (stereoKITTI, and FlyingThings3D) have unrealistic rates of dynamic motion, unrealistic correspondences, and unrealistic sampling patterns. As a result, progress on these benchmarks is misleading and may cause researchers to focus on the wrong problems. We evaluate a suite of top methods on a suite of real-world datasets (Argoverse 2.0, Waymo, and NuScenes) and report several conclusions. First, we find that performance on stereoKITTI is negatively correlated with performance on real-world data. Second, we find that one of this task's key components -- removing the dominant ego-motion -- is better solved by classic ICP than any tested method. Finally, we show that despite the emphasis placed on learning, most performance gains are caused by pre- and post-processing steps: piecewise- rigid refinement and ground removal. We demonstrate this through a baseline method that combines these processing steps with a learning-free test-time flow optimization. This baseline outperforms every evaluated method",https://openaccess.thecvf.com/content/WACV2024/html/Chodosh_Re-Evaluating_LiDAR_Scene_Flow_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chodosh_Re-Evaluating_LiDAR_Scene_Flow_WACV_2024_paper.pdf,,,2304.02150,main,Poster,https://ieeexplore.ieee.org/document/10484248/,"['Computer vision', 'Laser radar', 'Correlation', 'Dynamics', 'Estimation', 'Benchmark testing', 'Performance gain']","['Scene Flow', 'Test Method', 'Real-world Data', 'Real-world Datasets', 'Baseline Methods', 'Post-processing Step', 'Dynamic Motion', 'Iterative Closest Point', 'Popular Benchmark', 'Intersection Over Union', 'Point Cloud', 'Autonomous Vehicles', 'Optical Flow', 'Standard Benchmark', 'Static Objects', 'Dynamic Objects', 'Coordination Network', 'Inliers', 'Ground Points', 'Motion Compensation', 'Dynamic Point', 'Chamfer Distance', 'Height Map', 'LiDAR Scans', 'Ground Segment', 'Current Benchmark', 'Static Background', 'LiDAR Point', 'Rigid Parameters', 'Flow Estimation']","['Applications', 'Autonomous Driving', 'Algorithms', 'Datasets and evaluations']",1,"Popular benchmarks for self-supervised LiDAR scene flow (stereoKITTI, and FlyingThings3D) have unrealistic rates of dynamic motion, unrealistic correspondences, and unrealistic sampling patterns. As a result, progress on these benchmarks is misleading and may cause researchers to focus on the wrong problems. We evaluate a suite of top methods on a suite of real-world datasets (Argoverse 2.0, Waymo, and NuScenes) and report several conclusions. First, we find that performance on stereoKITTI is negatively correlated with performance on real-world data. Second, we find that one of this task’s key components –removing the dominant ego-motion –is better solved by classic ICP than any tested method. Finally, we show that despite the emphasis placed on learning, most performance gains are caused by pre- and post-processing steps: piecewise-rigid refinement and ground removal. We demonstrate this through a baseline method that combines these processing steps with a learning-free test-time flow optimization. This baseline outperforms every evaluated method."
Re-VoxelDet: Rethinking Neck and Head Architectures for High-Performance Voxel-Based 3D Detection,"Jae-Keun Lee, Jin-Hee Lee, Joohyun Lee, Soon Kwon, Heechul Jung","DGIST; Kyungpook National University; DGIST, FutureDrive Inc.; DGIST, FutureDrive Inc., Kyungpook National University",100.0,South Korea,0.0,,"Currently, widely employed LiDAR-based 3D object detectors adopt grid-based approaches to efficiently handle sparse point clouds. However, during this process, the down-sampled features inevitably lose spatial information, which can hinder the detectors from accurately predicting the location and size of objects. To address this issue, previous researches proposed sophisticatedly designed neck and head modules to effectively compensate for information loss. Inspired by the core insights of previous studies, we propose a novel voxel-based 3D object detector, named as Re-VoxelDet, which combines three distinct components to achieve both good detection capability and real-time performance. First, in order to learn features from diverse perspectives without additional computational costs during inference, we introduce Multi-view Voxel Backbone (MVBackbone). Second, to effectively compensate for abundant spatial and strong semantic information, we design Hierarchical Voxel-guided Auxiliary Neck (HVANeck), which attentively integrate hierarchically generated voxel-wise features with RPN blocks. Third, we present Rotation-based Group Head (RGHead), a simple yet effective head module that is designed with two groups according to the heading direction and aspect ratio of the objects. Through extensive experiments on the Argoverse2, nuScenes, and Waymo Open Dataset, we demonstrate the effectiveness of our approach. Our results significantly outperform existing state-of-the-art methods. We plan to release our model and code in the near future.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Re-VoxelDet_Rethinking_Neck_and_Head_Architectures_for_High-Performance_Voxel-Based_3D_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Re-VoxelDet_Rethinking_Neck_and_Head_Architectures_for_High-Performance_Voxel-Based_3D_WACV_2024_paper.pdf,,https://github.com/JH-Research/Re-VoxelDet,,main,Poster,https://ieeexplore.ieee.org/document/10484329/,"['Point cloud compression', 'Three-dimensional displays', 'Sensitivity', 'Semantics', 'Detectors', 'Object detection', 'Feature extraction']","['3D Detection', 'Voxel-based 3D', 'Prediction Accuracy', 'Spatial Information', 'Aspect Ratio', 'Additional Costs', 'Object Detection', 'Point Cloud', 'Region Proposal Network', 'Direct Object', 'Additional Computational Cost', 'Sparse Point Cloud', '3D Object Detection', 'Convolutional Network', 'Feature Size', 'Detection Performance', 'Pedestrian', 'Intersection Over Union', 'Detection Range', 'Bounding Box', 'Loss Of Spatial Information', 'Intersection Over Union Score', '3D Kernel', 'Stop Sign', 'Large Aspect Ratio', 'Test Split', 'School Bus', 'Mean Average Precision', 'Small Objects', 'Representational Capacity']","['Applications', 'Autonomous Driving', 'Applications', 'Robotics']",1,"LiDAR-based 3D object detectors usually adopt grid- based approaches to handle sparse point clouds efficiently. However, during this process, the down-sampled features inevitably lose spatial information, which can hinder the detectors from accurately predicting the location and size of objects. To address this issue, previous researches proposed sophisticatedly designed neck and head modules to effectively compensate for information loss. Inspired by the core insights of previous studies, we propose a novel voxel-based 3D object detector, named as Re-VoxelDet, which combines three distinct components to achieve both good detection capability and real-time performance. First, in order to learn features from diverse perspectives without additional computational costs during inference, we introduce Multiview Voxel Backbone (MVBackbone). Second, to effectively compensate for abundant spatial and strong semantic information, we design Hierarchical Voxel-guided Auxiliary Neck (HVANeck), which attentively integrates hierarchically generated voxel-wise features with RPN blocks. Third, we present Rotation-based Group Head (RGHead), a simple yet effective head module that is designed with two groups according to the heading direction and aspect ratio of the objects. Through extensive experiments on the Argoverse2, Waymo Open Dataset and nuScenes, we demonstrate the effectiveness of our approach. Our results significantly outperform existing state-of-the-art methods. We plan to release our model and code 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
 in the near future."
ReCLIP: Refine Contrastive Language Image Pre-Training With Source Free Domain Adaptation,"Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo, Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, Ram Nevatia",University of Southern California; Amazon,50.0,USA,50.0,USA,"Large-scale pre-training vision-language models (VLM) such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, a novel source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignment iteratively. With extensive experiments, we demonstrate that ReCLIP outperforms all the baselines with significant margin and improves the averaged accuracy of CLIP from 69.83% to 74.94% on 22 image classification benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Hu_ReCLIP_Refine_Contrastive_Language_Image_Pre-Training_With_Source_Free_Domain_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hu_ReCLIP_Refine_Contrastive_Language_Image_Pre-Training_With_Source_Free_Domain_WACV_2024_paper.pdf,,,2308.03793,main,Poster,https://ieeexplore.ieee.org/document/10483866/,"['Visualization', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Benchmark testing', 'Task analysis', 'Image classification']","['Domain Adaptation', 'Source-free Domain Adaptation', 'Contrastive Language-Image Pre-training', 'Image Classification', 'Adaptive Method', 'Average Accuracy', 'ImageNet', 'Target Domain', 'Large-scale Models', 'Visual Domain', 'Projective Space', 'Pseudo Labels', 'Domain Gap', 'Domain Adaptation Methods', 'Text Encoder', 'Visual Representation', 'Random Walk', 'Latent Space', 'Detailed Algorithm', 'Source Domain', 'Label Propagation', 'Visual Encoding', 'Average Similarity', 'Class Assignment', 'Multiple Templates', 'Neighborhood Relationship', 'Parallel Components', 'Class Weights', 'Projection Matrix']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Large-scale pre-trained vision-language models (VLM) such as CLIP [32] have demonstrated noteworthy zero-shot classification capability, achieving 76.3% top-1 accuracy on ImageNet without seeing any examples. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, a novel source-free domain adaptation method for VLMs, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels. Then, it deploys cross-modality self-training with the pseudo labels to update visual and text encoders, refine labels and reduce domain gaps and misalignment iteratively. With extensive experiments, we show that ReCLIP outperforms all the baselines significantly and improves the average accuracy of CLIP from 69.83% to 74.94% on 22 image classification benchmarks."
ReConPatch: Contrastive Patch Representation Learning for Industrial Anomaly Detection,"Jeeho Hyun, Sangyun Kim, Giyoung Jeon, Seung Hwan Kim, Kyunghoon Bae, Byung Jun Kang",LG AI Research,0.0,,100.0,South Korea,"Anomaly detection is crucial to the advanced identification of product defects such as incorrect parts, misaligned components, and damages in industrial manufacturing. Due to the rare observations and unknown types of defects, anomaly detection is considered to be challenging in machine learning. To overcome this difficulty, recent approaches utilize the common visual representations pre-trained from natural image datasets and distill the relevant features. However, existing approaches still have the discrepancy between the pre-trained feature and the target data, or require the input augmentation which should be carefully designed, particularly for the industrial dataset. In this paper, we introduce ReConPatch, which constructs discriminative features for anomaly detection by training a linear modulation of patch features extracted from the pre-trained model. ReConPatch employs contrastive representation learning to collect and distribute features in a way that produces a target-oriented and easily separable representation. To address the absence of labeled pairs for the contrastive learning, we utilize two similarity measures between data representations, pairwise and contextual similarities, as pseudo-labels. Our method achieves the state-of-the-art anomaly detection performance (99.72%) for the widely used and challenging MVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly detection performance (95.8%) for the BTAD dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Hyun_ReConPatch_Contrastive_Patch_Representation_Learning_for_Industrial_Anomaly_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hyun_ReConPatch_Contrastive_Patch_Representation_Learning_for_Industrial_Anomaly_Detection_WACV_2024_paper.pdf,,,2305.16713,main,Poster,,,,,,
Real Time GAZED: Online Shot Selection and Editing of Virtual Cameras From Wide-Angle Monocular Video Recordings,"Sudheer Achary, Rohit Girmaji, Adhiraj Anil Deshmukh, Vineet Gandhi","CVIT, KCIS, IIIT Hyderabad, India",100.0,India,0.0,,"Eliminating time-consuming post-production processes and delivering high-quality videos in today's fast-paced digital landscape are the key advantages of real-time approaches. To address these needs, we present Real Time GAZED: a real-time adaptation of the GAZED framework integrated with CineFilter, a novel real-time camera trajectory stabilization approach. It enables users to create professionally edited videos in real-time. Comparative evaluations against baseline methods, including the non-real-time GAZED, demonstrate that Real Time GAZED achieves similar editing results, ensuring high-quality video output. Furthermore, a user study confirms the aesthetic quality of the video edits produced by the Real Time GAZED approach. With these advancements in real-time camera trajectory optimization and video editing presented, the demand for immediate and dynamic content creation in industries such as live broadcasting, sports coverage, news reporting, and social media content creation can be met more efficiently.",https://openaccess.thecvf.com/content/WACV2024/html/Achary_Real_Time_GAZED_Online_Shot_Selection_and_Editing_of_Virtual_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Achary_Real_Time_GAZED_Online_Shot_Selection_and_Editing_of_Virtual_WACV_2024_paper.pdf,,,2311.15581,main,Poster,https://ieeexplore.ieee.org/document/10483706/,"['Industries', 'Computer vision', 'Social networking (online)', 'Broadcasting', 'Cameras', 'Real-time systems', 'Multimedia communication']","['Virtual Camera', 'Shot Selection', 'User Study', 'Content Creation', 'Video Editing', 'Real-time Trajectory', 'Objective Function', 'Function Of Life', 'Eye Contact', 'Manual Annotation', 'Penalty Term', 'Optimal Path', 'Virtual Simulation', 'Abrupt Transition', 'Multiple Cameras', 'Original Video', 'Smooth Trajectory', 'Recurrence Relation', 'Cost Matrix', 'Gaze Data', 'Editing Strategies', 'View Of Experience']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Applications', 'Embedded sensing / real-time techniques', 'Applications', 'Virtual / augmented reality']",,"Eliminating time-consuming post-prodution processes and delivering high-quality videos in today’s fast-paced digital landscape are the key advantages of real-time approaches. To address these needs, we present Real Time GAZED: a real-time adaptation of the GAZED framework integrated with CineFilter, a novel real-time camera trajectory stabilization approach. It enables users to create professionally edited videos in real-time. Comparative evaluations against baseline methods, including the non-real-time GAZED, demonstrate that Real Time GAZED achieves similar editing results, ensuring high-quality video output. Furthermore, a user study confirms the aesthetic quality of the video edits produced by the Real Time GAZED approach. With these advancements in real-time camera trajectory optimization and video editing presented, the demand for immediate and dynamic content creation in industries such as live broadcasting, sports coverage, news reporting, and social media content creation can be met more efficiently."
Real-Time 6-DoF Pose Estimation by an Event-Based Camera Using Active LED Markers,"Gerald Ebmer, Adam Loch, Minh Nhat Vu, Roberto Mecca, Germain Haessig, Christian Hartl-Nesic, Markus Vincze, Andreas Kugi","Automation and Control Institute (ACIN), TU Wien, Vienna; Austrian Institute of Technology (AIT), Vienna; Prophesee GmbH, Paris",66.66666666666666,Austria,33.33333333333334,France,"Real-time applications for autonomous operations depend largely on fast and robust vision-based localization systems. Since image processing tasks require processing large amounts of data, the computational resources often limit the performance of other processes. To overcome this limitation, traditional marker-based localization systems are widely used since they are easy to integrate and achieve reliable accuracy. However, classical marker-based localization systems significantly depend on standard cameras with low frame rates, which often lack accuracy due to motion blur. In contrast, event-based cameras provide high temporal resolution and a high dynamic range, which can be utilized for fast localization tasks, even under challenging visual conditions. This paper proposes a simple but effective event-based pose estimation system using active LED markers (ALM) for fast and accurate pose estimation. The proposed algorithm is able to operate in real time with a latency below 0.5 ms while maintaining output rates of 3 kHz. Experimental results in static and dynamic scenarios are presented to demonstrate the performance of the proposed approach in terms of computational speed and absolute accuracy, using the OptiTrack system as the basis for measurement. Moreover, we demonstrate the feasibility of the proposed approach by deploying the hardware, i.e., the event-based camera and ALM, and the software in a real quadcopter application.",https://openaccess.thecvf.com/content/WACV2024/html/Ebmer_Real-Time_6-DoF_Pose_Estimation_by_an_Event-Based_Camera_Using_Active_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ebmer_Real-Time_6-DoF_Pose_Estimation_by_an_Event-Based_Camera_Using_Active_WACV_2024_paper.pdf,http://almpose.github.io,https://github.com/almpose,,main,Poster,https://ieeexplore.ieee.org/document/10483643/,"['Location awareness', 'Visualization', 'Tracking', 'Pose estimation', 'Dynamics', 'Cameras', 'Light emitting diodes']","['Activation Markers', 'Pose Estimation', '6-DoF Pose', 'Event-based Cameras', '6-DoF Pose Estimation', 'Local System', 'Yield Rate', 'High Dynamic Range', 'Absolute Accuracy', 'Lower Frame', 'Standard Camera', 'Vision-based System', 'Central Point', '3D Space', 'Unmanned Aerial Vehicles', 'Position Error', 'Depth Camera', 'Position Estimation', 'Statistical Noise', 'Noise Floor', 'Orientation Error', 'Event Stream', 'Fast Motion', 'Reprojection Error', 'Blink Frequency', 'Sensor Plane', 'Radius Ri', 'Pose Information', 'Outdoor Experiments', 'Bias Adjustment']","['Applications', 'Embedded sensing / real-time techniques', 'Applications', 'Robotics']",1,"Real-time applications for autonomous operations depend largely on fast and robust vision-based localization systems. Since image processing tasks require processing large amounts of data, the computational resources often limit the performance of other processes. To overcome this limitation, traditional marker-based localization systems are widely used since they are easy to integrate and achieve reliable accuracy. However, classical marker-based localization systems significantly depend on standard cameras with low frame rates, which often lack accuracy due to motion blur. In contrast, event-based cameras provide high temporal resolution and a high dynamic range, which can be utilized for fast localization tasks, even under challenging visual conditions. This paper proposes a simple but effective event-based pose estimation system using active LED markers (ALM) for fast and accurate pose estimation. The proposed algorithm is able to operate in real time with a latency below 0.5 ms while maintaining output rates of 3 kHz. Experimental results in static and dynamic scenarios are presented to demonstrate the performance of the proposed approach in terms of computational speed and absolute accuracy, using the OptiTrack system as the basis for measurement. Moreover, we demonstrate the feasibility of the proposed approach by deploying the hardware, i.e., the event-based camera and ALM, and the software in a real quadcopter application. Our project page is available at: almpose.github.io"
Real-Time Polyp Detection in Colonoscopy Using Lightweight Transformer,"Youngbeom Yoo, Jae Young Lee, Dong-Jae Lee, Jiwoon Jeon, Junmo Kim",,,,,,"Colorectal cancer (CRC) represents a major global health challenge, and early detection of polyps is crucial in preventing its progression. Although colonoscopy is the gold standard for polyp detection, it has limitations, such as human error and missed detection rates. In response, computer-aided detection (CADe) systems have been developed to enhance the efficiency and accuracy of polyp detection. As deep learning gained prominence, the incorporation of Convolutional Neural Networks (CNNs) into CADe systems emerged as a breakthrough approach. However, CADe systems based on CNNs often demand significant computational resources, making them unsuitable for deployment in resource-constrained environments. To mitigate this, we propose a novel and lightweight polyp detection model that integrates a Transformer layer into the You Only Look Once (YOLO) architecture, focusing on optimizing the neck part responsible for feature fusion and rescaling. Our model demonstrates a substantial reduction in computational complexity and the number of parameters, without compromising detection performances. The lightweight model makes it accessible and feasibly deployable in medically underserved regions, serving a significant public interest by potentially expanding the reach of critical diagnostic tools for CRC prevention. By optimizing the architecture to reduce resource requirements while maintaining performance, our model becomes a practical solution to assist healthcare professionals in the real-time identification of polyps, even with resource-constraint devices.",https://openaccess.thecvf.com/content/WACV2024/html/Yoo_Real-Time_Polyp_Detection_in_Colonoscopy_Using_Lightweight_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yoo_Real-Time_Polyp_Detection_in_Colonoscopy_Using_Lightweight_Transformer_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484143/,"['YOLO', 'Performance evaluation', 'Computational modeling', 'Colonoscopy', 'Computer architecture', 'Transformers', 'Real-time systems']","['Real-time Detection', 'Polyp Detection', 'Lightweight Transformer', 'Colonoscopy Detection', 'Real-time Polyp Detection', 'Colorectal Cancer', 'Convolutional Neural Network', 'Feature Fusion', 'Computer-aided Diagnosis', 'Lightweight Model', 'Computational Complexity Reduction', 'Resource-constrained Environments', 'You Only Look Once', 'Part Of The Neck', 'Training Set', 'Local Features', 'Feature Maps', 'Object Detection', 'Attention Mechanism', 'Global Features', 'Frames Per Second', 'Vision Transformer', 'Transformer Block', 'Global Information', 'Channel Size', 'Bounding Box', 'Multi-level Features', 'Edge Devices', 'Presence Of Polyps']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",1,"Colorectal cancer (CRC) represents a major global health challenge, and early detection of polyps is crucial in preventing its progression. Although colonoscopy is the gold standard for polyp detection, it has limitations, such as human error and missed detection rates. In response, computer-aided detection (CADe) systems have been developed to enhance the efficiency and accuracy of polyp detection. As deep learning gained prominence, the incorporation of Convolutional Neural Networks (CNNs) into CADe systems emerged as a breakthrough approach. However, CADe systems based on CNNs often demand significant computational resources, making them unsuitable for deployment in resource-constrained environments. To mitigate this, we propose a novel and lightweight polyp detection model that integrates a Transformer layer into the You Only Look Once (YOLO) architecture, focusing on optimizing the neck part responsible for feature fusion and rescaling. Our model demonstrates a substantial reduction in computational complexity and the number of parameters, without compromising detection performances. The lightweight model makes it accessible and feasibly deployable in medically underserved regions, serving a significant public interest by potentially expanding the reach of critical diagnostic tools for CRC prevention. By optimizing the architecture to reduce resource requirements while maintaining performance, our model becomes a practical solution to assist healthcare professionals in the real-time identification of polyps, even with resource-constraint devices."
Real-Time User-Guided Adaptive Colorization With Vision Transformer,"Gwanghan Lee, Saebyeol Shin, Taeyoung Na, Simon S. Woo","Department of Artificial Intelligence, Sungkyunkwan University, South Korea; SK Telecom, South Korea; College of Computing and Informatics, Sungkyunkwan University, South Korea",66.66666666666666,South Korea,33.33333333333334,South Korea,"Recently, the vision transformer (ViT) has achieved remarkable performance in computer vision tasks and has been actively utilized in colorization. Vision transformer uses multi-head self attention to effectively propagate user hints to distant relevant areas in the image. However, despite the success of vision transformers in colorizing the image, heavy underlying ViT architecture and the large computational cost hinder active real-time user interaction for colorization applications. Several research removed redundant image patches to reduce the computational cost of ViT in image classification tasks. However, the existing efficient ViT methods cause severe performance degradation in colorization task since it completely removes the redundant patches. Thus, we propose a novel efficient ViT architecture for real-time interactive colorization, AdaColViT determines which redundant image patches and layers to reduce in the ViT. Unlike existing methods, our novel pruning method alleviates performance drop and flexibly allocates computational resources of input samples, effectively achieving actual acceleration. In addition, we demonstrate through extensive experiments on ImageNet-ctest10k, Oxford 102flowers, and CUB-200 datasets that our method outperforms the baseline methods.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Real-Time_User-Guided_Adaptive_Colorization_With_Vision_Transformer_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484528/,"['Degradation', 'Computer vision', 'Computer architecture', 'Transformers', 'Real-time systems', 'Computational efficiency', 'Task analysis']","['Vision Transformer', 'Computational Cost', 'Complete Removal', 'Input Samples', 'Human-computer Interaction', 'Image Patches', 'Image Classification Tasks', 'Pruning Method', 'Single Image', 'Color Images', 'Grayscale Images', 'Multiple Objects', 'Color Space', 'US Policy', 'Binary Decision', 'Attention Layer', 'Transformer Architecture', 'Transformer Layers', 'Attention Heads', 'Multi-head Self-attention', 'PSNR Values', 'Decision Network', 'Transformer Block', 'Huber Loss', 'Patch Selection', 'Redundant Model', 'Final Input']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.']",1,"Recently, the vision transformer (ViT) has achieved remarkable performance in computer vision tasks and has been actively utilized in colorization. Vision transformer uses multi-head self attention to effectively propagate user hints to distant relevant areas in the image. However, despite the success of vision transformers in colorizing the image, heavy underlying ViT architecture and the large computational cost hinder active real-time user interaction for colorization applications. Several research removed redundant image patches to reduce the computational cost of ViT in image classification tasks. However, the existing efficient ViT methods cause severe performance degradation in colorization task since it completely removes the redundant patches. Thus, we propose a novel efficient ViT architecture for real-time interactive colorization, AdaColViT determines which redundant image patches and layers to reduce in the ViT. Unlike existing methods, our novel pruning method alleviates performance drop and flexibly allocates computational resources of input samples, effectively achieving actual acceleration. In addition, we demonstrate through extensive experiments on ImageNet-ctest10k, Oxford 102flowers, and CUB-200 datasets that our method outperforms the baseline methods."
Real-Time Weakly Supervised Video Anomaly Detection,"Hamza Karim, Keval Doshi, Yasin Yilmaz",University of South Florida,100.0,USA,0.0,,"Weakly supervised video anomaly detection is an important problem in many real-world applications where during training there are some anomalous videos, in addition to nominal videos, without labelled frames to indicate when the anomaly happens. State-of-the-art methods in this domain typically focus on offline anomaly detection without any concern for real-time detection. Most of these methods rely on ad hoc feature aggregation techniques and the use of metric learning losses, which limit the ability of the models to detect anomalies in real-time. In line with the premise of deep neural networks, there also has been a growing interest in developing end-to-end approaches that can automatically learn effective features directly from the raw data. We propose the first real-time and end-to-end trained algorithm for weakly supervised video anomaly detection. Our training procedure builds upon recent action recognition literature and uses a trainable video model to learn visual features. This is in contrast to existing approaches which largely depend on pre-trained feature extractors. The proposed method significantly improves the anomaly detection speed and AUC performance compared to the existing methods. Specifically, on the UCF-Crime dataset, our method achieves 86.94% AUC with a decision period of 6.4 seconds while the competing methods achieve at most 85.92% AUC with a decision period of 273 seconds.",https://openaccess.thecvf.com/content/WACV2024/html/Karim_Real-Time_Weakly_Supervised_Video_Anomaly_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Karim_Real-Time_Weakly_Supervised_Video_Anomaly_Detection_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483693/,"['Training', 'Measurement', 'Visualization', 'Refining', 'Pipelines', 'Streaming media', 'Feature extraction']","['Anomaly Detection', 'Video Anomaly', 'Video Anomaly Detection', 'Neural Network', 'Deep Neural Network', 'Real-time Detection', 'Action Recognition', 'Feature Aggregation', 'Area Under Receiver Operating Characteristic Curve', 'Metric Learning', 'Domain Method', 'Learning Loss', 'Video Modeling', 'Decision Period', 'Computational Efficiency', 'Detection Performance', 'Video Games', 'Largest Value', 'Real-time Performance', 'Video Segments', 'Video Surveillance', 'Multiple Instance Learning', 'Real-time Inference', 'Multilayer Perceptron', 'Self-supervised Learning', 'Feature Encoder', 'Training Videos', 'Raw Video', '3D Network']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",6,"Weakly supervised video anomaly detection is an important problem in many real-world applications where during training there are some anomalous videos, in addition to nominal videos, without labelled frames to indicate when the anomaly happens. State-of-the-art methods in this domain typically focus on offline anomaly detection without any concern for real-time detection. Most of these methods rely on ad hoc feature aggregation techniques and the use of metric learning losses, which limit the ability of the models to detect anomalies in real-time. In line with the premise of deep neural networks, there also has been a growing interest in developing end-to-end approaches that can automatically learn effective features directly from the raw data. We propose the first real-time and end-to-end trained algorithm for weakly supervised video anomaly detection. Our training procedure builds upon recent action recognition literature and trains a large video model to learn visual features. This is in contrast to existing approaches which largely depend on pre-trained feature extractors. The proposed method significantly improves the anomaly detection speed and AUC performance compared to the existing methods. Specifically, on the UCF-Crime dataset, our method achieves 86.94% AUC with a decision period of 6.4 seconds while the competing methods achieve at most 85.92% AUC with a decision period of 273 seconds."
Recognition of Unseen Bird Species by Learning From Field Guides,"Andrés C. Rodríguez, Stefano D'Aronco, Rodrigo Caye Daudt, Jan D. Wegner, Konrad Schindler","Institute for Computational Science, University of Zurich, Switzerland; EcoVision Lab - Photogrammetry and Remote Sensing, ETH Zurich, Switzerland",100.0,Switzerland,0.0,,"We exploit field guides to learn bird species recognition, in particular zero-shot recognition of unseen species. Illustrations contained in field guides deliberately focus on discriminative properties of each species, and can serve as side information to transfer knowledge from seen to unseen bird species. We study two approaches: (1) a contrastive encoding of illustrations, which can be fed into standard zero-shot learning schemes; and (2) a novel method that leverages the fact that illustrations are also images and as such structurally more similar to photographs than other kinds of side information. Our results show that illustrations from field guides, which are readily available for a wide range of species, are indeed a competitive source of side information for zero-shot learning. On a subset of the iNaturalist2021 dataset with 749 seen and 739 unseen species, we obtain a classification accuracy of unseen bird species of 12% @top-1 and 38% @top-10, which shows the potential of field guides for challenging real-world scenarios with many species. Our code is available at https://github.com/ac-rodriguez/zsl_billow.",https://openaccess.thecvf.com/content/WACV2024/html/Rodriguez_Recognition_of_Unseen_Bird_Species_by_Learning_From_Field_Guides_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rodriguez_Recognition_of_Unseen_Bird_Species_by_Learning_From_Field_Guides_WACV_2024_paper.pdf,,https://github.com/ac-rodriguez/zsl_billow,,main,Poster,https://ieeexplore.ieee.org/document/10483859/,"['Visualization', 'Computer vision', 'Image recognition', 'Image coding', 'Zero-shot learning', 'Text recognition', 'Flowering plants']","['Bird Species', 'Field Guide', 'Source Of Information', 'Species Recognition', 'Zero-shot', 'ImageNet', 'Generative Adversarial Networks', 'Common Name', 'Latent Space', 'Harmonic Mean', 'Abundance Estimates', 'Target Domain', 'Domain Adaptation', 'Latent Representation', 'Variational Autoencoder', 'Source Domain', 'Visual Space', 'Contrastive Loss', 'Information For Recognition', 'World Datasets', 'Unseen Classes', 'iNaturalist', 'Images Of Species', 'Memory Bank', 'Class Prototypes', 'Description Of Classes', 'Image Descriptors', 'Vector Form', 'Photography', 'Types Of Information']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Animals / Insects']",2,"We exploit field guides to learn bird species recognition, in particular zero-shot recognition of unseen species. Illustrations contained in field guides deliberately focus on discriminative properties of each species, and can serve as side information to transfer knowledge from seen to unseen bird species. We study two approaches: (1) a contrastive encoding of illustrations, which can be fed into standard zero-shot learning schemes; and (2) a novel method that leverages the fact that illustrations are also images and as such structurally more similar to photographs than other kinds of side information. Our results show that illustrations from field guides, which are readily available for a wide range of species, are indeed a competitive source of side information for zero-shot learning. On a subset of the iNaturalist2021 dataset with 749 seen and 739 unseen species, we obtain a classification accuracy of unseen bird species of 12% @top-1 and 38% @top-10, which shows the potential of field guides for challenging real-world scenarios with many species. Our code is available at https://github.com/ac-rodriguez/zsl_billow."
RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement,"Gregor Köhler, Tassilo Wald, Constantin Ulrich, David Zimmerer, Paul F. Jäger, Jörg K.H. Franke, Simon Kohl, Fabian Isensee, Klaus H. Maier-Hein","Helmholtz Imaging, DKFZ; Applied Computer Vision Lab, DKFZ; German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany; Helmholtz Imaging, DKFZ; National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg; Pattern Analysis and Learning Group, Heidelberg University Hospital, Heidelberg, Germany; Latent Labs (latentlabs.com), London, UK; Interactive Machine Learning Group, DKFZ; German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany; National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg; Machine Learning Lab, University of Freiburg, Freiburg, Germany; Helmholtz Imaging, DKFZ; National Center for Tumor Diseases (NCT), NCT Heidelberg, a partnership between DKFZ and University Medical Center Heidelberg; German Cancer Research Center (DKFZ) Heidelberg, Division of Medical Image Computing, Germany; Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany",40.0,Germany,60.0,Germany,"Despite the remarkable success of deep learning systems over the last decade, a key difference still remains between neural network and human decision-making: As humans, we can not only form a decision on the spot, but also ponder, revisiting an initial guess from different angles, distilling relevant information, arriving at a better decision. Here, we propose RecycleNet, a latent feature recycling method, instilling the pondering capability for neural networks to refine initial decisions over a number of recycling steps, where outputs are fed back into earlier network layers in an iterative fashion. This approach makes minimal assumptions about the neural network architecture and thus can be implemented in a wide variety of contexts. Using medical image segmentation as the evaluation environment, we show that latent feature recycling enables the network to iteratively refine initial predictions even beyond the iterations seen during training, converging towards an improved decision. We evaluate this across a variety of segmentation benchmarks and show consistent improvements even compared with top-performing segmentation methods. This allows trading increased computation time for improved performance, which can be beneficial, especially for safety-critical applications.",https://openaccess.thecvf.com/content/WACV2024/html/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kohler_RecycleNet_Latent_Feature_Recycling_Leads_to_Iterative_Decision_Refinement_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484484/,"['Training', 'Image segmentation', 'Schedules', 'Computational modeling', 'Neural networks', 'Memory management', 'Recycling']","['Neural Network', 'Deep Learning', 'Image Segmentation', 'Iterative Refinement', 'Medical Image Segmentation', 'Wide Variety Of Contexts', 'Fair Comparison', 'Training Time', 'Part Of Network', 'Recurrent Neural Network', 'Additional Modifications', 'Human Cognition', 'Multivariate Approach', '5-fold Cross-validation', 'Segmentation Task', 'Inference Time', 'Segmentation Performance', 'Memory Consumption', 'Training Schedule', 'Forward Pass', 'Time Epochs', 'Segmentation Dataset', '3D Segmentation', 'Refinement Step', 'Recycling Process', 'Part Of Architecture', 'Increase In Time']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Biomedical / healthcare / medicine']",2,"Despite the remarkable success of deep learning systems over the last decade, a key difference still remains between neural network and human decision-making: As humans, we can not only form a decision on the spot, but also ponder, revisiting an initial guess from different angles, distilling relevant information, arriving at a better decision. Here, we propose RecycleNet, a latent feature recycling method, instilling the pondering capability for neural networks to refine initial decisions over a number of recycling steps, where outputs are fed back into earlier network layers in an iterative fashion. This approach makes minimal assumptions about the neural network architecture and thus can be implemented in a wide variety of contexts. Using medical image segmentation as the evaluation environment, we show that latent feature recycling enables the network to iteratively refine initial predictions even beyond the iterations seen during training, converging towards an improved decision. We evaluate this across a variety of segmentation benchmarks and show consistent improvements even compared with top-performing segmentation methods. This allows trading increased computation time for improved performance, which can be beneficial, especially for safety-critical applications."
Reducing the Side-Effects of Oscillations in Training of Quantized YOLO Networks,"Kartik Gupta, Akshay Asthana","Seeing Machines, Australia",0.0,,100.0,Australia,"Quantized networks use less computational and memory resources and are suitable for deployment on edge devices. While quantization-aware training QAT is the well-studied approach to quantize the networks at low precision, most research focuses on over-parameterized networks for classification with limited studies on popular and edge device friendly single-shot object detection and semantic segmentation methods like YOLO. Moreover, majority of QAT methods rely on Straight-through Estimator (STE) approximation which suffers from an oscillation phenomenon resulting in sub-optimal network quantization. In this paper, we show that it is difficult to achieve extremely low precision (4-bit and lower) for efficient YOLO models even with SOTA QAT methods due to oscillation issue and existing methods to overcome this problem are not effective on these models. To mitigate the effect of oscillation, we first propose Exponentially Moving Average (EMA) based update to the QAT model. Further, we propose a simple QAT correction method, namely QC, that takes only a single epoch of training after standard QAT procedure to correct the error induced by oscillating weights and activations resulting in a more accurate quantized model. With extensive evaluation on COCO dataset using various YOLO5 and YOLO7 variants, we show that our correction method improves quantized YOLO networks consistently on both object detection and segmentation tasks at low-precision (4-bit and 3-bit).",https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Reducing_the_Side-Effects_of_Oscillations_in_Training_of_Quantized_YOLO_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gupta_Reducing_the_Side-Effects_of_Oscillations_in_Training_of_Quantized_YOLO_WACV_2024_paper.pdf,,,2311.05109,main,Poster,https://ieeexplore.ieee.org/document/10483972/,"['YOLO', 'Training', 'Computer vision', 'Quantization (signal)', 'Image edge detection', 'Semantic segmentation', 'Memory management']","['YOLO Network', 'Object Detection', 'Semantic Segmentation', 'Low Precision', 'Memory Resources', 'Edge Devices', 'Quantification Model', 'Object Detection Task', 'COCO Dataset', 'Effect Of Oscillations', 'Oscillation Phenomenon', 'Single Shot Detector', 'Neural Network', 'Deep Neural Network', 'Scaling Factor', 'Convolutional Layers', 'Batch Normalization', 'Trainable Parameters', 'Network Efficiency', 'Quantum State', 'Semantic Segmentation Task', 'Latent State', 'Round Function', 'End Of Training', 'Quantization Levels', 'Correction Parameters', 'Post-hoc Correction', 'Shift Factor', 'Depthwise Convolution', 'Correction Step']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Quantized networks use less computational and memory resources and are suitable for deployment on edge devices. While quantization-aware training (QAT) is a well-studied approach to quantize the networks at low precision, most research focuses on over-parameterized networks for classification with limited studies on popular and edge device friendly single-shot object detection and semantic segmentation methods like YOLO. Moreover, majority of QAT methods rely on Straight Through Estimator (STE) approximation which suffers from an oscillation phenomenon resulting in sub-optimal network quantization. In this paper, we show that it is difficult to achieve extremely low precision (4-bit and lower) for efficient YOLO models even with SOTA QAT methods due to oscillation issue and existing methods to overcome this problem are not effective on these models. To mitigate the effect of oscillation, we first propose Exponentially Moving Average (EMA) based update to the QAT model. Further, we propose a simple QAT correction method, namely QC, that takes only a single epoch of training after standard Quantization-Aware Training (QAT) procedure to correct the error induced by oscillating weights and activations resulting in a more accurate quantized model. With extensive evaluation on COCO dataset using various YOLO5 and YOLO7 variants, we show that our correction method improves quantized YOLO networks consistently on both object detection and segmentation tasks at low-precision (4-bit and 3-bit)."
Reference-Based Restoration of Digitized Analog Videotapes,"Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo","University of Florence - Media Integration and Communication Center (MICC), Florence, Italy",100.0,Italy,0.0,,"Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.",https://openaccess.thecvf.com/content/WACV2024/html/Agnolucci_Reference-Based_Restoration_of_Digitized_Analog_Videotapes_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Agnolucci_Reference-Based_Restoration_of_Digitized_Analog_Videotapes_WACV_2024_paper.pdf,,https://github.com/miccunifi/TAPE,2310.14926,main,Poster,https://ieeexplore.ieee.org/document/10484271/,"['Degradation', 'Computer vision', 'Computational modeling', 'Memory', 'Magnetic devices', 'Computer architecture', 'Transformers']","['Reference Frame', 'Video Frames', 'Digital Video', 'Correction Approach', 'Data Storage Devices', 'Archive', 'Qualitative Results', 'Spatial Dimensions', 'Attention Mechanism', 'Real-world Datasets', 'Consecutive Frames', 'Unsupervised Manner', 'Video Quality', 'Temporal Consistency', 'Types Of Artifacts', 'Input Frames', 'Material For More Details', 'Typical Degradation', 'Deblurring', 'Sequence Of Tokens', 'Reference Ones', 'Photo-realistic Images', 'Part Of The Frame']","['Algorithms', 'Low-level and physics-based vision']",,"Analog magnetic tapes have been the main video data storage device for several decades. Videos stored on analog videotapes exhibit unique degradation patterns caused by tape aging and reader device malfunctioning that are different from those observed in film and digital video restoration tasks. In this work, we present a reference-based approach for the resToration of digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact detection to identify the cleanest frames of each video through textual prompts describing different artifacts. Then, we select the clean frames most similar to the input ones and employ them as references. We design a transformer-based Swin-UNet network that exploits both neighboring and reference frames via our Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on cross-attention and attention pooling to take advantage of the most useful parts of each reference frame. To address the absence of ground truth in real-world videos, we create a synthetic dataset of videos exhibiting artifacts that closely resemble those commonly found in analog videotapes. Both quantitative and qualitative experiments show the effectiveness of our approach compared to other state-of-the-art methods. The code, the model, and the synthetic dataset are publicly available at https://github.com/miccunifi/TAPE."
Refine and Redistribute: Multi-Domain Fusion and Dynamic Label Assignment for Unbiased Scene Graph Generation,"Yujie Zang, Yaochen Li, Yuan Gao, Yimou Guo, Wenneng Tang, Yanxue Li, Meklit Atlaw","School of Software Engineering, Xi'an Jiaotong University; N/A",50.0,China,50.0,,"Scene Graph Generation (SGG) plays an important role in enhancing visual image comprehension. However, existing approaches often struggle to represent implicit relationship features, resulting in a limited ability to distinguish predicates. Meanwhile, they are vulnerable to skewed instance distributions, which impairs effective training for fine-grained predicates. To address these problems, we propose a novel feature refinement and data redistribution framework (RAR). Specifically, a multi-domain fusion (MDF) module is designed to acquire comprehensive predicate representations, integrating global knowledge from the contextual domain and local details in the spatial-frequency domains. Then, we introduce a dynamic label assignment (DLA) strategy to tackle the long-tailed problem. Different predicate categories are adaptively grouped, accommodating varying training conditions. Guided by this strategy, we leverage a hierarchical auto-encoder to generate siamese samples, expanding the label cardinality. Furthermore, we explore the updated sample space to derive reliable samples and assign tailored labels, ultimately achieving the data rebalancing. Experiments on VG and GQA demonstrate that our model contributes to correcting prediction bias and achieves a significant improvement of approximately 10% in mean recall compared to baseline models.",https://openaccess.thecvf.com/content/WACV2024/html/Zang_Refine_and_Redistribute_Multi-Domain_Fusion_and_Dynamic_Label_Assignment_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zang_Refine_and_Redistribute_Multi-Domain_Fusion_and_Dynamic_Label_Assignment_for_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484313/,"['Training', 'Adaptation models', 'Visualization', 'Computer vision', 'Computer network reliability', 'Computational modeling', 'Predictive models']","['Scene Graph', 'Dynamic Assignment', 'Multidomain Fusion', 'Mean Of Recall', 'Feature Refinement', 'Transformer', 'Frequency Domain', 'Feature Space', 'Fast Fourier Transform', 'Object Detection', 'Visual Features', 'Validation Sample', 'Global Information', 'Spatial Domain', 'Confidence Score', 'Head Group', 'Dynamic Strategy', 'Subject And Object', 'Attention Map', 'Annotated Samples', 'Long-tailed Distribution', 'Deformable Convolution', 'Visual Question Answering', 'Tail Group', 'Regularization Constraint', 'Feed-forward Network', 'Receptive Field', 'Local Information']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Scene Graph Generation (SGG) plays an important role in enhancing visual image comprehension. However, existing approaches often struggle to represent implicit relationship features, resulting in a limited ability to distinguish predicates. Meanwhile, they are vulnerable to skewed instance distributions, which impairs effective training for fine-grained predicates. To address these problems, we propose a novel feature refinement and data redistribution framework (RAR). Specifically, a multi-domain fusion (MDF) module is designed to acquire comprehensive predicate representations, integrating global knowledge from the contextual domain and local details in the spatial-frequency domains. Then, we introduce a dynamic label assignment (DLA) strategy to tackle the long-tailed problem. Different predicate categories are adaptively grouped, accommodating varying training conditions. Guided by this strategy, we leverage a hierarchical auto-encoder to generate siamese samples, expanding the label cardinality. Furthermore, we explore the updated sample space to derive reliable samples and assign tailored labels, ultimately achieving the data rebalancing. Experiments on VG and GQA demonstrate that our model contributes to correcting prediction bias and achieves a significant improvement of approximately 10% in mean recall compared to baseline models."
Registered and Segmented Deformable Object Reconstruction From a Single View Point Cloud,"Pit Henrich, Balázs Gyenes, Paul Maria Scheikl, Gerhard Neumann, Franziska Mathis-Ullrich","Dep. Artiﬁcial Intelligence in Biomedical Engineering - FAU Erlangen-Nürnberg, Erlangen, Germany; Institute for Anthropomatics and Robotics - Karlsruhe Institute of Technology, Karlsruhe, Germany",100.0,Germany,0.0,,"In deformable object manipulation, we often want to interact with specific segments of an object that are only defined in non-deformed models of the object. We thus require a system that can recognize and locate these segments in sensor data of deformed real world objects. This is normally done using deformable object registration, which is problem specific and complex to tune. Recent methods utilize neural occupancy functions to improve deformable object registration by registering to an object reconstruction. Going one step further, we propose a system that in addition to reconstruction learns segmentation of the reconstructed object. As the resulting output already contains the information about the segments, we can skip the registration process. Tested on a variety of deformable objects in simulation and the real world, we demonstrate that our method learns to robustly find these segments. We also introduce a simple sampling algorithm to generate better training data for occupancy learning.",https://openaccess.thecvf.com/content/WACV2024/html/Henrich_Registered_and_Segmented_Deformable_Object_Reconstruction_From_a_Single_View_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Henrich_Registered_and_Segmented_Deformable_Object_Reconstruction_From_a_Single_View_WACV_2024_paper.pdf,,,2311.07357,main,Poster,https://ieeexplore.ieee.org/document/10484179/,"['Point cloud compression', 'Deformable models', 'Computer vision', 'Training data']","['Point Cloud', 'Object Reconstruction', 'Deformable Objects', 'Single-view Point Cloud', 'Training Data', 'Sensor Data', 'Real-world Objects', 'Simulated Object', 'Deformable Registration', 'Loss Function', '3D Reconstruction', 'Real-world Data', 'Points In Space', 'Intersection Over Union', '3D Space', 'Depth Images', '3D Mesh', 'Decision Boundary', 'Object Segmentation', 'Reconstruction Quality', 'Query Point', 'Signed Distance Function', 'Positional Encoding', 'L1 Loss', 'Pairs Of Positions', 'Query Vector', 'Voxel Grid', 'Occupancy Values', 'Local Bias', 'Bounding Box']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"In deformable object manipulation, we often want to interact with specific segments of an object that are only defined in non-deformed models of the object. We thus require a system that can recognize and locate these segments in sensor data of deformed real world objects. This is normally done using deformable object registration, which is problem specific and complex to tune. Recent methods utilize neural occupancy functions to improve deformable object registration by registering to an object reconstruction. Going one step further, we propose a system that in addition to reconstruction learns segmentation of the reconstructed object. As the resulting output already contains the information about the segments, we can skip the registration process. Tested on a variety of deformable objects in simulation and the real world, we demonstrate that our method learns to robustly find these segments. We also introduce a simple sampling algorithm to generate better training data for occupancy learning."
Removing the Quality Tax in Controllable Face Generation,"Yiwen Huang, Zhiqiu Yu, Xinjie Yi, Yue Wang, James Tompkin",Brown University,100.0,USA,0.0,,"3DMM conditioned face generation has gained traction due to its well-defined controllability; however, the trade-off is lower sample quality: Previous works such as DiscoFaceGAN and 3D-FM GAN show a significant FID gap compared to the unconditional StyleGAN, suggesting that there is a quality tax to pay for controllability. In this paper, we challenge the assumption that quality and controllability cannot coexist. To pinpoint the previous issues, we mathematically formalize the problem of 3DMM conditioned face generation. Then, we devise simple solutions to the problem under our proposed framework. This results in a new model that effectively removes the quality tax between 3DMM conditioned face GANs and the unconditional StyleGAN. Project webpage: https://visual.cs.brown.edu/taxfreegan",https://openaccess.thecvf.com/content/WACV2024/html/Huang_Removing_the_Quality_Tax_in_Controllable_Face_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huang_Removing_the_Quality_Tax_in_Controllable_Face_Generation_WACV_2024_paper.pdf,http://visual.cs.brown.edu/taxfreegan,,,main,Poster,https://ieeexplore.ieee.org/document/10483996/,"['Computer vision', 'Computational modeling', 'Finance', 'Controllability', 'Mathematical models', 'Faces']","['Face Generation', 'Fréchet Inception Distance', 'Convolution', 'Image Quality', 'Feature Maps', 'Facial Expressions', 'Parameter Space', 'Mutual Information', 'Precision And Recall', 'Magnitude Of Variation', 'Latent Space', 'Face Images', 'Image Space', 'Spherical Harmonics', 'Consistency Loss', 'Domain Gap', 'Image Editing', 'Photo-realistic Images', 'SOTA Methods', 'Resolution Stage']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"3DMM conditioned face generation has gained traction due to its well-defined controllability; however, the trade-off is lower sample quality: Previous works such as DiscoFace-GAN and 3D-FM GAN show a significant FID gap compared to the unconditional StyleGAN, suggesting that there is a quality tax to pay for controllability. In this paper, we challenge the assumption that quality and controllability cannot coexist. To pinpoint the previous issues, we mathematically formalize the problem of 3DMM conditioned face generation. Then, we devise simple solutions to the problem under our proposed framework. This results in a new model that effectively removes the quality tax between 3DMM conditioned face GANs and the unconditional StyleGAN.Project webpage: visual.cs.brown.edu/taxfreegan"
Repetitive Action Counting With Motion Feature Learning,"Xinjie Li, Huijuan Xu",Pennsylvania State University,100.0,USA,0.0,,"Repetitive action counting aims to count the number of repetitive actions in a video. The critical challenge of this task is to uncover the periodic pattern between repetitive actions by computing feature similarity between frames. However, existing methods only rely on the RGB feature of each frame to compute the feature similarity while neglecting the background change of repetitive actions. The abrupt background change may cause feature discrepancies of the same action moment and lead to errors in counting. To this end, we propose a two-branch framework, i.e., RGB and motion branches, with the motion branch complementing the RGB branch to enhance the foreground motion feature learning. Specifically, foreground motion features are highlighted with flow-guided attention on frame features. In addition, to alleviate the noise from moving background distractors and reinforce the periodic pattern, we propose a temporal self-similarity matrix reconstruction loss to improve the temporal correspondence between the same motion feature from different frames. Lastly, to make the motion feature effectively supplement the RGB feature, we present a novel variance-prompted loss weights generation technique to automatically generate dynamic loss weights for two branches in collaborative training. Extensive experiments are conducted on the RepCount and UCFRep datasets to verify our proposed method with state-of-the-art performance. Our method also achieves the best performance on the cross-dataset generalization experiment.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Repetitive_Action_Counting_With_Motion_Feature_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Repetitive_Action_Counting_With_Motion_Feature_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484422/,"['Representation learning', 'Training', 'Computer vision', 'Noise', 'Dynamics', 'Collaboration', 'Task analysis']","['Feature Learning', 'Motion Features', 'Repetitive Activities', 'Weight Loss', 'Reconstruction Loss', 'Dynamic Weight', 'Frame Features', 'Temporal Loss', 'Temporal Correspondence', 'RGB Features', 'Mean Square Error', 'Information Flow', 'Matrix Elements', 'Density Map', 'Flow Characteristics', 'Reconstruction Process', 'Video Sequences', 'Repetitive Tasks', 'Temporal Modulation', 'Red Rectangle', 'Counting Accuracy', 'Temporal Learning', 'Number Of Heads', 'RGB Information', 'Branch Flow', 'Dynamic Datasets', 'Manual Design', 'Video Features', 'Entire Training Process']","['Algorithms', 'Video recognition and understanding']",,"Repetitive action counting aims to count the number of repetitive actions in a video. The critical challenge of this task is to uncover the periodic pattern between repetitive actions by computing feature similarity between frames. However, existing methods only rely on the RGB feature of each frame to compute the feature similarity while neglecting the background change of repetitive actions. The abrupt background change may cause feature discrepancies of the same action moment and lead to errors in counting. To this end, we propose a two-branch framework, i.e., RGB and motion branches, with the motion branch complementing the RGB branch to enhance the foreground motion feature learning. Specifically, foreground motion features are highlighted with flow-guided attention on frame features. In addition, to alleviate the noise from moving background distractors and reinforce the periodic pattern, we propose a temporal self-similarity matrix reconstruction loss to improve the temporal correspondence between the same motion feature from different frames. Lastly, to make the motion feature effectively supplement the RGB feature, we present a novel variance-prompted loss weights generation technique to automatically generate dynamic loss weights for two branches in collaborative training. Extensive experiments are conducted on the RepCount and UCFRep datasets to verify our proposed method with state-of-the-art performance. Our method also achieves the best performance on the cross-dataset generalization experiment."
Residual Graph Convolutional Network for Bird's-Eye-View Semantic Segmentation,"Qiuxiao Chen, Xiaojun Qi","Computer Science, Utah State University, Old Main Hill Logan, Utah",100.0,USA,0.0,,"Retrieving spatial information and understanding the semantic information of the surroundings are important for Bird's-Eye-View (BEV) semantic segmentation. In the application of autonomous driving, autonomous vehicles need to be aware of their surroundings to drive safely. However, current BEV semantic segmentation techniques, deep Convolutional Neural Networks (CNNs) and transformers, have difficulties in efficiently obtaining the global semantic relationships of the surroundings. In this paper, we propose to incorporate a novel Residual Graph Convolutional (RGC) module in deep CNNs to acquire both the global information and the region-level semantic relationship in the multi-view image domain. Specifically, the RGC module employs a non-overlapping graph space projection to efficiently project the complete BEV information into graph space. It then builds interconnected spatial and channel graphs to extract spatial information between each node and channel information within each node (i.e., extract contextual relationships of the global features). Furthermore, it uses a downsample residual process to enhance the coordinate feature reuse to maintain the global information. The segmentation data augmentation and alignment module helps to simultaneously augment and align BEV features and ground truth to geometrically preserve their alignment to achieve better segmentation results. Our experimental results on the nuScenes benchmark dataset demonstrate that the RGC network outperforms four state-of-the-art networks and its four variants in terms of IoU and mIoU. The proposed RGC network achieves a higher mIoU of 3.1% than the best state-of-the-art network, BEVFusion. Code and models will be released.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Residual_Graph_Convolutional_Network_for_Birds-Eye-View_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Residual_Graph_Convolutional_Network_for_Birds-Eye-View_Semantic_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Restoring Degraded Old Films With Recursive Recurrent Transformer Networks,"Shan Lin, Edgar Simo-Serra",Waseda University,100.0,Japan,0.0,,"There exists a large number of old films that have not only artistic value but also historical significance. However, due to the degradation of analogue medium over time, old films often suffer from various deteriorations that make it difficult to restore them with existing approaches. In this work, we proposed a novel framework called Recursive Recurrent Transformer Network (RRTN) which is specifically designed for restoring degraded old films. Our approach introduces several key advancements, including a more accurate film noise mask estimation method, the utilization of second-order grid propagation and flow-guided deformable alignment, and the incorporation of a recursive structure to further improve the removal of challenging film noise. Through qualitative and quantitative evaluations, our approach demonstrates superior performance compared to existing approaches, effectively improving the restoration for difficult film noises that cannot be perfectly handled by existing approaches. The code and model are available at https://github.com/mountln/RRTN-old-film-restoration.",https://openaccess.thecvf.com/content/WACV2024/html/Lin_Restoring_Degraded_Old_Films_With_Recursive_Recurrent_Transformer_Networks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lin_Restoring_Degraded_Old_Films_With_Recursive_Recurrent_Transformer_Networks_WACV_2024_paper.pdf,,https://github.com/mountln/RRTN-old-film-restoration,,main,Poster,https://ieeexplore.ieee.org/document/10483892/,"['Degradation', 'Computer vision', 'Codes', 'Films', 'Noise', 'Estimation', 'Transformers']","['Quantitative Evaluation', 'Historical Significance', 'Artistic Value', 'Number Of Films', 'Recursive Structure', 'Convolutional Layers', 'Data Augmentation', 'Super-resolution', 'Correction Process', 'Optical Flow', 'Inference Time', 'Propagation Characteristics', 'Feature Alignment', 'Perceptual Loss', 'Adjacent Frames', 'Input Frames', 'Temporal Direction', 'Complex Noise', 'Optical Flow Estimation', 'Transformer Block', 'Recursive Step']","['Algorithms', 'Video recognition and understanding']",,"There exists a large number of old films that have not only artistic value but also historical significance. However, due to the degradation of analogue medium over time, old films often suffer from various deteriorations that make it difficult to restore them with existing approaches. In this work, we proposed a novel framework called Recursive Recurrent Transformer Network (RRTN) which is specifically designed for restoring degraded old films. Our approach introduces several key advancements, including a more accurate film noise mask estimation method, the utilization of second-order grid propagation and flow-guided deformable alignment, and the incorporation of a recursive structure to further improve the removal of challenging film noise. Through qualitative and quantitative evaluations, our approach demonstrates superior performance compared to existing approaches, effectively improving the restoration for difficult film noises that cannot be perfectly handled by existing approaches. The code and model are available at https://github.com/mountln/RRTN-old-film-restoration."
Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing,"Yating Xu, Conghui Hu, Gim Hee Lee","Department of Computer Science, National University of Singapore",100.0,Singapore,0.0,,"Existing works on weakly-supervised audio-visual video parsing adopt hybrid attention network (HAN) as the multi-modal embedding to capture the cross-modal context. It embeds the audio and visual modalities with a shared network, where the cross-attention is performed at the input. However, such an early fusion method highly entangles the two non-fully correlated modalities and leads to sub-optimal performance in detecting single-modality events. To deal with this problem, we propose the messenger-guided mid-fusion transformer to reduce the uncorrelated cross-modal context in the fusion. The messengers condense the full cross-modal context into a compact representation to only preserve useful cross-modal information. Furthermore, due to the fact that microphones capture audio events from all directions, while cameras only record visual events within a restricted field of view, there is a more frequent occurrence of unaligned cross-modal context from audio streams for visual event predictions. We thus propose cross-audio prediction consistency to suppress the impact of irrelevant audio information on visual event prediction. Experiments consistently illustrate the superior performance of our framework compared to existing state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_Rethink_Cross-Modal_Fusion_in_Weakly-Supervised_Audio-Visual_Video_Parsing_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_Rethink_Cross-Modal_Fusion_in_Weakly-Supervised_Audio-Visual_Video_Parsing_WACV_2024_paper.pdf,,,2311.08151,main,Poster,,,,,,
Rethinking Knowledge Distillation With Raw Features for Semantic Segmentation,"Tao Liu, Chenshu Chen, Xi Yang, Wenming Tan","Hikvision Research Institute, Hangzhou, Zhejiang, China",100.0,China,0.0,,"Most existing knowledge distillation methods for semantic segmentation focus on extracting various sophisticated knowledge from raw features. However, such knowledge is usually manually designed and relies on prior knowledge as in traditional feature engineering. In this paper, we aim to propose a simple and effective feature distillation method using raw features. To this end, we revisit the pioneering work in feature distillation, FitNets, which simply minimizes the mean squared error (MSE) loss between the teacher and student features. Our experiments show that this naive method yields good results, even surpassing some well-designed methods in some cases. However, it requires carefully tuning the weight of distillation loss. By decomposing the loss function of FitNets into a magnitude difference term and an angular difference term, we find the weight of the angular difference term is affected by the magnitudes of the teacher features and the student features. We experimentally show that the angular difference term plays a crucial role in feature distillation and the magnitude of the features produced by different models may vary significantly. Therefore, it is hard to determine a suitable loss weight for various models. To avoid the weight of the angular distillation term being affected by the magnitude of the features, we propose Angular Distillation and explore distilling angular information along different feature dimensions for semantic segmentation. Extensive experiments show that our simple method exhibits great robustness to hyper-parameters and achieves state-of-the-art distillation performance for semantic segmentation.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Rethinking_Knowledge_Distillation_With_Raw_Features_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Rethinking_Knowledge_Distillation_With_Raw_Features_for_Semantic_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484265/,"['Knowledge engineering', 'Computer vision', 'Sensitivity', 'Semantic segmentation', 'Feature extraction', 'Robustness', 'Tuning']","['Semantic Segmentation', 'Raw Features', 'Features For Semantic Segmentation', 'Weight Loss', 'Mean Square Error', 'Extensive Experiments', 'Feature Dimension', 'Model Weights', 'Angular Deviation', 'Term Weight', 'Distillation Method', 'Manual Design', 'Naive Method', 'Angular Information', 'Magnitude Characteristics', 'Distillation Loss', 'Learning Rate', 'Feature Maps', 'Object Detection', 'Forms Of Knowledge', 'PASCAL VOC', 'Feature-based Methods', 'Semantic Segmentation Methods', 'Locality Sensitive Hashing', 'Generative Adversarial Networks', 'Soft Labels', 'Gram Matrix', 'Segmentation Performance', 'Feature Points', 'Stochastic Gradient Descent']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Most existing knowledge distillation methods for semantic segmentation focus on extracting various sophisticated knowledge from raw features. However, such knowledge is usually manually designed and relies on prior knowledge as in traditional feature engineering. In this paper, we aim to propose a simple and effective feature distillation method using raw features. To this end, we revisit the pioneering work in feature distillation, FitNets, which simply minimizes the mean squared error (MSE) loss between the teacher and student features. Our experiments show that this naive method yields good results, even surpassing some well-designed methods in some cases. However, it requires carefully tuning the weight of distillation loss. By decomposing the loss function of FitNets into a magnitude difference term and an angular difference term, we find the weight of the angular difference term is affected by the magnitudes of the teacher features and the student features. We experimentally show that the angular difference term plays a crucial role in feature distillation and the magnitude of the features produced by different models may vary significantly. Therefore, it is hard to determine a suitable loss weight for various models. To avoid the weight of the angular distillation term being affected by the magnitude of the features, we propose Angular Distillation and explore distilling angular information along different feature dimensions for semantic segmentation. Extensive experiments show that our simple method exhibits great robustness to hyper-parameters and achieves state-of-the-art distillation performance for semantic segmentation."
Rethinking Multimodal Content Moderation From an Asymmetric Angle With Mixed-Modality,"Jialin Yuan, Ye Yu, Gaurav Mittal, Matthew Hall, Sandra Sajeev, Mei Chen",Oregon State University; Microsoft Inc.,50.0,USA,50.0,USA,"There is a rapidly growing need for multimodal content moderation (CM) as more and more content on social media is multimodal in nature. Existing unimodal CM systems may fail to catch harmful content that crosses modalities (e.g., memes or videos), which may lead to severe consequences. In this paper, we present a novel CM model, Asymmetric Mixed-Modal Moderation (AM3), to target multimodal and unimodal CM tasks. Specifically, to address the asymmetry in semantics between vision and language, AM3 has a novel asymmetric fusion architecture that is designed to not only fuse the common knowledge in both modalities but also to exploit the unique information in each modality. Unlike pre- vious works that focus on representing the two modalities in similar feature space while overlooking the intrinsic difference between the information conveyed in multimodality and in unimodality (asymmetry in modalities), we propose a novel cross-modality contrastive loss to learn the unique knowledge that only appears in multimodality. This is critical as some harmful intent may only be conveyed through the intersection of both modalities. With extensive experiments, we show that AM3 outperforms all existing state-of-the-art methods on both multimodal and unimodal CM benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Yuan_Rethinking_Multimodal_Content_Moderation_From_an_Asymmetric_Angle_With_Mixed-Modality_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_Rethinking_Multimodal_Content_Moderation_From_an_Asymmetric_Angle_With_Mixed-Modality_WACV_2024_paper.pdf,,,2305.10547,main,Poster,https://ieeexplore.ieee.org/document/10483998/,"['Computer vision', 'Art', 'Fuses', 'Social networking (online)', 'Semantics', 'Computer architecture', 'Benchmark testing']","['Content Moderation', 'Multimodal Content', 'Asymmetric Angle', 'Social Media', 'Semantic', 'Contrastive Loss', 'Modal Information', 'Unique Knowledge', 'Harmful Content', 'Training Set', 'Object Detection', 'Large-scale Datasets', 'Bounding Box', 'Language Model', 'Vision Tasks', 'Fake News', 'Tokenized', 'Hate Speech', 'Input Text', 'Visualization Of Datasets', 'Text Dataset', 'Position Embedding', 'Symmetric Design', 'Multimodal Dataset', 'Masked Language Model', 'Classification Head', 'Image Captioning', 'Language Mode', 'Distinct Knowledge', 'Classification Task']","['Applications', 'Social good', 'Algorithms', 'Vision + language and/or other modalities']",1,"There is a rapidly growing need for multimodal content moderation (CM) as more and more content on social media is multimodal in nature. Existing unimodal CM systems may fail to catch harmful content that crosses modalities (e.g., memes or videos), which may lead to severe consequences. In this paper, we present a novel CM model, Asymmetric Mixed-Modal Moderation (AM3), to target multimodal and unimodal CM tasks. Specifically, to address the asymmetry in semantics between vision and language, AM3 has a novel asymmetric fusion architecture that is designed to not only fuse the common knowledge in both modalities but also to exploit the unique information in each modality. Unlike previous works that focus on representing the two modalities into a similar feature space while overlooking the intrinsic difference between the information conveyed in multimodality and in unimodality (asymmetry in modalities), we propose a novel cross-modality contrastive loss to learn the unique knowledge that only appears in multimodality. This is critical as some harmful intent may only be conveyed through the intersection of both modalities. With extensive experiments, we show that AM3 outperforms all existing state-of-the-art methods on both multimodal and unimodal CM benchmarks."
Rethinking Visibility in Human Pose Estimation: Occluded Pose Reasoning via Transformers,"Pengzhan Sun, Kerui Gu, Yunsong Wang, Linlin Yang, Angela Yao",Communication University of China; National University of Singapore,100.0,"China, Singapore",0.0,,"Occlusion is a common challenge in human pose estimation. Curiously, learning from occluded keypoints hinders a model to detect visible keypoints. We speculate that the impairment is likely due to a forced correlation between keypoints and visual features of the occluders. As such, we propose a novel visibility-aware attention mechanism to eliminate unreliable occluding features. The explicit occlusion handling encourages the model to reason about occluded keypoints using evidence and contextual information from the visible keypoints. It also mitigates the damage of unreliable correlations of the occluded keypoints. Our method, when added to the strong baseline SimCC, improves by 1.3 AP and 0.7 AP with ResNet and HRNet respectively. It also surpasses the state-of-the-art I^2R-Net on CrowdPose by 0.3 AP and 0.6 AP^hard. The improvements highlight that rethinking visibility information is critical for developing effective human pose estimation systems.",https://openaccess.thecvf.com/content/WACV2024/html/Sun_Rethinking_Visibility_in_Human_Pose_Estimation_Occluded_Pose_Reasoning_via_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Rethinking_Visibility_in_Human_Pose_Estimation_Occluded_Pose_Reasoning_via_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484331/,"['Visualization', 'Computer vision', 'Correlation', 'Pose estimation', 'Transformers', 'Cognition', 'Context modeling']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', 'Visual Information', 'Convolutional Neural Network', 'Small Datasets', 'Visual Cues', 'Data Augmentation', 'Common Way', 'Bounding Box', 'Graph Convolutional Network', 'Explicit Method', 'Implicit Method', 'Siamese Network', 'Keypoint Detection', 'Occlusion Problem', 'MS COCO Dataset', 'Pose Estimation Methods', 'Transformer Block', 'Standard Evaluation Metrics', 'Keypoint Locations', 'Ground Truth Location']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Image recognition and understanding']",,"Occlusion is a common challenge in human pose estimation. Curiously, learning from occluded keypoints hinders a model to detect visible keypoints. We speculate that the impairment is likely due to a forced correlation between keypoints and visual features of the occluders. As such, we propose a novel visibility-aware attention mechanism to eliminate unreliable occluding features. The explicit occlusion handling encourages the model to reason about occluded keypoints using evidence and contextual information from the visible keypoints. It also mitigates the damage of unreliable correlations of the occluded keypoints. Our method, when added to the strong baseline SimCC, improves by 1.3 AP and 0.7 AP with ResNet and HRNet respectively. It also surpasses the state-of-the-art I
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
R-Net on CrowdPose by 0.3 AP and 0.6 AP
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">hard</sup>
. The improvements highlight that rethinking visibility information is critical for developing effective human pose estimation systems."
Reverse Knowledge Distillation: Training a Large Model Using a Small One for Retinal Image Matching on Limited Data,"Sahar Almahfouz Nasser, Nihar Gupte, Amit Sethi","Indian Institute of Technology Bombay, Mumbai, Maharashtra, India",100.0,India,0.0,,"Retinal image matching (RIM) plays a crucial role in monitoring disease progression and treatment response as retina is the only tissue where blood vessels can be directly observed. However, datasets with matched keypoints between temporally separated pairs of images are not available in abundance to train transformer-based models. Firstly, we release keypoint annotations for retinal images from multiple datasets to aid further research on RIM. Secondly, we propose a novel approach based on reverse knowledge distillation to train large models with limited data while preventing overfitting. We propose architectural modifications to a CNN-based semi-supervised method called SuperRetina [22] that helps improve its results on a publicly available dataset. We train a computationally heavier model based on a vision transformer encoder, utilizing the lighter CNN-based model. This approach, which we call reverse knowledge distillation (RKD), further improves the matching results even though it contrasts with the conventional knowledge distillation where lighter models are trained based on heavier ones is the norm. Further, we show that our technique generalizes to other domains, such as facial landmark matching.",https://openaccess.thecvf.com/content/WACV2024/html/Nasser_Reverse_Knowledge_Distillation_Training_a_Large_Model_Using_a_Small_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nasser_Reverse_Knowledge_Distillation_Training_a_Large_Model_Using_a_Small_WACV_2024_paper.pdf,,,2307.10698,main,Poster,https://ieeexplore.ieee.org/document/10484345/,"['Training', 'Computer vision', 'Annotations', 'Computational modeling', 'Image matching', 'Retina', 'Transformers']","['Image Registration', 'Retinal Images', 'Vision Transformer', 'Loss Function', 'Convolutional Neural Network', 'Convolutional Layers', 'Transfer Learning', 'Teacher Model', 'Object Recognition', 'Large Model', 'Small Model', 'Semantic Segmentation', 'Transformer Model', 'Image Point', 'Pose Estimation', 'Normal Images', 'Student Model', 'Long-range Dependencies', 'Scale-invariant Feature Transform', 'Viewpoint Changes', 'Keypoint Detection', 'Distillation Loss', 'Speeded Up Robust Features', 'Unlabeled Data', 'Median Error', 'Feature Point Detection', 'Unsupervised Techniques', 'Face Recognition', 'Object Detection', 'Computer Vision']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",2,"Retinal image matching (RIM) plays a crucial role in monitoring disease progression and treatment response as retina is the only tissue where blood vessels can be directly observed. However, datasets with matched keypoints between temporally separated pairs of images are not available in abundance to train transformer-based models. Firstly, we release keypoint annotations for retinal images from multiple datasets to aid further research on RIM. Secondly, we propose a novel approach based on reverse knowledge distillation to train large models with limited data while preventing overfitting. We propose architectural modifications to a CNN-based semi-supervised method called SuperRetina [22] that helps improve its results on a publicly available dataset. We train a computationally heavier model based on a vision transformer encoder, utilizing the lighter CNN-based model. This approach, which we call reverse knowledge distillation (RKD), further improves the matching results even though it contrasts with the conventional knowledge distillation where lighter models are trained based on heavier ones is the norm. Further, we show that our technique generalizes to other domains, such as facial landmark matching."
Revisiting Latent Space of GAN Inversion for Robust Real Image Editing,"Kai Katsumata, Duc Minh Vo, Bei Liu, Hideki Nakayama",The University of Tokyo; Microsoft Research,50.0,Japan,50.0,USA,"We present a generative adversarial network (GAN) inversion with high reconstruction and editing quality. GAN inversion algorithms with expressive latent spaces produce near-perfect inversion but are not robust to editing operations in latent space, leading to undesirable edited images, a phenomenon known as the trade-off between reconstruction and editing quality. To cope with the trade-off, we revisit the hyperspherical prior of StyleGANs Z and propose to combine an extended space of Z with highly capable inversion algorithms. Our approach maintains the reconstruction quality of seminal GAN inversion methods while improving their editing quality owing to the constrained nature of Z. Through comprehensive experiments with several GAN inversion algorithms, we demonstrate that our approach enhances image editing quality in 2D/3D GANs.",https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Revisiting_Latent_Space_of_GAN_Inversion_for_Robust_Real_Image_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Revisiting_Latent_Space_of_GAN_Inversion_for_Robust_Real_Image_WACV_2024_paper.pdf,,https://github.com/raven38/hypershpere-gan-inversion,,main,Poster,https://ieeexplore.ieee.org/document/10483826/,"['Computer vision', 'Extraterrestrial phenomena', 'Generative adversarial networks', 'Image reconstruction']","['Generative Adversarial Networks', 'Latent Space', 'Image Editing', 'Generative Adversarial Networks Inversion', 'Real Image Editing', 'Inverse Method', 'Reconstruction Quality', 'Inversion Algorithm', 'Hypersphere', 'High-quality Reconstruction', 'High Editing', 'StyleGAN', 'Edit Operations', 'Feature Space', 'Local Method', 'Target Image', 'Affine Transformation', 'Comparable Quality', 'High-density Regions', 'Reconstruction Performance', 'Latent Code', 'Mean Square Error Loss', 'Structural Similarity Index Measure', 'Generative Adversarial Networks Model', 'Perceptual Loss', 'Generation Layer']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Applications', 'Arts / games / social media']",1,"We present a generative adversarial network (GAN) inversion with high reconstruction and editing quality. GAN inversion algorithms with expressive latent spaces produce near-perfect inversion but are not robust to editing operations in a latent space, leading to undesirable edited images, a phenomenon known as the trade-off between reconstruction and editing quality. To cope with the trade-off, we revisit the hyperspherical prior of StyleGANs $\mathcal{Z}$ and propose to combine an extended space of $\mathcal{Z}$ with highly capable inversion algorithms. Our approach maintains the reconstruction quality of seminal GAN inversion methods while improving their editing quality owing to the constrained nature of $\mathcal{Z}$. Through comprehensive experiments with several GAN inversion algorithms, we demonstrate that our approach enhances the image editing quality in 2D/3D GANs.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Revisiting Pixel-Level Contrastive Pre-Training on Scene Images,"Zongshang Pang, Yuta Nakashima, Mayu Otani, Hajime Nagahara","Osaka University; CyberAgent, Inc.",50.0,Japan,50.0,Japan,"Contrastive image representation learning through instance discrimination has shown impressive transfer performance. Recent strategies have focused on pushing the limit of their transfer performance for dense prediction tasks, particularly when conducting pre-training on scene images with complex structures. Initial approaches employ pixel-level contrastive pre-training to optimize dense spatial features, while subsequent methods utilize region-mining algorithms to capture holistic regional semantics and address the issue of semantically inconsistent scene image crops. In this paper, we revisit pixel-level contrastive pre-training on scene images. Contrary to the assumption that pixel-level learning falls short in achieving these objectives, we demonstrate its under-explored potentials: (1) it can effectively learn holistic regional semantics more simply compared to region-level methods, and (2) it intrinsically provides tools to mitigate the impact of semantically inconsistent views involved with scene-level training images. We propose PixCon, a pixel-level contrastive learning framework, and explore two variants with different positive matching strategies to investigate the potential of pixel-level learning. Additionally, when PixCon incorporates a novel semantic reweighting approach tailored for scene image pre-training, it outperforms or matches the performance of previous region-level methods in object detection and semantic segmentation tasks across multiple benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Pang_Revisiting_Pixel-Level_Contrastive_Pre-Training_on_Scene_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pang_Revisiting_Pixel-Level_Contrastive_Pre-Training_on_Scene_Images_WACV_2024_paper.pdf,,https://github.com/pangzss/PixCon,,main,Poster,https://ieeexplore.ieee.org/document/10484155/,"['Training', 'Computer vision', 'Semantic segmentation', 'Semantics', 'Crops', 'Self-supervised learning', 'Object detection']","['Scene Images', 'Spatial Features', 'Object Detection', 'Representation Learning', 'Semantic Segmentation', 'Potential Learning', 'Self-supervised Learning', 'Transfer Performance', 'Matching Strategy', 'Object Detection Methods', 'Semantic Segmentation Task', 'Positive Matches', 'Similar Characteristics', 'Spatial Information', 'Input Image', 'Feature Maps', 'Data Augmentation', 'Transfer Learning', 'Batch Normalization', 'Semantic Similarity', 'Instance Segmentation', 'Asymmetric Structure', 'Semantic Consistency', 'Query Features', 'Pixel Features', 'Beginning Of Training', 'Multilayer Perception', 'Backbone Network', 'Random Cropping', 'Semantic Content']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Contrastive image representation learning through instance discrimination has shown impressive transfer performance. Recent strategies have focused on pushing the limit of their transfer performance for dense prediction tasks, particularly when conducting pre-training on scene images with complex structures. Initial approaches employ pixel-level contrastive pre-training to optimize dense spatial features, while subsequent methods utilize region-mining algorithms to capture holistic regional semantics and address the issue of semantically inconsistent scene image crops. In this paper, we revisit pixel-level contrastive pre-training on scene images. Contrary to the assumption that pixel-level learning falls short in achieving these objectives, we demonstrate its under-explored potentials: (1) it can effectively learn holistic regional semantics more simply compared to region-level methods, and (2) it intrinsically provides tools to mitigate the impact of semantically inconsistent views involved with scene-level training images. We propose PixCon, a pixel-level contrastive learning framework, and explore two variants with different positive matching strategies to investigate the potential of pixel-level learning. Additionally, when PixCon incorporates a novel semantic reweighting approach tailored for scene image pre-training, it outperforms or matches the performance of previous region-level methods in object detection and semantic segmentation tasks across multiple benchmarks.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Revisiting Token Pruning for Object Detection and Instance Segmentation,"Yifei Liu, Mathias Gehrig, Nico Messikommer, Marco Cannici, Davide Scaramuzza","Robotics and Perception Group, University of Zurich, Switzerland",100.0,Switzerland,0.0,,"Vision Transformers (ViTs) have shown impressive performance in computer vision, but their high computational cost, quadratic in the number of tokens, limits their adoption in computation-constrained applications. However, this large number of tokens may not be necessary, as not all tokens are equally important. In this paper, we investigate token pruning to accelerate inference for object detection and instance segmentation, extending prior works from image classification. Through extensive experiments, we offer four insights for dense tasks: (i) tokens should not be completely pruned and discarded, but rather preserved in the feature maps for later use. (ii) reactivating previously pruned tokens can further enhance model performance. (iii) a dynamic pruning rate based on images is better than a fixed pruning rate. (iv) a lightweight, 2-layer MLP can effectively prune tokens, achieving accuracy comparable with complex gating networks with a simpler design. We assess the effects of these design decisions on the COCO dataset and introduce an approach that incorporates these findings, showing a reduction in performance decline from  1.5 mAP to  0.3 mAP in both boxes and masks, compared to existing token pruning methods. In relation to the dense counterpart that utilizes all tokens, our method realizes an increase in inference speed, achieving up to 34% faster performance for the entire network and 46% for the backbone. Code will be publicly available.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Revisiting_Token_Pruning_for_Object_Detection_and_Instance_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Revisiting_Token_Pruning_for_Object_Detection_and_Instance_Segmentation_WACV_2024_paper.pdf,,https://github.com/uzh-rpg/svit/,2306.07050,main,Poster,https://ieeexplore.ieee.org/document/10483924/,"['Instance segmentation', 'Computer vision', 'Codes', 'Object detection', 'Transformers', 'Computational efficiency', 'Task analysis']","['Object Detection', 'Instance Segmentation', 'Image Classification', 'Feature Maps', 'Inference Speed', 'COCO Dataset', 'Vision Transformer', 'Classification Task', 'Scope Of This Paper', 'Input Image', 'Qualitative Results', 'Image Regions', 'Selective Modulators', 'Density Model', 'Sparse Model', 'Improve Model Performance', 'Field Of Computer Vision', 'Societal Impact', 'Attention Scores', 'Early Layers', 'Multi-head Self-attention', 'Gate Modulation', 'Detection Head']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",2,"Vision Transformers (ViTs) have shown impressive performance in computer vision, but their high computational cost, quadratic in the number of tokens, limits their adoption in computation-constrained applications. However, this large number of tokens may not be necessary, as not all tokens are equally important. In this paper, we investigate token pruning to accelerate inference for object detection and instance segmentation, extending prior works from image classification. Through extensive experiments, we offer four insights for dense tasks: (i) tokens should not be completely pruned and discarded, but rather preserved in the feature maps for later use. (ii) reactivating previously pruned tokens can further enhance model performance. (iii) a dynamic pruning rate based on images is better than a fixed pruning rate. (iv) a lightweight, 2-layer MLP can effectively prune tokens, achieving accuracy comparable with complex gating networks with a simpler design. We assess the effects of these design decisions on the COCO dataset and introduce an approach that incorporates these findings, showing a reduction in performance decline from ∼1.5 mAP to ∼0.3 mAP in both boxes and masks, compared to existing token pruning methods. In relation to the dense counterpart that utilizes all tokens, our method realizes an increase in inference speed, achieving up to 34% faster performance for the entire network and 46% for the backbone. Code: https://github.com/uzh-rpg/svit/"
Revolutionize the Oceanic Drone RGB Imagery With Pioneering Sun Glint Detection and Removal Techniques,"Jiangying Qin, Ming Li, Jie Zhao, Jiageng Zhong, Hanqi Zhang","Wuhan University; Technische Universität München; Wuhan University, ETH Zürich",100.0,"China, Germany, Switzerland",0.0,,"The issue of sun glint poses a significant challenge for ocean remote sensing with high-resolution ocean drone imagery, as it contaminates images and obstructs crucial features in shallow-waters, leading to inaccurate benthic substrates identification. While various physics-based statistical solutions have been proposed to address this optical issue in remote sensing, there is a lack of sun glint detection and removal methods specifically designed for high-resolution consumer-grade drone RGB imagery. In this paper, we present a pioneering pipeline for sun glint detection and removal in high-resolution drone RGB images, aiming to restore the real features that are hindered by sun glint. Our approach involves the development of a Foreground Attention-based Semantic Segmentation Network (FANet) for accurate and precise sun glint detection, while effective sun glint removal is achieved through pixel propagation using an optical flow field. Experimental results demonstrate the effectiveness of our FANet in identifying sun glint, achieving IoU accuracy of 81.34% for sun glint pixels and 99.52% for non-sun glint background pixels. Furthermore, the quantitative evaluation of sun glint removal using two well-known metrics show that our method outperforms the GAN-based image restoration method (DeepFillv2) and the conventional image interpolation method (Fast Marching Method, hereafter referred to as FMM). Thus, our pipeline lays the foundation for accurate and precise marine costal ecological monitoring and seafloor topographic mapping using consumer-grade drone at a low cost.",https://openaccess.thecvf.com/content/WACV2024/html/Qin_Revolutionize_the_Oceanic_Drone_RGB_Imagery_With_Pioneering_Sun_Glint_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Qin_Revolutionize_the_Oceanic_Drone_RGB_Imagery_With_Pioneering_Sun_Glint_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484034/,"['Pipelines', 'Sea measurements', 'Image restoration', 'Optical sensors', 'Sun', 'Task analysis', 'Remote sensing']","['Sun Glint', 'High-resolution Images', 'Quantitative Evaluation', 'Intersection Over Union', 'Semantic Segmentation', 'RGB Images', 'Optical Flow', 'Removal Method', 'Semantic Network', 'Background Pixels', 'Real Features', 'Semantic Segmentation Network', 'Ecological Monitoring', 'Loss Function', 'Deep Learning', 'Feature Maps', 'Image Segmentation', 'Cross-entropy Loss', 'Generative Adversarial Networks', 'Image Patches', 'Cross-entropy Loss Function', 'Optical Flow Estimation', 'Annotated Dataset', 'Output Side', 'Encoder-decoder Structure', 'Skip Connections', 'Habitat Mapping', 'Atrous Spatial Pyramid Pooling', 'Decoding Stage', 'Global Motion']","['Applications', 'Remote Sensing', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Environmental monitoring / climate change / ecology']",,"The issue of sun glint poses a significant challenge for ocean remote sensing with high-resolution ocean drone imagery, as it contaminates images and obstructs crucial features in shallow-waters, leading to inaccurate benthic substrates identification. While various physics-based statistical solutions have been proposed to address this optical issue in remote sensing, there is a lack of sun glint detection and removal methods specifically designed for high-resolution consumer-grade drone RGB imagery. In this paper, we present a pioneering pipeline for sun glint detection and removal in high-resolution drone RGB images, aiming to restore the real features that are hindered by sun glint. Our approach involves the development of a Foreground Attention-based Semantic Segmentation Network (FANet) for accurate and precise sun glint detection, while effective sun glint removal is achieved through pixel propagation using an optical flow field. Experimental results demonstrate the effectiveness of our FANet in identifying sun glint, achieving IoU accuracy of 81.34% for sun glint pixels and 99.52% for non-sun glint background pixels. Furthermore, the quantitative evaluation of sun glint removal using two well-known metrics show that our method outperforms the GAN-based image restoration method (DeepFillv2) and the conventional image interpolation method (Fast Marching Method, hereafter referred to as FMM). Thus, our pipeline lays the foundation for accurate and precise marine coastal ecological monitoring and seafloor topographic mapping using consumer-grade drone at a low cost."
Robust Category-Level 3D Pose Estimation From Diffusion-Enhanced Synthetic Data,"Jiahao Yang, Wufei Ma, Angtian Wang, Xiaoding Yuan, Alan Yuille, Adam Kortylewski","Peking University; Johns Hopkins University; University of Freiburg, Max Planck Institute for Informatics",100.0,"China, Germany, USA",0.0,,"Obtaining accurate 3D object poses is vital for numerous computer vision applications, such as 3D reconstruction and scene understanding. However, annotating real-world objects is time-consuming and challenging. While synthetically generated training data is a viable alternative, the domain shift between real and synthetic data is a significant challenge. In this work, we aim to narrow the performance gap between models trained on synthetic data and fully supervised models trained on a large amount of real data. We achieve this by approaching the problem from two perspectives: 1) We introduce P3D-Diffusion, a new synthetic dataset with accurate 3D annotations generated with a graphics-guided diffusion model. 2) We propose Cross-domain 3D Consistency, CC3D, for unsupervised domain adaptation of neural mesh models. In particular, we exploit the spatial relationships between features on the mesh surface and a contrastive learning scheme to guide the domain adaptation process. Combined, these two approaches enable our models to perform competitively with state-of-the-art models using only 10% of the respective real training images, while outperforming the SOTA model by a wide margin using only 50% of the real training data. By encouraging the diversity of synthetic data and generating the images with an OOD-aware manner, our model further demonstrates robust generalization to out-of-distribution scenarios despite being trained with minimal real data.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_Robust_Category-Level_3D_Pose_Estimation_From_Diffusion-Enhanced_Synthetic_Data_WACV_2024_paper.pdf,,https://github.com/YangYY06/synthetic_3d,,main,Poster,https://ieeexplore.ieee.org/document/10484073/,"['Adaptation models', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Annotations', 'Computational modeling', 'Pose estimation']","['Pose Estimation', 'Human Pose Estimation', '3D Pose', 'Robust 3D Pose', 'Training Data', 'Neural Model', '3D Reconstruction', 'Training Images', 'Domain Shift', 'Performance Gap', 'Domain Adaptation', 'Self-supervised Learning', 'Accurate Annotation', 'Synthetic Generation', 'Object Pose', 'Feature Maps', 'Joint Effect', 'Annotation Data', 'Latent Space', 'High-quality Images', 'Synthetic Images', 'Unlabeled Data', 'Style Transfer', 'CAD Model', 'Pseudo Labels', 'Domain-specific Features', 'Domain Gap', 'Realistic Images', 'Extracted Feature Maps', 'Real-world Images']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",,"Obtaining accurate 3D object poses is vital for numerous computer vision applications, such as 3D reconstruction and scene understanding. However, annotating real-world objects is time-consuming and challenging. While synthetically generated training data is a viable alternative, the domain shift between real and synthetic data is a significant challenge. In this work, we aim to narrow the performance gap between models trained on synthetic data and fully supervised models trained on a large amount of real data. We achieve this by approaching the problem from two perspectives: 1) We introduce P3D-Diffusion, a new synthetic dataset with accurate 3D annotations generated with a graphics-guided diffusion model. 2) We propose Cross-domain 3D Consistency, CC3D, for unsupervised domain adaptation of neural mesh models. In particular, we exploit the spatial relationships between features on the mesh surface and a contrastive learning scheme to guide the domain adaptation process. Combined, these two approaches enable our models to perform competitively with state-of-the-art models using only 10% of the respective real training images, while outperforming the SOTA model by a wide margin using only 50% of the real training data. By encouraging the diversity of synthetic data and generating the images with an OOD-aware manner, our model further demonstrates robust generalization to out-of-distribution scenarios despite being trained with minimal real data. The code is available at https://github.com/YangYY06/synthetic_3d."
Robust Eye Blink Detection Using Dual Embedding Video Vision Transformer,"Jeongmin Hong, Joseph Shin, Juhee Choi, Minsam Ko",Hanyang University ERICA Campus,100.0,South Korea,0.0,,"Eye blink detection serves as a crucial biomarker for evaluating both physical and mental states, garnering considerable attention in biometric and video-based studies. Among various methods, video-based eye blink detection has been particularly favored due to its non-invasive nature, enabling broader applications. However, capturing eye blinks from different camera angles poses significant challenges, primarily because the eye region is relatively small and eye blinks occur rapidly, necessitating a robust detection algorithm. To address these challenges, we introduce Dual Embedding Video Vision Transformer (DE-ViViT), a novel approach for eye blink detection that employs two different embedding strategies: (i) tubelet embedding and (ii) residual embedding. Each embedding can capture large and subtle changes within the eye movement sequence respectively. We rigorously evaluate our proposed method using HUST-LEBW, a publicly available dataset, as well as our newly collected multi-angle eye blink dataset (MAEB). The results indicate that the proposed model consistently outperforms existing methods across both datasets, with notably minor performance variations depending on the camera angles.",https://openaccess.thecvf.com/content/WACV2024/html/Hong_Robust_Eye_Blink_Detection_Using_Dual_Embedding_Video_Vision_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hong_Robust_Eye_Blink_Detection_Using_Dual_Embedding_Video_Vision_Transformer_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484125/,"['Computer vision', 'Biometrics (access control)', 'Cameras', 'Transformers', 'Robustness', 'Timing', 'Detection algorithms']","['Eye Blinks', 'Vision Transformer', 'Blink Detection', 'Dual Transformation', 'Eye Blink Detection', 'Performance Variables', 'Rigorous Evaluation', 'Robust Detection', 'Eye Region', 'Camera Angle', 'Learning Rate', 'Convolutional Neural Network', 'Comprehensive Assessment', 'Short-term Memory', 'Binary Classification', 'Performance Metrics', 'Long Short-term Memory', 'Left Eye', 'Multilayer Perceptron', 'Long Short-term Memory Network', 'Transformer Architecture', 'Attention Scores', 'Frontal View', 'Embedding Vectors', 'Transformer Encoder', 'Motion Vector', 'Image-based Detection', 'Facial Movements', 'Pre-defined Threshold', 'Eye Images']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Video recognition and understanding']",,"Eye blink detection serves as a crucial biomarker for evaluating both physical and mental states, garnering considerable attention in biometric and video-based studies. Among various methods, video-based eye blink detection has been particularly favored due to its non-invasive nature, enabling broader applications. However, capturing eye blinks from different camera angles poses significant challenges, primarily because the eye region is relatively small and eye blinks occur rapidly, necessitating a robust detection algorithm. To address these challenges, we introduce Dual Embedding Video Vision Transformer (DEViViT), a novel approach for eye blink detection that employs two different embedding strategies: (i) tubelet embedding and (ii) residual embedding. Each embedding can capture large and subtle changes within the eye movement sequence respectively. We rigorously evaluate our proposed method using HUST-LEBW, a publicly available dataset, as well as our newly collected multi-angle eye blink dataset (MAEB). The results indicate that the proposed model consistently outperforms existing methods across both datasets, with notably minor performance variations depending on the camera angles."
Robust Feature Learning and Global Variance-Driven Classifier Alignment for Long-Tail Class Incremental Learning,"Jayateja Kalla, Soma Biswas","Department of Electrical Engineering, Indian Institute of Science, Bangalore, India.",100.0,India,0.0,,"This paper introduces a two-stage framework designed to enhance long-tail class incremental learning, enabling the model to progressively learn new classes, while mitigating catastrophic forgetting in the context of long-tailed data distributions. Addressing the challenge posed by the under-representation of tail classes in long-tail class incremental learning, our approach achieves classifier alignment by leveraging global variance as an informative measure and class prototypes in the second stage. This process effectively captures class properties and eliminates the need for data balancing or additional layer tuning. Alongside traditional class incremental learning losses in the first stage, the proposed approach incorporates mixup classes to learn robust feature representations, ensuring smoother boundaries. The proposed framework can seamlessly integrate as a module with any class incremental learning method to effectively handle long-tail class incremental learning scenarios. Extensive experimentation on the CIFAR-100 and ImageNet-Subset datasets validates the approach's efficacy, showcasing its superiority over state-of-the-art techniques across various long-tail CIL settings.",https://openaccess.thecvf.com/content/WACV2024/html/Kalla_Robust_Feature_Learning_and_Global_Variance-Driven_Classifier_Alignment_for_Long-Tail_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kalla_Robust_Feature_Learning_and_Global_Variance-Driven_Classifier_Alignment_for_Long-Tail_WACV_2024_paper.pdf,,https://github.com/JAYATEJAK/GVAlign,2311.01227,main,Poster,https://ieeexplore.ieee.org/document/10483949/,"['Representation learning', 'Computer vision', 'Power measurement', 'Codes', 'Prototypes', 'Tail', 'Data models']","['Robust Features', 'Incremental Learning', 'Global Alignment', 'Class-incremental Learning', 'Extensive Experiments', 'Need For Data', 'Global Variables', 'Robust Representation', 'Long-tailed Distribution', 'Two-stage Framework', 'Class Prototypes', 'Catastrophic Forgetting', 'CIFAR-100 Dataset', 'Robust Feature Representation', 'Semantic', 'Learning Rate', 'Convolutional Neural Network', 'Stage 2', 'Feature Space', 'Average Accuracy', 'Two-stage Approach', 'Conventional Settings', 'Distillation Loss', 'Classification Of Samples', 'End Of The Task', 'Cross-entropy Loss', 'Incremental Steps', 'Training Tasks']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"This paper introduces a two-stage framework designed to enhance long-tail class incremental learning, enabling the model to progressively learn new classes, while mitigating catastrophic forgetting in the context of long-tailed data distributions. Addressing the challenge posed by the under-representation of tail classes in long-tail class incremental learning, our approach achieves classifier alignment by leveraging global variance as an informative measure and class prototypes in the second stage. This process effectively captures class properties and eliminates the need for data balancing or additional layer tuning. Alongside traditional class incremental learning losses in the first stage, the proposed approach incorporates mixup classes to learn robust feature representations, ensuring smoother boundaries. The proposed framework can seamlessly integrate as a module with any class incremental learning method to effectively handle long-tail class incremental learning scenarios. Extensive experimentation on the CIFAR-100 and ImageNet-Subset datasets validates the approach’s efficacy, showcasing its superiority over state-of-the-art techniques across various long-tail CIL settings. Code is available at https://github.com/JAYATEJAK/GVAlign."
Robust Learning via Conditional Prevalence Adjustment,"Minh Nguyen, Alan Q. Wang, Heejong Kim, Mert R. Sabuncu","Cornell University; Department of Radiology, Weill Cornell Medicine",100.0,USA,0.0,,"Healthcare data often come from multiple sites in which the correlations between confounding variables can vary widely. If deep learning models exploit these unstable correlations, they might fail catastrophically in unseen sites. Although many methods have been proposed to tackle unstable correlations, each has its limitations. For example, adversarial training forces models to completely ignore unstable correlations, but doing so may lead to poor predictive performance. Other methods (e.g. Invariant Risk Minimization) try to learn domain-invariant representations that rely only on stable associations by assuming a causal data-generating process (input X causes class label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X), which are common in computer vision. We propose a method called CoPA (Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that (1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z generate X, and (2) the unstable conditional prevalence in each site E fully accounts for the unstable correlations between X and Y. Our crucial observation is that confounding variables are routinely recorded in healthcare settings and the prevalence can be readily estimated, for example, from a set of (Y, Z) samples (no need for corresponding samples of X). CoPA can work even if there is a single training site, a scenario which is often overlooked by existing methods. Our experiments on synthetic and real data show CoPA beating competitive baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_Robust_Learning_via_Conditional_Prevalence_Adjustment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_Robust_Learning_via_Conditional_Prevalence_Adjustment_WACV_2024_paper.pdf,,,2310.15766,main,Poster,https://ieeexplore.ieee.org/document/10484341/,"['Training', 'Deep learning', 'Computer vision', 'Correlation', 'Risk minimization', 'Medical services', 'Predictive models']","['Robust Learning', 'Computer Vision', 'Training Sites', 'Domain-invariant Representations', 'Causal Relationship', 'Convolutional Layers', 'Internal Validity', 'Chest X-ray', 'Joint Effect', 'Fully-connected Layer', 'Backbone Network', 'Unlabeled Data', 'Domain Adaptation', 'Linear Layer', 'Domain Generalization', 'Validation Error', 'Image X', 'Parental Functioning', 'Empirical Risk Minimization', 'Causal Graph']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Healthcare data often come from multiple sites in which the correlations between confounding variables can vary widely. If deep learning models exploit these unstable correlations, they might fail catastrophically in unseen sites. Although many methods have been proposed to tackle unstable correlations, each has its limitations. For example, adversarial training forces models to completely ignore unstable correlations, but doing so may lead to poor predictive performance. Other methods (e.g. Invariant Risk Minimization) try to learn domain-invariant representations that rely only on stable associations by assuming a causal data-generating process (input X causes class label Y ). Thus, they may be ineffective for anti-causal tasks (Y causes X), which are common in computer vision. We propose a method called CoPA (Conditional Prevalence-Adjustment) for anti-causal tasks. CoPA assumes that (1) generation mechanism is stable, i.e. label Y and confounding variable(s) Z generate X, and (2) the unstable conditional prevalence in each site E fully accounts for the unstable correlations between X and Y. Our crucial observation is that confounding variables are routinely recorded in healthcare settings and the prevalence can be readily estimated, for example, from a set of (Y,Z) samples (no need for corresponding samples of X). CoPA can work even if there is a single training site, a scenario which is often overlooked by existing methods. Our experiments on synthetic and real data show CoPA beating competitive baselines."
Robust Object Detection in Challenging Weather Conditions,"Himanshu Gupta, Oleksandr Kotlyar, Henrik Andreasson, Achim J. Lilienthal","Perception for Intelligent Systems, TUM, Germany; AASS, Orebro University, Sweden",100.0,"Germany, Sweden",0.0,,"Object detection is crucial in diverse autonomous systems like surveillance, autonomous driving, and driver assistance, ensuring safety by recognizing pedestrians, vehicles, traffic lights, and signs. However, adverse weather conditions such as snow, fog, and rain pose a challenge, affecting detection accuracy and risking accidents and damage. This clearly demonstrates the need for robust object detection solutions that work in all weather conditions. We employed three strategies to enhance deep learning-based object detection in adverse weather: training on real-world all-weather images, training on images with synthetic augmented weather noise, and integrating object detection with adverse weather image denoising. The synthetic weather noise is generated using analytical methods, GAN networks, and style-transfer networks. We compared the performance of these strategies by training object detection models using real-world all-weather images from the BDD100K dataset and for assessment employed unseen real-world adverse weather images. Adverse weather denoising methods were evaluated by denoising real-world adverse weather images and the results of object detection on denoised and original noisy images were compared. We found that the model trained using all-weather real-world images performed best, while the strategy of doing object detection on denoised images performed worst.",https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Robust_Object_Detection_in_Challenging_Weather_Conditions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gupta_Robust_Object_Detection_in_Challenging_Weather_Conditions_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483822/,"['Training', 'Computer vision', 'Surveillance', 'Noise reduction', 'Noise', 'Object detection', 'Traffic control']","['Object Detection', 'Robust Detection', 'Robust Object', 'Robust Object Detection', 'Challenging Weather Conditions', 'Analytical Methods', 'Denoising', 'Adverse Conditions', 'Extreme Weather', 'Pedestrian', 'Traffic Light', 'Noisy Images', 'Real-world Images', 'Adverse Weather Conditions', 'Object Detection Model', 'Driver Assistance', 'Object Detection Results', 'Synthetic Noise', 'Training Set', 'Analysis Approach', 'Clear Image', 'Image Augmentation', 'Snowflake', 'Performance Of Detection Models', 'Transmission Map', 'Bounding Box', 'Snow Layer', 'Style Image', 'Mean Average Precision', 'Clear Weather']","['Applications', 'Autonomous Driving', 'Algorithms', 'Image recognition and understanding']",7,"Object detection is crucial in diverse autonomous systems like surveillance, autonomous driving, and driver assistance, ensuring safety by recognizing pedestrians, vehicles, traffic lights, and signs. However, adverse weather conditions such as snow, fog, and rain pose a challenge, affecting detection accuracy and risking accidents and damage. This clearly demonstrates the need for robust object detection solutions that work in all weather conditions. We employed three strategies to enhance deep learning-based object detection in adverse weather: training on real-world all-weather images, training on images with synthetic augmented weather noise, and integrating object detection with adverse weather image denoising. The synthetic weather noise is generated using analytical methods, GAN networks, and style-transfer networks. We compared the performance of these strategies by training object detection models using real-world all-weather images from the BDD100K dataset and for assessment employed unseen real-world adverse weather images. Adverse weather denoising methods were evaluated by denoising real-world adverse weather images and the results of object detection on denoised and original noisy images were compared. We found that the model trained using all-weather real-world images performed best, while the strategy of doing object detection on denoised images performed worst."
Robust Source-Free Domain Adaptation for Fundus Image Segmentation,"Lingrui Li, Yanfeng Zhou, Ge Yang","Institute of Automation, Chinese Academy of Sciences; School of Artifical Intelligence, University of Chinese Academy of Sciences",100.0,China,0.0,,"Unsupervised Domain Adaptation (UDA) is a learning technique that transfers knowledge learned in the source domain from labelled training data to the target domain with only unlabelled data. It is of significant importance to medical image segmentation because of the usual lack of labelled training data. Although extensive efforts have been made to optimize UDA techniques to improve the accuracy of segmentation models in the target domain, few studies have addressed the robustness of these models under UDA. In this study, we propose a two-stage training strategy for robust domain adaptation. In the source training stage, we utilize adversarial sample augmentation to enhance the robustness and generalization capability of the source model. And in the target training stage, we propose a novel robust pseudo-label and pseudo-boundary (PLPB) method, which effectively utilizes unlabeled target data to generate pseudo labels and pseudo boundaries that enable model self-adaptation without requiring source data. Extensive experimental results on cross-domain fundus image segmentation confirm the effectiveness and versatility of our method. Source code of this study is openly accessible at https://github.com/LinGrayy/PLPB.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Robust_Source-Free_Domain_Adaptation_for_Fundus_Image_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Robust_Source-Free_Domain_Adaptation_for_Fundus_Image_Segmentation_WACV_2024_paper.pdf,,https://github.com/LinGrayy/PLPB,2310.16665,main,Poster,https://ieeexplore.ieee.org/document/10483926/,"['Training', 'Image segmentation', 'Adaptation models', 'Computer vision', 'Costs', 'Computational modeling', 'Source coding']","['Image Segmentation', 'Domain Adaptation', 'Fundus Images', 'Robust Adaptation', 'Fundus Image Segmentation', 'Source-free Domain Adaptation', 'Robust Domain Adaptation', 'Data Sources', 'Medical Imaging', 'Training Stage', 'Target Domain', 'Unlabeled Data', 'Target Data', 'Source Model', 'Source Domain', 'Medical Image Segmentation', 'Pseudo Labels', 'Unlabeled Target Data', 'Standard Model', 'Medical Field', 'Open Domain', 'Clean Samples', 'Unsupervised Domain Adaptation Methods', 'Unseen Domains', 'Robust Source', 'Adversarial Examples', 'Target Model', 'Adversarial Attacks', 'Object Boundaries', 'Generative Adversarial Networks']","['Applications', 'Biomedical / healthcare / medicine']",,"Unsupervised Domain Adaptation (UDA) is a learning technique that transfers knowledge learned in the source domain from labelled training data to the target domain with only unlabelled data. It is of significant importance to medical image segmentation because of the usual lack of labelled training data. Although extensive efforts have been made to optimize UDA techniques to improve the accuracy of segmentation models in the target domain, few studies have addressed the robustness of these models under UDA. In this study, we propose a two-stage training strategy for robust domain adaptation. In the source training stage, we utilize adversarial sample augmentation to enhance the robustness and generalization capability of the source model. And in the target training stage, we propose a novel robust pseudo-label and pseudo-boundary (PLPB) method, which effectively utilizes unlabeled target data to generate pseudo labels and pseudo boundaries that enable model self-adaptation without requiring source data. Extensive experimental results on cross-domain fundus image segmentation confirm the effectiveness and versatility of our method. Source code of this study is openly accessible at https://github.com/LinGrayy/PLPB."
Robust TRISO-Fueled Pebble Identification by Digit Recognition,"Roshan Kenia, Jihane Mendil, Ahmed Jasim, Muthanna Al-Dahhan, Zhaozheng Yin","Department of Chemical & Biochemical Engineering, Missouri University of Science and Technology; Department of Computer Science, Stony Brook University",100.0,USA,0.0,,"Nuclear power plays a vital role in providing reliable and clean energy to fulfill increasing demands in electricity worldwide. It continues to be an essential source of national power supply as growing concerns about fossil fuel depletion, global warming, and emissions require utilizing sustainable energy sources. One area contributing to the growth of nuclear power is the development of reactors that have enhanced protection and security, thermal efficiency, and design. Reactor efficiency can be studied by the burnup that occurs when a TRISO-fueled pebble is inserted into the nuclear core and subsequently removed. The levels of burnup are measured based on the length of time the pebble spends within the core. In our design, each pebble is numbered by multiple digits printed in six locations using Ultra-High Temperature Ceramic paint. Naturally, computer vision techniques can be used to identify and time each pebble based on its digits as it enters and exits the core. We present a deep learning approach that successfully tags each pebble by identifying its digits from a video stream of the entrance and exit of the core. In a multi-step method, we extract only the clearest and most useful views of the pebble's digits to classify as it rolls by. This algorithm is robust against issues that occur for objects in movement such as motion blur, rotations, and glare. We outperform other state-of-the-art optical character recognition (OCR) models that fail to identify digits that are in motion. Our approach creates a safer and more efficient way to measure burnup within a core while contributing to the improvement of nuclear power produced by reactors.",https://openaccess.thecvf.com/content/WACV2024/html/Kenia_Robust_TRISO-Fueled_Pebble_Identification_by_Digit_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kenia_Robust_TRISO-Fueled_Pebble_Identification_by_Digit_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484176/,"['Real-time systems', 'Climate change', 'Visualization', 'Nuclear power generation', 'Inductors', 'Optical character recognition', 'Streaming media', 'Power supplies', 'Power measurement', 'Nuclear measurements', 'Security', 'Green energy', 'Temperature measurement']","['Optical Character Recognition', 'Nuclear Power', 'Recognition Model', 'Thermal Efficiency', 'Motion Blur', 'Multi-step Method', 'Reactor Core', 'Object Detection', 'Individual Models', 'Intersection Over Union', 'Precision And Recall', 'Detection Model', 'Bounding Box', 'Nuclear Reactors', 'Video Frames', 'Affine Transformation', 'Object Motion', 'Electoral System', 'Training Videos', 'Mask R-CNN', 'Spherical Objects', 'Test Videos', 'Reactor Outlet', 'Digital Dataset', 'Ultra-high-definition', 'Respective Datasets', 'Random Rotation', 'Random Sampling']","['Applications', 'Embedded sensing / real-time techniques', 'Applications', 'Visualization']",,"Nuclear power plays a vital role in providing reliable and clean energy to fulfill increasing demands in electricity worldwide. It continues to be an essential source of national power supply as growing concerns about fossil fuel depletion, global warming, and emissions require utilizing sustainable energy sources. One area contributing to the growth of nuclear power is the development of reactors that have enhanced protection and security, thermal efficiency, and design. Reactor efficiency can be studied by the burnup that occurs when a TRISO-fueled pebble is inserted into the nuclear core and subsequently removed. The levels of burnup are measured based on the length of time the pebble spends within the core. In our design, each pebble is numbered by multiple digits printed in six locations using Ultra-High Temperature Ceramic paint. Naturally, computer vision techniques can be used to identify and time each pebble based on its digits as it enters and exits the core. We present a deep learning approach that successfully tags each pebble by identifying its digits from a video stream of the entrance and exit of the core. In a multi-step method, we extract only the clearest and most useful views of the pebble’s digits to classify as it rolls by. This algorithm is robust against issues that occur for objects in movement such as motion blur, rotations, and glare. We outperform other state-of-the-art optical character recognition (OCR) models that fail to identify digits that are in motion. Our approach creates a safer and more efficient way to measure burnup within a core while contributing to the improvement of nuclear power produced by reactors.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Robust Unsupervised Domain Adaptation Through Negative-View Regularization,"Joonhyeok Jang, Sunhyeok Lee, Seonghak Kim, Jung-un Kim, Seonghyun Kim, Daeshik Kim",SHINSEGAE I&C; Korea Advanced Institute of Science and Technology (KAIST),100.0,South Korea,0.0,,"In the realm of Unsupervised Domain Adaptation (UDA), Vision Transformers (ViTs) have recently demonstrated remarkable adaptability surpassing that of traditional Convolutional Neural Networks (CNNs). Nevertheless, the patch-based structure of ViTs heavily relies on local features within image patches, potentially leading to reduced robustness when confronted with out-of-distribution (OOD) samples. To address this concern, we introduce a novel regularizer tailored specifically for UDA. By leveraging negative views, i.e. target-domain samples applied by negative augmentations, we make the learning process more intricate, thereby preventing models from taking shortcuts in spatial context recognition. We present a novel loss function, rooted in contrastive principles, to effectively distinguish between the negative views and original target samples. By integrating this novel regularizer with existing UDA methodologies, we guide ViTs to prioritize context relationships among local patches, thereby enhancing the robustness of ViTs. Our proposed Negative View-based Contrastive (NVC) regularizer substantially boosts the performance of baseline UDA methods across diverse benchmark datasets. Furthermore, we release new dataset, Retail-71, comprising 71 classes of images commonly encountered in retail stores. Through comprehensive experimentation, we showcase the effectiveness of our approach on traditional benchmarks as well as the novel retail domain. These results substantiate the robust adaptation capabilities of our proposed method. Our method is implemented at our repository.",https://openaccess.thecvf.com/content/WACV2024/html/Jang_Robust_Unsupervised_Domain_Adaptation_Through_Negative-View_Regularization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jang_Robust_Unsupervised_Domain_Adaptation_Through_Negative-View_Regularization_WACV_2024_paper.pdf,Not provided,Not provided,,main,Poster,https://ieeexplore.ieee.org/document/10483946/,"['Computer vision', 'Adaptation models', 'Target recognition', 'Benchmark testing', 'Transformers', 'Robustness', 'Convolutional neural networks']","['Domain Adaptation', 'Neural Network', 'Convolutional Neural Network', 'Local Features', 'Image Classification', 'Target Sample', 'Negative Views', 'Image Patches', 'Retail Stores', 'Vision Transformer', 'Unsupervised Domain Adaptation Methods', 'Target Domain Samples', 'Semantic', 'Positive Samples', 'Negative Samples', 'Global Context', 'Generative Adversarial Networks', 'Source Images', 'Target Domain', 'Source Domain', 'Self-supervised Learning', 'Regularization Loss', 'Motion Blur', 'Contrastive Loss', 'Unlabeled Target Data', 'Target Domain Images', 'Intermediate Domain', 'Source Dataset', 'Wasserstein Distance', 'Target Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"In the realm of Unsupervised Domain Adaptation (UDA), Vision Transformers (ViTs) have recently demonstrated remarkable adaptability surpassing that of traditional Convolutional Neural Networks (CNNs). Nevertheless, the patch-based structure of ViTs heavily relies on local features within image patches, potentially leading to reduced robustness when confronted with out-of-distribution (OOD) samples. To address this concern, we introduce a novel regularizer tailored specifically for UDA. By leveraging negative views, i.e. target-domain samples applied by negative augmentations, we make the learning process more intricate, thereby preventing models from taking shortcuts in spatial context recognition. We present a novel loss function, rooted in contrastive principles, to effectively distinguish between the negative views and original target samples. By integrating this novel regularizer with existing UDA methodologies, we guide ViTs to prioritize context relationships among local patches, thereby enhancing the robustness of ViTs. Our proposed Negative View-based Contrastive (NVC) regularizer substantially boosts the performance of baseline UDA methods across diverse benchmark datasets. Furthermore, we release new dataset, Retail-71, comprising 71 classes of images commonly encountered in retail stores. Through comprehensive experimentation, we showcase the effectiveness of our approach on traditional benchmarks as well as the novel retail domain. These results substantiate the robust adaptation capabilities of our proposed method. Our method is implemented at our repository."
RobustCLEVR: A Benchmark and Framework for Evaluating Robustness in Object-Centric Learning,"Nathan Drenkow, Mathias Unberath","1The Johns Hopkins University, 2The Johns Hopkins University Applied Physics Laboratory; 1The Johns Hopkins University",100.0,USA,0.0,,"Object-centric representation learning offers the potential to overcome limitations of image-level representations by explicitly parsing image scenes into their constituent components. While image-level representations typically lack robustness to natural image corruptions, the robustness of object-centric methods remains largely untested. To address this gap, we present the RobustCLEVR benchmark dataset and evaluation framework. Our framework takes a novel approach to evaluating robustness by enabling the specification of causal dependencies in the image generation process grounded in expert knowledge and capable of producing a wide range of image corruptions unattainable in existing robustness evaluations. Using our framework, we define several causal models of the image corruption process which explicitly encode assumptions about the causal relationships and distributions of each corruption type. We generate dataset variants for each causal model on which we evaluate state-of-the-art object-centric methods. Overall, we find that object-centric methods are not inherently robust to image corruptions. Our causal evaluation approach exposes model sensitivities not observed using conventional evaluation processes, yielding greater insight into robustness differences across algorithms. Lastly, while conventional robustness evaluations view corruptions as out-of-distribution, we use our causal framework to show that even training on in-distribution image corruptions does not guarantee increased model robustness. This work provides a step towards more concrete and substantiated understanding of model performance and deterioration under complex corruption processes of the real-world.",https://openaccess.thecvf.com/content/WACV2024/html/Drenkow_RobustCLEVR_A_Benchmark_and_Framework_for_Evaluating_Robustness_in_Object-Centric_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Drenkow_RobustCLEVR_A_Benchmark_and_Framework_for_Evaluating_Robustness_in_Object-Centric_WACV_2024_paper.pdf,,,2308.14899,main,Poster,https://ieeexplore.ieee.org/document/10484494/,"['Training', 'Representation learning', 'Computer vision', 'Sensitivity', 'Image synthesis', 'Computational modeling', 'Benchmark testing']","['Robust Method', 'Generation Process', 'Representation Learning', 'Causal Model', 'Causal Framework', 'Types Of Corruption', 'Differences In Performance', 'Deep Neural Network', 'Causal Inference', 'Real-world Data', 'Clean Data', 'Causal Mechanisms', 'Assumption Of Distribution', 'Domain Shift', 'Joint Distribution', 'Non-uniform Distribution', 'Clear Image', 'Imaging Conditions', 'Image Distortion', 'Appendix For Details', 'Directed Acyclic Graph', 'Causal Graph', 'Nuisance Factors', 'Half-normal', 'Scene Geometry', 'Real-world Conditions', 'Computer Vision', 'Training Dataset', 'Image Reconstruction', 'Uniform Distribution']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",,"Object-centric representation learning offers the potential to overcome limitations of image-level representations by explicitly parsing image scenes into their constituent components. While image-level representations typically lack robustness to natural image corruptions, the robustness of object-centric methods remains largely untested. To address this gap, we present the RobustCLEVR benchmark dataset and evaluation framework. Our framework takes a novel approach to evaluating robustness by enabling the specification of causal dependencies in the image generation process grounded in expert knowledge and capable of producing a wide range of image corruptions unattainable in existing robustness evaluations. Using our framework, we define several causal models of the image corruption process which explicitly encode assumptions about the causal relationships and distributions of each corruption type. We generate dataset variants for each causal model on which we evaluate state-of-the-art object-centric methods. Overall, we find that object-centric methods are not inherently robust to image corruptions. Our causal evaluation approach exposes model sensitivities not observed using conventional evaluation processes, yielding greater insight into robustness differences across algorithms. Lastly, while conventional robustness evaluations view corruptions as out-of-distribution, we use our causal framework to show that even training on in-distribution image corruptions does not guarantee increased model robustness. This work provides a step towards more concrete and substantiated understanding of model performance and deterioration under complex corruption processes of the real-world.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-Based Gaze Estimation,"Yoichiro Hisadome, Tianyi Wu, Jiawei Qin, Yusuke Sugano","Institute of Industrial Science, The University of Tokyo",100.0,Japan,0.0,,"Appearance-based gaze estimation has been actively studied in recent years. However, its generalization performance for unseen head poses is still a significant limitation for existing methods. This work proposes a generalizable multi-view gaze estimation task and a cross-view feature fusion method to address this issue. In addition to paired images, our method takes the relative rotation matrix between two cameras as additional input. The proposed network learns to extract rotatable feature representation by using relative rotation as a constraint and adaptively fuses the rotatable features via stacked fusion modules. This simple yet efficient approach significantly improves generalization performance under unseen head poses without significantly increasing computational cost. The model can be trained with random combinations of cameras without fixing the positioning and can generalize to unseen camera pairs during inference. Through experiments using multiple datasets, we demonstrate the advantage of the proposed method over baseline methods, including state-of-the-art domain generalization approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Hisadome_Rotation-Constrained_Cross-View_Feature_Fusion_for_Multi-View_Appearance-Based_Gaze_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hisadome_Rotation-Constrained_Cross-View_Feature_Fusion_for_Multi-View_Appearance-Based_Gaze_Estimation_WACV_2024_paper.pdf,,https://github.com/ut-vision/Rot-MVGaze,2305.12704,main,Poster,https://ieeexplore.ieee.org/document/10484134/,"['Computer vision', 'Head', 'Fuses', 'Computational modeling', 'Estimation', 'Cameras', 'Feature extraction']","['Feature Fusion', 'Gaze Estimation', 'Appearance-based Gaze Estimation', 'Feature Representation', 'Generalization Performance', 'Relationship Matrix', 'Baseline Methods', 'Domain Generalization', 'Head Pose', 'Pair Of Cameras', 'Improve Generalization Performance', 'Coordinate System', 'Input Image', 'Target Image', 'Reference Image', 'Face Images', 'Monocular', 'Target Domain', 'Inference Time', 'Domain Adaptation', 'Rotation Characteristics', 'Gaze Direction', 'Fusion Block', 'Backbone Feature', '3D Vector', 'Camera Calibration', 'Subject Of Active Research', 'Camera Position', 'Single Baseline', 'Feature Concatenation']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Image recognition and understanding']",1,"Appearance-based gaze estimation has been actively studied in recent years. However, its generalization performance for unseen head poses is still a significant limitation for existing methods. This work proposes a generalizable multi-view gaze estimation task and a cross-view feature fusion method to address this issue. In addition to paired images, our method takes the relative rotation matrix between two cameras as additional input. The proposed network learns to extract rotatable feature representation by using relative rotation as a constraint and adaptively fuses the rotatable features via stacked fusion modules. This simple yet efficient approach significantly improves generalization performance under unseen head poses without significantly increasing computational cost. The model can be trained with random combinations of cameras without fixing the positioning and can generalize to unseen camera pairs during inference. Through experiments using multiple datasets, we demonstrate the advantage of the proposed method over baseline methods, including state-of-the-art domain generalization approaches. The code will be available at https://github.com/ut-vision/Rot-MVGaze."
S3AD: Semi-Supervised Small Apple Detection in Orchard Environments,"Robert Johanson, Christian Wilms, Ole Johannsen, Simone Frintrop","Computer Vision Group, University of Hamburg, Germany",100.0,Germany,0.0,,"Crop detection is integral for precision agriculture applications such as automated yield estimation or fruit picking. However, crop detection, e.g., apple detection in orchard environments remains challenging due to a lack of large-scale datasets and the small relative size of the crops in the image. In this work, we address these challenges by reformulating the apple detection task in a semi-supervised manner. To this end, we provide the large, high-resolution dataset MAD comprising 105 labeled images with 14,667 annotated apple instances and 4,440 unlabeled images. Utilizing this dataset, we also propose a novel Semi-Supervised Small Apple Detection system S3AD based on contextual attention and selective tiling to improve the challenging detection of small apples, while limiting the computational overhead. We conduct an extensive evaluation on MAD and the MSU dataset, showing that S3AD substantially outperforms strong fully-supervised baselines, including several small object detection systems, by up to 14.9%. Additionally, we exploit the detailed annotations of our dataset w.r.t. apple properties to analyze the influence of relative size or level of occlusion on the results of various systems, quantifying current challenges.",https://openaccess.thecvf.com/content/WACV2024/html/Johanson_S3AD_Semi-Supervised_Small_Apple_Detection_in_Orchard_Environments_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Johanson_S3AD_Semi-Supervised_Small_Apple_Detection_in_Orchard_Environments_WACV_2024_paper.pdf,www.inf.uni-hamburg.de/mad,,,main,Poster,,,,,,
SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images,"Weiyi Xie, Nathalie Willems, Shubham Patil, Yang Li, Mayank Kumar",Stryker AI Research,0.0,,100.0,USA,"We propose a straightforward yet highly effective few-shot fine-tuning strategy for adapting the Segment Anything (SAM) to anatomical segmentation tasks in medical images. Our novel approach revolves around reformulating the mask decoder within SAM, leveraging few-shot embeddings derived from a limited set of labeled images (few-shot collection) as prompts for querying anatomical objects captured in image embeddings. This innovative reformulation greatly reduces the need for time-consuming online user interactions for labeling volumetric images, such as exhaustively marking points and bounding boxes to provide prompts slice by slice. With our method, users can manually segment a few 2D slices offline, and the embeddings of these annotated image regions serve as effective prompts for online segmentation tasks. Our method prioritizes the efficiency of the fine-tuning process by exclusively training the mask decoder through caching mechanisms while keeping the image encoder frozen. Importantly, this approach is not limited to volumetric medical images, but can generically be applied to any 2D/3D segmentation task. To thoroughly evaluate our method, we conducted extensive validation on four datasets, covering six anatomical segmentation tasks across two modalities. Furthermore, we conducted a comparative analysis of different prompting options within SAM and the fully-supervised nnU-Net. The results demonstrate the superior performance of our method compared to SAM employing only point prompts (50% improvement in IoU) and performs on-par with fully supervised methods whilst reducing the requirement of labeled data by at least an order of magnitude.",https://openaccess.thecvf.com/content/WACV2024/html/Xie_SAM_Fewshot_Finetuning_for_Anatomical_Segmentation_in_Medical_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xie_SAM_Fewshot_Finetuning_for_Anatomical_Segmentation_in_Medical_Images_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484400/,"['Training', 'Image segmentation', 'Computer vision', 'Decoding', 'Labeling', 'Task analysis', 'Biomedical imaging']","['Medical Imaging', 'Medical Image Segmentation', 'Anatomical Segmentation', 'Few-shot Fine-tuning', 'Intersection Over Union', 'Bounding Box', 'Segmentation Task', 'Volumetric Imaging', 'Fine-tuning Process', 'Image Encoder', 'Image Embedding', 'Vertebrate', 'Computed Tomography', 'Validation Set', '2D Images', 'Latent Space', 'MRI Images', 'Segmentation Accuracy', 'Left Atrium', 'Intersection Over Union Score', 'Ground Truth Segmentation', 'Segmentation Prediction', 'Positional Encoding', 'Segmentation Performance', 'Dice Score', 'Stability Score', 'Foundation Model', 'Automatic Mode']","['Algorithms', '3D computer vision', 'Applications', 'Biomedical / healthcare / medicine']",4,"We propose a straightforward yet highly effective few-shot fine-tuning strategy for adapting the Segment Anything (SAM) to anatomical segmentation tasks in medical images. Our novel approach revolves around reformulating the mask decoder within SAM, leveraging few-shot embeddings derived from a limited set of labeled images (few-shot collection) as prompts for querying anatomical objects captured in image embeddings. This innovative reformulation greatly reduces the need for time-consuming online user interactions for labeling volumetric images, such as exhaustively marking points and bounding boxes to provide prompts slice by slice. With our method, users can manually segment a few 2D slices offline, and the embeddings of these annotated image regions serve as effective prompts for online segmentation tasks. Our method prioritizes the efficiency of the fine-tuning process by exclusively training the mask decoder through caching mechanisms while keeping the image encoder frozen. Importantly, this approach is not limited to volumetric medical images, but can generically be applied to any 2D/3D segmentation task.To thoroughly evaluate our method, we conducted extensive validation on four datasets, covering six anatomical segmentation tasks across two modalities. Furthermore, we conducted a comparative analysis of different prompting options within SAM and the fully-supervised nnU-Net. The results demonstrate the superior performance of our method compared to SAM employing only point prompts (∼50% improvement in IoU) and performs on-par with fully supervised methods whilst reducing the requirement of labeled data by at least an order of magnitude."
SBCFormer: Lightweight Network Capable of Full-Size ImageNet Classification at 1 FPS on Single Board Computers,"Xiangyong Lu, Masanori Suganuma, Takayuki Okatani","Graduate School of Information Sciences, Tohoku University; Graduate School of Information Sciences, Tohoku University; RIKEN Center for AIP",100.0,Japan,0.0,,"Computer vision has become increasingly prevalent in solving real-world problems across diverse domains, including smart agriculture, fishery, and livestock management. These applications may not require processing many image frames per second, leading practitioners to use single board computers (SBCs). Although many lightweight networks have been developed for ""mobile/edge"" devices, they primarily target smartphones with more powerful processors and not SBCs with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called SBCFormer, which achieves high accuracy and fast computation on such low-end CPUs. The hardware constraints of these CPUs make the Transformer's attention mechanism preferable to convolution. However, using attention on low-end CPUs presents a challenge: high-resolution internal feature maps demand excessive computational resources, but reducing their resolution results in the loss of local image details. SBCFormer introduces an architectural design to address this issue. As a result, SBCFormer achieves the highest trade-off between accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a speed of 1.0 frame/sec on the SBC. Code is available at https://github.com/xyongLu/SBCFormer.",https://openaccess.thecvf.com/content/WACV2024/html/Lu_SBCFormer_Lightweight_Network_Capable_of_Full-Size_ImageNet_Classification_at_1_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lu_SBCFormer_Lightweight_Network_Capable_of_Full-Size_ImageNet_Classification_at_1_WACV_2024_paper.pdf,,https://github.com/xyongLu/SBCFormer,2311.03747,main,Poster,https://ieeexplore.ieee.org/document/10484311/,"['Smart agriculture', 'Computer vision', 'Visualization', 'Codes', 'Computational modeling', 'Streaming media', 'Transformers']","['Lightweight Network', 'Single-board Computer', 'Feature Maps', 'Computational Resources', 'Attention Mechanism', 'Local Details', 'Raspberry Pi', 'Livestock Management', 'Convolutional Neural Network', 'Mobile Devices', 'Deep Neural Network', 'Local Information', 'Convolutional Layers', 'Input Image', 'Image Classification', 'Object Detection', 'Global Features', 'Inference Time', 'Inference Accuracy', 'Vision Transformer', 'Local Stream', 'Lightweight Convolutional Neural Network', 'Recent Hybridization', 'Pointwise Convolution', 'Edge Devices', 'Depthwise Convolution', 'Limited Computational Resources', 'CPU Intel', 'Batch Normalization']","['Algorithms', 'Image recognition and understanding']",1,"Computer vision has become increasingly prevalent in solving real-world problems across diverse domains, including smart agriculture, fishery, and livestock management. These applications may not require processing many image frames per second, leading practitioners to use single board computers (SBCs). Although many lightweight networks have been developed for ""mobile/edge"" devices, they primarily target smartphones with more powerful processors and not SBCs with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called SBCFormer, which achieves high accuracy and fast computation on such low-end CPUs. The hardware constraints of these CPUs make the Transformer’s attention mechanism preferable to convolution. However, using attention on low-end CPUs presents a challenge: high-resolution internal feature maps demand excessive computational resources, but reducing their resolution results in the loss of local image details. SBCFormer introduces an architectural design to address this issue. As a result, SBCFormer achieves the highest trade-off between accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a speed of 1.0 frame/sec on the SBC. Code is available at https://github.com/xyongLu/SBCFormer."
SC-MIL: Supervised Contrastive Multiple Instance Learning for Imbalanced Classification in Pathology,"Dinkar Juyal, Siddhant Shingi, Syed Ashar Javed, Harshith Padigela, Chintan Shah, Anand Sampat, Archit Khosla, John Abel, Amaro Taylor-Weiner","PathAI Inc, Boston, USA; University of Massachusetts, Amherst, USA; PathAI Inc",33.33333333333333,USA,66.66666666666667,USA,"Multiple Instance learning (MIL) models have been extensively used in pathology to predict biomarkers and risk-stratify patients from gigapixel-sized images. Machine learning problems in medical imaging often deal with rare diseases, making it important for these models to work in a label-imbalanced setting. In pathology images, there is another level of imbalance, where given a positively labeled Whole Slide Image (WSI), only a fraction of pixels within it contribute to the positive label. This compounds the severity of imbalance and makes imbalanced classification in pathology challenging. Furthermore, these imbalances can occur in out-of-distribution (OOD) datasets when the models are deployed in the real-world. We leverage the idea that decoupling feature and classifier learning can lead to improved decision boundaries for label imbalanced datasets. To this end, we investigate the integration of supervised contrastive learning with multiple instance learning (SC-MIL). Specifically, we propose a joint-training MIL framework in the presence of label imbalance that progressively transitions from learning bag-level representations to optimal classifier learning. We perform experiments with different imbalance settings for two well-studied problems in cancer pathology: subtyping of non-small cell lung cancer and subtyping of renal cell carcinoma. SC-MIL provides large and consistent improvements over other techniques on both in-distribution (ID) and OOD held-out sets across multiple imbalanced settings.",https://openaccess.thecvf.com/content/WACV2024/html/Juyal_SC-MIL_Supervised_Contrastive_Multiple_Instance_Learning_for_Imbalanced_Classification_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Juyal_SC-MIL_Supervised_Contrastive_Multiple_Instance_Learning_for_Imbalanced_Classification_in_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483756/,"['Pathology', 'Computer vision', 'Biological system modeling', 'Lung cancer', 'Self-supervised learning', 'Machine learning', 'Predictive models']","['Learning Classifiers', 'Class Imbalance', 'Self-supervised Learning', 'Multiple Instance Learning', 'Non-small Cell Lung Cancer', 'Feature Learning', 'Imbalanced Datasets', 'Slide Images', 'Digital Pathology', 'Cancer Pathology', 'Present Framework', 'Imbalance In Levels', 'Fraction Of Pixels', 'Clinical Characteristics', 'Random Sampling', 'Squamous Cell Carcinoma', 'Feature Space', 'Cross-entropy', 'The Cancer Genome Atlas', 'Contrastive Loss', 'Degree Of Imbalance', 'Imbalance Ratio', 'Cross-entropy Loss', 'Two-stage Training', 'Patch Level', 'Minority Class', 'Stage Of Loss', 'Positive Instances', 'Individual Instances']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Multiple Instance learning (MIL) models have been extensively used in pathology to predict biomarkers and risk-stratify patients from gigapixel-sized images. Machine learning problems in medical imaging often deal with rare diseases, making it important for these models to work in a label-imbalanced setting. In pathology images, there is another level of imbalance, where given a positively labeled Whole Slide Image (WSI), only a fraction of pixels within it contribute to the positive label. This compounds the severity of imbalance and makes imbalanced classification in pathology challenging. Furthermore, these imbalances can occur in out-of-distribution (OOD) datasets when the models are deployed in the real-world. We leverage the idea that decoupling feature and classifier learning can lead to improved decision boundaries for label imbalanced datasets. To this end, we investigate the integration of supervised contrastive learning with multiple instance learning (SC-MIL). Specifically, we propose a joint-training MIL framework in the presence of label imbalance that progressively transitions from learning bag-level representations to optimal classifier learning. We perform experiments with different imbalance settings for two well-studied problems in cancer pathology: subtyping of non-small cell lung cancer and subtyping of renal cell carcinoma. SC-MIL provides large and consistent improvements over other techniques on both in-distribution (ID) and OOD held-out sets across multiple imbalanced settings."
SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture With Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation,"Yifei Chen, Binfeng Zou, Zhaoxin Guo, Yiyu Huang, Yifan Huang, Feiwei Qin, Qinhai Li, Changmiao Wang",Hangzhou Dianzi University; Second Affiliated Hospital of the Chinese University of Hong Kong; Shenzhen Research Institute of Big Data,100.0,"China, Hong Kong",0.0,,"Pulmonary embolism (PE) is a prevalent lung disease that can lead to right ventricular hypertrophy and failure in severe cases, ranking second in severity only to myocardial infarction and sudden death. Pulmonary artery CT angiography (CTPA) is a widely used diagnostic method for PE. However, PE detection presents challenges in clinical practice due to limitations in imaging technology. CTPA can produce noises similar to PE, making confirmation of its presence time-consuming and prone to overdiagnosis. Nevertheless, the traditional segmentation method of PE can not fully consider the hierarchical structure of features, local and global spatial features of PE CT images. In this paper, we propose an automatic PE segmentation method called SCUNet++ (Swin Conv UNet++). This method incorporates multiple fusion dense skip connections between the encoder and decoder, utilizing the Swin Transformer as the encoder. And fuses features of different scales in the decoder subnetwork to compensate for spatial information loss caused by the inevitable downsampling in Swin-UNet or other state-of-the-art methods, effectively solving the above problem. We provide a theoretical analysis of this method in detail and validate it on publicly available PE CT image datasets FUMPE and CAD-PE. The experimental results indicate that our proposed method achieved a Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our method exhibits strong performance in PE segmentation tasks, potentially enhancing the accuracy of automatic segmentation of PE and providing a powerful diagnostic tool for clinical physicians. Our source code and new FUMPE dataset are available at https://github.com/JustlfC03/SCUNet-plusplus.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_SCUNet_Swin-UNet_and_CNN_Bottleneck_Hybrid_Architecture_With_Multi-Fusion_Dense_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_SCUNet_Swin-UNet_and_CNN_Bottleneck_Hybrid_Architecture_With_Multi-Fusion_Dense_WACV_2024_paper.pdf,,https://github.com/JustlfC03/SCUNet-plusplus,,main,Poster,,,,,,
SCoRD: Subject-Conditional Relation Detection With Text-Augmented Data,"Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez",Adobe Research; Rice University,50.0,USA,50.0,USA,"We propose Subject-Conditional Relation Detection SCoRD, where conditioned on an input subject, the goal is to predict all its relations to other objects in a scene along with their locations. Based on the Open Images dataset, we propose a challenging OIv6-SCoRD benchmark such that the training and testing splits have a distribution shift in terms of the occurrence statistics of <subject, relation, object> triplets. To solve this problem, we propose an auto-regressive model that given a subject, it predicts its relations, objects, and object locations by casting this output as a sequence of tokens. First, we show that previous scene-graph prediction methods fail to produce as exhaustive an enumeration of relation-object pairs when conditioned on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for our relation-object predictions compared to the 49.75% obtained by a recent scene graph detector. Then, we show improved generalization on both relation-object and object-box predictions by leveraging during training relation-object pairs obtained automatically from textual captions and for which no object-box annotations are available. Particularly, for <subject, relation, object> triplets for which no object locations are available during training, we are able to obtain a recall@3 of 33.80% for relation-object pairs and 26.75% for their box locations.",https://openaccess.thecvf.com/content/WACV2024/html/Yang_SCoRD_Subject-Conditional_Relation_Detection_With_Text-Augmented_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_SCoRD_Subject-Conditional_Relation_Detection_With_Text-Augmented_Data_WACV_2024_paper.pdf,,,2308.12910,main,Poster,https://ieeexplore.ieee.org/document/10483698/,"['Training', 'Computer vision', 'Casting', 'Annotations', 'Detectors', 'Benchmark testing', 'Predictive models']","['Object Location', 'Objects In The Scene', 'Test Split', 'Caption Text', 'Open Image', 'Sequence Of Tokens', 'Box Location', 'Challenging Benchmark', 'Scene Graph', 'Sample Types', 'Image Features', 'Input Image', 'Bounding Box', 'Input Text', 'Ground Data', 'Image Captioning', 'Transformer Encoder', 'Object Boxes', 'Context Vector', 'Box Coordinates', 'Beam Search', 'Box Annotations', 'Transformer Decoder', 'Prediction Box', 'Text Encoder', 'Input Tokens', 'Image Encoder', 'Text Annotation', 'Unique Pairs']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding']",,"We propose Subject-Conditional Relation Detection (SCoRD), where conditioned on an input subject, the goal is to predict all its relations to other objects in a scene along with their locations. Based on the Open Images dataset, we propose a challenging OIv6-SCoRD benchmark such that the training and testing splits have a distribution shift in terms of the occurrence statistics of subject, relation, object triplets. To solve this problem, we propose an auto-regressive model that given a subject, it predicts its relations, objects, and object locations by casting this output as a sequence of tokens. First, we show that previous scene-graph prediction methods fail to produce as exhaustive an enumeration of relation-object pairs when conditioned on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for our relation-object predictions compared to the 49.75% obtained by a recent scene graph detector. Then, we show improved generalization on both relation-object and object-box predictions by leveraging during training relation-object pairs obtained automatically from textual captions and for which no object-box annotations are available. Particularly, for subject, relation, object triplets for which no object locations are available during training, we are able to obtain a recall@3 of 33.80% for relation-object pairs and 26.75% for their box locations."
SDNet: An Extremely Efficient Portrait Matting Model via Self-Distillation,"Ziwen Li, Bo Xu, Jiake Xie, Yong Tang, Cheng Lu",OPPO; Picup.AI; Xpeng,0.0,,100.0,China,"Most existing portrait matting models either require expensive auxiliary information or try to decompose the task into sub-tasks that are usually resource-hungry. These challenges limit its application on low-power computing devices. In addition, mobile networks tend to be less powerful than those cumbersome ones in feature representation mining. In this paper, we propose an extremely efficient portrait matting model via self-distillation (SDNet), that aims to provide a solution to performing accurate and effective portrait matting with limited computing resources. Our SDNet contains only 2M parameters, 2.2% of the parameters of MGM, and 1.5% of that of Matteformer. We introduce the training pipeline of self-distillation that can improve our lightweight baseline model without any parameter addition, network modification, or over-parameterized teacher models which need well-pretraining. Extensive experiments demonstrate the effectiveness of our self-distillation method and the lightweight SDNet network. Our SDNet outperforms the state-of-the-art (SOTA) lightweight approaches on both synthetic and real-world images.",https://openaccess.thecvf.com/content/WACV2024/html/Li_SDNet_An_Extremely_Efficient_Portrait_Matting_Model_via_Self-Distillation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_SDNet_An_Extremely_Efficient_Portrait_Matting_Model_via_Self-Distillation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484425/,"['Training', 'Performance evaluation', 'Adaptation models', 'Computer vision', 'Fuses', 'Computational modeling', 'Pipelines']","['Portrait Matting', 'Feature Representation', 'Teacher Model', 'Training Pipeline', 'Semantic', 'Mean Square Error', 'Deep Learning', 'Mobile Devices', 'Deep Neural Network', 'Convolutional Layers', 'Multiple Scales', 'Object Detection', 'Receptive Field', 'Generative Adversarial Networks', 'Multi-scale Features', 'Feature Aggregation', 'RGB Color', 'Student Model', 'Distillation Method', 'Lightweight Network', 'Alpha Matte', 'Salient Object Detection', 'Aggregation Module', 'Privileged Information', 'Neural Network', 'Convolution Operation', 'Soft Labels', 'Salient Object']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding']",,"Most existing portrait matting models either require expensive auxiliary information or try to decompose the task into sub-tasks that are usually resource-hungry. These challenges limit its application on low-power computing devices. In addition, mobile networks tend to be less powerful than those cumbersome ones in feature representation mining. In this paper, we propose an extremely efficient portrait matting model via self-distillation (SDNet), that aims to provide a solution to performing accurate and effective portrait matting with limited computing resources. Our SD-Net contains only 2M parameters, 2.2% of parameters of MGM, and 1.5% of that of Matteformer. We introduce the training pipeline of self-distillation that can improve our lightweight baseline model without any parameter addition, network modification, or over-parameterized teacher models which need well-pretraining. Extensive experiments demonstrate the effectiveness of our self-distillation method and the lightweight SDNet network. Our SDNet outperforms the state-of-the-art (SOTA) lightweight approaches on both synthetic and real-world images."
SEMA: Semantic Attention for Capturing Long-Range Dependencies in Egocentric Lifelogs,"Pravin Nagar, K.N. Ajay Shastry, Jayesh Chaudhari, Chetan Arora","University of Maryland, College Park; Indian Institute of Technology, Delhi",100.0,"India, USA",0.0,,"Transformer architecture is a de-facto standard for modeling global dependency in long sequences. However, quadratic space and time complexity for self-attention prohibits transformers from scaling to extremely long sequences (> 10k). Low-rank decomposition as a non-negative matrix factorization (NMF) of self-attention demonstrates remarkable performance in linear space and time complexity with strong theoretical guarantees. However, our analysis reveals that NMF-based works struggle to capture the rich spatio-temporal visual cues scattered across the long sequences resulting from egocentric lifelogs. To capture such cues, we propose a novel attention mechanism named SEMantic Attention (SEMA), which factorizes the self-attention matrix into a semantically meaningful subspace. We demonstrate SEMA in a representation learning setting, aiming to recover activity patterns in extremely long (weeks-long) egocentric lifelogs using a novel self-supervised training pipeline. Compared to the current state-of-the-art, we report significant improvement in terms of (NMI, AMI, and F-Score) for EgoRoutine, UTE, and Epic Kitchens datasets. Furthermore, to underscore the efficacy of SEMA, we extend its application to conventional video tasks such as online action detection, video recognition, and action localization.",https://openaccess.thecvf.com/content/WACV2024/html/Nagar_SEMA_Semantic_Attention_for_Capturing_Long-Range_Dependencies_in_Egocentric_Lifelogs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nagar_SEMA_Semantic_Attention_for_Capturing_Long-Range_Dependencies_in_Egocentric_Lifelogs_WACV_2024_paper.pdf,,https://github.com/Pravin74/Semantic_attention/,,main,Poster,https://ieeexplore.ieee.org/document/10483797/,"['Location awareness', 'Training', 'Representation learning', 'Visualization', 'Current transformers', 'Semantics', 'Redundancy']","['Semantic Attention', 'Time And Space', 'Activity Patterns', 'Time Complexity', 'Matrix Factorization', 'Positive Matrix', 'Attention Mechanism', 'Representation Learning', 'Space Complexity', 'Linear Time', 'Online Video', 'Non-negative Matrix Factorization', 'Linear Space', 'Linear Complexity', 'Theoretical Guarantees', 'Transformer Architecture', 'Global Dependencies', 'Video Recognition', 'Linear Time Complexity', 'Action Detection', 'Self-supervised Learning', 'Representative Frames', 'Low-rank Factorization', 'Pseudo Labels', 'Attention Matrix', 'Active Clusters', 'Video Summarization', 'Sequence Of Frames', 'Network Embedding', 'Spectral Clustering']","['Algorithms', 'Video recognition and understanding']",,"Transformer architecture is a defacto standard for modeling global dependency in long sequences. However, quadratic space and time complexity for self-attention prohibits transformers from scaling to extremely long sequences (> 10k). Low-rank decomposition as a non-negative matrix factorization (NMF) of self-attention demonstrates remarkable performance in linear space and time complexity with strong theoretical guarantees. However, our analysis reveals that NMF-based works struggle to capture the rich spatio-temporal visual cues scattered across the long sequences resulting from egocentric lifelogs. To capture such cues, we propose a novel attention mechanism named SEMantic Atention (SEMA), which factorizes the self-attention matrix into a semantically meaningful subspace. We demonstrate SEMA in a representation learning setting, aiming to recover activity patterns in extremely long (weeks-long) egocentric lifelogs using a novel self-supervised training pipeline. Compared to the current state-of-the-art, we report significant improvement in terms of (NMI, AMI, and F-Score) for EgoRoutine, UTE, and Epic Kitchens datasets. Furthermore, to underscore the efficacy of SEMA, we extend its application to conventional video tasks such as online action detection, video recognition, and action localization. Code is available at https://github.com/Pravin74/Semantic_attention/"
SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction,"Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski",University of Ulm; Robert Bosch Corporate Research; TU Wien; Bosch Center for Artificial Intelligence,50.0,"Austria, Germany",50.0,Germany,"In the field of 3D scene understanding, 3D scene graphs have emerged as a new scene representation that combines geometric and semantic information about objects and their relationships. However, learning semantic 3D scene graphs in a fully supervised manner is inherently difficult as it requires not only object-level annotations but also relationship labels. While pre-training approaches have helped to boost the performance of many methods in various fields, pre-training for 3D scene graph prediction has received little attention. Furthermore, we find in this paper that classical contrastive point cloud-based pre-training approaches are ineffective for 3D scene graph learning. To this end, we present SGRec3D, a novel self-supervised pre-training method for 3D scene graph prediction. We propose to reconstruct the 3D input scene from a graph bottleneck as a pretext task. Pre-training SGRec3D does not require object relationship labels, making it possible to exploit large-scale 3D scene understanding datasets, which were off-limits for 3D scene graph learning before. Our experiments demonstrate that in contrast to recent point cloud-based pre-training approaches, our proposed pre-training improves the 3D scene graph prediction considerably, which results in SOTA performance, outperforming other 3D scene graph models by +10% on object prediction and +4% on relationship prediction. Additionally, we show that only using a small subset of 10% labeled data during fine-tuning is sufficient to outperform the same model without pre-training.",https://openaccess.thecvf.com/content/WACV2024/html/Koch_SGRec3D_Self-Supervised_3D_Scene_Graph_Learning_via_Object-Level_Scene_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Koch_SGRec3D_Self-Supervised_3D_Scene_Graph_Learning_via_Object-Level_Scene_Reconstruction_WACV_2024_paper.pdf,kochsebastian.com/sgrec3d,,2309.15702,main,Poster,https://ieeexplore.ieee.org/document/10484453/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Annotations', 'Semantics', 'Predictive models', 'Data models']","['3D Scene', 'Scene Reconstruction', 'Scene Graph', '3D Scene Graph', '3D Graph', '3D Prediction', 'Pretext Task', 'Pre-training Method', 'Semantic Graph', 'Large Datasets', 'Computer Vision', '3D Reconstruction', 'Point Cloud', 'First Pass', 'Bounding Box', 'Object Classification', '3D Data', '3D Scanning', 'Additional Datasets', 'Object Shape', 'Node Features', '3D Point Cloud', '3D Datasets', 'Graph Neural Networks', 'Latent Representation', 'Edge Features', 'Pre-training Dataset', 'Latent Vector', 'Categorical Cross-entropy Loss', 'Final Feature Vector']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",4,"In the field of 3D scene understanding, 3D scene graphs have emerged as a new scene representation that combines geometric and semantic information about objects and their relationships. However, learning semantic 3D scene graphs in a fully supervised manner is inherently difficult as it requires not only object-level annotations but also relationship labels. While pre-training approaches have helped to boost the performance of many methods in various fields, pre-training for 3D scene graph prediction has received little attention. Furthermore, we find in this paper that classical contrastive point cloud-based pre-training approaches are ineffective for 3D scene graph learning. To this end, we present SGRec3D, a novel self-supervised pre-training method for 3D scene graph prediction. We propose to reconstruct the 3D input scene from a graph bottleneck as a pretext task. Pre-training SGRec3D does not require object relationship labels, making it possible to exploit large-scale 3D scene understanding datasets, which were off-limits for 3D scene graph learning before. Our experiments demonstrate that in contrast to recent point cloud-based pre-training approaches, our proposed pre-training improves the 3D scene graph prediction considerably, which results in SOTA performance, outperforming other 3D scene graph models by +10% on object prediction and +4% on relationship prediction. Additionally, we show that only using a small subset of 10% labeled data during fine-tuning is sufficient to outperform the same model without pre-training."
SICKLE: A Multi-Sensor Satellite Imagery Dataset Annotated With Multiple Key Cropping Parameters,"Depanshu Sani, Sandeep Mahato, Sourabh Saini, Harsh Kumar Agarwal, Charu Chandra Devshali, Saket Anand, Gaurav Arora, Thiagarajan Jayaraman","Indraprastha Institute of Information Technology, Delhi, India; MS Swaminathan Research Foundation, Chennai, India",50.0,India,50.0,India,"The availability of well-curated datasets has driven the success of Machine Learning (ML) models. Despite greater access to earth observation data in agriculture, there is a scarcity of curated and labelled datasets, which limits the potential of its use in training ML models for remote sensing (RS) in agriculture. To this end, we introduce a first-of-its-kind dataset called SICKLE, which constitutes a time-series of multi-resolution imagery from 3 distinct satellites: Landsat-8, Sentinel-1 and Sentinel-2. Our dataset constitutes multi-spectral, thermal and microwave sensors during January 2018 - March 2021 period. We construct each temporal sequence by considering the cropping practices followed by farmers primarily engaged in paddy cultivation in the Cauvery Delta region of Tamil Nadu, India; and annotate the corresponding imagery with key cropping parameters at multiple resolutions (i.e. 3m, 10m and 30m). Our dataset comprises 2, 370 season-wise samples from 388 unique plots, having an average size of 0.38 acres, for classifying 21 crop types across 4 districts in the Delta, which amounts to approximately 209, 000 satellite images. Out of the 2, 370 samples, 351 paddy samples from 145 plots are annotated with multiple crop parameters; such as the variety of paddy, its growing season and productivity in terms of per-acre yields. Ours is also one among the first studies that consider the growing season activities pertinent to crop phenology (spans sowing, transplanting and harvesting dates) as parameters of interest. We benchmark SICKLE on three tasks: crop type, crop phenology (sowing, transplanting, harvesting), and yield prediction.",https://openaccess.thecvf.com/content/WACV2024/html/Sani_SICKLE_A_Multi-Sensor_Satellite_Imagery_Dataset_Annotated_With_Multiple_Key_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sani_SICKLE_A_Multi-Sensor_Satellite_Imagery_Dataset_Annotated_With_Multiple_Key_WACV_2024_paper.pdf,https://sites.google.com/iiitd.ac.in/sickle/home,,2312.00069,main,Poster,https://ieeexplore.ieee.org/document/10484135/,"['Satellites', 'Image resolution', 'Pipelines', 'Phenology', 'Crops', 'Machine learning', 'Agriculture']","['Satellite Imagery', 'Multiple Crops', 'Multi-sensor Satellite', 'Benchmark', 'Machine Learning Models', 'Satellite Images', 'January 2018', 'Remote Sensing', 'Paddy Fields', 'Yield Prediction', 'Crop Types', 'Tamil Nadu', 'Harvest Date', 'Train Machine Learning Models', 'Multiple Resolutions', 'Sowing Date', 'Crop Parameters', 'Root Mean Square Error', 'Mean Square Error', 'Training Set', 'Phenological Data', 'Mean Absolute Error', 'Time Series Data', 'Crop Yield Prediction', 'Mean Absolute Percentage Error', 'Semantic Segmentation', 'Crop Yield', 'Yield Estimation', 'Plot Level', 'Field Data Collection']","['Applications', 'Agriculture', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Remote Sensing']",,"The availability of well-curated datasets has driven the success of Machine Learning (ML) models. Despite greater access to earth observation data in agriculture, there is a scarcity of curated and labelled datasets, which limits the potential of its use in training ML models for remote sensing (RS) in agriculture. To this end, we introduce a first-of-its-kind dataset called SICKLE, which constitutes a time-series of multi-resolution imagery from 3 distinct satellites: Landsat-8, Sentinel-1 and Sentinel-2. Our dataset constitutes multi-spectral, thermal and microwave sensors during January 2018 - March 2021 period. We construct each temporal sequence by considering the cropping practices followed by farmers primarily engaged in paddy cultivation in the Cauvery Delta region of Tamil Nadu, India; and annotate the corresponding imagery with key cropping parameters at multiple resolutions (i.e. 3m, 10m and 30m). Our dataset comprises 2, 370 season-wise samples from 388 unique plots, having an average size of 0.38 acres, for classifying 21 crop types across 4 districts in the Delta, which amounts to approximately 209,000 satellite images. Out of the 2,370 samples, 351 paddy samples from 145 plots are annotated with multiple crop parameters; such as the variety of paddy, its growing season and productivity in terms of per-acre yields. Ours is also one among the first studies that consider the growing season activities pertinent to crop phenology (spans sowing, transplanting and harvesting dates) as parameters of interest. We benchmark SICKLE on three tasks: crop type, crop phenology (sowing, transplanting, harvesting), and yield prediction."
SLoSH: Set Locality Sensitive Hashing via Sliced-Wasserstein Embeddings,"Yuzhe Lu, Xinran Liu, Andrea Soltoggio, Soheil Kolouri",Carnegie Mellon University; Loughborough University; Vanderbilt University,100.0,"UK, USA",0.0,,"Learning from set-structured data is an essential problem with many applications in machine learning and computer vision. This paper focuses on non-parametric and data-independent learning from set-structured data using approximate nearest neighbor (ANN) solutions, particularly locality-sensitive hashing. We consider the problem of set retrieval from an input set query. Such a retrieval problem requires: 1) an efficient mechanism to calculate the distances/dissimilarities between sets, and 2) an appropriate data structure for fast nearest-neighbor search. To that end, we propose to use Sliced-Wasserstein embedding as a computationally efficient set-2-vector operator that enables downstream ANN, with theoretical guarantees. The set elements are treated as samples from an unknown underlying distribution, and the Sliced-Wasserstein distance is used to compare sets. We demonstrate the effectiveness of our algorithm, denoted as Set Locality Sensitive Hashing (SLoSH), on various set retrieval datasets and compare our proposed embedding with standard set embedding approaches, including Generalized Mean (GeM) embedding/pooling, Featurewise Sort Pooling (FSPool), Covariance Pooling, and Wasserstein embedding and show consistent improvement in retrieval results.",https://openaccess.thecvf.com/content/WACV2024/html/Lu_SLoSH_Set_Locality_Sensitive_Hashing_via_Sliced-Wasserstein_Embeddings_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lu_SLoSH_Set_Locality_Sensitive_Hashing_via_Sliced-Wasserstein_Embeddings_WACV_2024_paper.pdf,,,2112.05872,main,Poster,https://ieeexplore.ieee.org/document/10484401/,"['Point cloud compression', 'Computer vision', 'Codes', 'Sensitivity analysis', 'Parallel processing', 'Nearest neighbor methods', 'Vectors']","['Locality Sensitive Hashing', 'Machine Learning', 'Computational Efficiency', 'Computer Vision', 'Machine Learning Applications', 'Optimal Transport', 'Nearest Neighbor Search', 'General Mean', 'Computer Vision Applications', 'Theoretical Guarantees', 'Convolutional Neural Network', 'Cardinality', 'Dimensional Space', 'Vector Space', 'Point Cloud', 'Empirical Distribution', 'Set Of Elements', 'Hilbert Space', 'Continuous Operation', 'Hash Function', 'Wasserstein Distance', 'Global Pooling', 'Code Length', 'Retrieval Performance', 'Random Projection', 'Ni Elements', 'MNIST Dataset', 'Quantile Function', 'Retrieval Accuracy', 'Deep Set']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Learning from set-structured data is an essential problem with many applications in machine learning and computer vision. This paper focuses on a non-parametric, data-independent, and efficient learning algorithm from setstructured data using optimal transport and approximate nearest neighbor (ANN) solutions, particularly localitysensitive hashing. We consider the problem of set retrieval from an input set query. This retrieval problem requires 1) an efficient mechanism to calculate the distances/dissimilarities between sets and 2) an appropriate data structure for a fast nearest-neighbor search. To that end, we propose to use Sliced-Wasserstein embedding as a computationally efficient ""set-2-vector"" operator that enables downstream ANN with theoretical guarantees. The set elements are treated as samples from an unknown underlying distribution, and the Sliced-Wasserstein distance is used to compare sets. We demonstrate the effectiveness of our algorithm, denoted as Set Locality Sensitive Hashing (SLoSH), on various set retrieval datasets and compare our proposed embedding with standard set embedding approaches, including Generalized Mean (GeM) embedding/pooling, Featurewise Sort Pooling (FSPool), Covariance Pooling, and Wasserstein embedding and show consistent improvement in retrieval results, both in terms of accuracy and computational efficiency."
SOAP: Cross-Sensor Domain Adaptation for 3D Object Detection Using Stationary Object Aggregation Pseudo-Labelling,"Chengjie Huang, Vahdat Abdelzad, Sean Sedwards, Krzysztof Czarnecki",University of Waterloo,100.0,Canada,0.0,,"We consider the problem of cross-sensor domain adaptation in the context of LiDAR-based 3D object detection and propose Stationary Object Aggregation Pseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary objects. In contrast to the current state-of-the-art in-domain practice of aggregating just a few input scans, SOAP aggregates entire sequences of point clouds at the input level to reduce the sensor domain gap. Then, by means of what we call quasi-stationary training and spatial consistency post-processing, the SOAP model generates accurate pseudo-labels for stationary objects, closing a minimum of 30.3% domain gap compared to few-frame detectors. Our results also show that state-of-the-art domain adaptation approaches can achieve even greater performance in combination with SOAP, in both the unsupervised and semi-supervised settings.",https://openaccess.thecvf.com/content/WACV2024/html/Huang_SOAP_Cross-Sensor_Domain_Adaptation_for_3D_Object_Detection_Using_Stationary_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huang_SOAP_Cross-Sensor_Domain_Adaptation_for_3D_Object_Detection_Using_Stationary_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483852/,"['Training', 'Point cloud compression', 'Computer vision', 'Adaptation models', 'Three-dimensional displays', 'Aggregates', 'Object detection']","['Object Detection', 'Domain Adaptation', 'Static Objects', '3D Object Detection', 'Point Cloud', 'Entire Sequence', 'Domain Gap', 'Coordinate System', 'Global System', 'Partial Sequences', 'Pedestrian', 'Bounding Box', 'Target Domain', 'Sequence Of Points', 'Source Domain', 'Local Coordinate System', 'Dynamic Objects', 'Scan Pattern', 'Global Coordinate System', 'State Of The Art Methods', 'Common Coordinate System', 'Sparse Point Cloud', 'Dense Objects', 'Source Domain Data', 'Target Domain Data', 'LiDAR Sensor', 'mAP Improvement', 'Multiple Frames', 'Labeled Target Domain', 'Domain Adaptation Methods']","['Algorithms', '3D computer vision', 'Applications', 'Autonomous Driving']",1,"We consider the problem of cross-sensor domain adaptation in the context of LiDAR-based 3D object detection and propose Stationary Object Aggregation Pseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary objects. In contrast to the current state-of-the-art indomain practice of aggregating just a few input scans, SOAP aggregates entire sequences of point clouds at the input level to reduce the sensor domain gap. Then, by means of what we call quasi-stationary training and spatial consistency post-processing, the SOAP model generates accurate pseudo-labels for stationary objects, closing a minimum of 30.3% domain gap compared to few-frame detectors. Our results also show that state-of-the-art domain adaptation approaches can achieve even greater performance in combination with SOAP, in both the unsupervised and semisupervised settings."
SSP: Semi-Signed Prioritized Neural Fitting for Surface Reconstruction From Unoriented Point Clouds,"Runsong Zhu, Di Kang, Ka-Hei Hui, Yue Qian, Shi Qiu, Zhen Dong, Linchao Bao, Pheng-Ann Heng, Chi-Wing Fu",Tencent AI Lab; Wuhan University; The Chinese University of Hong Kong,66.66666666666666,"China, Hong Kong",33.33333333333334,China,"Reconstructing 3D geometry from unoriented point clouds can benefit many downstream tasks. % Recent shape modeling methods mostly adopt implicit neural representation to fit a signed distance field (SDF) and optimize the network by unsigned supervision. % However, these methods occasionally have difficulty in finding the coarse shape for complicated objects, especially suffering from the ""ghost"" surfaces (i.e., fake surfaces that should not exist). % To guide the network quickly fit the coarse shape, we propose to utilize the signed supervision in regions that are obviously outside the object and can be easily determined, resulting in our semi-signed supervision. % To better recover high-fidelity details, a novel loss-based region sampling strategy and a progressive positional encoding (PE) method are applied to prioritize the optimization towards underfitting and complicated regions. % Specifically, we voxelize and partition the object space into sign-known and sign-uncertain regions, in which different supervisions are applied. % Besides, we adaptively adjust the sampling rate of each voxel according to the tracked reconstruction loss, so that the network can focus more on the complicated under-fitting regions. % We conduct extensive experiments to demonstrate that our method achieves state-of-the-art performance compared to the existing fitting-based methods and comparable performance to learning-based methods on multiple datasets. % The code is publicly available at https://github.com/Runsong123/SSP.",https://openaccess.thecvf.com/content/WACV2024/html/Zhu_SSP_Semi-Signed_Prioritized_Neural_Fitting_for_Surface_Reconstruction_From_Unoriented_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_SSP_Semi-Signed_Prioritized_Neural_Fitting_for_Surface_Reconstruction_From_Unoriented_WACV_2024_paper.pdf,,https://github.com/Runsong123/SSP,,main,Poster,https://ieeexplore.ieee.org/document/10483939/,"['Point cloud compression', 'Learning systems', 'Geometry', 'Surface reconstruction', 'Three-dimensional displays', 'Shape', 'Fitting']","['Point Cloud', 'Surface Reconstruction', 'Unoriented', 'Multiple Datasets', 'Learning-based Methods', 'Difficulty Finding', 'Positional Encoding', 'Signed Distance Function', 'Neural Network', 'Smooth Surface', 'Large Errors', 'Sampling Density', 'Visual Comparison', 'Noisy Data', 'Fine Details', 'Target Object', 'Complicated Structure', 'Implicit Function', 'Spatial Partitioning', 'Implicit Method', 'Accurate Surface', 'Input Point Cloud', 'Adaptive Sampling', 'Optimization Difficulty', 'Normal Orientation', 'Raw Point Cloud', 'Breadth-first Search', '2nd Row', 'Voxel Size', 'Loss Term']","['Algorithms', '3D computer vision', 'Applications', 'Visualization']",2,"Reconstructing 3D geometry from unoriented point clouds can benefit many downstream tasks. Recent shape modeling methods mostly adopt implicit neural representation to fit a signed distance field (SDF) and optimize the network by unsigned supervision. However, these methods occasionally have difficulty in finding the coarse shape for complicated objects, especially suffering from the ""ghost"" surfaces (i.e., fake surfaces that should not exist). To guide the network quickly fit the coarse shape, we propose to utilize the signed supervision in regions that are obviously outside the object and can be easily determined, resulting in our semi-signed supervision. To better recover high-fidelity details, a novel loss-based region sampling strategy and a progressive positional encoding (PE) method are applied to prioritize the optimization towards underfitting and complicated regions. Specifically, we voxelize and partition the object space into sign-known and sign-uncertain regions, in which different supervisions are applied. Besides, we adaptively adjust the sampling rate of each voxel according to the tracked reconstruction loss, so that the network can focus more on the complicated under-fitting regions. We conduct extensive experiments to demonstrate that our method achieves state-of-the-art performance compared to the existing fitting-based methods and comparable performance to learning-based methods on multiple datasets. The code is publicly available at https://github.com/Runsong123/SSP."
SSVOD: Semi-Supervised Video Object Detection With Sparse Annotations,"Tanvir Mahmud, Chun-Hao Liu, Burhaneddin Yaman, Diana Marculescu",Bosch Research North America; Amazon Prime Video; University of Texas at Austin,33.33333333333333,USA,66.66666666666667,USA,"Despite significant progress in semi-supervised learning for image object detection, several key issues are yet to be addressed for video object detection: (1) Achieving good performance for supervised video object detection greatly depends on the availability of annotated frames. (2) Despite having large inter-frame correlations in a video, collecting annotations for a large number of frames per video is expensive, time-consuming, and often redundant. (3) Existing semi-supervised techniques on static images can hardly exploit the temporal motion dynamics inherently present in videos. In this paper, we introduce SSVOD, an end-to-end semi-supervised video object detection framework that exploits motion dynamics of videos to utilize large-scale unlabeled frames with sparse annotations. To selectively assemble robust pseudo-labels across groups of frames, we introduce flow-warped predictions from nearby frames for temporal-consistency estimation. In particular, we introduce cross-IoU and cross-divergence based selection methods over a set of estimated predictions to include robust pseudo-labels for bounding boxes and class labels, respectively. To strike a balance between confirmation bias and uncertainty noise in pseudo-labels, we propose confidence threshold based combination of hard and soft pseudo-labels. Our method achieves significant performance improvements over existing methods on ImageNet-VID, Epic-KITCHENS, and YouTube-VIS datasets. Codes are available at https://github.com/enyac-group/SSVOD.git.",https://openaccess.thecvf.com/content/WACV2024/html/Mahmud_SSVOD_Semi-Supervised_Video_Object_Detection_With_Sparse_Annotations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mahmud_SSVOD_Semi-Supervised_Video_Object_Detection_With_Sparse_Annotations_WACV_2024_paper.pdf,,https://github.com/enyac-group/SSVOD.git,2309.01391,main,Poster,https://ieeexplore.ieee.org/document/10484301/,"['Uncertainty', 'Annotations', 'Dynamics', 'Noise', 'Estimation', 'Object detection', 'Detectors']","['Object Detection', 'Video Object Detection', 'Sparse Annotations', 'Temporal Dynamics', 'Class Labels', 'Bounding Box', 'Static Images', 'Confidence Threshold', 'Semi-supervised Learning', 'Confirmation Bias', 'Object Detection Framework', 'Reference Frame', 'Video Frames', 'Considerable Improvement', 'Additional Datasets', 'Loss Of Components', 'Flow Estimation', 'Suboptimal Performance', 'Training Videos', 'Pseudo Labels', 'Temporal Consistency', 'Semi-supervised Learning Framework', 'Unlabeled Set', 'Soft Classification', 'Flow Motion', 'Temporal Space', 'Motion Cues', 'Flow Map', 'Object Pairs']","['Algorithms', 'Video recognition and understanding', 'Applications', 'Autonomous Driving']",2,"Despite significant progress in semi-supervised learning for image object detection, several key issues are yet to be addressed for video object detection: (1) Achieving good performance for supervised video object detection greatly depends on the availability of annotated frames. (2) Despite having large inter-frame correlations in a video, collecting annotations for a large number of frames per video is expensive, time-consuming, and often redundant. (3) Existing semi-supervised techniques on static images can hardly exploit the temporal motion dynamics inherently present in videos. In this paper, we introduce SSVOD, an end-to-end semi-supervised video object detection framework that exploits motion dynamics of videos to utilize large-scale unlabeled frames with sparse annotations. To selectively assemble robust pseudo-labels across groups of frames, we introduce flow-warped predictions from nearby frames for temporal-consistency estimation. In particular, we introduce cross-IoU and cross-divergence based selection methods over a set of estimated predictions to include robust pseudo-labels for bounding boxes and class labels, respectively. To strike a balance between confirmation bias and uncertainty noise in pseudo-labels, we propose confidence threshold based combination of hard and soft pseudo-labels. Our method achieves significant performance improvements over existing methods on ImageNet-VID, Epic-KITCHENS, and YouTube-VIS datasets. Codes are available at https://github.com/enyacgroup/SSVOD.git."
STEP - Towards Structured Scene-Text Spotting,"Sergi Garcia-Bordils, Dimosthenis Karatzas, Marçal Rusiñol","Computer Vision Center, UAB, Spain; AllRead MLT",50.0,Spain,50.0,USA,"We introduce the structured scene-text spotting task, which requires a scene-text OCR system to spot text in the wild according to a query regular expression. Contrary to generic scene-text OCR, structured scene-text spotting seeks to dynamically condition both detection and recognition on user-provided regular expressions. To tackle this task, we propose the Structured TExt sPotter (STEP), a model that exploits the provided text structure to guide the OCR process. STEP is able to deal with regular expressions that contain spaces and it is not bound to detection at word-level granularity. Our approach enables accurate zero-shot structured text spotting in a wide variety of real-world reading scenarios and is solely trained on publicly available data. To demonstrate the effectiveness of our approach, we introduce a new challenging test dataset that contains several types of out-of-vocabulary structured text, reflecting important reading applications such as weight information, serial numbers, license plates etc. We demonstrate that STEP can provide specialized OCR performance on demand in all tested scenarios. The code and test dataset are released at https://github.com/CVC-DAG/STEP.",https://openaccess.thecvf.com/content/WACV2024/html/Garcia-Bordils_STEP_-_Towards_Structured_Scene-Text_Spotting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Garcia-Bordils_STEP_-_Towards_Structured_Scene-Text_Spotting_WACV_2024_paper.pdf,,https://github.com/CVC-DAG/STEP,,main,Poster,https://ieeexplore.ieee.org/document/10483597/,"['Training', 'Computer vision', 'Codes', 'Task analysis', 'License plate recognition']","['Scene Text Spotting', 'Test Dataset', 'Text Structure', 'License Plate', 'Transformer', 'Use Of Information', 'Image Features', 'Feature Maps', 'Bounding Box', 'Level Of Information', 'Dataset Characteristics', 'Language Model', 'Multi-scale Features', 'Word Level', 'Optical Character Recognition', 'Text Types', 'Qualitative Examples', 'Target Text', 'Spelling Mistakes', 'Post-processing Operations', 'Text Area', 'Deformable Layer', 'Multi-scale Feature Maps']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"We introduce the structured scene-text spotting task, which requires a scene-text OCR system to spot text in the wild according to a query regular expression. Contrary to generic scene-text OCR, structured scene-text spotting seeks to dynamically condition both detection and recognition on user-provided regular expressions. To tackle this task, we propose the Structured TExt sPotter (STEP), a model that exploits the provided text structure to guide the OCR process. STEP is able to deal with regular expressions that contain spaces and it is not bound to detection at wordlevel granularity. Our approach enables accurate zero-shot structured text spotting in a wide variety of real-world reading scenarios and is solely trained on publicly available data. To demonstrate the effectiveness of our approach, we introduce a new challenging test dataset that contains several types of out-of-vocabulary structured text, reflecting important reading applications such as weight information, serial numbers, license plates etc. We demonstrate that STEP can provide specialized OCR performance on demand in all tested scenarios. The code and test dataset are released at https://github.com/CVC-DAG/STEP."
STYLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-Based Domain Generalization,"Shirsha Bose, Ankit Jha, Enrico Fini, Mainak Singha, Elisa Ricci, Biplab Banerjee","Technical University of Munich, Germany; University of Trento, Italy; Indian Institute of Technology Bombay, India",100.0,"Germany, India, Italy",0.0,,"arge-scale foundation models, such as CLIP, have demonstrated impressive zero-shot generalization performance on downstream tasks, leveraging well-designed language prompts. However, these prompt learning techniques often struggle with domain shift, limiting their generalization capabilities. In our study, we tackle this issue by proposing STYLIP, a novel approach for Domain Generalization (DG) that enhances CLIP's classification performance across domains. Our method focuses on a domain-agnostic prompt learning strategy, aiming to disentangle the visual style and content information embedded in CLIP's pre-trained vision encoder, enabling effortless adaptation to novel domains during inference. To achieve this, we introduce a set of style projectors that directly learn the domain-specific prompt tokens from the extracted multi-scale style features. These generated prompt embeddings are subsequently combined with the multi-scale visual content features learned by a content projector. The projectors are trained in a contrastive manner, utilizing CLIP's fixed vision and text backbones. Through extensive experiments conducted in five different DG settings on multiple benchmark datasets, we consistently demonstrate that STYLIP outperforms the current state-of-the-art (SOTA) methods.",https://openaccess.thecvf.com/content/WACV2024/html/Bose_STYLIP_Multi-Scale_Style-Conditioned_Prompt_Learning_for_CLIP-Based_Domain_Generalization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bose_STYLIP_Multi-Scale_Style-Conditioned_Prompt_Learning_for_CLIP-Based_Domain_Generalization_WACV_2024_paper.pdf,,,2302.09251,main,Poster,https://ieeexplore.ieee.org/document/10484142/,"['Visualization', 'Computer vision', 'Limiting', 'Benchmark testing', 'Feature extraction', 'Task analysis']","['Domain Generalization', 'Visual Information', 'Information Content', 'Visual Features', 'Multiple Datasets', 'Domain Shift', 'Generalization Capability', 'Multi-scale Features', 'Content Features', 'Visual Content', 'Foundation Model', 'Style Features', 'Visual Style', 'Training Data', 'Feature Maps', 'Max-pooling', 'Latent Space', 'Random Vector', 'High-level Features', 'Formation Of Domains', 'Source Domain', 'Target Domain', 'Base Classes', 'Visual Space', 'Average Pooling', 'Text Encoder', 'Encoder Layer', 'Lth Layer', 'Output Feature Map', 'Global Average Pooling']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Image recognition and understanding']",5,"Large-scale foundation models, such as CLIP, have demonstrated impressive zero-shot generalization performance on downstream tasks, leveraging well-designed language prompts. However, these prompt learning techniques often struggle with domain shift, limiting their generalization capabilities. In our study, we tackle this issue by proposing StyLIP, a novel approach for Domain Generalization (DG) that enhances CLIP’s classification performance across domains. Our method focuses on a domain-agnostic prompt learning strategy, aiming to disentangle the visual style and content information embedded in CLIP’s pre-trained vision encoder, enabling effortless adaptation to novel domains during inference. To achieve this, we introduce a set of style projectors that directly learn the domain-specific prompt tokens from the extracted multi-scale style features. These generated prompt embeddings are subsequently combined with the multi-scale visual content features learned by a content projector. The projectors are trained in a contrastive manner, utilizing CLIP’s fixed vision and text backbones. Through extensive experiments conducted in five different DG settings on multiple benchmark datasets, we consistently demonstrate that StyLIP outperforms the current state-of-the-art (SOTA) methods."
Salient Object Detection for Images Taken by People With Vision Impairments,"Jarek Reynolds, Chandra Kanth Nagesh, Danna Gurari",University of Colorado Boulder,100.0,USA,0.0,,"Salient object detection is the task of producing a binary mask for an image that deciphers which pixels belong to the foreground object versus background. We introduce a new salient object detection dataset using images taken by people who are visually impaired who were seeking to better understand their surroundings, which we call VizWiz-SalientObject. Compared to seven existing datasets, VizWiz-SalientObject is the largest (i.e., 32,000 human-annotated images) and contains unique characteristics including a higher prevalence of text in the salient objects (i.e., in 68% of images) and salient objects that occupy a larger ratio of the images (i.e., on average,  50% coverage). We benchmarked ten modern models on our dataset. While most methods fall below human performance, struggling most for images with salient objects that are large, have less complex boundaries, and lack text as well as for lower quality images, one method one method is very close. To facilitate future extensions of this work, we publicly share the dataset at https://vizwiz.org/tasks-and-datasets/salient-object-detection.",https://openaccess.thecvf.com/content/WACV2024/html/Reynolds_Salient_Object_Detection_for_Images_Taken_by_People_With_Vision_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Reynolds_Salient_Object_Detection_for_Images_Taken_by_People_With_Vision_WACV_2024_paper.pdf,https://vizwiz.org/tasks-and-datasets/salient-object-detection,,2301.05323,main,Poster,https://ieeexplore.ieee.org/document/10484241/,"['Image quality', 'Computer vision', 'Grounding', 'Visual impairment', 'Object detection', 'Benchmark testing', 'Complexity theory']","['Visual Impairment', 'Salient Object', 'Salient Object Detection', 'Human Performance', 'Foreground Objects', 'Complex Boundary', 'Distinct Features', 'Convolutional Neural Network', 'Multiple Regions', 'Image Object', 'Intersection Over Union', 'Crowdsourcing', 'Food Packaging', 'Domain Shift', 'Large Objects', 'Shift Type', 'Coverage Ratio', 'Impression Management', 'Interesting Content', 'Visually Impaired People', 'Limitations Of Algorithms', 'Intersection Over Union Score', 'Annotation Task', 'Preliminary Questions', 'Visual Question', 'Type Of Content', 'Training Data', 'Visual Evidence', 'Computer Vision']","['Applications', 'Social good']",2,"Salient object detection is the task of producing a binary mask for an image that deciphers which pixels belong to the foreground object versus background. We introduce a new salient object detection dataset using images taken by people who are visually impaired who were seeking to better understand their surroundings, which we call VizWiz-SalientObject. Compared to seven existing datasets, VizWiz-SalientObject is the largest (i.e., 32,000 human-annotated images) and contains unique characteristics including a higher prevalence of text in the salient objects (i.e., in 68% of images) and salient objects that occupy a larger ratio of the images (i.e., on average, ∼50% coverage). We benchmarked ten modern models on our dataset. One method achieves nearly human performance while the rest struggle, mostly for images with salient objects that are large, have less complex boundaries, and lack text as well as for lower quality images. To facilitate future extensions, we share the dataset at https://vizwiz.org/tasks-anddatasets/salient-object-detection."
Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution,"Zhewei Huang, Ailin Huang, Xiaotao Hu, Chen Hu, Jun Xu, Shuchang Zhou","Nankai University; Guangdong Provincial Key Laboratory of Big Data Computing, CUHK; Megvii Technology; Megvii Technology; Nankai University",60.0,"China, Hong Kong",40.0,China,"The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual quality of videos, by simultaneously performing video frame interpolation (VFI) and video super-resolution (VSR). However, facing the challenge of the additional temporal dimension and scale inconsistency, most existing STVSR methods are complex and inflexible in dynamically modeling different motion amplitudes. In this work, we find that choosing an appropriate processing scale achieves remarkable benefits in flow-based feature propagation. We propose a novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects sub-networks with different processing scales for individual samples. Experiments on four public STVSR benchmarks demonstrate that SAFA achieves state-of-the-art performance. Our SAFA network outperforms recent state-of-the-art methods such as TMNet and VideoINR by an average improvement of over 0.5dB on PSNR, while requiring less than half the number of parameters and only 1/3 computational costs. Our code will be publicly released.",https://openaccess.thecvf.com/content/WACV2024/html/Huang_Scale-Adaptive_Feature_Aggregation_for_Efficient_Space-Time_Video_Super-Resolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huang_Scale-Adaptive_Feature_Aggregation_for_Efficient_Space-Time_Video_Super-Resolution_WACV_2024_paper.pdf,,https://github.com/megvii-research/WACV2024-SAFA,2310.17294,main,Poster,https://ieeexplore.ieee.org/document/10483849/,"['Training', 'Visualization', 'Superresolution', 'Pipelines', 'Estimation', 'Training data', 'Streaming media']","['Feature Aggregation', 'Video Super-resolution', 'Temporal Dimension', 'Peak Signal-to-noise Ratio', 'Propagation Characteristics', 'Comparative Method', 'Convolutional Neural Network', 'Image Features', 'Feature Maps', 'Flow Field', 'Optical Flow', 'Motion Model', 'Two-stage Method', 'Flow Estimation', 'Motion Estimation', 'Input Frames', 'Bicubic Interpolation', 'Optical Flow Estimation', 'Kernel-based Methods', 'Intermediate Frames', 'One-stage Methods', 'Deep Learning Era', 'Reconstruction Module', 'Intermediate Flow', 'Receptive Field', 'Video Clips']","['Algorithms', 'Computational photography', 'image and video synthesis']",3,"The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual quality of videos, by simultaneously performing video frame interpolation (VFI) and video super-resolution (VSR). However, facing the challenge of the additional temporal dimension and scale inconsistency, most existing STVSR methods are complex and inflexible in dynamically modeling different motion amplitudes. In this work, we find that choosing an appropriate processing scale achieves remarkable benefits in flow-based feature propagation. We propose a novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects sub-networks with different processing scales for individual samples. Experiments on four public STVSR benchmarks demonstrate that SAFA achieves state-of-the-art performance. Our SAFA network outperforms recent state-of-the-art methods such as TMNet [83] and VideoINR [10] by an average improvement of over 0.5dB on PSNR, while requiring less than half the number of parameters and only 1/3 computational costs."
ScanEnts3D: Exploiting Phrase-to-3D-Object Correspondences for Improved Visio-Linguistic Models in 3D Scenes,"Ahmed Abdelreheem, Kyle Olszewski, Hsin-Ying Lee, Peter Wonka, Panos Achlioptas",King Abdullah University of Science and Technology (KAUST); Snap Inc.,50.0,Saudi Arabia,50.0,USA,"The two popular datasets ScanRefer [20] and ReferIt3D [5] connect natural language to real-world 3D scenes. In this paper, we curate a complementary dataset extending both the aforementioned ones. We associate all objects mentioned in a referential sentence with their underlying instances inside a 3D scene. In contrast, previous work did this only for a single object per sentence. Our Scan Entities in 3D (ScanEnts3D) dataset provides explicit cor- respondences between 369k objects across 84k referential sentences, covering 705 real-world scenes. We propose novel architecture modifications and losses that enable learning from this new type of data and improve the performance for both neural listening and language generation. For neu- ral listening, we improve the SoTA in both the Nr3D and ScanRefer benchmarks by 4.3% and 5.0%, respectively. For language generation, we improve the SoTA by 13.2 CIDEr points on the Nr3D benchmark. For both of these tasks, the new type of data is only used to improve training, but no additional annotations are required at inference time. Our introduced dataset is available on the project's webpage at https://scanents3d.github.io/.",https://openaccess.thecvf.com/content/WACV2024/html/Abdelreheem_ScanEnts3D_Exploiting_Phrase-to-3D-Object_Correspondences_for_Improved_Visio-Linguistic_Models_in_3D_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Abdelreheem_ScanEnts3D_Exploiting_Phrase-to-3D-Object_Correspondences_for_Improved_Visio-Linguistic_Models_in_3D_WACV_2024_paper.pdf,https://scanents3d.github.io/,,2212.06250,main,Poster,,,,,,
Scene Text Image Super-Resolution Based on Text-Conditional Diffusion Models,"Chihiro Noguchi, Shun Fukuda, Masao Yamanaka","Toyota Motor Corporation, Japan",0.0,,100.0,Japan,"Scene Text Image Super-resolution (STISR) has recently achieved great success as a preprocessing method for scene text recognition. STISR aims to transform blurred and noisy low-resolution (LR) text images in real-world settings into clear high-resolution (HR) text images suitable for scene text recognition. In this study, we leverage text-conditional diffusion models (DMs), known for their impressive text-to-image synthesis capabilities, for STISR tasks. Our experimental results revealed that text-conditional DMs notably surpass existing STISR methods. Especially when texts from LR text images are given as input, the text-conditional DMs are able to produce superior quality super-resolution text images. Utilizing this capability, we propose a novel framework for synthesizing LR-HR paired text image datasets. This framework consists of three specialized text-conditional DMs, each dedicated to text image synthesis, super-resolution, and image degradation. These three modules are vital for synthesizing distinct LR and HR paired images, which are more suitable for training STISR methods. Our experiments confirmed that these synthesized image pairs significantly enhance the performance of STISR methods in the TextZoom evaluation.",https://openaccess.thecvf.com/content/WACV2024/html/Noguchi_Scene_Text_Image_Super-Resolution_Based_on_Text-Conditional_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Noguchi_Scene_Text_Image_Super-Resolution_Based_on_Text-Conditional_Diffusion_Models_WACV_2024_paper.pdf,,,2311.09759,main,Poster,https://ieeexplore.ieee.org/document/10484477/,"['Training', 'Degradation', 'Computer vision', 'Text recognition', 'Image synthesis', 'Superresolution', 'Transforms']","['Super-resolution', 'Diffusion Model', 'Image Texture', 'Scene Text', 'Performance Of Method', 'High-resolution Images', 'Image Pairs', 'Low-resolution Images', 'Optical Character Recognition', 'Blurred Images', 'Image Degradation', 'Gaussian Noise', 'Input Image', 'Dataset Size', 'Reversible Process', 'Recognition Accuracy', 'Image Generation', 'High-quality Images', 'Language Model', 'Textual Information', 'Single Image Super-resolution', 'Input Text', 'Textual Features', 'Blur Kernel', 'Forward Process', 'Bicubic Interpolation', 'Lowercase Letters', 'Text Encoder', 'Text Labels', 'Synthetic Images']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",4,"Scene Text Image Super-resolution (STISR) has recently achieved great success as a preprocessing method for scene text recognition. STISR aims to transform blurred and noisy low-resolution (LR) text images in real-world settings into clear high-resolution (HR) text images suitable for scene text recognition. In this study, we leverage text-conditional diffusion models (DMs), known for their impressive text-to-image synthesis capabilities, for STISR tasks. Our experimental results revealed that text-conditional DMs notably surpass existing STISR methods. Especially when texts from LR text images are given as input, the text-conditional DMs are able to produce superior quality super-resolution text images. Utilizing this capability, we propose a novel framework for synthesizing LR-HR paired text image datasets. This framework consists of three specialized text-conditional DMs, each dedicated to text image synthesis, super-resolution, and image degradation. These three modules are vital for synthesizing distinct LR and HR paired images, which are more suitable for training STISR methods. Our experiments confirmed that these synthesized image pairs significantly enhance the performance of STISR methods in the TextZoom evaluation."
SciOL and MuLMS-Img: Introducing a Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain,"Tim Tarsi, Heike Adel, Jan Hendrik Metzen, Dan Zhang, Matteo Finco, Annemarie Friedrich","Robert Bosch GmbH, Renningen, Germany; Hochschule der Medien, Stuttgart, Germany; Bosch Center for Artificial Intelligence, Renningen, Germany; University of Augsburg, Germany",50.0,Germany,50.0,Germany,"In scientific publications, a substantial part of the information is expressed via figures containing images and diagrams. Hence, the retrieval of relevant figures given a natural language query is an important real-world task. However, due to the lack of training and evaluation data, most existing approaches are either limited to one modality or focus on non-scientific domains, making their application to scientific publications challenging. In this paper, we address this gap by introducing two novel datasets: (1) SciOL, the largest openly-licensed pre-training corpus for multimodal models in the scientific domain, covering multiple sciences including materials science, physics, and computer science, and (2) MuLMS-Img, a high-quality dataset in the materials science domain, manually annotated for various image-text tasks. Our experiments show that pre-training large-scale vision-language models on SciOL increases performance considerably across a broad variety of image-text tasks including figure type classification, optical character recognition, captioning, and figure retrieval. Using MuLMS-Img, we show that integrating text-based features extracted via a fine-tuned model for a specific domain can boost cross-modal scientific figure retrieval performance by up to 50%.",https://openaccess.thecvf.com/content/WACV2024/html/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483603/,"['Training', 'Materials science and technology', 'Computational modeling', 'Optical character recognition', 'Training data', 'Performance gain', 'Feature extraction']","['Multimodal Model', 'Science Datasets', 'Training Data', 'Computer Science', 'Materials Science', 'Scientific Publications', 'High-quality Dataset', 'Optical Character Recognition', 'Retrieval Performance', 'Visual Representation', 'Bounding Box', 'Figure Caption', 'Domain Experts', 'Self-supervised Learning', 'Retrieval System', 'Supplementary S3', 'Body Of The Text', 'Semi-structured Format', 'PDF Files', 'Semi-structured Data', 'Text Extraction', 'Word Error Rate', 'Text Encoder', 'Multimodal Tasks', 'Text Query', 'Supplementary S6', 'Real-world Use Cases', 'Chart Types', 'Image Embedding', 'Class Labels']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",,"In scientific publications, a substantial part of the information is expressed via figures containing images and diagrams. Hence, the retrieval of relevant figures given a natural language query is an important real-world task. However, due to the lack of training and evaluation data, most existing approaches are either limited to one modality or focus on non-scientific domains, making their application to scientific publications challenging.In this paper, we address this gap by introducing two novel datasets: (1) SciOL, the largest openly-licensed pre-training corpus for multimodal models in the scientific domain, covering multiple sciences including materials science, physics, and computer science, and (2) MuLMS-Img, a high-quality dataset in the materials science domain, manually annotated for various image-text tasks. Our experiments show that pre-training large-scale vision-language models on SciOL increases performance considerably across a broad variety of image-text tasks including figure type classification, optical character recognition, captioning, and figure retrieval. Using MuLMS-Img, we show that integrating text-based features extracted via a fine-tuned model for a specific domain can boost cross-modal scientific figure retrieval performance by up to 50%."
SeaTurtleID2022: A Long-Span Dataset for Reliable Sea Turtle Re-Identification,"Lukáš Adam, Vojtěch Čermák, Kostas Papafitsoros, Lukas Picek",Czech Technical University; Queen Mary University of London; UWB and INRIA,100.0,"Czech Republic, France, UK",0.0,,"This paper introduces the first public large-scale, long-span dataset with sea turtle photographs captured in the wild - SeaTurtleID2022. The dataset contains 8729 photographs of 438 unique individuals collected within 13 years, making it the longest-spanned dataset for animal re-identification. Each photograph includes various annotations, e.g., identity, encounter timestamp, and body parts segmentation masks. Instead of a standard ""random"" split, the dataset allows for two realistic and ecologically motivated splits: (i) time-aware: a closed-set with training, validation, and test data from different days/years, and (ii) open-set: with new unknown individuals in test and validation sets. We show that time-aware splits are essential for benchmarking methods for re-identification, as random splits lead to performance overestimation. Furthermore, a baseline instance segmentation and re-identification performance over various body parts is provided. At last, an end-to-end system for sea turtle re-identification is proposed and evaluated. The proposed system based on Hybrid Task Cascade for head instance segmentation and ArcFace-trained feature-extractor achieved an accuracy of 86.8%.",https://openaccess.thecvf.com/content/WACV2024/html/Adam_SeaTurtleID2022_A_Long-Span_Dataset_for_Reliable_Sea_Turtle_Re-Identification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Adam_SeaTurtleID2022_A_Long-Span_Dataset_for_Reliable_Sea_Turtle_Re-Identification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484106/,"['Instance segmentation', 'Training', 'Computer vision', 'Annotations', 'Wildlife', 'Sociology', 'Benchmark testing']","['Sea Turtles', 'Body Parts', 'Validation Set', 'Instance Segmentation', 'Unknown Individuals', 'Training Set', 'Deep Learning', 'Evaluation Method', 'Object Detection', '3D Reconstruction', 'Face Recognition', 'Semantic Segmentation', 'Majority Voting', 'Baseline Performance', 'Information Leakage', 'Handcrafted Features', 'Local Descriptors', 'Original Resolution', 'Animal Identification', 'Metric Learning', 'Query Set', 'Caretta', 'Triplet Loss', 'Mask R-CNN', 'Fisheye Lens']","['Applications', 'Animals / Insects', 'Applications', 'Environmental monitoring / climate change / ecology']",3,"This paper introduces the first public large-scale, long-span dataset with sea turtle photographs captured in the wild -$SeaTurtleID2022$. The dataset contains 8729 photographs of 438 unique individuals collected within 13 years, making it the longest-spanned dataset for animal re-identification. Each photograph includes various annotations, e.g., identity, encounter timestamp, and body parts segmentation masks. Instead of a standard ’’random"" split, the dataset allows for two realistic and ecologically motivated splits: (i) time-aware: a closed-set with training, validation, and test data from different days/years, and (ii) open-set: with new unknown individuals in test and validation sets. We show that time-aware splits are essential for benchmarking methods for re-identification, as random splits lead to performance overestimation. Furthermore, a baseline instance segmentation and re-identification performance over various body parts is provided. At last, an end-to-end system for sea turtle re-identification is proposed and evaluated. The proposed system based on Hybrid Task Cascade for head instance segmentation and ArcFace-trained feature-extractor achieved an accuracy of 86.8%."
Second-Order Graph ODEs for Multi-Agent Trajectory Forecasting,"Song Wen, Hao Wang, Di Liu, Qilong Zhangli, Dimitris Metaxas",Rutgers University,100.0,USA,0.0,,"Trajectory forecasting of multiple agents is a fundamental task that has applications in various fields, such as autonomous driving, physical system modeling and smart cities. It is challenging because agent interactions and underlying continuous dynamics jointly affect its behavior. Existing approaches often rely on Graph Neural Networks (GNNs) or Transformers to extract agent interaction features. However, they tend to neglect how the distance and velocity information between agents impact their interactions dynamically. Moreover, previous methods use RNNs or first-order Ordinary Differential Equations (ODEs) to model temporal dynamics, which may lack interpretability with respect to how each agent is driven by interactions. To address these challenges, this paper proposes the Agent Graph ODE, a novel approach that models agent interactions and continuous second-order dynamics explicitly. Our method utilizes a variational autoencoder architecture, incorporating spatial-temporal Transformers with distance information and dynamic interaction graph construction in the encoder module. In the decoder module, we employ GNNs with distance information to model agent interactions, and use coupled second-order ODEs to capture the underlying continuous dynamics by modeling the relationship between acceleration and agent interactions. Experimental results show that our proposed Agent Graph ODE outperforms state-of-the-art methods in prediction accuracy. Moreover, our method performs well in sudden situations not seen in the training dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wen_Second-Order_Graph_ODEs_for_Multi-Agent_Trajectory_Forecasting_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484287/,"['Training', 'Dynamics', 'Systems modeling', 'Transformers', 'Feature extraction', 'Mathematical models', 'Trajectory']","['Ordinary Differential Equations', 'Second-order Ordinary Differential Equation', 'Trajectory Forecasting', 'Temporal Dynamics', 'Multiple Agents', 'Variational Autoencoder', 'Graph Neural Networks', 'Continuum Mechanics', 'Distance Information', 'First-order Differential Equations', 'Agent Interactions', 'Interaction Graph', 'Velocity Information', 'Dynamic Graph', 'Encoder Module', 'Decoder Module', 'Second-order Dynamics', 'Loss Function', 'Training Data', 'Conditional Variational Autoencoder', 'Latent Space', 'Future Trajectories', 'Newton’s Second Law', 'Evidence Lower Bound', 'Latent Vector', 'Latent Trajectory', 'Encoder Output', 'Position Of Agent', 'Generative Adversarial Networks']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Video recognition and understanding']",1,"Trajectory forecasting of multiple agents is a fundamental task that has applications in various fields, such as autonomous driving, physical system modeling and smart cities. It is challenging because agent interactions and underlying continuous dynamics jointly affect its behavior. Existing approaches often rely on Graph Neural Networks (GNNs) or Transformers to extract agent interaction features. However, they tend to neglect how the distance and velocity information between agents impact their interactions dynamically. Moreover, previous methods use RNNs or first-order Ordinary Differential Equations (ODEs) to model temporal dynamics, which may lack interpretability with respect to how each agent is driven by interactions. To address these challenges, this paper proposes the Agent Graph ODE, a novel approach that models agent interactions and continuous second-order dynamics explicitly. Our method utilizes a variational autoencoder architecture, incorporating spatial-temporal Transformers with distance information and dynamic interaction graph construction in the encoder module. In the decoder module, we employ GNNs with distance information to model agent interactions, and use coupled second-order ODEs to capture the underlying continuous dynamics by modeling the relationship between acceleration and agent interactions. Experimental results show that our proposed Agent Graph ODE outperforms state-of-the-art methods in prediction accuracy. Moreover, our method performs well in sudden situations not seen in the training dataset."
Seeing Stars: Learned Star Localization for Narrow-Field Astrometry,"Violet Felt, Justin Fletcher","United States Space Force; United States Space Force, 550 Lipoa Parkway, Kihei, HI",0.0,,100.0,USA,"Star localization in astronomical imagery is a computer vision task that underpins satellite tracking. Astronomical star extraction techniques often struggle to detect stars when applied to satellite tracking imagery due to the narrower fields of view and rate track observational modes of satellite tracking telescopes. We present a large dataset of real narrow-field rate-tracked imagery with ground truth stars, created using a combination of existing star detection techniques, an astrometric engine, and a star catalog. We train three state of the art object detection, instance segmentation, and line segment detection models on this dataset and evaluate them with object-wise, pixel-wise, and astrometric metrics. Our proposed approaches require no metadata; when paired with a lost-in-space astrometric engine, they find astrometric fits based solely on uncorrected image pixels. Experimental results on real data indicate the effectiveness of learned star detection: we report astrometric fit rates over double that of classical star detection algorithms, improved dim star recall, and comparable star localization residuals.",https://openaccess.thecvf.com/content/WACV2024/html/Felt_Seeing_Stars_Learned_Star_Localization_for_Narrow-Field_Astrometry_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Felt_Seeing_Stars_Learned_Star_Localization_for_Narrow-Field_Astrometry_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483914/,"['Location awareness', 'Computer vision', 'Satellites', 'Pipelines', 'Stars', 'Object detection', 'Telescopes']","['Field Of View', 'Imagery', 'Computer Vision', 'Object Detection', 'Detection Model', 'Line Segment', 'Instance Segmentation', 'Segmentation Detection', 'Object Detection Model', 'Narrow Field', 'Satellite Tracking', 'Deep Learning', 'Deep Learning Models', 'Bounding Box', 'Point Source', 'Semantic Segmentation', 'Object Position', 'Faster R-CNN', 'Recursive Algorithm', 'Pixel Spacing', 'Number Of Stars', 'Low Earth Orbit', 'Instantaneous Field Of View', 'Image Metadata', 'Flat Field', 'World Space', 'World Coordinate System', 'Angle Information', 'Angular Distance', 'Center Of The Star']","['Applications', 'Remote Sensing', 'Algorithms', 'Datasets and evaluations']",,"Star localization in astronomical imagery is a computer vision task that underpins satellite tracking. Astronomical star extraction techniques often struggle to detect stars when applied to satellite tracking imagery due to the narrower fields of view and rate track observational modes of satellite tracking telescopes. We present a large dataset of real narrow-field rate-tracked imagery with ground truth stars, created using a combination of existing star detection techniques, an astrometric engine, and a star catalog. We train three state of the art object detection, instance segmentation, and line segment detection models on this dataset and evaluate them with object-wise, pixel-wise, and astrometric metrics. Our proposed approaches require no metadata; when paired with a lost-in-space astrometric engine, they find astrometric fits based solely on uncorrected image pixels. Experimental results on real data indicate the effectiveness of learned star detection: we report astrometric fit rates over double that of classical star detection algorithms, improved dim star recall, and comparable star localization residuals."
"Segment Anything, From Space?","Simiao Ren, Francesco Luzi, Saad Lahrichi, Kaleb Kassaw, Leslie M. Collins, Kyle Bradbury, Jordan M. Malof","Computer Science, University of Montana; Division of Natural and Applied Sciences, Duke Kunshan University; Electrical and Computer Engineering, Duke University; Electrical and Computer Engineering, Duke University; Nicholas Institute for Energy, Environment & Sustainability, Duke University",100.0,"China, USA",0.0,,"Recently, the first foundation model developed specifically for image segmentation tasks was developed, termed the ""Segment Anything Model"" (SAM). SAM can segment objects in input imagery based on cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's performance extends to overhead imagery problems and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases due to the unique characteristics of overhead imagery and its common target objects. We report on these unique systematic failure cases for remote sensing imagery that may comprise useful future research for the community.",https://openaccess.thecvf.com/content/WACV2024/html/Ren_Segment_Anything_From_Space_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ren_Segment_Anything_From_Space_WACV_2024_paper.pdf,,,2304.13000,main,Poster,https://ieeexplore.ieee.org/document/10483606/,"['Instance segmentation', 'Image segmentation', 'Systematics', 'Annotations', 'Target recognition', 'Roads', 'Benchmark testing']","['Bounding Box', 'Target Object', 'Segmentation Task', 'Visual Model', 'Foundation Model', 'Land Use', 'Image Resolution', 'Central Point', 'Application Areas', 'Benchmark Datasets', 'Single Dataset', 'Target Class', 'Large Margin', 'Random Points', 'Ground Truth Labels', 'Instance Segmentation', 'Road Segments', 'Variable Resolution', 'Object Instances', 'U-Net Model', 'Solar Array']","['Applications', 'Remote Sensing', 'Algorithms', 'Image recognition and understanding']",16,"Recently, the first foundation model developed specifically for image segmentation tasks was developed, termed the ""Segment Anything Model"" (SAM). SAM can segment objects in input imagery based on cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM’s performance extends to overhead imagery problems and help guide the community’s response to its development. We examine SAM’s performance on a set of diverse and widely studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases due to the unique characteristics of overhead imagery and its common target objects. We report on these unique systematic failure cases for remote sensing imagery that may comprise useful future research for the community."
Self-Annotated 3D Geometric Learning for Smeared Points Removal,"Miaowei Wang, Daniel Morris",Michigan State University; University of Edinburgh,100.0,"UK, USA",0.0,,"There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Self-Annotated_3D_Geometric_Learning_for_Smeared_Points_Removal_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Self-Annotated_3D_Geometric_Learning_for_Smeared_Points_Removal_WACV_2024_paper.pdf,,https://github.com/wangmiaowei/wacv2024_smearedremover.git,2311.09029,main,Poster,https://ieeexplore.ieee.org/document/10484448/,"['Training', 'Interpolation', 'Three-dimensional displays', 'Filters', 'Annotations', 'Manuals', 'Detectors']","['Statistical Methods', 'Depth Map', 'Depth Camera', 'Valid Point', 'Foreground Objects', 'Background Objects', 'Pixel Depth', 'Dense Depth', 'Deep Learning', 'Reference Frame', '3D Reconstruction', 'Normal Vector', 'Color Images', 'Point Cloud', 'Confidence Score', 'Semantic Segmentation', 'Depth Images', 'Color Map', '3D Point', 'Consecutive Frames', 'Multipath Interference', 'Valid Pixels', 'Iterative Closest Point', 'Pose Estimation', 'Real Scenes', 'Surface Normals', 'Camera Pose', 'Multiple Frames', 'Segmentation Task', 'Manual Annotation']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding', 'Applications', 'Remote Sensing']",,"There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git."
Self-Sampling Meta SAM: Enhancing Few-Shot Medical Image Segmentation With Meta-Learning,"Tianang Leng, Yiming Zhang, Kun Han, Xiaohui Xie","Huazhong University of Science and Technology, Wuhan, China; University of California, Irvine, Irvine, California, United States; Tokyo Institute of Technology, Tokyo, Japan",100.0,"China, Japan, USA",0.0,,"While the Segment Anything Model (SAM) excels in semantic segmentation for general-purpose images, its performance significantly deteriorates when applied to medical images, primarily attributable to insufficient representation of medical images in its training dataset. Nonetheless, gathering comprehensive datasets and training models that are universally applicable is particularly challenging due to the long-tail problem common in medical images. To address this gap, here we present a Self-Sampling Meta SAM (SSM-SAM) framework for few-shot medical image segmentation. Our innovation lies in the design of three key modules: 1) An online fast gradient descent optimizer, further optimized by a meta-learner, which ensures swift and robust adaptation to new tasks. 2) A Self-Sampling module designed to provide well-aligned visual prompts for improved attention allocation; and 3) A robust attention-based decoder specifically designed for medical few-shot learning to capture relationship between different slices. Extensive experiments on a popular abdominal CT dataset and an MRI dataset demonstrate that the proposed method achieves significant improvements over state-of-the-art methods in few-shot segmentation, with an average improvements of 10.21% and 1.80% in terms of DSC, respectively. In conclusion, we present a novel approach for rapid online adaptation in interactive image segmentation, adapting to a new organ in just 0.83 minutes. Code is available at https://github.com/DragonDescentZerotsu/SSM-SAM",https://openaccess.thecvf.com/content/WACV2024/html/Leng_Self-Sampling_Meta_SAM_Enhancing_Few-Shot_Medical_Image_Segmentation_With_Meta-Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Leng_Self-Sampling_Meta_SAM_Enhancing_Few-Shot_Medical_Image_Segmentation_With_Meta-Learning_WACV_2024_paper.pdf,,https://github.com/DragonDescentZerotsu/SSM-SAM,2308.16466,main,Poster,https://ieeexplore.ieee.org/document/10483717/,"['Training', 'Metalearning', 'Image segmentation', 'Adaptation models', 'Visualization', 'Technological innovation', 'Training data']","['Medical Imaging', 'Image Segmentation', 'Medical Image Segmentation', 'Training Dataset', 'Dice Similarity Coefficient', 'Abdominal Computed Tomography', 'Interactive Segmentation', 'CT Datasets', 'Learning Rate', 'Computer Vision', 'Natural Images', 'Segmentation Model', 'Segmentation Task', 'Left Kidney', 'Self-supervised Learning', 'Query Image', 'Few-shot Learning', 'Foundation Model', 'Transformer Layers', 'Positional Encoding', 'Image Embedding', 'Image Encoder', 'Vision Transformer', 'Online Optimization', 'Unseen Classes']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"While the Segment Anything Model (SAM) excels in semantic segmentation for general-purpose images, its performance significantly deteriorates when applied to medical images, primarily attributable to insufficient representation of medical images in its training dataset. Nonetheless, gathering comprehensive datasets and training models that are universally applicable is particularly challenging due to the long-tail problem common in medical images.To address this gap, here we present a Self-Sampling Meta SAM (SSM-SAM) framework for few-shot medical image segmentation. Our innovation lies in the design of three key modules: 1) An online fast gradient descent optimizer, further optimized by a meta-learner, which ensures swift and robust adaptation to new tasks. 2) A Self-Sampling module designed to provide well-aligned visual prompts for improved attention allocation; and 3) A robust attention-based decoder specifically designed for few-shot medical image segmentation to capture relationship between different slices. Extensive experiments on a popular abdominal CT dataset and an MRI dataset demonstrate that the proposed method achieves significant improvements over state-of-the-art methods in few-shot segmentation, with an average improvements of 10.21% and 1.80% in terms of DSC, respectively. In conclusion, we present a novel approach for rapid online adaptation in interactive image segmentation, adapting to a new organ in just 0.83 minutes. Code is available at https://github.com/DragonDescentZerotsu/SSM-SAM"
Self-Supervised Denoising Transformer With Gaussian Process,"Rajeev Yasarla, Jeya Maria Jose Valanarasu, Vishwanath Sindagi, Vishal M. Patel","Johns Hopkins University, Department of Electrical and Computer Engineering, Baltimore, MD 21218, USA",100.0,USA,0.0,,"Convolutional neural network (CNN) based methods have been the main focus of recent developments for image denoising. However, these methods lack majorly in two ways: 1) They require a large amount of labeled data to perform well. 2) They do not have a good global understanding due to convolutional inductive biases. Recent emergence of Transformers and self-supervised learning methods have focused on tackling these issues. In this work, we address both these issues for image denoising and propose a new method: Self-Supervised denoising Transformer (SST-GP) with Gaussian Process. Our novelties are two fold: First, we propose a new way of doing self-supervision by incorporating Gaussian Processes (GP). Given a noisy image, we generate multiple noisy down-sampled images with random cyclic shifts. Using GP, we formulate a joint Gaussian distribution between these down-sampled images and learn the relation between their corresponding denoising function mappings to predict the pseudo-Ground truth (pseudo-GT) for each of the down-sampled images. This enables the network to learn noise present in the down-sampled images and achieve better denoising performance by using the joint relationship between down-sampled images with help of GP. Second, we propose a new transformer architecture - Denoising Transformer (Den-T) which is tailor-made for denoising application. Den-T has two transformer encoder branches - one which focuses on extracting fine context details and another to extract coarse context details. This helps Den-T to attend to both local and global information to effectively denoise the image. Finally, we train Den-T using the proposed self-supervised strategy using GP and achieve a better performance over recent unsupervised/self-supervised denoising approaches when validated on various denoising datasets like Kodak, BSD, Set-14 and SIDD. Codes will be made public after review.",https://openaccess.thecvf.com/content/WACV2024/html/Yasarla_Self-Supervised_Denoising_Transformer_With_Gaussian_Process_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yasarla_Self-Supervised_Denoising_Transformer_With_Gaussian_Process_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483578/,"['Noise reduction', 'Noise', 'Gaussian processes', 'Self-supervised learning', 'Network architecture', 'Gaussian distribution', 'Transformers']","['Gaussian Process', 'Convolutional Neural Network', 'Fine Details', 'Noisy Images', 'Global Understanding', 'Transformer Architecture', 'Cyclic Shift', 'Self-supervised Learning Methods', 'Joint Relationship', 'Gaussian Noise', 'Image Pairs', 'Joint Distribution', 'Latent Space', 'Network Weights', 'Clear Image', 'Gaussian Mixture Model', 'Noise Model', 'Shot Noise', 'Noise Characteristics', 'Smartphone Camera', 'Intermediate Vector', 'Synthetic Noise', 'Transformer Block', 'Synthetic Test', 'Vision Transformer', 'Image X', 'Depthwise Convolution', 'Computer Vision Applications', 'Real-world Images', 'Large Amount Of Images']","['Algorithms', 'Low-level and physics-based vision']",,"Convolutional neural network (CNN) based methods have been the main focus of recent developments for image denoising. However, these methods lack majorly in two ways: 1) They require a large amount of labeled data to perform well. 2) They do not have a good global understanding due to convolutional inductive biases. Recent emergence of Transformers and self-supervised learning methods have focused on tackling these issues. In this work, we address both these issues for image denoising and propose a new method: Self-Supervised denoising Transformer (SST-GP) with Gaussian Process. Our novelties are two fold: First, we propose a new way of doing self-supervision by incorporating Gaussian Processes (GP). Given a noisy image, we generate multiple noisy down-sampled images with random cyclic shifts. Using GP, we formulate a joint Gaussian distribution between these down-sampled images and learn the relation between their corresponding denoising function mappings to predict the pseudo-Ground truth (pseudo-GT) for each of the down-sampled images. This enables the network to learn noise present in the down-sampled images and achieve better denoising performance by using the joint relationship between down-sampled images with help of GP. Second, we propose a new transformer architecture - Denoising Transformer (Den-T) which is tailor-made for denoising application. Den-T has two transformer encoder branches - one which focuses on extracting fine context details and another to extract coarse context details. This helps Den-T to attend to both local and global information to effectively denoise the image. Finally, we train Den-T using the proposed self-supervised strategy using GP and achieve a better performance over recent unsupervised/self-supervised denoising approaches when validated on various denoising datasets like Kodak, BSD, Set-14 and SIDD."
Self-Supervised Edge Detection Reconstruction for Topology-Informed 3D Axon Segmentation and Centerline Detection,"Alec S. Xu, Nina I. Shamsi, Lars A. Gjesteby, Laura J. Brattain","MIT Lincoln Laboratory, Lexington, MA 02421, USA",100.0,USA,0.0,,"Many machine learning-based axon tracing methods rely on image datasets with segmentation labels. This requires manual annotation from domain experts, which is labor-intensive and not practical for large-scale brain mapping on hemisphere or whole brain tissue at cellular or sub-cellular resolution. Additionally, preserving axon structure topology is crucial to understanding neural connections and brain function. Self-supervised learning (SSL) is a machine learning framework that allows models to learn an auxiliary task on unannotated data to aid performance on a supervised target task. In this work, we propose a novel SSL auxiliary task of reconstructing an edge detector for the target task of topology-oriented axon segmentation and centerline detection. We pretrained 3D U-Nets on three different SSL tasks using a mouse brain dataset: our proposed task, predicting the order of permuted slices, and playing a Rubik's cube. We then evaluated these U-Nets and a baseline model on a different mouse brain dataset. Across all experiments, the U-Net pretrained on our proposed task improved the baseline's segmentation, topology-preservation, and centerline detection by up to 5.03%, 4.65%, and 5.41%, respectively. In contrast, there was no consistent improvement over the baseline observed with the slice-permutation and Rubik's cube pretrained U-Nets.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_Self-Supervised_Edge_Detection_Reconstruction_for_Topology-Informed_3D_Axon_Segmentation_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_Self-Supervised_Edge_Detection_Reconstruction_for_Topology-Informed_3D_Axon_Segmentation_and_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484392/,"['Training', 'Axons', 'Solid modeling', 'Three-dimensional displays', 'Image edge detection', 'Brain modeling', 'Data models']","['Edge Detection', 'Segmentation Detection', 'Axonal Segments', '3D Centerline', 'Centerline Detection', 'Detection Task', 'Domain Experts', 'Segmentation Task', 'Self-supervised Learning', 'Target Task', 'Auxiliary Task', '3D U-Net', 'Hyperparameters', 'Convolutional Layers', '3D Images', 'Input Image', 'Raw Images', 'Pallidum', 'Binary Image', 'Input Size', 'Dice Score', 'Canny Edge Detection', 'L2 Loss', 'Auxiliary Classifier', 'Background Artifacts', 'Segmentation Prediction', 'Decoder Output', 'Linear Layer', 'Edge Detection Method', 'Training Tasks']","['Applications', 'Biomedical / healthcare / medicine']",,"Many machine learning-based axon tracing methods rely on image datasets with segmentation labels. This requires manual annotation from domain experts, which is labor-intensive and not practical for large-scale brain mapping on hemisphere or whole brain tissue at cellular or sub-cellular resolution. Additionally, preserving axon structure topology is crucial to understanding neural connections and brain function. Self-supervised learning (SSL) is a machine learning framework that allows models to learn an auxiliary task on unannotated data to aid performance on a supervised target task. In this work, we propose a novel SSL auxiliary task of reconstructing an edge detector for the target task of topology-oriented axon segmentation and centerline detection. We pretrained 3D U-Nets on three different SSL tasks using a mouse brain dataset: our proposed task, predicting the order of permuted slices, and playing a Rubik’s cube. We then evaluated these U-Nets and a baseline model on a different mouse brain dataset. Across all experiments, the U-Net pretrained on our proposed task improved the baseline’s segmentation, topology-preservation, and centerline detection by up to 5.03%, 4.65%, and 5.41%, respectively. In contrast, there was no consistent improvement over the baseline observed with the slice-permutation and Rubik’s cube pretrained U-Nets."
Self-Supervised Learning With Masked Autoencoders for Teeth Segmentation From Intra-Oral 3D Scans,"Amani Almalki, Longin Jan Latecki","Department of Computer and Information Sciences, Temple University, Philadelphia, USA",100.0,USA,0.0,,"In modern dentistry, teeth localization, segmentation, and labeling from intra-oral 3D scans are crucial for improving dental diagnostics, treatment planning, and population-based studies on oral health. However, creating automated algorithms for teeth analysis is a challenging task due to the limited availability of accessible data for training, particularly from the point of view of deep learning. This study extends the self-supervised learning framework of the mesh masked autoencoder (MeshMAE) transformer. While the MeshMAE loss measures the quality of reconstructed masked mesh triangles, the loss of the proposed DentalMAE evaluates the predicted deep embeddings of masked mesh triangles. This yields a better generalization ability on a very limited number of 3D dental scans, as documented by our results on teeth segmentation of intra-oral scans. Our results show that masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on 3D intra-oral scans, increasing the overall accuracy over both MeshMAE and prior self-supervised pre-training.",https://openaccess.thecvf.com/content/WACV2024/html/Almalki_Self-Supervised_Learning_With_Masked_Autoencoders_for_Teeth_Segmentation_From_Intra-Oral_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Almalki_Self-Supervised_Learning_With_Masked_Autoencoders_for_Teeth_Segmentation_From_Intra-Oral_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484240/,"['Training', 'Three-dimensional displays', 'Transfer learning', 'Teeth', 'Self-supervised learning', 'Transformers', 'Loss measurement']","['3D Scanning', 'Self-supervised Learning', 'Tooth Segmentation', 'Training Data', 'Deep Learning', 'Oral Health', 'Intraoral Scanners', 'Convolutional Network', 'Convolutional Neural Network', 'Positive Predictive Value', 'Point Cloud', 'Segmentation Task', 'Transformer Model', '3D Coordinates', '3D Mesh', 'Time-consuming Task', 'Encoder Network', 'Tooth Structure', 'Central Incisors', 'Position Embedding', 'Vision Transformer', 'Dice Score', 'Architecture For Segmentation', 'Pre-training Tasks', 'Masking Strategy', 'Non-overlapping Patches', 'Dental Arch', 'Neural Network', 'Relational Coordination', 'Decoder Network']","['Applications', 'Biomedical / healthcare / medicine']",4,"In modern dentistry, teeth localization, segmentation, and labeling from intra-oral 3D scans are crucial for improving dental diagnostics, treatment planning, and population-based studies on oral health. However, creating automated algorithms for teeth analysis is a challenging task due to the limited availability of accessible data for training, particularly from the point of view of deep learning. This study extends the self-supervised learning framework of the mesh masked autoencoder (MeshMAE) transformer. While the MeshMAE loss measures the quality of reconstructed masked mesh triangles, the loss of the proposed DentalMAE evaluates the predicted deep embeddings of masked mesh triangles. This yields a better generalization ability on a very limited number of 3D dental scans, as documented by our results on teeth segmentation of intra-oral scans. Our results show that masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on 3D intra-oral scans, increasing the overall accuracy over both MeshMAE and prior self-supervised pre-training."
Self-Supervised Learning for Place Representation Generalization Across Appearance Changes,"Mohamed Adel Musallam, Vincent Gaudillière, Djamila Aouada","SnT, University of Luxembourg",100.0,Luxembourg,0.0,,"Visual place recognition is a key to unlocking spatial navigation for animals, humans and robots. While state-of-the-art approaches are trained in a supervised manner and, therefore, hardly capture the information needed for generalizing to unusual conditions. We argue that self-supervised learning may help abstracting the place representation so that it can be foreseen, irrespective of the conditions. More precisely, in this paper, we investigate learning features that are robust to appearance modifications while sensitive to geometric transformations in a self-supervised manner. This dual-purpose training is made possible by combining the two self-supervision main paradigms, i.e. contrastive and predictive learning. Our results on standard benchmarks reveal that jointly learning such appearance-robust and geometry-sensitive image descriptors leads to competitive visual place recognition results across adverse seasonal and illumination conditions without requiring any humanannotated labels.",https://openaccess.thecvf.com/content/WACV2024/html/Musallam_Self-Supervised_Learning_for_Place_Representation_Generalization_Across_Appearance_Changes_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Musallam_Self-Supervised_Learning_for_Place_Representation_Generalization_Across_Appearance_Changes_WACV_2024_paper.pdf,https://www.lmo.space,,,main,Poster,https://ieeexplore.ieee.org/document/10484309/,"['Training', 'Visualization', 'Solid modeling', 'Three-dimensional displays', 'Sensitivity', 'Navigation', 'Lighting']","['Changes In Appearance', 'Self-supervised Learning', 'Visual Recognition', 'Seasonal Conditions', 'Image Descriptors', 'Geometric Transformation', 'Self-supervised Manner', 'Place Recognition', 'Convolutional Neural Network', 'Rotation Angle', 'Spatial Arrangement', 'Representation Of Space', 'Reference Image', 'Dot Product', 'Equivalency', 'Image Representation', 'Image Retrieval', 'Prediction Loss', 'Contrastive Loss', 'Rich Representation', 'Group Of Transformations', 'Query Image', 'Encoding Model', 'Image Retrieval Task', 'Representations Of Place', 'Tolerance Window', 'Augmented Version', 'Viewpoint Changes', 'Spatial Layout', 'Multilayer Perceptron']","['Applications', 'Autonomous Driving', 'Algorithms', 'Image recognition and understanding']",,"Visual place recognition is a key to unlocking spatial navigation for animals, humans and robots. While state-of-the-art approaches are trained in a supervised manner and therefore hardly capture the information needed for generalizing to unusual conditions, we argue that self-supervised learning may help abstracting the place representation so that it can be foreseen, irrespective of the conditions. More precisely, in this paper, we investigate learning features that are robust to appearance modifications while sensitive to geometric transformations in a self-supervised manner. This dual-purpose training is made possible by combining the two self-supervision main paradigms, i.e. contrastive and predictive learning. Our results on standard benchmarks reveal that jointly learning such appearance-robust and geometry-sensitive image descriptors leads to competitive visual place recognition results across adverse seasonal and illumination conditions, without requiring any humanannotated labels.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Self-Supervised Learning for Visual Relationship Detection Through Masked Bounding Box Reconstruction,"Zacharias Anastasakis, Dimitrios Mallis, Markos Diomataris, George Alexandridis, Stefanos Kollias, Vassilis Pitsikalis","ETH, Zürich; National Technical University of Athens; Deeplab, Athens; SnT, University of Luxembourg",75.0,"Greece, Luxembourg, Switzerland",25.0,Greece,"We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. We make our code available at https://github.com/deeplab-ai/SelfSupervisedVRD.",https://openaccess.thecvf.com/content/WACV2024/html/Anastasakis_Self-Supervised_Learning_for_Visual_Relationship_Detection_Through_Masked_Bounding_Box_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Anastasakis_Self-Supervised_Learning_for_Visual_Relationship_Detection_Through_Masked_Bounding_Box_WACV_2024_paper.pdf,,https://github.com/deeplab-ai/SelfSupervisedVRD,2311.04834,main,Poster,https://ieeexplore.ieee.org/document/10484077/,"['Representation learning', 'Visualization', 'Computer vision', 'Codes', 'Self-supervised learning', 'Predictive models', 'Task analysis']","['Bounding Box', 'Visual Detection', 'Self-supervised Learning', 'Visual Relationship', 'Visual Representation', 'Representation Learning', 'Input Image', 'Spatial Features', 'Visual Features', 'Multilayer Perceptron', 'Large Ratio', 'Manual Annotation', 'Linguistic Features', 'Graph Convolutional Network', 'Objects In The Scene', 'Mean Square Error Loss', 'Long-tailed Distribution', 'Transformer Encoder', 'Object Reconstruction', 'Extract Visual Features', 'Few-shot Classification', 'Pretext Task', 'Self-supervised Manner', 'Self-supervised Task', 'Self-supervised Representation', 'Pre-training Tasks', 'Training Images']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. We make our code available at https://github.com/deeplabai/SelfSupervisedVRD."
Self-Supervised Learning of Semantic Correspondence Using Web Videos,"Donghyeon Kwon, Minsu Cho, Suha Kwak","Dept. of CSE, POSTECH; Graduate School of AI, POSTECH",100.0,South Korea,0.0,,"Existing datasets for semantic correspondence are often limited in terms of both the amount of labeled data and diversity of labeled keypoints due to the tremendous cost of manual correspondence labeling. To address this issue, we propose the first self-supervised learning framework that utilizes a large amount of web videos collected and annotated fully automatically. Our main motivation is that smooth changes between consecutive video frames allow to build accurate space-time correspondences with no human intervention. Hence, we establish space-time correspondences within each web video and leverage them for deriving pseudo correspondence labels between two distant frames of the video. In addition, we present a dedicated training strategy that facilitates stable training using web videos with such pseudo labels. Our experiments on public benchmarks demonstrated that the proposed method surpasses existing self-supervised learning models and that our self-supervised learning as pretraining for supervised learning improves performance substantially. Our codebase for web video crawling and pseudo label generation will be released public to promote future research.",https://openaccess.thecvf.com/content/WACV2024/html/Kwon_Self-Supervised_Learning_of_Semantic_Correspondence_Using_Web_Videos_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kwon_Self-Supervised_Learning_of_Semantic_Correspondence_Using_Web_Videos_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484187/,"['Training', 'Computational modeling', 'Semantics', 'Supervised learning', 'Self-supervised learning', 'Manuals', 'Benchmark testing']","['Self-supervised Learning', 'Semantic Correspondence', 'Web Videos', 'Supervised Learning', 'Video Frames', 'Consecutive Frames', 'Pseudo Labels', 'Smooth Changes', 'Consecutive Video Frames', 'Deep Neural Network', 'Transfer Learning', 'Class Labels', 'Image Pairs', 'Manual Annotation', 'Search Queries', 'Domain Adaptation', 'Semantic Model', 'Dataset Construction', 'Abrupt Transition', 'Affinity Matrix', 'Common Datasets', 'Isolation Forest', 'Pair Of Frames', 'Pixel In Frame', 'Video Retrieval', 'Viewpoint Changes', 'Domain Gap', 'Task Dataset', 'Intra-class Variance', 'PASCAL VOC Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Existing datasets for semantic correspondence are often limited in terms of both the amount of labeled data and diversity of labeled keypoints due to the tremendous cost of manual correspondence labeling. To address this issue, we propose the first self-supervised learning framework that utilizes a large amount of web videos collected and annotated fully automatically. Our main motivation is that smooth changes between consecutive video frames allow to build accurate space-time correspondences with no human intervention. Hence, we establish space-time correspondences within each web video and leverage them for deriving pseudo correspondence labels between two distant frames of the video. In addition, we present a dedicated training strategy that facilitates stable training using web videos with such pseudo labels. Our experiments on public benchmarks demonstrated that the proposed method surpasses existing self-supervised learning models and that our self-supervised learning as pretraining for supervised learning improves performance substantially. Our codebase for web video crawling and pseudo label generation will be released public to promote future research."
Self-Supervised Relation Alignment for Scene Graph Generation,"Bicheng Xu, Renjie Liao, Leonid Sigal","University of British Columbia, Vector Institute for AI, Canada CIFAR AI Chair",100.0,Canada,0.0,,"The goal of scene graph generation is to predict a graph from an input image, where nodes correspond to identified and localized objects and edges to their corresponding interaction predicates. Existing methods are trained in a fully supervised manner and focus on message passing mechanisms, loss functions, and/or bias mitigation. In this work we introduce a simple-yet-effective self-supervised relational alignment regularization designed to improve the scene graph generation performance. The proposed alignment is general and can be combined with any existing scene graph generation framework, where it is trained alongside the original model's objective. The alignment is achieved through distillation, where an auxiliary relation prediction branch, that mirrors and shares parameters with the supervised counterpart, is designed. In the auxiliary branch, relational input features are partially masked prior to message passing and predicate prediction. The predictions for masked relations are then aligned with the supervised counterparts after the message passing. We illustrate the effectiveness of this self-supervised relational alignment in conjunction with two scene graph generation architectures, SGTR and Neural Motifs, and show that in both cases we achieve significantly improved performance.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_Self-Supervised_Relation_Alignment_for_Scene_Graph_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_Self-Supervised_Relation_Alignment_for_Scene_Graph_Generation_WACV_2024_paper.pdf,,,2302.01403,main,Poster,https://ieeexplore.ieee.org/document/10483659/,"['Visualization', 'Computer vision', 'Message passing', 'Image edge detection', 'Genomics', 'Predictive models', 'Mirrors']","['Scene Graph', 'Input Image', 'Related Features', 'Image Features', 'Object Detection', 'Bounding Box', 'Sparse Data', 'Object Features', 'Ground Truth Labels', 'Two-stage Model', 'Two-stage Approach', 'Object Relations', 'Graph Neural Networks', 'Self-supervised Learning', 'Faster R-CNN', 'Label Distribution', 'Original Prediction', 'Object Proposals', 'Feature Refinement', 'Alignment Loss', 'One-stage Model', 'Bounding Box Location', 'Transformer Decoder', 'Row Of Table', 'Prediction Head', 'Attention Matrix', 'Learning Rate', 'Denoising Autoencoder', 'Potential Negative Impact', 'Neural Model']","['Algorithms', 'Image recognition and understanding']",,"The goal of scene graph generation is to predict a graph from an input image, where nodes correspond to identified and localized objects and edges to their corresponding interaction predicates. Existing methods are trained in a fully supervised manner and focus on message passing mechanisms, loss functions, and/or bias mitigation. In this work we introduce a simple-yet-effective self-supervised relational alignment regularization designed to improve the scene graph generation performance. The proposed alignment is general and can be combined with any existing scene graph generation framework, where it is trained alongside the original model’s objective. The alignment is achieved through distillation, where an auxiliary relation prediction branch, that mirrors and shares parameters with the supervised counterpart, is designed. In the auxiliary branch, relational input features are partially masked prior to message passing and predicate prediction. The predictions for masked relations are then aligned with the supervised counterparts after the message passing. We illustrate the effectiveness of this self-supervised relational alignment in conjunction with two scene graph generation architectures, SGTR [25] and Neural Motifs [53], and show that in both cases we achieve significantly improved performance."
Self-Supervised Representation Learning With Cross-Context Learning Between Global and Hypercolumn Features,"Zheng Gao, Chen Feng, Ioannis Patras","Queen Mary University of London, Mile End Road, London, E1 4NS",100.0,UK,0.0,,"Whilst contrastive learning yields powerful representations by matching different augmented views of the same instance, it lacks the ability to capture the similarities between different instances. One popular way to address this limitation is by learning global features (after the global pooling) to capture inter-instance relationships based on knowledge distillation, where the global features of the teacher are used to guide the learning of the global features of the student. Inspired by cross-modality learning, we extend this existing framework that only learns from global features by encouraging the global features and intermediate layer features to learn from each other. This leads to our novel self-supervised framework: cross-context learning between global and hypercolumn features (CGH), that enforces the consistency of instance relations between low- and high-level semantics. Specifically, we stack the intermediate feature maps to construct a ""hypercolumn"" representation so that we can measure instance relations using two contexts (hypercolumn and global feature) separately, and then use the relations of one context to guide the learning of the other. This cross-context learning allows the model to learn from the differences between the two contexts. The experimental results on linear classification and downstream tasks show that our method outperforms the state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Gao_Self-Supervised_Representation_Learning_With_Cross-Context_Learning_Between_Global_and_Hypercolumn_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gao_Self-Supervised_Representation_Learning_With_Cross-Context_Learning_Between_Global_and_Hypercolumn_WACV_2024_paper.pdf,,,2308.13392,main,Poster,https://ieeexplore.ieee.org/document/10484066/,"['Representation learning', 'Computer vision', 'Computational modeling', 'Semantics', 'Self-supervised learning', 'Task analysis', 'Context modeling']","['Global Features', 'Representation Learning', 'Self-supervised Learning', 'Self-supervised Representation Learning', 'Hypercolumn Features', 'Semantic', 'Feature Maps', 'Intermediate Layer', 'Global Pooling', 'Linear Classifier', 'Intermediate Features', 'Intermediate Feature Maps', 'Learning Rate', 'True Positive', 'K-nearest Neighbor', 'Object Detection', 'Precision And Recall', 'Weight Decay', 'Stochastic Gradient Descent', 'Memory Bank', 'Global Context', 'Global Pooling Layer', 'Global Average Pooling', 'Pseudo Labels', 'Feature Extraction Backbone', 'Latent Space', 'Convolutional Block', 'Projector', 'Softmax Operation']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",3,"Whilst contrastive learning yields powerful representations by matching different augmented views of the same instance, it lacks the ability to capture the similarities between different instances. One popular way to address this limitation is by learning global features (after the global pooling) to capture inter-instance relationships based on knowledge distillation, where the global features of the teacher are used to guide the learning of the global features of the student. Inspired by cross-modality learning, we extend this existing framework that only learns from global features by encouraging the global features and intermediate layer features to learn from each other. This leads to our novel self-supervised framework: cross-context learning between global and hypercolumn features (CGH), that enforces the consistency of instance relations between low-and high-level semantics. Specifically, we stack the intermediate feature maps to construct a ""hypercolumn"" representation so that we can measure instance relations using two contexts (hypercolumn and global feature) separately, and then use the relations of one context to guide the learning of the other. This cross-context learning allows the model to learn from the differences between the two contexts. The experimental results on linear classification and downstream tasks show that our method outperforms the state-of-the-art methods."
SemST: Semantically Consistent Multi-Scale Image Translation via Structure-Texture Alignment,"Ganning Zhao, Wenhui Cui, Suya You, C.-C. Jay Kuo","DEVCOM Army Research Laboratory, Los Angeles, California, USA; University of Southern California, Los Angeles, California, USA",50.0,USA,50.0,USA,"Unsupervised image-to-image translation learns cross-domain image mapping that transfers input from the source domain to output in the target domain while preserving its semantics. One challenge is that different semantic statistics in source and target domains result in content discrepancy known as semantic distortion. To address this problem, a novel I2I method that maintains semantic consistency in translation is proposed and named SemST in this work. SemST reduces semantic distortion by employing contrastive learning and aligning the structural and textural properties of input and output by maximizing their mutual information. Furthermore, a multi-scale approach is introduced to enhance translation performance, thereby enabling the applicability of SemST to domain adaptation in high-resolution images. Experiments show that SemST effectively mitigates semantic distortion and achieves state-of-the-art performance. Also, the application of SemST to domain adaptation is explored. It is demonstrated by preliminary experiments that SemST can be utilized as a beneficial pre-training for the semantic segmentation task.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_SemST_Semantically_Consistent_Multi-Scale_Image_Translation_via_Structure-Texture_Alignment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_SemST_Semantically_Consistent_Multi-Scale_Image_Translation_via_Structure-Texture_Alignment_WACV_2024_paper.pdf,,,2310.04995,main,Poster,https://ieeexplore.ieee.org/document/10484478/,"['Computer vision', 'Semantic segmentation', 'Semantics', 'Self-supervised learning', 'Distortion', 'Task analysis', 'Mutual information']","['Semantic Consistency', 'Structural Properties', 'High-resolution Images', 'Mutual Information', 'Semantic Segmentation', 'Target Domain', 'Domain Adaptation', 'Source Domain', 'Self-supervised Learning', 'Semantic Segmentation Task', 'Positive Samples', 'Contextual Information', 'Input Image', 'Negative Samples', 'Latent Space', 'Localization Prediction', 'Output Image', 'Synthetic Images', 'Global Crop', 'Random Cropping', 'Local Crop', 'Unsupervised Domain Adaptation Methods', 'Pixel Accuracy', 'Query Sample', 'Cycle Consistency Loss', 'Multiscale Framework', 'Domain Gap', 'Global Prediction', 'Translation Task', 'Downstream Segments']","['Applications', 'Arts / games / social media', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Autonomous Driving']",1,"Unsupervised image-to-image translation learns cross-domain image mapping that transfers input from the source domain to output in the target domain while preserving its semantics. One challenge is that different semantic statistics in source and target domains result in content discrepancy known as semantic distortion. To address this problem, a novel I2I method that maintains semantic consistency in translation is proposed and named SemST in this work. SemST reduces semantic distortion by employing contrastive learning and aligning the structural and textural properties of input and output by maximizing their mutual information. Furthermore, a multi-scale approach is introduced to enhance translation performance, thereby enabling the applicability of SemST to domain adaptation in high-resolution images. Experiments show that SemST effectively mitigates semantic distortion and achieves state-of-the-art performance. Also, the application of SemST to domain adaptation is explored. It is demonstrated by preliminary experiments that SemST can be utilized as a beneficial pre-training for the semantic segmentation task."
Semantic Fusion Augmentation and Semantic Boundary Detection: A Novel Approach to Multi-Target Video Moment Retrieval,"Cheng Huang, Yi-Lun Wu, Hong-Han Shuai, Ching-Chun Huang","National Yang Ming Chiao Tung University, Taiwan",100.0,Taiwan,0.0,,"Given an untrimmed video and a natural language query, video moment retrieval (VMR) aims to retrieve video moments described by the query. However, most existing VMR methods assume a one-to-one mapping between the input query and the target video moment (single-target VMR), disregarding the possibility that a video may contain multiple target moments that match the query description (multi-target VMR). Previous methods tackle multi-target VMR by incorporating false negative moments with the original target moment for multi-target training. However, existing methods cannot properly work when no false negative moments exist in the video, or when the identified false negative moments are noisy but are still being utilized as pseudo-labels. In this paper, we propose to tackle multi-target VMR by Semantic Fusion Augmentation and Semantic Boundary Detection (SFABD). Specifically, we use feature-level augmentation to generate augmented target moments, along with an intra-video contrastive loss to ensure feature consistency. Meanwhile, we perform semantic boundary detection to adaptively remove all false negatives from the negative set of contrastive loss to avoid semantic confusion. Extensive experiments conducted on Charades-STA, ActivityNet Captions, and QVHighlights show that our method achieves state-of-the-art performance on multi-target metrics and single-target metrics. The source code is available at https://github.com/basiclab/SFABD.",https://openaccess.thecvf.com/content/WACV2024/html/Huang_Semantic_Fusion_Augmentation_and_Semantic_Boundary_Detection_A_Novel_Approach_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Huang_Semantic_Fusion_Augmentation_and_Semantic_Boundary_Detection_A_Novel_Approach_WACV_2024_paper.pdf,,https://github.com/basiclab/SFABD,,main,Poster,https://ieeexplore.ieee.org/document/10484101/,"['Training', 'Computer vision', 'Source coding', 'Semantics', 'Natural languages', 'Noise measurement']","['Semantic Boundaries', 'Video Moment', 'Semantic Augmentation', 'Video Moment Retrieval', 'False Negative', 'Contrastive Loss', 'Set Loss', 'Positive Samples', 'Negative Samples', 'Similarity Score', 'Intersection Over Union', 'Latent Space', 'Learning Progress', 'Adaptive Threshold', 'Dimensional Feature Space', 'Maximum Similarity', 'Video Features', 'False Negative Samples', 'Video Encoding', 'Query Features', 'Semantic Clustering', 'Intersection Over Union Threshold', 'Pre-trained Encoder', 'Number Of Moments']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Given an untrimmed video and a natural language query, video moment retrieval (VMR) aims to retrieve video moments described by the query. However, most existing VMR methods assume a one-to-one mapping between the input query and the target video moment (single-target VMR), disregarding the possibility that a video may contain multiple target moments that match the query description (multi-target VMR). Previous methods tackle multi-target VMR by incorporating false negative moments with the original target moment for multi-target training. However, existing methods cannot properly work when no false negative moments exist in the video, or when the identified false negative moments are noisy but are still being utilized as pseudo-labels. In this paper, we propose to tackle multi-target VMR by Semantic Fusion Augmentation and Semantic Boundary Detection (SFABD). Specifically, we use feature-level augmentation to generate augmented target moments, along with an intra-video contrastive loss to ensure feature consistency. Meanwhile, we perform semantic boundary detection to adaptively remove all false negatives from the negative set of contrastive loss to avoid semantic confusion. Extensive experiments conducted on Charades-STA, ActivityNet Captions, and QVHighlights show that our method achieves state-of-the-art performance on multi-target metrics and single-target metrics. The source code is available at https://github.com/basiclab/SFABD."
Semantic Generative Augmentations for Few-Shot Counting,"Perla Doubinsky, Nicolas Audebert, Michel Crucianu, Hervé Le Borgne","CEDRIC (EA4329), Cnam Paris, France; Université Paris-Saclay, CEA List, Palaiseau, France",100.0,France,0.0,,"With the availability of powerful text-to-image diffusion models, recent works have explored the use of synthetic data to improve image classification performances. These works show that it can effectively augment or even replace real data. In this work, we investigate how synthetic data can benefit few-shot class-agnostic counting. This requires to generate images that correspond to a given input number of objects. However, text-to-image models struggle to grasp the notion of count. We propose to rely on a double conditioning of Stable Diffusion with both a prompt and a density map in order to augment a training dataset for few-shot counting. Due to the small dataset size, the fine-tuned model tends to generate images close to the training images. We propose to enhance the diversity of synthesized images by exchanging captions between images thus creating unseen configurations of object types and spatial layout. Our experiments show that our diversified generation strategy significantly improves the counting accuracy of two recent and performing few-shot counting models on FSC147 and CARPK.",https://openaccess.thecvf.com/content/WACV2024/html/Doubinsky_Semantic_Generative_Augmentations_for_Few-Shot_Counting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Doubinsky_Semantic_Generative_Augmentations_for_Few-Shot_Counting_WACV_2024_paper.pdf,,,2311.16122,main,Poster,https://ieeexplore.ieee.org/document/10483667/,"['Training', 'Measurement', 'Adaptation models', 'Semantic segmentation', 'Semantics', 'Object detection', 'Data models']","['Image Classification', 'Density Map', 'Training Images', 'Diffusion Model', 'Number Of Objects', 'Spatial Layout', 'Counting Accuracy', 'Root Mean Square Error', 'Training Set', 'Validation Set', 'Data Augmentation', 'Bounding Box', 'Latent Space', 'New Combinations', 'Additional Input', 'Feature Matching', 'Unseen Data', 'Synthetic Images', 'Augmentation Strategy', 'Semantic Map', 'Query Image', 'Count Datasets', 'Augmented Number', 'Text Encoder', 'Color Jittering', 'Object Counting', 'L2 Loss', 'Open Set', 'Spatial Configuration']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Image recognition and understanding']",,"With the availability of powerful text-to-image diffusion models, recent works have explored the use of synthetic data to improve image classification performances. These works show that it can effectively augment or even replace real data. In this work, we investigate how synthetic data can benefit few-shot class-agnostic counting. This requires to generate images that correspond to a given input number of objects. However, text-to-image models struggle to grasp the notion of count. We propose to rely on a double conditioning of Stable Diffusion with both a prompt and a density map in order to augment a training dataset for few-shot counting. Due to the small dataset size, the fine-tuned model tends to generate images close to the training images. We propose to enhance the diversity of synthesized images by exchanging captions between images thus creating unseen configurations of object types and spatial layout. Our experiments show that our diversified generation strategy significantly improves the counting accuracy of two recent and performing few-shot counting models on FSC147 and CARPK."
Semantic Labels-Aware Transformer Model for Searching Over a Large Collection of Lecture-Slides,"K. V. Jobin, Anand Mishra, C. V. Jawahar",IIT Jodhpur; IIIT Hyderabad,100.0,India,0.0,,"Massive Open Online Courses (MOOCs) enable easy access to many educational materials, particularly lecture slides, on the web. Searching through them based on user queries becomes an essential problem due to the availability of such vast information. To address this, we present Lecture Slide Deck Search Engine -- a model that supports natural language queries and hand-drawn sketches and performs searches on a large collection of slide images on computer science topics. This search engine is trained using a novel semantic label-aware transformer model that extracts the semantic labels in the slide images and seamlessly encodes them with the visual cues from the slide images and textual cues from the natural language query. Further, to study the problem in a challenging setting, we introduce a novel dataset, namely the Lecture Slide Deck (LecSD) Dataset containing 54K slide images from the Data Structure, computer networks, and optimization courses and provide associated manual annotation for the query in the form of natural language or hand-drawn sketch. The proposed Lecture Slide Deck Search Engine outperforms the competitive baselines and achieves nearly 4% superior Recall@1 on an absolute scale compared to the state-of-the-art approach. We firmly believe that this work will open up promising directions for improving the accessibility and usability of educational resources, enabling students and educators to find and utilize lecture materials more effectively.",https://openaccess.thecvf.com/content/WACV2024/html/Jobin_Semantic_Labels-Aware_Transformer_Model_for_Searching_Over_a_Large_Collection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jobin_Semantic_Labels-Aware_Transformer_Model_for_Searching_Over_a_Large_Collection_WACV_2024_paper.pdf,https://jobinkv.github.io/lecsd,https://github.com/jobinkv/lecsd,,main,Poster,https://ieeexplore.ieee.org/document/10483774/,"['Visualization', 'Annotations', 'Computational modeling', 'Semantics', 'Natural languages', 'Search engines', 'Benchmark testing']","['Transformer Model', 'Semantic Model', 'Data Structure', 'Search Engine', 'Natural Language', 'Computer Science', 'Digital Networks', 'Online Courses', 'Manual Annotation', 'Slide Images', 'Semantic Labels', 'Massive Open Online Courses', 'Open Online Courses', 'Training Set', 'Validation Set', 'Local Features', 'Paraphrase', 'Latent Space', 'Bar Charts', 'Text Query', 'Multiple Instance Learning', 'Automatic Annotation', 'Segmentation Module', 'Retrieval System', 'Science Courses', 'Hours Of Video', 'Line Graph', 'Image Retrieval', 'Video Lectures']","['Applications', 'Education', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",1,"Massive Open Online Courses (MOOCs) enable easy access to many educational materials, particularly lecture slides, on the web. Searching through them based on user queries becomes an essential problem due to the availability of such vast information. To address this, we present Lecture Slide Deck Search Engine – a model that supports natural language queries and hand-drawn sketches and performs searches on a large collection of slide images on computer science topics. This search engine is trained using a novel semantic label-aware transformer model that extracts the semantic labels in the slide images and seamlessly encodes them with the visual cues from the slide images and textual cues from the natural language query. Further, to study the problem in a challenging setting, we introduce a novel dataset, namely the Lecture Slide Deck (LecSD) Dataset containing 54K slide images from the Data Structure, Computer Networks, and Optimization courses and provide associated manual annotation for the query in the form of natural language or hand-drawn sketch. The proposed Lecture Slide Deck Search Engine outperforms the competitive baselines and achieves nearly 4% superior Recall@1 on an absolute scale compared to the state-of-the-art approach. We firmly believe that this work will open up promising directions for improving the accessibility and usability of educational resources, enabling students and educators to find and utilize lecture materials more effectively."
Semantic Transfer From Head to Tail: Enlarging Tail Margin for Long-Tailed Visual Recognition,"Shan Zhang, Yao Ni, Jinhao Du, Yanxia Liu, Piotr Koniusz",Peking University; Australian National University; Beijing Union University; Data61 CSIRO,100.0,"Australia, China",0.0,,"Deep neural networks excel in visual recognition tasks,but their success hinges on access to balanced datasets. Yet, real-world datasets often exhibit a long-tailed distribution, compromising network efficiency and hampering generalization on unseen data. To enhance the model's generalization in long-tailed scenarios, we present a novel feature augmentation approach termed SeMAntic tRansfer from head to Tail (SMART), which enriches the feature patterns for tail samples by transferring semantic covariance from the head classes to the tail classes along semantically correlating dimensions. This strategy boosts the model's generalization ability by implicitly and adaptively weighting the logits, thereby widening the classification margin of tail classes. Inspired by the success of this weighting, we further incorporate a semantic-aware weighting strategy for the loss tied to tail samples. This amplifies the effect of enlarging the margin for tail classes. We are the first to provide theoretical analysis that demonstrates a large semantic diversity in tail samples can increase class margins during the training stage, leading to improved generalization. Empirical observations support our theory. Notably, with no need for extra data or learnable parameters, SMART achieves state-of-the-art results on five long-tailed benchmark datasets: CIFAR-10/100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Semantic_Transfer_From_Head_to_Tail_Enlarging_Tail_Margin_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Semantic_Transfer_From_Head_to_Tail_Enlarging_Tail_Margin_for_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484350/,"['Training', 'Visualization', 'Head', 'Semantics', 'Tail', 'Benchmark testing', 'Fasteners']","['Deep Neural Network', 'Generalization Ability', 'Balanced Dataset', 'Augmentation Approach', 'Long-tailed Distribution', 'Tail Samples', 'Visual Recognition Tasks', 'Need For Parameters', 'Loss Function', 'Covariance Matrix', 'Deep Models', 'Feature Space', 'Softmax', 'Data Augmentation', 'Cross-entropy Loss', 'Stochastic Gradient Descent', 'Image Generation', 'Semantic Similarity', 'Similar Classification', 'Decision Boundary', 'Imbalanced Datasets', 'Popular Benchmark', 'Minority Class', 'Characteristics Of Head', 'Imbalance Ratio', 'Frequent Category', 'Resampling Method', 'Generalization Error']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Animals / Insects', 'Applications', 'Virtual / augmented reality']",2,"Deep neural networks excel in visual recognition tasks, but their success hinges on access to balanced datasets. Yet, real-world datasets often exhibit a long-tailed distribution, compromising network efficiency and hampering generalization on unseen data. To enhance the model’s generalization in long-tailed scenarios, we present a novel feature augmentation approach termed SeMAntic tRansfer from head to Tail (SMART), which enriches the feature patterns for tail samples by transferring semantic covariance from the head classes to the tail classes along semantically correlating dimensions. This strategy boosts the model’s generalization ability by implicitly and adaptively weighting the logits, thereby widening the classification margin of tail classes. Inspired by the success of this weighting, we further incorporate a semantic-aware weighting strategy for the loss tied to tail samples. This amplifies the effect of enlarging the margin for tail classes. We are the first to provide theoretical analysis that demonstrates a large semantic diversity in tail samples can increase class margins during the training stage, leading to improved generalization. Empirical observations support our theory. Notably, with no need for extra data or learnable parameters, SMART achieves state-of-the-art results on five long-tailed benchmark datasets: CIFAR-10/100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018."
Semantic-Aware Video Representation for Few-Shot Action Recognition,"Yutao Tang, Benjamín Béjar, René Vidal",University of Pennsylvania; Johns Hopkins University; Paul Scherrer Institut,100.0,"Switzerland, USA",0.0,,"Recent work on action recognition leverages 3D features and textual information to achieve state-of-the-art performance. However, most of the current few-shot action recognition methods still rely on 2D frame-level representations, often require additional components to model temporal relations, and employ complex distance functions to achieve accurate alignment of these representations. In addition, existing methods struggle to effectively integrate textual semantics, some resorting to concatenation or addition of textual and visual features, and some using text merely as an additional supervision without truly achieving feature fusion and information transfer from different modalities. In this work, we propose a simple yet effective Semantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these issues. We show that directly leveraging a 3D feature extractor combined with an effective feature-fusion scheme, and a simple cosine similarity for classification can yield better performance without the need of extra components for temporal modeling or complex distance functions. We introduce an innovative scheme to encode the textual semantics into the video representation which adaptively fuses features from text and video, and encourages the visual encoder to extract more semantically consistent features. In this scheme, SAFSAR achieves alignment and fusion in a compact way. Experiments on five challenging few-shot action recognition benchmarks under various settings demonstrate that the proposed SAFSAR model significantly improves the state-of-the-art performance.",https://openaccess.thecvf.com/content/WACV2024/html/Tang_Semantic-Aware_Video_Representation_for_Few-Shot_Action_Recognition_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tang_Semantic-Aware_Video_Representation_for_Few-Shot_Action_Recognition_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484029/,"['Adaptation models', 'Visualization', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Fuses', 'Semantics']","['Action Recognition', 'Few-shot Action Recognition', 'Contralateral', 'Distance Function', 'Visual Features', 'Recognition Model', 'Textual Features', 'Temporal Distance', 'Compact Way', 'Challenging Benchmark', 'Semantic Consistency', 'Action Recognition Model', 'Discriminative Features', 'Classifier Training', 'Types Of Datasets', 'Language Model', 'Class Assignment', 'Spatiotemporal Characteristics', 'Temporal Dependencies', 'Action Classes', 'Support Set', 'Few-shot Learning', 'Temporal Alignment', '2D Feature', 'Class Prototypes', 'Transformer Layers', 'Video Features', 'Final Representation', 'Test Classes', 'Spatiotemporal Representation']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",4,"Recent work on action recognition leverages 3D features and textual information to achieve state-of-the-art performance. However, most of the current few-shot action recognition methods still rely on 2D frame-level representations, often require additional components to model temporal relations, and employ complex distance functions to achieve accurate alignment of these representations. In addition, existing methods struggle to effectively integrate textual semantics, some resorting to concatenation or addition of textual and visual features, and some using text merely as an additional supervision without truly achieving feature fusion and information transfer from different modalities. In this work, we propose a simple yet effective Semantic-Aware Few-Shot Action Recognition (SAFSAR) model to address these issues. We show that directly leveraging a 3D feature extractor combined with an effective feature-fusion scheme, and a simple cosine similarity for classification can yield better performance without the need of extra components for temporal modeling or complex distance functions. We introduce an innovative scheme to encode the textual semantics into the video representation which adaptively fuses features from text and video, and encourages the visual encoder to extract more semantically consistent features. In this scheme, SAFSAR achieves alignment and fusion in a compact way. Experiments on five challenging few-shot action recognition benchmarks under various settings demonstrate that the proposed SAFSAR model significantly improves the state-of-the-art performance."
Semi-Supervised Scene Change Detection by Distillation From Feature-Metric Alignment,"Seonhoon Lee, Jong-Hwan Kim",KAIST,100.0,South Korea,0.0,,"Scene change detection (SCD) is a critical task for various applications, such as visual surveillance, anomaly detection, and mobile robotics. Recently, supervised methods for SCD have been developed for urban and indoor environments where input image pairs are typically unaligned due to differences in camera viewpoints. However, supervised SCD methods require pixel-wise change labels and alignment labels for the target domain, which can be both time-consuming and expensive to collect. To tackle this issue, we design an unsupervised loss with regularization methods based on the feature-metric alignment of input image pairs. The proposed unsupervised loss enables the SCD model to jointly learn the flow and the change maps on the target domain. In addition, we propose a semi-supervised learning method based on a distillation loss for the robustness of the SCD model. The proposed learning method is based on the student-teacher structure and incorporates the unsupervised loss of the unlabeled target data and the supervised loss of the labeled synthetic data. Our method achieves considerable performance improvement on the target domain through the proposed unsupervised and distillation loss, using only 10% of the target training dataset without using any labels of the target data.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Semi-Supervised_Scene_Change_Detection_by_Distillation_From_Feature-Metric_Alignment_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Semi-Supervised_Scene_Change_Detection_by_Distillation_From_Feature-Metric_Alignment_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483629/,"['Training', 'Visualization', 'Surveillance', 'Robot vision systems', 'Semisupervised learning', 'Feature extraction', 'Robustness']","['Change Detection', 'Scene Change Detection', 'Image Pairs', 'Indoor Environments', 'Target Domain', 'Unlabeled Data', 'Target Data', 'Anomaly Detection', 'Regularization Method', 'Mobile Robot', 'Semi-supervised Learning', 'Change Map', 'Semi-supervised Methods', 'Joint Learning', 'Distillation Loss', 'Camera Viewpoint', 'Considerable Performance Improvement', 'Unlabeled Target Data', 'Environmental Changes', 'Image Features', 'Flow Map', 'Student Network', 'Teacher Network', 'Multi-level Features', 'Optical Flow', 'Optical Flow Estimation', 'Query Image', 'Target Dataset', 'Query Features', 'Reference Image']","['Algorithms', 'Image recognition and understanding']",2,"Scene change detection (SCD) is a critical task for various applications, such as visual surveillance, anomaly detection, and mobile robotics. Recently, supervised methods for SCD have been developed for urban and indoor environments where input image pairs are typically unaligned due to differences in camera viewpoints. However, supervised SCD methods require pixel-wise change labels and alignment labels for the target domain, which can be both time-consuming and expensive to collect. To tackle this issue, we design an unsupervised loss with regularization methods based on the feature-metric alignment of input image pairs. The proposed unsupervised loss enables the SCD model to jointly learn the flow and the change maps on the target domain. In addition, we propose a semi-supervised learning method based on a distillation loss for the robustness of the SCD model. The proposed learning method is based on the student-teacher structure and incorporates the unsupervised loss of the unlabeled target data and the supervised loss of the labeled synthetic data. Our method achieves considerable performance improvement on the target domain through the proposed unsupervised and distillation loss, using only 10% of the target training dataset without using any labels of the target data."
Semi-Supervised Semantic Depth Estimation Using Symbiotic Transformer and NearFarMix Augmentation,"Md Awsafur Rahman, Shaikh Anowarul Fattah","Dept. of EEE, BUET, Bangladesh",100.0,Bangladesh,0.0,,"In computer vision, depth estimation is crucial for domains like robotics, autonomous vehicles, augmented reality, and virtual reality. Integrating semantics with depth enhances scene understanding through reciprocal information sharing. However, the scarcity of semantic information in datasets poses challenges. Existing convolutional approaches with limited local receptive fields hinder the full utilization of the symbiotic potential between depth and semantics. This paper introduces a dataset-invariant semi-supervised strategy to address the scarcity of semantic information. It proposes the Depth Semantics Symbiosis module, leveraging the Symbiotic Transformer for achieving comprehensive mutual awareness by information exchange within both local and global contexts. Additionally, a novel augmentation, NearFarMix is introduced to combat overfitting and compensate both depth-semantic tasks by strategically merging regions from two images, generating diverse and structurally consistent samples with enhanced control. Extensive experiments on NYU-Depth-V2 and KITTI datasets demonstrate the superiority of our proposed techniques in indoor and outdoor environments.",https://openaccess.thecvf.com/content/WACV2024/html/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rahman_Semi-Supervised_Semantic_Depth_Estimation_Using_Symbiotic_Transformer_and_NearFarMix_Augmentation_WACV_2024_paper.pdf,,,2308.14400,main,Poster,https://ieeexplore.ieee.org/document/10484272/,"['Symbiosis', 'Computer vision', 'Semantics', 'Merging', 'Information sharing', 'Estimation', 'Computer architecture']","['Transformer', 'Depth Estimation', 'Information Exchange', 'Receptive Field', 'Semantic Information', 'Global Context', 'Autonomous Vehicles', 'Scene Understanding', 'KITTI Dataset', 'Convolution', 'Decoding', 'Attention Mechanism', 'Teacher Model', 'Symbiotic Relationship', 'Semantic Segmentation', 'Depth Map', 'Semantic Features', 'Depth Information', 'Semi-supervised Learning', 'Student Model', 'Depth Features', 'Query Features', 'Semantic Labels', 'Supplement For Details', 'Depth Threshold']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In computer vision, depth estimation is crucial for domains like robotics, autonomous vehicles, augmented reality, and virtual reality. Integrating semantics with depth enhances scene understanding through reciprocal information sharing. However, the scarcity of semantic information in datasets poses challenges. Existing convolutional approaches with limited local receptive fields hinder the full utilization of the symbiotic potential between depth and semantics. This paper introduces a dataset-invariant semi-supervised strategy to address the scarcity of semantic information. It proposes the Depth Semantics Symbiosis module, leveraging the Symbiotic Transformer for achieving comprehensive mutual awareness by information exchange within both local and global contexts. Additionally, a novel augmentation, NearFarMix is introduced to combat overfitting and compensate both depth-semantic tasks by strategically merging regions from two images, generating diverse and structurally consistent samples with enhanced control. Extensive experiments on NYU-Depth-V2 and KITTI datasets demonstrate the superiority of our proposed techniques in indoor and outdoor environments."
Separable Self and Mixed Attention Transformers for Efficient Object Tracking,"Goutam Yelluru Gopal, Maria A. Amer","Department of Electrical and Computer Engineering, Concordia University, Montréal, Québec, Canada",100.0,Canada,0.0,,"The deployment of transformers for visual object tracking has shown state-of-the-art results on several benchmarks. However, the transformer-based models are under-utilized for Siamese lightweight tracking due to the computational complexity of their attention blocks. This paper proposes an efficient self and mixed attention transformer-based architecture for lightweight tracking. The proposed backbone utilizes the separable mixed attention transformers to fuse the template and search regions during feature extraction to generate superior feature encoding. Our prediction head performs global contextual modeling of the encoded features by leveraging efficient self-attention blocks for robust target state estimation. With these contributions, the proposed lightweight tracker deploys a transformer-based backbone and head module concurrently for the first time. Our ablation study testifies to the effectiveness of the proposed combination of backbone and head modules. Simulations show that our Separable Self and Mixed Attention-based Tracker, SMAT, surpasses the performance of related lightweight trackers on GOT10k, TrackingNet, LaSOT, NfS30, UAV123, and AVisT datasets, while running at 37 fps on CPU, 158 fps on GPU, and having 3.8M parameters. For example, it significantly surpasses the closely related trackers E.T.Track and MixFormerV2-S on GOT10k-test by a margin of 7.9% and 5.8%, respectively, in the AO metric. The tracker code and model is available at https://github.com/goutamyg/SMAT",https://openaccess.thecvf.com/content/WACV2024/html/Gopal_Separable_Self_and_Mixed_Attention_Transformers_for_Efficient_Object_Tracking_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gopal_Separable_Self_and_Mixed_Attention_Transformers_for_Efficient_Object_Tracking_WACV_2024_paper.pdf,,https://github.com/goutamyg/SMAT,2309.03979,main,Poster,https://ieeexplore.ieee.org/document/10484266/,"['Target tracking', 'Computational modeling', 'Computer architecture', 'Color', 'Predictive models', 'Benchmark testing', 'Transformers']","['Object Tracking', 'Tracking Efficiency', 'Mixed Attention', 'Global Model', 'Tracking Performance', 'Feature Encoder', 'Search Region', 'Prediction Head', 'Template Region', 'Convolutional Neural Network', 'Feature Representation', 'Bounding Box', 'Target Object', 'Feature Fusion', 'Convolutional Neural Network Layers', 'L1 Loss', 'Test Videos', 'Transformer Layers', 'Bounding Box Regression', 'Target Template', 'Transformer Block', 'Template Feature', 'Regression Branch', 'Vision Transformer', 'Change In Aspect Ratio', 'Feature Fusion Method', 'Classification Branch', 'Context Vector', 'Challenging Benchmark', 'Mixed Block']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",7,"The deployment of transformers for visual object tracking has shown state-of-the-art results on several benchmarks. However, the transformer-based models are underutilized for Siamese lightweight tracking due to the computational complexity of their attention blocks. This paper proposes an efficient self and mixed attention transformer-based architecture for lightweight tracking. The proposed backbone utilizes the separable mixed attention transformers to fuse the template and search regions during feature extraction to generate superior feature encoding. Our prediction head performs global contextual modeling of the encoded features by leveraging efficient self-attention blocks for robust target state estimation. With these contributions, the proposed lightweight tracker deploys a transformer-based backbone and head module concurrently for the first time. Our ablation study testifies to the effectiveness of the proposed combination of backbone and head modules. Simulations show that our Separable Self and Mixed Attention-based Tracker, SMAT, surpasses the performance of related lightweight trackers on GOT10k, TrackingNet, LaSOT, NfS30, UAV123, and AVisT datasets, while running at 37 fps on CPU, 158 fps on GPU, and having 3.8M parameters. For example, it significantly surpasses the closely related trackers E.T.Track and MixFormerV2-S on GOT10ktest by a margin of 7.9% and 5.8%, respectively, in the AO metric. The tracker code and model is available at https://github.com/goutamyg/SMAT."
SequenceMatch: Revisiting the Design of Weak-Strong Augmentations for Semi-Supervised Learning,Khanh-Binh Nguyen,"Sungkyunkwan University, South Korea",100.0,South Korea,0.0,,"Semi-supervised learning (SSL) has become popular in recent years because it allows the training of a model using a large amount of unlabeled data. However, one issue that many SSL methods face is the confirmation bias, which occurs when the model is overfitted to the small labeled training dataset and produces overconfident, incorrect predictions. To address this issue, we propose SequenceMatch, an efficient SSL method that utilizes multiple data augmentations. The key element of SequenceMatch is the inclusion of a medium augmentation for unlabeled data. By taking advantage of different augmentations and the consistency constraints between each pair of augmented examples, SequenceMatch helps reduce the divergence between the prediction distribution of the model for weakly and strongly augmented examples. In addition, SequenceMatch defines two different consistency constraints for high and low-confidence predictions. As a result, SequenceMatch is more data-efficient than ReMixMatch, and more time-efficient than both ReMixMatch (x4) and CoMatch (x2) while having higher accuracy. Despite its simplicity, SequenceMatch consistently outperforms prior methods on standard benchmarks, such as CIFAR-10/100, SVHN, and STL-10. It also surpasses prior state-of-the-art methods by a large margin on large-scale datasets such as ImageNet, with a 38.46% error rate.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_SequenceMatch_Revisiting_the_Design_of_Weak-Strong_Augmentations_for_Semi-Supervised_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_SequenceMatch_Revisiting_the_Design_of_Weak-Strong_Augmentations_for_Semi-Supervised_Learning_WACV_2024_paper.pdf,,https://github.com/beandkay/SequenceMatch,2310.15787,main,Poster,,,,,,
Sequential Transformer for End-to-End Video Text Detection,"Jun-Bo Zhang, Meng-Biao Zhao, Fei Yin, Cheng-Lin Liu","SKL of MAIS, Institute of Automation of Chinese Academy of Sciences, and School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",100.0,China,0.0,,"In existing methods of video text detection, the detection and tracking branches are usually independent of each other, and although they jointly optimize the backbone network, the tracking-by-detection paradigm still needs to be used during the inference stage. To address this issue, we propose a novel video text detection framework based on sequential transformer, which decodes detection and tracking tasks in parallel, without explicitly setting up a tracking branch. To achieve this, we first introduce the concept of instance query, which learns long-term context information in the video sequence. Then, based on the instance query, the transformer decoder is used to predict the entire box and mask sequence of the text instance in one pass. As a result, the tracking task is realized naturally. In addition, the proposed method can be applied to the scene text detection task seamlessly, without modifying any modules. To the best of our knowledge, this is the first framework to unify the tasks of scene text detection and video text detection. Our model achieves state-of-the-art performance on four video text datasets (YVT, RT-1K, BOVText, and BiRViT-1K), and competitive results on three scene text datasets (CTW1500, MSRA-TD500, and Total-Text). The code is available at https://github.com/zjb-1/SeqVideoText.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Sequential_Transformer_for_End-to-End_Video_Text_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Sequential_Transformer_for_End-to-End_Video_Text_Detection_WACV_2024_paper.pdf,,https://github.com/zjb-1/SeqVideoText,,main,Poster,,,,,,
ShARc: Shape and Appearance Recognition for Person Identification In-the-Wild,"Haidong Zhu, Wanrong Zheng, Zhaoheng Zheng, Ram Nevatia",University of Southern California,100.0,USA,0.0,,"Identifying individuals in unconstrained video settings is a valuable yet challenging task in biometric analysis due to variations in appearances, environments, degradations, and occlusions. In this paper, we present ShARc, a multimodal approach for video-based person identification in uncontrolled environments that emphasizes 3-D body shape, pose, and appearance. We introduce two encoders: a Pose and Shape Encoder (PSE) and an Aggregated Appearance Encoder (AAE). PSE encodes the body shape via binarized silhouettes, skeleton motions, and 3-D body shape, while AAE provides two levels of temporal appearance feature aggregation: attention-based feature aggregation and averaging aggregation. For attention-based feature aggregation, we employ spatial and temporal attention to focus on key areas for person distinction. For averaging aggregation, we introduce a novel flattening layer after averaging to extract more distinguishable information and reduce overfitting of attention. We utilize centroid feature averaging for gallery registration. We demonstrate significant improvements over existing state-of-the-art methods on public datasets, including CCVID, MEVID, and BRIAR.",https://openaccess.thecvf.com/content/WACV2024/html/Zhu_ShARc_Shape_and_Appearance_Recognition_for_Person_Identification_In-the-Wild_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_ShARc_Shape_and_Appearance_Recognition_for_Person_Identification_In-the-Wild_WACV_2024_paper.pdf,,,2310.15946,main,Poster,,,,,,
ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection From RGB-Thermal Drone Imagery,"Rudraksh Kapil, Seyed Mojtaba Marvasti-Zadeh, Nadir Erbilgin, Nilanjan Ray","University of Alberta, Canada",100.0,Canada,0.0,,"Accurate detection of individual tree crowns from remote sensing data poses a significant challenge due to the dense nature of forest canopy and the presence of diverse environmental variations, e.g., overlapping canopies, occlusions, and varying lighting conditions. Additionally, the lack of data for training robust models adds another limitation in effectively studying complex forest conditions. This paper presents a novel method for detecting shadowed tree crowns and provides a challenging dataset comprising roughly 50k paired RGB-thermal images to facilitate future research for illumination-invariant detection. The proposed method (ShadowSense) is entirely self-supervised, leveraging domain adversarial training without source domain annotations for feature extraction and foreground feature alignment for feature pyramid networks to adapt domain-invariant representations by focusing on visible foreground regions, respectively. It then fuses complementary information of both modalities to effectively improve upon the predictions of an RGB-trained detector and boost the overall accuracy. Extensive experiments demonstrate the superiority of the proposed method over both the baseline RGB-trained detector and state-of-the-art techniques that rely on unsupervised domain adaptation or early image fusion. Our code and data are available: https://github.com/rudrakshkapil/ShadowSense",https://openaccess.thecvf.com/content/WACV2024/html/Kapil_ShadowSense_Unsupervised_Domain_Adaptation_and_Feature_Fusion_for_Shadow-Agnostic_Tree_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kapil_ShadowSense_Unsupervised_Domain_Adaptation_and_Feature_Fusion_for_Shadow-Agnostic_Tree_WACV_2024_paper.pdf,,https://github.com/rudrakshkapil/ShadowSense,2310.16212,main,Poster,https://ieeexplore.ieee.org/document/10483815/,"['Training', 'Annotations', 'Lighting', 'Vegetation', 'Detectors', 'Forestry', 'Feature extraction']","['Feature Fusion', 'Domain Adaptation', 'Tree Crown', 'Drone Imagery', 'Tree Crown Detection', 'Image Pairs', 'Source Domain', 'Challenging Dataset', 'Feature Alignment', 'Feature Pyramid Network', 'Early Fusion', 'Foreground Regions', 'Domain-invariant Representations', 'Model Performance', 'Infrared Imaging', 'Feature Maps', 'Intersection Over Union', 'RGB Images', 'Background Regions', 'Domain Discriminator', 'Thermal Characteristics', 'Translation Method', 'Manual Annotation', 'Thermal Mode', 'Unsupervised Domain Adaptation Methods', 'Fused Feature Map', 'Forest Monitoring', 'Baseline Detection']","['Applications', 'Remote Sensing', 'Algorithms', 'Image recognition and understanding']",1,"Accurate detection of individual tree crowns from remote sensing data poses a significant challenge due to the dense nature of forest canopy and the presence of diverse environmental variations, e.g., overlapping canopies, occlusions, and varying lighting conditions. Additionally, the lack of data for training robust models adds another limitation in effectively studying complex forest conditions. This paper presents a novel method for detecting shadowed tree crowns and provides a challenging dataset comprising roughly 50k paired RGB-thermal images to facilitate future research for illumination-invariant detection. The proposed method (ShadowSense) is entirely self-supervised, leveraging domain adversarial training without source domain annotations for feature extraction and foreground feature alignment for feature pyramid networks to adapt domain-invariant representations by focusing on visible foreground regions, respectively. It then fuses complementary information of both modalities to effectively improve upon the predictions of an RGB-trained detector and boost the overall accuracy. Extensive experiments demonstrate the superiority of the proposed method over both the baseline RGB-trained detector and state-of-the-art techniques that rely on unsupervised domain adaptation or early image fusion. Our code and data are available: https://github.com/rudrakshkapil/ShadowSense."
Shape From Shading for Robotic Manipulation,"Arkadeep Narayan Chaudhury, Leonid Keselman, Christopher G. Atkeson","The Robotics Institute, Carnegie Mellon University",100.0,USA,0.0,,"Controlling illumination can generate high quality information about object surface normals and depth discontinuities at a low computational cost. In this work we demonstrate a robot workspace-scaled controlled illumination approach that generates high quality information for table top scale objects for robotic manipulation. With our low angle of incidence directional illumination approach, we can precisely capture surface normals and depth discontinuities of monochromatic Lambertian objects. We show that this approach to shape estimation is 1) valuable for general purpose grasping with a single point vacuum gripper, 2) can measure the deformation of known objects, and 3) can estimate pose of known objects and track unknown objects in the robot's workspace.",https://openaccess.thecvf.com/content/WACV2024/html/Chaudhury_Shape_From_Shading_for_Robotic_Manipulation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chaudhury_Shape_From_Shading_for_Robotic_Manipulation_WACV_2024_paper.pdf,,,2304.11824,main,Poster,https://ieeexplore.ieee.org/document/10484082/,"['Reflectivity', 'Shape', 'Deformation', 'Shape measurement', 'Lighting', 'Sensors', 'Object recognition']","['Robot Manipulator', 'Photometric Stereo', 'Workspace', 'Object Surface', 'Surface Depth', 'Direct Illumination', 'Unknown Objects', 'Surface Normals', 'Point Cloud', 'Surface Reflectance', 'Quadratic Model', 'Ambient Light', 'Depth Map', 'Object Shape', 'Manipulation Tasks', 'Depth Camera', 'General Objective', 'Pose Estimation', 'Supplement For Details', 'Camera Pose', 'Commercial Sensors', 'Reflective Objects', 'Discontinuity Surface', 'Object Geometry', 'Low Incidence Angles', 'Pose Changes', 'Suction Cup', 'Robotic Tasks', 'Camera View']","['Applications', 'Robotics', 'Algorithms', '3D computer vision', 'Algorithms', 'Low-level and physics-based vision']",1,"Controlling illumination can generate high quality information about object surface normals and depth discontinuities at a low computational cost. In this work we demonstrate a robot workspace-scaled controlled illumination approach that generates high quality information for table top scale objects for robotic manipulation. With our low angle of incidence directional illumination approach, we can precisely capture surface normals and depth discontinuities of monochromatic Lambertian objects. We show that this approach to shape estimation is 1) valuable for general purpose grasping with a single point vacuum gripper, 2) can measure the deformation of known objects, and 3) can estimate pose of known objects and track unknown objects in the robot’s workspace."
Shape-Biased CNNs Are Not Always Superior in Out-of-Distribution Robustness,"Xinkuan Qiu, Meina Kan, Yongbin Zhou, Yanchao Bi, Shiguang Shan","Peng Cheng Laboratory, Shenzhen 518055, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100085, China; Beijing Normal University, Beijing 100875, China; University of Chinese Academy of Sciences, Beijing 100049, China",100.0,China,0.0,,"In recent years, Out-of-Distribution (o.o.d) Robustness has garnered increasing attention in Deep Learning, and shape-biased Convolutional Neural Networks (CNNs) are believed to exhibit higher robustness, attributed to the inherent shape-based decision rule of human cognition. In this work, we delve deeper into the intricate relationship between shape/texture information and o.o.d robustness by leveraging a carefully curated ""Category-Balanced ImageNet"" dataset. We find that shape information is not always superior in distinguishing distinct categories and shape-biased model is not always superior across various o.o.d scenarios. Motivated by these insightful findings, we design a novel method named Shape-Texture Adaptive Recombination (STAR) to achieve higher o.o.d robustness. A category-balanced dataset is firstly used to pretrain a debiased backbone and three specialized heads, each adept at robustly extracting shape, texture, and debiased features. Subsequently, an instance-adaptive recombination head is trained to adaptively adjust the contributions of these distinctive features for each given instance. Through comprehensive experiments, our proposed method achieves state-of-the-art o.o.d robustness across various scenarios such as image corruptions, adversarial attacks, style shifts, and dataset shifts, demonstrating its effectiveness.",https://openaccess.thecvf.com/content/WACV2024/html/Qiu_Shape-Biased_CNNs_Are_Not_Always_Superior_in_Out-of-Distribution_Robustness_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Qiu_Shape-Biased_CNNs_Are_Not_Always_Superior_in_Out-of-Distribution_Robustness_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Shape-Guided Diffusion With Inside-Outside Attention,"Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach, Trevor Darrell",Meta AI; The University of Hong Kong; UC Berkeley; BMW Group,50.0,"Hong Kong, USA",50.0,USA,"We introduce precise object silhouette as a new form of user control in text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our training-free method uses an Inside-Outside Attention mechanism during the inversion and generation process to apply a shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degradation in text alignment or image realism according to both automatic metrics and annotator ratings. Our data and code will be made available at https://shape-guided-diffusion.github.io.",https://openaccess.thecvf.com/content/WACV2024/html/Park_Shape-Guided_Diffusion_With_Inside-Outside_Attention_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Park_Shape-Guided_Diffusion_With_Inside-Outside_Attention_WACV_2024_paper.pdf,,https://shape-guided-diffusion.github.io,2212.00210,main,Poster,https://ieeexplore.ieee.org/document/10483675/,"['Degradation', 'Measurement', 'Computer vision', 'Codes', 'Shape', 'Benchmark testing', 'Task analysis']","['Diffusion Model', 'Spatial Regions', 'Inversion Process', 'Segmentation Model', 'Latent Space', 'Source Images', 'Inference Time', 'Synthetic Images', 'Attention Map', 'Unconditional Model', 'Background Pixels', 'Masked Images', 'Prior Methods', 'Version Of Image', 'Input Text', 'Image Editing', 'Self-attention Layer', 'Image Inpainting']","['Algorithms', 'Computational photography', 'image and video synthesis']",6,"We introduce precise object silhouette as a new constraint in text-to-image diffusion models, which we dub Shape-Guided Diffusion. Our training-free method uses an Inside-Outside Attention mechanism during the inversion and generation process to apply a shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degradation in text alignment or image realism according to both automatic metrics and annotator ratings. Our data and code will be made available at https://shape-guided-diffusion.github.io."
Sharp-NeRF: Grid-Based Fast Deblurring Neural Radiance Fields Using Sharpness Prior,"Byeonghyeon Lee, Howoong Lee, Usman Ali, Eunbyung Park","Department of Artificial Intelligence, Sungkyunkwan University; Department of Electrical and Computer Engineering, Sungkyunkwan University",100.0,South Korea,0.0,,"Neural Radiance Fields (NeRF) has shown its remarkable performance in neural rendering-based novel view synthesis. However, NeRF suffers from severe visual quality degradation when the input images have been captured under imperfect conditions, such as poor illumination, defocus blurring and lens aberrations. Especially, defocus blur is quite common in the images when they are normally captured using cameras. Although few recent studies have proposed to render sharp images of considerably high-quality, yet they still face many key challenges. In particular, those methods have employed a Multi-Layer Perceptron (MLP) based NeRF which requires tremendous computational time. To overcome these shortcomings, this paper proposes a novel technique Sharp-NeRF---a grid-based NeRF that renders clean and sharp images from the input blurry images within a half an hour training. To do so, we used several grid-based kernels to accurately model the sharpness/blurriness of the scene. The sharpness level of the pixels is computed to learn the spatially varying blur kernels. We have conducted experiments on the benchmarks consisting of blurry images and have evaluated full-reference and non-reference metrics. The qualitative and quantitative results have revealed that our approach renders the sharp novel views with vivid colors and fine details, and it has considerably faster training time than the previous works. Our code is available at https://github.com/benhenryL/SharpNeRF.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_Sharp-NeRF_Grid-Based_Fast_Deblurring_Neural_Radiance_Fields_Using_Sharpness_Prior_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_Sharp-NeRF_Grid-Based_Fast_Deblurring_Neural_Radiance_Fields_Using_Sharpness_Prior_WACV_2024_paper.pdf,,https://github.com/benhenryL/SharpNeRF,,main,Poster,https://ieeexplore.ieee.org/document/10484517/,"['Training', 'Degradation', 'Measurement', 'Visualization', 'Lighting', 'Rendering (computer graphics)', 'Kernel']","['Deblurring', 'Radiance Field', 'Neural Radiance Fields', 'Training Time', 'Input Image', 'Multilayer Perceptron', 'Visual Quality', 'Clear Image', 'Pixel Level', 'Image Sharpness', 'Optical Aberrations', 'Blurry Images', 'View Synthesis', 'Blur Kernel', 'Neural Network', 'Image Processing', 'Random Sampling', 'Convolutional Neural Network', 'Image Quality', 'Additive Noise', 'Neural Field', 'Motion Blur', 'Blurred Images', 'Sharpe Ratio', 'Quality Metrics', 'Point Spread Function', 'Peak Signal-to-noise Ratio', 'Depth Of Field', 'Neighboring Pixels', 'Sample Patches']","['Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",3,"Neural Radiance Fields (NeRF) has shown its remarkable performance in neural rendering-based novel view synthesis. However, NeRF suffers from severe visual quality degradation when the input images have been captured under imperfect conditions, such as poor illumination, defocus blurring and lens aberrations. Especially, defocus blur is quite common in the images when they are normally captured using cameras. Although few recent studies have proposed to render sharp images of considerably high-quality, yet they still face many key challenges. In particular, those methods have employed a Multi-Layer Perceptron (MLP) based NeRF which requires tremendous computational time. To overcome these shortcomings, this paper proposes a novel technique Sharp-NeRF—a grid-based NeRF that renders clean and sharp images from the input blurry images within a half an hour training. To do so, we used several grid-based kernels to accurately model the sharpness/blurriness of the scene. The sharpness level of the pixels is computed to learn the spatially varying blur kernels. We have conducted experiments on the benchmarks consisting of blurry images and have evaluated full-reference and non-reference metrics. The qualitative and quantitative results have revealed that our approach renders the sharp novel views with vivid colors and fine details, and it has considerably faster training time than the previous works. Our code is available at https://github.com/benhenryL/SharpNeRF."
Show Your Face: Restoring Complete Facial Images From Partial Observations for VR Meeting,"Zheng Chen, Zhiqi Zhang, Junsong Yuan, Yi Xu, Lantao Liu",University at Buffalo; OPPO US Research Center; Indiana University,66.66666666666666,USA,33.33333333333334,USA,"Virtual Reality (VR) headsets allow users to interact with the virtual world. However, the device physically blocks visual connections among users, causing huge inconveniences for VR meetings. To address this issue, studies have been conducted to restore human faces from images captured by Headset Mounted Cameras (HMC). Unfortunately, existing approaches heavily rely on high-resolution person-specific 3D models which are prohibitively expensive to apply to large-scale scenarios. Our goal is to design an efficient framework for restoring users' facial data in VR meetings. Specifically, we first build a new dataset, named Facial Image Composition (FIC) data which approximates the real HMC images from a VR headset. By leveraging the heterogeneity of the HMC images, we decompose the restoration problem into a local geometry transformation and global color/style fusion. Then we propose a 2D light-weight facial image composition network (FIC-Net), where three independent local models are responsible for transforming raw HMC patches and the global model performs a fusion of the transformed HMC patches with a pre-recorded reference image. Finally, we also propose a stage-wise training strategy to optimize the generalization of our FIC-Net. We have validated the effectiveness of our proposed FIC-Net through extensive experiments.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Show_Your_Face_Restoring_Complete_Facial_Images_From_Partial_Observations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Show_Your_Face_Restoring_Complete_Facial_Images_From_Partial_Observations_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483286/,"['Headphones', 'Training', 'Solid modeling', 'Visualization', 'Three-dimensional displays', 'Computational modeling', 'Virtual reality']","['Face Images', 'Partial Observation', 'Global Model', 'Training Strategy', 'Independent Model', 'Reference Image', 'Human Faces', 'Composite Image', 'Virtual Reality Headset', 'Face Data', 'Training Data', 'Test Data', 'Infrared Imaging', 'Skin Color', 'Left Eye', 'Latent Space', 'Skip Connections', 'Color Pattern', 'Ground Truth Image', 'Mean Square Error Loss', 'Latent Code', 'Facial Color', 'Homogeneous Image', 'Structural Similarity Index Measure', 'Heterogeneous Images', 'Real Use Case', 'Real Faces', 'Intermediate Image', 'Decoder Features', 'Texture Patterns']","['Applications', 'Virtual / augmented reality']",,"Virtual Reality (VR) headsets allow users to interact with the virtual world. However, the device physically blocks visual connections among users, causing huge inconveniences for VR meetings. To address this issue, studies have been conducted to restore human faces from images captured by Headset Mounted Cameras (HMC). Unfortunately, existing approaches heavily rely on high-resolution person-specific 3D models which are prohibitively expensive to apply to large-scale scenarios. Our goal is to design an efficient framework for restoring users’ facial data in VR meetings. Specifically, we first build a new dataset, named Facial Image Composition (FIC) data which approximates the real HMC images from a VR headset. By leveraging the heterogeneity of the HMC images, we decompose the restoration problem into a local geometry transformation and global color/style fusion. Then we propose a 2D light-weight facial image composition network (FIC-Net), where three independent local models are responsible for transforming raw HMC patches and the global model performs a fusion of the transformed HMC patches with a pre-recorded reference image. Finally, we also propose a stage-wise training strategy to optimize the generalization of our FIC-Net. We have validated the effectiveness of our proposed FIC-Net through extensive experiments."
SigmML: Metric Meta-Learning for Writer Independent Offline Signature Verification in the Space of SPD Matrices,"Alexios Giazitzis, Elias N. Zois","Telsip Laboratory, UniWA",0.0,,100.0,Poland,"The handwritten signature has been identified as one of the most popular biometric means of human consent and/or presence for transactions held by any number of physical or legal entities. Automated signature verification (ASV), merge popular scientific branches such as computer vision, pattern recognition and/or data-driven machine learning algorithms. Up to now, several metric learning approaches for designing a writer-independent signature verifier, have been developed within a Euclidean framework by means of having their operations closed with respect to real vector spaces. In this work, we propose, for the first time in the ASV literature, the use of a meta-learning framework in the space of the Symmetric Positive Definite (SPD) manifold as a means to learn a pairwise similarity metric for writer-independent ASV. To begin, pairs of handwritten signatures are converted into a multidimensional distance vector with elements corresponding SPD distances between spatial segments of corresponding covariance pairs. We propose a novel meta-learning approach which explores the structure of the input gradients of the SPD manifold by means of a recurrent model, constrained by the geometry of the SPD manifold. The experimental protocols utilize two popular signature datasets of Western and Asian origin in two blind-intra and blind-inter (or cross-lingual) transfer learning approach. It also provide evidence of the discriminating nature of the proposed framework at least when summarized against other State-of-the-Art models, typically realized under a framework of Euclidean, or vector space, nature.",https://openaccess.thecvf.com/content/WACV2024/html/Giazitzis_SigmML_Metric_Meta-Learning_for_Writer_Independent_Offline_Signature_Verification_in_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Giazitzis_SigmML_Metric_Meta-Learning_for_Writer_Independent_Offline_Signature_Verification_in_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484049/,"['Metalearning', 'Manifolds', 'Handwriting recognition', 'Computer vision', 'Symmetric matrices', 'Protocols', 'Machine learning algorithms']","['Positive Definite Matrix', 'Space Of Matrices', 'Symmetric Positive Definite Matrix', 'Signature Verification', 'Vector Space', 'Legal Personality', 'Metric Learning', 'Transfer Learning Approach', 'Objective Function', 'Learning Rate', 'Covariance Matrix', 'Gradient Descent', 'Second Derivative', 'Symmetric Matrix', 'Euclidean Space', 'Learnable Parameters', 'Matrix M', 'Vector Representation', 'Mahalanobis Distance', 'Riemannian Manifold', 'Tangent Space', 'Block Diagonal Matrix', 'Learning Stage', 'Gradient Information', 'Projection Operator', 'Covariance Parameters']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"The handwritten signature has been identified as one of the most popular biometric means of human consent and/or presence for transactions held by any number of physical or legal entities. Automated signature verification (ASV), merge popular scientific branches such as computer vision, pattern recognition and/or data-driven machine learning algorithms. Up to now, several metric learning approaches for designing a writer-independent signature verifier, have been developed within a Euclidean framework by means of having their operations closed with respect to real vector spaces. In this work, we propose, for the first time in the ASV literature, the use of a meta-learning framework in the space of the Symmetric Positive Definite (SPD) manifold as a means to learn a pairwise similarity metric for writer-independent ASV. To begin, pairs of handwritten signatures are converted into a multidimensional distance vector with elements corresponding SPD distances between spatial segments of corresponding covariance pairs. We propose a novel meta-learning approach which explores the structure of the input gradients of the SPD manifold by means of a recurrent model, constrained by the geometry of the SPD manifold. The experimental protocols utilize two popular signature datasets of Western and Asian origin in two blind-intra and blind-inter (or cross-lingual) transfer learning approach. It also provide evidence of the discriminating nature of the proposed framework at least when summarized against other State-of-the-Art models, typically realized under a framework of Euclidean, or vector space, nature."
Sign Language Production With Latent Motion Transformer,"Pan Xie, Taiying Peng, Yao Du, Qipeng Zhang","Beihang University, Beijing, China",100.0,China,0.0,,"Sign Language Production (SLP) is the tough task of turning sign language into sign videos. The main goal of SLP is to create these videos using a sign gloss. In this research, we've developed a new method to make high-quality sign videos without using human poses as a middle step. Our model works in two main parts: first, it learns from a generator and the video's hidden features, and next, it uses another model to understand the order of these hidden features. To make this method even better for sign videos, we make several significant improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn downsampled latent representations. (ii) In the second stage, we introduce sequence-to-sequence attention to better leverage conditional information. (iii) The separated two-stage training discards the realistic visual semantic of the latent codes in the second stage. To endow the latent sequences semantic information, we extend the token-level autoregressive latent codes learning with perceptual loss and reconstruction loss for the prior model with visual perception. Compared with previous state-of-the-art approaches, our model performs consistently better on two word-level sign language datasets, i.e., WLASL and NMFs-CSL.",https://openaccess.thecvf.com/content/WACV2024/html/Xie_Sign_Language_Production_With_Latent_Motion_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xie_Sign_Language_Production_With_Latent_Motion_Transformer_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483712/,"['Training', 'Sign language', 'Visualization', 'Codes', 'Semantics', 'Production', 'Transformers']","['Sign Language', 'Sign Language Production', 'Latent Representation', 'Reconstruction Loss', 'Perceptual Loss', 'Human Pose', 'Latent Code', 'Convolutional Network', 'Autoregressive Model', 'Attention Mechanism', 'Cross-entropy Loss', 'Latent Space', 'Hand Movements', 'Additional Loss', 'Transformer Model', 'Cross-entropy Loss Function', 'Variational Autoencoder', 'Prior Learning', 'Transformer Architecture', 'Quadratic Complexity', 'Strand Of Work']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Sign Language Production (SLP) is the tough task of turning sign language into sign videos. The main goal of SLP is to create these videos using a sign gloss. In this research, we’ve developed a new method to make high-quality sign videos without using human poses as a middle step. Our model works in two main parts: first, it learns from a generator and the video’s hidden features, and next, it uses another model to understand the order of these hidden features. To make this method even better for sign videos, we make several significant improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn downsampled latent representations. (ii) In the second stage, we introduce sequence-to-sequence attention to better leverage conditional information. (iii) The separated two-stage training discards the realistic visual semantic of the latent codes in the second stage. To endow the latent sequences semantic information, we extend the token-level autoregressive latent codes learning with perceptual loss and reconstruction loss for the prior model with visual perception. Compared with previous state-of-the-art approaches, our model performs consistently better on two word-level sign language datasets, i.e., WLASL and NMFs-CSL."
SimA: Simple Softmax-Free Attention for Vision Transformers,"Soroush Abbasi Koohpayegani, Hamed Pirsiavash","University of California, Davis",100.0,USA,0.0,,"Recently, vision transformers have become very popular. However, deploying them in many applications is computationally expensive partly due to the Softmax layer in the attention block. We introduce a simple yet effective, Softmax-free attention block, SimA, which normalizes query and key matrices with simple l1-norm instead of using Softmax layer. Then, the attention block in SimA is a simple multiplication of three matrices, so SimA can dynamically change the ordering of the computation at the test time to achieve linear computation on the number of tokens or the number of channels. We empirically show that SimA applied to three SOTA variations of transformers, DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models, without any need for Softmax layer. Interestingly, changing SimA from multi-head to single-head has only a small effect on the accuracy, which further simplifies the attention block. Moreover, we show that SimA is much faster on small edge devices, e.g., Raspberry Pi, which we believe is due to higher complexity of Softmax layer on those devices. The code is available here: https://github.com/UCDvision/sima",https://openaccess.thecvf.com/content/WACV2024/html/Koohpayegani_SimA_Simple_Softmax-Free_Attention_for_Vision_Transformers_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Koohpayegani_SimA_Simple_Softmax-Free_Attention_for_Vision_Transformers_WACV_2024_paper.pdf,,https://github.com/UCDvision/sima,2206.08898,main,Poster,https://ieeexplore.ieee.org/document/10483688/,"['Performance evaluation', 'Visualization', 'Image segmentation', 'Image resolution', 'Image edge detection', 'Switches', 'Self-supervised learning']","['Vision Transformer', 'Simple Attention', 'Softmax Layer', 'Edge Devices', 'Attention Block', 'Key Matrix', 'Training Set', 'Limited Resources', 'Convolutional Neural Network', 'Image Resolution', 'Validation Set', 'Image Classification', 'Object Detection', 'Attention Mechanism', 'Weight Decay', 'Feed-forward Network', 'Semantic Segmentation', 'Dot Product', 'Self-supervised Learning', 'Output Of Block', 'Multi-head Self-attention', 'Transformer Architecture', 'AdamW Optimizer', 'Attention Values', 'Key Vector', 'Standard Transformation', 'Matrix Size', 'Inference Time']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",7,"Recently, vision transformers have become very popular. However, deploying them in many applications is computationally expensive partly due to the Softmax layer in the attention block. We introduce a simple yet effective, Softmaxfree attention block, SimA, which normalizes query and key matrices with simple ℓ
<inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</inf>
-norm instead of using Softmax layer. Then, the attention block in SimA is a simple multiplication of three matrices, so SimA can dynamically change the ordering of the computation at the test time to achieve linear computation on the number of tokens or the number of channels. We empirically show that SimA applied to three SOTA variations of transformers, DeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models, without any need for Softmax layer. Interestingly, changing SimA from multi-head to single-head has only a small effect on the accuracy, which further simplifies the attention block. Moreover, we show that SimA is much faster on small edge devices, e.g., Raspberry Pi, which we believe is due to higher complexity of Softmax layer on those devices. The code is available here: https://github.com/UCDvision/sima"
Simple Post-Training Robustness Using Test Time Augmentations and Random Forest,"Gilad Cohen, Raja Giryes",Tel Aviv University,100.0,Israel,0.0,,"Although Deep Neural Networks (DNNs) achieve excellent performance on many real-world tasks, they are highly vulnerable to adversarial attacks. A leading defense against such attacks is adversarial training, a technique in which a DNN is trained to be robust to adversarial attacks by introducing adversarial noise to its input. This procedure is effective but must be done during the training phase. In this work, we propose Augmented Random Forest (ARF), a simple and easy-to-use strategy for robustifying an existing pretrained DNN without modifying its weights. For every image, we generate randomized test time augmentations by applying diverse color, blur, noise, and geometric transforms. Then we use the DNN's logits output to train a simple random forest to predict the real class label. Our method achieves state-of-the-art adversarial robustness on a diversity of white and black box attacks with minimal compromise on the natural images' classification. We test ARF also against numerous adaptive white-box attacks and it shows excellent results when combined with adversarial training.",https://openaccess.thecvf.com/content/WACV2024/html/Cohen_Simple_Post-Training_Robustness_Using_Test_Time_Augmentations_and_Random_Forest_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cohen_Simple_Post-Training_Robustness_Using_Test_Time_Augmentations_and_Random_Forest_WACV_2024_paper.pdf,,https://github.com/giladcohen/ARF,2109.08191,main,Poster,https://ieeexplore.ieee.org/document/10483881/,"['Training', 'Threat modeling', 'Adaptation models', 'Image color analysis', 'Artificial neural networks', 'Transforms', 'Robustness']","['Random Forest', 'Test-time Augmentation', 'Deep Neural Network', 'Natural Images', 'Color Variation', 'Adversarial Training', 'White Box', 'Adversarial Attacks', 'White-box Attack', 'Pre-trained Deep Neural Networks', 'Adversarial Robustness', 'Black-box Attacks', 'Output Logits', 'Training Data', 'Computation Time', 'Step Size', 'Input Image', 'Machine Learning Classifiers', 'Random Forest Classifier', 'Adversarial Examples', 'Random Forest Training', 'Threat Model', 'Image X', 'Defense Techniques', 'Fast Gradient Sign Method', 'Vanilla', 'Random Training', 'Output Of The Deep Neural Network', 'Embedding Vectors']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",,"Although Deep Neural Networks (DNNs) achieve excellent performance on many real-world tasks, they are highly vulnerable to adversarial attacks. A leading defense against such attacks is adversarial training, a technique in which a DNN is trained to be robust to adversarial attacks by introducing adversarial noise to its input. This procedure is effective but must be done during the training phase. In this work, we propose Augmented Random Forest (ARF), a simple and easy-to-use strategy for robustifying an existing pretrained DNN without modifying its weights. For every image, we generate randomized test time augmentations by applying diverse color, blur, noise, and geometric transforms. Then we use the DNN’s logits output to train a simple random forest to predict the real class label. Our method achieves state-of-the-art adversarial robustness on a diversity of white and black box attacks with minimal compromise on the natural images’ classification. We test ARF also against numerous adaptive white-box attacks and it shows excellent results when combined with adversarial training. https://github.com/giladcohen/ARF."
Simple Token-Level Confidence Improves Caption Correctness,"Suzanne Petryk, Spencer Whitehead, Joseph E. Gonzalez, Trevor Darrell, Anna Rohrbach, Marcus Rohrbach",UC Berkeley; Meta,50.0,USA,50.0,USA,"The ability to judge whether a caption correctly describes an image is a critical part of vision-language understanding. However, state-of-the-art models often misinterpret the correctness of fine-grained details, leading to errors in outputs such as hallucinating objects in generated captions or poor compositional reasoning. In this work, we explore Token-Level Confidence, or TLC, as a simple yet surprisingly effective method to assess caption correctness. Specifically, we fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency. Compared to sequence-level scores from pretrained models, TLC with algebraic confidence more than doubles image and group scores for compositional reasoning on Winoground. When training data are available, a learned confidence estimator provides further improved performance, reducing object hallucination rates in MS COCO Captions by a relative 30% over the original model and setting a new state-of-the-art.",https://openaccess.thecvf.com/content/WACV2024/html/Petryk_Simple_Token-Level_Confidence_Improves_Caption_Correctness_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Petryk_Simple_Token-Level_Confidence_Improves_Caption_Correctness_WACV_2024_paper.pdf,,,2305.07021,main,Poster,https://ieeexplore.ieee.org/document/10484304/,"['Computer vision', 'Aggregates', 'Training data', 'Cognition', 'Data models']","['Training Data', 'Hallucinations', 'Confidence Estimation', 'Image Captioning', 'Training Set', 'Time Step', 'Image Features', 'Validation Set', 'Additional Details', 'Autoregressive', 'Model Size', 'Language Model', 'Part-of-speech', 'Inference Time', 'Standard Metrics', 'Matching Score', 'Transformer Encoder', 'Sequence Of Tokens', 'Beam Search', 'Decoding Method', 'Algebraic Function']","['Algorithms', 'Vision + language and/or other modalities']",,"The ability to judge whether a caption correctly describes an image is a critical part of vision-language understanding. However, state-of-the-art models often misinterpret the correctness of fine-grained details, leading to errors in outputs such as hallucinating objects in generated captions or poor compositional reasoning. In this work, we explore Token-Level Confidence, or TLC, as a simple yet surprisingly effective method to assess caption correctness. Specifically, we fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency. Compared to sequence-level scores from pretrained models, TLC with algebraic confidence measures achieves a relative improvement in accuracy by 10% on verb understanding in SVO-Probes and more than doubles image and group scores for compositional reasoning in Winoground. When training data are available, a learned confidence estimator provides further improved performance, reducing object hallucination rates in MS COCO Captions by a relative 30% over the original model and setting a new state-of-the-art."
SimpliMix: A Simplified Manifold Mixup for Few-Shot Point Cloud Classification,"Minmin Yang, Weiheng Chai, Jiyang Wang, Senem Velipasalar",Syracuse University,100.0,USA,0.0,,"Few-shot learning often assumes that base classes are abundant and diverse with plentiful well-labeled samples for each class. This ensures that models can generalize effectively from a small amount of data by leveraging prior knowledge learned from base classes. This assumption holds for 2D few-shot learning since the benchmark datasets are large and diverse. However, 3D point cloud few-shot benchmarks are low in magnitude and diversity. We conduct experiments and show that many existing methods overlook this issue and suffer from overfitting on base classes, which hinders generalization ability and test performance. To alleviate the overfitting issue, we propose a simplified manifold mixup, referred to as the SimpliMix, which mixes hidden representations and forces the models to learn more generalized features. We incorporate SimpliMix into existing prototype-based models, perform experiments on ModelNet40-FS, ModelNet40-C-FS and ScanObjectNN-FS datasets, and improve the models by a significant margin. We further conduct cross-domain few-shot classification experiments and show that networks with SimpliMix learn more generalized and transferable features and achieve better performance. The code is available at https://github.com/LexieYang/SimpliMix",https://openaccess.thecvf.com/content/WACV2024/html/Yang_SimpliMix_A_Simplified_Manifold_Mixup_for_Few-Shot_Point_Cloud_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yang_SimpliMix_A_Simplified_Manifold_Mixup_for_Few-Shot_Point_Cloud_Classification_WACV_2024_paper.pdf,,https://github.com/LexieYang/SimpliMix,,main,Poster,https://ieeexplore.ieee.org/document/10484267/,"['Point cloud compression', 'Manifolds', 'Adaptation models', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Computational modeling']","['Point Cloud', 'Point Cloud Classification', 'Manifold Mixup', 'Generalization Ability', 'Base Classes', '3D Point Cloud', 'Few-shot Learning', 'Few-shot Classification', 'Convolutional Neural Network', 'Image Classification', 'Image Dataset', 'Singular Value', 'Mixed Samples', 'Singular Value Decomposition', 'Generative Adversarial Networks', 'Airplane', 'Depth Images', 'Beta Distribution', 'Decision Boundary', 'Mixed Data', 'Support Set', '3D Datasets', 'Query Sample', 'Point Cloud Data', 'Query Set', 'CAD Model', 'Soft Labels', 'Partial Point', 'Point Cloud Features', 'Source Domain']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Few-shot learning often assumes that base classes are abundant and diverse with plentiful well-labeled samples for each class. This ensures that models can generalize effectively from a small amount of data by leveraging prior knowledge learned from base classes. This assumption holds for 2D few-shot learning since the benchmark datasets are large and diverse. However, 3D point cloud few-shot benchmarks are low in magnitude and diversity. We conduct experiments and show that many existing methods overlook this issue and suffer from overfitting on base classes, which hinders generalization ability and test performance. To alleviate the overfitting issue, we propose a simplified manifold mixup, referred to as the SimpliMix, which mixes hidden representations and forces the models to learn more generalized features. We incorporate SimpliMix into existing prototype-based models, perform experiments on ModelNet40-FS, ModelNet40-C-FS and ScanObjectNN-FS datasets, and improve the models by a significant margin. We further conduct cross-domain few-shot classification experiments and show that networks with SimpliMix learn more generalized and transferable features and achieve better performance. The code is available at https://github.com/LexieYang/SimpliMix"
Single Domain Generalization via Normalised Cross-Correlation Based Convolutions,"WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, David Suter, Alireza Bab-Hadiashar","Edith Cowan University (ECU), Australia; RMIT University, Australia",100.0,"Australia, USA",0.0,,"Deep learning techniques often perform poorly in the presence of domain shift, where the test data follows a different distribution than the training data. The most practically desirable approach to address this issue is Single Domain Generalization (S-DG), which aims to train robust models using data from a single source. Prior work on S-DG has primarily focused on using data augmentation techniques to generate diverse training data. In this paper, we explore an alternative approach by investigating the robustness of linear operators, such as convolution and dense layers commonly used in deep learning. We propose a novel operator called XCNorm that computes the normalized cross-correlation between weights and an input feature patch. This approach is invariant to both affine shifts and changes in energy within a local feature patch and eliminates the need for commonly used non-linear activation functions. We show that deep neural networks composed of this operator are robust to common semantic distribution shifts. Furthermore, our empirical results on single-domain generalization benchmarks demonstrate that our proposed technique performs comparably to the state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Chuah_Single_Domain_Generalization_via_Normalised_Cross-Correlation_Based_Convolutions_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chuah_Single_Domain_Generalization_via_Normalised_Cross-Correlation_Based_Convolutions_WACV_2024_paper.pdf,,https://github.com/...,2307.05901,main,Poster,https://ieeexplore.ieee.org/document/10483833/,"['Deep learning', 'Sensitivity', 'Convolution', 'Computational modeling', 'Semantics', 'Training data', 'Data augmentation']","['Single Domain', 'Domain Generalization', 'Normalized Cross-correlation', 'Single Domain Generalization', 'Neural Network', 'Training Data', 'Deep Neural Network', 'Energy Change', 'Input Features', 'Data Augmentation', 'Dense Layer', 'Domain Shift', 'Deep Learning Techniques', 'Linear Operator', 'Data Augmentation Techniques', 'Patch Features', 'Semantic Change', 'Training Set', 'Convolutional Neural Network', 'Feature Maps', 'Source Domain', 'Target Domain', 'Unseen Domains', 'Gradient Scale', 'Input Patch', 'Medical Image Classification', 'Types Of Corruption', 'Digital Pathology', 'Inductive Bias', 'Feature Channels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Deep learning techniques often perform poorly in the presence of domain shift, where the test data follows a different distribution than the training data. The most practically desirable approach to address this issue is Single Domain Generalization (S-DG), which aims to train robust models using data from a single source. Prior work on S-DG has primarily focused on using data augmentation techniques to generate diverse training data. In this paper, we explore an alternative approach by investigating the robustness of linear operators, such as convolution and dense layers commonly used in deep learning. We propose a novel operator called ""XCNorm"" that computes the normalized cross-correlation between weights and an input feature patch. This approach is invariant to both affine shifts and changes in energy within a local feature patch and eliminates the need for commonly used non-linear activation functions. We show that deep neural networks composed of this operator are robust to common semantic distribution shifts. Furthermore, our empirical results on single-domain generalization benchmarks demonstrate that our proposed technique performs comparably to the stateof-the-art methods.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Single Frame Semantic Segmentation Using Multi-Modal Spherical Images,"Suresh Guttikonda, Jason Rambach",German Research Center for Artificial Intelligence (DFKI),0.0,,100.0,Germany,"In recent years, the research community has shown a lot of interest to panoramic images that offer a 360-degree directional perspective. Multiple data modalities can be fed, and complimentary characteristics can be utilized for more robust and rich scene interpretation based on semantic segmentation, to fully realize the potential. Existing research, however, mostly concentrated on pinhole RGB-X semantic segmentation. In this study, we propose a transformer-based cross-modal fusion architecture to bridge the gap between multi-modal fusion and omnidirectional scene perception. We employ distortion-aware modules to address extreme object deformations and panorama distortions that result from equirectangular representation. Additionally, we conduct cross-modal interactions for feature rectification and information exchange before merging the features in order to communicate long-range contexts for bi-modal and tri-modal feature streams. In thorough tests using combinations of four different modality types in three indoor panoramic-view datasets, our technique achieved state-of-the-art mIoU performance: 60.60% on Stanford2D3DS (RGB-HHA), 71.97% on Structured3D (RGB-D-N), and 35.92% on Matterport3D (RGB-D).",https://openaccess.thecvf.com/content/WACV2024/html/Guttikonda_Single_Frame_Semantic_Segmentation_Using_Multi-Modal_Spherical_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Guttikonda_Single_Frame_Semantic_Segmentation_Using_Multi-Modal_Spherical_Images_WACV_2024_paper.pdf,,https://github.com/sguttikon/SFSS-MMSI,2308.09369,main,Poster,https://ieeexplore.ieee.org/document/10484419/,"['Three-dimensional displays', 'Laser radar', 'Deformation', 'Semantic segmentation', 'Merging', 'Computer architecture', 'Distortion']","['Semantic Segmentation', 'Spherical Image', 'Pinhole', 'Features In Order', 'Panoramic Images', 'Deformable Objects', 'Cross-modal Interactions', 'Fusion Architecture', 'Feature Maps', 'Geometric Shapes', 'Training Epochs', 'Feature Fusion', 'Image Distortion', 'Test Split', 'Multimodal Features', 'Vision Transformer', 'Feature Fusion Module', 'Embedding Module']","['Algorithms', '3D computer vision']",3,"In recent years, the research community has shown a lot of interest to panoramic images that offer a 360° directional perspective. Multiple data modalities can be fed, and complimentary characteristics can be utilized for more robust and rich scene interpretation based on semantic segmentation, to fully realize the potential. Existing research, however, mostly concentrated on pinhole RGB-X semantic segmentation. In this study, we propose a transformer-based cross-modal fusion architecture to bridge the gap between multi-modal fusion and omnidirectional scene perception. We employ distortion-aware modules to address extreme object deformations and panorama distortions that result from equirectangular representation. Additionally, we conduct cross-modal interactions for feature rectification and information exchange before merging the features in order to communicate long-range contexts for bi-modal and tri-modal feature streams. In thorough tests using combinations of four different modality types in three indoor panoramic-view datasets, our technique achieved state-of-the-art mIoU performance: 60.60% on Stanford2D3DS [2] (RGB-HHA), 71.97% on Structured3D [44] (RGB-D-N), and 35.92% on Matterport3D [5] (RGB-D). 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
"Single-Image Deblurring, Trajectory and Shape Recovery of Fast Moving Objects With Denoising Diffusion Probabilistic Models","Radim Spetlik, Denys Rozumnyi, Jiří Matas","Czech Technical University in Prague, Faculty of Electrical Engineering; Department of Computer Science, ETH Zurich; Czech Technical University in Prague, Faculty of Electrical Engineering",100.0,"Czech Republic, Switzerland",0.0,,"Blurry appearance of fast moving objects in video frames was successfully used to reconstruct the object appearance and motion in both 2D and 3D domains. The proposed method addresses the novel, severely ill-posed, task of single-image fast moving object deblurring, shape, and trajectory recovery -- previous approaches require at least three consecutive video frames. Given a single image, the method outputs the object 2D appearance and position in a series of sub-frames as if captured by a high-speed camera (i.e. temporal super-resolution). The proposed SI-DDPM-FMO method is trained end-to-end on a synthetic dataset with various moving objects, yet it generalizes well to real-world data from several publicly available datasets. SI-DDPM-FMO performs similarly to or better than recent multi-frame methods and a carefully designed baseline method.",https://openaccess.thecvf.com/content/WACV2024/html/Spetlik_Single-Image_Deblurring_Trajectory_and_Shape_Recovery_of_Fast_Moving_Objects_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Spetlik_Single-Image_Deblurring_Trajectory_and_Shape_Recovery_of_Fast_Moving_Objects_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483672/,"['Three-dimensional displays', 'Shape', 'Noise reduction', 'Superresolution', 'Estimation', 'Probabilistic logic', 'Trajectory']","['Denoising', 'Fast-moving Objects', 'Single Image Deblurring', 'Diffusion Probabilistic Models', 'Single Image', 'Video Frames', 'Baseline Methods', 'High-speed Camera', 'Consecutive Frames', 'Object Appearance', 'Serial Position', 'Benchmark', 'Neural Network', 'Time Step', 'Training Dataset', 'Center Of Mass', 'Fair Comparison', 'Gaussian Noise', '3D Reconstruction', 'Complex Shapes', 'Trajectory Estimation', 'Rotational Symmetry', 'Blur Kernel', 'Image Sharpness', 'Single Input Image', 'Spherical Objects', 'Peak Signal-to-noise Ratio', 'Structural Similarity Index Measure', 'Prediction Network', 'High-speed Video']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Low-level and physics-based vision']",1,"Blurry appearance of fast moving objects in video frames was successfully used to reconstruct the object appearance and motion in both 2D and 3D domains. The proposed method addresses the novel, severely ill-posed, task of single-image fast moving object deblurring, shape, and trajectory recovery – previous approaches require at least three consecutive video frames. Given a single image, the method outputs the object 2D appearance and position in a series of sub-frames as if captured by a high-speed camera (i.e. temporal super-resolution). The proposed SI-DDPM-FMO method is trained end-to-end on a synthetic dataset with various moving objects, yet it generalizes well to real-world data from several publicly available datasets. SI-DDPM-FMO performs similarly to or better than recent multi-frame methods and a carefully designed baseline method."
Sketch-Based Video Object Localization,"Sangmin Woo, So-Yeong Jeon, Jinyoung Park, Minji Son, Sumin Lee, Changick Kim","KAIST; LG Electronics; KAIST, Korea Agency for Defense Development",66.66666666666666,South Korea,33.33333333333334,South Korea,"We introduce Sketch-based Video Object Localization (SVOL), a new task aimed at localizing spatio-temporal object boxes in video queried by the input sketch. We first outline the challenges in the SVOL task and build the Sketch-Video Attention Network (SVANet) with the following design principles: (i) to consider temporal information of video and bridge the domain gap between sketch and video; (ii) to accurately identify and localize multiple objects simultaneously; (iii) to handle various styles of sketches; (iv) to be classification-free. In particular, SVANet is equipped with a Cross-modal Transformer that models the interaction between learnable object tokens, query sketch, and video through attention operations, and learns upon a per-frame set matching strategy that enables frame-wise prediction while utilizing global video context. We evaluate SVANet on a newly curated SVOL dataset. By design, SVANet successfully learns the mapping between the query sketches and video objects, achieving state-of-the-art results on the SVOL benchmark. We further confirm the effectiveness of SVANet via extensive ablation studies and visualizations. Lastly, we demonstrate its transfer capability on unseen datasets and novel categories, suggesting its high scalability in real-world applications. Codes are available at https://github.com/sangminwoo/SVOL.",https://openaccess.thecvf.com/content/WACV2024/html/Woo_Sketch-Based_Video_Object_Localization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Woo_Sketch-Based_Video_Object_Localization_WACV_2024_paper.pdf,,https://github.com/sangminwoo/SVOL,2304.00450,main,Poster,https://ieeexplore.ieee.org/document/10483837/,"['Location awareness', 'Bridges', 'Visualization', 'Computer vision', 'Scalability', 'Predictive models', 'Transformers']","['Object Location', 'Global Context', 'Multiple Objects', 'Attention Operation', 'Localization Accuracy', 'Bounding Box', 'Input Sequence', 'Video Frames', 'Target Object', 'Thick Line', 'Symbolic Meaning', 'Matching Results', 'Object Parts', 'Temporal Context', 'L1 Loss', 'Objective Scores', 'Strong Baseline', 'Transfer Task', 'Positional Encoding', 'Video Object', 'Ground-truth Box', 'Matching Cost', 'Prediction Head', 'Box Coordinates', 'Video Dataset', 'Bounding Box Location', 'Poor Performance', 'Ground Truth Set']","['Applications', 'Smartphones / end user devices', 'Applications', 'Arts / games / social media']",,"We introduce Sketch-based Video Object Localization (SVOL), a new task aimed at localizing spatio-temporal object boxes in video queried by the input sketch. We first outline the challenges in the SVOL task and build the Sketch-Video Attention Network (SVANet) with the following design principles: (i) to consider temporal information of video and bridge the domain gap between sketch and video; (ii) to accurately identify and localize multiple objects simultaneously; (iii) to handle various styles of sketches; (iv) to be classification-free. In particular, SVANet is equipped with a Cross-modal Transformer that models the interaction between learnable object tokens, query sketch, and video through attention operations, and learns upon a per-frame set matching strategy that enables frame-wise prediction while utilizing global video context. We evaluate SVANet on a newly curated SVOL dataset. By design, SVANet successfully learns the mapping between the query sketches and video objects, achieving state-of-the-art results on the SVOL benchmark. We further confirm the effectiveness of SVANet via extensive ablation studies and visualizations. Lastly, we demonstrate its transfer capability on unseen datasets and novel categories, suggesting its high scalability in real-world applications. Codes are available at https://github.com/sangminwoo/SVOL."
Slice and Conquer: A Planar-to-3D Framework for Efficient Interactive Segmentation of Volumetric Images,"Wonwoo Cho, Dongmin Choi, Hyesu Lim, Jinho Choi, Saemee Choi, Hyun-seok Min, Sungbin Lim, Jaegul Choo","Letsur Inc., KAIST; KAIST; Korea University; Tomocube Inc.",75.0,South Korea,25.0,South Korea,"Interactive segmentation methods have been investigated to address the potential need for additional refinement in automatic segmentation via human-in-the-loop techniques. For accurate segmentation of 3D images, we propose Slice-and-Conquer, a novel planar-to-3D pipeline formulating volumetric mask construction into two stages: 1) 2D interactive segmentation and 2) guided 3D segmentation. Specifically, the first stage enables users to focus on a single 2D slice and provides the corresponding 2D prediction results as strong shape priors. Taking the planar guidance, an accurate 3D mask can be constructed with minimal interactions. To support a flexible iterative refinement, our system recommends a next slice to annotate at the end of the second stage. Since volumetric segmentation can be completed by consecutively annotating a few recommended 2D slices, our method significantly reduces the cognitive burden of exploring volumetric space for users. Through extensive experiments on various datasets of 3D biomedical images, we demonstrate the effectiveness of the proposed pipeline.",https://openaccess.thecvf.com/content/WACV2024/html/Cho_Slice_and_Conquer_A_Planar-to-3D_Framework_for_Efficient_Interactive_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cho_Slice_and_Conquer_A_Planar-to-3D_Framework_for_Efficient_Interactive_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Small Objects Matters in Weakly-Supervised Semantic Segmentation,"Cheolhyun Mun, Sanghuk Lee, Youngjung Uh, Junsuk Choe, Hyeran Byun","Sogang University, Seoul, Korea; Yonsei University, Seoul, Korea; Samsung Research, Seoul, Korea; SOCAR AI Research, Seoul, Korea",50.0,South Korea,50.0,South Korea,"Weakly-supervised semantic segmentation (WSSS) performs pixel-wise classification given only image-level labels for training. Despite the difficulty of this task, the research community has achieved promising results over the last five years. Still, current WSSS literature misses the detailed sense of how well the methods perform on different sizes of objects. Thus we propose a novel evaluation metric to provide a comprehensive assessment across different object sizes and collect a size-balanced evaluation set to complement PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods struggle in capturing small objects. Furthermore, we propose a size-balanced cross-entropy loss coupled with a proper training strategy. It generally improves existing WSSS methods as validated upon ten baselines on three different datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mun_Small_Objects_Matters_in_Weakly-Supervised_Semantic_Segmentation_WACV_2024_paper.pdf,,,2309.14117,main,Poster,https://ieeexplore.ieee.org/document/10484406/,"['Training', 'Measurement', 'Computer vision', 'Semantic segmentation', 'Task analysis']","['Semantic Segmentation', 'Small Objects', 'Weakly Supervised Semantic Segmentation', 'Comprehensive Assessment', 'Cross-entropy Loss', 'Training Strategy', 'Object Size', 'Objects Of Different Sizes', 'Loss Function', 'Benchmark', 'Model Performance', 'Bounding Box', 'Large Objects', 'Imbalance Problem', 'Imbalanced Datasets', 'Training Objective', 'Imbalanced Distribution', 'Semantic Segmentation Task', 'Fisher Information Matrix', 'Large Instances', 'Small Instances', 'PASCAL VOC Dataset', 'Instance Size', 'Class Activation Maps', 'Corner Cases', 'MS COCO Dataset', 'Pixel Accuracy', 'Semantic Segmentation Models', 'Pixel Classification', 'Segmentation Model']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Datasets and evaluations']",,"Weakly-supervised semantic segmentation (WSSS) performs pixel-wise classification given only image-level labels for training. Despite the difficulty of this task, the research community has achieved promising results over the last five years. Still, current WSSS literature misses the detailed sense of how well the methods perform on different sizes of objects. Thus we propose a novel evaluation metric to provide a comprehensive assessment across different object sizes and collect a size-balanced evaluation set to complement PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods struggle in capturing small objects. Furthermore, we propose a size-balanced cross-entropy loss coupled with a proper training strategy. It generally improves existing WSSS methods as validated upon ten baselines on three different datasets."
So You Think You Can Track?,"Derek Gloudemans, Gergely Zachár, Yanbing Wang, Junyi Ji, Matt Nice, Matt Bunting, William W. Barbour, Jonathan Sprinkle, Benedetto Piccoli, Maria Laura Delle Monache, Alexandre Bayen, Benjamin Seibold, Daniel B. Work",Temple University; Rutgers University-Camden; UC Berkeley; Vanderbilt University,100.0,USA,0.0,,"This work introduces a multi-camera tracking dataset consisting of 234 hours of video data recorded concurrently from 234 overlapping HD cameras covering a 4.2 mile stretch of 8-10 lane interstate highway near Nashville, TN. The video is recorded during a period of high traffic density with 500+ objects typically visible within the scene and typical object longevities of 3-15 minutes. GPS trajectories from 270 vehicle passes through the scene are manually corrected in the video data to provide a set of ground-truth trajectories for recall-oriented tracking metrics, and object detections are provided for each camera in the scene (159 million total before cross-camera fusion). Initial benchmarking of tracking-by-detection algorithms is performed against the GPS trajectories, and a best HOTA of only 9.5% is obtained (best recall 75.9% at IOU 0.1, 47.9 average IDs per ground truth object), indicating the benchmarked trackers do not perform sufficiently well at the long temporal and spatial durations required for traffic scene understanding.",https://openaccess.thecvf.com/content/WACV2024/html/Gloudemans_So_You_Think_You_Can_Track_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gloudemans_So_You_Think_You_Can_Track_WACV_2024_paper.pdf,http://i24motion.org,,,main,Poster,https://ieeexplore.ieee.org/document/10484183/,"['Road transportation', 'Measurement', 'Transportation', 'Object detection', 'Benchmark testing', 'Cameras', 'Trajectory']","['Object Detection', 'Interstate', 'Video Data', 'Hourly Data', 'Vehicle Trajectory', 'Scene Information', 'Ground Truth Object', 'Ground Truth Trajectory', 'GPS Trajectories', 'Coordinate System', 'Pedestrian', 'Bounding Box', 'Corresponding Points', 'Manual Annotation', 'Motion Model', 'Object Position', 'Orthonormal Basis', 'Object Tracking', 'Direction Of Travel', 'Image Coordinates', 'Multiple Object Tracking', 'Hours Of Video', 'Single Scene', 'Primary Axis', 'GPS Tracking', 'Subset Of Points', 'Lane Markings', 'Video Dataset', 'Homography Matrix', 'Tracking Algorithm']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Video recognition and understanding']",3,"This work introduces a multi-camera tracking dataset consisting of 234 hours of video data recorded concurrently from 234 overlapping HD cameras covering a 4.2 mile stretch of 8-10 lane interstate highway near Nashville, TN. Video is recorded in cooperation with Tennessee State Department of Transportation and its policies. The video is recorded during a period of high traffic density with 500+ objects typically visible within the scene and typical object longevities of 3-15 minutes. GPS trajectories from 270 vehicle passes through the scene are manually corrected in the video data to provide a set of ground-truth trajectories for recall-oriented tracking metrics, and object detections are provided for each camera in the scene (159 million total before cross-camera fusion). Initial benchmarking of tracking-by-detection algorithms is performed against the GPS trajectories, and a best HOTA of only 9.5% is obtained (best recall 75.9% at IOU 0.1, 47.9 average IDs per ground truth object), indicating the benchmarked trackers do not perform sufficiently well at the long temporal and spatial durations required for traffic scene understanding. Video data, scene information, and vehicle trajectories are made publicly available at i24motion.org."
Soft Curriculum for Learning Conditional GANs With Noisy-Labeled and Uncurated Unlabeled Data,"Kai Katsumata, Duc Minh Vo, Tatsuya Harada, Hideki Nakayama","The University of Tokyo; The University of Tokyo, RIKEN",100.0,Japan,0.0,,"Label-noise or curated unlabeled data are used to compensate for the assumption of clean labeled data in training the conditional generative adversarial network; however, satisfying such an extended assumption is occasionally laborious or impractical. As a step towards generative modeling accessible to everyone, we introduce a novel conditional image generation framework that accepts noisy-labeled and uncurated unlabeled data during training: (i) closed-set and open-set label noise in labeled data and (ii) closed-set and open-set unlabeled data. To combat it, we propose soft curriculum learning, which assigns instance-wise weights for adversarial training while assigning new labels for unlabeled data and correcting wrong labels for labeled data. Unlike popular curriculum learning, which uses a threshold to pick the training samples, our soft curriculum controls the effect of each training instance by using the weights predicted by the auxiliary classifier, resulting in the preservation of useful samples while ignoring harmful ones. Our experiments show that our approach outperforms existing semi-supervised and label-noise robust methods in terms of both quantitative and qualitative performance. In particular, the proposed approach matches the performance of (semi-)supervised GANs even with less than half the labeled data.",https://openaccess.thecvf.com/content/WACV2024/html/Katsumata_Soft_Curriculum_for_Learning_Conditional_GANs_With_Noisy-Labeled_and_Uncurated_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Soft_Curriculum_for_Learning_Conditional_GANs_With_Noisy-Labeled_and_Uncurated_WACV_2024_paper.pdf,,https://github.com/raven38/NOSSGAN,2307.08319,main,Poster,https://ieeexplore.ieee.org/document/10484140/,"['Training', 'Computer vision', 'Image synthesis', 'Noise', 'Computer architecture', 'Generative adversarial networks', 'Data models']","['Unlabeled Data', 'Conditional Generative Adversarial Network', 'Generative Adversarial Networks', 'Image Generation', 'Curriculum Learning', 'Label Noise', 'Auxiliary Classifier', 'Cross-entropy Loss', 'ImageNet', 'Image Recognition', 'Noisy Data', 'Labeled Samples', 'Classification Loss', 'Quantitative Metrics', 'Semi-supervised Learning', 'Correct Label', 'Label Space', 'Assignment Of Data', 'Noisy Labels', 'Usage Ratio', 'Fréchet Inception Distance', 'Robust Learning', 'Extra Noise', 'Soft Labels', 'Imperfect Data']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",,"Label-noise or curated unlabeled data are used to compensate for the assumption of clean labeled data in training the conditional generative adversarial network; however, satisfying such an extended assumption is occasionally laborious or impractical. As a step towards generative modeling accessible to everyone, we introduce a novel conditional image generation framework that accepts noisy-labeled and uncurated unlabeled data during training: (i) closed-set and open-set label noise in labeled data and (ii) closed-set and open-set unlabeled data. To combat it, we propose soft curriculum learning, which assigns instance-wise weights for adversarial training while assigning new labels for unlabeled data and correcting wrong labels for labeled data. Unlike popular curriculum learning, which uses a threshold to pick the training samples, our soft curriculum controls the effect of each training instance by using the weights predicted by the auxiliary classifier, resulting in the preservation of useful samples while ignoring harmful ones. Our experiments show that our approach outperforms existing semi-supervised and label-noise robust methods in terms of both quantitative and qualitative performance. In particular, the proposed approach matches the performance of (semi-)supervised GANs even with less than half the labeled data.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Solving the Plane-Sphere Ambiguity in Top-Down Structure-From-Motion,"Lars Haalck, Benjamin Risse","Institute for Geoinformatics and Institute for Computer Science, University of Münster, Germany",100.0,Germany,0.0,,"Drone-based land surveys and tracking applications with a moving camera require three-dimensional reconstructions from videos recorded using a downward facing camera and are usually generated by Structure-from-Motion (SfM) algorithms. Unfortunately, monocular SfM pipelines can fail in the presence of lens distortion due to a critical configuration resulting in a plane-sphere ambiguity which is characterized by severe curvatures of the reconstructions and erroneous relative camera pose estimations. We propose a 4-point minimal solver for the relative pose estimation for two views sharing the same radial distortion parameters (i.e. from the same camera) with a viewing direction perpendicular to the ground plane. To extract 3D reconstructions from continuous videos, the relative pose of pairwise frames is estimated by using the solver with RANSAC and the Sampson error where globally consistent distortion parameters are determined by taking the medial of all values. Moreover, we propose an additional regularizer for the final bundle adjustment to remove any remaining curvature of the reconstruction if necessary. We tested our methods on synthetic and real-world data and our results demonstrate a significant reduction of curvature and more accurate relative pose estimations. Our algorithm can be easily integrated into existing pipelines and is therefore a practical solution to resolve the plane-sphere ambiguity in a variety of top-down SfM applications.",https://openaccess.thecvf.com/content/WACV2024/html/Haalck_Solving_the_Plane-Sphere_Ambiguity_in_Top-Down_Structure-From-Motion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Haalck_Solving_the_Plane-Sphere_Ambiguity_in_Top-Down_Structure-From-Motion_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Sound3DVDet: 3D Sound Source Detection Using Multiview Microphone Array and RGB Images,"Yuhang He, Sangyun Shin, Anoop Cherian, Niki Trigoni, Andrew Markham","Mitsubishi Electric Research Labs, Cambridge, MA, US; Department of Computer Science, University of Oxford, Oxford, UK",100.0,"UK, USA",0.0,,"Spatial localization of 3D sound sources is an important problem in many real world scenarios, especially when the sources may not have any visually distinguishable characteristics; e.g., finding a gas leak, a malfunctioning motor, etc. In this paper, we cast this task in a novel audio-visual setting, by introducing an acoustic-camera rig consisting of a centered pinhole RGB camera and an uniform circular array of four coplanar microphones. Using this setup, we propose Sound3DVDet - a 3D sound source localization Transformer model that takes as input the neural embeddings of the sound signals from the microphones and multiview images (with known poses), and learns to minimize the reprojection error between the predicted locations of the sound sources by the two modalities and the ground truth as the camera moves. When training to minimize this consistency loss, the model learns an implicit association between the audio heard at the microphones and the 3D spatial location in the RGB image, which is sufficient to localize the sources in 3D from a single RGB view. To evaluate our method, we introduce a new dataset: Sound3DVDet Dataset, consisting of nearly 6k scenes produced using the SoundSpaces simulator. We conduct extensive experiments on our dataset and shows the efficacy of our approach against closely related methods, demonstrating significant improvements in the localization accuracy.",https://openaccess.thecvf.com/content/WACV2024/html/He_Sound3DVDet_3D_Sound_Source_Detection_Using_Multiview_Microphone_Array_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/He_Sound3DVDet_3D_Sound_Source_Detection_Using_Multiview_Microphone_Array_and_WACV_2024_paper.pdf,,https://github.com/yuhanghe01/Sound3DVDet,,main,Poster,https://ieeexplore.ieee.org/document/10483915/,"['Location awareness', 'Solid modeling', 'Three-dimensional displays', 'Predictive models', 'Position measurement', 'Transformers', 'Motors']","['RGB Images', 'Sound Source', 'Microphone Array', 'Multi-view RGB Images', 'Intermediate Layer', '3D Position', 'Sound Localization', 'Uniform Array', 'Single View', 'Gas Leakage', 'Bipartite Matching', 'Sound Cues', 'Neural Network', 'Image Features', 'Object Detection', 'Class Labels', 'Feed-forward Network', 'Distance Threshold', 'Mean Average Precision', 'Multi-view Images', 'Detection Head', 'Camera Pose', 'Sound Detection', 'Deep Supervision', 'Direction Of Arrival', '3D Detection', 'Physical Surface', 'Short-time Fourier Transform', 'Camera Coordinate System']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"Spatial localization of 3D sound sources is an important problem in many real world scenarios, especially when the sources may not have any visually distinguishable characteristic; e.g., finding a gas leak, a malfunctioning motor, etc. In this paper, we cast this task in a novel audio-visual setting, by introducing an acoustic-camera rig consisting of a centered pinhole RGB camera and a uniform circular array of four coplanar microphones. Using this setup, we propose Sound3DVDet – a 3D sound source localization Transformer model that treats this task as a set prediction problem. It first learns a set of initial sound source locations (dubbed queries) from a single view of the microphone array signal, then feeds the query set to a sequence of Transformerlike layers for refinement. Each query arising from each layer repeatedly aggregates sound source cues from other views. We deeply supervise the initial sound source queries, intermediate layer queries, and the final output by measuring their respective discrepancy against ground truth queries via bipartite matching. To evaluate our method, we introduce a new dataset: Sound3DVDet Dataset, consisting of nearly 6k scenes produced using the SoundSpaces simulator. We conduct extensive experiments on our dataset and show the efficacy of our approach against closely related methods, demonstrating significant improvements in the localization accuracy. Code is available at https://github.com/yuhanghe01/Sound3DVDet."
Source-Guided Similarity Preservation for Online Person Re-Identification,"Hamza Rami, Jhony H. Giraldo, Nicolas Winckler, Stéphane Lathuilière","Atos.; LTCI, Télémécom Paris, Institut Polytechnique de Paris.",50.0,France,50.0,France,"Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source-domain dataset to a target domain observed as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift. In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Rami_Source-Guided_Similarity_Preservation_for_Online_Person_Re-Identification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rami_Source-Guided_Similarity_Preservation_for_Online_Person_Re-Identification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484465/,"['Adaptation models', 'Data privacy', 'Computer vision', 'Streaming media', 'Benchmark testing', 'Feature extraction', 'Regulation']","['Domain Shift', 'Target Domain', 'Target Data', 'Domain Adaptation', 'Support Set', 'Unsupervised Domain Adaptation Methods', 'Catastrophic Forgetting', 'Feature Space', 'Teacher Model', 'Similarity Matrix', 'Target Image', 'Additional Loss', 'Lifelong Learning', 'Privacy Protection', 'Incremental Learning', 'Source Domain', 'Student Model', 'Target Dataset', 'Previous Tasks', 'Pseudo Labels', 'Maximum Mean Discrepancy', 'Unlabeled Target Data', 'Student Network', 'Strong Baseline', 'Triplet Loss', 'Intermediate Domain', 'Bounding Box', 'Teacher Network', 'Loss Function']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"Online Unsupervised Domain Adaptation (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source-domain dataset to a target domain observed as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and domain shift. In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA benchmarks."
Sparse Convolutional Networks for Surface Reconstruction From Noisy Point Clouds,"Tao Wang, Jing Wu, Ze Ji, Yu-Kun Lai",Cardiff University,100.0,UK,0.0,,"Reconstructing accurate 3D surfaces from noisy point clouds is a fundamental problem in computer vision. Among different approaches, neural implicit methods that map 3D coordinates to occupancy values benefit from the learning capabilities of deep neural networks and the flexible topology of implicit representations, and achieve promising reconstruction results. However, existing methods utilize standard (dense) 3D convolutional neural networks for feature extraction and occupancy prediction, which significantly restricts the capability to reconstruct details. In this paper, we propose a neural implicit method based on sparse convolutions, where features and network calculations only focus on grid points close to the surface to be reconstructed. This allows us to build significantly higher resolution 3D grids and reconstruct high-fidelity details. We further build a 3D residual UNet to extract features which are robust to noise, while ensuring details are retained. A 3D position along with features extracted at the position are fed into the occupancy probability predictor network to obtain occupancy. As features at nearby grid points to the query position may not exist due to the sparse nature, we propose a normalized weight interpolation approach to obtain smooth interpolation with sparse data. Experimental results demonstrate that our method achieves promising results, both qualitatively and quantitatively, outperforming existing methods.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_Sparse_Convolutional_Networks_for_Surface_Reconstruction_From_Noisy_Point_Clouds_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Sparse_Convolutional_Networks_for_Surface_Reconstruction_From_Noisy_Point_Clouds_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483767/,"['Point cloud compression', 'Surface reconstruction', 'Interpolation', 'Computer vision', 'Three-dimensional displays', 'Feature extraction', 'Topology']","['Point Cloud', 'Surface Reconstruction', 'Sparse Convolution', 'Noisy Point Clouds', 'High-resolution', 'Neural Network', 'Deep Neural Network', 'Grid Points', 'Reconstruction Results', 'Implicit Method', 'Computer Vision Problems', '3D Grid', 'Occupancy Values', 'Implicit Representation', '3D U-Net', 'Smooth Surface', '3D Reconstruction', 'Grid Cells', 'Intersection Over Union', 'Radial Basis Function', 'Sparse Grid', 'Voxel Grid', 'Trilinear Interpolation', 'Explicit Representation', 'Sparse Tensor', 'Cell Corners', 'Feature Points', '3D Shape', 'Latent Code', 'Chamfer Distance']","['Algorithms', '3D computer vision']",1,"Reconstructing accurate 3D surfaces from noisy point clouds is a fundamental problem in computer vision. Among different approaches, neural implicit methods that map 3D coordinates to occupancy values benefit from the learning capabilities of deep neural networks and the flexible topology of implicit representations, achieving promising reconstruction results. However, existing methods utilize standard (dense) 3D convolutional neural networks for feature extraction and occupancy prediction, which significantly restricts their capability to reconstruct details. In this paper, we propose a neural implicit method based on sparse convolutions, where features and network calculations only focus on grid points close to the surface to be reconstructed. This allows us to build significantly higher resolution 3D grids and reconstruct high-fidelity details. We further build a 3D residual UNet to extract features which are robust to noise, while ensuring details are retained. A 3D position along with features extracted at the position are fed into the occupancy probability predictor network to obtain occupancy. As features at nearby grid points to the query position may not exist due to the sparse nature, we propose a normalized weight interpolation approach to obtain smooth interpolation with sparse data. Experimental results demonstrate that our method achieves promising results, both qualitatively and quantitatively, outperforming existing methods."
Spatio-Temporal Filter Analysis Improves 3D-CNN for Action Classification,"Takumi Kobayashi, Jiaxing Ye",National Institute of Advanced Industrial Science and Technology,100.0,Japan,0.0,,"As 2D-CNNs are growing in image recognition literature, 3D-CNNs are enthusiastically applied to video action recognition. While spatio-temporal (3D) convolution successfully stems from spatial (2D) convolution, it is still unclear how the convolution works for encoding temporal motion patterns in 3D-CNNs. In this paper, we shed light on the mechanism of feature extraction through analyzing the spatio-temporal filters from a temporal viewpoint. The analysis not only describes characteristics of the two action datasets, Something-Something-v2 (SSv2) and Kinetics-400, but also reveals how temporal dynamics are characterized through stacked spatio-temporal convolutions. Based on the analysis, we propose methods to improve temporal feature extraction, covering temporal filter representation and temporal data augmentation. The proposed method contributes to enlarging temporal receptive field of 3D-CNN without touching its fundamental architecture, thus keeping the computation cost. In the experiments on action classification using SSv2 and Kinetics-400, it produces favorable performance improvement of 3D-CNNs.",https://openaccess.thecvf.com/content/WACV2024/html/Kobayashi_Spatio-Temporal_Filter_Analysis_Improves_3D-CNN_for_Action_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kobayashi_Spatio-Temporal_Filter_Analysis_Improves_3D-CNN_for_Action_Classification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484138/,"['Filters', 'Image recognition', 'Costs', 'Three-dimensional displays', 'Convolution', 'Dynamics', 'Computer architecture']","['Action Classes', 'Spatial Patterns', 'Computational Cost', 'Temporal Dynamics', 'Data Augmentation', 'Temporal Features', 'Receptive Field', 'Image Recognition', 'Motion Patterns', 'Temporal Filtering', '3D Convolution', 'Extraction Mechanism', 'Temporal Representation', 'Spatial Convolution', 'Temporal Field', 'Video Action Recognition', 'Sampling Rate', 'Convolutional Neural Network', 'Active Targeting', 'Temporal Dimension', 'Video Sequences', 'Shallow Layers', 'Spatial Filter', 'Augmentation Techniques', 'Filter Weights', 'Empirical Evaluation', 'Single Filter', 'Considerable Improvement', 'Video Frames', 'Deeper Layers']","['Algorithms', 'Video recognition and understanding']",,"As 2D-CNNs are growing in image recognition literature, 3D-CNNs are enthusiastically applied to video action recognition. While spatio-temporal (3D) convolution successfully stems from spatial (2D) convolution, it is still unclear how the convolution works for encoding temporal motion patterns in 3D-CNNs. In this paper, we shed light on the mechanism of feature extraction through analyzing the spatio-temporal filters from a temporal viewpoint. The analysis not only describes characteristics of the two action datasets, Something-Something-v2 (SSv2) and Kinetics-400, but also reveals how temporal dynamics are characterized through stacked spatio-temporal convolutions. Based on the analysis, we propose methods to improve temporal feature extraction, covering temporal filter representation and temporal data augmentation. The proposed method contributes to enlarging temporal receptive field of 3D-CNN without touching its fundamental architecture, thus keeping the computation cost. In the experiments on action classification using SSv2 and Kinetics-400, it produces favorable performance improvement of 3D-CNNs."
SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer From a Spectral Perspective,"Zipeng Xu, Songlong Xing, Enver Sangineto, Nicu Sebe","University of Trento, Italy; University of Modena and Reggio Emilia, Italy",100.0,Italy,0.0,,"Owing to the power of vision-language foundation models, e.g., CLIP, the area of image synthesis has seen recent important advances. Particularly, for style transfer, CLIP enables transferring more general and abstract styles without collecting the style images in advance, as the style can be efficiently described with natural language, and the result is optimized by minimizing the CLIP similarity between the text description and the stylized image. However, directly using CLIP to guide style transfer leads to undesirable artifacts (mainly written words and unrelated visual entities) spread over the image. In this paper, we propose SpectralCLIP, which is based on a spectral representation of the CLIP embedding sequence, where most of the common artifacts occupy specific frequencies. By masking the band including these frequencies, we can condition the generation process to adhere to the target style properties (e.g., color, texture, paint stroke, etc.) while excluding the generation of larger-scale structures corresponding to the artifacts. Experimental results show that SpectralCLIP prevents the generation of artifacts effectively in quantitative and qualitative terms, without impairing the stylisation quality. We also apply SpectralCLIP to text-conditioned image generation and show that it prevents written words in the generated images. Our code is available at https://github.com/zipengxuc/SpectralCLIP.",https://openaccess.thecvf.com/content/WACV2024/html/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xu_SpectralCLIP_Preventing_Artifacts_in_Text-Guided_Style_Transfer_From_a_Spectral_WACV_2024_paper.pdf,,https://github.com/zipengxuc/SpectralCLIP,2303.09270,main,Poster,https://ieeexplore.ieee.org/document/10484323/,"['Visualization', 'Computer vision', 'Codes', 'Image synthesis', 'Image color analysis', 'Natural languages', 'Paints']","['Style Transfer', 'Spectral Perspective', 'Natural Language', 'Image Generation', 'Textual Descriptions', 'Image Synthesis', 'Spectral Representation', 'Frequency Band', 'Single Image', 'Qualitative Results', 'User Study', 'Reference Image', 'Changes In Appearance', 'Image Representation', 'Frequency Of Filter', 'Direct Losses', 'Filtering Strategy', 'Band Combinations', 'Band Selection', 'Multiple Styles', 'Range Of Styles']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities']",1,"Owing to the power of vision-language foundation models, e.g., CLIP, the area of image synthesis has seen recent important advances. Particularly, for style transfer, CLIP enables transferring more general and abstract styles without collecting the style images in advance, as the style can be efficiently described with natural language, and the result is optimized by minimizing the CLIP similarity between the text description and the stylized image. However, directly using CLIP to guide style transfer leads to undesirable artifacts (mainly written words and unrelated visual entities) spread over the image. In this paper, we propose SpectralCLIP, which is based on a spectral representation of the CLIP embedding sequence, where most of the common artifacts occupy specific frequencies. By masking the band including these frequencies, we can condition the generation process to adhere to the target style properties (e.g., color, texture, paint stroke, etc.) while excluding the generation of larger-scale structures corresponding to the artifacts. Experimental results show that SpectralCLIP prevents the generation of artifacts effectively in quantitative and qualitative terms, without impairing the stylisation quality. We also apply SpectralCLIP to text-conditioned image generation and show that it prevents written words in the generated images. Our code is available at https://github.com/zipengxuc/SpectralCLIP."
Spectroformer: Multi-Domain Query Cascaded Transformer Network for Underwater Image Enhancement,"Raqib Khan, Priyanka Mishra, Nancy Mehta, Shruti S. Phutke, Santosh Kumar Vipparthi, Sukumar Nandi, Subrahmanyam Murala","CVPR Lab, Indian Institute of Technology Ropar, India; Computer Vision Lab, CAIDAS, IFI, University of Würzburg, Germany; Indian Institute of Technology Guwahati, India; Institute for Integrated and Intelligent Systems, Griffith Univeristy, Australia; CVPR Lab, School of Computer Science and Statistics, Trinity College Dublin, Ireland",100.0,"Australia, Germany, India, Ireland",0.0,,"Underwater images often suffer from color distortion, haze, and limited visibility due to light refraction and absorption in water. These challenges significantly impact autonomous underwater vehicle applications, necessitating efficient image enhancement techniques. To address these challenges, we propose a Multi-Domain Query Cascaded Transformer Network for underwater image enhancement. Our approach includes a novel Multi-Domain Query Cascaded Attention mechanism that integrates localized transmission features and global illumination features. To improve feature propagation from the encoder to the decoder, we propose a Spatio-Spectro Fusion-Based Attention Block. Additionally, we introduce a Hybrid Fourier-Spatial Upsampling Block, which uniquely combines Fourier and spatial upsampling techniques to enhance feature resolution effectively. We evaluate our method on benchmark synthetic and real-world underwater image datasets, demonstrating its superiority through extensive ablation studies and comparative analysis. The testing code is available at: https: //github.com/Mdraqibkhan/Spectroformer.",https://openaccess.thecvf.com/content/WACV2024/html/Khan_Spectroformer_Multi-Domain_Query_Cascaded_Transformer_Network_for_Underwater_Image_Enhancement_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Khan_Spectroformer_Multi-Domain_Query_Cascaded_Transformer_Network_for_Underwater_Image_Enhancement_WACV_2024_paper.pdf,,https://github.com/Mdraqibkhan/Spectroformer,,main,Poster,https://ieeexplore.ieee.org/document/10484001/,"['Computer vision', 'Codes', 'Image color analysis', 'Lighting', 'Estimation', 'Transformers', 'Distortion']","['Image Enhancement', 'Underwater Image', 'Underwater Image Enhancement', 'Light Absorption', 'Extensive Analysis', 'Attention Mechanism', 'Real-world Datasets', 'Spatial Techniques', 'Real-world Images', 'Autonomous Underwater Vehicles', 'Color Distortion', 'Fourier Techniques', 'Training Set', 'Image Quality', 'Computer Vision', 'Input Features', 'Object Detection', 'Convolution Operation', 'Generative Adversarial Networks', 'Depth Estimation', 'Spatial Domain', 'Attention Feature', 'Color Correction', 'Dot Product', 'White Balance', 'Skip Connections', 'HSV Color', 'Dark Channel', 'Depthwise Convolution']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",3,"Underwater images often suffer from color distortion, haze, and limited visibility due to light refraction and absorption in water. These challenges significantly impact autonomous underwater vehicle applications, necessitating efficient image enhancement techniques. To address these challenges, we propose a Multi-Domain Query Cascaded Transformer Network for underwater image enhancement. Our approach includes a novel Multi-Domain Query Cascaded Attention mechanism that integrates localized transmission features and global illumination features. To improve feature propagation from the encoder to the decoder, we propose a Spatio-Spectro Fusion-Based Attention Block. Additionally, we introduce a Hybrid Fourier-Spatial Up-sampling Block, which uniquely combines Fourier and spatial upsampling techniques to enhance feature resolution effectively. We evaluate our method on benchmark synthetic and real-world underwater image datasets, demonstrating its superiority through extensive ablation studies and comparative analysis. The testing code is available at: https://github.com/Mdraqibkhan/Spectroformer."
Specular Object Reconstruction Behind Frosted Glass by Differentiable Rendering,"Takafumi Iwaguchi, Hiroyuki Kubo, Hiroshi Kawasaki","Chiba University, Japan; Kyushu University, Japan",100.0,Japan,0.0,,"This paper addresses the problem of reconstructing scenes behind optical diffusers, which is common in applications such as imaging through frosted glass. We propose a new approach that exploits specular reflection to capture sharp light distributions with a point light source, which can be used to detect reflections in low signal-to-noise scenarios. In this paper, we propose a rasterizer-based differentiable renderer to solve this problem by minimizing the difference between the captured and rendered images. Because our method can simultaneously optimize multiple observations for different light source positions, it is confirmed that ambiguities of the scene are efficiently eliminated by increasing the number of observations. Experiments show that the proposed method can reconstruct a scene with several mirror-like objects behind the diffuser in both simulated and real environments.",https://openaccess.thecvf.com/content/WACV2024/html/Iwaguchi_Specular_Object_Reconstruction_Behind_Frosted_Glass_by_Differentiable_Rendering_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Iwaguchi_Specular_Object_Reconstruction_Behind_Frosted_Glass_by_Differentiable_Rendering_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484514/,"['Computer vision', 'Shape', 'Glass', 'Rendering (computer graphics)', 'Optical imaging', 'Reflection', 'Computational efficiency']","['Object Reconstruction', 'Frosted Glass', 'Differentiable Rendering', 'Light Source', 'Number Of Observations', 'Multiple Observations', 'Specular Reflection', 'Light Position', 'Point Light Source', 'Small Area', 'Image Resolution', 'Smooth Surface', 'Global Optimization', 'Multiple Images', 'Diffuse Reflectance', 'Object Shape', 'Normal Direction', '3D Mesh', 'Ground Truth Image', 'CNN-based Methods', 'Mirror Surface', 'Normal Map', 'Reflection Pattern', 'Shape Reconstruction', 'Convex Shape', 'Surface Diffusion', 'Set Of Scenes', 'Light Rays']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', '3D computer vision']",,"This paper addresses the problem of reconstructing scenes behind optical diffusers, which is common in applications such as imaging through frosted glass. We propose a new approach that exploits specular reflection to capture sharp light distributions with a point light source, which can be used to detect reflections in low signal-to-noise scenarios. In this paper, we propose a rasterizer-based differentiable renderer to solve this problem by minimizing the difference between the captured and rendered images. Because our method can simultaneously optimize multiple observations for different light source positions, it is confirmed that ambiguities of the scene are efficiently eliminated by increasing the number of observations. Experiments show that the proposed method can reconstruct a scene with several mirror-like objects behind the diffuser in both simulated and real environments."
"SphereCraft: A Dataset for Spherical Keypoint Detection, Matching and Camera Pose Estimation","Christiano Gava, Yunmin Cho, Federico Raue, Sebastian Palacio, Alain Pagani, Andreas Dengel","DFKI; DFKI, University of Kaiserslautern-Landau; Aimmo Germany GmbH",33.33333333333333,Germany,66.66666666666667,Germany,"This paper introduces SphereCraft, a dataset specifically designed for spherical keypoint detection, matching, and camera pose estimation. The dataset addresses the limitations of existing datasets by providing extracted keypoints from various detectors, along with their ground truth correspondences. Synthetic scenes with photo-realistic rendering and accurate 3D meshes are included, as well as real-world scenes acquired from different spherical cameras. SphereCraft enables the development and evaluation of algorithms targeting multiple camera viewpoints, advancing the state-of-the-art in computer vision tasks involving spherical images. Our dataset is available at https://dfki.github.io/spherecraftweb/.",https://openaccess.thecvf.com/content/WACV2024/html/Gava_SphereCraft_A_Dataset_for_Spherical_Keypoint_Detection_Matching_and_Camera_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gava_SphereCraft_A_Dataset_for_Spherical_Keypoint_Detection_Matching_and_Camera_WACV_2024_paper.pdf,https://dfki.github.io/spherecraftweb/,https://github.com/dfki/spherecraft,,main,Poster,https://ieeexplore.ieee.org/document/10483906/,"['Training', 'Computer vision', 'Three-dimensional displays', 'Pose estimation', 'Predictive models', 'Cameras', 'Rendering (computer graphics)']","['Pose Estimation', 'Camera Pose', 'Keypoint Detection', 'Camera Pose Estimation', 'Computer Vision', '3D Mesh', 'Spherical Image', 'Training Data', 'Image Resolution', 'Multiple Images', 'Image Pairs', 'Semantic Segmentation', 'RGB Images', 'Depth Map', 'Local Neighborhood', 'Cyanoacrylate', 'Unit Sphere', 'Real Scenes', 'Geodesic Distance', 'Structure From Motion', 'Panoramic Images', 'Geometric Transformation', 'Train-test Split', 'Keypoint Locations', 'Non-maximum Suppression', 'Relative Pose', 'Tangent Plane', 'Bounding Box', 'Object Detection', 'Camera Orientation']","['Algorithms', 'Datasets and evaluations', 'Algorithms', '3D computer vision']",,"This paper introduces SphereCraft, a dataset specifically designed for spherical keypoint detection, matching, and camera pose estimation. The dataset addresses the limitations of existing datasets by providing extracted keypoints from various detectors, along with their ground truth correspondences. Synthetic scenes with photo-realistic rendering and accurate 3D meshes are included, as well as real-world scenes acquired from different spherical cameras. SphereCraft enables the development and evaluation of algorithms targeting multiple camera viewpoints, advancing the state-of-the-art in computer vision tasks involving spherical images. Our dataset is available at https://dfki.github.io/spherecraftweb/."
Spiking Denoising Diffusion Probabilistic Models,"Jiahang Cao, Ziqing Wang, Hanzhong Guo, Hao Cheng, Qiang Zhang, Renjing Xu","Renmin University of China; The Hong Kong University of Science and Technology (Guangzhou), North Carolina State University; The Hong Kong University of Science and Technology (Guangzhou)",100.0,"China, Hong Kong",0.0,,"Spiking neural networks (SNNs) have ultra-low energy consumption and high biological plausibility due to their binary and bio-driven nature compared with artificial neural networks (ANNs). While previous research has primarily focused on enhancing the performance of SNNs in classification tasks, the generative potential of SNNs remains relatively unexplored. In our paper, we put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new class of SNN-based generative models that achieve high sample quality. To fully exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net architecture, which achieves comparable performance to its ANN counterpart using only 4 time steps, resulting in significantly reduced energy consumption. Extensive experimental results reveal that our approach achieves state-of-the-art on the generative tasks and substantially outperforms other SNN-based generative models, achieving up to 12x and 6x improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we propose a threshold-guided strategy that can further improve the performances by 2.69% in a training-free manner. The SDDPM symbolizes a significant advancement in the field of SNN generation, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SDDPM.",https://openaccess.thecvf.com/content/WACV2024/html/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cao_Spiking_Denoising_Diffusion_Probabilistic_Models_WACV_2024_paper.pdf,,https://github.com/AndyCao1125/SDDPM,2306.17046,main,Poster,https://ieeexplore.ieee.org/document/10484223/,"['Training', 'Energy consumption', 'Computer vision', 'Biological system modeling', 'Noise reduction', 'Energy conservation', 'Computer architecture']","['Diffusion Probabilistic Models', 'Neural Network', 'Energy Consumption', 'Time Step', 'Artificial Neural Network', 'Energy Efficiency', 'Spiking Neural Networks', 'Field Generation', 'U-Net Architecture', 'Number Of Steps', 'Generative Adversarial Networks', 'Diffusion Model', 'Latent Space', 'Image Generation', 'Low Energy Consumption', 'Generalization Capability', 'Residual Block', 'Artificial Neural Network Model', 'Variational Autoencoder', 'Stochastic Differential Equations', 'Fréchet Inception Distance', 'Number Of Time Steps', 'Standard Wiener Process', 'Deep Generative Models']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Visualization']",3,"Spiking neural networks (SNNs) have ultra-low energy consumption and high biological plausibility due to their binary and bio-driven nature compared with artificial neural networks (ANNs). While previous research has primarily focused on enhancing the performance of SNNs in classification tasks, the generative potential of SNNs remains relatively unexplored. In our paper, we put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new class of SNN-based generative models that achieve high sample quality. To fully exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net architecture, which achieves comparable performance to its ANN counterpart using only 4 time steps, resulting in significantly reduced energy consumption. Extensive experimental results reveal that our approach achieves state-of-the-art on the generative tasks and substantially outperforms other SNN-based generative models, achieving up to 12× and 6× improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we propose a threshold-guided strategy that can further improve the performances by 2.69% in a training-free manner. The SDDPM symbolizes a significant advancement in the field of SNN generation, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SDDPM."
Spiking Neural Networks for Active Time-Resolved SPAD Imaging,"Yang Lin, Edoardo Charbon","Ecole polytechnique fédérale de Lausanne, Neuchâtel, Switzerland",100.0,Switzerland,0.0,,"Single-photon avalanche diodes (SPADs) are detectors capable of capturing single photons and of performing photon counting. SPADs have an exceptional temporal resolution and are thus highly suitable for time-resolved imaging applications. Applications span from biomedical research to consumers with SPADs integrated in smartphones and mixed-reality headsets. While conventional SPAD imaging systems typically employ photon time-tagging and histogram-building in the workflow, the pulse signal output of a SPAD naturally lends itself as input to spiking neural networks (SNNs). Leveraging this potential, SNNs offer real-time, energy-efficient, and intelligent processing with high throughput. In this paper, we propose two SNN frameworks, namely the Transporter SNN and the Reversed Start-stop SNN, along with corresponding hardware schemes for active time-resolved SPAD imaging. These frameworks convert phase-coded spike trains into density- and interspike-interval-coded ones, enabling training with rate-based warm-up and Surrogate Gradient. The SNNs are evaluated on fluorescence lifetime imaging. The results demonstrate that the accuracy of shallow SNNs is on par with established benchmarks. Our vision is to integrate SNNs in SPAD sensors and to explore advanced SNNs within the proposed schemes for high-level applications.",https://openaccess.thecvf.com/content/WACV2024/html/Lin_Spiking_Neural_Networks_for_Active_Time-Resolved_SPAD_Imaging_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lin_Spiking_Neural_Networks_for_Active_Time-Resolved_SPAD_Imaging_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483903/,"['Training', 'Imaging', 'Virtual reality', 'Throughput', 'Real-time systems', 'Sensors', 'Smart phones']","['Neural Network', 'Spiking Neural Networks', 'Time-resolved Imaging', 'Single-photon Avalanche Diode', 'Single Photon', 'Fluorescence Lifetime', 'Spike Trains', 'Fluorescence Lifetime Imaging Microscopy', 'Artificial Neural Network', 'Background Noise', 'Long Short-term Memory', 'Reference Signal', 'Neuron Model', 'Mean Absolute Percentage Error', 'Number Of Time Steps', 'Synaptic Weights', 'Spike Rate', 'Complex Topology', 'Single-photon Detectors', 'Biological Neural Networks', 'Passive Imaging', 'Backpropagation Through Time', 'Hardware Level', 'Time-to-digital Converter', 'Dynamic Vision Sensor', 'Spiking Neuron Model', 'Photon Rate', 'Hardware Structure', 'Coding Phase']","['Applications', 'Embedded sensing / real-time techniques', 'Applications', 'Biomedical / healthcare / medicine']",6,"Single-photon avalanche diodes (SPADs) are detectors capable of capturing single photons and of performing photon counting. SPADs have an exceptional temporal resolution and are thus highly suitable for time-resolved imaging applications. Applications span from biomedical research to consumers with SPADs integrated in smartphones and mixed-reality headsets. While conventional SPAD imaging systems typically employ photon time-tagging and histogram-building in the workflow, the pulse signal output of a SPAD naturally lends itself as input to spiking neural networks (SNNs). Leveraging this potential, SNNs offer real-time, energy-efficient, and intelligent processing with high throughput. In this paper, we propose two SNN frameworks, namely the Transporter SNN and the Reversed Start-stop SNN, along with corresponding hardware schemes for active time-resolved SPAD imaging. These frameworks convert phase-coded spike trains into density- and interspike-interval-coded ones, enabling training with rate-based warm-up and Surrogate Gradient. The SNNs are evaluated on fluorescence lifetime imaging. The results demonstrate that the accuracy of shallow SNNs is on par with established benchmarks. Our vision is to integrate SNNs in SPAD sensors and to explore advanced SNNs within the proposed schemes for high-level applications."
Steering Prototypes With Prompt-Tuning for Rehearsal-Free Continual Learning,"Zhuowei Li, Long Zhao, Zizhao Zhang, Han Zhang, Di Liu, Ting Liu, Dimitris N. Metaxas",Google Cloud AI; Rutgers University; Google Research,33.33333333333333,USA,66.66666666666667,USA,"In the context of continual learning, prototypes--as representative class embeddings--offer advantages in memory conservation and the mitigation of catastrophic forgetting. However, challenges related to semantic drift and prototype interference persist. In this study, we introduce the Contrastive Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning, underpinned by a contrastive learning objective, we effectively address both aforementioned challenges. Our evaluations on four challenging class-incremental benchmarks reveal that CPP achieves a significant 4% to 6% improvement over state-of-the-art methods. Importantly, CPP operates without a rehearsal buffer and narrows the performance divergence between continual and offline joint learning, suggesting an innovative scheme for Transformer-based continual learning systems.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Steering_Prototypes_With_Prompt-Tuning_for_Rehearsal-Free_Continual_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Steering_Prototypes_With_Prompt-Tuning_for_Rehearsal-Free_Continual_Learning_WACV_2024_paper.pdf,Not provided,Not provided,2303.09447,main,Poster,https://ieeexplore.ieee.org/document/10483725/,"['Representation learning', 'Computer vision', 'Semantics', 'Prototypes', 'Interference', 'Self-supervised learning', 'Benchmark testing']","['Incremental Learning', 'Learning Objectives', 'Self-supervised Learning', 'Semantic Change', 'Catastrophic Forgetting', 'Deep Neural Network', 'Multilayer Perceptron', 'Training Stage', 'Latent Space', 'Linear Classifier', 'Empirical Validation', 'Contrastive Loss', 'Absolute Improvement', 'Class Prototypes', 'Transformer-based Methods', 'Bregman Divergence']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",3,"In the context of continual learning, prototypes—as representative class embeddings—offer advantages in memory conservation and the mitigation of catastrophic forgetting. However, challenges related to semantic drift and prototype interference persist. In this study, we introduce the Contrastive Prototypical Prompt (CPP) approach. Through task-specific prompt-tuning, underpinned by a contrastive learning objective, we effectively address both aforementioned challenges. Our evaluations on four challenging class-incremental benchmarks reveal that CPP achieves a significant 4% to 6% improvement over state-of-the-art methods. Importantly, CPP operates without a rehearsal buffer and narrows the performance divergence between continual and offline joint-learning, suggesting an innovative scheme for Transformer-based continual learning systems
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
"Stereo Conversion With Disparity-Aware Warping, Compositing and Inpainting","Lukas Mehl, Andrés Bruhn, Markus Gross, Christopher Schroers","Institute for Visualization and Interactive Systems, University of Stuttgart; DisneyResearch | Studios, Switzerland; Computer Graphics Lab, Department of Computer Science, ETH Zurich",100.0,"Germany, Switzerland",0.0,,"Despite of exciting advances in image-based rendering and novel view synthesis, it is still challenging to achieve high-resolution results that can reach production-level quality when applying such methods to the task of stereo conversion. At the same time, only very few dedicated stereo conversion approaches exist, which also fall short in terms of the required quality. Hence, in this paper, we present a novel method for high-resolution 2D-to-3D conversion. It is fully differentiable in all of its stages and performs disparity-informed warping, consistent foreground-background compositing, and background-aware inpainting. To enable temporal consistency in the resulting video, we propose a strategy to integrate information from additional video frames. Extensive ablation studies validate our design choices, leading to a fully automatic model that outperforms existing approaches by a large margin (49-70% LPIPS error reduction). Finally, inspired from current practices in manual stereo conversion, we introduce optional interactive tools into our model, which allow to steer the conversion process and make it significantly more applicable for 3D film production.",https://openaccess.thecvf.com/content/WACV2024/html/Mehl_Stereo_Conversion_With_Disparity-Aware_Warping_Compositing_and_Inpainting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mehl_Stereo_Conversion_With_Disparity-Aware_Warping_Compositing_and_Inpainting_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483709/,"['Visualization', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Systematics', 'Production', 'Manuals']","['Stereoconversion', 'Design Choices', 'Large Margin', '3D Video', 'View Synthesis', 'Input Image', 'Optical Flow', 'Strong Changes', 'Manhattan Distance', 'Depth Estimation', 'Multiple Frames', 'Image Synthesis', 'Feature Pyramid', 'Horizontal Displacement', 'Local Background', 'Supplement For Details', 'Camera Pose', 'Reconstruction Time', 'Input Frames', 'Left View', 'Target Frame', 'High Visual Quality', 'Disparity Estimation', 'Pyramid Level', 'Single Frame', 'Optical Flow Estimation', 'Changes In Content']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', '3D computer vision']",,"Despite of exciting advances in image-based rendering and novel view synthesis, it is still challenging to achieve high-resolution results that can reach production-level quality when applying such methods to the task of stereo conversion. At the same time, only very few dedicated stereo conversion approaches exist, which also fall short in terms of the required quality. Hence, in this paper, we present a novel method for high-resolution 2D-to-3D conversion. It is fully differentiable in all of its stages and performs disparity-informed warping, consistent foreground-background compositing, and background-aware inpainting. To enable temporal consistency in the resulting video, we propose a strategy to integrate information from additional video frames. Extensive ablation studies validate our design choices, leading to a fully automatic model that outperforms existing approaches by a large margin (49-70% LPIPS error reduction). Finally, inspired from current practices in manual stereo conversion, we introduce optional interactive tools into our model, which allow to steer the conversion process and make it significantly more applicable for 3D film production."
Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended Reality,"Ziang Cheng, Jiayu Yang, Hongdong Li",Australian National University; Tencent XR Vision Labs,50.0,Australia,50.0,China,"Real-time Stereo Matching is a cornerstone task for Extended Reality (XR) applications, such as 3D scene understanding, video pass-through, and mixed-reality games. Despite significant advancements, getting accurate depth information in real time on a low-power mobile device remains a challenge. One of the main difficulties is the lack of high-quality indoor video stereo data captured by head-mounted VR or AR glasses. To address this, we introduce a novel video stereo synthetic dataset that comprises photorealistic renderings of various indoor scenes and realistic camera motion captured by a moving VR/AR head-mounted display (HMD). Our newly proposed dataset enables one to develop a novel framework for continuous video-rate stereo matching. As another contribution, we also propose a new video-based stereo matching approach tailored for XR applications, which achieves real-time inference at an impressive 134fps on a standard desktop computer, or 30fps on a battery-powered HMD. Our key insight is that disparity and contextual information are highly correlated and redundant between consecutive stereo frames. By unrolling an iterative cost aggregation in time (i.e. in temporal dimension), we are able to distribute and reuse the aggregated features over time. This leads to a substantial reduction in computation without sacrificing accuracy. We conducted extensive evaluations and demonstrated that our method achieves superior performance compared to the current state-of-the-art, making it a strong contender for real-time stereo matching in VR/AR applications.",https://openaccess.thecvf.com/content/WACV2024/html/Cheng_Stereo_Matching_in_Time_100_FPS_Video_Stereo_Matching_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cheng_Stereo_Matching_in_Time_100_FPS_Video_Stereo_Matching_for_WACV_2024_paper.pdf,,https://github.com/za-cheng/XR-Stereo,,main,Poster,https://ieeexplore.ieee.org/document/10484071/,"['Three-dimensional displays', 'Extended reality', 'Resists', 'Streaming media', 'Rendering (computer graphics)', 'Real-time systems', 'X reality']","['Stereo Matching', 'Temporal Dimension', 'Consecutive Frames', 'Head-mounted Display', 'Camera Motion', 'Low-power Devices', 'High-quality Video', 'Cost Aggregation', 'Computational Reduction', 'Standard Desktop Computer', 'Random Noise', 'Mirror Image', 'Learning-based Methods', 'Indoor Environments', 'Hidden State', 'Optical Flow', 'Video Sequences', 'Current Frame', 'Image Synthesis', 'Inference Speed', 'Cost Volume', 'Camera Pose', 'Matching Cost', 'Stereo Images', 'Disparity Estimation', 'Endpoint Error', 'Visual Odometry', 'Indoor Scenarios', 'Stereo Camera', 'Transparency Window']","['Applications', 'Virtual / augmented reality', 'Algorithms', '3D computer vision', 'Algorithms', 'Datasets and evaluations']",2,"Real-time Stereo Matching is a cornerstone task for Extended Reality (XR) applications, such as 3D scene understanding, video pass-through, and mixed-reality games. Despite significant advancements, getting accurate depth information in real time on a low-power mobile device remains a challenge. One of the main difficulties is the lack of high-quality indoor video stereo data captured by head-mounted VR or AR glasses. To address this, we introduce a novel video stereo synthetic dataset that comprises photorealistic renderings of various indoor scenes and realistic camera motion captured by a moving VR/AR head-mounted display (HMD). Our newly proposed dataset enables one to develop a novel framework for continuous video-rate stereo matching.As another contribution, we also propose a new video-based stereo matching approach tailored for XR applications, which achieves real-time inference at an impressive 134fps on a standard desktop computer, or 30fps on a battery-powered HMD. Our key insight is that disparity and contextual information are highly correlated and redundant between consecutive stereo frames. By unrolling an iterative cost aggregation in time (i.e. in temporal dimension), we are able to distribute and reuse the aggregated features over time. This leads to a substantial reduction in computation without sacrificing accuracy. We conducted extensive evaluations and demonstrated that our method achieves superior performance compared to the current state-of-the-art, making it a strong contender for real-time stereo matching in VR/AR applications. Our dataset is released on https://github.com/za-cheng/XR-Stereo."
Stochastic Binary Network for Universal Domain Adaptation,"Saurabh Kumar Jain, Sukhendu Das","Visualization and Perception Lab, Department of Computer Science Engineering, Indian Institute of Technology, Madras, India",100.0,India,0.0,,"Universal domain adaptation (UniDA) is the unsupervised domain adaptation with label shift. UniDA aims to classify unlabeled target samples into one of the ""known"" categories or into a single ""unknown"" category. Its main challenge lies in detecting private classes from both domains and performing alignment between the common classes. Current methods employ various techniques and loss functions to address these challenges. However, these methods commonly represent classifiers as point weight vectors, which are prone to overfitting by the source domain samples due to the lack of supervision from the target domain. Consequently, these classifiers struggle to separate target samples into known and unknown categories effectively. To address this, we introduce a novel framework called Stochastic Binary Network for Universal Domain Adaptation (STUN). STUN uses a Stochastic binary classifier for each class, whose weight is modeled as Gaussian distribution, enabling to sample an arbitrary number of classifiers while keeping the model size same as of two classifiers. Consistency between these sampled classifiers is used to derive the confidence scores for both source and target samples, which facilitates the alignment of common classes using weighted adversarial learning. Finally, we use deep discriminative clustering to formulate a loss function for solving the problem of fragmented feature distributions in the target domain. Extensive ablation studies and state-of-the-art results across three standard benchmark datasets show the efficacy of our framework.",https://openaccess.thecvf.com/content/WACV2024/html/Jain_Stochastic_Binary_Network_for_Universal_Domain_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jain_Stochastic_Binary_Network_for_Universal_Domain_Adaptation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483867/,"['Training', 'Adaptation models', 'Computer vision', 'Stochastic processes', 'Benchmark testing', 'Adversarial machine learning', 'Vectors']","['Domain Adaptation', 'Universal Domain Adaptation', 'Binary Classification', 'Benchmark Datasets', 'Target Sample', 'Weight Vector', 'Generative Adversarial Networks', 'Common Categories', 'Confidence Score', 'Target Domain', 'Source Domain', 'Unknown Category', 'Training Time', 'Data Augmentation', 'Multi-label', 'Large-scale Datasets', 'Kullback-Leibler', 'Domain Shift', 'Dense Clusters', 'Negative Transfer', 'Unknown Samples', 'Unlabeled Data', 'Soft Labels', 'Class Weights', 'Diagonal Covariance Matrix', 'Consistency Regularization', 'Decision Boundary', 'Increase In Computation Time', 'Target Data']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding']",1,"Universal domain adaptation (UniDA) is the unsupervised domain adaptation with label shift. UniDA aims to classify unlabeled target samples into one of the ""known"" categories or into a single ""unknown"" category. Its main challenge lies in detecting private classes from both domains and performing alignment between the common classes. Current methods employ various techniques and loss functions to address these challenges. However, these methods commonly represent classifiers as point weight vectors, which are prone to overfitting by the source domain samples due to the lack of supervision from the target domain. Consequently, these classifiers struggle to separate target samples into known and unknown categories effectively. To address this, we introduce a novel framework called Stochastic Binary Network for Universal Domain Adaptation (STUN). STUN uses a Stochastic binary classifier for each class, whose weight is modeled as Gaus-sian distribution, enabling to sample an arbitrary number of classifiers while keeping the model size same as of two classifiers. Consistency between these sampled classifiers is used to derive the confidence scores for both source and target samples, which facilitates the alignment of common classes using weighted adversarial learning. Finally, we use deep discriminative clustering to formulate a loss function for solving the problem of fragmented feature distributions in the target domain. Extensive ablation studies and state-of-the-art results across three standard benchmark datasets show the efficacy of our framework."
StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction,"Tianyuan Yuan, Yicheng Liu, Yue Wang, Yilun Wang, Hang Zhao",Tsinghua University; University of Southern California,100.0,"China, USA",0.0,,"High-Definition (HD) maps are essential for the safety of autonomous driving systems. While existing techniques employ camera images and onboard sensors to generate vectorized high-precision maps, they are constrained by their reliance on single-frame input. This approach limits their stability and performance in complex scenarios such as occlusions, largely due to the absence of temporal information. Moreover, their performance diminishes when applied to broader perception ranges. In this paper, we present StreamMapNet, a novel online mapping pipeline adept at long-sequence temporal modeling of videos. StreamMapNet employs multi-point attention and temporal information which empowers the construction of large-range local HD maps with high stability and further addresses the limitations of existing methods. Furthermore, we critically examine widely used online HD Map construction benchmark and datasets, Argoverse2 and nuScenes, revealing significant bias in the existing evaluation protocols. We propose to resplit the benchmarks according to geographical spans, promoting fair and precise evaluations. Experimental results validate that StreamMapNet significantly outperforms existing methods across all settings while maintaining an online inference speed of 14.2 FPS. Our code is available at https://github.com/yuantianyuan01/StreamMapNet.",https://openaccess.thecvf.com/content/WACV2024/html/Yuan_StreamMapNet_Streaming_Mapping_Network_for_Vectorized_Online_HD_Map_Construction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yuan_StreamMapNet_Streaming_Mapping_Network_for_Vectorized_Online_HD_Map_Construction_WACV_2024_paper.pdf,,https://github.com/yuantianyuan01/StreamMapNet,2308.12570,main,Poster,https://ieeexplore.ieee.org/document/10484447/,"['Image sensors', 'Computer vision', 'Protocols', 'Codes', 'Pipelines', 'Benchmark testing', 'Stability analysis']","['Map Construction', 'Temporal Information', 'Temporal Model', 'Web Map', 'Inference Speed', 'Onboard Sensors', 'Training Set', 'Validation Set', 'Object Detection', 'Multilayer Perceptron', 'Autonomous Vehicles', 'Path Planning', 'Temporal Association', 'Gated Recurrent Unit', 'Current Frame', 'Memory Characteristics', 'Object Detection Task', 'Memory Cost', 'Vector Map', '3D Object Detection', 'Map Elements', 'Matching Cost', 'Road Boundary', 'Transformer Decoder', 'Bipartite Matching', 'Polyline', 'Previous Frame', 'Feature Pyramid Network']","['Applications', 'Autonomous Driving', 'Algorithms', '3D computer vision', 'Algorithms', 'Image recognition and understanding']",22,"High-Definition (HD) maps are essential for the safety of autonomous driving systems. While existing techniques employ camera images and onboard sensors to generate vectorized high-precision maps, they are constrained by their reliance on single-frame input. This approach limits their stability and performance in complex scenarios such as occlusions, largely due to the absence of temporal information. Moreover, their performance diminishes when applied to broader perception ranges. In this paper, we present StreamMapNet, a novel online mapping pipeline adept at long-sequence temporal modeling of videos. StreamMapNet employs multi-point attention and temporal information which empowers the construction of large-range local HD maps with high stability and further addresses the limitations of existing methods. Furthermore, we critically examine widely used online HD Map construction benchmark and datasets, Argoverse2 and nuScenes, revealing significant bias in the existing evaluation protocols. We propose to resplit the benchmarks according to geographical spans, promoting fair and precise evaluations. Experimental results validate that StreamMapNet significantly outperforms existing methods across all settings while maintaining an online inference speed of 14.2 FPS. Our code is available at https://github.com/yuantianyuan01/StreamMapNet."
StyleAvatar: Stylizing Animatable Head Avatars,"Juan C. Pérez, Thu Nguyen-Phuoc, Chen Cao, Artsiom Sanakoyeu, Tomas Simon, Pablo Arbeláez, Bernard Ghanem, Ali Thabet, Albert Pumarola",KAUST; Meta; Meta; 1Work performed as part of Juan’s internship at Meta.; Universidad de los Andes,60.0,"Colombia, Saudi Arabia, USA",40.0,USA,"AR/VR applications promise to provide people with a genuine feeling of mutual presence when communicating via their personalized avatars. While realistic avatars are essential in various social settings, the vast possibilities of a virtual world can also generate interest in using stylized avatars for other purposes. We introduce StyleAvatar, the first method for semantic stylization of animatable head avatars. StyleAvatar directly stylizes the avatar representation, rather than stylizing its renders. Specifically, given a model generating the avatar, StyleAvatar first disentangles geometry and texture manipulations, and then stylizes the avatar by fine-tuning a subset of the model's weights. Our method has multiple virtues, including the ability to describe styles using images or text, preserving the avatar's animatable capacity, providing control over identity preservation, and disentangling texture and geometry modifications. Experiments have shown that our approach consistently works across skin tones, challenging hair styles, extreme views, and diverse facial expressions.",https://openaccess.thecvf.com/content/WACV2024/html/Perez_StyleAvatar_Stylizing_Animatable_Head_Avatars_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Perez_StyleAvatar_Stylizing_Animatable_Head_Avatars_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484167/,"['Geometry', 'Hair', 'Computer vision', 'Head', 'Avatars', 'Semantics', 'Skin']","['Semantic', 'Facial Expressions', 'Identity Preservation', 'Point Cloud', 'Latent Space', 'Facial Features', 'Image Statistics', 'Style Transfer']","['Applications', 'Virtual / augmented reality', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"AR/VR applications promise to provide people with a genuine feeling of mutual presence when communicating via their personalized avatars. While realistic avatars are essential in various social settings, the vast possibilities of a virtual world can also generate interest in using stylized avatars for other purposes. We introduce StyleAvatar, the first method for semantic stylization of animatable head avatars. StyleAvatar directly stylizes the avatar representation, rather than stylizing its renders. Specifically, given a model generating the avatar, StyleAvatar first disentangles geometry and texture manipulations, and then stylizes the avatar by fine-tuning a subset of the model’s weights. Our method has multiple virtues, including the ability to describe styles using images or text, preserving the avatar’s animatable capacity, providing control over identity preservation, and disentangling texture and geometry modifications. Experiments have shown that our approach consistently works across skin tones, challenging hair styles, extreme views, and diverse facial expressions.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
StyleGAN-Fusion: Diffusion Guided Domain Adaptation of Image Generators,"Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, Ahmed Elgammal","Rutgers University, Playform AI; Bytedance Inc.; Rutgers University",66.66666666666666,USA,33.33333333333334,China,"Can a text-to-image diffusion model be used as a training objective for adapting a GAN generator to another domain? In this paper, we show that the classifier-free guidance can be leveraged as a critic and enable generators to distill knowledge from large-scale text-to-image diffusion models. Generators can be efficiently shifted into new domains indicated by text prompts without access to ground truth samples from target domains. We demonstrate the effectiveness and controllability of our method through extensive experiments. Although not trained to minimize CLIP loss, our model achieves equally high CLIP scores and significantly lower FID than prior work on short prompts and outperforms the baseline qualitatively and quantitatively on long and complicated prompts. To our best knowledge, the proposed method is the first attempt at incorporating large-scale pre-trained diffusion models and distillation sampling for text-driven image generator domain adaptation and gives a quality previously beyond possible. Moreover, we extend our work to 3D-aware style-based generators and DreamBooth guidance.",https://openaccess.thecvf.com/content/WACV2024/html/Song_StyleGAN-Fusion_Diffusion_Guided_Domain_Adaptation_of_Image_Generators_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Song_StyleGAN-Fusion_Diffusion_Guided_Domain_Adaptation_of_Image_Generators_WACV_2024_paper.pdf,http://projectwebpage.com,,,main,Poster,https://ieeexplore.ieee.org/document/10484324/,"['Training', 'Adaptation models', 'Visualization', 'Computer vision', 'Codes', 'Computational modeling', 'Controllability']","['Image Generation', 'Domain Adaptation', 'Diffusion Model', 'Target Domain', 'Training Objective', 'Semantic', 'Image Features', 'Single Image', 'Latent Space', 'Visual Quality', 'Similar Images', 'Image Domain', 'Ground Truth Image', 'Baseline Imaging', 'Image X', 'Domain Adaptation Methods', 'Latent Code', 'Image Fidelity']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Vision + language and/or other modalities']",4,"Can a text-to-image diffusion model be used as a training objective for adapting a GAN generator to another domain? In this paper, we show that the classifier-free guidance can be leveraged as a critic and enable generators to distill knowledge from large-scale text-to-image diffusion models. Generators can be efficiently shifted into new domains indicated by text prompts without access to groundtruth samples from target domains. We demonstrate the effectiveness and controllability of our method through extensive experiments. Although not trained to minimize CLIP loss, our model achieves equally high CLIP scores and significantly lower FID than prior work on short prompts, and outperforms the baseline qualitatively and quantitatively on long and complicated prompts. To our best knowledge, the proposed method is the first attempt at incorporating large-scale pre-trained diffusion models and distillation sampling for text-driven image generator domain adaptation and gives a quality previously beyond possible. Moreover, we extend our work to 3D-aware style-based generators and DreamBooth guidance. For code and more visual samples, please visit our Project Webpage."
StyleGenes: Discrete and Efficient Latent Distributions for GANs,"Evangelos Ntavelis, Mohamad Shahbazi, Iason Kastanis, Martin Danelljan, Luc Van Gool","Computer Vision Lab, ETH Zurich, CH; KU Leuven, BE; Computer Vision Lab, ETH Zurich, CH; CSEM, CH",75.0,"Belgium, Switzerland",25.0,Switzerland,"We propose a discrete latent distribution for Generative Adversarial Networks (GANs). Instead of drawing latent vectors from a continuous prior, we sample from a finite set of learnable latents. However, a direct parametrization of such a distribution leads to an intractable linear increase in memory in order to ensure sufficient sample diversity. We address this key issue by taking inspiration from the encoding of information in biological organisms. Instead of learning a separate latent vector for each sample, we split the latent space into a set of genes. For each gene, we train a small bank of gene variants. Thus, by independently sampling a variant for each gene and combining them into the final latent vector, our approach can represent a vast number of unique latent samples from a compact set of learnable parameters. Interestingly, our gene-inspired latent encoding allows for new and intuitive approaches to latent-space exploration, enabling conditional sampling from our unconditionally trained model. Our approach preserves state-of-the-art photo-realism while achieving better disentanglement than the widely-used StyleMapping network.",https://openaccess.thecvf.com/content/WACV2024/html/Ntavelis_StyleGenes_Discrete_and_Efficient_Latent_Distributions_for_GANs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ntavelis_StyleGenes_Discrete_and_Efficient_Latent_Distributions_for_GANs_WACV_2024_paper.pdf,,,2305.00599,main,Poster,https://ieeexplore.ieee.org/document/10484010/,"['Computer vision', 'Codes', 'Diversity reception', 'Genomics', 'DNA', 'Generative adversarial networks', 'Vectors']","['Generative Adversarial Networks', 'Discrete Distribution', 'Latent Distribution', 'Latent Space', 'Encoding Of Information', 'Latent Vector', 'Order Memory', 'Smooth Surface', 'Codebook', 'Number Of Images', 'Conditional Independence', 'Multilayer Perceptron', 'Standard Normal Distribution', 'Continuous Distribution', 'Image Generation', 'Learning Spaces', 'Hair Color', 'Image Synthesis', 'Discrete Samples', 'Embedding Learning', 'Latent Code', 'Fréchet Inception Distance', 'StyleGAN', 'Black Hair', 'Vector Quantization', 'Generative Adversarial Network Framework', 'Collection Of Variants', 'Output Image']","['Algorithms', 'Computational photography', 'image and video synthesis']",,"We propose a discrete latent distribution for Generative Adversarial Networks (GANs). Instead of drawing latent vectors from a continuous prior, we sample from a finite set of learnable latents. However, a direct parametrization of such a distribution leads to an intractable linear increase in memory in order to ensure sufficient sample diversity. We address this key issue by taking inspiration from the encoding of information in biological organisms. Instead of learning a separate latent vector for each sample, we split the latent space into a set of genes. For each gene, we train a small bank of gene variants. Thus, by independently sampling a variant for each gene and combining them into the final latent vector, our approach can represent a vast number of unique latent samples from a compact set of learnable parameters. Interestingly, our gene-inspired latent encoding allows for new and intuitive approaches to latent-space exploration, enabling conditional sampling from our unconditionally trained model. Moreover, our approach preserves state-of-the-art photo-realism while achieving better disentanglement than the widely-used StyleMapping network."
SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse Volume Rendering,"Mohammed Brahimi, Bjoern Haefner, Tarun Yenamandra, Bastian Goldluecke, Daniel Cremers","University of Konstanz; Technical University of Munich, Munich Center for Machine Learning",100.0,Germany,0.0,,"We propose an end-to-end inverse rendering pipeline called SupeRVol that allows us to recover 3D shape and material parameters from a set of color images in a super-resolution manner. To this end, we represent both the bidirectional reflectance distribution function (BRDF) and the signed distance function (SDF) by multi-layer perceptrons. In order to obtain both the surface shape and its reflectance properties, we revert to a differentiable volume renderer with a physically based illumination model that allows us to decouple reflectance and lighting. This physical model takes into account the effect of the camera's point spread function thereby enabling a reconstruction of shape and material in a super-resolution quality. Experimental validation confirms that SupeRVol achieves state of the art performance in terms of inverse rendering quality. It generates reconstructions that are sharper than the individual input images, making this method ideally suited for 3D modeling from low-resolution imagery.",https://openaccess.thecvf.com/content/WACV2024/html/Brahimi_SupeRVol_Super-Resolution_Shape_and_Reflectance_Estimation_in_Inverse_Volume_Rendering_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Brahimi_SupeRVol_Super-Resolution_Shape_and_Reflectance_Estimation_in_Inverse_Volume_Rendering_WACV_2024_paper.pdf,,,2212.04968,main,Poster,https://ieeexplore.ieee.org/document/10484114/,"['Reflectivity', 'Training', 'Geometry', 'Solid modeling', 'Three-dimensional displays', 'Shape', 'Superresolution']","['Volume Rendering', 'Low Resolution', 'Input Image', '3D Reconstruction', 'Multilayer Perceptron', 'Material Parameters', 'Individual Images', 'Point Spread Function', 'Reflectance Properties', 'Shape Reconstruction', 'Signed Distance Function', 'High-resolution', 'Neural Network', 'Gaussian Kernel', 'Parametrized', 'Learnable Parameters', 'Image Formation', 'Explicit Model', 'Neural Representations', 'Low-resolution Images', 'View Synthesis', 'Input Image Resolution', 'Super-resolution Model', 'Image Synthesis', 'Scene Reconstruction', 'Point Light Source']","['Algorithms', '3D computer vision']",,"We propose an end-to-end inverse rendering pipeline called SupeRVol that allows us to recover 3D shape and material parameters from a set of color images in a superresolution manner. To this end, we represent both the bidirectional reflectance distribution function’s (BRDF) parameters and the signed distance function (SDF) by multi-layer perceptrons (MLPs). In order to obtain both the surface shape and its reflectance properties, we revert to a differentiable volume renderer with a physically based illumination model that allows us to decouple reflectance and lighting. This physical model takes into account the effect of the camera’s point spread function thereby enabling a reconstruction of shape and material in a super-resolution quality. Experimental validation confirms that SupeRVol achieves state of the art performance in terms of inverse rendering quality. It generates reconstructions that are sharper than the individual input images, making this method ideally suited for 3D modeling from low-resolution imagery."
Synergizing Contrastive Learning and Optimal Transport for 3D Point Cloud Domain Adaptation,"Siddharth Katageri, Arkadipta De, Chaitanya Devaguptapu, VSSV Prasad, Charu Sharma, Manohar Kaul","Fujitsu Research India; IIIT Hyderabad, India",50.0,India,50.0,India,"Recently, the fundamental problem of unsupervised domain adaptation (UDA) on 3D point clouds has been motivated by a wide variety of applications in robotics, virtual reality, and scene understanding, to name a few. The point cloud data acquisition procedures manifest themselves as significant domain discrepancies and geometric variations among both similar and dissimilar classes. The standard domain adaptation methods developed for images do not directly translate to point cloud data because of their complex geometric nature. To address this challenge, we leverage the idea of multimodality and alignment between distributions. We propose a new UDA architecture for point cloud classification that benefits from multimodal contrastive learning to get better class separation in both domains individually. Further, the use of optimal transport (OT) aims at learning source and target data distributions jointly to reduce the cross-domain shift and provide a better alignment. We conduct a comprehensive empirical study on PointDA-10 and GraspNetPC-10 and show that our method achieves state-of the-art performance on GraspNetPC-10 (with approx. 4-12% margin) and best average performance on PointDA-10. Our ablation studies and decision boundary analysis also validate the significance of our contrastive learning module and OT alignment.",https://openaccess.thecvf.com/content/WACV2024/html/Katageri_Synergizing_Contrastive_Learning_and_Optimal_Transport_for_3D_Point_Cloud_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Katageri_Synergizing_Contrastive_Learning_and_Optimal_Transport_for_3D_Point_Cloud_WACV_2024_paper.pdf,,https://siddharthkatageri.github.io/COT,2308.14126,main,Poster,https://ieeexplore.ieee.org/document/10484057/,"['Point cloud compression', 'Computer vision', 'Three-dimensional displays', 'Data acquisition', 'Self-supervised learning', 'Virtual reality', 'Object detection']","['Point Cloud', '3D Point', 'Domain Adaptation', 'Optimal Transport', 'Self-supervised Learning', '3D Point Cloud', 'Point Cloud Domain', 'Decision Boundary', 'Point Cloud Data', 'Geometric Variables', 'Scene Understanding', 'Multimodal Learning', 'Domain Adaptation Methods', 'Feature Space', 'Feature Learning', 'Target Sample', 'Representation Learning', 'Latent Space', 'Target Domain', 'Source Domain', '3D Features', 'Maximum Mean Discrepancy', 'Domain Alignment', 'Wasserstein Distance', 'Point Cloud Features', 'Coupling Matrix', 'Multimodal Information', 'Target Label', '3D Information']","['Algorithms', '3D computer vision', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",4,"Recently, the fundamental problem of unsupervised domain adaptation (UDA) on 3D point clouds has been motivated by a wide variety of applications in robotics, virtual reality, and scene understanding, to name a few. The point cloud data acquisition procedures manifest themselves as significant domain discrepancies and geometric variations among both similar and dissimilar classes. The standard domain adaptation methods developed for images do not directly translate to point cloud data because of their complex geometric nature. To address this challenge, we leverage the idea of multimodality and alignment between distributions. We propose a new UDA architecture for point cloud classification that benefits from multimodal contrastive learning to get better class separation in both domains individually. Further, the use of optimal transport (OT) aims at learning source and target data distributions jointly to reduce the cross-domain shift and provide a better alignment. We conduct a comprehensive empirical study on PointDA-10 and GraspNetPC-10 and show that our method achieves state-of-the-art performance on GraspNetPC-10 (with ≈ 4-12% margin) and best average performance on PointDA-10. Our ablation studies and decision boundary analysis also validate the significance of our contrastive learning module and OT alignment. https://siddharthkatageri.github.io/COT."
SynergyNet: Bridging the Gap Between Discrete and Continuous Representations for Precise Medical Image Segmentation,"Vandan Gorade, Sparsh Mittal, Debesh Jha, Ulas Bagci","Northwestern University, Chicago, IL; Indian Institute of Technology Roorkee, India",100.0,"India, USA",0.0,,"In recent years, continuous latent space (CLS) and discrete latent space (DLS) deep learning models have been proposed for medical image analysis for improved performance. However, these models encounter distinct challenges. CLS models capture intricate details but often lack interpretability in terms of structural representation and robustness due to their emphasis on low-level features. Conversely, DLS models offer interpretability, robustness, and the ability to capture coarse-grained information thanks to their structured latent space. However, DLS models have limited efficacy in capturing fine-grained details. To address the limitations of both DLS and CLS models, we propose SynergyNet, a novel bottleneck architecture designed to enhance existing encoder-decoder segmentation frameworks. SynergyNet seamlessly integrates discrete and continuous representations to harness complementary information and successfully preserves both fine and coarsegrained details in the learned representations. Our extensive experiment on multi-organ segmentation and cardiac datasets demonstrates that SynergyNet outperforms other state of the art methods including TransUNet: dice scores improving by 2.16%, and Hausdorff scores improving by 11.13%, respectively. When evaluating skin lesion and brain tumor segmentation datasets, we observe a remarkable improvements of 1.71% in Intersection-overUnion scores for skin lesion segmentation and of 8.58% for brain tumor segmentation. Our innovative approach paves the way for enhancing the overall performance and capabilities of deep learning models in the critical domain of medical image analysis.",https://openaccess.thecvf.com/content/WACV2024/html/Gorade_SynergyNet_Bridging_the_Gap_Between_Discrete_and_Continuous_Representations_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gorade_SynergyNet_Bridging_the_Gap_Between_Discrete_and_Continuous_Representations_for_WACV_2024_paper.pdf,,,2310.17764,main,Poster,https://ieeexplore.ieee.org/document/10484387/,"['Deep learning', 'Image segmentation', 'Analytical models', 'Computer vision', 'Image analysis', 'Computer architecture', 'Skin']","['Medical Imaging', 'Medical Image Segmentation', 'Precise Segmentation', 'Deep Learning Models', 'Skin Lesions', 'Intersection Over Union', 'Representation Learning', 'Latent Space', 'Medical Image Analysis', 'Segmentation Dataset', 'Fine-grained Details', 'Brain Tumor Segmentation', 'Impaired Function', 'Quantum', 'Synapse', 'Input Image', 'Softmax Function', 'Segmentation Results', 'Segmentation Accuracy', 'Dice Similarity Coefficient', 'Attention Heads', 'Attention Weights', 'Vector Quantization', 'Fine-grained Structure', 'Image X', 'Relevance Score', 'Binary Cross Entropy', 'Hidden Dimension', 'Robust Segmentation', 'Self-attention Mechanism']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Image recognition and understanding']",5,"In recent years, continuous latent space (CLS) and discrete latent space (DLS) deep learning models have been proposed for medical image analysis for improved performance. However, these models encounter distinct challenges. CLS models capture intricate details but often lack interpretability in terms of structural representation and robustness due to their emphasis on low-level features. Conversely, DLS models offer interpretability, robustness, and the ability to capture coarse-grained information thanks to their structured latent space. However, DLS models have limited efficacy in capturing fine-grained details. To address the limitations of both DLS and CLS models, we propose SynergyNet, a novel bottleneck architecture designed to enhance existing encoder-decoder segmentation frameworks. SynergyNet seamlessly integrates discrete and continuous representations to harness complementary information and successfully preserves both fine and coarse-grained details in the learned representations. Our extensive experiment on multi-organ segmentation and cardiac datasets demonstrates that SynergyNet outperforms other state of the art methods including TransUNet: dice scores improving by 2.16%, and Hausdorff scores improving by 11.13%, respectively. When evaluating skin lesion and brain tumor segmentation datasets, we observe a remarkable improvement of 1.71% in Intersection-over-Union scores for skin lesion segmentation and of 8.58% for brain tumor segmentation. Our innovative approach paves the way for enhancing the overall performance and capabilities of deep learning models in the critical domain of medical image analysis."
SynthProv: Interpretable Framework for Profiling Identity Leakage,"Jaisidh Singh, Harshil Bhatia, Mayank Vatsa, Richa Singh, Aparna Bharati","Lehigh University, PA, USA; IIT Jodhpur, India",100.0,"India, USA",0.0,,"Generative Adversarial Networks (GANs) can generate hyperrealistic face images of synthetic identities based on a latent understanding of real images from a large training set. Despite their proficiency, the term ""synthetic identity"" remains ambiguous, and the uniqueness of the faces GANs produce is rarely assessed. Recent studies have found that identities from the training data can unintentionally appear in the faces generated by StyleGAN2, but the cause of this phenomenon is unclear. In this work, we propose a novel framework, SynthProv, that utilizes the improved interpolation ability of StyleGAN2 latent space and employs image composition to analyze leakage. This is the first method that goes beyond detection and traces the source or provenance of constituent identity signals in the generated image. Experiments show that SynthProv succeeds in both detection and provenance tasks using multiple matching strategies. We identify identities from FFHQ and CelebA-HQ training datasets with the highest leakage into the latent space as ""leaking reals"". Analyzing latent space behavior to evaluate generative model privacy via leakage is an important research direction, as undetected leaking reals pose a significant threat to training data privacy. Our code is available at https://github.com/jaisidhsingh/SynthProv",https://openaccess.thecvf.com/content/WACV2024/html/Singh_SynthProv_Interpretable_Framework_for_Profiling_Identity_Leakage_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Singh_SynthProv_Interpretable_Framework_for_Profiling_Identity_Leakage_WACV_2024_paper.pdf,,https://github.com/jaisidhsingh/SynthProv,,main,Poster,https://ieeexplore.ieee.org/document/10484178/,"['Training', 'Data privacy', 'Privacy', 'Interpolation', 'Computational modeling', 'Training data', 'Data models']","['Identity Leakage', 'Privacy', 'Training Set', 'Training Data', 'Generative Adversarial Networks', 'Latent Space', 'Face Images', 'Composite Image', 'Cause Of This Phenomenon', 'Identical Signals', 'Identity Information', 'Training Images', 'Image Generation', 'Representation Of Space', 'Information Leakage', 'Learning Spaces', 'Synthetic Images', 'Latent Representation', 'Directions In Space', 'Matching Score', 'Synthetic Composites', 'Latent Vector', 'Face Matching', 'Real Faces', 'Deepfake', 'Respective Parent', 'Presence Of Leakage', 'Latent Distribution', 'Query Image', 'Dissimilarity Score']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",2,"Generative Adversarial Networks (GANs) can generate hyperrealistic face images of synthetic identities based on a latent understanding of real images from a large training set. Despite their proficiency, the term ""synthetic identity"" remains ambiguous, and the uniqueness of the faces GANs produce is rarely assessed. Recent studies have found that identities from the training data can unintentionally appear in the faces generated by StyleGAN2, but the cause of this phenomenon is unclear. In this work, we propose a novel framework, SynthProv, that utilizes the improved interpolation ability of StyleGAN2 latent space and employs image composition to analyze leakage. This is the first method that goes beyond detection and traces the source or provenance of constituent identity signals in the generated image. Experiments show that SynthProv succeeds in both detection and provenance tasks using multiple matching strategies. We identify identities from FFHQ and CelebA-HQ training datasets with the highest leakage into the latent space as ""leaking reals"". Analyzing latent space behavior to evaluate generative model privacy via leakage is an important research direction, as undetected leaking reals pose a significant threat to training data privacy. Our code is available at https://github.com/jaisidhsingh/SynthProv."
SyntheWorld: A Large-Scale Synthetic Dataset for Land Cover Mapping and Building Change Detection,"Jian Song, Hongruixuan Chen, Naoto Yokoya","The University of Tokyo, Japan; RIKEN AIP, Japan",100.0,Japan,0.0,,"Synthetic datasets, recognized for their cost effectiveness, play a pivotal role in advancing computer vision tasks and techniques. However, when it comes to remote sensing image processing, the creation of synthetic datasets becomes challenging due to the demand for larger-scale and more diverse 3D models. This complexity is compounded by the difficulties associated with real remote sensing datasets, including limited data acquisition and high annotation costs, which amplifies the need for high-quality synthetic alternatives. To address this, we present SyntheWorld, a synthetic dataset unparalleled in quality, diversity, and scale. It includes 40,000 images with submeter-level pixels and fine-grained land cover annotations of eight categories, and it also provides 40,000 pairs of bitemporal image pairs with building change annotations for building change detection. We conduct experiments on multiple benchmark remote sensing datasets to verify the effectiveness of SyntheWorld and to investigate the conditions under which our synthetic data yield advantages. The dataset is available at https://github.com/JTRNEO/SyntheWorld.",https://openaccess.thecvf.com/content/WACV2024/html/Song_SyntheWorld_A_Large-Scale_Synthetic_Dataset_for_Land_Cover_Mapping_and_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Song_SyntheWorld_A_Large-Scale_Synthetic_Dataset_for_Land_Cover_Mapping_and_WACV_2024_paper.pdf,,https://github.com/JTRNEO/SyntheWorld,2309.01907,main,Poster,https://ieeexplore.ieee.org/document/10483785/,"['Computer vision', 'Solid modeling', 'Costs', 'Three-dimensional displays', 'Annotations', 'Buildings', 'Land surface']","['Land Cover Map', 'Building Change', 'Building Change Detection', 'Computer Vision', 'Remote Sensing', 'Image Pairs', 'Remote Sensing Images', 'Remote Sensing Image Processing', 'Urban Areas', 'Distancing Measures', 'F1 Score', 'Object Detection', 'Virtual World', 'Aerial Images', 'Semantic Segmentation', 'Real-world Datasets', 'Optical Flow', 'Synthetic Images', 'High-quality Dataset', 'Field Of Computer Vision', 'Domain Dataset', 'Flow Algorithm', 'Semantic Labels', 'Optical Flow Algorithm', 'Paired Datasets']",,2,"Synthetic datasets, recognized for their cost effectiveness, play a pivotal role in advancing computer vision tasks and techniques. However, when it comes to remote sensing image processing, the creation of synthetic datasets becomes challenging due to the demand for larger-scale and more diverse 3D models. This complexity is compounded by the difficulties associated with real remote sensing datasets, including limited data acquisition and high annotation costs, which amplifies the need for high-quality synthetic alternatives. To address this, we present SyntheWorld, a synthetic dataset unparalleled in quality, diversity, and scale. It includes 40,000 images with submeter-level pixels and fine-grained land cover annotations of eight categories, and it also provides 40,000 pairs of bitemporal image pairs with building change annotations for building change detection. We conduct experiments on multiple benchmark remote sensing datasets to verify the effectiveness of SyntheWorld and to investigate the conditions under which our synthetic data yield advantages. The dataset is available at https://github.com/JTRNEO/SyntheWorld."
"Synthesizing Anyone, Anywhere, in Any Pose","Håkon Hukkelås, Frank Lindseth",Norwegian University of Science and Technology,100.0,Norway,0.0,,"We address the task of in-the-wild human figure synthesis, where the primary goal is to synthesize a full body given any region in any image. In-the-wild human figure synthesis has long been a challenging and under-explored task, where current methods struggle to handle extreme poses, occluding objects, and complex backgrounds. Our main contribution is TriA-GAN, a keypoint-guided GAN that can synthesize Anyone, Anywhere, in Any given pose. Key to our method is projected GANs combined with a well-crafted training strategy, where our simple generator architecture can successfully handle the challenges of in-the-wild full-body synthesis. We show that TriA-GAN significantly improves over previous in-the-wild full-body synthesis methods, all while requiring less conditional information for synthesis (keypoints v.s. DensePose). Finally, we show that the latent space of TriA-GAN is compatible with standard unconditional editing techniques, enabling text-guided editing of generated human figures.",https://openaccess.thecvf.com/content/WACV2024/html/Hukkelas_Synthesizing_Anyone_Anywhere_in_Any_Pose_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hukkelas_Synthesizing_Anyone_Anywhere_in_Any_Pose_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484384/,"['Training', 'Deepfakes', 'Watermarking', 'Generators', 'Web sites', 'Reliability', 'Multimedia communication']","['Training Strategy', 'Generative Adversarial Networks', 'Latent Space', 'Complex Background', 'Human Figure', 'Image Quality', 'Image Regions', 'Receptive Field', 'ImageNet', 'Residual Block', 'Skip Connections', 'Pre-trained Network', 'Content Creation', 'Progressive Training', 'Improve Image Quality', 'U-Net Architecture', 'Missing Regions', 'Image Inpainting', 'Fréchet Inception Distance', 'Synthesis Quality', 'Pre-training Tasks']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"We address the task of in-the-wild human figure synthesis, where the primary goal is to synthesize a full body given any region in any image. In-the-wild human figure synthesis has long been a challenging and under-explored task, where current methods struggle to handle extreme poses, occluding objects, and complex backgrounds.Our main contribution is TriA-GAN, a keypoint-guided GAN that can synthesize Anyone, Anywhere, in Any given pose. Key to our method is projected GANs combined with a well-crafted training strategy, where our simple generator architecture can successfully handle the challenges of in-the-wild full-body synthesis. We show that TriA-GAN significantly improves over previous in-the-wild full-body synthesis methods, all while requiring less conditional information for synthesis (keypoints vs. DensePose). Finally, we show that the latent space of TriA-GAN is compatible with standard unconditional editing techniques, enabling text-guided editing of generated human figures."
Synthesizing Coherent Story With Auto-Regressive Latent Diffusion Models,"Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen",New York University; Alibaba Group; University of Waterloo,66.66666666666666,"Canada, USA",33.33333333333334,China,"Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. It also extends the text-conditioned method to multimodal conditioning. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the adopted challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",https://openaccess.thecvf.com/content/WACV2024/html/Pan_Synthesizing_Coherent_Story_With_Auto-Regressive_Latent_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pan_Synthesizing_Coherent_Story_With_Auto-Regressive_Latent_Diffusion_Models_WACV_2024_paper.pdf,,https://this https URL,2211.10950,main,Poster,https://ieeexplore.ieee.org/document/10484137/,"['Uniform resource locators', 'Visualization', 'Adaptation models', 'Computer vision', 'Codes', 'Computational modeling', 'Decoding']","['Autoregressive Model', 'Diffusion Model', 'Latent Model', 'Coherent Story', 'Quantitative Results', 'Series Of Images', 'Coherent Imaging', 'Large-scale Evaluation', 'Diffusion Process', 'Generative Adversarial Networks', 'Latent Space', 'Image Generation', 'Visual Quality', 'Source Images', 'Real-world Use', 'Real-world Case', 'Test Bed', 'Forward Process', 'Text Encoder', 'Real-world Use Cases']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities', 'Applications', 'Visualization']",6,"Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. It also extends the text-conditioned method to multimodal conditioning. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the adopted challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency. Code available at this https URL"
TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains,"Alexander Naumann, Felix Hertlein, Laura Dörr, Kai Furmans","FZI and KIT, Karlsruhe, Germany",100.0,Germany,0.0,,"Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar.",https://openaccess.thecvf.com/content/WACV2024/html/Naumann_TAMPAR_Visual_Tampering_Detection_for_Parcel_Logistics_in_Postal_Supply_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Naumann_TAMPAR_Visual_Tampering_Detection_for_Parcel_Logistics_in_Postal_Supply_WACV_2024_paper.pdf,https://a-nau.github.io/tampar,https://github.com/a-nau/tampar,,main,Poster,https://ieeexplore.ieee.org/document/10483965/,"['Measurement', 'Visualization', 'Systematics', 'Sensitivity analysis', 'Shape', 'Supply chains', 'Pipelines']","['Supply Chain', 'Sensitivity Analysis', 'Change Detection', 'Single Image', 'Viewing Angle', 'Changes In Appearance', 'Image Distortion', 'Side Surface', 'Corner Points', 'Visible Surface', 'Keypoint Detection', 'Perspective Transformation', 'Visible Side', 'Change Detection Approach', 'Learning Rate', 'Similarity Measure', 'Input Image', 'Real-world Data', 'Bounding Box', 'Real-world Datasets', '3D Bounding Box', 'Front Side', 'Histogram Of Oriented Gradients', 'Visible Edges', 'Strong Distortion', 'Instance Segmentation', 'Rightmost Point', 'Detection Dataset', 'Security Considerations', 'Simple Threshold']","['Applications', 'Commercial / retail', 'Applications', 'Robotics', 'Applications', 'Smartphones / end user devices']",,"Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar."
TCP: Triplet Contrastive-Relationship Preserving for Class-Incremental Learning,"Shiyao Li, Xuefei Ning, Shanghang Zhang, Lidong Guo, Tianchen Zhao, Huazhong Yang, Yu Wang","Tsinghua University, China; Peking University, China",100.0,China,0.0,,"In class-incremental learning (CIL), when deep neural networks learn new classes, their recognition performance in old classes will drop significantly. This phenomenon is widely known as catastrophic forgetting. To alleviate catastrophic forgetting, existing methods store a small portion of old class data with a memory buffer and replay it while learning new classes. These methods suffer from a severe imbalance problem between old and new classes. In this paper, we discover that the imbalance problem in CIL makes it difficult to preserve the feature relation of old classes and hard to learn the feature relation between old and new classes. To mitigate the above two issues, we design a triplet contrastive preserving (TCP) loss to preserve old knowledge, and propose an asymmetric augmented contrastive learning (A2CL) method to learn new classes. Comprehensive experiments demonstrate the effectiveness of our method, which increases the average accuracies by 1.26% and 0.95% on CIFAR-100 and ImageNet. Especially under smaller memory buffer settings where the imbalance problem is more severe, our method can surpass the baselines by a large margin (up to 3.2%). We also show that TCP can be easily plugged into other methods and further improve their performance.",https://openaccess.thecvf.com/content/WACV2024/html/Li_TCP_Triplet_Contrastive-Relationship_Preserving_for_Class-Incremental_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_TCP_Triplet_Contrastive-Relationship_Preserving_for_Class-Incremental_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484441/,"['Computer vision', 'Self-supervised learning', 'Artificial neural networks']","['Class-incremental Learning', 'Deep Neural Network', 'Average Accuracy', 'Portion Of Data', 'Imbalance Problem', 'Self-supervised Learning', 'Severe Imbalance', 'Catastrophic Forgetting', 'Small Portion Of Data', 'Training Data', 'Feature Values', 'Feature Space', 'Data Augmentation', 'Exact Value', 'Representation Of Space', 'Incremental Learning', 'Augmentation Strategy', 'Contrastive Loss', 'Triplet Loss', 'Forward Transfer', 'Distillation Loss', 'Phase Increment', 'CIFAR-100 Dataset']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In class-incremental learning (CIL), when deep neural networks learn new classes, their recognition performance in old classes will drop significantly. This phenomenon is widely known as catastrophic forgetting. To alleviate catastrophic forgetting, existing methods store a small portion of old class data with a memory buffer and replay it while learning new classes. These methods suffer from a severe imbalance problem between old and new classes. In this paper, we discover that the imbalance problem in CIL makes it difficult to preserve the feature relation of old classes and hard to learn the feature relation between old and new classes. To mitigate the above two issues, we design a triplet contrastive preserving (TCP) loss to preserve old knowledge, and propose an asymmetric augmented contrastive learning (A2CL) method to learn new classes. Comprehensive experiments demonstrate the effectiveness of our method, which increases the average accuracies by 1.26% and 0.95% on CIFAR-100 and ImageNet. Especially under smaller memory buffer settings where the imbalance problem is more severe, our method can surpass the baselines by a large margin (up to 3.2%). We also show that TCP can be easily plugged into other methods and further improve their performance."
TEGLO: High Fidelity Canonical Texture Mapping From Single-View Images,"Vishal Vinod, Tanmay Shah, Dmitry Lagun","University of California, San Diego; Google Research",50.0,USA,50.0,USA,"Recent work in Neural Fields (NFs) learn 3D representations from class-specific single view image collections. However, they are unable to reconstruct the input data preserving high-frequency details. Further, these methods do not disentangle appearance from geometry and hence are not suitable for tasks such as texture transfer and editing. In this work, we propose TEGLO (Textured EG3D-GLO) for learning 3D representations from single view in-the-wild image collections for a given class of objects. We accomplish this by training a conditional Neural Radiance Field (NeRF) without any explicit 3D supervision. We equip our method with editing capabilities by creating a dense correspondence mapping to a 2D canonical space. We demonstrate that such mapping enables texture transfer and texture editing without requiring meshes with shared topology. Our key insight is that by mapping the input image pixels onto the texture space we can achieve near perfect reconstruction (>74 dB PSNR at 1024^2 resolution). Our formulation allows for high quality 3D consistent novel view synthesis with high-frequency details even at megapixel image resolutions.",https://openaccess.thecvf.com/content/WACV2024/html/Vinod_TEGLO_High_Fidelity_Canonical_Texture_Mapping_From_Single-View_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Vinod_TEGLO_High_Fidelity_Canonical_Texture_Mapping_From_Single-View_Images_WACV_2024_paper.pdf,https://teglo-nerf.github.io,https://github.com/teglo-nerf,2303.13743,main,Poster,https://ieeexplore.ieee.org/document/10484168/,"['Training', 'Geometry', 'Computer vision', 'Three-dimensional displays', 'Image resolution', 'Topology', 'Task analysis']","['Canonical Map', 'Single-view Image', 'Input Image', 'Image Pixels', 'Image Collection', '3D Representation', 'Single View', 'Consistent View', '3D Learning', 'View Synthesis', 'Perfect Reconstruction', 'Dense Correspondence', 'Qualitative Results', '3D Reconstruction', 'Image Object', 'RGB Images', 'Baseline Methods', '3D Point', 'Discrete Space', 'Ground Truth Image', 'Camera Pose', 'Surface Normals', 'Texture Representation', 'Surface Points', 'RGB Values', 'Reconstruction Loss', 'Surface Field']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"Recent work in Neural Fields (NFs) learn 3D representations from class-specific single view image collections. However, they are unable to reconstruct the input data preserving high-frequency details. Further, these methods do not disentangle appearance from geometry and hence are not suitable for tasks such as texture transfer and editing. In this work, we propose TEGLO (Textured EG3D-GLO) for learning 3D representations from single view in-the-wild image collections for a given class of objects. We accomplish this by training a conditional Neural Radiance Field (NeRF) without any explicit 3D supervision. We equip our method with editing capabilities by creating a dense correspondence mapping to a 2D canonical space. We demonstrate that such mapping enables texture transfer and texture editing without requiring meshes with shared topology. Our key insight is that by mapping the input image pixels onto the texture space we can achieve near perfect reconstruction (≥ 74 dB PSNR at 1024
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 resolution). Our formulation allows for high quality 3D consistent novel view synthesis with high-frequency details even at megapixel image resolutions. Project Page: teglo-nerf.github.io"
THInImg: Cross-Modal Steganography for Presenting Talking Heads in Images,"Lin Zhao, Hongxuan Li, Xuefei Ning, Xinru Jiang","Department of Computer Science, University of British Columbia; Department of Electronic Engineering, Tsinghua University; TKLNDST, CS, Nankai University",100.0,"Canada, China",0.0,,"Cross-modal Steganography is the practice of concealing secret signals in publicly available cover signals (distinct from the modality of the secret signals) unobtrusively. While previous approaches primarily concentrated on concealing a relatively small amount of information, we propose THInImg, which manages to hide lengthy audio data (and subsequently decode talking head video) inside an identity image by leveraging the properties of human face, which can be effectively utilized for covert communication, transmission and copyright protection. THInImg consists of two parts: the encoder and decoder. Inside the encoder-decoder pipeline, we introduce a novel architecture that substantially increase the capacity of hiding audio in images. Moreover, our framework can be extended to iteratively hide multiple audio clips into an identity image, offering multiple levels of control over permissions. We conduct extensive experiments to prove the effectiveness of our method, demonstrating that THInImg can present up to 80 seconds of high quality talking-head video (including audio) in an identity image with 160x160 resolution.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_THInImg_Cross-Modal_Steganography_for_Presenting_Talking_Heads_in_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_THInImg_Cross-Modal_Steganography_for_Presenting_Talking_Heads_in_Images_WACV_2024_paper.pdf,,,2311.17177,main,Poster,https://ieeexplore.ieee.org/document/10484053/,"['Steganography', 'Computer vision', 'Image resolution', 'Pipelines', 'Computer architecture', 'Copyright protection', 'Encoding']","['Head Imaging', 'Talking Head', 'Identical Images', 'Audio Data', 'Audio Clips', 'Small Amount Of Information', 'Training Set', 'Image Quality', 'Source Code', 'Number Of Images', 'Deep Convolutional Neural Network', 'Generative Adversarial Networks', 'Spatial Domain', 'Video Frames', 'Face Images', 'Peak Signal-to-noise Ratio', 'Shallow Layers', 'Short-time Fourier Transform', 'Video Quality', 'Mean Opinion Score', 'Audio Quality', 'Least Significant Bit', 'Human Auditory System', 'Audiovisual Information', 'Number Of Utterances']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Computational photography', 'image and video synthesis']",2,"Cross-modal Steganography is the practice of concealing secret signals in publicly available cover signals (distinct from the modality of the secret signals) unobtrusively. While previous approaches primarily concentrated on concealing a relatively small amount of information, we propose THInImg, which manages to hide lengthy audio data (and subsequently decode talking head video) inside an identity image by leveraging the properties of human face, which can be effectively utilized for covert communication, transmission and copyright protection. THInImg consists of two parts: the encoder and decoder. Inside the encoder-decoder pipeline, we introduce a novel architecture that substantially increase the capacity of hiding audio in images. Moreover, our framework can be extended to iteratively hide multiple audio clips into an identity image, offering multiple levels of control over permissions. We conduct extensive experiments to prove the effectiveness of our method, demonstrating that THInImg can present up to 80 seconds of high quality talking-head video (including audio) in an identity image with 160×160 resolution."
TIAM - A Metric for Evaluating Alignment in Text-to-Image Generation,"Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille","Universitè Paris-Saclay, CEA, List, F-91120, Palaiseau, France",100.0,France,0.0,,"The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically depending on the noise used as a seed for the images. We also quantify the influence of the number of concepts in the prompt, their order as well as their (color) attributes. Finally, our method allows us to identify some seeds that produce better images than others, opening novel directions of research on this understudied topic.",https://openaccess.thecvf.com/content/WACV2024/html/Grimal_TIAM_-_A_Metric_for_Evaluating_Alignment_in_Text-to-Image_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Grimal_TIAM_-_A_Metric_for_Evaluating_Alignment_in_Text-to-Image_Generation_WACV_2024_paper.pdf,,https://github.com/grimalPaul/TIAM,,main,Poster,,,,,,
TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain,"Shen Zheng, Changjie Lu, Srinivasa G. Narasimhan",Carnegie Mellon University; University of Illinois Urbana-Champaign,100.0,USA,0.0,,"Rain generation algorithms have the potential to improve the generalization of deraining methods and scene understanding in rainy conditions. However, in practice, they produce artifacts and distortions and struggle to control the amount of rain generated due to a lack of proper constraints. In this paper, we propose an unpaired image-to-image translation framework for generating realistic rainy images. We first introduce a Triangular Probability Similarity (TPS) constraint to guide the generated images toward clear and rainy images in the discriminator manifold, thereby minimizing artifacts and distortions during rain generation. Unlike conventional contrastive learning approaches, which indiscriminately push negative samples away from the anchors, we propose a Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing force of negative samples based on the semantic similarity between the clear and the rainy images and the feature similarity between the anchor and the negative samples. Experiments demonstrate realistic rain generation with minimal artifacts and distortions, which benefits image deraining and object detection in rain. Furthermore, the method can be used to generate realistic snowy and night images, underscoring its potential for broader applicability. Code is available at https://github.com/ShenZheng2000/TPSeNCE.",https://openaccess.thecvf.com/content/WACV2024/html/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zheng_TPSeNCE_Towards_Artifact-Free_Realistic_Rain_Generation_for_Deraining_and_Object_WACV_2024_paper.pdf,,https://github.com/ShenZheng2000/TPSeNCE,2311.00660,main,Poster,https://ieeexplore.ieee.org/document/10484120/,"['Manifolds', 'Rain', 'Semantics', 'Noise', 'Estimation', 'Lighting', 'Object detection']","['Object Detection', 'Semantic Similarity', 'Estimation Strategy', 'Clear Image', 'Self-supervised Learning', 'Realistic Images', 'Image Object Detection', 'Lack Of Constraints', 'Amount Of Rain', 'Rainy Conditions', 'Minimal Artifacts', 'Object Size', 'Heavy Rain', 'Weighting Scheme', 'IoU Threshold']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Low-level and physics-based vision']",1,"Rain generation algorithms have the potential to improve the generalization of deraining methods and scene understanding in rainy conditions. However, in practice, they produce artifacts and distortions and struggle to control the amount of rain generated due to a lack of proper constraints. In this paper, we propose an unpaired image-to-image translation framework for generating realistic rainy images. We first introduce a Triangular Probability Similarity (TPS) constraint to guide the generated images toward clear and rainy images in the discriminator manifold, thereby minimizing artifacts and distortions during rain generation. Unlike conventional contrastive learning approaches, which indiscriminately push negative samples away from the anchors, we propose a Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing force of negative samples based on the semantic similarity between the clear and the rainy images and the feature similarity between the anchor and the negative samples. Experiments demonstrate realistic rain generation with minimal artifacts and distortions, which benefits image deraining and object detection in rain. Furthermore, the method can be used to generate realistic snowy and night images, underscoring its potential for broader applicability. Code is available at https://github.com/ShenZheng2000/TPSeNCE."
TSA2: Temporal Segment Adaptation and Aggregation for Video Harmonization,"Zeyu Xiao, Yurui Zhu, Xueyang Fu, Zhiwei Xiong",University of Science and Technology of China,100.0,China,0.0,,"Video composition merges the foreground and background of different videos, presenting challenges due to variations in capture conditions (e.g., saturation, brightness, and contrast). Video harmonization is a vital process in achieving a realistic composite by seamlessly adjusting the foreground's appearance to match the background. In this paper, we propose TSA2, a novel method for video harmonization that incorporates temporal segment adaptation and aggregation. TSA2 divides the inharmonious input sequence into temporal segments, each corresponding to a different frame rate, allowing effective utilization of complementary information within each segment. The method includes the Temporal Segment Adaptation module, which learns and remaps the distribution difference between background and foreground regions, and the Temporal Segment Aggregation module, which emphasizes and aggregates cross-segment information through element-wise correlations. Experimental results demonstrate that TSA2 outperforms advanced image and video harmonization methods quantitatively and qualitatively.",https://openaccess.thecvf.com/content/WACV2024/html/Xiao_TSA2_Temporal_Segment_Adaptation_and_Aggregation_for_Video_Harmonization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Xiao_TSA2_Temporal_Segment_Adaptation_and_Aggregation_for_Video_Harmonization_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding,"Shuo Wang, Jing Li, Zibo Zhao, Dongze Lian, Binbin Huang, Xiaomei Wang, Zhengxin Li, Shenghua Gao","National University of Singapore; Xiaohongshu Inc.; Fudan University; ShanghaiTech University; ShanghaiTech University, Shanghai Engineering Research Center of Intelligent Vision and Imaging, Shanghai Engineering Research Center of Energy E ﬃcient and Custom AI IC",80.0,"China, Singapore",20.0,China,"Holistic scene understanding includes semantic segmentation, surface normal estimation, object boundary detection, depth estimation, etc. The key aspect of this problem is to learn representation effectively, as each subtask builds upon not only correlated but also distinct attributes. Inspired by visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed TSP-Transformer, for holistic scene understanding. It features a vanilla transformer in the early stage and tasks-specific prompts transformer encoder in the lateral stage, where tasks-specific prompts are augmented. By doing so, the transformer layer learns the generic information from the shared parts and is endowed with task-specific capacity. First, the tasks-specific prompts serve as induced priors for each task effectively. Moreover, the task-specific prompts can be seen as switches to favor task-specific representation learning for different tasks. Extensive experiments on NYUD-v2 and PASCAL-Context show that our method achieves state-of-the-art performance, validating the effectiveness of our method for holistic scene understanding.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_TSP-Transformer_Task-Specific_Prompts_Boosted_Transformer_for_Holistic_Scene_Understanding_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_TSP-Transformer_Task-Specific_Prompts_Boosted_Transformer_for_Holistic_Scene_Understanding_WACV_2024_paper.pdf,,https://github.com/tb2-sy/TSP-Transformer,,main,Poster,https://ieeexplore.ieee.org/document/10484255/,"['Representation learning', 'Computer vision', 'Fuses', 'Semantic segmentation', 'Estimation', 'Transformers', 'Decoding']","['Scene Understanding', 'Holistic Scene Understanding', 'Semantic Segmentation', 'Holistic Understanding', 'Depth Estimation', 'Boundary Detection', 'Transformer Layers', 'Transformer Encoder', 'Convolutional Neural Network', 'Intersection Over Union', 'Multilayer Perceptron', 'Unified Model', 'Feature Fusion', 'Image Patches', 'Task Characteristics', 'Multi-task Learning', 'Encoder Layer', 'Saliency Detection', 'Neural Architecture Search', 'Multi-head Self-attention', 'Task-specific Features', 'Layer In Stage', 'Vision Transformer', 'Monocular Depth Estimation', 'Transformer Decoder', 'Element-wise Summation', 'Shared Layers', 'Encoder Module', 'Multi-scale Features']","['Algorithms', 'Image recognition and understanding']",,"Holistic scene understanding includes semantic segmentation, surface normal estimation, object boundary detection, depth estimation, etc. The key aspect of this problem is to learn representation effectively, as each subtask builds upon not only correlated but also distinct attributes. Inspired by visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed TSP-Transformer, for holistic scene understanding. It features a vanilla transformer in the early stage and tasks-specific prompts transformer encoder in the lateral stage, where tasks-specific prompts are augmented. By doing so, the transformer layer learns the generic information from the shared parts and is endowed with task-specific capacity. First, the tasks-specific prompts serve as induced priors for each task effectively. Moreover, the task-specific prompts can be seen as switches to favor task-specific representation learning for different tasks. Extensive experiments on NYUD-v2 and PASCAL-Context show that our method achieves state-of-the-art performance, validating the effectiveness of our method for holistic scene understanding. We also provide our code in the following link 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering,"Xiulong Liu, Zhikang Dong, Peng Zhang",Bytedance Inc; University of Washington; Stony Brook University,66.66666666666666,USA,33.33333333333334,China,"In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that delves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0, this model surpasses all the existing benchmarks, improving accuracy by 2% on MUSIC-AVQA v2.0, setting a new state-of-the-art performance.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Tackling_Data_Bias_in_MUSIC-AVQA_Crafting_a_Balanced_Dataset_for_WACV_2024_paper.pdf,,https://github.com/DragonLiu1995/MUSIC-AVQA-v2.0,,main,Poster,https://ieeexplore.ieee.org/document/10484352/,"['Computer vision', 'Reviews', 'Computational modeling', 'Benchmark testing', 'Task analysis', 'Videos']","['Question Answering', 'Balanced Dataset', 'Benchmark', 'Skewed Distribution', 'Types Of Questions', 'Question Categories', 'Training Set', 'Feature Maps', 'Identity Crisis', 'Visual Map', 'Balance Test', 'Bias Test', 'Types Of Instruments', 'Balanced Distribution', 'Issue Of Bias', 'Balanced Set', 'Number Of Instruments', 'Answer Categories', 'Total Accuracy', 'Frequent Category', 'Visual Question Answering', 'Frequent Answer', 'Balanced Training Set', 'Sound Instrument', 'Visual Context', 'Visual Question', 'Representation Learning', 'Visual Modality']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Video recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",3,"In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model’s ability to effectively reason across these diverse modalities is compromised, impeding further advancement.In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that delves deeper into the audiovisual-text interrelation. On MUSIC-AVQA v2.0, this model surpasses all the existing benchmarks, improving accuracy by 2% on MUSIC-AVQA v2.0, setting a new state-of-theart performance. Dataset: https://github.com/DragonLiu1995/MUSIC-AVQA-v2.0/"
Taming Normalizing Flows,"Shimon Malnick, Shai Avidan, Ohad Fried",Tel Aviv University; Reichman University,100.0,Israel,0.0,,"We propose an algorithm for taming Normalizing Flow models - changing the probability that the model will produce a specific image or image category. We focus on Normalizing Flows because they can calculate the exact generation probability likelihood for a given image. We demonstrate taming using models that generate human faces, a subdomain with many interesting privacy and bias considerations. Our method can be used in the context of privacy, e.g., removing a specific person from the output of a model, and also in the context of debiasing by forcing a model to output specific image categories according to a given distribution. Taming is achieved with a fast fine-tuning process without retraining from scratch, achieving the goal in a matter of minutes. We evaluate our method qualitatively and quantitatively, showing that the generation quality remains intact, while the desired changes are applied.",https://openaccess.thecvf.com/content/WACV2024/html/Malnick_Taming_Normalizing_Flows_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Malnick_Taming_Normalizing_Flows_WACV_2024_paper.pdf,https://shimonmalnick.github.io/taming_norm_flows,,2211.16488,main,Poster,https://ieeexplore.ieee.org/document/10483877/,"['Privacy', 'Computer vision', 'Computational modeling', 'Estimation', 'Probability', 'Data models', 'Faces']","['Normal Flow', 'Human Faces', 'Generation Probability', 'Debiasing', 'Training Set', 'Training Data', 'Population Groups', 'Probability Density', 'Probability Density Function', 'Kullback-Leibler', 'Latent Space', 'Precision Rate', 'Image Space', 'Jacobian Matrix', 'Incremental Learning', 'Output Distribution', 'Identical Images', 'Latent Vector', 'Negative Log-likelihood']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"We propose an algorithm for taming Normalizing Flow models — changing the probability that the model will produce a specific image or image category. We focus on Normalizing Flows because they can calculate the exact generation probability likelihood for a given image. We demonstrate taming using models that generate human faces, a subdomain with many interesting privacy and bias considerations. Our method can be used in the context of privacy, e.g., removing a specific person from the output of a model, and also in the context of debiasing by forcing a model to output specific image categories according to a given distribution. Taming is achieved with a fast fine-tuning process without retraining from scratch, achieving the goal in a matter of minutes. We evaluate our method qualitatively and quantitatively, showing that the generation quality remains intact, while the desired changes are applied."
Task-Oriented Human-Object Interactions Generation With Implicit Neural Representations,"Quanzhou Li, Jingbo Wang, Chen Change Loy, Bo Dai","S-Lab, Nanyang Technological University; Shanghai AI Laboratory",50.0,Singapore,50.0,China,"Digital human motion synthesis is a vibrant research field with applications in movies, AR/VR, and video games. Whereas methods were proposed to generate natural and realistic human motions, most only focus on modeling humans and largely ignore object movements. Generating task-oriented human-object interaction motions in simulation is challenging. For different intents of using the objects, humans conduct various motions, which requires the human first to approach the objects and then make them move consistently with the human instead of staying still. Also, to deploy in downstream applications, the synthesized motions are desired to be flexible in length, providing options to personalize the predicted motions for various purposes. To this end, we propose TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations, which generates full human-object interaction motions to conduct specific tasks, given only the task type, the object, and a starting human status. TOHO generates human-object motions in four steps: 1) it first estimates the object's final position given the task intent; 2) it then generates keyframe poses grasping the objects; 3) after that, it infills the keyframes and generates continuous motions; 4) finally, it applies a compact closed-form object motion estimation to generate the object motion. Our method generates continuous motions that are parameterized only by the temporal coordinate, which allows for upsampling of the sequence to arbitrary frames and adjusting the motion speeds by designing the temporal coordinate vector. This work takes a step further toward general human-scene interaction simulation.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Task-Oriented_Human-Object_Interactions_Generation_With_Implicit_Neural_Representations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Task-Oriented_Human-Object_Interactions_Generation_With_Implicit_Neural_Representations_WACV_2024_paper.pdf,,,2303.13129,main,Poster,https://ieeexplore.ieee.org/document/10484213/,"['Video games', 'Computer vision', 'Motion estimation', 'Grasping', 'Motion pictures', 'Vectors', 'Digital humans']","['Human-object Interaction', 'Implicit Neural Representation', 'Video Games', 'Types Of Tasks', 'Object Motion', 'Downstream Applications', 'Human Motion', 'Continuous Motion', 'Temporal Coordination', 'Natural Motion', 'Compact Objects', 'Real Motion', 'Right-hand', 'Autoregressive Model', 'Kullback-Leibler', 'Object Shape', 'Object-oriented', 'Object Position', 'Global Translation', 'Human Pose', 'Human Shape', 'Motion Sequences', 'Unseen Objects', 'Object Trajectory', 'Ground Truth Trajectory', 'Offset Vector', 'Contact Ratio', 'Pose Prediction', 'Use Of Motion', 'Labeling Task']","['Algorithms', '3D computer vision', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",3,"Digital human motion synthesis is a vibrant research field with applications in movies, AR/VR, and video games. Whereas methods were proposed to generate natural and realistic human motions, most only focus on modeling humans and largely ignore object movements. Generating task-oriented human-object interaction motions in simulation is challenging. For different intents of using the objects, humans conduct various motions, which requires the human first to approach the objects and then make them move consistently with the human instead of staying still. Also, to deploy in downstream applications, the synthesized motions are desired to be flexible in length, providing options to personalize the predicted motions for various purposes. To this end, we propose TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations, which generates full human-object interaction motions to conduct specific tasks, given only the task type, the object, and a starting human status. TOHO generates human-object motions in four steps: 1) it first estimates the object’s final position given the task intent; 2) it then generates keyframe poses grasping the objects; 3) after that, it infills the keyframes and generates continuous motions; 4) finally, it applies a compact closed-form object motion estimation to generate the object motion. Our method generates continuous motions that are parameterized only by the temporal coordinate, which allows for upsampling of the sequence to arbitrary frames and adjusting the motion speeds by designing the temporal coordinate vector. This work takes a step further toward general human-scene interaction simulation."
Temporal Context Enhanced Referring Video Object Segmentation,"Xiao Hu, Basavaraj Hampiholi, Heiko Neumann, Jochen Lang","Ulm University, Germany; University of Ottawa, Canada",100.0,"Canada, Germany",0.0,,"The goal of Referring Video Object Segmentation is to extract an object from a video clip based on a given expression. While previous methods have utilized the transformer's multi-modal learning capabilities to aggregate information from different modalities, they have mainly focused on spatial information and paid less attention to temporal information. To enhance the learning of temporal information, we propose TCE-RVOS with a novel frame token fusion (FTF) structure and a novel instance query transformer (IQT). Our technical innovations maximize the potential information gain of videos over single images. Our contributions also include a new classification of two widely used validation datasets for investigation of challenging cases. Our experimental results demonstrate that TCE-RVOS effectively captures temporal information and outperforms the previous state-of-the-art methods by increasing the J&F score by 4.0 and 1.9 points using ResNet-50 and VSwin-Tiny as the backbone on Ref-Youtube-VOS, respectively, and +2.0 mAP on A2D-Sentences dataset by using VSwin-Tiny backbone. The code is available at https://github.com/haliphinx/TCE-RVOS",https://openaccess.thecvf.com/content/WACV2024/html/Hu_Temporal_Context_Enhanced_Referring_Video_Object_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hu_Temporal_Context_Enhanced_Referring_Video_Object_Segmentation_WACV_2024_paper.pdf,,https://github.com/haliphinx/TCE-RVOS,,main,Poster,https://ieeexplore.ieee.org/document/10483823/,"['Technological innovation', 'Object segmentation', 'Streaming media', 'Transformers', 'Real-time systems', 'Data models', 'Decoding']","['Object Segmentation', 'Temporal Context', 'Video Object Segmentation', 'Transformer', 'Temporal Information', 'Video Clips', 'Model Performance', 'Validation Set', 'Visual Information', 'Visual Features', 'Bounding Box', 'Video Frames', 'Linguistic Features', 'Feature Aggregation', 'Self-supervised Learning', 'Instance Segmentation', 'Challenging Scenarios', 'Segmentation Quality', 'Motion Blur', 'Early Fusion', 'Encoding Stage', 'Post-processing Stage', 'Frame Features', 'ResNet-50 Backbone', 'Temporal Aggregation', 'Temporal Learning', 'Multi-scale Features', 'Feature Pyramid Network', 'Frame Information', 'Objects In The Scene']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Video recognition and understanding']",1,"The goal of Referring Video Object Segmentation is to extract an object from a video clip based on a given expression. While previous methods have utilized the transformer’s multi-modal learning capabilities to aggregate information from different modalities, they have mainly focused on spatial information and paid less attention to temporal information. To enhance the learning of temporal information, we propose TCE-RVOS with a novel frame token fusion (FTF) structure and a novel instance query transformer (IQT). Our technical innovations maximize the potential information gain of videos over single images. Our contributions also include a new classification of two widely used validation datasets for investigation of challenging cases. Our experimental results demonstrate that TCERVOS effectively captures temporal information and outperforms the previous state-of-the-art methods by increasing the J&F score by 4.0 and 1.9 points using ResNet-50 and VSwin-Tiny as the backbone on Ref-Youtube-VOS, respectively, and +2.0 mAP on A2D-Sentences dataset by using VSwin-Tiny backbone. The code is available at https://github.com/haliphinx/TCE-RVOS"
Temporally-Consistent Video Semantic Segmentation With Bidirectional Occlusion-Guided Feature Propagation,"Razieh Kaviani Baghbaderani, Yuanxin Li, Shuangquan Wang, Hairong Qi","The University of Tennessee, Knoxville, TN, USA; SOC R &D, Samsung Semiconductor, Inc.",100.0,"South Korea, USA",0.0,,"Despite recent progress in static image segmentation, video segmentation is still challenging due to the need for an accurate, fast, and temporally consistent model. Conducting per-frame static image segmentation is not acceptable since it is computationally prohibitive and prone to temporal inconsistency. In this paper, we present bidirectional occlusion-guided feature propagation (BOFP) method with the goal of improving temporal consistency of segmentation results without sacrificing segmentation accuracy, while at the same time keeping the operations at a low computation cost. It leverages temporal coherence in the video by feature propagation from keyframes to other frames along the motion paths in both forward and backward directions. We propose an occlusion-based attention network to estimate the distorted areas based on bidirectional optical flows, and utilize them as cues for correcting and fusing the propagated features. Extensive experiments on benchmark datasets demonstrate that the proposed BOFP method achieves superior performance in terms of temporal consistency while maintaining comparable level of segmentation accuracy at a low computation cost, striking a great balance among the three metrics essential to evaluate video segmentation solutions.",https://openaccess.thecvf.com/content/WACV2024/html/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Baghbaderani_Temporally-Consistent_Video_Semantic_Segmentation_With_Bidirectional_Occlusion-Guided_Feature_Propagation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484220/,"['Measurement', 'Computer vision', 'Costs', 'Semantic segmentation', 'Motion segmentation', 'Computational modeling', 'Coherence']","['Semantic Segmentation', 'Propagation Characteristics', 'Bidirectional Propagation', 'Video Semantic Segmentation', 'Distortion', 'Computational Cost', 'Image Segmentation', 'Attention Network', 'Forward Direction', 'Segmentation Accuracy', 'Optical Flow', 'Low Computational Cost', 'Backward Direction', 'Bidirectional Flow', 'Video Segments', 'Temporal Consistency', 'Convolutional Layers', 'Object Detection', 'Flow Field', 'Segmentation Model', 'Frames Per Second', 'Attention Map', 'Current Frame', 'High-level Features', 'Optical Flow Estimation', 'Video Frames', 'Temporal Continuity', 'Object Boundaries', 'Flow Estimation', 'Fully Convolutional Network']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",,"Despite recent progress in static image segmentation, video segmentation is still challenging due to the need for an accurate, fast, and temporally consistent model. Conducting per-frame static image segmentation on a video is not acceptable since it is computationally prohibitive and prone to temporal inconsistency. In this paper, we present bidirectional occlusion-guided feature propagation (BOFP) method with the goal of improving temporal consistency of segmentation results without sacrificing segmentation accuracy, while at the same time keeping the operations at a low computation cost. It leverages temporal coherence in the video by feature propagation from keyframes to other frames along the motion paths in both forward and backward directions. We propose an occlusion-based attention network to estimate the distorted areas based on bidirectional optical flows, and utilize them as cues for correcting and fusing the propagated features. Extensive experiments on benchmark datasets demonstrate that the proposed BOFP method achieves superior performance in terms of temporal consistency while maintaining comparable level of segmentation accuracy at a low computation cost, striking a great balance among the three performance metrics essential to evaluate video segmentation solutions."
Text-Guided Face Recognition Using Multi-Granularity Cross-Modal Contrastive Learning,"Md Mahedi Hasan, Shoaib Meraj Sami, Nasser Nasrabadi","West Virginia University, Morgantown, West Virginia, USA",100.0,USA,0.0,,"State-of-the-art face recognition (FR) models often experience a significant performance drop when dealing with facial images in surveillance scenarios where images are in low quality and often corrupted with noise. Leveraging facial characteristics, such as freckles, scars, gender, and ethnicity, becomes highly beneficial in improving FR performance in such scenarios. In this paper, we introduce text-guided face recognition (TGFR) to analyze the impact of integrating facial attributes in the form of natural language descriptions. We hypothesize that adding semantic information into the loop can significantly improve the image understanding capability of an FR algorithm compared to other soft biometrics. However, learning a discriminative joint embedding within the multimodal space poses a considerable challenge due to the semantic gap in the unaligned image-text representations, along with the complexities arising from ambiguous and incoherent textual descriptions of the face. To address these challenges, we introduce a face-caption alignment module (FCAM), which incorporates cross-modal contrastive losses across multiple granularities to maximize the mutual information between local and global features of the face-caption pair. Within FCAM, we refine both facial and textual features for learning aligned and discriminative features. We also design a face-caption fusion module (FCFM) that applies fine-grained interactions and coarse-grained associations among cross-modal features. Through extensive experiments conducted on three face-caption datasets, proposed TGFR demonstrates remarkable improvements, particularly on low-quality images, over existing FR models and outperforms other related methods and benchmarks.",https://openaccess.thecvf.com/content/WACV2024/html/Hasan_Text-Guided_Face_Recognition_Using_Multi-Granularity_Cross-Modal_Contrastive_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Hasan_Text-Guided_Face_Recognition_Using_Multi-Granularity_Cross-Modal_Contrastive_Learning_WACV_2024_paper.pdf,,,2312.09367,main,Poster,https://ieeexplore.ieee.org/document/10484443/,"['Visualization', 'Face recognition', 'Surveillance', 'Semantics', 'Noise', 'Natural languages', 'Self-supervised learning']","['Face Recognition', 'Self-supervised Learning', 'Local Features', 'Global Features', 'Mutual Information', 'Discriminative Features', 'Facial Features', 'Face Images', 'Textual Descriptions', 'Textual Features', 'Contrastive Loss', 'Low-quality Images', 'Facial Attributes', 'Alignment Module', 'Natural Language Descriptions', 'Face Recognition Model', 'Image Quality', 'Image Features', 'Image Classification', 'Representation Learning', 'Text Modality', 'Text Encoder', 'True Pairs', 'Fusion Strategy', 'Image Encoder', 'Visual Modality', 'Word Embedding', 'Local Image Features', 'Loss Of Identity', 'Foundation Model']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"State-of-the-art face recognition (FR) models often experience a significant performance drop when dealing with facial images in surveillance scenarios where images are in low quality and often corrupted with noise. Leveraging facial characteristics, such as freckles, scars, gender, and ethnicity, becomes highly beneficial in improving FR performance in such scenarios. In this paper, we introduce text-guided face recognition (TGFR) to analyze the impact of integrating facial attributes in the form of natural language descriptions. We hypothesize that adding semantic information into the loop can significantly improve the image understanding capability of an FR algorithm compared to other soft biometrics. However, learning a discriminative joint embedding within the multimodal space poses a considerable challenge due to the semantic gap in the unaligned image-text representations, along with the complexities arising from ambiguous and incoherent textual descriptions of the face. To address these challenges, we introduce a face-caption alignment module (FCAM), which incorporates cross-modal contrastive losses across multiple granularities to maximize the mutual information between local and global features of the face-caption pair. Within FCAM, we refine both facial and textual features for learning aligned and discriminative features. We also design a face-caption fusion module (FCFM) that applies fine-grained interactions and coarse-grained associations among cross-modal features. Through extensive experiments conducted on three face-caption datasets, proposed TGFR demonstrates remarkable improvements, particularly on low-quality images, over existing FR models and outperforms other related methods and benchmarks."
Text-to-Image Editing by Image Information Removal,"Zhongping Zhang, Jian Zheng, Zhiyuan Fang, Bryan A. Plummer",Boston University; Amazon Alexa AI,50.0,USA,50.0,USA,"Diffusion models have demonstrated impressive performance in text-guided image generation. Current methods that leverage the knowledge of these models for image editing either fine-tune them using the input image (e.g., Imagic) or incorporate structure information as additional constraints (e.g., ControlNet). However, fine-tuning large-scale diffusion models on a single image can lead to severe overfitting issues and lengthy inference time. Information leakage from pretrained models also make it challenging to preserve image content not related to the text input. Additionally, methods that incorporate structural guidance (e.g., edge maps, semantic maps, keypoints) find retaining attributes like colors and textures difficult. Using the input image as a control could mitigate these issues, but since these models are trained via reconstruction, a model can simply hide information about the original image when encoding it to perfectly reconstruct the image without learning the editing task. To address these challenges, we propose a text-to-image editing model with an Image Information Removal module (IIR) that selectively erases color-related and texture-related information from the original image, allowing us to better preserve the text-irrelevant content and avoid issues arising from information hiding. Our experiments on CUB, Outdoor Scenes, and COCO reports our approach achieves the best editability-fidelity trade-off results. In addition, a user study on COCO shows that our edited images are preferred 35% more often than prior work.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_Text-to-Image_Editing_by_Image_Information_Removal_WACV_2024_paper.pdf,,,2305.17489,main,Poster,,,,,,
Text-to-Image Models for Counterfactual Explanations: A Black-Box Approach,"Guillaume Jeanneret, Loïc Simon, Frédéric Jurie","Normandy University, ENSICAEN, UNICAEN, CNRS, GREYC, Caen, France",100.0,France,0.0,,"This paper addresses the challenge of generating Counterfactual Explanations (CEs), involving the identification and modification of the fewest necessary features to alter a classifier's prediction for a given image. Our proposed method, Text-to-Image Models for Counterfactual Explanations (TIME), is a black-box counterfactual technique based on distillation. Unlike previous methods, this approach requires solely the image and its prediction, omitting the need for the classifier's structure, parameters, or gradients. Before generating the counterfactuals, TIME introduces two distinct biases into Stable Diffusion in the form of textual embeddings: the context bias, associated with the image's structure, and the class bias, linked to class-specific features learned by the target classifier. After learning these biases, we find the optimal latent code applying the classifier's predicted class token and regenerate the image using the target embedding as conditioning, producing the counterfactual explanation. Extensive empirical studies validate that TIME can generate explanations of comparable effectiveness even when operating within a black-box setting.",https://openaccess.thecvf.com/content/WACV2024/html/Jeanneret_Text-to-Image_Models_for_Counterfactual_Explanations_A_Black-Box_Approach_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jeanneret_Text-to-Image_Models_for_Counterfactual_Explanations_A_Black-Box_Approach_WACV_2024_paper.pdf,,,2309.07944,main,Poster,,,,,,
Textron: Weakly Supervised Multilingual Text Detection Through Data Programming,"Dhruv Kudale, Badri Vishal Kasuba, Venkatapathy Subramanian, Parag Chaudhuri, Ganesh Ramakrishnan","Department of Computer Science and Engineering, IIT Bombay, India",100.0,India,0.0,,"Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.com/IITB-LEAP-OCR/TEXTRON",https://openaccess.thecvf.com/content/WACV2024/html/Kudale_Textron_Weakly_Supervised_Multilingual_Text_Detection_Through_Data_Programming_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kudale_Textron_Weakly_Supervised_Multilingual_Text_Detection_Through_Data_Programming_WACV_2024_paper.pdf,,https://github.com/IITB-LEAP-OCR/TEXTRON1,,main,Poster,https://ieeexplore.ieee.org/document/10484139/,"['Training', 'Computer vision', 'Computational modeling', 'Text detection', 'Training data', 'Manuals', 'Programming']","['Bilingual', 'Optical Character Recognition', 'Prediction Model', 'Deep Learning', 'Computer Vision', 'Deep Learning Models', 'Quality Of Training Data', 'Indian Languages', 'Convolutional Neural Network', 'Hyperparameters', 'Validation Set', 'Input Image', 'Unsupervised Learning', 'Bounding Box', 'Binary Image', 'Pixel Level', 'Training Objective', 'Binary Map', 'English Dataset', 'Fraction Of Pixels', 'Word Level', 'Label Generation', 'Weak Supervision', 'Fully Convolutional Network', 'Unlabeled Images', 'Edge Detection', 'Sobel Operator']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",,"Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for low-resource or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose Textron, a Data Programming-based approach, where users can plug various text detection methods into a weak supervision-based learning frame-work. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. Textron can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that Textron can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at https://github.Com/IITB-LEAP-OCR/TEXTRON"
Textual Alchemy: CoFormer for Scene Text Understanding,"Gayatri Deshmukh, Onkar Susladkar, Dhruv Makwana, Sparsh Mittal, Sai Chandra Teja R.",IIT Roorkee; Green PMU Semi Pvt Ltd; Independent Researcher,33.33333333333333,India,66.66666666666667,India,"The paper presents CoFormer (Convolutional Fourier Transformer), a robust and adaptable transformer architecture designed for a range of scene text tasks. CoFormer integrates convolution and Fourier operations into the transformer architecture. Thus, it leverages convolution properties such as shared weights, local receptive fields, and spatial subsampling, while the Fourier operation emphasizes composite characteristics from the frequency domain. The research further proposes the first pretraining datasets, named Textverse10M-E and Textverse10M-H. Using these datasets, we demonstrate the efficacy of pretraining for scene text understanding. CoFormer achieves state-of-theart results with and without pretraining on two downstream tasks: scene text recognition and scene text style transfer. The paper presents LISTNet (Language Invariant Style Transfer), a novel framework for bi-lingual scene text style transfer. It also introduces three datasets, viz., TST500K for scene text style transfer, CSTR2.5M and Akshara550 for scene text recognition.",https://openaccess.thecvf.com/content/WACV2024/html/Deshmukh_Textual_Alchemy_CoFormer_for_Scene_Text_Understanding_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Deshmukh_Textual_Alchemy_CoFormer_for_Scene_Text_Understanding_WACV_2024_paper.pdf,,https://github.com/CandleLabAI/CoFormer-WACV-2024,,main,Poster,https://ieeexplore.ieee.org/document/10484153/,"['Computer vision', 'Convolution', 'Text recognition', 'Frequency-domain analysis', 'Superresolution', 'Computer architecture', 'Transformers']","['Scene Text', 'Fourier Transform', 'Frequency Domain', 'Convolution Operation', 'Optical Character Recognition', 'Style Transfer', 'Transformer Architecture', 'Local Receptive Field', 'Neural Network', 'Input Image', 'Feature Maps', 'Image Patches', 'Content Features', 'Image Texture', 'Inverse Fourier Transform', 'Improve Model Performance', 'Bidirectional Long Short-term Memory', 'Masked Images', 'Contrastive Loss', 'L1 Loss', 'Target Text', 'Atrous Spatial Pyramid Pooling', 'Style Image', 'Inductive Bias', 'Hidden Representation', 'Perceptual Loss', 'Style Features', 'Critical Modulator', 'Text Structure', 'Linear Layer']","['Applications', 'Visualization', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"The paper presents CoFormer (Convolutional Fourier Transformer), a robust and adaptable transformer architecture designed for a range of scene text tasks. CoFormer integrates convolution and Fourier operations into the transformer architecture. Thus, it leverages convolution properties such as shared weights, local receptive fields, and spatial subsampling, while the Fourier operation emphasizes composite characteristics from the frequency domain. The research further proposes two new pretraining datasets, named Textverse10M-E and Textverse10M-H. Using these datasets, we demonstrate the efficacy of pretraining for scene text understanding. CoFormer achieves state-of-the-art results with and without pretraining on two downstream tasks: scene text recognition (STR) and scene text editing (STE). The paper further proposes LISTNet (Language Invariant Style Transfer), a novel framework for bi-lingual STE. It also introduces three datasets, viz., TST500K for STE, CSTR2.5M and Akshara550 for STR. The source-code of CoFormer is available at https://github.com/CandleLabAI/CoFormer-WACV-2024."
The Background Also Matters: Background-Aware Motion-Guided Objects Discovery,"Sandra Kara, Hejer Ammar, Florian Chabot, Quoc-Cuong Pham","Universit´e Paris-Saclay, CEA, List, F-91120, Palaiseau, France",100.0,France,0.0,,"Recent works have shown that objects discovery can largely benefit from the inherent motion information in video data. However, these methods lack a proper background processing, resulting in an over-segmentation of the non-object regions into random segments. This is a critical limitation given the unsupervised setting, where object segments and noise are not distinguishable. To address this limitation we propose BMOD, a Background-aware Motion-guided Objects Discovery method. Concretely, we leverage masks of moving objects extracted from optical flow and design a learning mechanism to extend them to the true foreground composed of both moving and static objects. The background, a complementary concept of the learned foreground class, is then isolated in the object discovery process. This enables a joint learning of the objects discovery task and the object/non-object separation. The conducted experiments on synthetic and real-world datasets show that integrating our background handling with various cutting-edge methods brings each time a considerable improvement. Specifically, we improve the objects discovery performance with a large margin, while establishing a strong baseline for object/non-object separation.",https://openaccess.thecvf.com/content/WACV2024/html/Kara_The_Background_Also_Matters_Background-Aware_Motion-Guided_Objects_Discovery_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kara_The_Background_Also_Matters_Background-Aware_Motion-Guided_Objects_Discovery_WACV_2024_paper.pdf,,,2311.02633,main,Poster,https://ieeexplore.ieee.org/document/10483584/,"['Measurement', 'Location awareness', 'Learning systems', 'Motion segmentation', 'Noise', 'Semantics', 'Noise reduction']","['Considerable Improvement', 'Video Data', 'Optical Flow', 'Motion Information', 'Static Objects', 'Semantic', 'Unsupervised Methods', 'Color Space', 'Background Regions', 'Background Model', 'Attention Map', 'Clusters In Space', 'Object Regions', 'Foreground Objects', 'KITTI Dataset', 'Background Class', 'Video Object', 'Motion Cues', 'Vision Transformer', 'Human Supervision', 'Bipartite Matching', 'Foreground Regions', 'Meaningful Regions', 'Candidate Objects', 'Space Reconstruction', 'Object Location', 'Unsupervised Way', 'Adjusted Rand Index', 'Complex Scenarios', 'Latent Space']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Video recognition and understanding']",1,"Recent works have shown that objects discovery can largely benefit from the inherent motion information in video data. However, these methods lack a proper background processing, resulting in an over-segmentation of the non-object regions into random segments. This is a critical limitation given the unsupervised setting, where object segments and noise are not distinguishable. To address this limitation we propose BMOD, a Background-aware Motion-guided Objects Discovery method. Concretely, we leverage masks of moving objects extracted from optical flow and design a learning mechanism to extend them to the true foreground composed of both moving and static objects. The background, a complementary concept of the learned foreground class, is then isolated in the object discovery process. This enables a joint learning of the objects discovery task and the object/non-object separation. The conducted experiments on synthetic and real-world datasets show that integrating our background handling with various cutting-edge methods brings each time a considerable improvement. Specifically, we improve the objects discovery performance with a large margin, while establishing a strong baseline for object/non-object separation."
The Growing Strawberries Dataset: Tracking Multiple Objects With Biological Development Over an Extended Period,"Junhan Wen, Camiel R. Verschoor, Chengming Feng, Irina-Mona Epure, Thomas Abeel, Mathijs de Weerdt",Delft University of Technology; Birds.ai B.V.,50.0,Netherlands,50.0,Netherlands,"Multiple Object Tracking (MOT) is a rapidly developing research field that targets precise and reliable tracking of objects. Unfortunately, most available MOT datasets typically contain short video clips only, disregarding the indispensable requirement for adequately capturing substantial long-term variations in real-world scenarios. Long-term MOT poses unique challenges due to changes in both the objects and the environment, which remain relatively unexplored. To fill the gap, we propose a time-lapse image dataset inspired by the growth monitoring of strawberries, dubbed ""The Growing Strawberries"" Dataset (GSD). The data was captured hourly by six cameras, covering a span of 16 months in 2021 and 2022. During this time, it encompassed a total of 24 plants in two separate greenhouses. The changes in appearance, weight, and position during the ripening process, along with variations in the illumination during data collection, distinguish the task from previous MOT research. These practical issues resulted in a drastic performance downgrade in the track identification and association tasks of state-of-the-art MOT algorithms. We believe ""The Growing Strawberries"" will provide a platform for evaluating such long-term MOT tasks and inspire future research. The dataset is available at https://doi.org/10.4121/e3b31ece-cc88-4638-be10-8ccdd4c5f2f7.v1.",https://openaccess.thecvf.com/content/WACV2024/html/Wen_The_Growing_Strawberries_Dataset_Tracking_Multiple_Objects_With_Biological_Development_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wen_The_Growing_Strawberries_Dataset_Tracking_Multiple_Objects_With_Biological_Development_WACV_2024_paper.pdf,https://doi.org/10.4121/e3b31ece-cc88-4638-be10-8ccdd4c5f2f7.v1,,,main,Poster,https://ieeexplore.ieee.org/document/10483635/,"['Measurement', 'Visualization', 'Target tracking', 'Greenhouses', 'Switches', 'Biology', 'Trajectory']","['Track Multiple Objects', 'Greenhouse', 'Ripening', 'Image Dataset', 'Changes In Appearance', 'Short Video', 'Object Tracking', 'Growth Monitoring', 'Illumination Variations', 'Long-term Task', 'Multiple Object Tracking', 'Human Activities', 'Visible Light', 'Botanical', 'Object Detection', 'Kalman Filter', 'Color Space', 'Object Motion', 'Appearance Features', 'Downstream Applications', 'Temporal Datasets', 'Objective Variables', 'Irregular Movements', 'HSV Color', 'CIELAB Color Space', 'Dark Images', 'Heat Pipe', 'Benchmark Experiments', 'RGB Camera', 'Object Appearance']","['Applications', 'Agriculture', 'Algorithms', 'Datasets and evaluations']",,"Multiple Object Tracking (MOT) is a rapidly developing research field that targets precise and reliable tracking of objects. Unfortunately, most available MOT datasets typically contain short video clips only, disregarding the indispensable requirement for adequately capturing substantial long-term variations in real-world scenarios. Long-term MOT poses unique challenges due to changes in both the objects and the environment, which remain relatively unexplored. To fill the gap, we propose a time-lapse image dataset inspired by the growth monitoring of strawberries, dubbed The Growing Strawberries Dataset (GSD). The data was captured hourly by six cameras, covering a span of 16 months in 2021 and 2022. During this time, it encompassed a total of 24 plants in two separate greenhouses. The changes in appearance, weight, and position during the ripening process, along with variations in the illumination during data collection, distinguish the task from previous MOT research. These practical issues resulted in a drastic performance downgrade in the track identification and association tasks of state-of-the-art MOT algorithms. We believe The Growing Strawberries will provide a platform for evaluating such long-term MOT tasks and inspire future research. The dataset is available at https://doi.org/10.4121/e3b31ece-cc88-4638-be10-8ccdd4c5f2f7.v1."
The Paleographer's Eye ex machina: Using Computer Vision To Assist Humanists in Scribal Hand Identification,"Samuel Grieggs, C. E. M. Henderson, Sebastian Sobecki, Alexandra Gillespie, Walter Scheirer",Indiana University of Pennsylvania; University of Notre Dame; University of Toronto,100.0,"Canada, USA",0.0,,"The steady digitization of medieval manuscripts is rapidly changing the field of paleography, challenging existing assumptions about handwriting and book production. This development has identified historically important centers for the production of scribal texts, and even individual scribes themselves. For example, scholars of late medieval English literature have identified the copyists of a number of literary manuscripts, and the important role of London government clerks in shaping literary culture. However, traditional paleography has no agreed-upon methodology or fixed criteria for the attribution of handwriting to a particular community, period, or scribe. The approach taken by paleographers is inherently qualitative and subject to personal bias. Even those wielding the mighty ""paleographer's eye"" cannot claim objectivity. Computer vision offers solutions with spectacular performance on writer identification and retrieval benchmarks, but these have not been widely adopted by the paleography community because they tend not to hold up in practice. In this work, we attempt to bridge the divide with a software package designed not to automate paleography, but to augment the paleographer's eye. We introduce automated handwriting identification tools for which the results can be quickly visually understood and assessed, and used as one feature among many by expert paleographers when attributing previously unknown scribal hands. We also demonstrate a use case for our software by analyzing several items believed to be written by Thomas Hoccleve, a highly productive clerk of the Privy Seal who also happens to be an important fifteenth-century English poet.",https://openaccess.thecvf.com/content/WACV2024/html/Grieggs_The_Paleographers_Eye_ex_machina_Using_Computer_Vision_To_Assist_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Grieggs_The_Paleographers_Eye_ex_machina_Using_Computer_Vision_To_Assist_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Think Before You Simulate: Symbolic Reasoning To Orchestrate Neural Computation for Counterfactual Question Answering,"Adam Ishay, Zhun Yang, Joohyung Lee, Ilgu Kang, Dongjae Lim","Samsung Research, S. Korea; Arizona State University, AZ, USA",50.0,USA,50.0,South Korea,"Causal and temporal reasoning about video dynamics is a challenging problem. While neuro-symbolic models that combine symbolic reasoning with neural-based perception and prediction have shown promise, they exhibit limitations, especially in answering counterfactual questions. This paper introduces a method to enhance a neuro-symbolic model for counterfactual reasoning, leveraging symbolic reasoning about causal relations among events. We define the notion of a causal graph to represent such relations and use Answer Set Programming (ASP), a declarative logic programming method, to find how to coordinate perception and simulation modules. We validate the effectiveness of our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves state-of-the-art performance on the CLEVRER challenge, significantly outperforming existing models. In the case of the CRAFT benchmark, we leverage a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a dynamics simulator. Our findings show that this method can further improve its performance on counterfactual questions by providing alternative prompts instructed by symbolic causal reasoning.",https://openaccess.thecvf.com/content/WACV2024/html/Ishay_Think_Before_You_Simulate_Symbolic_Reasoning_To_Orchestrate_Neural_Computation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ishay_Think_Before_You_Simulate_Symbolic_Reasoning_To_Orchestrate_Neural_Computation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484055/,"['Computer vision', 'Logic programming', 'Computational modeling', 'Computer architecture', 'Benchmark testing', 'Predictive models', 'Cognition']","['Symbolic Reasoning', 'Counterfactual Question', 'Causal Relationship', 'Language Model', 'Simulation Module', 'Causal Reasoning', 'Program Logic', 'Causal Graph', 'Simulation Model', 'Black Box', 'Types Of Questions', 'Objective Conditions', 'Results Question', 'Output Error', 'Original Video', 'Final Answer', 'Object In Frame', 'Object Trajectory', 'Horizontal Edges', 'Scene Description']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Vision + language and/or other modalities']",,"Causal and temporal reasoning about video dynamics is a challenging problem. While neuro-symbolic models that combine symbolic reasoning with neural-based perception and prediction have shown promise, they exhibit limitations, especially in answering counterfactual questions. This paper introduces a method to enhance a neuro-symbolic model for counterfactual reasoning, leveraging symbolic reasoning about causal relations among events. We define the notion of a causal graph to represent such relations and use Answer Set Programming (ASP), a declarative logic programming method, to find how to coordinate perception and simulation modules. We validate the effectiveness of our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves state-of-the-art performance on the CLEVRER challenge, significantly outperforming existing models. In the case of the CRAFT benchmark, we leverage a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a dynamics simulator. Our findings show that this method can further improve its performance on counterfactual questions by providing alternative prompts instructed by symbolic causal reasoning."
Time To Shine: Fine-Tuning Object Detection Models With Synthetic Adverse Weather Images,"Thomas Rothmeier, Werner Huber, Alois C. Knoll",Technical University of Munich; University of Applied Sciences Ingolstadt,100.0,Germany,0.0,,"The detection of vehicles, pedestrians, and obstacles plays an important role in the decision-making process of autonomous vehicles. While existing methods achieve high detection accuracy under good environmental conditions, they often fail in adverse weather conditions due to limited visibility, blurred contours, and low contrast. These ""edge-case"" scenarios are not well represented in existing datasets and are not handled properly by object detection algorithms. In our work, we propose a novel approach to synthesising photorealistic and highly diverse scenarios that can be used to fine-tune object detection algorithms in adverse weather conditions such as snow, fog, and rain. The approach uses the Midjourney text-to-image model to create accurate synthetic images of desired weather conditions. Our experiments show that training with our dataset significantly improves detection accuracy in harsh weather conditions. Our results are compared to baseline models and models fine-tuned on augmented clear weather images.",https://openaccess.thecvf.com/content/WACV2024/html/Rothmeier_Time_To_Shine_Fine-Tuning_Object_Detection_Models_With_Synthetic_Adverse_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rothmeier_Time_To_Shine_Fine-Tuning_Object_Detection_Models_With_Synthetic_Adverse_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484404/,"['Training', 'Computer vision', 'Rain', 'Pedestrians', 'Computational modeling', 'Snow', 'Image edge detection']","['Extreme Weather', 'Object Detection', 'Synthetic Images', 'Object Detection Model', 'Weather Images', 'Adverse Conditions', 'Pedestrian', 'Autonomous Vehicles', 'Clear Image', 'Image Augmentation', 'Adverse Weather Conditions', 'Vehicle Detection', 'Harsh Weather Conditions', 'Clear Weather', 'Training Data', 'Training Dataset', 'Benchmark Datasets', 'Bounding Box', 'Generative Adversarial Networks', 'Rain Conditions', 'Rainy Conditions', 'Respective Domains', 'Rain And Snow', 'COCO Dataset', 'Small Objects', 'Snowflake', 'Weather Data', 'Faster R-CNN', 'Large Objects']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",2,"The detection of vehicles, pedestrians, and obstacles plays an important role in the decision-making process of autonomous vehicles. While existing methods achieve high detection accuracy under good environmental conditions, they often fail in adverse weather conditions due to limited visibility, blurred contours, and low contrast. These ""edge-case"" scenarios are not well represented in existing datasets and are not handled properly by object detection algorithms. In our work, we propose a novel approach to synthesising photorealistic and highly diverse scenarios that can be used to fine-tune object detection algorithms in adverse weather conditions such as snow, fog, and rain. The approach uses the Midjourney text-to-image model to create accurate synthetic images of desired weather conditions. Our experiments show that training with our dataset significantly improves detection accuracy in harsh weather conditions. Our results are compared to baseline models and models fine-tuned on augmented clear weather images."
Token Fusion: Bridging the Gap Between Token Pruning and Token Merging,"Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, Hongxia Jin",Michigan State University Samsung Research America; Samsung Research America,50.0,USA,50.0,USA,"Vision Transformers (ViTs) have emerged as powerful backbones in computer vision, outperforming many traditional CNNs. However, their computational overhead, largely attributed to the self-attention mechanism, makes deployment on resource-constrained edge devices challenging. Multiple solutions rely on token pruning or token merging. In this paper, we introduce ""Token Fusion"" (ToFu), a method that amalgamates the benefits of both token pruning and token merging. Token pruning proves advantageous when the model exhibits sensitivity to input interpolations, while token merging is effective when the model manifests close to linear responses to inputs. We combine this to propose a new scheme called Token Fusion. Moreover, we tackle the limitations of average merging, which doesn't preserve the intrinsic feature norm, resulting in distributional shifts. To mitigate this, we introduce MLERP merging, a variant of the SLERP technique, tailored to merge multiple tokens while maintaining the norm distribution. ToFu is versatile, applicable to ViTs with or without additional training. Our empirical evaluations indicate that ToFu establishes new benchmarks in both classification and image generation tasks concerning computational efficiency and model accuracy.",https://openaccess.thecvf.com/content/WACV2024/html/Kim_Token_Fusion_Bridging_the_Gap_Between_Token_Pruning_and_Token_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kim_Token_Fusion_Bridging_the_Gap_Between_Token_Pruning_and_Token_WACV_2024_paper.pdf,,,2312.01026,main,Poster,https://ieeexplore.ieee.org/document/10484445/,"['Training', 'Computer vision', 'Sensitivity', 'Image synthesis', 'Computational modeling', 'Merging', 'Linearity']","['Convolutional Neural Network', 'Computer Vision', 'Classification Task', 'Domain Shift', 'Image Generation', 'Edge Devices', 'Traditional Convolutional Neural Network', 'Vision Transformer', 'Attention Mechanism', 'Attention Module', 'Linear Behavior', 'Transformation Efficiency', 'Maximum Norm', 'Subsequent Operations', 'Top-1 Accuracy', 'Early Layers', 'Low-rank Approximation', 'Bipartite Matching', 'Transformer Block']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",4,"Vision Transformers (ViTs) have emerged as powerful backbones in computer vision, outperforming many traditional CNNs. However, their computational overhead, largely attributed to the self-attention mechanism, makes deployment on resource-constrained edge devices challenging. Multiple solutions rely on token pruning or token merging. In this paper, we introduce ""Token Fusion"" (ToFu), a method that amalgamates the benefits of both token pruning and token merging. Token pruning proves advantageous when the model exhibits sensitivity to input interpolations, while token merging is effective when the model manifests close to linear responses to inputs. We combine this to propose a new scheme called Token Fusion. Moreover, we tackle the limitations of average merging, which doesn’t preserve the intrinsic feature norm, resulting in distributional shifts. To mitigate this, we introduce MLERP merging, a variant of the SLERP technique, tailored to merge multiple tokens while maintaining the norm distribution. ToFu is versatile, applicable to ViTs with or without additional training. Our empirical evaluations indicate that ToFu establishes new benchmarks in both classification and image generation tasks concerning computational efficiency and model accuracy."
Top-Down Beats Bottom-Up in 3D Instance Segmentation,"Maksim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, Danila Rukhovich",Samsung Research,0.0,,100.0,South Korea,"Most 3D instance segmentation methods exploit a bottom-up strategy, typically including resource-exhaustive post-processing. For point grouping, bottom-up methods rely on prior assumptions about the objects in the form of hyperparameters, which are domain-specific and need to be carefully tuned. On the contrary, we address 3D instance segmentation with a TD3D: the pioneering cluster-free, fully-convolutional and entirely data-driven approach trained in an end-to-end manner. This is the first top-down method outperforming bottom-up approaches in 3D domain. With its straightforward pipeline, it performs outstandingly well on the standard benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS. Besides, our method is much faster on inference than the current state-of-the-art grouping-based approaches: our flagship modification is 1.9x faster than the most accurate bottom-up method, while being more accurate, and our faster modification shows state-of-the-art accuracy running at 2.6x speed. Code is available at https://github.com/SamsungLabs/td3d.",https://openaccess.thecvf.com/content/WACV2024/html/Kolodiazhnyi_Top-Down_Beats_Bottom-Up_in_3D_Instance_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Kolodiazhnyi_Top-Down_Beats_Bottom-Up_in_3D_Instance_Segmentation_WACV_2024_paper.pdf,,https://github.com/SamsungLabs/td3d,2302.02871,main,Poster,https://ieeexplore.ieee.org/document/10484444/,"['Instance segmentation', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Pipelines', 'Benchmark testing', 'Standards']","['3D Instance Segmentation', 'Hyperparameters', 'Bottom-up Approach', 'Prior Assumptions', 'Standard Benchmark', '3D Segmentation', 'Bottom-up Methods', 'Top-down Methods', '3D Domain', 'Point Cloud', 'Bounding Box', '3D Point', '3D Features', 'Test Split', 'Semantic Labels', 'Binary Segmentation', '3D Convolution', 'Initial Proposal', 'Number Of Proposals', 'Frequent Category', '3D Bounding Box', 'Proposal Generation', '3D Object Detection', 'Ground-truth Bounding Box', 'Object Proposals', 'IoU Threshold', 'Transformer-based Methods', '3D U-Net', 'Set Of Proposals']","['Algorithms', '3D computer vision']",9,"Most 3D instance segmentation methods exploit a bottom-up strategy, typically including resource-exhaustive post-processing. For point grouping, bottom-up methods rely on prior assumptions about the objects in the form of hyperparameters, which are domain-specific and need to be carefully tuned. On the contrary, we address 3D instance segmentation with a TD3D: the pioneering cluster-free, fully-convolutional and entirely data-driven approach trained in an end-to-end manner. This is the first top-down method outperforming bottom-up approaches in 3D domain. With its straightforward pipeline, it demonstrates outstanding accuracy and generalization ability on the standard indoor benchmarks: ScanNet v2, its extension ScanNet200, and S3DIS, as well as on the aerial STPLS3D dataset. Besides, our method is much faster on inference than the current state-of-the-art grouping-based approaches: our flagship modification is 1.9x faster than the most accurate bottom-up method, while being more accurate, and our faster modification shows state-of-the-art accuracy running at 2.6x speed. Code is available at https://github.com/SamsungLabs/td3d."
Torque Based Structured Pruning for Deep Neural Network,"Arshita Gupta, Tien Bau, Joonsoo Kim, Zhe Zhu, Sumit Jha, Hrishikesh Garud",Samsung Research America; Google,0.0,,100.0,USA,"Structured pruning is a popular way of convolutional neural network (CNN) acceleration. However, current state of the art pruning techniques require modifications to the network architecture, implementation of complex gradient update rules or repetitive training and long fine-tuning stages. Our novel physics-inspired approach for structured pruning aims to solve these issues. Analogous to 'Torque' we apply a force that consolidates the weights of a convolutional layer around a selected pivot point during training. Using the distance-dependency nature of torque, we can encourage high density of weights in filters around this point and increase filter sparsity as we move away. Filters away from the pivot point can be pruned, resulting in a minimum loss of information. We can control the tightness of the weights by varying the hyper-parameters, thus assisting us in creating a more compact network. Our proposed technique is jointly able to perform both filter learning and filter importance sorting. Additionally, our method is easy to implement, requires no change to model architecture and needs very little to no fine-tuning. We show that our approach reaches competitive results with previous state-of-the-art by evaluating popular networks such as VGGNet and ResNet on multiple image classification tasks. Notably, our method can reduce the parameter count of VGGNet by 96% and still maintain the accuracy achieved by the full-size model without any fine-tuning. This makes our method both latency and memory efficient for hardware deployment.",https://openaccess.thecvf.com/content/WACV2024/html/Gupta_Torque_Based_Structured_Pruning_for_Deep_Neural_Network_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gupta_Torque_Based_Structured_Pruning_for_Deep_Neural_Network_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484493/,"['Training', 'Filters', 'Torque', 'Memory management', 'Network architecture', 'Hardware', 'Convolutional neural networks']","['Neural Network', 'Deep Neural Network', 'Convolutional Neural Network', 'Hyperparameters', 'Popular Way', 'Pivot Point', 'Minimal Loss Of Information', 'Pruning Techniques', 'Fixed Point', 'Part Of Network', 'Second Derivative', 'Recent Approaches', 'ImageNet', 'Active Area Of Research', 'Position Vector', 'Reduction In Parameters', 'Number Of Filters', 'Convolutional Neural Network Layers', 'Absolute Magnitude', 'Zero Vector', 'Accuracy Drop', 'Filters In Layer', 'Filter Weights', 'Pre-trained Weights', 'Pruning Method', 'Fine-tuning Step', 'CIFAR-100 Dataset', 'Magnitude Of Torque', 'Random Initialization', 'Training Environment']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Smartphones / end user devices']",,"Structured pruning is a popular way of convolutional neural network (CNN) acceleration. However, current state of the art pruning techniques require modifications to the network architecture, implementation of complex gradient update rules or repetitive training and long fine-tuning stages. Our novel physics-inspired approach for structured pruning aims to solve these issues. Analogous to ‘Torque’ we apply a force that consolidates the weights of a convolutional layer around a selected pivot point during training. Using the distance-dependency nature of torque, we can encourage high density of weights in filters around this point and increase filter sparsity as we move away. Filters away from the pivot point can be pruned, resulting in a minimum loss of information. We can control the tightness of the weights by varying the hyper-parameters, thus assisting us in creating a more compact network. Our proposed technique is jointly able to perform both filter learning and filter importance sorting. Additionally, our method is easy to implement, requires no change to model architecture and needs very little to no fine-tuning. We show that our approach reaches competitive results with previous state-of-the-art by evaluating popular networks such as VGGNet and ResNet on multiple image classification tasks. Notably, our method can reduce the parameter count of VGGNet by 96% and still maintain the accuracy achieved by the full-size model without any fine-tuning. This makes our method both latency and memory efficient for hardware deployment."
Toward Planet-Wide Traffic Camera Calibration,"Khiem Vuong, Robert Tamburo, Srinivasa G. Narasimhan",Carnegie Mellon University,100.0,USA,0.0,,"Despite the widespread deployment of outdoor cameras, their potential for automated analysis remains largely untapped due, in part, to calibration challenges. The absence of precise camera calibration data, including intrinsic and extrinsic parameters, hinders accurate real-world distance measurements from captured videos. To address this, we present a scalable framework that utilizes street-level imagery to reconstruct a metric 3D model, facilitating precise calibration of in-the-wild traffic cameras. Notably, our framework achieves 3D scene reconstruction and accurate localization of over 100 global traffic cameras and is scalable to any camera with sufficient street-level imagery. For evaluation, we introduce a dataset of 20 fully calibrated traffic cameras, demonstrating our method's significant enhancements over existing automatic calibration techniques. Furthermore, we highlight our approach's utility in traffic analysis by extracting insights via 3D vehicle reconstruction and speed measurement, thereby opening up the potential of using outdoor cameras for automated analysis.",https://openaccess.thecvf.com/content/WACV2024/html/Vuong_Toward_Planet-Wide_Traffic_Camera_Calibration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Vuong_Toward_Planet-Wide_Traffic_Camera_Calibration_WACV_2024_paper.pdf,,,2311.04243,main,Poster,https://ieeexplore.ieee.org/document/10484499/,"['Location awareness', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Cameras', 'Distance measurement', 'Calibration']","['Camera Calibration', 'Traffic Cameras', 'Scalable', 'Distancing Measures', 'Localization Accuracy', '3D Reconstruction', 'Speed Measurement', 'Video Capture', 'Intrinsic Parameters', '3D Scene', 'Extrinsic Parameters', 'Precise Calibration', 'Bounding Box', 'Focal Length', 'Ground Plane', 'Cyanoacrylate', 'Feature Matching', 'Image Database', 'Manual Measurements', 'Metric Scale', 'Camera Pose', 'Google Street View', 'Query Image', 'Relative Pose', 'Bundle Adjustment', 'Panoramic Images', 'Road Markings', 'Lane Markings', 'Scene Geometry']","['Applications', 'Social good', 'Algorithms', '3D computer vision', 'Applications', 'Autonomous Driving']",1,"Despite the widespread deployment of outdoor cameras, their potential for automated analysis remains largely untapped due, in part, to calibration challenges. The absence of precise camera calibration data, including intrinsic and extrinsic parameters, hinders accurate real-world distance measurements from captured videos. To address this, we present a scalable framework that utilizes street-level imagery to reconstruct a metric 3D model, facilitating precise calibration of in-the-wild traffic cameras. Notably, our framework achieves 3D scene reconstruction and accurate localization of over 100 global traffic cameras and is scalable to any camera with sufficient street-level imagery. For evaluation, we introduce a dataset of 20 fully calibrated traffic cameras, demonstrating our method’s significant enhancements over existing automatic calibration techniques. Furthermore, we highlight our approach’s utility in traffic analysis by extracting insights via 3D vehicle reconstruction and speed measurement, thereby opening up the potential of using outdoor cameras for automated analysis. Code and dataset will be available on the project website."
Towards Accurate Disease Segmentation in Plant Images: A Comprehensive Dataset Creation and Network Evaluation,"Komuravelli Prashanth, Jaladi Sri Harsha, Sivapuram Arun Kumar, Jaladi Srilekha","Indian Institute of Technology Tirupati, Tirupati, India; Professor Jayashankar Telangana State Agricultural University, Hyderabad, India",100.0,India,0.0,,"Automated disease segmentation in plant images plays a crucial role in identifying and mitigating the impact of plant diseases on agricultural productivity. In this study, we address the problem of Northern Leaf Blight (NLB) disease segmentation in maize plants. We present a comprehensive dataset of 1000 plant images annotated with NLB disease regions. We employ the Mask R-CNN and Cascaded Mask R-CNN models with various backbone architectures to perform NLB disease segmentation. The experimental results demonstrate the effectiveness of the models in accurately delineating NLB disease regions. Specifically, the ResNet Strikes Back-50 backbone architecture achieves the highest mean average precision (mAP) score, indicating its ability to capture intricate details of NLB disease spots. Additionally, the cascaded approach enhances segmentation accuracy compared to the single-stage Mask R-CNN models. Our findings provide valuable insights into the performance of different backbone architectures and contribute to the development of automated NLB disease segmentation methods in plant images. The generated dataset and experimental results serve as a resource for further research in plant disease segmentation and management.",https://openaccess.thecvf.com/content/WACV2024/html/Prashanth_Towards_Accurate_Disease_Segmentation_in_Plant_Images_A_Comprehensive_Dataset_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Prashanth_Towards_Accurate_Disease_Segmentation_in_Plant_Images_A_Comprehensive_Dataset_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483904/,"['Training', 'Productivity', 'Image segmentation', 'Plant diseases', 'Pathology', 'Plants (biology)', 'Refining']","['Image Segmentation', 'Comprehensive Dataset', 'Segmentation Accuracy', 'Dataset Creation', 'Plant Disease', 'Maize Plants', 'Mean Average Precision', 'Score Map', 'Mask R-CNN', 'Backbone Architecture', 'Model Performance', 'Convolutional Network', 'Convolutional Neural Network', 'Growth Stages', 'Computer Vision', 'Deep Learning Models', 'Image Dataset', 'Object Detection', 'Intersection Over Union', 'Precision And Recall', 'Role In Performance', 'Choice Architecture', 'Instance Segmentation', 'ResNet-50 Backbone', 'Disease Dataset', 'Semantic Segmentation', 'Bounding Box', 'Delineation Of Regions', 'Accurate Annotation', 'Differences In Disease Severity']",,,"Automated disease segmentation in plant images plays a crucial role in identifying and mitigating the impact of plant diseases on agricultural productivity. In this study, we address the problem of Northern Leaf Blight (NLB) disease segmentation in maize plants. We present a comprehensive dataset of 1000 plant images annotated with NLB disease regions. We employ the Mask R-CNN and Cascaded Mask R-CNN models with various backbone architectures to perform NLB disease segmentation. The experimental results demonstrate the effectiveness of the models in accurately delineating NLB disease regions. Specifically, the ResNet Strikes Back-50 backbone architecture achieves the highest mean average precision (mAP) score, indicating its ability to capture intricate details of NLB disease spots. Additionally, the cascaded approach enhances segmentation accuracy compared to the single-stage Mask R-CNN models. Our findings provide valuable insights into the performance of different backbone architectures and contribute to the development of automated NLB disease segmentation methods in plant images. The generated dataset and experimental results serve as a resource for further research in plant disease segmentation and management."
Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding,"Joshua Feinglass, Yezhou Yang","Active Perception Group, Arizona State University",100.0,USA,0.0,,"Object proposal generation serves as a standard pre-processing step in Vision-Language (VL) tasks (image captioning, visual question answering, etc.). The performance of object proposals generated for VL tasks is currently evaluated across all available annotations, a protocol that we show is misaligned - higher scores do not necessarily correspond to improved performance on downstream VL tasks. Our work serves as a study of this phenomenon and explores the effectiveness of semantic grounding to mitigate its effects. To this end, we propose evaluating object proposals against only a subset of available annotations, selected by thresholding an annotation importance score. Importance of object annotations to VL tasks is quantified by extracting relevant semantic information from text describing the image. We show that our method is consistent and demonstrates greatly improved alignment with annotations selected by image captioning metrics and human annotation when compared against existing techniques. Lastly, we compare current detectors used in the Scene Graph Generation (SGG) benchmark as a use case, which serves as an example of when traditional object proposal evaluation techniques are misaligned.",https://openaccess.thecvf.com/content/WACV2024/html/Feinglass_Towards_Addressing_the_Misalignment_of_Object_Proposal_Evaluation_for_Vision-Language_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Feinglass_Towards_Addressing_the_Misalignment_of_Object_Proposal_Evaluation_for_Vision-Language_WACV_2024_paper.pdf,,https://github.com/JoshuaFeinglass/VL-detector-eval,2309.00215,main,Poster,https://ieeexplore.ieee.org/document/10484224/,"['Measurement', 'Visualization', 'Protocols', 'Annotations', 'Grounding', 'Semantics', 'Question answering (information retrieval)']","['Object Proposals', 'Vision-language Tasks', 'Semantic Grounding', 'Importance Scores', 'Image Captioning', 'Visual Question Answering', 'Object Annotations', 'Scene Graph', 'Humoral Response', 'F1 Score', 'Object Detection', 'Image Object', 'Intersection Over Union', 'Bounding Box', 'Important Objective', 'Item Response Theory', 'Object Relations', 'Larger Scores', 'Heat Kernel', 'Ball Sports', 'Object Selection', 'Critical Object', 'COCO Dataset', 'Ground Truth Annotations', 'Annotated Regions', 'Object Regions', 'Process In Order', 'Rank Correlation']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",1,"Object proposal generation serves as a standard preprocessing step in Vision-Language (VL) tasks (image captioning, visual question answering, etc.). The performance of object proposals generated for VL tasks is currently evaluated across all available annotations, a protocol that we show is ""misaligned"" - higher scores do not necessarily correspond to improved performance on downstream VL tasks. Our work serves as a study of this phenomenon and explores the effectiveness of semantic grounding to mitigate its effects. To this end, we propose evaluating object proposals against only a subset of available annotations, selected by thresholding an annotation importance score. Importance of object annotations to VL tasks is quantified by extracting relevant semantic information from text describing the image. We show that our method is consistent and demonstrates greatly improved alignment with annotations selected by image captioning metrics and human annotation when compared against existing techniques. Lastly, we compare current detectors used in the Scene Graph Generation (SGG) benchmark as a use case, which serves as an example of when traditional object proposal evaluation techniques are misaligned
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Towards Better Structured Pruning Saliency by Reorganizing Convolution,"Xinglong Sun, Humphrey Shi","SHI Labs @ Georgia Tech & UIUC & U of Oregon, Picsart AI Research (PAIR); Stanford Univeristy, SHI Labs @ Georgia Tech & UIUC & U of Oregon",100.0,USA,0.0,,"We present SPSRC, a novel, simple and effective framework to extract enhanced Structured Pruning Saliency scores by Reorganizing Convolution. We observe that performance of pruning methods have gradually plateaued recently and propose to make better use of the learned convolutional kernel weights simply after a few steps of transformations. We firstly re-organize the convolutional operations between layers as matrix multiplications and then use the singular values and the matrix norms of the transformed matrices as saliency scores to decide what channels to prune or keep. We show both analytically and empirically that the long-standing kernel-norm-based channel importance measurement, like L1 magnitude, is not precise enough possessing a very obvious weakness of lacking spatial saliency but can be improved with SPSRC. We conduct extensive experiments across different settings and configurations and compare with the counterparts without our SPSRC along with other popular methods, observing obvious improvements. Our code is available at https://github.com/AlexSunNik/SPSRC/tree/main.",https://openaccess.thecvf.com/content/WACV2024/html/Sun_Towards_Better_Structured_Pruning_Saliency_by_Reorganizing_Convolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Towards_Better_Structured_Pruning_Saliency_by_Reorganizing_Convolution_WACV_2024_paper.pdf,,https://github.com/AlexSunNik/SPSRC/tree/main,,main,Poster,https://ieeexplore.ieee.org/document/10484080/,"['Convolutional codes', 'Computer vision', 'Tensors', 'Convolution', 'Matrix decomposition', 'Data mining', 'Kernel']","['Singular Value', 'Convolution Operation', 'Matrix Multiplication', 'Convolution Kernel', 'Pruning Method', 'Convolutional Neural Network', 'Theoretical Analysis', 'Convolutional Layers', 'Input Features', 'ImageNet', 'Singular Value Decomposition', 'Central Element', 'Frobenius Norm', 'Output Channels', 'Importance Scores', 'Compression Ratio', 'Output Feature Map', 'Nuclear Norm', 'Top-1 Accuracy', 'Spectral Norm', 'Frobenius Norm Of A Matrix', 'Ith Layer', 'Baseline Network', 'Network Pruning', 'Decrease In Accuracy', 'Stochastic Gradient Descent', 'Weight Matrix', 'Scaling Factor', 'Learning-based Methods']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"We present SPSRC, a novel, simple and effective framework to extract enhanced Structured Pruning Saliency scores by Reorganizing Convolution. We observe that performance of pruning methods have gradually plateaued recently and propose to make better use of the learned convolutional kernel weights simply after a few steps of transformations. We firstly re-organize the convolutional operations between layers as matrix multiplications and then use the singular values and the matrix norms of the transformed matrices as saliency scores to decide what channels to prune or keep. We show both analytically and empirically that the long-standing kernel-norm-based channel importance measurement, like L1 magnitude, is not precise enough possessing a very obvious weakness of lacking spatial saliency but can be improved with SPSRC. We conduct extensive experiments across different settings and configurations and compare with the counterparts without our SPSRC along with other popular methods, observing obvious improvements. Our code is available at: https://github.com/AlexSunNik/SPSRC/tree/main."
Towards Diverse and Consistent Typography Generation,"Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, Kota Yamaguchi","Kyushu University, Japan; CyberAgent, Japan",50.0,Japan,50.0,Japan,"In this work, we consider the typography generation task that aims at producing diverse typographic styling for the given graphic document. We formulate typography generation as a fine-grained attribute generation for multiple text elements and build an autoregressive model to generate diverse typography that matches the input design context. We further propose a simple yet effective sampling approach that respects the consistency and distinction principle of typography so that generated examples share consistent typographic styling across text elements. Our empirical study shows that our model successfully generates diverse typographic designs while preserving a consistent typographic structure.",https://openaccess.thecvf.com/content/WACV2024/html/Shimoda_Towards_Diverse_and_Consistent_Typography_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shimoda_Towards_Diverse_and_Consistent_Typography_Generation_WACV_2024_paper.pdf,,,2309.02099,main,Poster,https://ieeexplore.ieee.org/document/10484522/,"['Graphics', 'Computer vision', 'Impedance matching', 'Buildings', 'Color', 'Task analysis', 'Context modeling']","['Autoregressive Model', 'Textual Elements', 'Corruption', 'K-means', 'User Study', 'Attention Mechanism', 'Color Difference', 'Latent Space', 'Background Image', 'User Preferences', 'Vector Graphics', 'Font Size', 'Graphic Design', 'Diversity Score', 'Line Spacing', 'Style Transfer', 'Transformer Block']","['Applications', 'Arts / games / social media', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",1,"In this work, we consider the typography generation task that aims at producing diverse typographic styling for the given graphic document. We formulate typography generation as a fine-grained attribute generation for multiple text elements and build an autoregressive model to generate diverse typography that matches the input design context. We further propose a simple yet effective sampling approach that respects the consistency and distinction principle of typography so that generated examples share consistent typographic styling across text elements. Our empirical study shows that our model successfully generates diverse typographic designs while preserving a consistent typographic structure."
Towards Domain-Aware Knowledge Distillation for Continual Model Generalization,"Nikhil Reddy, Mahsa Baktashmotlagh, Chetan Arora","UQ, Australia; IIT Delhi, India; UQ-IIT Delhi Research Academy (UQIDAR)",100.0,"Australia, India",0.0,,"Generalization on unseen domains is critical for Deep Neural Networks (DNNs) to perform well in real-world applications such as autonomous navigation.  However, catastrophic forgetting limit the ability of domain generalization and unsupervised domain adaption approaches to adapt to constantly changing target domains. To overcome these challenges, We propose DoSe framework, a Domain-aware Self-Distillation method based on batch normalization prototypes to facilitate continual model generalization across varying target domains. Specifically, we enforce the consistency of batch normalization statistics between two batches of images sampled from the same target domain distribution between the student and teacher models. To alleviate catastrophic forgetting, we introduce a novel exemplar-based replay buffer to identify difficult samples for the model to retain the knowledge. Specifically, we demonstrate that identifying difficult samples and updating the model periodically using them can help in preserving knowledge learned from previously seen domains. We conduct extensive experiments on two real-world datasets ACDC, C-Driving, and one synthetic dataset SHIFT to verify the efficiency of the proposed DoSe framework. On ACDC, our method outperforms existing SOTA in Domain Generalization, Unsupervised Domain Adaptation, and Daytime settings by 26%, 14%, and 70% respectively.",https://openaccess.thecvf.com/content/WACV2024/html/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Reddy_Towards_Domain-Aware_Knowledge_Distillation_for_Continual_Model_Generalization_WACV_2024_paper.pdf,https://dose-iitd.github.io/,,,main,Poster,,,,,,
Towards More Realistic Membership Inference Attacks on Large Diffusion Models,"Jan Dubiński, Antoni Kowalczuk, Stanisław Pawlak, Przemyslaw Rokita, Tomasz Trzciński, Paweł Morawiecki","Warsaw University of Technology, IDEAS NCBR, Tooploox; Warsaw University of Technology; Polish Academy of Sciences",100.0,Poland,0.0,,"Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a new dataset to establish a fair evaluation setup and apply it to Stable Diffusion, also applicable to other generative models. With the proposed dataset, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future.",https://openaccess.thecvf.com/content/WACV2024/html/Dubinski_Towards_More_Realistic_Membership_Inference_Attacks_on_Large_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dubinski_Towards_More_Realistic_Membership_Inference_Attacks_on_Large_Diffusion_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483613/,"['Training', 'Privacy', 'Data privacy', 'Computer vision', 'Computational modeling', 'Closed box', 'Reliability']","['Diffusion Model', 'Inference Attacks', 'Membership Inference', 'Membership Inference Attacks', 'Training Set', 'Effects Of Attacks', 'Fair Evaluation', 'Evaluation Setup', 'Training Data', 'Test Data', 'Denoising', 'Probabilistic Model', 'Binary Classification', 'Lawsuits', 'Types Of Attacks', 'Latent Representation', 'Image X', 'Attack Methods', 'Pixel Error', 'Image Embedding', 'PCA Components', 'Set Of Attacks', 'White-box Attack']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",3,"Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community as a membership inference attack. Our focus is on Stable Diffusion, and we address the challenge of designing a fair evaluation framework to answer this membership question. We propose a new dataset to establish a fair evaluation setup and apply it to Stable Diffusion, also applicable to other generative models. With the proposed dataset, we execute membership attacks (both known and newly introduced). Our research reveals that previously proposed evaluation setups do not provide a full understanding of the effectiveness of membership inference attacks. We conclude that the membership inference attack remains a significant challenge for large diffusion models (often deployed as black-box systems), indicating that related privacy and copyright issues will persist in the foreseeable future."
Towards Realistic Generative 3D Face Models,"Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando De la Torre",Carnegie Mellon University; Meta Reality Labs,50.0,USA,50.0,USA,"In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shapes by leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms most state-of-the-art (SOTA) methods in the well-known NoW and REALY benchmarks for 3D face reconstruction. It also outperforms the SOTA reconstruction models in recovering rendered faces' identities across novel poses. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces.",https://openaccess.thecvf.com/content/WACV2024/html/Rai_Towards_Realistic_Generative_3D_Face_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rai_Towards_Realistic_Generative_3D_Face_Models_WACV_2024_paper.pdf,https://aashishrai3799.github.io/Towards-Realistic-Generative-3D-Face-Models,https://github.com/aashishrai3799/Towards-Realistic-Generative-3D-Face-Models,2304.12483,main,Poster,https://ieeexplore.ieee.org/document/10483940/,"['Solid modeling', 'Three-dimensional displays', 'Limiting', 'Shape', 'Computational modeling', 'Semantics', 'Rendering (computer graphics)']","['Face Model', '3D Face', '3D Face Model', 'Realistic 3D Face', 'Benchmark', '3D Reconstruction', 'Latent Space', '2D Model', '3D Shape', 'Fetishism', 'Virtual Avatar', 'Synthetic Data Generation', 'Model Parameters', '2D Images', 'Image Generation', 'Face Images', '3D Scanning', 'Shape Model', 'Spherical Harmonics', 'Latent Vector', 'Shape Optimization', 'Shape Reconstruction', 'Latent Code', 'Displacement Maps', 'Smooth Mesh', 'Multiple Poses', 'Choropleth Maps', 'Landmark Detection', 'Coarse Mesh', 'Detailed Shape']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",7,"In recent years, there has been significant progress in 2D generative face models fueled by applications such as animation, synthetic data generation, and digital avatars. However, due to the absence of 3D information, these 2D models often struggle to accurately disentangle facial attributes like pose, expression, and illumination, limiting their editing capabilities. To address this limitation, this paper proposes a 3D controllable generative face model to produce high-quality albedo and precise 3D shapes by leveraging existing 2D generative models. By combining 2D face generative models with semantic face manipulation, this method enables editing of detailed 3D rendered faces. The proposed framework utilizes an alternating descent optimization approach over shape and albedo. Differentiable rendering is used to train high-quality shapes and albedo without 3D supervision. Moreover, this approach outperforms most state-of-the-art (SOTA) methods in the well-known NoW and REALY benchmarks for 3D face re construction. It also outperforms the SOTA reconstruction models in recovering rendered faces’ identities across novel poses. Additionally, the paper demonstrates direct control of expressions in 3D faces by exploiting latent space leading to text-based editing of 3D faces."
Towards Visual Saliency Explanations of Face Verification,"Yuhang Lu, Zewei Xu, Touradj Ebrahimi","EPFL, Lausanne, Switzerland",100.0,Switzerland,0.0,,"In the past years, deep convolutional neural networks have been pushing the frontier of face recognition (FR) techniques in both verification and identification scenarios. Despite the high accuracy, they are often criticized for lacking explainability. There has been an increasing demand for understanding the decision-making process of deep face recognition systems. Recent studies have investigated the usage of visual saliency maps as an explanation, but they often lack a discussion and analysis in the context of face recognition. This paper concentrates on explainable face verification tasks and conceives a new explanation framework. Firstly, a definition of the saliency-based explanation method is provided, which focuses on the decisions made by the deep FR model. Secondly, a new model-agnostic explanation method named CorrRISE is proposed to produce saliency maps, which reveal both the similar and dissimilar regions of any given pair of face images. Then, an evaluation methodology is designed to measure the performance of general visual saliency explanation methods in face verification. Finally, substantial visual and quantitative results have shown that the proposed CorrRISE method demonstrates promising results in comparison with other state-of-the-art explainable face verification approaches.",https://openaccess.thecvf.com/content/WACV2024/html/Lu_Towards_Visual_Saliency_Explanations_of_Face_Verification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lu_Towards_Visual_Saliency_Explanations_of_Face_Verification_WACV_2024_paper.pdf,,,2305.08546,main,Poster,https://ieeexplore.ieee.org/document/10483988/,"['Visualization', 'Computer vision', 'Face recognition', 'Decision making', 'Convolutional neural networks', 'Task analysis']","['Face Recognition', 'Visual Saliency', 'Visual Explanation', 'Convolutional Neural Network', 'Quantitative Results', 'Deep Models', 'Deep Convolutional Neural Network', 'Image Pairs', 'Similar Regions', 'Face Images', 'Exploratory Methods', 'Evaluation Methodology', 'Saliency Map', 'Face Recognition Model', 'Classification Task', 'Input Image', 'Image Pixels', 'Similarity Score', 'Visual Comparison', 'Patch Size', 'Matched Pairs', 'Image Retrieval', 'Sanity Check', 'Verification System', 'Challenging Scenarios', 'Vision Tasks', 'Verification Method', 'Accuracy Verification', 'Input Pair', 'Class Activation Maps']","['Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",2,"In the past years, deep convolutional neural networks have been pushing the frontier of face recognition (FR) techniques in both verification and identification scenarios. Despite the high accuracy, they are often criticized for lacking explainability. There has been an increasing demand for understanding the decision-making process of deep face recognition systems. Recent studies have investigated the usage of visual saliency maps as an explanation, but they often lack a discussion and analysis in the context of face recognition. This paper concentrates on explainable face verification tasks and conceives a new explanation framework. Firstly, a definition of the saliency-based explanation method is provided, which focuses on the decisions made by the deep FR model. Secondly, a new model-agnostic explanation method named CorrRISE is proposed to produce saliency maps, which reveal both the similar and dissimilar regions of any given pair of face images. Then, an evaluation methodology is designed to measure the performance of general visual saliency explanation methods in face verification. Finally, substantial visual and quantitative results have shown that the proposed CorrRISE method demonstrates promising results in comparison with other state-of-the-art explainable face verification approaches."
Towards a Dynamic Vision Sensor-Based Insect Camera Trap,"Eike Gebauer, Sebastian Thiele, Pierre Ouvrard, Adrien Sicard, Benjamin Risse","Institute for Geoinformatics, University of Münster, Germany; Faculty of Mathematics and Computer Science, University of Münster, Germany; Dep. of Plant Biology, Uppsala BioCenter, Swedish University of Agricultural Sciences, Sweden",100.0,"Germany, Sweden",0.0,,"This paper introduces a visual real-time insect monitoring approach capable of detecting and tracking tiny and fast-moving objects in cluttered wildlife conditions using an RGB-DVS stereo-camera system. By building on the intrinsic benefits of event vision data acquisition, we demonstrate that insect presence can be detected at an extremely high temporal rate (on average more than 40 times real-time) while surpassing the spatial and spectral sensitivity of conventional colour-based sensing. Our DVS-based detection and tracking algorithm extracts insect locations over time, and we evaluated our system based on 81104 manually annotated stereo-frames with 34453 insect appearances featuring highly varying scenes and imaging conditions (including clutter, wind-induced motion, etc.). Comparing our algorithm to two state-of-the-art deep learning algorithms reveals superior results in both detection performance and computational speed. Using the DVS as a trigger for the temporally synchronised RGB camera, we are able to correctly identify 73% of images with and without insects which can be increased to 76% with parameters optimised for different scenes. Overall, our study suggests that DVS-based sensing can be used for visual insect monitoring by enabling reliable real-time insect detection in wildlife conditions while significantly reducing the necessity for data storage, manual labour and energy.",https://openaccess.thecvf.com/content/WACV2024/html/Gebauer_Towards_a_Dynamic_Vision_Sensor-Based_Insect_Camera_Trap_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gebauer_Towards_a_Dynamic_Vision_Sensor-Based_Insect_Camera_Trap_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483831/,"['Deep learning', 'Visualization', 'Insects', 'Buildings', 'Wildlife', 'Cameras', 'Real-time systems']","['Camera Traps', 'Deep Learning', 'Real-time Detection', 'Detection Performance', 'Tracking Algorithm', 'RGB Camera', 'Stereo Camera', 'Presence Of Insects', 'Dynamic Vision Sensor', 'Tiny Objects', 'Field Of View', 'F1 Score', 'Processing Speed', 'Event Rate', 'Object Detection', 'Real-time Performance', 'Bounding Box', 'Cluster Centers', 'High Energy Consumption', 'Mahalanobis Distance', 'Event Stream', 'Global Configuration', 'High Dynamic Range', 'Noisy Information', 'Feature Tracking', 'Decay Factor', 'Trigger Signal', 'Temporal Synchrony', 'Vision Sensors', 'Dynamic Scenes']","['Applications', 'Animals / Insects', 'Algorithms', 'Video recognition and understanding', 'Applications', 'Environmental monitoring / climate change / ecology']",,"This paper introduces a visual real-time insect monitoring approach capable of detecting and tracking tiny and fast-moving objects in cluttered wildlife conditions using an RGB-DVS stereo-camera system. By building on the intrinsic benefits of event vision data acquisition, we demonstrate that insect presence can be detected at an extremely high temporal rate (on average more than 40 times real-time) while surpassing the spatial and spectral sensitivity of conventional colour-based sensing. Our DVS-based detection and tracking algorithm extracts insect locations over time, and we evaluated our system based on 81104 manually annotated stereo-frames with 34453 insect appearances featuring highly varying scenes and imaging conditions (including clutter, wind-induced motion, etc.). Comparing our algorithm to two state-of-the-art deep learning algorithms reveals superior results in both detection performance and computational speed. Using the DVS as a trigger for the temporally synchronised RGB camera, we are able to correctly identify 73% of images with and without insects which can be increased to 76% with parameters optimised for different scenes. Overall, our study suggests that DVS-based sensing can be used for visual insect monitoring by enabling reliable real-time insect detection in wildlife conditions while significantly reducing the necessity for data storage, manual labour and energy."
Tracking Skiers From the Top to the Bottom,"Matteo Dunnhofer, Luca Sordi, Niki Martinel, Christian Micheloni","Machine Learning and Perception Lab, University of Udine, Udine, Italy",100.0,Italy,0.0,,"Skiing is a popular winter sport discipline with a long history of competitive events. In this domain, computer vision has the potential to enhance the understanding of athletes' performance, but its application lags behind other sports due to limited studies and datasets. This paper makes a step forward in filling such gaps. A thorough investigation is performed on the task of skier tracking in a video capturing his/her complete performance. Obtaining continuous and accurate skier localization is preemptive for further higher-level performance analyses. To enable the study, the largest and most annotated dataset for computer vision in skiing, SkiTB, is introduced. Several visual object tracking algorithms, including both established methodologies and a newly introduced skier-optimized baseline algorithm, are tested using the dataset. The results provide valuable insights into the applicability of different tracking methods for vision-based skiing analysis. SkiTB, code, and results are available at https://machinelearning.uniud.it/datasets/skitb.",https://openaccess.thecvf.com/content/WACV2024/html/Dunnhofer_Tracking_Skiers_From_the_Top_to_the_Bottom_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dunnhofer_Tracking_Skiers_From_the_Top_to_the_Bottom_WACV_2024_paper.pdf,https://machinelearning.uniud.it/datasets/skitb,,2312.09723,main,Poster,https://ieeexplore.ieee.org/document/10483844/,"['Computer vision', 'Visualization', 'Target tracking', 'Refining', 'Cameras', 'Robustness', 'Performance analysis']","['Disciplines', 'Athletes', 'Computer Vision', 'Tracking Algorithm', 'Video Tracking', 'Object Tracking', 'Video Capture', 'Benchmark', 'Training Set', 'Extreme Weather', 'Object Detection', 'Video Frames', 'Tracking Performance', 'Evaluation Protocol', 'Tracking Accuracy', 'Pose Estimation', 'Fast Motion', 'Extreme Weather Conditions', 'Multiple Cameras', 'Background Clutter', 'Alpine Skiing', 'Video Collection', '2D Pose', '3D Pose']","['Applications', 'Social good', 'Algorithms', 'Video recognition and understanding']",1,"Skiing is a popular winter sport discipline with a long history of competitive events. In this domain, computer vision has the potential to enhance the understanding of athletes’ performance, but its application lags behind other sports due to limited studies and datasets. This paper makes a step forward in filling such gaps. A thorough investigation is performed on the task of skier tracking in a video capturing his/her complete performance. Obtaining continuous and accurate skier localization is preemptive for further higher-level performance analyses. To enable the study, the largest and most annotated dataset for computer vision in skiing, SkiTB, is introduced. Several visual object tracking algorithms, including both established methodologies and a newly introduced skier-optimized baseline algorithm, are tested using the dataset. The results provide valuable insights into the applicability of different tracking methods for vision-based skiing analysis. SkiTB, code, and results are available at https://machinelearning.uniud.it/datasets/skitb."
Tracking Tiny Insects in Cluttered Natural Environments Using Refinable Recurrent Neural Networks,"Lars Haalck, Sebastian Thiele, Benjamin Risse","Institute for Geoinformatics and Institute for Computer Science, University of Münster, Germany",100.0,Germany,0.0,,"Visual tracking of tiny and low-contrast objects such as insects in cluttered natural environments is a very challenging computer vision task. This is particularly true for machine learning algorithms, which usually require distinct visual foreground features to reliably identify the object of interest. Here, we propose a novel deep learning-based tracking framework capable of detecting tiny and visually camouflaged ants (covering only a few pixels) in complex and dynamic high-resolution videos. In particular, we introduce refinable recurrent Hourglass Networks, which combine colour and temporal information to continuously detect insects recorded using a freely moving camera. Moreover, this architecture provides comprehensible heatmaps of positional estimations and a seamless integration of optional user-input to further refine the tracking results if necessary. We evaluated our algorithm on an extremely challenging wildlife ant dataset with a resolution of 1024x1024 and report a mean deviation of 19 pixels from the ground truth (object  30 px) without any user input. By providing only 0.6% manual locations this accuracy can be improved to a mean deviation of 9 pixels. A comparison to a well known deep learning-based single frame detection algorithm (YOLOv7), two state-of-the-art tracking methods (ToMP and KeepTrack), a probabilistic tracking framework and a comprehensive ablation study reveal superior performances in all our experiments. Our tracking framework therefore provides a foundation for challenging tiny single-object tracking scenarios and a practical and interactive solution for biologists and ecologists.",https://openaccess.thecvf.com/content/WACV2024/html/Haalck_Tracking_Tiny_Insects_in_Cluttered_Natural_Environments_Using_Refinable_Recurrent_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Haalck_Tracking_Tiny_Insects_in_Cluttered_Natural_Environments_Using_Refinable_Recurrent_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483788/,"['Heating systems', 'Visualization', 'Computer vision', 'Uncertainty', 'Insects', 'Wildlife', 'Manuals']","['Recurrent Network', 'Recurrent Neural Network', 'Tiny Insects', 'Challenging Task', 'Visual Features', 'Temporal Information', 'Object Of Interest', 'Single Frame', 'User Input', 'Challenging Dataset', 'Single Algorithm', 'Tracking Framework', 'Deep Learning-based Framework', 'Tiny Objects', 'Local Information', 'Spatial Information', 'Feature Maps', 'Object Detection', 'Data Augmentation', 'Abrupt Changes', 'Small Objects', 'Factor Graph', 'Bounding Box', 'Input Modalities', 'Challenging Scenarios', 'Object Tracking', 'Hidden State', 'Low Contrast', 'Single Object', 'Additional Input']","['Applications', 'Animals / Insects', 'Algorithms', 'Video recognition and understanding', 'Applications', 'Environmental monitoring / climate change / ecology']",,"Visual tracking of tiny and low-contrast objects such as insects in cluttered natural environments is a very challenging computer vision task. This is particularly true for machine learning algorithms, which usually require distinct visual foreground features to reliably identify the object of interest. Here, we propose a novel deep learning-based tracking framework capable of detecting tiny and visually camouflaged ants (covering only a few pixels) in complex and dynamic high-resolution videos. In particular, we introduce refinable recurrent Hourglass Networks, which combine color and temporal information to continuously detect insects recorded using a freely moving camera. Moreover, this architecture provides comprehensible heatmaps of positional estimations and a seamless integration of optional user-input to further refine the tracking results if necessary. We evaluated our algorithm on an extremely challenging wildlife ant dataset with a resolution of 1024 × 1024 and report a mean deviation of 19 pixels from the ground truth (object ≈ 30 px) without any user input. By providing only 0.6% manual locations this accuracy can be improved to a mean deviation of 9 pixels. A comparison to a well known deep learning-based single frame detection algorithm (YOLOv7), two state-of-the-art tracking methods (ToMP and KeepTrack), a probabilistic tracking framework and a comprehensive ablation study reveal superior performances in all our experiments. Our tracking framework therefore provides a foundation for challenging tiny singleobject tracking scenarios and a practical and interactive solution for biologists and ecologists."
Training Ensembles With Inliers and Outliers for Semi-Supervised Active Learning,"Vladan Stojnić, Zakaria Laskar, Giorgos Tolias","Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague",100.0,Czech Republic,0.0,,"Deep active learning in the presence of outlier examples poses a realistic yet challenging scenario. Acquiring unlabeled data for annotation requires a delicate balance between avoiding outliers to conserve the annotation budget and prioritizing useful inlier examples for effective training. In this work, we present an approach that leverages three highly synergistic components, which are identified as key ingredients: joint classifier training with inliers and outliers, semi-supervised learning through pseudo-labeling, and model ensembling. Our work demonstrates that ensembling significantly enhances the accuracy of pseudo-labeling and improves the quality of data acquisition. By enabling semi-supervision through the joint training process, where outliers are properly handled, we observe a substantial boost in classifier accuracy through the use of all available unlabeled examples. Notably, we reveal that the integration of joint training renders explicit outlier detection unnecessary; a conventional component for acquisition in prior work. The three key components align seamlessly with numerous existing approaches. Through empirical evaluations, we showcase that their combined use leads to a performance increase. Remarkably, despite its simplicity, our proposed approach outperforms all other methods in terms of performance. Code: https://github.com/vladan-stojnic/active-outliers",https://openaccess.thecvf.com/content/WACV2024/html/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Stojnic_Training_Ensembles_With_Inliers_and_Outliers_for_Semi-Supervised_Active_Learning_WACV_2024_paper.pdf,,https://github.com/vladan-stojnic/active-outliers,,main,Poster,https://ieeexplore.ieee.org/document/10484252/,"['Training', 'Computer vision', 'Codes', 'Annotations', 'Source coding', 'Data acquisition', 'Semisupervised learning']","['Active Learning', 'Semi-supervised Learning', 'Classification Accuracy', 'Ensemble Model', 'Classifier Training', 'Key Ingredient', 'Outlier Detection', 'Joint Training', 'Presence Of Learning', 'Unlabeled Examples', 'Scoring Function', 'ImageNet', 'Large Margin', 'Performance Gap', 'Measures Of Dispersion', 'Presence Of Outliers', 'Acquisition Function', 'Self-supervised Learning', 'Top Performers', 'Large Divergence', 'Outlier Filtering', 'Unlabeled Set', 'Ensemble Of Networks', 'Maximum Confidence', 'Statistical Dispersion']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Deep active learning in the presence of outlier examples poses a realistic yet challenging scenario. Acquiring unlabeled data for annotation requires a delicate balance between avoiding outliers to conserve the annotation budget and prioritizing useful inlier examples for effective training. In this work, we present an approach that leverages three highly synergistic components, which are identified as key ingredients: joint classifier training with inliers and outliers, semi-supervised learning through pseudo-labeling, and model ensembling. Our work demonstrates that ensembling significantly enhances the accuracy of pseudolabeling and improves the quality of data acquisition. By enabling semi-supervision through the joint training process, where outliers are properly handled, we observe a substantial boost in classifier accuracy through the use of all available unlabeled examples. Notably, we reveal that the integration of joint training renders explicit outlier detection unnecessary; a conventional component for acquisition in prior work. The three key components align seamlessly with numerous existing approaches. Through empirical evaluations, we showcase that their combined use leads to a performance increase. Remarkably, despite its simplicity, our proposed approach outperforms all other methods in terms of performance. Code: https://github.com/vladan-stojnic/active-outliers"
Training-Based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection,"Seyed Mojtaba Marvasti-Zadeh, Nilanjan Ray, Nadir Erbilgin",University of Alberta,100.0,Canada,0.0,,"Semi-supervised object detection (SSOD) aims to improve the performance and generalization of existing object detectors by utilizing limited labeled data and extensive unlabeled data. Despite many advances, recent SSOD methods are still challenged by inadequate model refinement using the classical exponential moving average (EMA) strategy, the consensus of Teacher-Student models in the latter stages of training (i.e., losing their distinctiveness), and noisy/misleading pseudo-labels. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore additional patterns in unlabeled data. Our approach can be integrated into established SSOD methods and is empirically validated using two baseline methods, with and without cascade regression, to generate more reliable pseudo-labels. Extensive experiments demonstrate the superior performance of our approach over state-of-the-art SSOD methods. Specifically, the proposed approach outperforms the baseline Unbiased-Teacher-v2 (& Unbiased-Teacher-v1) method by an average mAP margin of 2.23, 2.1, and 3.36 (& 2.07, 1.9, and 3.27) on COCO-standard, COCO-additional, and Pascal VOC datasets, respectively.",https://openaccess.thecvf.com/content/WACV2024/html/Marvasti-Zadeh_Training-Based_Model_Refinement_and_Representation_Disagreement_for_Semi-Supervised_Object_Detection_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Marvasti-Zadeh_Training-Based_Model_Refinement_and_Representation_Disagreement_for_Semi-Supervised_Object_Detection_WACV_2024_paper.pdf,,,2307.13755,main,Poster,https://ieeexplore.ieee.org/document/10484429/,"['Training', 'Computer vision', 'Object detection', 'Detectors', 'Data models', 'Reliability', 'Noise measurement']","['Object Detection', 'Model Refinement', 'Semi-supervised Object Detection', 'Patterns In Data', 'Training Stage', 'Baseline Methods', 'Unlabeled Data', 'Student Model', 'PASCAL VOC', 'PASCAL VOC Dataset', 'Time Step', 'Learning Rate', 'Data Augmentation', 'Teacher Model', 'Model Weights', 'Learnable Parameters', 'Semi-supervised Learning', 'Semantic Representations', 'Classical Scheme', 'Robust Representation', 'Basic Detection', 'MS COCO Dataset', 'Unlabeled Images', 'Loss Coefficient', 'Ablation Analysis', 'Empirical Experiments', 'Noisy Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Semi-supervised object detection (SSOD) aims to improve the performance and generalization of existing object detectors by utilizing limited labeled data and extensive unlabeled data. Despite many advances, recent SSOD methods are still challenged by inadequate model refinement using the classical exponential moving average (EMA) strategy, the consensus of Teacher-Student models in the latter stages of training (i.e., losing their distinctiveness), and noisy/misleading pseudo-labels. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model’s weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore additional patterns in unlabeled data. Our approach can be integrated into established SSOD methods and is empirically validated using two baseline methods, with and without cascade regression, to generate more reliable pseudo-labels. Extensive experiments demonstrate the superior performance of our approach over state-of-the-art SSOD methods. Specifically, the proposed approach outperforms the baseline Unbiased-Teacher-v2 (& Unbiased-Teacher-v1) method by an average mAP margin of 2.23, 2.1, and 3.36 (& 2.07, 1.9 and 3.27) on COCO-standard, COCO-additional, and Pascal VOC datasets, respectively."
Training-Free Content Injection Using H-Space in Diffusion Models,"Jaeseok Jeong, Mingi Kwon, Youngjung Uh","Yonsei University, Seoul, Republic of Korea",100.0,South Korea,0.0,,"Diffusion models (DMs) synthesize high-quality images in various domains. However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. Recently, the bottleneck feature of the U-Net, namely h-space, is found to convey the semantics of the resulting image. It enables StyleCLIP-like latent editing within DMs. In this paper, we explore further usage of h-space beyond attribute editing, and introduce a method to inject the content of one image into another image by combining their features in the generative processes. Briefly, given the original generative process of the other image, 1) we gradually blend the bottleneck feature of the content with proper normalization, and 2) we calibrate the skip connections to match the injected content. Unlike custom-diffusion approaches, our method does not require time-consuming optimization or fine-tuning. Instead, our method manipulates intermediate features within a feed-forward generative process. Furthermore, our method does not require supervision from external networks.",https://openaccess.thecvf.com/content/WACV2024/html/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jeong_Training-Free_Content_Injection_Using_H-Space_in_Diffusion_Models_WACV_2024_paper.pdf,https://curryjung.github.io/DiffStyle/,,,main,Poster,https://ieeexplore.ieee.org/document/10484256/,"['Computer vision', 'Computational modeling', 'Semantics', 'Process control', 'Optimization']","['Diffusion Model', 'Semantic', 'Generation Process', 'Skip Connections', 'Intermediate Features', 'Denoising', 'Feature Maps', 'Reversible Process', 'Generative Adversarial Networks', 'Latent Space', 'Hair Color', 'Forward Process', 'Color Distribution', 'Style Transfer', 'Exemplary Images', 'Asymmetric Process', 'Fréchet Inception Distance']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Computational photography', 'image and video synthesis']",3,"Diffusion models (DMs) synthesize high-quality images in various domains. However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. Recently, the bottleneck feature of the U-Net, namely h-space, is found to convey the semantics of the resulting image. It enables StyleCLIP-like latent editing within DMs. In this paper, we explore further usage of h-space beyond attribute editing, and introduce a method to inject the content of one image into another image by combining their features in the generative processes. Briefly, given the original generative process of the other image, 1) we gradually blend the bottleneck feature of the content with proper normalization, and 2) we calibrate the skip connections to match the injected content. Unlike custom-diffusion approaches, our method does not require time-consuming optimization or fine-tuning. Instead, our method manipulates intermediate features within a feed-forward generative process. Furthermore, our method does not require supervision from external networks. Project page: https://curryjung.github.io/DiffStyle/"
Training-Free Layout Control With Cross-Attention Guidance,"Minghao Chen, Iro Laina, Andrea Vedaldi","Visual Geometry Group, University of Oxford",100.0,UK,0.0,,"Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images.",https://openaccess.thecvf.com/content/WACV2024/html/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf,,,2304.03373,main,Poster,https://ieeexplore.ieee.org/document/10484388/,"['Training', 'Visualization', 'Computer vision', 'Layout', 'Semantics', 'Noise', 'Benchmark testing']","['Visual Information', 'Image Generation', 'Need For Training', 'Textual Information', 'Attention Map', 'Denoising', 'Scaling Factor', 'Generation Process', 'Bounding Box', 'Generative Adversarial Networks', 'Diffusion Model', 'Spatial Dependence', 'Window Function', 'Image X', 'Image Editing', 'Latent Code', 'Initial Noise', 'Image Fidelity', 'Text Encoder']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc']",30,"Recent diffusion-based generators can produce high-quality images from textual prompts. However, they often disregard textual instructions that specify the spatial layout of the composition. We propose a simple approach that achieves robust layout control without the need for training or fine-tuning of the image generator. Our technique manipulates the cross-attention layers that the model uses to interface textual and visual information and steers the generation in the desired direction given, e.g., a user-specified layout. To determine how to best guide attention, we study the role of attention maps and explore two alternative strategies, forward and backward guidance. We thoroughly evaluate our approach on three benchmarks and provide several qualitative examples and a comparative analysis of the two strategies that demonstrate the superiority of backward guidance compared to forward guidance, as well as prior work. We further demonstrate the versatility of layout guidance by extending it to applications such as editing the layout and context of real images."
Training-Free Object Counting With Prompts,"Zenglin Shi, Ying Sun, Mengmi Zhang","Nanyang Technological University (NTU), Singapore; CFAR, Agency for Science, Technology and Research, Singapore; I2R, Agency for Science, Technology and Research, Singapore",33.33333333333333,Singapore,66.66666666666667,Singapore,"This paper tackles the problem of object counting in images. Existing approaches rely on extensive training data with point annotations for each object, making data collection labor-intensive and time-consuming. To overcome this, we propose a training-free object counter that treats the counting task as a segmentation problem. Our approach leverages the Segment Anything Model (SAM), known for its high-quality masks and zero-shot segmentation capability. However, the vanilla mask generation method of SAM lacks class-specific information in the masks, resulting in inferior counting accuracy. To overcome this limitation, we introduce a prior-guided mask generation method that incorporates three types of priors into the segmentation process, enhancing efficiency and accuracy. Additionally, we tackle the issue of counting objects specified through text by proposing a two-stage approach that combines reference object selection and prior-guided mask generation. Extensive experiments on standard datasets demonstrate the competitive performance of our training-free counter compared to learning-based approaches. This paper presents a promising solution for counting objects in various scenarios without the need for extensive data collection and counting-specific training. Code is available at https://github.com/shizenglin/training-free-object-counter.",https://openaccess.thecvf.com/content/WACV2024/html/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shi_Training-Free_Object_Counting_With_Prompts_WACV_2024_paper.pdf,,https://github.com/shizenglin/training-free-object-counter,2307.00038,main,Poster,https://ieeexplore.ieee.org/document/10483595/,"['Training', 'Image segmentation', 'Computer vision', 'Codes', 'Annotations', 'Training data', 'Data collection']","['Object Counting', 'Need For Training', 'Learning-based Approaches', 'Two-stage Approach', 'Segmentation Process', 'Counting Accuracy', 'Segmentation Problem', 'Reference Object', 'Counting Task', 'Need For Data Collection', 'Counting Problem', 'Image Features', 'Image Object', 'Density Map', 'Training Images', 'Bounding Box', 'Segmentation Model', 'Counting Method', 'Baseline Methods', 'Target Object', 'Similarity Map', 'Foundation Model', 'Counting Approach', 'T Point', 'Image Encoder', 'Binary Map', 'Current Batch', 'Training Categories', 'Segmented Regions', 'Variety Of Scenarios']","['Algorithms', 'Image recognition and understanding']",6,"This paper tackles the problem of object counting in images. Existing approaches rely on extensive training data with point annotations for each object, making data collection labor-intensive and time-consuming. To overcome this, we propose a training-free object counter that treats the counting task as a segmentation problem. Our approach leverages the Segment Anything Model (SAM), known for its high-quality masks and zero-shot segmentation capability. However, the vanilla mask generation method of SAM lacks class-specific information in the masks, resulting in inferior counting accuracy. To overcome this limitation, we introduce a prior-guided mask generation method that incorporates three types of priors into the segmentation process, enhancing efficiency and accuracy. Additionally, we tackle the issue of counting objects specified through text by proposing a two-stage approach that combines reference object selection and prior-guided mask generation. Extensive experiments on standard datasets demonstrate the competitive performance of our training-free counter compared to learning-based approaches. This paper presents a promising solution for counting objects in various scenarios without the need for extensive data collection and counting-specific training. Code is available at https://github.com/shizenglin/training-free-object-counter."
TransFed: A Way To Epitomize Focal Modulation Using Transformer-Based Federated Learning,"Tajamul Ashraf, Fuzayil Bin Afzal Mir, Iqra Altaf Gillani","NIT Srinagar, Srinagar, India; IIT Delhi, New Delhi, India",100.0,India,0.0,,"Federated learning has emerged as a promising paradigm for collaborative machine learning, enabling multiple clients to train a model while preserving data privacy jointly. Tailored federated learning takes this concept further by accommodating client heterogeneity and facilitating the learning of personalized models. While the utilization of transformers within federated learning has attracted significant interest, there remains a need to investigate the effects of federated learning algorithms on the latest focal modulation-based transformers. In this paper, we investigate this relationship and uncover the detrimental effects of federated averaging (FedAvg) algorithms on Focal Modulation, particularly in scenarios with heterogeneous data. To address this challenge, we propose TransFed, a novel transformer-based federated learning framework that not only aggregates model parameters but also learns tailored Focal Modulation for each client. Instead of employing a conventional customization mechanism that maintains client-specific focal modulation layers locally, we introduce a learn-to-tailor approach that fosters client collaboration, enhancing scalability and adaptation in TransFed. Our method incorporates a hyper network on the server, responsible for learning personalized projection matrices for the focal modulation layers. This enables the generation of client-specific keys, values, and queries. Furthermore, we provide an analysis of adaptation bounds for TransFed using the learn-to-customize mechanism. Through intensive experiments on datasets related to pneumonia classification, we demonstrate that TransFed, in combination with the learn-to-tailor approach, achieves superior performance in scenarios with non-IID data distributions, surpassing existing methods. Overall, TransFed paves the way for leveraging focal Modulation in federated learning, advancing the capabilities of focal modulated transformer models in decentralized environments.",https://openaccess.thecvf.com/content/WACV2024/html/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ashraf_TransFed_A_Way_To_Epitomize_Focal_Modulation_Using_Transformer-Based_Federated_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483650/,"['Data privacy', 'Computer vision', 'Pneumonia', 'Federated learning', 'Scalability', 'Modulation', 'Collaboration']","['Federated Learning', 'Focal Modulation', 'Scalable', 'Data Distribution', 'Heterogeneous Data', 'Projection Matrix', 'Transformer Model', 'Key Generation', 'Federated Learning Framework', 'Federated Learning Algorithm', 'Convolutional Neural Network', 'Global Model', 'Stochastic Gradient Descent', 'Vanilla', 'Beta Distribution', 'Attention Map', 'Embedding Vectors', 'Local Dataset', 'Local Training', 'Benchmark Methods', 'Communication Rounds', 'Vision Transformer', 'Stochastic Gradient Descent Optimizer']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Biomedical / healthcare / medicine']",1,"Federated learning has emerged as a promising paradigm for collaborative machine learning, enabling multiple clients to train a model while preserving data privacy jointly. Tailored federated learning takes this concept further by accommodating client heterogeneity and facilitating the learning of personalized models. While the utilization of transformers within federated learning has attracted significant interest, there remains a need to investigate the effects of federated learning algorithms on the latest focal modulation-based transformers. In this paper, we investigate this relationship and uncover the detrimental effects of federated averaging (FedAvg) algorithms on Focal Modulation, particularly in scenarios with heterogeneous data. To address this challenge, we propose TransFed, a novel transformer-based federated learning framework that not only aggregates model parameters but also learns tailored Focal Modulation for each client. Instead of employing a conventional customization mechanism that maintains client-specific focal modulation layers locally, we introduce a learn-to-tailor approach that fosters client collaboration, enhancing scalability and adaptation in TransFed. Our method incorporates a hyper network on the server, responsible for learning personalized projection matrices for the focal modulation layers. This enables the generation of client-specific keys, values, and queries.Furthermore, we provide an analysis of adaptation bounds for TransFed using the learn-to-customize mechanism. Through intensive experiments on datasets related to pneumonia classification, we demonstrate that TransFed, in combination with the learn-to-tailor approach, achieves superior performance in scenarios with non-IID data distributions, surpassing existing methods. Overall, TransFed paves the way for leveraging focal Modulation in federated learning, advancing the capabilities of focal modulated transformer models in decentralized environments."
TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation,"Yahia Dalbah, Jean Lahoud, Hisham Cholakkal",Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI),100.0,UAE,0.0,,"Scene understanding plays an essential role in enabling autonomous driving and maintaining high standards of performance and safety. To address this task, cameras and laser scanners (LiDARs) have been the most commonly used sensors, with radars being less popular. Despite that, radars remain low-cost, information-dense, and fast-sensing techniques that are resistant to adverse weather conditions. While multiple works have been previously presented for radar-based scene semantic segmentation, the nature of the radar data still poses a challenge due to the inherent noise and sparsity, as well as the disproportionate foreground and background. In this work, we propose a novel approach to the semantic segmentation of radar scenes using a multi-input fusion of radar data through a novel architecture and loss functions that are tailored to tackle the drawbacks of radar perception. Our novel architecture includes an efficient attention block that adaptively captures important feature information. Our method, TransRadar, outperforms state-of-the-art methods on the CARRADA and RADIal datasets while having smaller model sizes.",https://openaccess.thecvf.com/content/WACV2024/html/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dalbah_TransRadar_Adaptive-Directional_Transformer_for_Real-Time_Multi-View_Radar_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/YahiDar/TransRadar,2310.02260,main,Poster,https://ieeexplore.ieee.org/document/10483848/,"['Adaptation models', 'Laser radar', 'Fuses', 'Semantic segmentation', 'Radar detection', 'Radar', 'Computer architecture']","['Semantic Segmentation', 'Loss Function', 'Sparsity', 'Radar Data', 'Attention Block', 'Smaller Model Size', 'Convolutional Neural Network', 'Convolutional Layers', 'Feature Maps', 'Object Detection', 'Intersection Over Union', 'Probability Function', 'Radar Images', 'Combined Loss', 'Radar Signal', 'Point Cloud Data', 'Object Detection Task', 'Semantic Segmentation Task', 'Adaptive Sampling', 'LiDAR Sensor', 'Noisy Nature', 'Multi-head Self-attention', 'Loss Of Coherence', 'Atrous Spatial Pyramid Pooling', 'Radar Frequency', 'Object Velocity', 'Class Imbalance', 'Cross-entropy', 'Receptive Field']","['Algorithms', 'Image recognition and understanding']",3,"Scene understanding plays an essential role in enabling autonomous driving and maintaining high standards of performance and safety. To address this task, cameras and laser scanners (LiDARs) have been the most commonly used sensors, with radars being less popular. Despite that, radars remain low-cost, information-dense, and fast-sensing techniques that are resistant to adverse weather conditions. While multiple works have been previously presented for radar-based scene semantic segmentation, the nature of the radar data still poses a challenge due to the inherent noise and sparsity, as well as the disproportionate foreground and background. In this work, we propose a novel approach to the semantic segmentation of radar scenes using a multi-input fusion of radar data through a novel architecture and loss functions that are tailored to tackle the drawbacks of radar perception. Our novel architecture includes an efficient attention block that adaptively captures important feature information. Our method, TransRadar, outperforms state-of-the-art methods on the CARRADA [26] and RADIal [28] datasets while having smaller model sizes. https://github.com/YahiDar/TransRadar"
TriCoLo: Trimodal Contrastive Loss for Text To Shape Retrieval,"Yue Ruan, Han-Hung Lee, Yiming Zhang, Ke Zhang, Angel X. Chang","Simon Fraser University, Alberta Machine Intelligence Institute (Amii); Simon Fraser University",100.0,Canada,0.0,,"Text-to-shape retrieval is an increasingly relevant problem with the growth of 3D shape data. Recent work on contrastive losses for learning joint embeddings over multimodal data has been successful at tasks such as retrieval and classification. Thus far, work on joint representation learning for 3D shapes and text has focused on improving embeddings through modeling of complex attention between representations, or multi-task learning. We propose a trimodal learning scheme over text, multi-view images and 3D shape voxels, and show that with large batch contrastive learning we achieve good performance on text-to-shape retrieval without complex attention mechanisms or losses. Our experiments serve as a foundation for follow-up work on building trimodal embeddings for text-image-shape.",https://openaccess.thecvf.com/content/WACV2024/html/Ruan_TriCoLo_Trimodal_Contrastive_Loss_for_Text_To_Shape_Retrieval_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ruan_TriCoLo_Trimodal_Contrastive_Loss_for_Text_To_Shape_Retrieval_WACV_2024_paper.pdf,https://3dlg-hcvc.github.io/tricolo/,,,main,Poster,https://ieeexplore.ieee.org/document/10484523/,"['Representation learning', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Systematics', 'Shape', 'Buildings']","['Contrastive Loss', 'Shape Retrieval', 'Complex Mechanisms', 'Attention Mechanism', '3D Shape', 'Multi-task Learning', 'Self-supervised Learning', 'Follow-up Work', 'Multi-view Images', '3D Voxel', 'Joint Embedding', 'Loss Function', 'Image Resolution', 'Validation Set', 'Batch Size', 'Number Of Images', 'Point Cloud', 'Latent Space', 'Disambiguation', 'Image Representation', 'Text Encoder', 'Triplet Loss', 'Textual Descriptions', '3D Representation', 'Image Encoder', 'Word Embedding', 'Reconstruction Loss', 'Image Embedding', '2D Feature', 'Voxel Resolution']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', '3D computer vision']",3,"Text-to-shape retrieval is an increasingly relevant problem with the growth of 3D shape data. Recent work on contrastive losses for learning joint embeddings over multimodal data [45] has been successful at tasks such as retrieval and classification. Thus far, work on joint representation learning for 3D shapes and text has focused on improving embeddings through modeling of complex attention between representations [53], or multi-task learning [25]. We propose a trimodal learning scheme over text, multi-view images and 3D shape voxels, and show that with large batch contrastive learning we achieve good performance on text-to-shape retrieval without complex attention mechanisms or losses. Our experiments serve as a foundation for follow-up work on building trimodal embeddings for text-image-shape."
TriPlaneNet: An Encoder for EG3D Inversion,"Ananta R. Bhattarai, Matthias Nießner, Artem Sevastopolsky",Technical University of Munich (TUM),100.0,Germany,0.0,,"Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those applied to 3D GANs may fail to extrapolate the result onto the novel view, whereas optimization-based 3D GAN inversion methods are time-consuming and can require at least several minutes per image. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. Our work introduces a fast technique that bridges the gap between the two approaches by directly utilizing the tri-plane representation presented for the EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. The renderings are similar in quality to the ones produced by optimization-based techniques and outperform the ones by encoder-based methods. As we empirically prove, this is a consequence of directly operating in the tri-plane space, not in the GAN parameter space, while making use of an encoder-based trainable approach. Finally, we demonstrate significantly more correct embedding of a face image in 3D than for all the baselines, further strengthened by a probably symmetric prior enabled during training.",https://openaccess.thecvf.com/content/WACV2024/html/Bhattarai_TriPlaneNet_An_Encoder_for_EG3D_Inversion_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bhattarai_TriPlaneNet_An_Encoder_for_EG3D_Inversion_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484235/,"['Training', 'Convolutional codes', 'Computer vision', 'Three-dimensional displays', 'Inverse problems', 'Rendering (computer graphics)', 'Real-time systems']","['Inverse Method', 'Humeral Head', 'Optimization-based Methods', 'Latent Code', 'Input Image', 'Feature Maps', 'Autoencoder', '3D Space', 'Target Image', 'Mirror Image', 'Latent Space', 'Human Faces', '3D Representation', '3D Geometry', 'Image Synthesis', 'Yaw Angle', 'Encoder Network', 'Image X', 'Implicit Representation', 'Optimization-based Approach', 'View Synthesis', 'Volume Rendering']","['Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",9,"Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those applied to 3D GANs may fail to extrapolate the result onto the novel view, whereas optimization-based 3D GAN in-version methods are time-consuming and can require at least several minutes per image. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. Our work introduces a fast technique that bridges the gap between the two approaches by directly utilizing the tri-plane representation presented for the EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. The renderings are similar in quality to the ones produced by optimization-based techniques and outperform the ones by encoder-based methods. As we empirically prove, this is a consequence of directly operating in the tri-plane space, not in the GAN parameter space, while making use of an encoder-based trainable approach. Finally, we demonstrate significantly more correct embedding of a face image in 3D than for all the baselines, further strengthened by a probably symmetric prior enabled during training."
Triplet Attention Transformer for Spatiotemporal Predictive Learning,"Xuesong Nie, Xi Chen, Haoyuan Jin, Zhihang Zhu, Yunfeng Yan, Donglian Qi",The University of Hong Kong; Zhejiang University,100.0,"China, Hong Kong",0.0,,"Spatiotemporal predictive learning offers a self-supervised learning paradigm that enables models to learn both spatial and temporal patterns by predicting future sequences based on historical sequences. Mainstream methods are dominated by recurrent units, yet they are limited by their lack of parallelization and often underperform in real-world scenarios. To improve prediction quality while maintaining computational efficiency, we propose an innovative triplet attention transformer designed to capture both inter-frame dynamics and intra-frame static features. Specifically, the model incorporates the Triplet Attention Module (TAM), which replaces traditional recurrent units by exploring self-attention mechanisms in temporal, spatial, and channel dimensions. In this configuration: (i) temporal tokens contain abstract representations of inter-frame, facilitating the capture of inherent temporal dependencies; (ii) spatial and channel attention combine to refine the intra-frame representation by performing fine-grained interactions across spatial and channel dimensions. Alternating temporal, spatial, and channel-level attention allows our approach to learn more complex short- and long-range spatiotemporal dependencies. Extensive experiments demonstrate performance surpassing existing recurrent-based and recurrent-free methods, achieving state-of-the-art under multi-scenario examination including moving object trajectory prediction, traffic flow prediction, driving scene prediction, and human motion capture.",https://openaccess.thecvf.com/content/WACV2024/html/Nie_Triplet_Attention_Transformer_for_Spatiotemporal_Predictive_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nie_Triplet_Attention_Transformer_for_Spatiotemporal_Predictive_Learning_WACV_2024_paper.pdf,,,2310.18698,main,Poster,https://ieeexplore.ieee.org/document/10484415/,"['Computer vision', 'Computational modeling', 'Self-supervised learning', 'Predictive models', 'Parallel processing', 'Transformers', 'Motion capture']","['Triplet Attention', 'Parallelization', 'Spatial Dimensions', 'Real-world Scenarios', 'Motion Capture', 'Spatial Attention', 'Channel Dimension', 'Recurrent Unit', 'Human Motion', 'Self-supervised Learning', 'Channel Attention', 'Self-attention Mechanism', 'Long-range Dependencies', 'Traffic Prediction', 'Historical Sequence', 'Spatio-temporal Dependencies', 'Root Mean Square Error', 'Mean Square Error', 'Convolution', 'Forecasting', 'Temporal Attention', 'Long Short-term Memory', 'Vision Transformer', 'Structural Similarity Index Measure', 'Peak Signal-to-noise Ratio', 'Long Short-term Memory Architecture', 'Spatiotemporal Model', 'Inherent Complexity', 'Visual Task', 'Attention Mechanism']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",3,"Spatiotemporal predictive learning offers a self-supervised learning paradigm that enables models to learn both spatial and temporal patterns by predicting future sequences based on historical sequences. Mainstream methods are dominated by recurrent units, yet they are limited by their lack of parallelization and often underperform in real-world scenarios. To improve prediction quality while maintaining computational efficiency, we propose an innovative triplet attention transformer designed to capture both inter-frame dynamics and intra-frame static features. Specifically, the model incorporates the Triplet Attention Module (TAM), which replaces traditional recurrent units by exploring self-attention mechanisms in temporal, spatial, and channel dimensions. In this configuration: (i) temporal tokens contain abstract representations of inter-frame, facilitating the capture of inherent temporal dependencies; (ii) spatial and channel attention combine to refine the intra-frame representation by performing fine-grained interactions across spatial and channel dimensions. Alternating temporal, spatial, and channel-level attention allows our approach to learn more complex short-and long-range spatiotemporal dependencies. Extensive experiments demonstrate performance surpassing existing recurrent-based and recurrent-free methods, achieving state-of-the-art under multi-scenario examination including moving object trajectory prediction, traffic flow prediction, driving scene prediction, and human motion capture."
Tunable Hybrid Proposal Networks for the Open World,"Matthew Inkawhich, Nathan Inkawhich, Hai Li, Yiran Chen",Air Force Research Laboratory; Duke University,50.0,USA,50.0,USA,"Current state-of-the-art object proposal networks are trained with a closed-world assumption, meaning they learn to only detect objects of the training classes. These models fail to provide high recall in open-world environments where important novel objects may be encountered. While a handful of recent works attempt to tackle this problem, they fail to consider that the optimal behavior of a proposal network can vary significantly depending on the data and application. Our goal is to provide a flexible proposal solution that can be easily tuned to suit a variety of open-world settings. To this end, we design a Tunable Hybrid Proposal Network (THPN) that leverages an adjustable hybrid architecture, a novel self-training procedure, and dynamic loss components to optimize the tradeoff between known and unknown object detection performance. To thoroughly evaluate our method, we devise several new challenges which invoke varying degrees of label bias by altering known class diversity and label count. We find that in every task, THPN easily outperforms existing baselines (e.g., RPN, OLN). Our method is also highly data efficient, surpassing baseline recall with a fraction of the labeled data.",https://openaccess.thecvf.com/content/WACV2024/html/Inkawhich_Tunable_Hybrid_Proposal_Networks_for_the_Open_World_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Inkawhich_Tunable_Hybrid_Proposal_Networks_for_the_Open_World_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484129/,"['Training', 'Computer vision', 'Protocols', 'Object detection', 'Computer architecture', 'Distance measurement', 'Data models']","['Proposal Network', 'Open World', 'Object Detection', 'Object Classification', 'Classifier Training', 'Degree Of Bias', 'Region Proposal Network', 'Object Proposals', 'Hyperparameters', 'Stage 2', 'Total Loss', 'Bounding Box', 'Detection Task', 'Class I', 'Candidate Regions', 'Local Quality', 'Semi-supervised Learning', 'Faster R-CNN', 'Original Label', 'Feature Pyramid Network', 'Instances In Set', 'Ground-truth Box', 'Partial Labels', 'Identity Labels']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Current state-of-the-art object proposal networks are trained with a closed-world assumption, meaning they learn to only detect objects of the training classes. These models fail to provide high recall in open-world environments where important novel objects may be encountered. While a handful of recent works attempt to tackle this problem, they fail to consider that the optimal behavior of a proposal network can vary significantly depending on the data and application. Our goal is to provide a flexible proposal solution that can be easily tuned to suit a variety of open-world settings. To this end, we design a Tunable Hybrid Proposal Network (THPN) that leverages an adjustable hybrid architecture, a novel self-training procedure, and dynamic loss components to optimize the tradeoff between known and unknown object detection performance. To thoroughly evaluate our method, we devise several new challenges which invoke varying degrees of label bias by altering known class diversity and label count. We find that in every task, THPN easily outperforms existing baselines (e.g., RPN, OLN). Our method is also highly data efficient, surpassing baseline recall with a fraction of the labeled data."
U3DS3: Unsupervised 3D Semantic Scene Segmentation,"Jiaxu Liu, Zhengdi Yu, Toby P. Breckon, Hubert P. H. Shum","Department of Engineering, Durham University, UK; Department of Computer Science, Durham University, UK",100.0,UK,0.0,,"Contemporary point cloud segmentation approaches largely rely on richly annotated 3D training data. However, it is both time-consuming and challenging to obtain consistently accurate annotations for such 3D scene data. Moreover, there is still a lack of investigation into fully unsupervised scene segmentation for point clouds, especially for holistic 3D scenes. This paper presents U3DS3, as a step towards completely unsupervised point cloud segmentation for any holistic 3D scenes. To achieve this, U3DS3 leverages a generalized unsupervised segmentation method for both object and background across both indoor and outdoor static 3D point clouds with no requirement for model pre-training, by leveraging only the inherent information of the point cloud to achieve full 3D scene segmentation. The initial step of our proposed approach involves generating superpoints based on the geometric characteristics of each scene. Subsequently, it undergoes a learning process through a spatial clustering-based methodology, followed by iterative training using pseudo-labels generated in accordance with the cluster centroids. Moreover, by leveraging the invariance and equivariance of the volumetric representations, we apply the geometric transformation on voxelized features to provide two sets of descriptors for robust representation learning. Finally, our evaluation provides state-of-the-art results on the ScanNet and SemanticKITTI, and competitive results on the S3DIS, benchmark datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_U3DS3_Unsupervised_3D_Semantic_Scene_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_U3DS3_Unsupervised_3D_Semantic_Scene_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
UGPNet: Universal Generative Prior for Image Restoration,"Hwayoon Lee, Kyoungkook Kang, Hyeongmin Lee, Seung-Hwan Baek, Sunghyun Cho",GENGENAI; POSTECH,50.0,South Korea,50.0,USA,"Recent image restoration methods can be broadly categorized into two classes: (1) regression methods that recover the rough structure of the original image without synthesizing high-frequency details and (2) generative methods that synthesize perceptually-realistic high-frequency details even though the resulting image deviates from the original structure of the input. While both directions have been extensively studied in isolation, merging their benefits with a single framework has been rarely studied. In this paper, we propose UGPNet, a universal image restoration framework that can effectively achieve the benefits of both approaches by simply adopting a pair of an existing regression model and a generative model. UGPNet first restores the image structure of a degraded input using a regression model and synthesizes a perceptually-realistic image with a generative model on top of the regressed output. UGPNet then combines the regressed output and the synthesized output, resulting in a final result that faithfully reconstructs the structure of the original image in addition to perceptually-realistic textures. Our extensive experiments on deblurring, denoising, and super-resolution demonstrate that UGPNet can successfully exploit both regression and generative methods for high-fidelity image restoration.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_UGPNet_Universal_Generative_Prior_for_Image_Restoration_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_UGPNet_Universal_Generative_Prior_for_Image_Restoration_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483805/,"['Computer vision', 'Uncertainty', 'Computational modeling', 'Superresolution', 'Noise reduction', 'Merging', 'Measurement uncertainty']","['General Method', 'Denoising', 'Regression Method', 'Input Structure', 'Deblurring', 'Universal Framework', 'Input Image', 'Feature Maps', 'Direct Approach', 'Latent Space', 'Domain Features', 'Clear Image', 'Residual Block', 'Output Of Module', 'Ground Truth Image', 'Regression Network', 'L1 Loss', 'Inference Speed', 'Synthesis Module', 'Image X', 'Code Space', 'Latent Code', 'Regression-based Methods', 'Restoration Tasks', 'Direct Regression', 'Training Distribution', 'Regression Loss', 'Deep Neural Network', 'Residual Information', 'Neural Network']","['Algorithms', 'Low-level and physics-based vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",2,"Recent image restoration methods can be broadly categorized into two classes: (1) regression methods that recover the rough structure of the original image without synthesizing high-frequency details and (2) generative methods that synthesize perceptually-realistic high-frequency details even though the resulting image deviates from the original structure of the input. While both directions have been extensively studied in isolation, merging their benefits with a single framework has been rarely studied. In this paper, we propose UGPNet, a universal image restoration framework that can effectively achieve the benefits of both approaches by simply adopting a pair of an existing regression model and a generative model. UGPNet first restores the image structure of a degraded input using a regression model and synthesizes a perceptually-realistic image with a generative model on top of the regressed output. UGPNet then combines the regressed output and the synthesized output, resulting in a final result that faithfully reconstructs the structure of the original image in addition to perceptually-realistic textures. Our extensive experiments on deblurring, denoising, and super-resolution demonstrate that UGPNet can successfully exploit both regression and generative methods for high-fidelity image restoration."
UNSPAT: Uncertainty-Guided SpatioTemporal Transformer for 3D Human Pose and Shape Estimation on Videos,"Minsoo Lee, Hyunmin Lee, Bumsoo Kim, Seunghwan Kim",LG AI Research,0.0,,100.0,South Korea,"We propose an efficient framework for 3D human pose and shape estimation from a video, named Uncertainty-Guided SpatioTemporal Transformer (UNSPAT). Unlike previous video-based methods that consider temporal relationships with global average pooled features, our approach incorporates both spatial and temporal dimensions without compromising spatial information. We address the excessive complexity of spatiotemporal attention through two modules: Spatial Alignment Module (SAM) and Space2Batch. The modules align input features and compute temporal attention at every spatial position in a batch-wise manner. Furthermore, our uncertainty-guided attention re-weighting module improves performance by diminishing the impact of artifacts. We demonstrate the effectiveness of the UNSPAT on widely used benchmark datasets and achieve state-of-the-art performance. Our method is robust to challenging scenes, such as occlusion, and cluttered backgrounds, showing its potential for real-world applications.",https://openaccess.thecvf.com/content/WACV2024/html/Lee_UNSPAT_Uncertainty-Guided_SpatioTemporal_Transformer_for_3D_Human_Pose_and_Shape_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Lee_UNSPAT_Uncertainty-Guided_SpatioTemporal_Transformer_for_3D_Human_Pose_and_Shape_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484359/,"['Computer vision', 'Three-dimensional displays', 'Shape', 'Estimation', 'Benchmark testing', 'Transformers', 'Spatiotemporal phenomena']","['3D Shape', 'Pose Estimation', 'Shape Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', '3D Shape Estimation', 'Spatiotemporal Transformer', 'Spatial Information', 'Temporal Dimension', 'Spatial Dimensions', 'Temporal Relationship', 'Benchmark Datasets', 'Spatial Module', 'Global Average Pooling', 'Efficient Framework', '3D Pose', 'Spatial Alignment', 'Impact Of Artifacts', 'Model Parameters', 'Temporal Axis', 'Pose Parameters', 'Spatial Axis', 'Reconstruction Performance', 'Shape Parameter', '3D Mesh', 'Transformer Architecture', 'Inverse Kinematics', 'Misinformation', 'Reconstruction Accuracy']","['Algorithms', '3D computer vision', 'Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",,"We propose an efficient framework for 3D human pose and shape estimation from a video, named Uncertainty-Guided SpatioTemporal Transformer (UNSPAT). Unlike previous video-based methods that consider temporal relationships with global average pooled features, our approach incorporates both spatial and temporal dimensions without compromising spatial information. We address the excessive complexity of spatiotemporal attention through two modules: Spatial Alignment Module (SAM) and Space2Batch. The modules align input features and compute temporal attention at every spatial position in a batch-wise manner. Furthermore, our uncertainty-guided attention re-weighting module improves performance by diminishing the impact of artifacts. We demonstrate the effectiveness of the UNSPAT on widely used benchmark datasets and achieve state-of-the-art performance. Our method is robust to challenging scenes, such as occlusion, and cluttered backgrounds, showing its potential for real-world applications."
UOW-Vessel: A Benchmark Dataset of High-Resolution Optical Satellite Images for Vessel Detection and Segmentation,"Ly Bui, Son Lam Phung, Yang Di, Thanh Le, Tran Thanh Phong Nguyen, Sandy Burden, Abdesselam Bouzerdoum","University of Wollongong, Hamad Bin Khalifa University; University of Wollongong",100.0,Australia,0.0,,"In this paper, we introduce UOW-Vessel, a benchmark dataset of high-resolution optical satellite images for vessel detection and segmentation. Our dataset consists of 3,500 images, collected from 14 countries across 4 continents. With a total of 35,598 instances in 10 vessel categories, UOW-Vessel is to date the largest satellite image dataset for vessel recognition. Furthermore, compared to the existing public datasets that only provide bounding box ground-truth, our new dataset offers more accurate polygon annotations of vessel objects. This dataset is expected to support instance segmentation-based approaches, which is a less investigated area in vessel surveillance. We also report extensive evaluations of the recent algorithms for instance segmentation on the new benchmark dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Bui_UOW-Vessel_A_Benchmark_Dataset_of_High-Resolution_Optical_Satellite_Images_for_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bui_UOW-Vessel_A_Benchmark_Dataset_of_High-Resolution_Optical_Satellite_Images_for_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484385/,"['Instance segmentation', 'Image recognition', 'Annotations', 'Target recognition', 'Surveillance', 'Semantic segmentation', 'Object detection']","['Optical Tomography', 'High-resolution Images', 'Satellite Images', 'Benchmark Datasets', 'High-resolution Satellite', 'Vessel Segmentation', 'High-resolution Satellite Images', 'High-resolution Optical Images', 'High-resolution Optical Satellite Images', 'Bounding Box', 'Largest Dataset', 'Instance Segmentation', 'Object Annotations', 'Model Performance', 'Performance Of Method', 'Image Size', 'Object Detection', 'Large-scale Datasets', 'Stochastic Gradient Descent', 'Annotation Data', 'You Only Look Once', 'Mask R-CNN', 'Two-stage Method', 'Predicted Bounding Box', 'Intersection Over Union Threshold', 'Instance Segmentation Methods', 'Mean Average Precision', 'Frames Per Second', 'Arbitrary Orientation', 'Semantic Segmentation']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",2,"In this paper, we introduce UOW-Vessel, a benchmark dataset of high-resolution optical satellite images for vessel detection and segmentation. Our dataset consists of 3,500 images, collected from 14 countries across 4 continents. With a total of 35,598 instances in 10 vessel categories, UOW-Vessel is to date the largest satellite image dataset for vessel recognition. Furthermore, compared to the existing public datasets that only provide bounding box ground-truth, our new dataset offers more accurate polygon annotations of vessel objects. This dataset is expected to support instance segmentation-based approaches, which is a less investigated area in vessel surveillance. We also report extensive evaluations of the recent algorithms for instance segmentation on the new benchmark dataset."
USDN: A Unified Sample-Wise Dynamic Network With Mixed-Precision and Early-Exit,"Ji-Ye Jeon, Xuan Truong Nguyen, Soojung Ryu, Hyuk-Jae Lee",Seoul National University; SK Telecom,50.0,South Korea,50.0,South Korea,"To reduce computation in deep neural network inference, a promising approach is to design a network with multiple internal classifiers (ICs) and adaptively select an execution path based on the complexity of a given input. However, quantizing an input-adaptive network, a must-do task for network deployment on edge devices, is a non-trivial task due to jointly allocating its computation budget along with network layers and IC locations. In this paper, we propose Unified Sample-wise Dynamic Network (USDN) with a mixed-precision and early-exit framework that obtains both the optimal location of ICs and layer-wise bit configurations under a given computation budget. The proposed USDN comprises multiple groups of layers, with each group representing a varying degree of complexity for input samples. Experimental results demonstrate that our approach reduces computational cost of the previous work by 12.78% while achieving higher accuracy on ImageNet dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jeon_USDN_A_Unified_Sample-Wise_Dynamic_Network_With_Mixed-Precision_and_Early-Exit_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483727/,"['Degradation', 'Computer vision', 'Quantization (signal)', 'Costs', 'Computational modeling', 'Artificial neural networks', 'Complexity theory']","['Dynamic Network', 'Unique Network', 'Neural Network', 'Computational Cost', 'Deep Neural Network', 'ImageNet Dataset', 'Edge Devices', 'Neural Network Inference', 'Execution Path', 'Objective Function', 'Training Dataset', 'Validation Set', 'Feature Maps', 'Quantification Method', 'Validation Dataset', 'Design Space', 'Computational Overhead', 'Reward Function', 'Residual Connection', 'Feature Map Size', 'Uniform Quantization', 'Inference Cost', 'Exit Rate', 'Architecture Parameters', 'Exit Point', 'Multinomial Distribution', 'Distillation Loss']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Embedded sensing / real-time techniques']",1,"To reduce computation in deep neural network inference, a promising approach is to design a network with multiple internal classifiers (ICs) and adaptively select an execution path based on the complexity of a given input. However, quantizing an input-adaptive network, a must-do task for network deployment on edge devices, is a non-trivial task due to jointly allocating its computation budget along with network layers and IC locations. In this paper, we propose Unified Sample-wise Dynamic Network (USDN) with a mixed-precision and early-exit framework that obtains both the optimal location of ICs and layer-wise bit configurations under a given computation budget. The proposed USDN comprises multiple groups of layers, with each group representing a varying degree of complexity for input samples. Experimental results demonstrate that our approach reduces computational cost of the previous work by 12.78% while achieving higher accuracy on ImageNet dataset."
Uncertainty Estimation in Instance Segmentation With Star-Convex Shapes,"Qasim M. K. Siddiqui, Sebastian Starke, Peter Steinbach","Hemholtz-Zentrum Dresden-Rossendorf, Dresden, Germany",0.0,,100.0,Germany,"Instance segmentation has witnessed promising advancements through deep neural network-based algorithms. However, these models often exhibit incorrect predictions with unwarranted confidence levels. Consequently, evaluating prediction uncertainty becomes critical for informed decision-making. Existing methods primarily focus on quantifying uncertainty in classification or regression tasks, lacking emphasis on instance segmentation. Our research addresses the challenge of estimating spatial certainty associated with the location of instances with star-convex shapes. Two distinct clustering approaches are evaluated which compute spatial and fractional certainty per instance employing samples by the Monte-Carlo Dropout or Deep Ensemble technique. Our study demonstrates that combining spatial and fractional certainty scores yields improved calibrated estimation over individual certainty scores. Notably, our experimental results show that the Deep Ensemble technique alongside our novel radial clustering approach proves to be an effective strategy. Our findings emphasize the significance of evaluating the calibration of estimated certainties for model reliability and decision-making.",https://openaccess.thecvf.com/content/WACV2024/html/Siddiqui_Uncertainty_Estimation_in_Instance_Segmentation_With_Star-Convex_Shapes_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Siddiqui_Uncertainty_Estimation_in_Instance_Segmentation_With_Star-Convex_Shapes_WACV_2024_paper.pdf,,,2309.10513,main,Poster,https://ieeexplore.ieee.org/document/10483748/,"['Instance segmentation', 'Uncertainty', 'Monte Carlo methods', 'Shape', 'Decision making', 'Estimation', 'Predictive models']","['Uncertainty Estimation', 'Instance Segmentation', 'Classification Task', 'Incorrect Predictions', 'Ensemble Technique', 'Classification Uncertainty', 'Dropout Technique', 'Neural Network', 'Deep Neural Network', 'Convolutional Layers', 'Dropout Rate', 'Clustering Algorithm', 'Effective Technique', 'Object Detection', 'Radial Distance', 'Prediction Set', 'Specific Instances', 'Forward Pass', 'Non-maximum Suppression', 'Spatial Similarity', 'Objective Probability', 'Epistemic Uncertainty', 'Bayesian Neural Network', 'Median Prediction', 'Aleatoric Uncertainty', 'Entire Training Dataset', 'Single Shot Detector', 'Training Data', 'Dropout Layer', 'Absolute Difference']","['Algorithms', 'Image recognition and understanding', 'Applications', 'Biomedical / healthcare / medicine', 'Applications', 'Visualization']",,"Instance segmentation has witnessed promising advancements through deep neural network-based algorithms. However, these models often exhibit incorrect predictions with unwarranted confidence levels. Consequently, evaluating prediction uncertainty becomes critical for informed decision-making. Existing methods primarily focus on quantifying uncertainty in classification or regression tasks, lacking emphasis on instance segmentation. Our research addresses the challenge of estimating spatial certainty associated with the location of instances with star-convex shapes. Two distinct clustering approaches are evaluated which compute spatial and fractional certainty per instance employing samples by the Monte-Carlo Dropout or Deep Ensemble technique. Our study demonstrates that combining spatial and fractional certainty scores yields improved calibrated estimation over individual certainty scores. Notably, our experimental results show that the Deep Ensemble technique alongside our novel radial clustering approach proves to be an effective strategy. Our findings emphasize the significance of evaluating the calibration of estimated certainties for model reliability and decision-making."
Uncertainty-Weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation,"Kira Maag, Asja Fischer","Ruhr University Bochum, Germany; Technical University of Berlin, Germany",100.0,Germany,0.0,,"State-of-the-art deep neural networks have been shown to be extremely powerful in a variety of perceptual tasks like semantic segmentation. However, these networks are vulnerable to adversarial perturbations of the input which are imperceptible for humans but lead to incorrect predictions. Treating image segmentation as a sum of pixel-wise classifications, adversarial attacks developed for classification models were shown to be applicable to segmentation models as well. In this work, we present simple uncertainty-based weighting schemes for the loss functions of such attacks that (i) put higher weights on pixel classifications which can more easily perturbed and (ii) zero-out the pixel-wise losses corresponding to those pixels that are already confidently misclassified. The weighting schemes can be easily integrated into the loss function of a range of well-known adversarial attackers with minimal additional computational overhead, but lead to significant improved perturbation performance, as we demonstrate in our empirical analysis on several datasets and models.",https://openaccess.thecvf.com/content/WACV2024/html/Maag_Uncertainty-Weighted_Loss_Functions_for_Improved_Adversarial_Attacks_on_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Maag_Uncertainty-Weighted_Loss_Functions_for_Improved_Adversarial_Attacks_on_Semantic_Segmentation_WACV_2024_paper.pdf,,,2310.17436,main,Poster,https://ieeexplore.ieee.org/document/10483959/,"['Computer vision', 'Analytical models', 'Semantic segmentation', 'Perturbation methods', 'Computational modeling', 'Artificial neural networks', 'Task analysis']","['Loss Function', 'Semantic Segmentation', 'Adversarial Attacks', 'Neural Network', 'Deep Neural Network', 'Segmentation Model', 'Computational Overhead', 'Pixel Classification', 'Weighting Scheme', 'Minimal Overhead', 'Adversarial Perturbations', 'Prediction Error', 'Image Pixels', 'Measurement Uncertainty', 'Types Of Attacks', 'Validation Images', 'White Box', 'Image X', 'Semantic Segmentation Task', 'Semantic Segmentation Models', 'Adversarial Examples', 'Attack Performance', 'Perturbation Magnitude', 'Different Types Of Attacks', 'Subset Of Pixels', 'Fast Gradient Sign Method', 'Range Of Transformations', 'Ground-truth Class', 'Low Runtime', 'Erroneous Predictions']","['Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Applications', 'Autonomous Driving']",,"State-of-the-art deep neural networks have been shown to be extremely powerful in a variety of perceptual tasks like semantic segmentation. However, these networks are vulnerable to adversarial perturbations of the input which are imperceptible for humans but lead to incorrect predictions. Treating image segmentation as a sum of pixel-wise classifications, adversarial attacks developed for classification models were shown to be applicable to segmentation models as well. In this work, we present simple uncertainty-based weighting schemes for the loss functions of such attacks that (i) put higher weights on pixel classifications which can more easily perturbed and (ii) zero-out the pixel-wise losses corresponding to those pixels that are already confidently misclassified. The weighting schemes can be easily integrated into the loss function of a range of well-known adversarial attackers with minimal additional computational overhead, but lead to significant improved perturbation performance, as we demonstrate in our empirical analysis on several datasets and models."
Understanding Dark Scenes by Contrasting Multi-Modal Observations,"Xiaoyu Dong, Naoto Yokoya","The University of Tokyo, RIKEN AIP",100.0,Japan,0.0,,"Understanding dark scenes based on multi-modal image data is challenging, as both the visible and auxiliary modalities provide limited semantic information for the task. Previous methods focus on fusing the two modalities but neglect the correlations among semantic classes when minimizing losses to align pixels with labels, resulting in inaccurate class predictions. To address these issues, we introduce a supervised multi-modal contrastive learning approach to increase the semantic discriminability of the learned multi-modal feature spaces by jointly performing cross-modal and intra-modal contrast under the supervision of the class correlations. The cross-modal contrast encourages same-class embeddings from across the two modalities to be closer and pushes different-class ones apart. The intra-modal contrast forces same-class or different-class embeddings within each modality to be together or apart. We validate our approach on a variety of tasks that cover diverse light conditions and image modalities. Experiments show that our approach can effectively enhance dark scene understanding based on multi-modal images with limited semantics by shaping semantic-discriminative feature spaces. Comparisons with previous methods demonstrate our state-of-the-art performance. Code and pretrained models are available at https://github.com/palmdong/SMMCL.",https://openaccess.thecvf.com/content/WACV2024/html/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Dong_Understanding_Dark_Scenes_by_Contrasting_Multi-Modal_Observations_WACV_2024_paper.pdf,,https://github.com/palmdong/SMMCL,2308.12320,main,Poster,https://ieeexplore.ieee.org/document/10483702/,"['Computer vision', 'Correlation', 'Codes', 'Semantic segmentation', 'Semantics', 'Self-supervised learning', 'Task analysis']","['Feature Space', 'Multimodal Imaging', 'Multimodal Approach', 'Self-supervised Learning', 'Scene Understanding', 'Multimodal Learning', 'Multimodal Imaging Data', 'Convolution', 'Positive Samples', 'Negative Samples', 'Image Segmentation', 'Segmentation Method', 'Semantic Segmentation', 'Feature Fusion', 'Segmentation Task', 'Contextual Cues', 'Dark Environment', 'Contrastive Loss', 'Low Discrimination', 'Semantic Context', 'Unsupervised Domain Adaptation Methods', 'Semantic Segmentation Methods', 'Scene Segmentation', 'Thermal Mode', 'Outdoor Scenes', 'Poor Visibility', 'Multimodal Methods', 'Ablation', 'Projector']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",3,"Understanding dark scenes based on multi-modal image data is challenging, as both the visible and auxiliary modalities provide limited semantic information for the task. Previous methods focus on fusing the two modalities but neglect the correlations among semantic classes when minimizing losses to align pixels with labels, resulting in inaccurate class predictions. To address these issues, we introduce a supervised multi-modal contrastive learning approach to increase the semantic discriminability of the learned multi-modal feature spaces by jointly performing cross-modal and intra-modal contrast under the supervision of the class correlations. The cross-modal contrast encourages same-class embeddings from across the two modalities to be closer and pushes different-class ones apart. The intra-modal contrast forces same-class or different-class embeddings within each modality to be together or apart. We validate our approach on a variety of tasks that cover diverse light conditions and image modalities. Experiments show that our approach can effectively enhance dark scene understanding based on multi-modal images with limited semantics by shaping semantic-discriminative feature spaces. Comparisons with previous methods demonstrate our state-of-the-art performance. Code and pretrained models are available at https://github.com/palmdong/SMMCL."
Understanding Hyperbolic Metric Learning Through Hard Negative Sampling,"Yun Yue, Fangzhou Lin, Guanyi Mou, Ziming Zhang","Worcester Polytechnic Institute, USA",100.0,USA,0.0,,"In recent years, there has been a growing trend of incorporating hyperbolic geometry methods into computer vision. While these methods have achieved state-of-the-art performance on various metric learning tasks using hyperbolic distance measurements, the underlying theoretical analysis supporting this superior performance remains under-exploited. In this study, we investigate the effects of integrating hyperbolic space into metric learning, particularly when training with contrastive loss. We identify a need for a comprehensive comparison between Euclidean and hyperbolic spaces regarding the temperature effect in the contrastive loss within the existing literature. To address this gap, we conduct an extensive investigation to benchmark the results of Vision Transformers (ViTs) using a hybrid objective function that combines loss from Euclidean and hyperbolic spaces. Additionally, we provide a theoretical analysis of the observed performance improvement. We also reveal that hyperbolic metric learning is highly related to hard negative sampling, providing insights for future work. This work will provide valuable data points and experience in understanding hyperbolic image embeddings. To shed more light on problem-solving and encourage further investigation into our approach, our code is available online.",https://openaccess.thecvf.com/content/WACV2024/html/Yue_Understanding_Hyperbolic_Metric_Learning_Through_Hard_Negative_Sampling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yue_Understanding_Hyperbolic_Metric_Learning_Through_Hard_Negative_Sampling_WACV_2024_paper.pdf,,https://github.com/YunYunY/HypMix,,main,Poster,https://ieeexplore.ieee.org/document/10483696/,"['Measurement', 'Geometry', 'Training', 'Temperature measurement', 'Computer vision', 'Extraterrestrial measurements', 'Transformers']","['Negative Samples', 'Metric Learning', 'Hard Negative Samples', 'Distancing Measures', 'Computer Vision', 'Euclidean Space', 'Contrastive Loss', 'Vision Transformer', 'Hyperbolic Geometry', 'Hyperbolic Space', 'Exponential Growth', 'Stochastic Gradient Descent', 'Representation Learning', 'Latent Space', 'Temperature Parameters', 'Decision Boundary', 'Curve Parameters', 'Image Retrieval', 'Self-supervised Learning', 'Computer Graphics', 'Euclidean Geometry', 'Hypersphere', 'Loss Of Space', 'Distinct Geometries', 'Gradient Analysis', 'Linear Projection']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",1,"In recent years, there has been a growing trend of incorporating hyperbolic geometry methods into computer vision. While these methods have achieved state-of-the-art performance on various metric learning tasks using hyperbolic distance measurements, the underlying theoretical analysis supporting this superior performance remains underexploited. In this study, we investigate the effects of integrating hyperbolic space into metric learning, particularly when training with contrastive loss. We identify a need for a comprehensive comparison between Euclidean and hyperbolic spaces regarding the temperature effect in the contrastive loss within the existing literature. To address this gap, we conduct an extensive investigation to benchmark the results of Vision Transformers (ViTs) using a hybrid objective function that combines loss from Euclidean and hyperbolic spaces. Additionally, we provide a theoretical analysis of the observed performance improvement. We also reveal that hyperbolic metric learning is highly related to hard negative sampling, providing insights for future work. This work will provide valuable data points and experience in understanding hyperbolic image embeddings. To shed more light on problem-solving and encourage further investigation into our approach, our code 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
 is available online."
Unified Concept Editing in Diffusion Models,"Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzyńska, David Bau",Technion; Northeastern University; Massachusetts Institute of Technology,100.0,"Israel, USA",0.0,,"Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work.",https://openaccess.thecvf.com/content/WACV2024/html/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gandikota_Unified_Concept_Editing_in_Diffusion_Models_WACV_2024_paper.pdf,,https://uniﬁed.baulab.info,2308.14761,main,Poster,https://ieeexplore.ieee.org/document/10484056/,"['Training', 'Ethics', 'Computer vision', 'Codes', 'Closed-form solutions', 'Scalability', 'Computational modeling']","['Diffusion Model', 'Content Moderation', 'Profession', 'Training Data', 'Objective Function', 'Social Bias', 'Visual Features', 'Cognitive Model', 'Image Generation', 'Data Curation', 'Projection Matrix', 'Language Model', 'Gender Ratio', 'Inference Time', 'Prior Methods', 'Target Output', 'Linear Projection', 'Multiple Concepts']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', 'Vision + language and/or other modalities']",14,"Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models.We present scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and perform extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info."
"United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning From Videos","Siddhant Bansal, Chetan Arora, C. V. Jawahar","CVIT, IIIT, Hyderabad; IIT, Delhi",100.0,India,0.0,,"Given multiple videos of the same task, procedure learning addresses identifying the key-steps and determining their order to perform the task. For this purpose, existing approaches use the signal generated from a pair of videos. This makes key-steps discovery challenging as the algorithms lack inter-videos perspective. Instead, we propose an unsupervised Graph-based Procedure Learning (GPL) framework. GPL consists of the novel UnityGraph that represents all the videos of a task as a graph to obtain both intra-video and inter-videos context. Further, to obtain similar embeddings for the same key-steps, the embeddings of UnityGraph are updated in an unsupervised manner using the Node2Vec algorithm. Finally, to identify the key-steps, we cluster the embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and EgoProceL datasets and achieve an average improvement of 2% on third-person datasets and 3.6% on EgoProceL over the state-of-the-art.",https://openaccess.thecvf.com/content/WACV2024/html/Bansal_United_We_Stand_Divided_We_Fall_UnityGraph_for_Unsupervised_Procedure_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bansal_United_We_Stand_Divided_We_Fall_UnityGraph_for_Unsupervised_Procedure_WACV_2024_paper.pdf,,,2311.03550,main,Poster,https://ieeexplore.ieee.org/document/10484201/,"['Computer vision', 'Computational modeling', 'Clustering algorithms', 'Benchmark testing', 'Task analysis', 'Videos']","['Learning Procedure', 'Unsupervised Manner', 'Multiple Video', 'Window Size', 'Time Complexity', 'Temporal Relationship', 'Video Clips', 'Representation Learning', 'Semantic Similarity', 'Video For Instructions', 'Embedding Learning', 'Number Of Videos', 'Single Video', 'Pre-trained ResNet-50', 'Walk Length', 'Hours Of Video']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Given multiple videos of the same task, procedure learning addresses identifying the key-steps and determining their order to perform the task. For this purpose, existing approaches use the signal generated from a pair of videos. This makes key-steps discovery challenging as the algorithms lack inter-videos perspective. Instead, we propose an unsupervised Graph-based Procedure Learning (GPL) framework. GPL consists of the novel UnityGraph that represents all the videos of a task as a graph to obtain both intra-video and inter-videos context. Further, to obtain similar embeddings for the same key-steps, the embeddings of UnityGraph are updated in an unsupervised manner using the Node2Vec algorithm. Finally, to identify the key-steps, we cluster the embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and EgoProceL datasets and achieve an average improvement of 2% on third-person datasets and 3.6% on EgoProceL over the state-of-the-art."
Universal Semi-Supervised Model Adaptation via Collaborative Consistency Training,"Zizheng Yan, Yushuang Wu, Yipeng Qin, Xiaoguang Han, Shuguang Cui, Guanbin Li","FNii, CUHKSZ; SSE, CUHKSZ; Sun Yat-sen University; Research Institute, Sun Yat-sen University, Shenzhen; Cardiff University",100.0,"China, Hong Kong, UK",0.0,,"In this paper, we introduce a realistic and challenging domain adaptation problem called Universal Semi-supervised Model Adaptation (USMA), which i) requires only a pre-trained source model, ii) allows the source and target domain to have different label sets, i.e., they share a common label set and hold their own private label set, and iii) requires only a few labeled samples in each class of the target domain. To address USMA, we propose a collaborative consistency training framework that regularizes the prediction consistency between two models, i.e., a pre-trained source model and its variant pre-trained with target data only, and combines their complementary strengths to learn a more powerful model. The rationale of our framework stems from the observation that the source model performs better on common categories than the target-only model, while on target-private categories, the target-only model performs better. We also propose a two-perspective, i.e., sample-wise and class-wise, consistency regularization to improve the training. Experimental results demonstrate the effectiveness of our method on several benchmark datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Yan_Universal_Semi-Supervised_Model_Adaptation_via_Collaborative_Consistency_Training_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yan_Universal_Semi-Supervised_Model_Adaptation_via_Collaborative_Consistency_Training_WACV_2024_paper.pdf,,,2307.03449,main,Poster,https://ieeexplore.ieee.org/document/10483992/,"['Training', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Collaboration', 'Predictive models', 'Semisupervised learning']","['Collaborative Training', 'Classification Of Samples', 'Benchmark Datasets', 'Labeled Samples', 'Target Domain', 'Target Data', 'Source Model', 'Domain Adaptation', 'Source Domain', 'Training Framework', 'Collaborative Framework', 'Real Domain', 'Consistency Regularization', 'Loss Function', 'Artificial Neural Network', 'Loss Of Generality', 'Mutual Information', 'Semi-supervised Learning', 'Consistency Loss', 'Minimum Entropy', 'Pseudo Labels', 'Cross-correlation Matrix', 'Domain Alignment', 'Domain Adaptation Methods', 'Unsupervised Domain Adaptation Methods', 'Domain Gap']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In this paper, we introduce a realistic and challenging domain adaptation problem called Universal Semi-supervised Model Adaptation (USMA), which i) requires only a pre-trained source model, ii) allows the source and target domain to have different label sets, i.e., they share a common label set and hold their own private label set, and iii) requires only a few labeled samples in each class of the target domain. To address USMA, we propose a collaborative consistency training framework that regularizes the prediction consistency between two models, i.e., a pre-trained source model and its variant pre-trained with target data only, and combines their complementary strengths to learn a more powerful model. The rationale of our framework stems from the observation that the source model performs better on common categories than the target-only model, while on target-private categories, the target-only model performs better. We also propose a two-perspective, i.e., sample-wise and class-wise, consistency regularization to improve the training. Experimental results demonstrate the effectiveness of our method on several benchmark datasets."
"Universal Test-Time Adaptation Through Weight Ensembling, Diversity Weighting, and Prior Correction","Robert A. Marsden, Mario Döbler, Bin Yang",University of Stuttgart,100.0,Germany,0.0,,"Since distribution shifts are likely to occur during test-time and can drastically decrease the model's performance, online test-time adaptation (TTA) continues to update the model after deployment, leveraging the current test data. Clearly, a method proposed for online TTA has to perform well for all kinds of environmental conditions. By introducing the variable factors domain non-stationarity and temporal correlation, we first unfold all practically relevant settings and define the entity as universal TTA. We want to highlight that this is the first work that covers such a broad spectrum, which is indispensable for the use in practice. To tackle the problem of universal TTA, we identify and highlight several challenges a self-training based method has to deal with: 1) model bias and the occurrence of trivial solutions when performing entropy minimization on varying sequence lengths with and without multiple domain shifts, 2) loss of generalization which exacerbates the adaptation to multiple domain shifts and the occurrence of catastrophic forgetting, and 3) performance degradation due to shifts in class prior. To prevent the model from becoming biased, we leverage a dataset and model-agnostic certainty and diversity weighting. In order to maintain generalization and prevent catastrophic forgetting, we propose to continually weight-average the source and adapted model. To compensate for disparities in the class prior during test-time, we propose an adaptive prior correction scheme that reweights the model's predictions. We evaluate our approach, named ROID, on a wide range of settings, datasets, and models, setting new standards in the field of universal TTA. Code is available at: https://github.com/mariodoebler/test-time-adaptation",https://openaccess.thecvf.com/content/WACV2024/html/Marsden_Universal_Test-Time_Adaptation_Through_Weight_Ensembling_Diversity_Weighting_and_Prior_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Marsden_Universal_Test-Time_Adaptation_Through_Weight_Ensembling_Diversity_Weighting_and_Prior_WACV_2024_paper.pdf,,https://github.com/mariodoebler/test-time-adaptation,,main,Poster,https://ieeexplore.ieee.org/document/10483953/,"['Degradation', 'Adaptation models', 'Computer vision', 'Correlation', 'Stability criteria', 'Predictive models', 'Minimization']","['Test-time Adaptation', 'Test Data', 'Loss Of Generality', 'Performance Degradation', 'Domain Shift', 'Temporal Correlation', 'Adaptive Scheme', 'Standard Field', 'Wide Range Of Settings', 'Trivial Solution', 'Multiple Shifts', 'Catastrophic Forgetting', 'Test Samples', 'Time Step', 'Corruption', 'Transformer', 'Deep Neural Network', 'Cross-entropy', 'Single Domain', 'Current Distribution', 'Self-supervised Learning', 'Vision Transformer', 'Target Domain', 'Good Adaptability', 'Source Distribution', 'ImageNet', 'Batch Normalization', 'Group Normalization', 'Momentum Term', 'Class Distribution']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Since distribution shifts are likely to occur during testtime and can drastically decrease the model’s performance, online test-time adaptation (TTA) continues to update the model after deployment, leveraging the current test data. Clearly, a method proposed for online TTA has to perform well for all kinds of environmental conditions. By introducing the variable factors domain non-stationarity and temporal correlation, we first unfold all practically relevant settings and define the entity as universal TTA. We want to highlight that this is the first work that covers such a broad spectrum, which is indispensable for the use in practice. To tackle the problem of universal TTA, we identify and highlight several challenges a self-training based method has to deal with: 1) model bias and the occurrence of trivial solutions when performing entropy minimization on varying sequence lengths with and without multiple domain shifts, 2) loss of generalization which exacerbates the adaptation to multiple domain shifts and the occurrence of catastrophic forgetting, and 3) performance degradation due to shifts in class prior. To prevent the model from becoming biased, we leverage a dataset and model-agnostic certainty and diversity weighting. In order to maintain generalization and prevent catastrophic forgetting, we propose to continually weightaverage the source and adapted model. To compensate for disparities in the class prior during test-time, we propose an adaptive prior correction scheme that reweights the model’s predictions. We evaluate our approach, named ROID, on a wide range of settings, datasets, and models, setting new standards in the field of universal TTA. Code is available at: https://github.com/mariodoebler/testtime-adaptation"
Unsupervised 3D Pose Estimation With Non-Rigid Structure-From-Motion Modeling,"Haorui Ji, Hui Deng, Yuchao Dai, Hongdong Li","School of Electronics and Information, Northwestern Polytechnical University; The Australian National University",100.0,"Australia, China",0.0,,"Most existing 3D human pose estimation work rely heavily on the powerful memory capability of networks to obtain suitable 2D-3D mappings from the training data. Few works have studied the modeling of human posture deformation in motion. In this paper, we propose a new modeling method for human pose deformations and design an accompanying diffusion-based motion prior. Inspired by the field of non-rigid structure-from-motion, we divide the task of reconstructing 3D human skeletons in motion into the estimation of a 3D reference skeleton, and a frame-by-frame skeleton deformation. A mixed spatial-temporal NRSfMformer is used to simultaneously estimate the 3D reference skeleton and the skeleton deformation of each frame from 2D observations sequence, and then sum them up to obtain the pose of each frame. Subsequently, a loss term based on the diffusion model is used to ensure that the pipeline learns the correct prior motion knowledge. Finally, we have evaluated our proposed method on mainstream datasets and obtained superior results outperforming the state-of-the-art.",https://openaccess.thecvf.com/content/WACV2024/html/Ji_Unsupervised_3D_Pose_Estimation_With_Non-Rigid_Structure-From-Motion_Modeling_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ji_Unsupervised_3D_Pose_Estimation_With_Non-Rigid_Structure-From-Motion_Modeling_WACV_2024_paper.pdf,,,2308.10705,main,Poster,,,,,,
Unsupervised Co-Generation of Foreground-Background Segmentation From Text-to-Image Synthesis,"Yeruru Asrar Ahmed, Anurag Mittal","Department of Computer Science and Engineering, Indian Institute of Technology Madras",100.0,India,0.0,,"Text-to-Image (T2I) synthesis is a challenging task requiring modelling both textual and image domains and their relationship. The substantial improvement in image quality achieved by recent works has paved the way for numerous applications such as language-aided image editing, computer-aided design, text-based image retrieval, and training data augmentation. In this work, we ask a simple question: Along with realistic images, can we obtain any useful by-product (e.g., foreground / background or multi-class segmentation masks, detection labels) in an unsupervised way that will also benefit other computer vision tasks and applications?. In an attempt to answer this question, we explore generating realistic images and their corresponding foreground / background segmentation masks from the given text. To achieve this, we experiment the concept of co-segmentation along with GAN. Specifically, a novel GAN architecture called Co-Segmentation Inspired GAN (COS-GAN) is proposed that generates two or more images simultaneously from different noise vectors and utilises a spatial co-attention mechanism between the image features to produce realistic segmentation masks for each of the generated images. The advantages of such an architecture are two-fold: 1) The generated segmentation masks can be used to focus on foreground and background exclusively to improve the quality of generated images, and 2) the segmentation masks can be used as a training target for other tasks, such as object localisation and segmentation. Extensive experiments conducted on CUB, Oxford-102, and COCO datasets show that COS-GAN is able to improve visual quality and generate reliable foreground / background masks for the generated images.",https://openaccess.thecvf.com/content/WACV2024/html/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ahmed_Unsupervised_Co-Generation_of_Foreground-Background_Segmentation_From_Text-to-Image_Synthesis_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Unsupervised Domain Adaptation for Semantic Segmentation With Pseudo Label Self-Refinement,"Xingchen Zhao, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-Pang Chiu, Supun Samarasekera","Northeastern University, Boston, MA, USA; SRI International, Princeton, NJ, USA",100.0,USA,0.0,,"Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the quality of pseudo labels and select highly reliable ones, PRN helps self-training of segmentation models to be robust against pseudo label noise propagation during different stages of adaptation. We evaluate our approach on benchmark datasets with three different domain shifts, and our approach consistently performs significantly better than the previous state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2024/html/Zhao_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_With_Pseudo_Label_Self-Refinement_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_With_Pseudo_Label_Self-Refinement_WACV_2024_paper.pdf,,,2310.16979,main,Poster,https://ieeexplore.ieee.org/document/10483680/,"['Training', 'Adaptation models', 'Semantic segmentation', 'Refining', 'Noise', 'Self-supervised learning', 'Benchmark testing']","['Semantic Segmentation', 'Domain Adaptation', 'Pseudo Labels', 'Teacher Model', 'Student Model', 'Label Noise', 'Refinement Network', 'Data Sources', 'Image Pixels', 'Cross-entropy Loss', 'Target Image', 'Source Images', 'Ground Truth Labels', 'Target Domain', 'Target Data', 'Source Domain', 'Self-supervised Learning', 'Part Of Table', 'Student Network', 'Domain Gap', 'Masking Noise', 'Noisy Labels', 'Target Domain Data', 'Absolute Improvement', 'Source Labels', 'Target Domain Images', 'Qualitative Examples', 'Semantic Segmentation Models', 'Unlabeled Data']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",5,"Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the quality of pseudo labels and select highly reliable ones, PRN helps self-training of segmentation models to be robust against pseudo label noise propagation during different stages of adaptation. We evaluate our approach on benchmark datasets with three different domain shifts, and our approach consistently performs significantly better than the previous state-of-the-art methods."
Unsupervised Domain Adaptation of MRI Skull-Stripping Trained on Adult Data to Newborns,"Abbas Omidi, Aida Mohammadshahi, Neha Gianchandani, Regan King, Lara Leijser, Roberto Souza","University of Calgary, Calgary, Alberta, Canada",100.0,Canada,0.0,,"Skull-stripping is an important first step when analyzing brain Magnetic Resonance Imaging (MRI) data. Deep learning-based supervised segmentation models, such as the U-net model, have shown promising results in automating this segmentation task. However, when it comes to newborn MRI data, there are no publicly available brain MRI datasets that come with manually annotated segmentation masks to be used as labels during the training of these models. Manual segmentation of brain MR images is time-consuming, labor-intensive, and requires expertise. Furthermore, using a segmentation model trained on adult brain MR images for segmenting newborn brain images is not effective due to a large domain shift between adult and newborn data. As a result, there is a need for more efficient and accurate skull-stripping methods for newborns' brain MRIs. In this paper, we present an unsupervised approach to adapt a U-net skull-stripping model trained on adult MRI to work effectively on newborns. Our results demonstrate the effectiveness of our novel unsupervised approach in enhancing segmentation accuracy. Our proposed method achieved an overall Dice coefficient of 0.916 +- 0.032 (mean +- std), and our ablation studies confirmed the effectiveness of our proposal. Remarkably, despite being unsupervised, our model's performance stands in close proximity to that of the current state-of-the-art supervised models against which we conducted our comparisons. These findings indicate the potential of this method as a valuable, easier, and faster tool for supporting healthcare professionals in the examination of MR images of newborn brains. All the codes are available at: https://github.com/abbasomidi77/DAUnet.",https://openaccess.thecvf.com/content/WACV2024/html/Omidi_Unsupervised_Domain_Adaptation_of_MRI_Skull-Stripping_Trained_on_Adult_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Omidi_Unsupervised_Domain_Adaptation_of_MRI_Skull-Stripping_Trained_on_Adult_Data_WACV_2024_paper.pdf,,https://github.com/abbasomidi77/DAUnet,,main,Poster,https://ieeexplore.ieee.org/document/10484289/,"['Training', 'Image segmentation', 'Pediatrics', 'Adaptation models', 'Magnetic resonance imaging', 'Brain modeling', 'Data models']","['Magnetic Resonance Imaging', 'Domain Adaptation', 'Image Segmentation', 'Unsupervised Learning', 'Brain Magnetic Resonance Imaging', 'Adult Brain', 'Domain Shift', 'Segmentation Model', 'Magnetic Resonance Imaging Data', 'Segmentation Accuracy', 'Segmentation Task', 'Dice Similarity Coefficient', 'U-Net Model', 'Newborn Brain', 'Convolutional Neural Network', 'Image Resolution', 'Gray Matter', 'Deep Learning Models', 'T1-weighted Images', 'Target Image', 'Source Domain', 'Domain Adaptation Techniques', 'Source Images', 'Dice Loss', 'Target Domain', 'Domain-invariant Features', 'Hausdorff Distance', 'Sum In Equation', 'Discriminator Loss', 'Binary Cross-entropy Loss']","['Applications', 'Biomedical / healthcare / medicine', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods', 'Algorithms', 'Image recognition and understanding']",7,"Skull-stripping is an important first step when analyzing brain Magnetic Resonance Imaging (MRI) data. Deep learning-based supervised segmentation models, such as the U-net model, have shown promising results in automating this segmentation task. However, when it comes to newborn MRI data, there are no publicly available brain MRI datasets that come with manually annotated segmentation masks to be used as labels during the training of these models. Manual segmentation of brain MR images is time-consuming, labor-intensive, and requires expertise. Furthermore, using a segmentation model trained on adult brain MR images for segmenting newborn brain images is not effective due to a large domain shift between adult and newborn data. As a result, there is a need for more efficient and accurate skull-stripping methods for newborns’ brain MRIs. In this paper, we present an unsupervised approach to adapt a U-net skull-stripping model trained on adult MRI to work effectively on newborns. Our results demonstrate the effectiveness of our novel unsupervised approach in enhancing segmentation accuracy. Our proposed method achieved an overall Dice coefficient of 0.916 ± 0.032 (mean ± std), and our ablation studies confirmed the effectiveness of our proposal. Remarkably, despite being unsupervised, our model’s performance stands in close proximity to that of the current state-of-the-art supervised models against which we conducted our comparisons. These findings indicate the potential of this method as a valuable, easier, and faster tool for supporting healthcare professionals in the examination of MR images of newborn brains. All the codes are available at: https://github.com/abbasomidi77/DAUnet."
Unsupervised Event-Based Video Reconstruction,"Gereon Fox, Xingang Pan, Ayush Tewari, Mohamed Elgharib, Christian Theobalt","MIT; Nanyang Technological University; MPI Informatik, Saarland Informatics Campus",100.0,"Germany, Singapore, USA",0.0,,"Event cameras report events whenever an individual pixel changes brightness. The discrete and asynchronous nature of events makes recovering pixel brightness signals a challenging task, even if conventional brightness frames are recorded along with events. Recent works have addressed this task with neural networks, which tend to be biased towards their training distribution. All methods need to deal with noise in the events to produce very high output framerates. We introduce a new approach to event-based reconstruction, not learning-based: Our model assigns each event an explicit confidence weight to account for the uncertainty arising from noise. We also introduce a novel loss term to balance confidences against each other and show that interpolation of brightness signals between events can benefit from Bezier curves. We demonstrate that allowing brightness changes between exposures can improve reconstruction quality. Our evaluation shows that our method improves the state of the art in the tasks of event-based deblurring and event-based frame interpolation.",https://openaccess.thecvf.com/content/WACV2024/html/Fox_Unsupervised_Event-Based_Video_Reconstruction_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fox_Unsupervised_Event-Based_Video_Reconstruction_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484000/,"['Training', 'Interpolation', 'Uncertainty', 'Brightness', 'Noise', 'Semantics', 'Neural networks']","['Neural Network', 'Loss Term', 'Brightness Changes', 'Training Distribution', 'Bezier Curve', 'Dynamic Vision Sensor', 'Degrees Of Freedom', 'Training Data', 'Exposure Time', 'Gradient Descent', 'Free Parameters', 'Control Points', 'Supplementary Video', 'Short Exposure Time', 'Signal Representation', 'Parabola', 'Bright Signal', 'Loss Of Confidence', 'Strong Noise', 'Motion Blur', 'Input Frames', 'Pair Of Frames', 'Free Parameters Of The Model', 'Brightness Levels']","['Algorithms', 'Computational photography', 'image and video synthesis']",,"Event cameras report events whenever an individual pixel changes brightness. The discrete and asynchronous nature of events makes recovering pixel brightness signals a challenging task, even if conventional brightness frames are recorded along with events. Recent works have addressed this task with neural networks, which tend to be biased towards their training distribution. All methods need to deal with noise in the events to produce very high output framerates. We introduce a new approach to event-based reconstruction, not learning-based: Our model assigns each event an explicit confidence weight to account for the uncertainty arising from noise. We also introduce a novel loss term to balance confidences against each other and show that interpolation of brightness signals between events can benefit from Bézier curves. We demonstrate that allowing brightness changes between exposures can improve reconstruction quality. Our evaluation shows that our method improves the state of the art in the tasks of event-based de-blurring and event-based frame interpolation."
Unsupervised Exemplar-Based Image-to-Image Translation and Cascaded Vision Transformers for Tagged and Untagged Cardiac Cine MRI Registration,"Meng Ye, Mikael Kanski, Dong Yang, Leon Axel, Dimitris Metaxas",Rutgers University; NVIDIA; New York University School of Medicine,66.66666666666666,"Canada, USA",33.33333333333334,USA,"Multi-modal registration between tagged and untagged cardiac cine magnetic resonance (MR) images remains difficult, due to the domain gap and large deformations between the two modalities. Recent work using an image-to-image translation (I2I) module to overcome the domain gap can convert the multi-modal into a mono-modal registration task and take advantage of advanced mono-modal registration architectures. However, they often ignore two issues: the sample-specific style of each image to be registered during I2I and large hybrid rigid and non-rigid deformations between modalities. We first propose an exemplar-based I2I module capable of unsupervised cross-domain correspondence learning to enforce the style consistency between the fake image and the image to be registered. Then we propose an efficient cascaded vision transformer-based registration network to predict both the affine and non-rigid deformations, in which a single feature embedding subnetwork is shared by the two stages of deformation prediction. We validated our method on a clinical cardiac MR dataset with paired but unaligned untagged and tagged MR images. The results show that our method outperforms traditional methods significantly in terms of the I2I quality and multi-modal image registration accuracy.",https://openaccess.thecvf.com/content/WACV2024/html/Ye_Unsupervised_Exemplar-Based_Image-to-Image_Translation_and_Cascaded_Vision_Transformers_for_Tagged_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ye_Unsupervised_Exemplar-Based_Image-to-Image_Translation_and_Cascaded_Vision_Transformers_for_Tagged_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Unsupervised Graphic Layout Grouping With Transformers,"Jialiang Zhu, Danqing Huang, Chunyu Wang, Mingxi Cheng, Ji Li, Han Hu, Xin Geng, Baining Guo","Microsoft Research Asia; Southeast University, Microsoft Research Asia; Southeast University",66.66666666666666,China,33.33333333333334,USA,"Graphic design conveys messages through the combination of text, images and other visual elements. Unstructured designs such as overloaded social media graphics may fail to communicate their intended messages effectively. To address this issue, layout grouping offers a solution by organizing design elements into perceptual groups. While most methods rely on heuristic Gestalt principles, they often lack the context modeling ability needed to handle complex layouts. In this work, we reformulate the layout grouping task as a set prediction problem. It uses Transformers to learn a set of group tokens at various hierarchies, enabling it to reason the membership of the elements more effectively. The self-attention mechanism in Transformers boosts its context modeling ability, which enables it to handle complex layouts more accurately. To reduce annotation costs, we also propose an unsupervised learning strategy that pre-trains on noisy pseudo-labels induced by a novel heuristic algorithm. This approach then bootstraps to self-refine the noisy labels, further improving the accuracy of our model. Our extensive experiments demonstrate the effectiveness of our method, which outperforms existing state-of-the-art approaches in terms of accuracy and efficiency.",https://openaccess.thecvf.com/content/WACV2024/html/Zhu_Unsupervised_Graphic_Layout_Grouping_With_Transformers_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhu_Unsupervised_Graphic_Layout_Grouping_With_Transformers_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483828/,"['Training', 'Visualization', 'Annotations', 'Social networking (online)', 'Layout', 'Transformers', 'Noise measurement']","['Transformer', 'Graphic Layout', 'Heuristic Algorithm', 'Graphic Design', 'Noisy Labels', 'Gestalt Principles', 'Perceptual Grouping', 'Unsupervised Strategy', 'Group Level', 'Hierarchical Structure', 'Parsing', 'Separate Groups', 'Attention Mechanism', 'Bounding Box', 'Learning Objectives', 'Group Elements', 'Quantitative Metrics', 'Adaptive Threshold', 'Text Box', 'Hierarchical Groups', 'List Of Elements', 'Prediction Head', 'Attention Matrix', '1st Row', '2nd Row', 'Attention Layer', 'Pre-defined Threshold', 'Training Labels']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",1,"Graphic design conveys messages through the combination of text, images and other visual elements. Unstructured designs such as overloaded social media graphics may fail to communicate their intended messages effectively. To address this issue, layout grouping offers a solution by organizing design elements into perceptual groups. While most methods rely on heuristic Gestalt principles, they often lack the context modeling ability needed to handle complex layouts. In this work, we reformulate the layout grouping task as a set prediction problem. It uses Transformers to learn a set of group tokens at various hierarchies, enabling it to reason the membership of the elements more effectively. The self-attention mechanism in Transformers boosts its context modeling ability, which enables it to handle complex layouts more accurately. To reduce annotation costs, we also propose an unsupervised learning strategy that pre-trains on noisy pseudo-labels induced by a novel heuristic algorithm. This approach then bootstraps to self-refine the noisy labels, further improving the accuracy of our model. Our extensive experiments demonstrate the effectiveness of our method, which outperforms existing state-of-the-art approaches in terms of accuracy and efficiency."
Unsupervised Model-Based Learning for Simultaneous Video Deflickering and Deblotching,"Anuj Fulari, Satish Mulleti, Ajit Rajwade","IIT Bombay, India",100.0,India,0.0,,"Vintage videos, as well as modern day videos acquired at high frame rates, suffer from a visually disturbing artifact called flicker, which is the rapid change in average intensity across consecutive frames. Vintage videos also suffer from blotch artifacts, i.e., each video frame contains small regions at random locations with undefined pixel values. We present a model-based learning approach to remove flicker as well as blotches simultaneously. Our work uses a pixel-wise affine intensity model for flicker between neighboring frames, with coefficients that vary smoothly in the spatial sense but randomly across time. Due to smooth spatial variation, the flicker coefficients for any given frame can be modelled as linear combinations of low frequency discrete cosine transform (DCT) bases. We also model blotches as heavy-tailed but sparse artifacts affecting every frame. We then present a novel framework to restore the video frames by jointly estimating the blotches as well as the DCT coefficients of the flicker, via convex optimization. Given the high computational cost of the optimization based method for processing an entire video, we use a deep unrolled neural network approach to achieve similar restoration quality at significantly reduced cost. Our approach is completely unsupervised and model based, and hence simple and interpretable. It produces high quality reconstructions, in terms of visual appeal as well as numerical metrics, on a variety of vintage videos as well as high speed videos. It does not suffer from generalization issues unlike some recent state of the art supervised methods which use end to end neural networks for restoration.",https://openaccess.thecvf.com/content/WACV2024/html/Fulari_Unsupervised_Model-Based_Learning_for_Simultaneous_Video_Deflickering_and_Deblotching_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Fulari_Unsupervised_Model-Based_Learning_for_Simultaneous_Video_Deflickering_and_Deblotching_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483977/,"['Measurement', 'Visualization', 'Computational modeling', 'Neural networks', 'Jitter', 'Real-time systems', 'Numerical models']","['Model-based Learning', 'Neural Network', 'Smoothing', 'Volume Change', 'Deep Network', 'Video Frames', 'Convex Optimization', 'Consecutive Frames', 'Heavy-tailed', 'High Frame Rate', 'High-speed Video', 'Discrete Cosine Transform', 'Convolutional Neural Network', 'Cost Function', 'Recurrent Network', 'Markov Chain Monte Carlo', 'Video Clips', 'Optical Flow', 'Video Sequences', 'Flow Vector', 'Types Of Artifacts', 'Motion Vector', 'Pair Of Frames', 'Backward Flow', 'Residual Flow', 'Temporal Consistency', 'YouTube', 'Forward Flow', 'Optical Flow Method']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Low-level and physics-based vision']",,"Vintage videos, as well as modern day videos acquired at high frame rates, suffer from a visually disturbing artifact called flicker, which is the rapid change in average intensity across consecutive frames. Vintage videos also suffer from blotch artifacts, i.e., each video frame contains small regions at random locations with undefined pixel values. We present a model-based learning approach to remove flicker as well as blotches simultaneously. Our work uses a pixel-wise affine intensity model for flicker between neighboring frames, with coefficients that vary smoothly in the spatial sense but randomly across time. Due to smooth spatial variation, the flicker coefficients for any given frame can be modelled as linear combinations of low-frequency discrete cosine transform (DCT) bases. We also model blotches as heavy-tailed but sparse artifacts affecting every frame. We then present a novel framework to restore the video frames by jointly estimating the blotches as well as the DCT coefficients of the flicker via convex optimization. Given the high computational cost of the optimization-based method for processing an entire video, we use a deep unrolled neural network approach to achieve similar restoration quality at significantly reduced cost. Our approach is completely unsupervised and model-based, and hence simple and interpretable. It produces high-quality reconstructions, in terms of visual appeal as well as numerical metrics, on a variety of vintage videos as well as high-speed videos. It does not suffer from generalization issues unlike some recent state-of-the-art supervised methods which use end-to-end neural networks for restoration."
Unsupervised and Semi-Supervised Co-Salient Object Detection via Segmentation Frequency Statistics,"Souradeep Chakraborty, Shujon Naha, Muhammet Bastan, Amit Kumar K. C., Dimitris Samaras","Stony Brook University; Visual Search & AR, Amazon",50.0,USA,50.0,USA,"In this paper, we address the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enable us to develop a semi-supervised method. While previous works have mostly focused on fully supervised CoSOD, less attention has been allocated to detecting co-salient objects when limited segmentation annotations are available for training. Our simple yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. For the first time, we show that a large unlabeled dataset e.g. ImageNet-1k can be effectively leveraged to significantly improve unsupervised CoSOD performance. Our unsupervised model is a great pre-training initialization for our semi-supervised model SS-CoSOD, especially when very limited labeled data is available for training. To avoid propagating erroneous signals from predictions on unlabeled data, we propose a confidence estimation module to guide our semi-supervised training. Extensive experiments on three CoSOD benchmark datasets show that both of our unsupervised and semi-supervised models outperform the corresponding state-of-the-art models by a significant margin (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model).",https://openaccess.thecvf.com/content/WACV2024/html/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Chakraborty_Unsupervised_and_Semi-Supervised_Co-Salient_Object_Detection_via_Segmentation_Frequency_Statistics_WACV_2024_paper.pdf,,,2311.06654,main,Poster,https://ieeexplore.ieee.org/document/10483764/,"['Training', 'Representation learning', 'Computer vision', 'Computational modeling', 'Semantic segmentation', 'Estimation', 'Object detection']","['Object Detection', 'Frequency Statistics', 'Co-salient Objects', 'C=O Groups', 'Semantic Segmentation', 'Unlabeled Data', 'Self-supervised Learning', 'Confidence Estimation', 'Unsupervised Model', 'Salient Object', 'Semi-supervised Model', 'Segmentation Annotations', 'Unsupervised Learning', 'Image Regions', 'Teacher Model', 'Common Objects', 'Backbone Network', 'Group Of Images', 'Semi-supervised Learning', 'Segmentation Map', 'Pseudo Labels', 'Student Network', 'Unlabeled Images', 'Supplement For Details', 'Salient Object Detection', 'Student Model', 'Teacher Network', 'Unlabeled Set', 'Data Split', 'Attention Map']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Applications', 'Commercial / retail']",2,"In this paper, we address the detection of co-occurring salient objects (CoSOD) in an image group using frequency statistics in an unsupervised manner, which further enable us to develop a semi-supervised method. While previous works have mostly focused on fully supervised CoSOD, less attention has been allocated to detecting co-salient objects when limited segmentation annotations are available for training. Our simple yet effective unsupervised method US-CoSOD combines the object co-occurrence frequency statistics of unsupervised single-image semantic segmentations with salient foreground detections using self-supervised feature learning. For the first time, we show that a large unlabeled dataset e.g. ImageNet-1k can be effectively leveraged to significantly improve unsupervised CoSOD performance. Our unsupervised model is a great pre-training initialization for our semi-supervised model SS-CoSOD, especially when very limited labeled data is available for training. To avoid propagating erroneous signals from predictions on unlabeled data, we propose a confidence estimation module to guide our semi-supervised training. Extensive experiments on three CoSOD benchmark datasets show that both of our unsupervised and semi-supervised models outperform the corresponding state-of-the-art models by a significant margin (e.g., on the Cosal2015 dataset, our US-CoSOD model has an 8.8% F-measure gain over a SOTA unsupervised co-segmentation model and our SS-CoSOD model has an 11.81% F-measure gain over a SOTA semi-supervised CoSOD model)."
Using Early Readouts To Mediate Featural Bias in Distillation,"Rishabh Tiwari, Durga Sivasubramanian, Anmol Mekala, Ganesh Ramakrishnan, Pradeep Shenoy","Google Research, India; University of Massachusetts Amherst; IIT Bombay",66.66666666666666,"India, USA",33.33333333333334,USA,"Deep networks tend to learn spurious feature-label correlations in real-world supervised learning tasks. This vulnerability is aggravated in distillation, where a student model may have lesser representational capacity than the corresponding teacher model. Often, knowledge of specific spurious correlations is used to reweight instances & rebalance the learning process. We propose a novel early readout mechanism whereby we attempt to predict the label using representations from earlier network layers. We show that these early readouts automatically identify problem instances or groups in the form of confident, incorrect predictions. Leveraging these signals to modulate the distillation loss on an instance level allows us to substantially improve not only group fairness measures across benchmark datasets, but also overall accuracy of the student model. We also provide secondary analyses that bring insight into the role of feature learning in supervision and distillation.",https://openaccess.thecvf.com/content/WACV2024/html/Tiwari_Using_Early_Readouts_To_Mediate_Featural_Bias_in_Distillation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tiwari_Using_Early_Readouts_To_Mediate_Featural_Bias_in_Distillation_WACV_2024_paper.pdf,,,2310.18590,main,Poster,https://ieeexplore.ieee.org/document/10484228/,"['Representation learning', 'Computer vision', 'Adaptation models', 'Correlation', 'Computational modeling', 'Supervised learning', 'Benchmark testing']","['Early Readout', 'Supervised Learning', 'Joint Effect', 'Teacher Model', 'Benchmark Datasets', 'Student Model', 'Distillation Loss', 'Training Data', 'Ethnic Minority', 'Hyperparameters', 'Feature Maps', 'Average Accuracy', 'Large Model', 'Final Layer', 'Early Stopping', 'Truth Labels', 'Standard Training', 'Top Left', 'Hair Color', 'Waterbirds', 'Group Labels', 'Debiasing', 'Need For Annotation', 'Weighting Scheme', 'Early Stopping Criterion']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Deep networks tend to learn spurious feature-label correlations in real-world supervised learning tasks. This vulnerability is aggravated in distillation, where a student model may have lesser representational capacity than the corresponding teacher model. Often, knowledge of specific spurious correlations is used to reweight instances & rebalance the learning process. We propose a novel early readout mechanism whereby we attempt to predict the label using representations from earlier network layers. We show that these early readouts automatically identify problem instances or groups in the form of confident, incorrect predictions. Leveraging these signals to modulate the distillation loss on an instance level allows us to substantially improve not only group fairness measures across benchmark datasets, but also overall accuracy of the student model. We also provide secondary analyses that bring insight into the role of feature learning in supervision and distillation."
VCISR: Blind Single Image Super-Resolution With Video Compression Synthetic Data,"Boyang Wang, Bowen Liu, Shiyu Liu, Fengyu Yang","University of Michigan, Ann Arbor",100.0,USA,0.0,,"In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail to address degradations caused by video compression, such as mosquito noise, ringing, blockiness, and staircase noise. In this work, we for the first time, present a video compression-based degradation model to synthesize low-resolution image data in the blind SISR task. Our proposed image synthesizing method is widely applicable to existing image datasets, so that a single degraded image can contain distortions caused by the lossy video compression algorithms. This overcomes the leak of feature diversity in video data and thus retains the training efficiency. By introducing video coding artifacts to SISR degradation models, neural networks can super-resolve images with the ability to restore video compression degradations, and achieve better results on restoring generic distortions caused by image compression as well. Our proposed approach achieves superior performance in SOTA no-reference Image Quality Assessment, and shows better visual quality on various datasets. In addition, we evaluate the SISR neural network trained with our degradation model on video super-resolution (VSR) datasets. Compared to architectures specifically designed for the VSR purpose, our method exhibits similar or better performance, evidencing that the presented strategy on infusing video-based degradation is generalizable to address more complicated compression artifacts even without temporal cues. The code is available at https://github.com/Kiteretsu77/VCISR-official.",https://openaccess.thecvf.com/content/WACV2024/html/Wang_VCISR_Blind_Single_Image_Super-Resolution_With_Video_Compression_Synthetic_Data_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Wang_VCISR_Blind_Single_Image_Super-Resolution_With_Video_Compression_Synthetic_Data_WACV_2024_paper.pdf,,https://github.com/Kiteretsu77/VCISR-official,2311.00996,main,Poster,https://ieeexplore.ieee.org/document/10483569/,"['Degradation', 'Image coding', 'Superresolution', 'Pipelines', 'Noise', 'Neural networks', 'Video compression']","['Super-resolution', 'Video Compression', 'Single Image Super-resolution', 'Neural Network', 'Image Dataset', 'Network Training', 'Visual Quality', 'Low-resolution Images', 'Image Compression', 'Compression Algorithm', 'Compression Artifacts', 'High-resolution Images', 'Search Algorithm', 'Real-world Scenarios', 'Color Space', 'Shot Noise', 'Quantization Parameter', 'L1 Loss', 'Real-world Images', 'Frames Per Second', 'JPEG Compression', 'Discrete Cosine Transform', 'Temporal Compression', 'Bitrate', 'Super-resolution Network', 'Parameters Of The Encoder', 'Perceptual Loss', 'Video Dataset']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",2,"In the blind single image super-resolution (SISR) task, existing works have been successful in restoring image-level unknown degradations. However, when a single video frame becomes the input, these works usually fail to address degradations caused by video compression, such as mosquito noise, ringing, blockiness, and staircase noise. In this work, we for the first time, present a video compressionbased degradation model to synthesize low-resolution image data in the blind SISR task. Our proposed image synthesizing method is widely applicable to existing image datasets, so that a single degraded image can contain distortions caused by the lossy video compression algorithms. This overcomes the leak of feature diversity in video data and thus retains the training efficiency. By introducing video coding artifacts to SISR degradation models, neural networks can super-resolve images with the ability to restore video compression degradations, and achieve better results on restoring generic distortions caused by image compression as well. Our proposed approach achieves superior performance in SOTA no-reference Image Quality Assessment, and shows better visual quality on various datasets. In addition, we evaluate the SISR neural network trained with our degradation model on video super-resolution (VSR) datasets. Compared to architectures specifically designed for the VSR purpose, our method exhibits similar or better performance, evidencing that the presented strategy on infusing video-based degradation is generalizable to address more complicated compression artifacts even without temporal cues. The code is available at https://github.com/Kiteretsu77/VCISR-official."
VD-GR: Boosting Visual Dialog With Cascaded Spatial-Temporal Multi-Modal Graphs,"Adnen Abdessaied, Lei Shi, Andreas Bulling","Institute for Visualization and Interactive Systems, University of Stuttgart, Germany",100.0,Germany,0.0,,"We propose VD-GR -- a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of VD-GR is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributions: First, we use multi-modal GNNs to process the features of each modality (image, question, and dialog history) and exploit their local structures before performing BERT global attention. Second, we propose hub-nodes that link to all other nodes within one modality graph, allowing the model to propagate information from one GNN (modality) to the other in a cascaded manner. Third, we augment the BERT hidden states with fine-grained multi-modal GNN features before passing them to the next VD-GR layer. Evaluations on VisDial v1.0, VisDial v0.9, VisDialConv, and VisPro show that VD-GR achieves new state-of-the-art results on all datasets.",https://openaccess.thecvf.com/content/WACV2024/html/Abdessaied_VD-GR_Boosting_Visual_Dialog_With_Cascaded_Spatial-Temporal_Multi-Modal_Graphs_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Abdessaied_VD-GR_Boosting_Visual_Dialog_With_Cascaded_Spatial-Temporal_Multi-Modal_Graphs_WACV_2024_paper.pdf,http://web-page-link-provided-in-text.com,,,main,Poster,https://ieeexplore.ieee.org/document/10483739/,"['Visualization', 'Computer vision', 'Boosting', 'Graph neural networks', 'History']","['Visual Dialog', 'Multimodal Graph', 'Local Structure', 'Language Model', 'Hidden State', 'Graph Neural Networks', 'Fine-grained Features', 'Pre-trained Language Models', 'Recent Models', 'Attention Mechanism', 'Image Object', 'Visual Task', 'Global Context', 'Visual Input', 'Question Wording', 'Node Features', 'Linear Layer', 'Graph Construction', 'Residual Connection', 'Masked Images', 'Masked Language Model', 'Special Token', 'Multimodal Tasks', 'Absolute Point', 'Lack Of Context', 'Graph Features', 'Index Of Layer']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Datasets and evaluations']",,"We propose $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ – a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributions: First, we use multi-modal GNNs to process the features of each modality (image, question, and dialog history) and exploit their local structures before performing BERT global attention. Second, we propose hub-nodes that link to all other nodes within one modality graph, allowing the model to propagate information from one GNN (modality) to the other in a cascaded manner. Third, we augment the BERT hidden states with fine-grained multi-modal GNN features before passing them to the next $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ layer. Evaluations on VisDial v1.0, VisDial v0.9, VisDialConv, and VisPro show that $\mathbb{V}\mathbb{D}{\text{ - }}\mathbb{G}\mathbb{R}$ achieves new state-of-the-art results across all four datasets."
VEATIC: Video-Based Emotion and Affect Tracking in Context Dataset,"Zhihang Ren, Jefferson Ortega, Yifan Wang, Zhimin Chen, Yunhui Guo, Stella X. Yu, David Whitney","University of California, Berkeley; University of Michigan, Ann Arbor; University of Texas at Dallas; University of California, Berkeley",100.0,USA,0.0,,"Human affect recognition has been a significant topic in psychophysics and computer vision. However, the currently published datasets have many limitations. For example, most datasets contain frames that contain only information about facial expressions. Due to the limitations of previous datasets, it is very hard to either understand the mechanisms for affect recognition of humans or generalize well on common cases for computer vision models trained on those datasets. In this work, we introduce a brand new large dataset, the Video-based Emotion and Affect Tracking in Context Dataset (VEATIC), that can conquer the limitations of the previous datasets. VEATIC has 124 video clips from Hollywood movies, documentaries, and home videos with continuous valence and arousal ratings of each frame via real-time annotation. Along with the dataset, we propose a new computer vision task to infer the affect of the selected character via both context and character information in each video frame. Additionally, we propose a simple model to benchmark this new computer vision task. We also compare the performance of the pretrained model using our dataset with other similar datasets. Experiments show the competing results of our pretrained model via VEATIC, indicating the generalizability of VEATIC. Our dataset is available at https://veatic.github.io.",https://openaccess.thecvf.com/content/WACV2024/html/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.pdf,,https://veatic.github.io,2309.06745,main,Poster,https://ieeexplore.ieee.org/document/10484004/,"['Computer vision', 'Visualization', 'Annotations', 'Computational modeling', 'Streaming media', 'Benchmark testing', 'Prediction algorithms']","['Video-based Emotion', 'Large Datasets', 'Computer Vision', 'Contextual Information', 'Facial Expressions', 'Feature Information', 'Documentary', 'Video Clips', 'Video Frames', 'Psychophysics', 'Previous Datasets', 'Affect Recognition', 'Valence Ratings', 'Arousal Ratings', 'Hollywood Films', 'Home Video', 'Root Mean Square Error', 'Convolutional Neural Network', 'Contextual Factors', 'Emotional States', 'Concordance Correlation Coefficient', 'Emotion Recognition', 'Temporal Information', 'Video Dataset', 'Long Short-term Memory', 'Gated Recurrent Unit', 'Temporal Context', 'Vision Transformer', 'Emotion Recognition Task', 'Emotions Of Others']","['Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Video recognition and understanding']",3,"Human affect recognition has been a significant topic in psychophysics and computer vision. However, the currently published datasets have many limitations. For example, most datasets contain frames that contain only information about facial expressions. Due to the limitations of previous datasets, it is very hard to either understand the mechanisms for affect recognition of humans or generalize well on common cases for computer vision models trained on those datasets. In this work, we introduce a brand new large dataset, the Video-based Emotion and Affect Tracking in Context Dataset (VEATIC), that can conquer the limitations of the previous datasets. VEATIC has 124 video clips from Hollywood movies, documentaries, and home videos with continuous valence and arousal ratings of each frame via real-time annotation. Along with the dataset, we propose a new computer vision task to infer the affect of the selected character via both context and character information in each video frame. Additionally, we propose a simple model to benchmark this new computer vision task. We also compare the performance of the pretrained model using our dataset with other similar datasets. Experiments show the competing results of our pretrained model via VEATIC, indicating the generalizability of VEATIC. Our dataset is available at https://veatic.github.io."
VMFormer: End-to-End Video Matting With Transformer,"Jiachen Li, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Yunchao Wei, Humphrey Shi","SHI Labs @Georgia Tech & Oregon & UIUC, Picsart AI Research (PAIR); SHI Labs @Georgia Tech & Oregon & UIUC; Picsart AI Research (PAIR); BJTU",75.0,"China, USA",25.0,Ukraine,"Video matting aims to predict the alpha mattes for each frame from a given input video sequence. Recent solutions to video matting have been dominated by deep convolutional neural networks (CNN) for the past few years, which have become the de-facto standard for academia and industry. However, they have the inbuilt inductive bias of locality and do not capture the global characteristics of an image due to the CNN-based architectures. They also need long-range temporal modeling considering computational costs when dealing with feature maps of multiple frames. In this paper, we propose VMFormer: a transformer-based end-to-end method for video matting. It makes predictions on alpha mattes of each frame from learnable queries given a video input sequence. Specifically, it leverages self-attention layers to build global integration of feature sequences with short-range temporal modeling on successive frames. We further apply queries to learn global representations through cross-attention in the transformer decoder with long-range temporal modeling upon all queries. In the prediction stage, both queries and corresponding feature maps are used to make the final prediction of alpha mattes. Experiments show that VMFormer outperforms previous CNN-based video matting methods on synthetic benchmarks with different input resolutions, as an end-to-end video matting solution built upon a full vision transformer with predictions on the learnable queries. The project is open-sourced at https://chrisjuniorli.github.io/project/VMFormer.",https://openaccess.thecvf.com/content/WACV2024/html/Li_VMFormer_End-to-End_Video_Matting_With_Transformer_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_VMFormer_End-to-End_Video_Matting_With_Transformer_WACV_2024_paper.pdf,https://chrisjuniorli.github.io/project/VMFormer,https://github.com/chrisjuniorli/VMFormer,2208.12801,main,Poster,,,,,,
Video Instance Matting,"Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Humphrey Shi",SHI Labs @Georgia Tech & Oregon & UIUC; Picsart AI Research (PAIR),50.0,USA,50.0,Ukraine,"Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting (VIM), that is, estimating the alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality (VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin.",https://openaccess.thecvf.com/content/WACV2024/html/Li_Video_Instance_Matting_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Li_Video_Instance_Matting_WACV_2024_paper.pdf,,https://github.com/SHI-Labs/VIM,2311.04212,main,Poster,https://ieeexplore.ieee.org/document/10483660/,"['Measurement', 'Instance segmentation', 'Computer vision', 'Video sequences', 'Neural networks', 'Benchmark testing', 'Task analysis']","['Video Instance', 'Benchmark', 'Video Clips', 'Video Frames', 'Video Sequences', 'Alpha Matte', 'True Positive', 'Feature Maps', 'Intersection Over Union', 'Temporal Information', 'Background Image', 'Consecutive Frames', 'Temporal Context', 'Video Editing', 'Track Quality', 'Atrous Spatial Pyramid Pooling', 'Recognition Quality', 'Instance Segmentation Methods']","['Algorithms', 'Video recognition and understanding']",3,"Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting (VIM), that is, estimating alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality (VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin. The project is opensourced at https://github.com/SHI-Labs/VIM."
Video-kMaX: A Simple Unified Approach for Online and Near-Online Video Panoptic Segmentation,"Inkyu Shin, Dahun Kim, Qihang Yu, Jun Xie, Hong-Seok Kim, Bradley Green, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen",KAIST; Google Research,50.0,South Korea,50.0,USA,"Video Panoptic Segmentation (VPS) aims to achieve comprehensive pixel-level scene understanding by segmenting all pixels and associating objects in a video. Current solutions can be categorized into online and near-online approaches. Evolving over the time, each category has its own specialized designs, making it nontrivial to adapt models between different categories. To alleviate the discrepancy, in this work, we propose a unified approach for online and near-online VPS. The meta architecture of the proposed Video-kMaX consists of two components: within-clip segmenter (for clip-level segmentation) and cross-clip associater (for association beyond clips). We propose clip-kMaX (clip k-means mask transformer) and LA-MB (locationaware memory buffer) to instantiate the segmenter and associater, respectively. Our general formulation includes the online scenario as a special case by adopting clip length of one. Without bells and whistles, Video-kMaX sets a new state-of-the-art on KITTI-STEP and VIPSeg for video panoptic segmentation Code will be made publicly available. Code and models are available at this link: https://github.com/dlsrbgg33/video_kmax.",https://openaccess.thecvf.com/content/WACV2024/html/Shin_Video-kMaX_A_Simple_Unified_Approach_for_Online_and_Near-Online_Video_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shin_Video-kMaX_A_Simple_Unified_Approach_for_Online_and_Near-Online_Video_WACV_2024_paper.pdf,,thislink,,main,Poster,https://ieeexplore.ieee.org/document/10484382/,"['Computer vision', 'Adaptation models', 'Codes', 'Memory modules', 'Computer architecture', 'Streaming media', 'Transformers']","['Online Video', 'Panoptic Segmentation', 'Video Panoptic Segmentation', 'General Form', 'Segmentation Model', 'Video Object', 'Shorter Length', 'Local Features', 'Image Segmentation', 'Bounding Box', 'Video Clips', 'Semantic Segmentation', 'Cluster Centers', 'Appearance Features', 'Online Methods', 'Current Frame', 'Instance Segmentation', 'Matching Accuracy', 'Online Setting', 'Memory Characteristics', 'Object Memory', 'Memory Module', 'Transformer Decoder', 'Long-term Association', 'Object In Frame', 'Hierarchical Strategy', 'Object Tracking', 'Target Object', 'Decoding']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Image recognition and understanding']",2,"Video Panoptic Segmentation (VPS) aims to achieve comprehensive pixel-level scene understanding by segmenting all pixels and associating objects in a video. Current solutions can be categorized into online and near-online approaches. Evolving over the time, each category has its own specialized designs, making it nontrivial to adapt models between different categories. To alleviate the discrepancy, in this work, we propose a unified approach for online and near-online VPS. The meta architecture of the proposed Video-kMaX consists of two components: within-clip segmenter (for clip-level segmentation) and cross-clip associater (for association beyond clips). We propose clip-kMaX (clip k-means mask transformer) and LA-MB (location-aware memory buffer) to instantiate the segmenter and associater, respectively. Our general formulation includes the online scenario as a special case by adopting clip length of one. Without bells and whistles, Video-kMaX sets a new state-of-the-art on KITTI-STEP and VIPSeg for video panoptic segmentation Code and models are available at this link."
"VideoFACT: Detecting Video Forgeries Using Attention, Scene Context, and Forensic Traces","Tai D. Nguyen, Shengbang Fang, Matthew C. Stamm","Drexel University, Philadelphia, PA, USA",100.0,USA,0.0,,"Fake videos represent an important misinformation threat. While existing forensic networks have demonstrated strong performance on image forgeries, recent results reported on the Adobe VideoSham dataset show that these networks fail to identify fake content in videos. In response, we propose VideoFACT - a new network that is able to detect and localize a wide variety of video forgeries and manipulations. To overcome challenges that existing networks face when analyzing videos, our network utilizes both forensic embeddings to capture traces left by manipulation, context embeddings to control for variation in forensic traces introduced by video coding, and a deep self-attention mechanism to estimate the quality and relative importance of local forensic embeddings. We create several new video forgery datasets and use these, along with publicly available data, to experimentally evaluate our network's performance. These results show that our proposed network is able to identify a diverse set of video forgeries, including those not encountered during training. Furthermore, we show that our network can be fine-tuned to achieve even stronger performance on challenging AI-based manipulations.",https://openaccess.thecvf.com/content/WACV2024/html/Nguyen_VideoFACT_Detecting_Video_Forgeries_Using_Attention_Scene_Context_and_Forensic_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Nguyen_VideoFACT_Detecting_Video_Forgeries_Using_Attention_Scene_Context_and_Forensic_WACV_2024_paper.pdf,,https://github.com/ductai199x/videofact-wacv-2024,2211.15775,main,Poster,https://ieeexplore.ieee.org/document/10483886/,"['Video coding', 'Training', 'Image forensics', 'Computer vision', 'Codes', 'Forensics', 'Forgery']","['Forensic Trace', 'Fine-tuned', 'Video Content', 'Video Coding', 'Local Embedding', 'Contextual Embedding', 'Local Variations', 'Local Network', 'False Alarm', 'Compressive Strength', 'Video Frames', 'Low-level Features', 'Localizer', 'Attention Map', 'Video Dataset', 'Camera Model', 'Original Video', 'Transformer Encoder', 'Video Compression', 'Spatial Attention Map', 'Edit Operations', 'Joint Embedding', 'Code Parameters', 'Scene Regions', 'Scene Content', 'Stochastic Gradient Descent', 'Localization Performance', 'Effect Of Compression', 'Training Dataset']","['Applications', 'Social good']",2,"Fake videos represent an important misinformation threat. While existing forensic networks have demonstrated strong performance on image forgeries, recent results reported on the Adobe VideoSham dataset show that these networks fail to identify fake content in videos. In response, we propose VideoFACT - a new network that is able to detect and localize a wide variety of video forgeries and manipulations. To overcome challenges that existing networks face when analyzing videos, our network utilizes both forensic embeddings to capture traces left by manipulation, context embeddings to control for variation in forensic traces introduced by video coding, and a deep self-attention mechanism to estimate the quality and relative importance of local forensic embeddings. We create several new video forgery datasets and use these, along with publicly available data, to experimentally evaluate our network’s performance. These results show that our proposed network is able to identify a diverse set of video forgeries, including those not encountered during training. Furthermore, we show that our network can be fine-tuned to achieve even stronger performance on challenging AI-based manipulations. (Code is available at: https://github.com/ductai199x/videofact-wacv-2024)"
Vikriti-ID: A Novel Approach for Real Looking Fingerprint Data-Set Generation,"Rishabh Shukla, Aditya Sinha, Vansh Singh, Harkeerat Kaur","Indian Institute of Technology Jammu, Jammu, India",100.0,India,0.0,,"Fingerprint recognition research faces significant challenges due to the limited availability of extensive and publicly available fingerprint databases. Existing databases lack a sufficient number of identities and fingerprint impressions, which hinders progress in areas such as Fingerprintbased access control. To address this challenge, we present Vikriti-ID, a synthetic fingerprint generator capable of generating unique fingerprints with multiple impressions. Using Vikriti-ID, we generated a large database containing 500000 unique fingerprints, each with 10 associated impressions. We then demonstrate the effectiveness of the database generated by Vikriti-ID by evaluating it for imposter-genuine score distribution and EER score. Apart from this we also trained a deep network to check the usability of data. We trained a deep network on both Vikriti-ID generated data as well as public data. This generated data achieved an Equal Error Rate(EER) of 0.16%, AUC of 0.89%. This improvement is possible due to the limitations of existing publicly available data-set, which struggle in numbers or multiple impressions.",https://openaccess.thecvf.com/content/WACV2024/html/Shukla_Vikriti-ID_A_Novel_Approach_for_Real_Looking_Fingerprint_Data-Set_Generation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Shukla_Vikriti-ID_A_Novel_Approach_for_Real_Looking_Fingerprint_Data-Set_Generation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484524/,"['Access control', 'Computer vision', 'Databases', 'Fingerprint recognition', 'Generators', 'Usability', 'Faces']","['Fingerprint Dataset', 'Deep Network', 'Access Control', 'Unique Fingerprint', 'Synthetic Generation', 'Fingerprint Database', 'Equal Error Rate', 'Receiver Operating Characteristic Curve', 'Unique Identification', 'Generative Adversarial Networks', 'Term In Equation', 'Learning-based Methods', 'Random Matrix', 'Variational Autoencoder', 'Matching Score', 'Intra-class Variance', 'Random Signal', 'Regularization Loss', 'Generation Of Intermediates', 'Lack Of Training Data', 'False Acceptance Rate', 'Intermediate Image', 'GAN-based Methods', 'Variational Autoencoder Model']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose']",1,"Fingerprint recognition research faces significant challenges due to the limited availability of extensive and publicly available fingerprint databases. Existing databases lack a sufficient number of identities and fingerprint impressions, which hinders progress in areas such as Fingerprint-based access control. To address this challenge, we present Vikriti-ID, a synthetic fingerprint generator capable of generating unique fingerprints with multiple impressions. Using Vikriti-ID, we generated a large database containing 500000 unique fingerprints, each with 10 associated impressions. We then demonstrate the effectiveness of the database generated by Vikriti-ID by evaluating it for imposter-genuine score distribution and EER score. Apart from this we also trained a deep network to check the usability of data. We trained the network inspired from [13], on both Vikriti-ID generated data as well as public data. This generated data achieved an Equal Error Rate(EER) of 0.16%, AUC of 0.89%. This improvement is possible due to the limitations of existing publicly available data sets, which struggle in numbers or multiple impressions."
Vision Transformer for Multispectral Satellite Imagery: Advancing Landcover Classification,Ryan Rad,"Northeastern University, Khoury College of Computer Science",100.0,USA,0.0,,"Climate change is a global issue with significant impacts on ecosystems and human populations. Accurately classifying land cover from multi-spectral satellite imagery plays a crucial role in understanding the Earth's changing landscape and its implications for environmental processes. However, traditional methods struggle with challenges like limited data availability and capturing complex spatial-spectral relationships. Vision Transformers have emerged as a promising alternative to convolutional neural networks (CNN architectures), harnessing the power of self-attention mechanisms to capture global and long-range dependencies. However, their application to multi-spectral images is still limited. In this paper, we propose a novel Vision Transformer designed for multi-spectral satellite image datasets of limited size to perform reliable land cover identification with forty-four classes. We conduct extensive experiments on a curated dataset, simulating scenarios with limited data availability, and compare our approach to alternative architectures. The results demonstrate the potential of our Vision Transformer-based method in achieving accurate land cover classification, contributing to improving climate change modeling and environmental understanding.",https://openaccess.thecvf.com/content/WACV2024/html/Rad_Vision_Transformer_for_Multispectral_Satellite_Imagery_Advancing_Landcover_Classification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Rad_Vision_Transformer_for_Multispectral_Satellite_Imagery_Advancing_Landcover_Classification_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
Visual Narratives: Large-Scale Hierarchical Classification of Art-Historical Images,"Matthias Springstein, Stefanie Schneider, Javad Rahnama, Julian Stalter, Maximilian Kristen, Eric Müller-Budack, Ralph Ewerth","Ludwig Maximilian University of Munich, Germany; TIB – Leibniz Information Centre for Science and Technology, Germany; L3S Research Center, Leibniz University Hannover, Germany; Reply GmbH, Germany",50.0,Germany,50.0,Germany,"Iconography refers to the methodical study and interpretation of thematic content in the visual arts, distinguishing it, e.g., from purely formal or aesthetic considerations. In iconographic studies, Iconclass is a widely used taxonomy that encapsulates historical, biblical, and literary themes, among others. However, given the hierarchical nature and inherent complexity of such a taxonomy, it is highly desirable to use automated methods for (Iconclass-based) image classification. Previous studies either focused narrowly on certain subsets of narratives or failed to exploit Iconclass's hierarchical structure. In this paper, we propose a novel approach for Hierarchical Multi-label Classification (HMC) of iconographic concepts in images. We present three strategies, including Large Language Models (LLMs), for the generation of textual image descriptions using keywords extracted from Iconclass. These descriptions are utilized to pre-train a Vision-Language Model (VLM) based on a newly introduced data set of 477,569 images with more than 20,000 Iconclass concepts, far more than considered in previous studies. Furthermore, we present five approaches to multi-label classification, including a novel transformer decoder that leverages hierarchical information from the Iconclass taxonomy. Experimental results show the superiority of this approach over reasonable baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Springstein_Visual_Narratives_Large-Scale_Hierarchical_Classification_of_Art-Historical_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Springstein_Visual_Narratives_Large-Scale_Hierarchical_Classification_of_Art-Historical_Images_WACV_2024_paper.pdf,,https://github.com/TIBHannover/iconclass-classification,,main,Poster,https://ieeexplore.ieee.org/document/10483897/,"['Visualization', 'Computer vision', 'Taxonomy', 'Transformers', 'Data models', 'Decoding', 'Thesauri']","['Image Classification', 'Hierarchical Classification', 'Hierarchical Structure', 'Multi-label', 'Classification Approach', 'Visual Arts', 'Language Model', 'Textual Descriptions', 'Conceptual Categories', 'Theme In The Literature', 'Image Descriptors', 'Iconography', 'Concept Of Image', 'Hierarchical Information', 'Transformer Decoder', 'Training Set', 'Computer Vision', 'Classification Task', 'Cross-entropy Loss', 'Taxonomic Levels', 'Vision Transformer', 'Image Captioning', 'Levels Of Hierarchy', 'Set Of Concepts', 'Text Encoder', 'Classification Head', 'Transformer Model', 'Creation Date', 'Attention Heads', 'Sigmoid Activation']","['Applications', 'Arts / games / social media', 'Algorithms', 'Image recognition and understanding', 'Algorithms', 'Vision + language and/or other modalities']",,"Iconography refers to the methodical study and interpretation of thematic content in the visual arts, distinguishing it, e.g., from purely formal or aesthetic considerations. In iconographic studies, Iconclass is a widely used taxonomy that encapsulates historical, biblical, and literary themes, among others. However, given the hierarchical nature and inherent complexity of such a taxonomy, it is highly desirable to use automated methods for (Iconclass-based) image classification. Previous studies either focused narrowly on certain subsets of narratives or failed to exploit Iconclass’s hierarchical structure. In this paper, we propose a novel approach for Hierarchical Multi-label Classification (HMC) of iconographic concepts in images. We present three strategies, including Language Models (LMs), for the generation of textual image descriptions using keywords extracted from Iconclass. These descriptions are utilized to pre-train a Vision-Language Model (VLM) based on a newly introduced data set of 477,569 images with more than 20,000 Iconclass concepts, far more than considered in previous studies. Furthermore, we present five approaches to multi-label classification, including a novel transformer decoder that leverages hierarchical information from the Iconclass taxonomy. Experimental results show the superiority of this approach over reasonable baselines."
Visually Guided Audio Source Separation With Meta Consistency Learning,"Md Amirul Islam, Seyed Shahabeddin Nabavi, Irina Kezele, Yang Wang, Yuanhao Yu, Jin Tang",Concordia University; Huawei Noah’s Ark Lab,50.0,Canada,50.0,China,"In this paper, we tackle the problem of visually guided audio source separation in the context of both known and unknown objects (e.g., musical instruments). Recent successful end-to-end deep learning approaches adopt a single network with fixed parameters to generalize across unseen test videos. However, it can be challenging to generalize in cases where the distribution shift between training and test videos is higher as they fail to utilize internal information of unknown test videos. Based on this observation, we introduce a meta-consistency driven test time adaptation scheme that enables the pretrained model to quickly adapt to known and unknown test music videos in order to bring substantial improvements. In particular, we design a self-supervised audio-visual consistency objective as an auxiliary task that learns the synchronization between audio and its corresponding visual embedding. Concretely, we apply a meta-consistency training scheme to further optimize the pretrained model for effective and faster test time adaptation. We obtain substantial performance gains with only a smaller number of gradient updates and without any additional parameters for the task of audio source separation. Extensive experimental results across datasets demonstrate the effectiveness of our proposed method.",https://openaccess.thecvf.com/content/WACV2024/html/Islam_Visually_Guided_Audio_Source_Separation_With_Meta_Consistency_Learning_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Islam_Visually_Guided_Audio_Source_Separation_With_Meta_Consistency_Learning_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484190/,"['Training', 'Adaptation models', 'Visualization', 'Source separation', 'Instruments', 'Music', 'Performance gain']","['Source Separation', 'Audio Source', 'Audio Source Separation', 'Adaptive Model', 'Musical Instruments', 'Music Videos', 'Test Videos', 'Number Of Updates', 'Gradient Update', 'Auxiliary Task', 'Test Samples', 'Learning Rate', 'Convolutional Neural Network', 'Visual Network', 'Video Frames', 'Primary Task', 'Unknown Samples', 'Consecutive Frames', 'Outer Loop', 'Separate Networks', 'Meta Learning', 'Signal-to-interference Ratio', 'Separation Performance', 'Self-supervised Learning', 'Mixed Signals', 'Short-time Fourier Transform', 'Separate Tasks', 'Consistency Loss', 'Transformer Encoder', 'Multimodal Representation']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"In this paper, we tackle the problem of visually guided audio source separation in the context of both known and unknown objects (e.g., musical instruments). Recent successful end-to-end deep learning approaches adopt a single network with fixed parameters to generalize across unseen test videos. However, it can be challenging to generalize in cases where the distribution shift between training and test videos is higher as they fail to utilize internal information of unknown test videos. Based on this observation, we introduce a meta-consistency driven test time adaptation scheme that enables the pretrained model to quickly adapt to known and unknown test music videos in order to bring substantial improvements. In particular, we design a self-supervised audio-visual consistency objective as an auxiliary task that learns the synchronization between audio and its corresponding visual embedding. Concretely, we apply a meta-consistency training scheme to further optimize the pretrained model for effective and faster test time adaptation. We obtain substantial performance gains with only a smaller number of gradient updates and without any additional parameters for the task of audio source separation. Extensive experimental results across datasets demonstrate the effectiveness of our proposed method."
Volumetric Disentanglement for 3D Scene Manipulation,"Sagie Benaim, Frederik Warburg, Peter Ebert Christensen, Serge Belongie",University of Copenhagen; Technical University of Denmark,100.0,Denmark,0.0,,"Recently, advances in differential volumetric rendering enabled significant breakthroughs in the photo-realistic and fine-detailed reconstruction of complex 3D scenes, which is key for many virtual reality applications. However, in the context of augmented reality, one may also wish to effect semantic manipulations or augmentations of objects within a scene. To this end, we propose a volumetric framework for (i) disentangling or separating, the volumetric representation of a given foreground object from the background, and (ii) semantically manipulating the foreground object, as well as the background. Our method enables the separate control of pixel color and depth as well as 3D similarity transformations of both the foreground and background objects. We subsequently demonstrate our framework's applicability on several downstream manipulation tasks, going beyond the placement and movement of foreground objects. These tasks include object camouflage, non-negative 3D object inpainting, 3D object translation, 3D object inpainting, and 3D text-based object manipulation. Our framework takes as input a set of 2D masks specifying the desired foreground object for training views, together with the associated 2D views and poses, and produces a foreground-background disentanglement that respects the surrounding illumination, reflections, and partial occlusions, which can be applied to both training and novel views.",https://openaccess.thecvf.com/content/WACV2024/html/Benaim_Volumetric_Disentanglement_for_3D_Scene_Manipulation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Benaim_Volumetric_Disentanglement_for_3D_Scene_Manipulation_WACV_2024_paper.pdf,https://sagiebenaim.github.io/volumetric-disentanglement/,,2206.02776,main,Poster,https://ieeexplore.ieee.org/document/10483671/,"['Training', 'Computer vision', 'Three-dimensional displays', 'Semantics', 'Lighting', 'Color', 'Rendering (computer graphics)']","['Volumetric', '3D Scene', 'Manipulation Tasks', 'Object Motion', 'Foreground Objects', 'Background Objects', 'Depth Control', '2D Setting', '2D Pose', 'Strawberry', 'Manual Annotation', 'Image Patches', 'Individual Objects', '3D Information', 'Tree Trunks', 'Objects In The Scene', 'Reconstruction Loss', 'Masked Images', 'Occluded Objects', 'Neural Field', '3D Manipulation', '3D Bounding Box', 'Target Text', 'Disentangled Representation', '2D Counterparts', 'Background Scene', 'Texture Of Objects', 'Set Of Views']","['Applications', 'Virtual / augmented reality', 'Algorithms', '3D computer vision']",2,"Recently, advances in differential volumetric rendering enabled significant breakthroughs in the photo-realistic and fine-detailed reconstruction of complex 3D scenes, which is key for many virtual reality applications. However, in the context of augmented reality, one may also wish to effect semantic manipulations or augmentations of objects within a scene. To this end, we propose a volumetric framework for (i) disentangling or separating, the volumetric representation of a given foreground object from the background, and (ii) semantically manipulating the foreground object, as well as the background. Our framework takes as input a set of 2D masks specifying the desired foreground object for training views, together with the associated 2D views and poses, and produces a foreground-background disentanglement that respects the surrounding illumination, reflections, and partial occlusions, which can be applied to both training and novel views. Our method enables the separate control of pixel color and depth as well as 3D similarity transformations of both the foreground and background objects. We subsequently demonstrate our framework’s applicability on several downstream manipulation tasks, going beyond the placement and movement of foreground objects. These tasks include object camouflage, non-negative 3D object in-painting, 3D object translation, 3D object inpainting, and 3D text-based object manipulation. The project webpage is provided in https://sagiebenaim.github.io/volumetric-disentanglement/."
WATCH: Wide-Area Terrestrial Change Hypercube,"Connor Greenwell, Jon Crall, Matthew Purri, Kristin Dana, Nathan Jacobs, Armin Hadzic, Scott Workman, Matt Leotta","Washington University in St. Louis; Rutgers University; Kitware, Inc.; DZYNE Technologies",75.0,USA,25.0,USA,"Monitoring Earth activity using data collected from multiple satellite imaging platforms in a unified way is a significant challenge, especially with large variability in image resolution, spectral bands, and revisit rates. Further, the availability of sensor data varies across time as new platforms are launched. In this work, we introduce an adaptable framework and network architecture capable of predicting on subsets of the available platforms, bands, or temporal ranges it was trained on. Our system, called WATCH, is highly general and can be applied to a variety of geospatial tasks. In this work, we analyze the performance of WATCH using the recent IARPA SMART public dataset and metrics. We focus primarily on the problem of broad area search for heavy construction sites. Experiments validate the robustness of WATCH during inference to limited sensor availability, as well the the ability to alter inference-time spatial or temporal sampling. WATCH is open source and available for use on this or other remote sensing problems. Code and model weights are available at: https://gitlab.kitware.com/computer-vision/geowatch",https://openaccess.thecvf.com/content/WACV2024/html/Greenwell_WATCH_Wide-Area_Terrestrial_Change_Hypercube_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Greenwell_WATCH_Wide-Area_Terrestrial_Change_Hypercube_WACV_2024_paper.pdf,,https://gitlab.kitware.com/computer-vision/geowatch,,main,Poster,https://ieeexplore.ieee.org/document/10483919/,"['Measurement', 'Satellites', 'Imaging', 'Watches', 'Network architecture', 'Search problems', 'Robustness']","['Open-source', 'Spectral Bands', 'Construction Sites', 'Multiple Platforms', 'Change Detection', 'Multilayer Perceptron', 'Spatial Coverage', 'Temporal Window', 'Transformer Model', 'Composite Image', 'Multiple Sensors', 'Landsat 8', 'Multi-task Learning', 'Tokenized', 'Self-supervised Learning', 'Number Of Time Steps', 'Prediction Step', 'Binary Classification Task', 'Positional Encoding', 'Vision Transformer', 'Sequence Of Tokens', 'Sentinel-2 Imagery', 'Input Tokens', 'Technical Malfunction', 'Secondary Loss', 'Time Step', 'Image Pairs', 'Input Image', 'Time Series', 'Input Modalities']","['Applications', 'Remote Sensing', 'Applications', 'Environmental monitoring / climate change / ecology']",2,"Monitoring Earth activity using data collected from multiple satellite imaging platforms in a unified way is a significant challenge, especially with large variability in image resolution, spectral bands, and revisit rates. Further, the availability of sensor data varies across time as new platforms are launched. In this work, we introduce an adaptable framework and network architecture capable of predicting on subsets of the available platforms, bands, or temporal ranges it was trained on. Our system, called WATCH, is highly general and can be applied to a variety of geospatial tasks. In this work, we analyze the performance of WATCH using the recent IARPA SMART public dataset and metrics. We focus primarily on the problem of broad area search for heavy construction sites. Experiments validate the robustness of WATCH during inference to limited sensor availability, as well the the ability to alter inference-time spatial or temporal sampling. WATCH is open source and available for use on this or other remote sensing problems. Code and model weights are available at: https://gitlab.kitware.com/computer-vision/geowatch"
Wakening Past Concepts Without Past Data: Class-Incremental Learning From Online Placebos,"Yaoyao Liu, Yingying Li, Bernt Schiele, Qianru Sun","Max Planck Institute for Informatics, Saarland Informatics Campus; Singapore Management University; University of Illinois Urbana-Champaign; Johns Hopkins University",100.0,"Germany, Singapore, USA",0.0,,"Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that ""using new class data for KD"" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by ""using the placebos of old classes for KD"", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-forward computation of KD. We formulate the policy training process as an online Markov Decision Process (MDP), and introduce an online learning algorithm to solve this MDP problem without causing much computation costs. In experiments, we show that our method 1) is surprisingly effective even when there is no class overlap between placebos and original old class data, 2) does not require any additional supervision or memory budget, and 3) significantly outperforms a number of top-performing CIL methods, in particular when using lower memory budgets for old class exemplars, e.g., five exemplars per class.",https://openaccess.thecvf.com/content/WACV2024/html/Liu_Wakening_Past_Concepts_Without_Past_Data_Class-Incremental_Learning_From_Online_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Liu_Wakening_Past_Concepts_Without_Past_Data_Class-Incremental_Learning_From_Online_WACV_2024_paper.pdf,,https://github.com/yaoyao-liu/online-placebos,2310.16115,main,Poster,https://ieeexplore.ieee.org/document/10484461/,"['Training', 'Learning systems', 'Adaptation models', 'Costs', 'Markov decision processes', 'Memory management', 'Streaming media']","['Placebo', 'Class-incremental Learning', 'Computational Cost', 'Online Learning', 'Markov Decision Process', 'Online Learning Algorithm', 'Training Data', 'Test Data', 'Evaluation Of Function', 'Percentage Points', 'Feature Space', 'Test Accuracy', 'Phase Data', 'Unlabeled Data', 'Reward Function', 'Policy Learning', 'Buffer Size', 'Unlabeled Images', 'Distillation Loss', 'Phase Increment', 'Online Learning Methods']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",1,"Not forgetting old class knowledge is a key challenge for class-incremental learning (CIL) when the model continuously adapts to new classes. A common technique to address this is knowledge distillation (KD), which penalizes prediction inconsistencies between old and new models. Such prediction is made with almost new class data, as old class data is extremely scarce due to the strict memory limitation in CIL. In this paper, we take a deep dive into KD losses and find that ""using new class data for KD"" not only hinders the model adaption (for learning new classes) but also results in low efficiency for preserving old class knowledge. We address this by ""using the placebos of old classes for KD"", where the placebos are chosen from a free image stream, such as Google Images, in an automatical and economical fashion. To this end, we train an online placebo selection policy to quickly evaluate the quality of streaming images (good or bad placebos) and use only good ones for one-time feed-forward computation of KD. We formulate the policy training process as an online Markov Decision Process (MDP), and introduce an online learning algorithm to solve this MDP problem without causing much computation costs. In experiments, we show that our method 1) is surprisingly effective even when there is no class overlap between placebos and original old class data, 2) does not require any additional supervision or memory budget, and 3) significantly outperforms a number of top-performing CIL methods, in particular when using lower memory budgets for old class exemplars, e.g., five exemplars per class.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
WalkFormer: Point Cloud Completion via Guided Walks,"Mohang Zhang, Yushi Li, Rong Chen, Yushan Pan, Jia Wang, Yunzhe Wang, Rong Xiang","Dalian Maritime University; School of Advanced Technology, Xi’an Jiaotong-Liverpool University; Suzhou University of Science and Technology; The Hong Kong Polytechnic University",100.0,"China, Hong Kong",0.0,,"Point clouds are often sparse and incomplete in real-world scenarios. The prevailing methods for point cloud completion typically rely on encoding the partial points and then decoding complete points from a global feature vector, which might lose the existing patterns and elaborate structures. To address these issues, we propose WalkFormer, a novel approach to predict complete point clouds through a partial deformation process. Concretely, our method samples locally dominant points based on feature similarity and moves the points to form the missing part. Since these points maintain representative information of the surrounding structures, they are appropriately selected as the starting points for multiple guided walks. Furthermore, we design a Route Transformer module to exploit and aggregate the walk information with topological relations. These guided walks facilitate the learning of long-range dependencies for predicting shape deformation. Qualitative and quantitative evaluations demonstrate that our proposed approach achieves superior performance compared to state-of-the-art methods in the 3D point cloud completion task.",https://openaccess.thecvf.com/content/WACV2024/html/Zhang_WalkFormer_Point_Cloud_Completion_via_Guided_Walks_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_WalkFormer_Point_Cloud_Completion_via_Guided_Walks_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484164/,"['Point cloud compression', 'Three-dimensional displays', 'Deformation', 'Shape', 'Computational modeling', 'Predictive models', 'Transformers']","['Point Cloud', 'Point Cloud Completion', 'Transformer', 'Deformation Process', '3D Point Cloud', 'Long-range Dependencies', 'Partial Point', 'Missing Parts', 'Convolution', 'Previous Step', 'Input Features', 'K-nearest Neighbor', 'Feature Learning', '3D Space', 'Feature Points', '3D Coordinates', 'Displacement Vector', 'Neighborhood Characteristics', 'Topological Information', 'Encoder Layer', 'Shape Completion', 'Earth Mover’s Distance', 'Input Point Cloud', 'Subset Of Points', 'Chamfer Distance', 'Earth Mover', 'Raw Point Cloud', 'Points In Step', 'Contralateral']","['Algorithms', '3D computer vision']",3,"Point clouds are often sparse and incomplete in real-world scenarios. The prevailing methods for point cloud completion typically rely on encoding the partial points and then decoding complete points from a global feature vector, which might lose the existing patterns and elaborate structures. To address these issues, we propose WalkFormer, a novel approach to predict complete point clouds through a partial deformation process. Concretely, our method samples locally dominant points based on feature similarity and moves the points to form the missing part. Since these points maintain representative information of the surrounding structures, they are appropriately selected as the starting points for multiple guided walks. Furthermore, we design a Route Transformer module to exploit and aggregate the walk information with topological relations. These guided walks facilitate the learning of long-range dependencies for predicting shape deformation. Qualitative and quantitative evaluations demonstrate that our proposed approach achieves superior performance compared to state-of-the-art methods in the 3D point cloud completion task."
Watch Where You Head: A View-Biased Domain Gap in Gait Recognition and Unsupervised Adaptation,"Gavriel Habib, Noa Barzilay, Or Shimshi, Rami Ben-Ari, Nir Darshan","OriginAI, Israel",0.0,,100.0,Israel,"Gait Recognition is a computer vision task aiming to identify people by their walking patterns. Although existing methods often show high performance on specific datasets, they lack the ability to generalize to unseen scenarios. Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain. There are only a few works on UDA for gait recognition proposing solutions to limited scenarios. In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, caused by the bias in the target domain to viewing angle or walking direction. We then suggest a remedy to reduce this bias with a novel triplet selection strategy combined with curriculum learning. To this end, we present Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA). We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL, justifying the view bias and showing the superiority of our proposed method over prior UDA works.",https://openaccess.thecvf.com/content/WACV2024/html/Habib_Watch_Where_You_Head_A_View-Biased_Domain_Gap_in_Gait_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Habib_Watch_Where_You_Head_A_View-Biased_Domain_Gap_in_Gait_WACV_2024_paper.pdf,,,2307.06751,main,Poster,https://ieeexplore.ieee.org/document/10484485/,"['Legged locomotion', 'Computer vision', 'Adaptation models', 'Computational modeling', 'Task analysis', 'Gait recognition']","['Domain Gap', 'Unsupervised Adaptation', 'Gait Recognition', 'Viewing Angle', 'Target Domain', 'Domain Adaptation', 'Source Domain', 'Curriculum Learning', 'Walking Pattern', 'Walking Direction', 'Unlabeled Target Domain', 'Training Set', 'Positive Samples', 'Validation Set', 'Negative Samples', 'Entire Sequence', 'Latent Space', 'Direct Test', 'RGB Images', 'Model-based Approach', 'Triplet Loss', '2D Keypoints', 'Unordered Set', 'Cross-view', 'Similar Views', 'Yaw Angle', 'Diverse Views', 'Re-identification Task', 'Target Distribution', 'Number Of Triplets']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Gait Recognition is a computer vision task aiming to identify people by their walking patterns. Although existing methods often show high performance on specific datasets, they lack the ability to generalize to unseen scenarios. Unsupervised Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised manner on a source domain, to an unlabelled target domain. There are only a few works on UDA for gait recognition proposing solutions to limited scenarios. In this paper, we reveal a fundamental phenomenon in adaptation of gait recognition models, caused by the bias in the target domain to viewing angle or walking direction. We then suggest a remedy to reduce this bias with a novel triplet selection strategy combined with curriculum learning. To this end, we present Gait Orientation-based method for Unsupervised Domain Adaptation (GOUDA). We provide extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP, GREW, and Gait3D, and on three backbones, GaitSet, Gait-Part, and GaitGL, justifying the view bias and showing the superiority of our proposed method over prior UDA works."
WaveMixSR: Resource-Efficient Neural Network for Image Super-Resolution,"Pranav Jeevan, Akella Srinidhi, Pasunuri Prathiba, Amit Sethi",Indian Institute of Technology Bombay,100.0,India,0.0,,"Image super-resolution research recently has been dominated by transformer models which need higher computational resources than CNNs due to the quadratic complexity of self-attention. We propose a new neural network -- WaveMixSR -- for image super-resolution based on the WaveMix architecture which uses a 2D-discrete wavelet transform for spatial token-mixing. Unlike transformer-based models, WaveMixSR does not unroll the image as a sequence of pixels/patches. It uses the inductive bias of convolutions along with the lossless token-mixing property of wavelet transform to achieve higher performance while requiring fewer resources and training data. We compare the performance of our network with other state-of-the-art methods for image super-resolution. Our experiments show that WaveMixSR achieves competitive performance in all datasets and reaches state-of-the-art performance in the BSD100 dataset on multiple super-resolution tasks. Our model is able to achieve this performance using less training data and computational resources while maintaining high parameter efficiency compared to current state-of-the-art models.",https://openaccess.thecvf.com/content/WACV2024/html/Jeevan_WaveMixSR_Resource-Efficient_Neural_Network_for_Image_Super-Resolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jeevan_WaveMixSR_Resource-Efficient_Neural_Network_for_Image_Super-Resolution_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484459/,"['Wavelet transforms', 'Computational modeling', 'Superresolution', 'Neural networks', 'Training data', 'Transformers', 'Data models']","['Neural Network', 'Super-resolution', 'Training Data', 'Convolutional Neural Network', 'Computational Resources', 'Wavelet Transform', 'Transformer Model', 'Lossless', 'Performance In Dataset', 'Inductive Bias', 'Super-resolution Task', 'High Computational Resources', 'High-resolution Images', 'Convolutional Layers', 'Feature Maps', 'Image Reconstruction', 'Number Of Images', 'Receptive Field', 'Multilayer Perceptron', 'Natural Images', 'Low-resolution Images', 'Bilinear Interpolation', 'Vision Transformer', 'Haar Wavelet', 'Huber Loss', 'Channel Attention', 'Bicubic Interpolation', 'Vision Tasks', 'Residual Network', 'Single Image Super-resolution']","['Applications', 'Visualization', 'Applications', 'Social good']",4,"Image super-resolution research recently has been dominated by transformer models which need higher computational resources than CNNs due to the quadratic complexity of self-attention. We propose a new neural network – WaveMixSR – for image super-resolution based on the WaveMix architecture which uses a 2D-discrete wavelet transform for spatial token-mixing. Unlike transformer-based models, WaveMixSR does not unroll the image as a sequence of pixels/patches. It uses the inductive bias of convolutions along with the lossless token-mixing property of wavelet transform to achieve higher performance while requiring fewer resources and training data. We compare the performance of our network with other state-of-the-art methods for image super-resolution. Our experiments show that WaveMixSR achieves competitive performance in all datasets and reaches state-of-the-art performance in the BSD100 dataset on multiple super-resolution tasks. Our model is able to achieve this performance using less training data and computational resources while maintaining high parameter efficiency compared to current state-of-the-art models."
Weakly-Supervised Deepfake Localization in Diffusion-Generated Images,"Dragoș-Constantin Țânțaru, Elisabeta Oneață, Dan Oneață",University Politehnica of Bucharest; Bitdefender,50.0,Romania,50.0,Romania,"The remarkable generative capabilities of denoising diffusion models have raised new concerns regarding the authenticity of the images we see every day on the Internet. However, the vast majority of existing deepfake detection models are tested against previous generative approaches (e.g. GAN) and usually provide only a ""fake"" or ""real"" label per image. We believe a more informative output would be to augment the per-image label with a localization map indicating which regions of the input have been manipulated. To this end, we frame this task as a weakly-supervised localization problem and identify three main categories of methods (based on either explanations, local scores or attention), which we compare on an equal footing by using the Xception network as the common backbone architecture. We provide a careful analysis of all the main factors that parameterize the design space: choice of method, type of supervision, dataset and generator used in the creation of manipulated images; our study is enabled by constructing datasets in which only one of the components is varied. Our results show that weakly-supervised localization is attainable, with the best performing detection method (based on local scores) being less sensitive to the looser supervision than to the mismatch in terms of dataset or generator.",https://openaccess.thecvf.com/content/WACV2024/html/Tantaru_Weakly-Supervised_Deepfake_Localization_in_Diffusion-Generated_Images_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Tantaru_Weakly-Supervised_Deepfake_Localization_in_Diffusion-Generated_Images_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483724/,"['Location awareness', 'Training', 'Measurement', 'Deepfakes', 'Noise reduction', 'Generative adversarial networks', 'Generators']","['Detection Methods', 'Method Of Choice', 'Diffusion Model', 'General Types', 'Local Map', 'Input Regions', 'Training Data', 'Detection Performance', 'Intersection Over Union', 'Reversible Process', 'Local Method', 'Latent Space', 'Image Generation', 'Local Use', 'Variational Autoencoder', 'Diffusion Imaging', 'Source Dataset', 'Left Plot', 'Level Of Supervision', 'Off-diagonal Entries', 'Image-level Labels', 'Fake Images', 'Fake Data']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Adversarial learning', 'adversarial attack and defense methods']",2,"The remarkable generative capabilities of denoising diffusion models have raised new concerns regarding the authenticity of the images we see every day on the Internet. However, the vast majority of existing deepfake detection models are tested against previous generative approaches (e.g. GAN) and usually provide only a ""fake"" or ""real"" label per image. We believe a more informative output would be to augment the per-image label with a localization map indicating which regions of the input have been manipulated. To this end, we frame this task as a weakly-supervised localization problem and identify three main categories of methods (based on either explanations, local scores or attention), which we compare on an equal footing by using the Xception network as the common backbone architecture. We provide a careful analysis of all the main factors that parameterize the design space: choice of method, type of supervision, dataset and generator used in the creation of manipulated images; our study is enabled by constructing datasets in which only one of the components is varied. Our results show that weakly-supervised localization is attainable, with the best performing detection method (based on local scores) being less sensitive to the looser supervision than to the mismatch in terms of dataset or generator."
Weakly-Supervised Representation Learning for Video Alignment and Analysis,"Guy Bar-Shalom, George Leifman, Michael Elad","Verily, melad@google.com; Verily, gleifman@google.com; Verily, guy.b@cs.technion.ac.il, 1Technion - Israel Institute of Technology, Department of Computer Science",33.33333333333333,Israel,66.66666666666667,USA,"Many tasks in video analysis and understanding boil down to the need for frame-based feature learning, aiming to encapsulate the relevant visual content so as to enable simpler and easier subsequent processing. While supervised strategies for this learning task can be envisioned, self and weakly-supervised alternatives are preferred due to the difficulties in getting labeled data. This paper introduces LRProp -- a novel weakly-supervised representation learning approach, with an emphasis on the application of temporal alignment between pairs of videos of the same action category. The proposed approach uses a transformer encoder for extracting frame-level features, and employs the DTW algorithm within the training iterations in order to identify the alignment path between video pairs. Through a process referred to as ""pair-wise position propagation"", the probability distributions of these correspondences per location are matched with the similarity of the frame-level features via KL-divergence minimization. The proposed algorithm uses also a regularized SoftDTW loss for better tuning the learned features. Our novel representation learning paradigm consistently outperforms the state of the art on temporal alignment tasks, establishing a new performance bar over several downstream video analysis applications.",https://openaccess.thecvf.com/content/WACV2024/html/Bar-Shalom_Weakly-Supervised_Representation_Learning_for_Video_Alignment_and_Analysis_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bar-Shalom_Weakly-Supervised_Representation_Learning_for_Video_Alignment_and_Analysis_WACV_2024_paper.pdf,,,2302.04064,main,Poster,https://ieeexplore.ieee.org/document/10483582/,"['Representation learning', 'Measurement', 'Training', 'Visualization', 'Heuristic algorithms', 'Feature extraction', 'Transformers']","['Representation Learning', 'Video Analysis', 'Feature Learning', 'Action Classes', 'Dynamic Time Warping', 'Transformer Encoder', 'Temporal Alignment', 'Training Data', 'Objective Function', 'Support Vector Machine', 'Validation Set', 'Data Augmentation', 'Latent Space', 'Video Frames', 'Average Precision', 'Transformer Model', 'Accuracy Metrics', 'Phase Progression', 'Loss Of Components', 'Self-supervised Learning', 'Pair Of Frames', 'Pretext Task', 'Classification Phase', 'Weak Supervision', 'Frame Index', 'Feature Alignment']","['Algorithms', 'Video recognition and understanding', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",,"Many tasks in video analysis and understanding boil down to the need for frame-based feature learning, aiming to encapsulate the relevant visual content so as to enable simpler and easier subsequent processing. While supervised strategies for this learning task can be envisioned, self and weakly-supervised alternatives are preferred due to the difficulties in getting labeled data. This paper introduces LRProp – a novel weakly-supervised representation learning approach, with an emphasis on the application of temporal alignment between pairs of videos of the same action category. The proposed approach uses a transformer encoder for extracting frame-level features, and employs the DTW algorithm within the training iterations in order to identify the alignment path between video pairs. Through a process referred to as ""pair-wise position propagation"", the probability distributions of these correspondences per location are matched with the similarity of the frame-level features via KL-divergence minimization. The proposed algorithm uses also a regularized SoftDTW loss for better tuning the learned features. Our novel representation learning paradigm consistently outperforms the state of the art on temporal alignment tasks, establishing a new performance bar over several downstream video analysis applications."
What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion,"Pu Cao, Lu Yang, Dongxv Liu, Xiaoya Yang, Tianrui Huang, Qing Song","Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications, Metavatar",100.0,China,0.0,,"Recently, inversion methods have been exploring the incorporation of additional high-rate information from pretrained generators (such as weights or intermediate features) to improve the refinement of inversion and editing results from embedded latent codes. While such techniques have shown reasonable improvements in reconstruction, they often lead to a decrease in editing capability, especially when dealing with complex images that contain occlusions, detailed backgrounds, and artifacts. A vital crux is refining inversion results, avoiding editing capability degradation. To address this problem, we propose a novel refinement mechanism called Domain-Specific Hybrid Refinement (DHR), which draws on the advantages and disadvantages of two mainstream refinement techniques. We find that the weight modulation can gain favorable editing results but is vulnerable to these complex image areas and feature modulation is efficient at reconstructing. Hence, we divide the image into two domains and process them with these two methods separately. We first propose a Domain-Specific Segmentation module to automatically segment images into in-domain and out-of-domain parts according to their invertibility and editability without additional data annotation, where our hybrid refinement process aims to maintain the editing capability for in-domain areas and improve fidelity for both of them. We achieve this through Hybrid Modulation Refinement, which respectively refines these two domains by weight modulation and feature modulation. Our proposed method is compatible with all latent code embedding methods. Extension experiments demonstrate that our approach achieves state-of-the-art in real image inversion and editing. Code is available at https://github.com/caopulan/Domain-Specific_Hybrid_Refinement_Inversion.",https://openaccess.thecvf.com/content/WACV2024/html/Cao_What_Decreases_Editing_Capability_Domain-Specific_Hybrid_Refinement_for_Improved_GAN_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cao_What_Decreases_Editing_Capability_Domain-Specific_Hybrid_Refinement_for_Improved_GAN_WACV_2024_paper.pdf,,https://github.com/caopulan/Domain-Specific_Hybrid_Refinement_Inversion,2301.12141,main,Poster,https://ieeexplore.ieee.org/document/10483745/,"['Degradation', 'Image segmentation', 'Computer vision', 'Codes', 'Annotations', 'Modulation', 'Generators']","['Editing Capabilities', 'Image Segmentation', 'Annotation Data', 'Inverse Method', 'Complex Image', 'Intermediate Features', 'Refinement Process', 'Feature Module', 'Weight Space', 'Latent Code', 'Promising Results', 'Input Image', 'Generative Adversarial Networks', 'Latent Space', 'Segmentation Results', 'Image Distortion', 'Divide-and-conquer', 'Spatial Details', 'Refinement Method', 'Real-world Images', 'Inversion Results', 'Cause Of Degradation', 'Encoding Processes', 'Parallel Optimization', 'Lipstick', 'Output Space']","['Algorithms', 'Computational photography', 'image and video synthesis', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc']",1,"Recently, inversion methods have been exploring the incorporation of additional high-rate information from pretrained generators (such as weights or intermediate features) to improve the refinement of inversion and editing results from embedded latent codes. While such techniques have shown reasonable improvements in reconstruction, they often lead to a decrease in editing capability, especially when dealing with complex images that contain occlusions, detailed backgrounds, and artifacts. To address this problem, we propose a novel refinement mechanism called Domain-Specific Hybrid Refinement (DHR), which draws on the advantages and disadvantages of two mainstream refinement techniques. We find that the weight modulation can gain favorable editing results but is vulnerable to these complex image areas and feature modulation is efficient at reconstructing. Hence, we divide the image into two domains and process them with these two methods separately. We first propose a Domain-Specific Segmentation module to automatically segment images into in-domain and out-of-domain parts according to their invertibility and editability without additional data annotation, where our hybrid refinement process aims to maintain the editing capability for in-domain areas and improve fidelity for both of them. We achieve this through Hybrid Modulation Refinement, which respectively refines these two domains by weight modulation and feature modulation. Our proposed method is compatible with all latent code embedding methods. Extension experiments demonstrate that our approach achieves state-of-the-art in real image inversion and editing. Code is available at https: //github.com/caopulan/Domain-Specific_ Hybrid_Refinement_Inversion."
What's Outside the Intersection? Fine-Grained Error Analysis for Semantic Segmentation Beyond IoU,"Maximilian Bernhard, Roberto Amoroso, Yannic Kindermann, Lorenzo Baraldi, Rita Cucchiara, Volker Tresp, Matthias Schubert","LMU Munich; LMU Munich, MCML; University of Modena and Reggio Emilia",100.0,"Germany, Italy",0.0,,"Semantic segmentation represents a fundamental task in computer vision with various application areas such as autonomous driving, medical imaging, or remote sensing. For evaluating and comparing semantic segmentation models, the mean intersection over union (mIoU) is currently the gold standard. However, while mIoU serves as a valuable benchmark, it does not offer insights into the types of errors incurred by a model. Moreover, different types of errors may have different impacts on downstream applications. To address this issue, we propose an intuitive method for the systematic categorization of errors, thereby enabling a fine-grained analysis of semantic segmentation models. Since we assign each erroneous pixel to precisely one error type, our method seamlessly extends the popular IoU-based evaluation by shedding more light on the false positive and false negative predictions. Our approach is model- and dataset-agnostic, as it does not rely on additional information besides the predicted and ground-truth segmentation masks. In our experiments, we demonstrate that our method accurately assesses model strengths and weaknesses on a quantitative basis, thus reducing the dependence on time-consuming qualitative model inspection. We analyze a variety of state-of-the-art semantic segmentation models, revealing systematic differences across various architectural paradigms. Exploiting the gained insights, we showcase that combining two models with complementary strengths in a straightforward way is sufficient to consistently improve mIoU, even for models setting the current state of the art on ADE20K. We release a toolkit for our evaluation method at https://github.com/mxbh/beyond-iou.",https://openaccess.thecvf.com/content/WACV2024/html/Bernhard_Whats_Outside_the_Intersection_Fine-Grained_Error_Analysis_for_Semantic_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Bernhard_Whats_Outside_the_Intersection_Fine-Grained_Error_Analysis_for_Semantic_Segmentation_WACV_2024_paper.pdf,,https://github.com/mxbh/beyond-iou,,main,Poster,,,,,,
What's in the Flow? Exploiting Temporal Motion Cues for Unsupervised Generic Event Boundary Detection,"Sourabh Vasant Gothe, Vibhav Agarwal, Sourav Ghosh, Jayesh Rajkumar Vachhani, Pranay Kashyap, Barath Raj Kandur Raja","Samsung R&D Institute Bangalore, India",100.0,India,0.0,,"Generic Event Boundary Detection (GEBD) task aims to recognize generic, taxonomy-free boundaries that segment a video into meaningful events. Current methods typically involve a neural model trained on a large volume of data, demanding substantial computational power and storage space. We explore two pivotal questions pertaining to GEBD: Can non-parametric algorithms outperform unsupervised neural methods? Does motion information alone suffice for high performance? This inquiry drives us to algorithmically harness motion cues for identifying generic event boundaries in videos. In this work, we propose FlowGEBD, a non-parametric, unsupervised technique for GEBD. Our approach entails two algorithms utilizing optical flow: (i) Pixel Tracking and (ii) Flow Normalization. By conducting thorough experimentation on the challenging Kinetics-GEBD and TAPOS datasets, our results establish FlowGEBD as the new state-of-the-art (SOTA) among unsupervised methods. FlowGEBD exceeds the neural models on the Kinetics-GEBD dataset by obtaining an F1@0.05 score of 0.713 with an absolute gain of 31.7% compared to the unsupervised baseline and achieves an average F1 score of 0.623 on the TAPOS validation dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Gothe_Whats_in_the_Flow_Exploiting_Temporal_Motion_Cues_for_Unsupervised_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gothe_Whats_in_the_Flow_Exploiting_Temporal_Motion_Cues_for_Unsupervised_WACV_2024_paper.pdf,,,,main,Poster,,,,,,
When 3D Bounding-Box Meets SAM: Point Cloud Instance Segmentation With Weak-and-Noisy Supervision,"Qingtao Yu, Heming Du, Chen Liu, Xin Yu","School of Information and Electrical Engineering, The University of Queensland; College of Engineering Computing and Cybernetics, Australian National University",100.0,Australia,0.0,,"Learning from bounding-boxes annotations has shown great potential in weakly-supervised 3D point cloud in- stance segmentation. However, we observed that existing methods would suffer severe performance degradation with perturbed bounding box annotations. To tackle this is- sue, we propose a complementary image prompt-induced weakly-supervised point cloud instance segmentation (CIP- WPIS) method. CIP-WPIS leverages pretrained knowledge embedded in the 2D foundation model SAM and 3D geo- metric prior to achieve accurate point-wise instance labels from the bounding box annotations. Specifically, CIP-WPIS first selects image views in which 3D candidate points of an instance are fully visible. Then, we generate complemen- tary background and foreground prompts from projections to obtain SAM 2D instance mask predictions. According to these, we assign the confidence values to points indicating the likelihood of points belonging to the instance. Furthermore, we utilize 3D geometric homogeneity provided by superpoints to decide the final instance label assignments. In this fashion, we achieve high-quality 3D point-wise in- stance labels. Extensive experiments on both Scannet-v2 and S3DIS benchmarks proves that our method not only achieves state-of-the-art performance for bounding-boxes supervised point cloud instance segmentation, but also exhibits robustness against noisy 3D bounding-box annotations.",https://openaccess.thecvf.com/content/WACV2024/html/Yu_When_3D_Bounding-Box_Meets_SAM_Point_Cloud_Instance_Segmentation_With_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Yu_When_3D_Bounding-Box_Meets_SAM_Point_Cloud_Instance_Segmentation_With_WACV_2024_paper.pdf,,,2309.00828,main,Poster,,,,,,
WildlifeDatasets: An Open-Source Toolkit for Animal Re-Identification,"Vojtěch Čermák, Lukas Picek, Lukáš Adam, Kostas Papafitsoros",University of West Bohemia; Queen Mary University of London; Czech Technical University in Prague,100.0,"Czech Republic, UK",0.0,,"In this paper, we present WildlifeDatasets - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub.",https://openaccess.thecvf.com/content/WACV2024/html/Cermak_WildlifeDatasets_An_Open-Source_Toolkit_for_Animal_Re-Identification_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Cermak_WildlifeDatasets_An_Open-Source_Toolkit_for_Animal_Re-Identification_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483925/,"['Deep learning', 'Computer vision', 'Analytical models', 'Biological system modeling', 'Computational modeling', 'Wildlife', 'Libraries']","['Open-source Toolkit', 'Machine Learning', 'Deep Learning', 'Computer Vision', 'Deep Learning Approaches', 'Wide Range Of Species', 'Local Descriptors', 'Local Learning', 'Easy Integration', 'Fine-tuned Model', 'Foundation Model', 'Training Set', 'Training Data', 'Development Of Methods', 'Hyperparameter Tuning', 'Competitive Performance', 'Vision Tasks', 'Progress In The Field', 'Metric Learning', 'Query Set', 'Triplet Loss', 'Query Image', 'Unseen Domains', 'Backbone Architecture', 'Giraffe', 'Re-identification Methods']","['Applications', 'Environmental monitoring / climate change / ecology', 'Applications', 'Animals / Insects']",4,"In this paper, we present WildlifeDatasets – an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We show-case the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species – MegaDescriptor – that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub."
Wino Vidi Vici: Conquering Numerical Instability of 8-Bit Winograd Convolution for Accurate Inference Acceleration on Edge,"Pierpaolo Mori, Lukas Frickenstein, Shambhavi Balamuthu Sampath, Moritz Thoma, Nael Fasfous, Manoj Rohit Vemparala, Alexander Frickenstein, Christian Unger, Walter Stechele, Daniel Mueller-Gritschneder, Claudio Passerone","BMW Group, Munich, Germany; Technical University of Munich, Munich, Germany; Politecnico Di Torino, Turin, Italy",66.66666666666666,"Germany, Italy",33.33333333333334,Germany,"Winograd-based convolution can reduce the total number of operations needed for convolutional neural network (CNN) inference on edge devices. Most edge hardware accelerators use low-precision, 8-bit integer arithmetic units to improve energy efficiency and latency. This makes CNN quantization a critical step before deploying the model on such an edge device. To extract the benefits of fast Winograd-based convolution and efficient integer quantization, the two approaches must be combined. Research has shown that the transform required to execute convolutions in the Winograd domain results in numerical instability and severe accuracy degradation when combined with quantization, making the two techniques incompatible on edge hardware. This paper proposes a novel training scheme to achieve efficient Winograd-accelerated, quantized CNNs. 8-bit quantization is applied to all the intermediate results of the Winograd convolution without sacrificing task-related accuracy. This is achieved by introducing clipping factors in the intermediate quantization stages as well as using the complex numerical system to improve the transform. We achieve 2.8x and 2.1x reduction in MAC operations on ResNet-20-CIFAR-10 and ResNet-18-ImageNet, respectively, with no accuracy degradation.",https://openaccess.thecvf.com/content/WACV2024/html/Mori_Wino_Vidi_Vici_Conquering_Numerical_Instability_of_8-Bit_Winograd_Convolution_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Mori_Wino_Vidi_Vici_Conquering_Numerical_Instability_of_8-Bit_Winograd_Convolution_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484072/,"['Training', 'Degradation', 'Quantization (signal)', 'Convolution', 'Transforms', 'Inference algorithms', 'Numerical models']","['Numerical Instability', 'Inference Accuracy', 'Winograd Convolution', 'Convolutional Neural Network', 'Edge Devices', 'Scaling Factor', 'Convolutional Layers', 'Feature Maps', 'Imaginary Part', 'Spatial Dimensions', 'Convolution Operation', 'Transformation Matrix', 'Convolutional Neural Network Model', 'Prediction Quality', 'Element-wise Multiplication', 'Numerical Errors', 'Integer Multiple', 'Output Feature Map', 'Expansion Factor', '32-bit Floating-point', 'Standard Convolution', 'Standard Quantum', 'Hardware Overhead', 'ResNet-18 Model', 'Tile Size', 'Training Iterations', 'Values Of Quantities', 'Matrix Multiplication', 'Real Reduction', 'Forward Pass']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",2,"Winograd-based convolution can reduce the total number of operations needed for convolutional neural network (CNN) inference on edge devices. Most edge hardware accelerators use low-precision, 8-bit integer arithmetic units to improve energy efficiency and latency. This makes CNN quantization a critical step before deploying the model on such an edge device. To extract the benefits of fast Winograd-based convolution and efficient integer quantization, the two approaches must be combined. Research has shown that the transform required to execute convolutions in the Winograd domain results in numerical instability and severe accuracy degradation when combined with quantization, making the two techniques incompatible on edge hardware. This paper proposes a novel training scheme to achieve efficient Winograd-accelerated, quantized CNNs. 8-bit quantization is applied to all the intermediate results of the Winograd convolution without sacrificing task-related accuracy. This is achieved by introducing clipping factors in the intermediate quantization stages as well as using the complex numerical system to improve the transform. We achieve 2.8× and 2.1× reduction in MAC operations on ResNet-20-CIFAR-10 and ResNet-18-ImageNet, respectively, with no accuracy degradation."
You Can Run but Not Hide: Improving Gait Recognition With Intrinsic Occlusion Type Awareness,"Ayush Gupta, Rama Chellappa",Johns Hopkins University,100.0,USA,0.0,,"While gait recognition has seen many advances in recent years, the occlusion problem has largely been ignored. This problem is especially important for gait recognition from uncontrolled outdoor sequences at range - since any small obstruction can affect the recognition system. Most current methods assume the availability of complete body information while extracting the gait features. When parts of the body are occluded, these methods may hallucinate and output a corrupted gait signature as they try to look for body parts which are not present in the input at all. To address this, we exploit the learned occlusion type while extracting identity features from videos. Thus, in this work, we propose an occlusion aware gait recognition method which can be used to model intrinsic occlusion awareness into potentially any state-of-the-art gait recognition method. Our experiments on the challenging GREW and BRIAR datasets show that networks enhanced with this occlusion awareness perform better at recognition tasks than their counterparts trained on similar occlusions.",https://openaccess.thecvf.com/content/WACV2024/html/Gupta_You_Can_Run_but_Not_Hide_Improving_Gait_Recognition_With_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Gupta_You_Can_Run_but_Not_Hide_Improving_Gait_Recognition_With_WACV_2024_paper.pdf,,,2312.02290,main,Poster,https://ieeexplore.ieee.org/document/10484217/,"['Computer vision', 'Detectors', 'Feature extraction', 'Data mining', 'Task analysis', 'Gait recognition', 'Videos']","['Type Of Occlusion', 'Gait Recognition', 'Body Parts', 'Advances In Recent Years', 'Experimental Section', 'Convolutional Layers', 'Large-scale Datasets', 'Image Sensor', 'Wearable Sensors', '3D Mesh', 'Channel Dimension', 'Stride Length', 'Intermediate Features', 'Illumination Changes', 'Mesh Model', 'Input Frames', 'Missing Parts', 'Uncontrolled Environment', 'Atmospheric Turbulence', 'Frame Features', 'Transient Mode', 'RGB Video', 'Irrelevant Details', 'Classification Head', 'Video Capture', 'Vision-based Methods']","['Algorithms', 'Biometrics', 'face', 'gesture', 'body pose', 'Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms']",5,"While gait recognition has seen many advances in recent years, the occlusion problem has largely been ignored. This problem is especially important for gait recognition from uncontrolled outdoor sequences at range - since any small obstruction can affect the recognition system. Most current methods assume the availability of complete body information while extracting the gait features. When parts of the body are occluded, these methods may hallucinate and output a corrupted gait signature as they try to look for body parts which are not present in the input at all. To address this, we exploit the learned occlusion type while extracting identity features from videos. Thus, in this work, we propose an occlusion aware gait recognition method which can be used to model intrinsic occlusion awareness into potentially any state-of-the-art gait recognition method. Our experiments on the challenging GREW and BRIAR datasets show that networks enhanced with this occlusion awareness perform better at recognition tasks than their counterparts trained on similar occlusions."
ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection,"Thinh Phan, Khoa Vo, Duy Le, Gianfranco Doretto, Donald Adjeroh, Ngan Le","FPT Software AI Center, Vietnam; West Virginia University, Morgantown, West Virginia, USA; AICV Lab, University of Arkansas, Fayetteville, Arkansas, USA",66.66666666666666,USA,33.33333333333334,Vietnam,"Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising openset setting by leveraging large-scale contrastive visuallanguage (ViL) pretrained models. However, existing zeroshot TAD methods have limitations on how to properly construct the strong relationship between two Interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zeroshot proposal classification. The former is a Transformerbased module that detects action events while selectively collecting crucial semantic embeddings for later Recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach's superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories. Code is available at https: //github.com/UARK-AICV/ZEETAD.",https://openaccess.thecvf.com/content/WACV2024/html/Phan_ZEETAD_Adapting_Pretrained_Vision-Language_Model_for_Zero-Shot_End-to-End_Temporal_Action_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Phan_ZEETAD_Adapting_Pretrained_Vision-Language_Model_for_Zero-Shot_End-to-End_Temporal_Action_WACV_2024_paper.pdf,,https://github.com/UARK-AICV/ZEETAD,2311.00729,main,Poster,,,,,,
ZIGNeRF: Zero-Shot 3D Scene Representation With Invertible Generative Neural Radiance Fields,"Kanghyeok Ko, Minhyeok Lee","Chung-Ang University, Seoul, South Korea",100.0,South Korea,0.0,,"Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing multi-view images by learning the distribution of a set of unposed images. Despite the aptitude of existing Generative NeRFs in generating 3D-consistent high-quality random samples within data distribution, the creation of a 3D representation of a singular input image remains a formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative model that executes zero-shot Generative Adversarial Network (GAN) inversion for the generation of multi-view images from a single out-of-distribution image. The model is underpinned by a novel inverter that maps out-of-domain images into the latent code of the generator manifold. Notably, ZIGNeRF is capable of disentangling the object from the background and executing 3D operations such as 360-degree rotation or depth and horizontal translation. The efficacy of our model is validated using multiple real-image datasets: Cats, AFHQ, CelebA, CelebA-HQ, and CompCars.",https://openaccess.thecvf.com/content/WACV2024/html/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Ko_ZIGNeRF_Zero-Shot_3D_Scene_Representation_With_Invertible_Generative_Neural_Radiance_WACV_2024_paper.pdf,,,2306.02741,main,Poster,https://ieeexplore.ieee.org/document/10483955/,"['Manifolds', 'Training', 'Solid modeling', 'Adaptation models', 'Three-dimensional displays', 'Codes', 'Generative adversarial networks']","['Invertible', '3D Scene', 'Neural Radiance Fields', 'Input Image', 'Generative Adversarial Networks', 'Inverter', 'Image Generation', 'Image Representation', 'Latent Code', 'Multi-view Images', 'Horizontal Translation', 'Random Sampling', 'Image Reconstruction', '3D Reconstruction', 'Skin Color', 'Multilayer Perceptron', 'Latent Space', 'Final Image', 'Residual Block', 'Real-world Images', 'Fréchet Inception Distance', 'Single Scene', 'Camera Pose', 'Perceptual Loss', 'Neural Field', 'Text Encoder', 'Structural Similarity Index Measure', 'Unseen Images', 'Inception Distance', 'Generative Adversarial Networks Loss']","['Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Algorithms', '3D computer vision', 'Algorithms', 'Computational photography', 'image and video synthesis']",1,"Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing multi-view images by learning the distribution of a set of unposed images. Despite the aptitude of existing Generative NeRFs in generating 3D-consistent high-quality random samples within data distribution, the creation of a 3D representation of a singular input image remains a formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative model that executes zero-shot Generative Adversarial Network (GAN) inversion for the generation of multi-view images from a single out-of-distribution image. The model is underpinned by a novel inverter that maps out-of-domain images into the latent code of the generator manifold. Notably, ZIGNeRF is capable of disentangling the object from the background and executing 3D operations such as 360degree rotation or depth and horizontal translation. The efficacy of our model is validated using multiple real-image datasets: Cats, AFHQ, CelebA, CelebA-HQ, and CompCars."
ZRG: A Dataset for Multimodal 3D Residential Rooftop Understanding,"Isaac Corley, Jonathan Lwowski, Peyman Najafirad",University of Texas at San Antonio; Zeitview,50.0,USA,50.0,USA,"A crucial part of any home is the roof over our heads to protect us from the elements. In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset for residential rooftop understanding. ZRG is a large-scale residential rooftop inspection dataset of over 20k properties from across the U.S. and includes high resolution aerial orthomosaics, digital surface models (DSM), colored point clouds, and 3D roof wireframe annotations. We provide an in-depth analysis and perform several experimental baselines including roof outline extraction, monocular height estimation, and planar roof structure extraction, to illustrate a few of the numerous applications unlocked by this dataset.",https://openaccess.thecvf.com/content/WACV2024/html/Corley_ZRG_A_Dataset_for_Multimodal_3D_Residential_Rooftop_Understanding_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Corley_ZRG_A_Dataset_for_Multimodal_3D_Residential_Rooftop_Understanding_WACV_2024_paper.pdf,,https://github.com/isaaccorley/zrg,2304.13219,main,Poster,https://ieeexplore.ieee.org/document/10484013/,"['Point cloud compression', 'Geometry', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Annotations', 'Estimation']","['Residential Rooftops', 'High-resolution', 'Digital Elevation Model', 'Large-scale Datasets', 'Point Cloud', 'Planar Structure', 'Monocular', 'Height Estimation', 'Roof Structure', 'Spatial Resolution', 'Low Resolution', '3D Reconstruction', 'Increase In Performance', 'Unmanned Aerial Vehicles', 'Segmentation Model', 'Aerial Images', 'Semantic Segmentation', 'Humanitarian Aid', '3D Mesh', 'Solar Panels', 'Graph Neural Networks', 'Model Metrics', 'Baseline Experiments', 'Mean Average Precision', '3D Information', 'Residential Property', 'Residential Structures']","['Algorithms', 'Datasets and evaluations', 'Algorithms', '3D computer vision']",,"A crucial part of any home is the roof over our heads to protect us from the elements. In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset for residential rooftop understanding. ZRG is a large-scale residential rooftop dataset of over 20k properties collected through roof inspections from across the U.S. and contains multiple modalities including high resolution aerial orthomosaics, digital surface models (DSM), colored point clouds, and 3D roof wireframe annotations. We provide an in-depth analysis and perform several experimental baselines including roof outline extraction, monocular height estimation, and planar roof structure extraction, to illustrate a few of the numerous potential applications unlocked by this dataset.
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
Zero-Shot Building Attribute Extraction From Large-Scale Vision and Language Models,"Fei Pan, Sangryul Jeon, Brian Wang, Frank Mckenna, Stella X. Yu","University of Michigan, Ann Arbor; University of Michigan, Ann Arbor and University of California, Berkeley; University of California, Berkeley; Pusan National University",100.0,"South Korea, USA",0.0,,"Modern building recognition methods, exemplified by the BRAILS framework, utilize supervised learning to extract information from satellite and street-view images for image classification and semantic segmentation tasks. However, each task module requires human-annotated data, hindering the scalability and robustness to regional variations and annotation imbalances. In response, we propose a new zero-shot workflow for building attribute extraction that utilizes large-scale vision and language models to mitigate reliance on external annotations. The proposed workflow contains two key components: image-level captioning and segment-level captioning for the building images based on the vocabularies pertinent to structural and civil engineering. These two components generate descriptive captions by computing feature representations of the image and the vocabularies, and facilitating a semantic match between the visual and textual representations. Consequently, our framework offers a promising avenue to enhance AI-driven captioning for building attribute extraction in the structural and civil engineering domains, ultimately reducing reliance on human annotations while bolstering performance and adaptability.",https://openaccess.thecvf.com/content/WACV2024/html/Pan_Zero-Shot_Building_Attribute_Extraction_From_Large-Scale_Vision_and_Language_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Pan_Zero-Shot_Building_Attribute_Extraction_From_Large-Scale_Vision_and_Language_Models_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10484462/,"['Image segmentation', 'Vocabulary', 'Visualization', 'Satellites', 'Annotations', 'Computational modeling', 'Buildings']","['Language Model', 'Visual Model', 'Large-scale Models', 'Large-scale Language Models', 'Large-scale Vision', 'Vocabulary', 'Supervised Learning', 'Image Classification', 'Image Segmentation', 'Feature Representation', 'Visual Representation', 'Satellite Images', 'Civil Engineering', 'Engineering Structures', 'Street View Images', 'Training Set', 'Validation Set', 'Classification Task', 'Object Detection', 'Building Information Modelling', 'Roof Type', 'Building Information', 'Semantic Segmentation', 'Year Of Construction', 'Image Captioning', 'Self-supervised Learning', 'Image Embedding', 'Image Encoder', 'West Coast']","['Applications', 'Structural engineering / civil engineering', 'Algorithms', 'Image recognition and understanding']",,"Existing building recognition methods, exemplified by BRAILS, utilize supervised learning to extract information from satellite and street-view images for classification and segmentation. However, each task module requires human-annotated data, hindering the scalability and robustness to regional variations and annotation imbalances. In response, we propose a new zero-shot workflow for building attribute extraction that utilizes large-scale vision and language models to mitigate reliance on external annotations. The proposed workflow contains two key components: image-level captioning and segment-level captioning for the building images based on the vocabularies pertinent to structural and civil engineering. These two components generate descriptive captions by computing feature representations of the image and the vocabularies, and facilitating a semantic match between the visual and textual representations. Consequently, our framework offers a promising avenue to enhance AI-driven captioning for building attribute extraction in the structural and civil engineering domains, ultimately reducing reliance on human annotations while bolstering performance and adaptability."
Zero-Shot Video Moment Retrieval From Frozen Vision-Language Models,"Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, Yang Liu","Adobe Research; Queen Mary University of London; WICT, Peking University",66.66666666666666,"China, UK",33.33333333333334,USA,"Accurate video moment retrieval (VMR) requires universal visual-textual correlations that can handle unknown vocabulary and unseen scenes. However, the learned correlations are likely either biased when derived from a limited amount of moment-text data which is hard to scale up because of the prohibitive annotation cost (fully-supervised), or unreliable when only the video-text pairwise relationships are available without fine-grained temporal annotations (weakly supervised). Recently, the vision-language models (VLM) demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual-textual correlations derived from large-scale vision-language pairwise web data, which has also shown benefits to VMR by fine-tuning in the target domains. In this work, we propose a zero-shot method for adapting generalisable visual textual priors from arbitrary VLM to facilitate moment-text alignment, without the need for accessing the VMR data. To this end, we devise a conditional feature refinement module to generate boundary-aware visual features conditioned on text queries to enable better moment boundary understanding. Additionally, we design a bottom-up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex-query retrieval tasks into individual action retrievals, thereby maximizing the benefits of VLM. Extensive experiments conducted on three VMR benchmark datasets demonstrate the notable performance advantages of our zero-shot algorithm, especially in the novel-word and novel-location out-of-distribution setups.",https://openaccess.thecvf.com/content/WACV2024/html/Luo_Zero-Shot_Video_Moment_Retrieval_From_Frozen_Vision-Language_Models_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Luo_Zero-Shot_Video_Moment_Retrieval_From_Frozen_Vision-Language_Models_WACV_2024_paper.pdf,,,2309.00661,main,Poster,https://ieeexplore.ieee.org/document/10483682/,"['Vocabulary', 'Visualization', 'Computer vision', 'Correlation', 'Costs', 'Annotations', 'Transfer learning']","['Video Moment', 'Video Moment Retrieval', 'Vision-language Models', 'Visual Features', 'Large-scale Data', 'Target Domain', 'Feature Refinement', 'Superior Performance', 'Abrupt Changes', 'Intersection Over Union', 'Unsupervised Methods', 'Correlation Score', 'Limited Dataset', 'Large-scale Models', 'Test Split', 'Fundamental Unit', 'Cartesian Product', 'Visual Encoding', 'Text Encoder']","['Algorithms', 'Vision + language and/or other modalities', 'Algorithms', 'Video recognition and understanding']",2,"Accurate video moment retrieval (VMR) requires universal visual-textual correlations that can handle unknown vocabulary and unseen scenes. However, the learned correlations are likely either biased when derived from a limited amount of moment-text data which is hard to scale up because of the prohibitive annotation cost (fully-supervised), or unreliable when only the video-text pairwise relationships are available without fine-grained temporal annotations (weakly-supervised). Recently, the vision-language models (VLM) demonstrate a new transfer learning paradigm to benefit different vision tasks through the universal visual-textual correlations derived from large-scale vision-language pairwise web data, which has also shown benefits to VMR by fine-tuning in the target domains.In this work, we propose a zero-shot method for adapting generalisable visual-textual priors from arbitrary VLM to facilitate moment-text alignment, without the need for accessing the VMR data. To this end, we devise a conditional feature refinement module to generate boundary-aware visual features conditioned on text queries to enable better moment boundary understanding. Additionally, we design a bottom-up proposal generation strategy that mitigates the impact of domain discrepancies and breaks down complex-query retrieval tasks into individual action retrievals, thereby maximizing the benefits of VLM. Extensive experiments conducted on three VMR benchmark datasets demonstrate the notable performance advantages of our zero-shot algorithm, especially in the novel-word and novel-location out-of-distribution setups."
dacl10k: Benchmark for Semantic Bridge Damage Segmentation,"Johannes Flotzinger, Philipp J. Rösch, Thomas Braml",University of the Bundeswehr Munich,100.0,Germany,0.0,,"Reliably identifying reinforced concrete defects (RCDs) plays a crucial role in assessing the structural integrity, traffic safety, and long-term durability of concrete bridges, which represent the most common bridge type worldwide. Nevertheless, available datasets for the recognition of RCDs are small in terms of size and class variety, which questions their usability in real-world scenarios and their role as a benchmark. Our contribution to this problem is ""dacl10k"", an exceptionally diverse RCD dataset for multi-label semantic segmentation comprising 9,920 images deriving from real-world bridge inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge components that play a key role in the building assessment and recommending actions, such as restoration works, traffic load limitations or bridge closures. In addition, we examine baseline models for dacl10k which are subsequently evaluated. The best model achieves a mean intersection-over-union of 0.42 on the test set. dacl10k, along with our baselines, will be openly accessible to researchers and practitioners, representing the currently biggest dataset regarding number of images and class diversity for semantic segmentation in the bridge inspection domain.",https://openaccess.thecvf.com/content/WACV2024/html/Flotzinger_dacl10k_Benchmark_for_Semantic_Bridge_Damage_Segmentation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Flotzinger_dacl10k_Benchmark_for_Semantic_Bridge_Damage_Segmentation_WACV_2024_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10483794/,"['Bridges', 'Semantic segmentation', 'Semantics', 'Telecommunication traffic', 'Inspection', 'Benchmark testing', 'Concrete']","['Number Of Images', 'Real-world Scenarios', 'Semantic Segmentation', 'Role In Assessment', 'Traffic Safety', 'Reinforced Concrete', 'Wrong Classification', 'Concrete Bridge', 'Semantic Segmentation Datasets', 'Weather', 'Protective Equipment', 'Unmanned Aerial Vehicles', 'Gene Defects', 'Domain Experts', 'Types Of Damage', 'Civil Engineering', 'Fineness', 'Test Split', 'Feature Pyramid Network', 'Camera Pose', 'External Team', 'Auxiliary Loss', 'Number Of Polygons', 'Inspection Of Structures', 'Mean Intersection Over Union', 'Validation Split', 'Concrete Surface', 'Damage Recognition']","['Applications', 'Structural engineering / civil engineering', 'Algorithms', 'Datasets and evaluations', 'Algorithms', 'Image recognition and understanding']",4,"Reliably identifying reinforced concrete defects (RCDs) plays a crucial role in assessing the structural integrity, traffic safety, and long-term durability of concrete bridges, which represent the most common bridge type worldwide. Nevertheless, available datasets for the recognition of RCDs are small in terms of size and class variety, which questions their usability in real-world scenarios and their role as a benchmark. Our contribution to this problem is ""dacl10k"", an exceptionally diverse RCD dataset for multi-label semantic segmentation comprising 9,920 images deriving from real-world bridge inspections. dacl10k distinguishes 12 damage classes as well as 6 bridge components that play a key role in the building assessment and recommending actions, such as restoration works, traffic load limitations or bridge closures. In addition, we examine baseline models for dacl10k which are subsequently evaluated. The best model achieves a mean intersection-over-union of 0.42 on the test set. dacl10k, along with our baselines, will be openly accessible to researchers and practitioners, representing the currently biggest dataset regarding number of images and class diversity for semantic segmentation in the bridge inspection domain."
iBARLE: imBalance-Aware Room Layout Estimation,"Taotao Jing, Lichen Wang, Naji Khosravan, Zhiqiang Wan, Zachary Bessinger, Zhengming Ding, Sing Bing Kang",Tulane University; Zillow Group,50.0,USA,50.0,USA,"Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to well train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of: (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD dataset illustrate that iBARLE has state-of-the-art performance compared with other layout estimation baselines.",https://openaccess.thecvf.com/content/WACV2024/html/Jing_iBARLE_imBalance-Aware_Room_Layout_Estimation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Jing_iBARLE_imBalance-Aware_Room_Layout_Estimation_WACV_2024_paper.pdf,,,2308.15050,main,Poster,https://ieeexplore.ieee.org/document/10483703/,"['Training', 'Visualization', 'Computer vision', 'Shape', 'Layout', 'Estimation', 'Linear programming']","['Room Layout', 'Layout Estimation', 'Complex Structure', 'Real-world Datasets', 'Camera Position', 'Domain Generalization', 'Appearance Variations', 'Complex Layout', 'Training Data', 'Convolutional Neural Network', 'Feature Maps', 'Learning Strategies', 'Complex Samples', 'Attention In Recent Years', 'Visual Features', 'Normal Vector', 'Learning Objectives', 'Data Space', 'Ground Plane', 'Space Complexity', 'Panoramic Images', 'Camera Pose', 'Depth Prediction', 'Simple Shapes', 'Simple Layout', 'Imbalanced Data', 'Extract Visual Features', 'Multi-head Self-attention']","['Algorithms', 'Image recognition and understanding', 'Algorithms', 'Generative models for image', 'video', '3D', 'etc.', 'Applications', 'Virtual / augmented reality']",,"Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to well train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of: (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD [6] dataset illustrate that iBARLE has state-of-the-art performance compared with other layout estimation baselines."
pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation,"Manogna Sreenivas, Goirik Chakrabarty, Soma Biswas",IISER Pune; IISc Bangalore,100.0,India,0.0,,"Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo source samples. The test samples are strategically aligned with these pseudo source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework.",https://openaccess.thecvf.com/content/WACV2024/html/Sreenivas_pSTarC_Pseudo_Source_Guided_Target_Clustering_for_Fully_Test-Time_Adaptation_WACV_2024_paper.html,,https://openaccess.thecvf.com/content/WACV2024/papers/Sreenivas_pSTarC_Pseudo_Source_Guided_Target_Clustering_for_Fully_Test-Time_Adaptation_WACV_2024_paper.pdf,,,2309.00846,main,Poster,https://ieeexplore.ieee.org/document/10484495/,"['Training', 'Computer vision', 'Adaptation models', 'Protocols', 'Machine learning', 'Artificial neural networks', 'Data models']","['Test-time Adaptation', 'Data Sources', 'Test Data', 'Test Samples', 'Domain Shift', 'Real-world Scenarios', 'Improve Prediction Accuracy', 'Source Class', 'Machine Learning Concepts', 'Corruption', 'Deep Network', 'Batch Size', 'Target Sample', 'Batch Of Samples', 'Target Features', 'Inverse Model', 'Target Domain', 'Unlabeled Data', 'Source Model', 'Source Domain', 'Pseudo Labels', 'Self-supervised Learning', 'Continuous Adaptation', 'Online Manner', 'Prior Methods', 'Prediction Vector', 'Batch Tests', 'Real Domain', 'Real Shift', 'Trivial Solution']","['Algorithms', 'Machine learning architectures', 'formulations', 'and algorithms', 'Algorithms', 'Image recognition and understanding']",,"Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo-source samples. The test samples are strategically aligned with these pseudo-source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC’s effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework."
