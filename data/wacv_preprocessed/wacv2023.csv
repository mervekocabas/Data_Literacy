title,author,aff,university_affiliation,university_country,company_affiliation,company_country,abstract,site,oa,pdf,project,github,arxiv,track,status,ieee_link,ieee_keywords,ieee_index_terms,ieee_author_keywords,ieee_citations,ieee_abstract
360MVSNet: Deep Multi-View Stereo Network With 360deg Images for Indoor Scene Reconstruction,"Ching-Ya Chiu, Yu-Ting Wu, I-Chao Shen, Yung-Yu Chuang",The University of Tokyo; National Taiwan University; National Taipei University,100,"Japan, Taiwan",0,,"Recent multi-view stereo methods have achieved promising results with the advancement of deep learning techniques. Despite of the progress, due to the limited fields of view of regular images, reconstructing large indoor environments still requires collecting many images with sufficient visual overlap, which is quite labor-intensive. 360deg images cover a much larger field of view than regular images and would facilitate the capture process. In this paper, we present 360MVSNet, the first deep learning network for multi-view stereo with 360deg images. Our method combines uncertainty estimation with a spherical sweeping module for 360deg images captured from multiple viewpoints in order to construct multi-scale cost volumes. By regressing volumes in a coarse-to-fine manner, high-resolution depth maps can be obtained. Furthermore, we have constructed EQMVS, a large-scale synthetic dataset that consists of over 50K pairs of RGB and depth maps in equirectangular projection. Experimental results demonstrate that our method can reconstruct large synthetic and real-world indoor scenes with significantly better completeness than previous traditional and learning-based methods while saving both time and effort in the data acquisition process.",https://openaccess.thecvf.com/content/WACV2023/html/Chiu_360MVSNet_Deep_Multi-View_Stereo_Network_With_360deg_Images_for_Indoor_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chiu_360MVSNet_Deep_Multi-View_Stereo_Network_With_360deg_Images_for_Indoor_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
3D Change Localization and Captioning From Dynamic Scans of Indoor Scenes,"Yue Qiu, Shintaro Yamamoto, Ryosuke Yamada, Ryota Suzuki, Hirokatsu Kataoka, Kenji Iwata, Yutaka Satoh",National Institute of Advanced Industrial Science and Technology (AIST),100,Japan,0,,"Daily indoor scenes often involve constant changes due to human activities. To recognize scene changes, existing change captioning methods focus on describing changes from two images of a scene. However, to accurately perceive and appropriately evaluate physical changes and then identify the geometry of changed objects, recognizing and localizing changes in 3D space is crucial. Therefore, we propose a task to explicitly localize changes in 3D bounding boxes from two point clouds and describe detailed scene changes, including change types, object attributes, and spatial locations. Moreover, we create a simulated dataset with various scenes, allowing generating data without labor costs. We further propose a framework that allows different 3D object detectors to be incorporated in the change detection process, after which captions are generated based on the correlations of different change regions. The proposed framework achieves promising results in both change detection and captioning. Furthermore, we also evaluated on data collected from real scenes. The experiments show that pretraining on the proposed dataset increases the change detection accuracy by +12.8% (mAP0.25) when applied to real-world data. We believe that our proposed dataset and discussion could provide both a new benchmark and insights for future studies in scene change understanding.",https://openaccess.thecvf.com/content/WACV2023/html/Qiu_3D_Change_Localization_and_Captioning_From_Dynamic_Scans_of_Indoor_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qiu_3D_Change_Localization_and_Captioning_From_Dynamic_Scans_of_Indoor_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030121/,"['Location awareness', 'Point cloud compression', 'Three-dimensional displays', 'Image recognition', 'Limiting', 'Detectors', 'Benchmark testing']","['Dynamic Scan', 'Change Detection', 'Object Detection', 'Point Cloud', '3D Space', 'Bounding Box', 'Scene Images', 'Constant Change', 'Scene Changes', '3D Detection', '3D Object Detection', '3D Bounding Box', 'Model Performance', 'Transformer', 'Regional Characteristics', 'Real-world Applications', 'Feature Dimension', '2D Images', 'Object Classification', 'Feature Points', 'Dynamic Scenes', '3D Scanning', 'Real-world Environments', 'Predicted Bounding Box', '3D Point Cloud', '3D Environment', 'Detection Head', '3D Scene', 'Detection Dataset', 'Point Cloud Processing']","['Algorithms: Vision + language and/or other modalities', '3D computer vision']",6,"Daily indoor scenes often involve constant changes due to human activities. To recognize scene changes, existing change captioning methods focus on describing changes from two images of a scene. However, to accurately perceive and appropriately evaluate physical changes and then identify the geometry of changed objects, recognizing and localizing changes in 3D space is crucial. Therefore, we propose a task to explicitly localize changes in 3D bounding boxes from two point clouds and describe detailed scene changes, including change types, object attributes, and spatial locations. Moreover, we create a simulated dataset with various scenes, allowing generating data without labor costs. We further propose a framework that allows different 3D object detectors to be incorporated in the change detection process, after which captions are generated based on the correlations of different change regions. The proposed framework achieves promising results in both change detection and captioning. Furthermore, we also evaluated on data collected from real scenes. The experiments show that pretraining on the proposed dataset increases the change detection accuracy by +12.8% (mAP0.25) when applied to real-world data. We believe that our proposed dataset and discussion could provide both a new benchmark and in-sights for future studies in scene change understanding."
3D GAN Inversion With Pose Optimization,"Jaehoon Ko, Kyusun Cho, Daewon Choi, Kwangrok Ryoo, Seungryong Kim","Korea University, South Korea",100,South Korea,0,,"With the recent advances in NeRF-based 3D aware GANs quality, projecting an image into the latent space of these 3D-aware GANs has a natural advantage over 2D GAN inversion: not only does it allow multi-view consistent editing of the projected image, but it also enables 3D reconstruction and novel view synthesis when given only a single image. However, the explicit viewpoint control acts as a main hindrance in the 3D GAN inversion process, as both camera pose and latent code have to be optimized simultaneously to reconstruct the given image. Most works that explore the latent space of the 3D-aware GANs rely on ground-truth camera viewpoint or deformable 3D model, thus limiting their applicability. In this work, we introduce a generalizable 3D GAN inversion method that infers camera viewpoint and latent code simultaneously to enable multi-view consistent semantic image editing. The key to our approach is to leverage pre-trained estimators for better initialization and utilize the pixel-wise depth calculated from NeRF parameters to better reconstruct the given image. We conduct extensive experiments on image reconstruction and editing both quantitatively and qualitatively, and further compare our results with 2D GAN-based editing to demonstrate the advantages of utilizing the latent space of 3D GANs. Additional results and visualizations are available at https://hypernerf.github.io/.",https://openaccess.thecvf.com/content/WACV2023/html/Ko_3D_GAN_Inversion_With_Pose_Optimization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ko_3D_GAN_Inversion_With_Pose_Optimization_WACV_2023_paper.pdf,,https://3dgan-inversion.github.io/,2210.07301,main,Poster,https://ieeexplore.ieee.org/document/10030750/,"['Visualization', 'Solid modeling', 'Three-dimensional displays', 'Codes', 'Semantics', 'Process control', 'Aerospace electronics']","['Generative Adversarial Networks', 'Pose Optimization', 'Generative Adversarial Networks Inversion', '3D Generative Adversarial Networks', 'Single Image', '3D Reconstruction', 'Latent Space', 'Inverse Method', 'Inversion Process', 'Camera Pose', 'Image Editing', 'Latent Code', 'Explicit Control', 'View Synthesis', 'Camera Viewpoint', 'Loss Function', 'Manifold', 'High-resolution Images', 'Quantitative Evaluation', 'Image Generation', 'Image X', 'Fréchet Inception Distance', 'Optimization Step', 'Latent Representation', 'Pose Estimation', 'Regularization Loss', 'Reconstruction Accuracy', 'Semantic Properties', 'Depth Map', 'StyleGAN']","['Algorithms: 3D computer vision', 'Biometrics', 'face', 'gesture', 'body pose', 'Computational photography', 'image and video synthesis']",29,"With the recent advances in NeRF-based 3D aware GANs quality, projecting an image into the latent space of these 3D-aware GANs has a natural advantage over 2D GAN inversion: not only does it allow multi-view consistent editing of the projected image, but it also enables 3D reconstruction and novel view synthesis when given only a single image. However, the explicit viewpoint control acts as a main hindrance in the 3D GAN inversion process, as both camera pose and latent code have to be optimized simultaneously to reconstruct the given image. Most works that explore the latent space of the 3D-aware GANs rely on ground-truth camera viewpoint or deformable 3D model, thus limiting their applicability. In this work, we introduce a generalizable 3D GAN inversion method that infers camera viewpoint and latent code simultaneously to enable multi-view consistent semantic image editing. The key to our approach is to leverage pre-trained estimators for better initialization and utilize the pixel-wise depth calculated from NeRF parameters to better reconstruct the given image. We conduct extensive experiments on image reconstruction and editing both quantitatively and qualitatively, and further compare our results with 2D GAN-based editing to demonstrate the advantages of utilizing the latent space of 3D GANs. Additional results and visualizations are available at https://3dgan-inversion.github.io/."
3D Neural Sculpting (3DNS): Editing Neural Signed Distance Functions,"Petros Tzathas, Petros Maragos, Anastasios Roussos","College of Engineering, Mathematics and Physical Sciences, University of Exeter, UK; School of Electrical & Computer Engineering, National Technical University of Athens, Greece; Institute of Computer Science (ICS), Foundation for Research & Technology - Hellas (FORTH), Greece",100,"Greece, UK",0,,"In recent years, implicit surface representations through neural networks that encode the signed distance have gained popularity and have achieved state-of-the-art results in various tasks (e.g. shape representation, shape reconstruction and learning shape priors). However, in contrast to conventional shape representations such as polygon meshes, the implicit representations cannot be easily edited and existing works that attempt to address this problem are extremely limited. In this work, we propose the first method for efficient interactive editing of signed distance functions expressed through neural networks, allowing free-form editing. Inspired by 3D sculpting software for meshes, we use a brush-based framework that is intuitive and can in the future be used by sculptors and digital artists. In order to localize the desired surface deformations, we regulate the network by using a copy of it to sample the previously expressed surface. We introduce a novel framework for simulating sculpting-style surface edits, in conjunction with interactive surface sampling and efficient adaptation of network weights. We qualitatively and quantitatively evaluate our method in various different 3D objects and under many different edits. The reported results clearly show that our method yields high accuracy, in terms of achieving the desired edits, while in the same time preserving the geometry outside the interaction areas.",https://openaccess.thecvf.com/content/WACV2023/html/Tzathas_3D_Neural_Sculpting_3DNS_Editing_Neural_Signed_Distance_Functions_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tzathas_3D_Neural_Sculpting_3DNS_Editing_Neural_Signed_Distance_Functions_WACV_2023_paper.pdf,,,2209.13971,main,Poster,https://ieeexplore.ieee.org/document/10030429,"['Geometry', 'Surface reconstruction', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Neural networks', 'Software']","['Signed Distance Function', 'Interactive', 'Neural Network', '3D Mesh', '3D Software', 'Implicit Representation', 'Digital Art', 'Functional Networks', 'Normal Vector', 'Point Cloud', 'Multilayer Perceptron', 'Stationary Distribution', 'Bounding Box', 'Radial Basis Function', 'Spatial Coordinates', 'Neural Representations', 'Point-of-sale', '3D Shape', 'Set Of Networks', 'Implicit Function', 'Analytical Representation', 'Chamfer Distance', 'Voxel Grid', 'Tangent Plane', 'Implicit Function Theorem', 'Rest Of Surface', 'Loss Function', 'Markov Chain']",['Algorithms: 3D computer vision'],,"In recent years, implicit surface representations through neural networks that encode the signed distance have gained popularity and have achieved state-of-the-art results in various tasks (e.g. shape representation, shape reconstruction, and learning shape priors). However, in contrast to conventional shape representations such as polygon meshes, the implicit representations cannot be easily edited and existing works that attempt to address this problem are extremely limited. In this work, we propose the first method for efficient interactive editing of signed distance functions expressed through neural networks, allowing free-form editing. Inspired by 3D sculpting software for meshes, we use a brush-based framework that is intuitive and can in the future be used by sculptors and digital artists. In order to localize the desired surface deformations, we regulate the network by using a copy of it to sample the previously expressed surface. We introduce a novel framework for simulating sculpting-style surface edits, in conjunction with interactive surface sampling and efficient adaptation of network weights. We qualitatively and quantitatively evaluate our method in various different 3D objects and under many different edits. The reported results clearly show that our method yields high accuracy, in terms of achieving the desired edits, while at the same time preserving the geometry outside the interaction areas."
3D-SpLineNet: 3D Traffic Line Detection Using Parametric Spline Representations,"Maximilian Pittner, Alexandru Condurache, Joel Janai","Institute of Signal Processing, University of Lubeck, 23562 Lubeck, Germany; Bosch Mobility Solutions, Robert Bosch GmbH, 71229 Leonberg, Germany",50,Germany,50,Germany,"Monocular 3D traffic line detection jointly tackles the detection of lane markings and regression of their 3D location. The greatest challenge is the exact estimation of various line shapes in the world, which highly depends on the chosen representation. While anchor-based and grid-based line representations have been proposed, all suffer from the same limitation, the necessity of discretizing the 3D space. To address this limitation, we present an anchor-free parametric lane representation, which defines traffic lines as continuous curves in 3D space. Choosing splines as our representation, we show their superiority over polynomials of different degrees that were proposed in previous 2D lane detection approaches. Our continuous representation allows us to model even complex lane shapes at any position in the 3D space, while implicitly enforcing smoothness constraints. Our model is validated on a synthetic 3D lane dataset including a variety of scenes in terms of complexity of road shape and illumination. We outperform the state-of-the-art in nearly all geometric performance metrics and achieve a great leap in the detection rate. In contrast to discrete representations, our parametric model requires no post-processing achieving highest processing speed. Additionally, we provide a thorough analysis over different parametric representations for 3D lane detection. The code and trained models are available on our project website https://3d-splinenet.github.io/.",https://openaccess.thecvf.com/content/WACV2023/html/Pittner_3D-SpLineNet_3D_Traffic_Line_Detection_Using_Parametric_Spline_Representations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pittner_3D-SpLineNet_3D_Traffic_Line_Detection_Using_Parametric_Spline_Representations_WACV_2023_paper.pdf,https://3d-splinenet.github.io/,https://3d-splinenet.github.io/,,main,Poster,https://ieeexplore.ieee.org/document/10030921/,"['Measurement', 'Solid modeling', 'Three-dimensional displays', 'Shape', 'Lane detection', 'Roads', 'Lighting']","['Traffic Detection', 'Traffic Lines', 'Spline Representation', 'Discretion', 'Parametrized', '3D Space', 'Complex Shapes', 'Position In Space', 'Polynomial Of Degree', 'Line Shape', 'Continuous Curve', 'Lane Markings', 'Convolutional Neural Network', 'Deep Neural Network', 'Feature Maps', 'Line Segment', 'Vertical Component', 'Representational Analysis', 'Domain Adaptation', 'Curve Parameters', 'Detection Head', '3D Geometry', '3D Line', 'Bezier Curve', '3D Curves', 'Ground Truth 3D', 'Road Markings', 'Anchor-based Methods', '3rd Degree', '3D World']","['Applications: Robotics', '3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Monocular 3D traffic line detection jointly tackles the detection of lane markings and regression of their 3D location. The greatest challenge is the exact estimation of various line shapes in the world, which highly depends on the chosen representation. While anchor-based and grid-based line representations have been proposed, all suffer from the same limitation, the necessity of discretizing the 3D space. To address this limitation, we present an anchor-free parametric lane representation, which defines traffic lines as continuous curves in 3D space. Choosing splines as our representation, we show their superiority over polynomials of different degrees that were proposed in previous 2D lane detection approaches. Our continuous representation allows us to model even complex lane shapes at any position in the 3D space, while implicitly enforcing smoothness constraints. Our model is validated on a synthetic 3D lane dataset including a variety of scenes in terms of complexity of road shape and illumination. We outperform the state-of-the-art in nearly all geometric performance metrics and achieve a great leap in the detection rate. In contrast to discrete representations, our parametric model requires no post-processing achieving highest processing speed. Additionally, we provide a thorough analysis over different parametric representations for 3D lane detection. The code and trained models are available on our project website https://3d-splinenet.github.io/."
3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling,"Stathis Galanakis, Baris Gecer, Alexandros Lattas, Stefanos Zafeiriou",Huawei; Imperial College London,50,UK,50,China,"Facial 3D Morphable Models are a main computer vision subject with countless applications and have been highly optimized in the last two decades. The tremendous improvements of deep generative networks have created various possibilities for improving such models and have attracted wide interest. Moreover, the recent advances in neural radiance fields, are revolutionising novel-view synthesis of known scenes. In this work, we present a facial 3D Morphable Model, which exploits both of the above, and can accurately model a subject's identity, pose and expression and render it in arbitrary illumination. This is achieved by utilizing a powerful deep style-based generator to overcome two main weaknesses of neural radiance fields, their rigidity and rendering speed. We introduce a style-based generative network that synthesizes in one pass all and only the required rendering samples of a neural radiance field. We create a vast labelled synthetic dataset of facial renders, and train the network, so that it can accurately model and generalize on facial identity, pose and appearance. Finally, we show that this model can accurately be fit to ""in-the-wild"" facial images of arbitrary pose and illumination, extract the facial characteristics, and be used to re-render the face in controllable conditions.",https://openaccess.thecvf.com/content/WACV2023/html/Galanakis_3DMM-RF_Convolutional_Radiance_Fields_for_3D_Face_Modeling_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Galanakis_3DMM-RF_Convolutional_Radiance_Fields_for_3D_Face_Modeling_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030332/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Lighting', 'Rendering (computer graphics)', 'Generators']","['3D Face', '3D Face Model', 'Radiance Field', 'Face Images', 'Neural Field', 'Deep Generative Network', 'Model Parameters', 'Statistical Models', 'Training Data', 'Training Time', 'Single Image', '3D Reconstruction', 'Identification Of Features', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Latent Space', 'Human Faces', 'Implicit Function', 'Single Pass', 'Camera Position', 'Latent Vector', 'Volume Rendering', 'Identity Vector', 'View Of The Scene', 'Synthesis Network', 'Signed Distance Function', 'Illumination Variations']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision']",7,"Facial 3D Morphable Models are a main computer vision subject with countless applications and have been highly optimized in the last two decades. The tremendous improvements of deep generative networks have created various possibilities for improving such models and have attracted wide interest. Moreover, the recent advances in neural radiance fields, are revolutionising novel-view synthesis of known scenes. In this work, we present a facial 3D Morphable Model, which exploits both of the above, and can accurately model a subject’s identity, pose and expression and render it in arbitrary illumination. This is achieved by utilizing a powerful deep style-based generator to overcome two main weaknesses of neural radiance fields, their rigidity and rendering speed. We introduce a style-based generative network that synthesizes in one pass all and only the required rendering samples of a neural radiance field. We create a vast labelled synthetic dataset of facial renders, and train the network, so that it can accurately model and generalize on facial identity, pose and appearance. Finally, we show that this model can accurately be fit to ""in-the-wild"" facial images of arbitrary pose and illumination, extract the facial characteristics, and be used to re-render the face in controllable conditions."
"A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials","Chuqiao Li, Zhiwu Huang, Danda Pani Paudel, Yabin Wang, Mohamad Shahbazi, Xiaopeng Hong, Luc Van Gool","Harbin Institute of Technology, China; Xi’an Jiaotong University, China; ETH Zürich, Switzerland; Singapore Management University, Singapore; KU Leuven, Belgium",100,"Belgium, China, Singapore, Switzerland",0,,"There have been emerging a number of benchmarks and techniques for the detection of deepfakes. However, very few works study the detection of incrementally appearing deepfakes in the real-world scenarios. To simulate the wild scenes, this paper suggests a continual deepfake detection benchmark (CDDB) over a new collection of deepfakes from both known and unknown generative models. The suggested CDDB designs multiple evaluations on the detection over easy, hard, and long sequence of deepfake tasks, with a set of appropriate measures. In addition, we exploit multiple approaches to adapt multiclass incremental learning methods, commonly used in the continual visual recognition, to the continual deepfake detection problem. We evaluate existing methods, including their adapted ones, on the proposed CDDB. Within the proposed benchmark, we explore some commonly known essentials of standard continual learning. Our study provides new insights on these essentials in the context of continual deepfake detection. The suggested CDDB is clearly more challenging than the existing benchmarks, which thus offers a suitable evaluation avenue to the future research. Both data and code are available at https://github.com/Coral79/CDDB.",https://openaccess.thecvf.com/content/WACV2023/html/Li_A_Continual_Deepfake_Detection_Benchmark_Dataset_Methods_and_Essentials_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_A_Continual_Deepfake_Detection_Benchmark_Dataset_Methods_and_Essentials_WACV_2023_paper.pdf,,https://github.com/Coral79/CDDB,2205.05467,main,Poster,https://ieeexplore.ieee.org/document/10030946/,"['Learning systems', 'Deepfakes', 'Visualization', 'Computer vision', 'Adaptation models', 'Codes', 'Benchmark testing']","['Benchmark', 'Deepfake Detection', 'Real-world Scenarios', 'Incremental Learning', 'Sequential Task', 'Unknown Model', 'Multi-label', 'ImageNet', 'Real Samples', 'Detection Task', 'Classification Loss', 'Null Space', 'Multi-task Learning', 'Image Synthesis', 'Unknown Source', 'Previous Tasks', 'Independent Classification', 'Upper Triangular', 'Evaluation Scenarios', 'Real Class', 'Catastrophic Forgetting', 'Multi-class Model', 'Distillation Loss', 'Fake Images', 'Easy Evaluation', 'Sample Xi', 'Source Model']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",12,"There have been emerging a number of benchmarks and techniques for the detection of deepfakes. However, very few works study the detection of incrementally appearing deepfakes in the real-world scenarios. To simulate the wild scenes, this paper suggests a continual deepfake detection benchmark (CDDB) over a new collection of deepfakes from both known and unknown generative models. The suggested CDDB designs multiple evaluations on the detection over easy, hard, and long sequence of deepfake tasks, with a set of appropriate measures. In addition, we exploit multiple approaches to adapt multiclass incremental learning methods, commonly used in the continual visual recognition, to the continual deepfake detection problem. We evaluate existing methods, including their adapted ones, on the proposed CDDB. Within the proposed benchmark, we explore some commonly known essentials of standard continual learning. Our study provides new insights on these essentials in the context of continual deepfake detection. The suggested CDDB is clearly more challenging than the existing benchmarks, which thus offers a suitable evaluation avenue to the future research. Both data and code are available at https://github.com/Coral79/CDDB."
A Deep Neural Framework To Detect Individual Advertisement (Ad) From Videos,Zongyi Liu,"FGBS, Amazon.com, 2121 7th Ave, Seattle, WA, USA",0,,100,USA,"Detecting commercial Ads from a video is important. For example, the commercial break frequency and duration are two metrics to measure the user experience for streaming service providers such as Amazon IMDb TV. The detection can be done intrusively by intercepting the network traffic and then parsing the service providers data and logs, or non-intrusively by capturing the videos streamed by content providers and then analyzing using the computer vision technologies. In this paper, we present a non-intrusive framework that is able to not only detect an Ad section, but also segment out individual Ads. We show that our algorithm is not only scalable because it uses light weight audio data to do global segmentation, but also robust as the Ad classifier is able to handle different types of contents captured from the popular streaming services such as the IMDb TV, Hulu, CrackleTV, and Prime Video (PV) live sports.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_A_Deep_Neural_Framework_To_Detect_Individual_Advertisement_Ad_From_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_A_Deep_Neural_Framework_To_Detect_Individual_Advertisement_Ad_From_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030792/,"['Computer vision', 'TV', 'Telecommunication traffic', 'Streaming media', 'Motion pictures', 'User experience', 'Frequency measurement']","['Computer Vision', 'Live Streaming', 'Audio Data', 'Computer Vision Technology', 'Sampling Rate', 'False Positive Rate', 'Long Short-term Memory', 'Transfer Learning', 'Video Clips', 'Short Segments', 'Video Data', 'Handcrafted Features', 'Short Video', 'Long Short-term Memory Model', 'Video Segments', 'Triplet Loss', 'Audio Clips', 'Vision Transformer', 'Segmentation Module', 'Fast Pathway', 'DNN Model', 'Temporal Attention', 'Video Playback', 'Post-processing Step', 'Scene Changes', 'Output Pathways', 'False Positive', 'Segmentation Step', 'Support Vector Machine']","['Applications: Commercial/retail', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",1,"Detecting commercial Ads from a video is important. For example, the commercial break frequency and duration are two metrics to measure the user experience for streaming service providers such as Amazon Freevee. The detection can be done intrusively by intercepting the network traffic and then parsing the service providers data and logs, or non-intrusively by capturing the videos streamed by content providers and then analyzing using the computer vision technologies. In this paper, we present a non-intrusive framework that is able to not only detect an Ad section, but also segment out individual Ads. We show that our algorithm is scalable because it uses light weight audio data to do global segmentation, as well as is domain crossing (movies, TVs and live streaming sports) captured from the popular streaming services such as the Freevee and the Prime Video (PV) live sports."
A Morphology Focused Diffusion Probabilistic Model for Synthesis of Histopathology Images,"Puria Azadi Moghadam, Sanne Van Dalen, Karina C. Martin, Jochen Lennerz, Stephen Yip, Hossein Farahani, Ali Bashashati",University of British Columbia; Eindhoven University of Technology; BC Cancer Agency; Harvard Medical School,75,"Canada, Netherlands, USA",25,Canada,"Visual microscopic study of diseased tissue by pathologists has been the cornerstone for cancer diagnosis and prognostication for more than a century. Recently, deep learning methods have made significant advances in the analysis and classification of tissue images. However, there has been limited work on the utility of such models in generating histopathology images. These synthetic images have several applications in pathology including utilities in education, proficiency testing, privacy, and data sharing. Recently, diffusion probabilistic models were introduced to generate high quality images. Here, for the first time, we investigate the potential use of such models along with prioritized morphology weighting and color normalization to synthesize high quality histopathology images of brain cancer. Our detailed results show that diffusion probabilistic models are capable of synthesizing a wide range of histopathology images and have superior performance compared to generative adversarial networks.",https://openaccess.thecvf.com/content/WACV2023/html/Moghadam_A_Morphology_Focused_Diffusion_Probabilistic_Model_for_Synthesis_of_Histopathology_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Moghadam_A_Morphology_Focused_Diffusion_Probabilistic_Model_for_Synthesis_of_Histopathology_WACV_2023_paper.pdf,,,2209.13167,main,Poster,https://ieeexplore.ieee.org/document/10030927/,"['Visualization', 'Histopathology', 'Image color analysis', 'Computational modeling', 'Microscopy', 'Morphology', 'Brain modeling']","['Image Synthesis', 'Histopathological Images', 'Diffusion Probabilistic Models', 'Generative Adversarial Networks', 'Synthetic Images', 'Use Of Such Models', 'Neural Network', 'Specific Characteristics', 'Hematoxylin And Eosin', 'Distribution Of Cells', 'Random Noise', 'Reversible Process', 'Diffusion Model', 'Latent Space', 'Image Generation', 'Model Discrimination', 'Low-grade Gliomas', 'Imbalanced Datasets', 'Variational Autoencoder', 'Digital Pathology', 'Fréchet Inception Distance', 'Objective Metrics', 'Pathological Images', 'Inception Distance', 'Generative Adversarial Networks Model', 'Board-certified Pathologist', 'Extracellular Matrix']",['Applications: Biomedical/healthcare/medicine'],44,"Visual microscopic study of diseased tissue by pathologists has been the cornerstone for cancer diagnosis and prognostication for more than a century. Recently, deep learning methods have made significant advances in the analysis and classification of tissue images. However, there has been limited work on the utility of such models in generating histopathology images. These synthetic images have several applications in pathology including utilities in education, proficiency testing, privacy, and data sharing. Recently, diffusion probabilistic models were introduced to generate high quality images. Here, for the first time, we investigate the potential use of such models along with prioritized morphology weighting and color normalization to synthesize high quality histopathology images of brain cancer. Our detailed results show that diffusion probabilistic models are capable of synthesizing a wide range of histopathology images and have superior performance compared to generative adversarial networks."
A Neural Video Codec With Spatial Rate-Distortion Control,"Noor Fathima, Jens Petersen, Guillaume Sautière, Auke Wiggers, Reza Pourreza",Qualcomm AI Research,0,,100,USA,"Neural video compression algorithms are nearly competitive with hand-crafted codecs in terms of rate-distortion performance and subjective quality. However, many neural codecs are inflexible black boxes, and give users little to no control over the reconstruction quality and bitrate. In this work, we present a flexible neural video codec that combines ideas from variable-bitrate codecs and region-of-interest-based coding. By conditioning our model on a global rate-distortion tradeoff parameter and a region-of-interest (ROI) mask, we obtain fine control over the per-frame bitrate and the reconstruction quality in the ROI. The resulting codec enables practical use cases such as coding under bitrate constraints with fixed ROI quality, while taking a negligible hit in overall rate-distortion performance. We find that our codec is best utilized when the sequence contains complex motion, such as scenes with camera panning or sports videos, where we substantially outperform non-ROI codecs in the region of interest with BD-rate savings exceeding 60% in some cases.",https://openaccess.thecvf.com/content/WACV2023/html/Fathima_A_Neural_Video_Codec_With_Spatial_Rate-Distortion_Control_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Fathima_A_Neural_Video_Codec_With_Spatial_Rate-Distortion_Control_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030824/,"['Teleconferencing', 'Surveillance', 'Bit rate', 'Rate-distortion', 'Quality control', 'Video compression', 'Distortion']","['Video Codec', 'Reconstruction Quality', 'Trade-off Parameter', 'Compression Algorithm', 'Video Compression', 'Neural Compression', 'Practical Use Cases', 'Factorization', 'Latent Variables', 'Training Methods', 'Peak Signal-to-noise Ratio', 'Video Sequences', 'Sequence Of Frames', 'Semantic Annotation', 'Structural Similarity Index Measure', 'Region Of Interest Extraction', 'Latent Scale', 'Region-of-interest Regions']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",3,"Neural video compression algorithms are nearly competitive with hand-crafted codecs in terms of rate-distortion performance and subjective quality. However, many neural codecs are inflexible black boxes, and give users little to no control over the reconstruction quality and bitrate. In this work, we present a flexible neural video codec that combines ideas from variable-bitrate codecs and region-of-interest-based coding. By conditioning our model on a global rate-distortion tradeoff parameter and a region-of-interest (ROI) mask, we obtain dynamic control over the per-frame bitrate and the reconstruction quality in the ROI at test time. The resulting codec enables practical use cases such as coding under bitrate constraints with fixed ROI quality, while taking a negligible hit in performance compared to a fixed-rate model. We find that our codec performs best on sequences with complex motion, where we substantially outperform non-ROI codecs in the region of interest with Bjøntegaard-Delta rate savings exceeding 60%."
A Priority Map for Vision-and-Language Navigation With Trajectory Plans and Feature-Location Cues,"Jason Armitage, Leonardo Impett, Rico Sennrich","University of Zurich, Switzerland; University of Cambridge, UK",100,"Switzerland, UK",0,,"In a busy city street, a pedestrian surrounded by distractions can pick out a single sign if it is relevant to their route. Artificial agents in outdoor Vision-and-Language Navigation (VLN) are also confronted with detecting supervisory signal on environment features and location in inputs. To boost the prominence of relevant features in transformer-based systems without costly preprocessing and pretraining, we take inspiration from priority maps - a mechanism described in neuropsychological studies. We implement a novel priority map module and pretrain on auxiliary tasks using low-sample datasets with high-level representations of routes and environment-related references to urban features. A hierarchical process of trajectory planning - with subsequent parameterised visual boost filtering on visual inputs and prediction of corresponding textual spans - addresses the core challenge of cross-modal alignment and feature-level localisation. The priority map module is integrated into a feature-location framework that doubles the task completion rates of standalone transformers and attains state-of-the-art performance for transformer-based systems on the Touchdown benchmark for VLN. We release code (https://github.com/JasonArmitage-res/PM-VLN) and data (https://zenodo.org/record/6891965#.YtwoS3ZBxD8).",https://openaccess.thecvf.com/content/WACV2023/html/Armitage_A_Priority_Map_for_Vision-and-Language_Navigation_With_Trajectory_Plans_and_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Armitage_A_Priority_Map_for_Vision-and-Language_Navigation_With_Trajectory_Plans_and_WACV_2023_paper.pdf,,https://github.com/JasonArmitage-res/PM-VLN,2207.11717,main,Poster,https://ieeexplore.ieee.org/document/10030783/,"['Training', 'Visualization', 'Navigation', 'Trajectory planning', 'Filtering', 'Urban areas', 'Transformer cores']","['Trajectory Planning', 'Priority Map', 'Environmental Characteristics', 'Visual Input', 'Intelligence Agencies', 'Urban Quality', 'Auxiliary Task', 'Visual Information', 'Urban Environments', 'Object Detection', 'Training Strategy', 'Main Model', 'Visual Attention', 'Temporal Sequence', 'Step Count', 'Transformer Model', 'Submodule', 'Visual Perspective', 'Encoder Layer', 'Combined Output', 'Trajectory Estimation', 'Attention Heads', 'Human Attention', 'Multimodal Tasks', 'Shortest Path Distance', 'Linguistic Input', 'Variant Results', 'Linguistic Information', 'Activity Prediction', 'Ablation']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"In a busy city street, a pedestrian surrounded by distractions can pick out a single sign if it is relevant to their route. Artificial agents in outdoor Vision-and-Language Navigation (VLN) are also confronted with detecting supervisory signal on environment features and location in inputs. To boost the prominence of relevant features in transformer-based systems without costly preprocessing and pretraining, we take inspiration from priority maps - a mechanism described in neuropsychological studies. We implement a novel priority map module and pretrain on auxiliary tasks using low-sample datasets with high-level representations of routes and environment-related references to urban features. A hierarchical process of trajectory planning -with subsequent parameterised visual boost filtering on visual inputs and prediction of corresponding textual spans - addresses the core challenge of cross-modal alignment and feature-level localisation. The priority map module is integrated into a feature-location framework that doubles the task completion rates of standalone transformers and attains state-of-the-art performance for transformer-based systems on the Touchdown benchmark for VLN. We release code (https://github.com/JasonArmitage-res/PM-VLN) and data (https://zenodo.org/record/6891965.YtwoS3ZBxD8)."
A Protocol for Evaluating Model Interpretation Methods From Visual Explanations,"Hamed Behzadi-Khormouji, José Oramas","University of Antwerp, imec-IDLab",100,Belgium,0,,"With the continuous development of Convolutional Neural Networks (CNNs), there is an increasing requirement towards the understanding of the representations they internally encode. The task of studying such encoded representations is referred to as model interpretation. Efforts along this direction, despite being proved efficient, stand with two weaknesses. First, there is low semanticity on the feedback they provide which leads toward subjective visualizations. Second, there is no unified protocol for the quantitative evaluation of interpretation methods which makes the comparison between current and future methods complex.\nTo address these issues, we propose a unified evaluation protocol for the quantitative evaluation of interpretation methods. This is achieved by enhancing existing interpretation methods to be capable of generating visual explanations and then linking these explanations with a semantic label. To achieve this, we introduce the Weighted Average Intersection-over-Union (WAIoU) metric to estimate the coverage rate between explanation heatmaps and semantic annotations. This is complemented with an analysis of several binarization techniques for heatmaps, necessary when measuring coverage. Experiments considering several interpretation methods covering different CNN architectures pre-trained on multiple datasets show the effectiveness of the proposed protocol.",https://openaccess.thecvf.com/content/WACV2023/html/Behzadi-Khormouji_A_Protocol_for_Evaluating_Model_Interpretation_Methods_From_Visual_Explanations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Behzadi-Khormouji_A_Protocol_for_Evaluating_Model_Interpretation_Methods_From_Visual_Explanations_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030980/,"['Heating systems', 'Measurement', 'Visualization', 'Computer vision', 'Protocols', 'Semantics', 'Computer architecture']","['Model Interpretation', 'Visual Explanation', 'Model Interpretation Methods', 'Convolutional Neural Network', 'Coverage Rate', 'Convolutional Neural Network Architecture', 'Evaluation Protocol', 'Semantic Labels', 'Unified Protocol', 'Semantic Annotation', 'Understanding Of Representation', 'Training Set', 'Convolutional Layers', 'Input Image', 'Active Layer', 'Latent Space', 'Intensity Threshold', 'Activation Maps', 'Eyebrows', 'Visual Patterns', 'Intersection Over Union Value', 'Coverage Performance', 'Semantic Knowledge', 'Relevant Units', 'Accurate Coverage', 'Proportion Of Units', 'Qualitative Examples', 'Cumulative Distribution Function Curve', 'Face Parts', 'Output Probability']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Visualization']",,"With the continuous development of Convolutional Neural Networks (CNNs), there is an increasing requirement towards the understanding of the representations they internally encode. The task of studying such encoded representations is referred to as model interpretation. Efforts along this direction, despite being proved efficient, stand with two weaknesses. First, there is low semanticity on the feedback they provide which leads toward subjective visualizations. Second, there is no unified protocol for the quantitative evaluation of interpretation methods which makes the comparison between current and future methods complex.To address these issues, we propose a unified evaluation protocol for the quantitative evaluation of interpretation methods. This is achieved by enhancing existing interpretation methods to be capable of generating visual explanations and then linking these explanations with a semantic label. To achieve this, we introduce the Weighted Average Intersection-over-Union (WAIoU) metric to estimate the coverage rate between explanation heatmaps and semantic annotations. This is complemented with an analysis of several binarization techniques for heatmaps, necessary when measuring coverage. Experiments considering several interpretation methods covering different CNN architectures pre-trained on multiple datasets show the effectiveness of the proposed protocol."
A Quality Aware Sample-to-Sample Comparison for Face Recognition,"Mohammad Saeed Ebrahimi Saadabadi, Sahar Rahimi Malakshan, Ali Zafari, Moktari Mostofa, Nasser M. Nasrabadi",West Virginia University,100,USA,0,,"Currently available face datasets mainly consist of a large number of high-quality and a small number of low-quality samples. As a result, a Face Recognition (FR) network fails to learn the distribution of low-quality samples since they are less frequent during training (underrepresented). Moreover, current state-of-the-art FR training paradigms are based on the sample-to-center comparison (i.e., Softmax-based classifier), which results in a lack of uniformity between train and test metrics. This work integrates a quality-aware learning process at the sample level into the classification training paradigm (QAFace). In this regard, Softmax centers are adaptively guided to pay more attention to low-quality samples by using a quality-aware function. Accordingly, QAFace adds a quality-based adjustment to the updating procedure of the Softmax-based classifier to improve the performance on the underrepresented low-quality samples. Our method adaptively finds and assigns more attention to the recognizable low-quality samples in the training datasets. In addition, QAFace ignores the unrecognizable low-quality samples using the feature magnitude as a proxy for quality. As a result, QAFace prevents class centers from getting distracted from the optimal direction. The proposed method is superior to the state-of-the-art algorithms in extensive experimental results on the CFP-FP, LFW, CPLFW, CALFW, AgeDB, IJB-B, and IJB-C datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Saadabadi_A_Quality_Aware_Sample-to-Sample_Comparison_for_Face_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Saadabadi_A_Quality_Aware_Sample-to-Sample_Comparison_for_Face_Recognition_WACV_2023_paper.pdf,,,2306.04,main,Poster,,,,,,
A Simple and Efficient Pipeline To Build an End-to-End Spatial-Temporal Action Detector,"Lin Sui, Chen-Lin Zhang, Lixin Gu, Feng Han","State Key Laboratory for Novel Software Technology, Nanjing University, China; DataElem Inc., Beijing, China; 14Paradigm Inc., Beijing, China",33.33333333,China,66.66666667,China,"Spatial-temporal action detection is a vital part of video understanding. Current spatial-temporal action detection methods mostly use an object detector to obtain person candidates and classify these person candidates into different action categories. So-called two-stage methods are heavy and hard to apply in real-world applications. Some existing methods build one-stage pipelines, But a large performance drop exists with the vanilla one-stage pipeline and extra classification modules are needed to achieve comparable performance. In this paper, we explore a simple and effective pipeline to build a strong one-stage spatial-temporal action detector. The pipeline is composed by two parts: one is a simple end-to-end spatial-temporal action detector. The proposed end-to-end detector has minor architecture changes to current proposal-based detectors and does not add extra action classification modules. The other part is a novel labeling strategy to utilize unlabeled frames in sparse annotated data. We named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost and around 80% FLOPs reduction. Our code will be released at https://github.com/4paradigm-CV/SE-STAD.",https://openaccess.thecvf.com/content/WACV2023/html/Sui_A_Simple_and_Efficient_Pipeline_To_Build_an_End-to-End_Spatial-Temporal_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sui_A_Simple_and_Efficient_Pipeline_To_Build_an_End-to-End_Spatial-Temporal_WACV_2023_paper.pdf,,https://github.com/4paradigm-CV/SE-STAD,2206.03064,main,Poster,,,,,,
A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation,"Georgy Ponimatkin, Nermin Samet, Yang Xiao, Yuming Du, Renaud Marlet, Vincent Lepetit","LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France; Valeo.ai, Paris, France; LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France",66.66666667,France,33.33333333,France,"We propose a simple, yet powerful approach for unsupervised object segmentation in videos. We introduce an objective function whose minimum represents the mask of the main salient object over the input sequence. It only relies on independent image features and optical flows, which can be obtained using off-the-shelf self-supervised methods. It scales with the length of the sequence with no need for superpixels or sparsification, and it generalizes to different datasets without any specific training. This objective function can actually be derived from a form of spectral clustering applied to the entire video. Our method achieves on-par performance with the state of the art on standard benchmarks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually and practically much simpler.",https://openaccess.thecvf.com/content/WACV2023/html/Ponimatkin_A_Simple_and_Powerful_Global_Optimization_for_Unsupervised_Video_Object_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ponimatkin_A_Simple_and_Powerful_Global_Optimization_for_Unsupervised_Video_Object_WACV_2023_paper.pdf,,https://ponimatkin.github.io/ssl-vos,2209.09341,main,Poster,https://ieeexplore.ieee.org/document/10030403/,"['Training', 'Computer vision', 'Semantics', 'Object segmentation', 'Linear programming', 'Solids', 'Standards']","['Object Segmentation', 'Video Object Segmentation', 'Unsupervised Video Object Segmentation', 'Objective Function', 'Sequence Length', 'Image Features', 'State Of The Art', 'Optical Flow', 'Main Object', 'Standard Benchmark', 'Spectral Clustering', 'Video Segments', 'Salient Object', 'Large Datasets', 'Unsupervised Learning', 'Image Object', 'Dot Product', 'Local Image', 'Manual Annotation', 'Location In Frame', 'Video Object', 'Affinity Matrix', 'Appearance Features', 'Eigenvectors Of Matrix', 'Object Appearance', 'Video Sequences', 'Spectral Method', 'Target Dataset']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",9,"We propose a simple, yet powerful approach for unsupervised object segmentation in videos. We introduce an objective function whose minimum represents the mask of the main salient object over the input sequence. It only relies on independent image features and optical flows, which can be obtained using off-the-shelf self-supervised methods. It scales with the length of the sequence with no need for superpixels or sparsification, and it generalizes to different datasets without any specific training. This objective function can actually be derived from a form of spectral clustering applied to the entire video. Our method achieves on-par performance with the state of the art on standard bench-marks (DAVIS2016, SegTrack-v2, FBMS59), while being conceptually and practically much simpler."
A Suspect Identification Framework Using Contrastive Relevance Feedback,"Devansh Gupta, Aditya Saini, Sarthak Bhagat, Shagun Uppal, Rishi Raj Jain, Drishti Bhasin, Ponnurangam Kumaraguru, Rajiv Ratn Shah","IIT Roorkee; Precog Lab, IIIT Hyderabad; MIDAS Lab, IIIT Delhi",100,India,0,,"Suspect Identification is one of the most pivotal aspects of a forensic and criminal investigation. A significant amount of time and skill is devoted to creating sketches for it and requires a fair amount of recollections from the witness to provide a useful sketch. We devise a method that aims to automate the process of suspect identification and model this problem by iteratively retrieving images from feedback provided by the user. Compared to standard image retrieval tasks, interactive facial image retrieval is specifically more challenging due to the high subjectivity involved in describing a person's facial attributes and appropriately evolving with the preferences put forward by the user. Our method uses a relatively simpler form of supervision by utilizing the user's feedback to label images as either similar or dissimilar to their mental image of the suspect based on which we propose a loss function using the contrastive learning paradigm that is optimized in an online fashion. We validate the efficacy of our proposed approach using a carefully designed testbed to simulate user feedback and a large-scale user study. We empirically show that our method iteratively improves personalization, leading to faster convergence and enhanced recommendation relevance, thereby, improving user satisfaction. Our proposed framework is being designed for real-time use in the metropolitan crime investigation department, and thus is also equipped with a user-friendly web interface with a real-time experience for suspect retrieval.",https://openaccess.thecvf.com/content/WACV2023/html/Gupta_A_Suspect_Identification_Framework_Using_Contrastive_Relevance_Feedback_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_A_Suspect_Identification_Framework_Using_Contrastive_Relevance_Feedback_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030819/,"['Visualization', 'Computer vision', 'Forensics', 'Image retrieval', 'Real-time systems', 'Inference algorithms', 'Task analysis']","['Relevant Feedback', 'Identification Of Suspects', 'User Study', 'Face Images', 'Image Retrieval', 'Self-supervised Learning', 'User Feedback', 'Form Of Supervision', 'User-friendly Web Interface', 'Image Retrieval Task', 'Training Set', 'Variety Of Factors', 'K-nearest Neighbor', 'Number Of Images', 'Target Image', 'Latent Space', 'Online Training', 'Visual Memory', 'Low-dimensional Space', 'Similar Images', 'Similar Note', 'Image Embedding', 'Disentangled Representation', 'Unsupervised Representation Learning', 'Query Image', 'Histogram Of Oriented Gradients', 'Unsupervised Representation', 'Network Project', 'Projective Space']",['Applications: Social good'],3,"Suspect Identification is one of the most pivotal aspects of a forensic and criminal investigation. A significant amount of time and skill is devoted to creating sketches for it and requires a fair amount of recollections from the witness to provide a useful sketch. We devise a method that aims to automate the process of suspect identification and model this problem by iteratively retrieving images from feedback provided by the user. Compared to standard image retrieval tasks, interactive facial image retrieval is specifically more challenging due to the high subjectivity involved in describing a person’s facial attributes and appropriately evolving with the preferences put forward by the user. Our method uses a relatively simpler form of supervision by utilizing the user’s feedback to label images as either similar or dissimilar to their mental image of the suspect based on which we propose a loss function using the contrastive learning paradigm that is optimized in an online fashion. We validate the efficacy of our proposed approach using a carefully designed testbed to simulate user feedback and a large-scale user study. We empirically show that our method iteratively improves personalization, leading to faster convergence and enhanced recommendation relevance, thereby, improving user satisfaction. Our proposed framework is being designed for real-time use in the metropolitan crime investigation department, and thus is also equipped with a user-friendly web interface with a real-time experience for suspect retrieval."
AFPSNet: Multi-Class Part Parsing Based on Scaled Attention and Feature Fusion,"Njuod Alsudays, Jing Wu, Yu-Kun Lai, Ze Ji","Cardiff University, UK",100,UK,0,,"Multi-class part parsing is a dense prediction task that seeks to simultaneously detect multiple objects and the semantic parts within these objects in the scene. This problem is important in providing detailed object understanding, but is challenging due to the existence of both class-level and part-level ambiguities. In this paper, we propose to integrate an attention refinement module and a feature fusion module to tackle the part-level ambiguity. The attention refinement module aims to enhance the feature representations by focusing on important features. The feature fusion module aims to improve the fusion operation for different scales of features. We also propose an object-to-part training strategy to tackle the class-level ambiguity, which improves the localization of parts by exploiting prior knowledge of objects. The experimental results demonstrated the effectiveness of the proposed modules and the training strategy, and showed that our proposed method achieved state-of-the-art performance on the benchmark dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Alsudays_AFPSNet_Multi-Class_Part_Parsing_Based_on_Scaled_Attention_and_Feature_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Alsudays_AFPSNet_Multi-Class_Part_Parsing_Based_on_Scaled_Attention_and_Feature_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030184/,"['Training', 'Location awareness', 'Computer vision', 'Fuses', 'Semantics', 'Focusing', 'Benchmark testing']","['Feature Fusion', 'Part Parsing', 'Benchmark', 'Training Strategy', 'Characteristic Scale', 'Prediction Task', 'Objects In The Scene', 'Feature Fusion Module', 'Features Of Different Scales', 'Batch Size', 'Feature Maps', 'Attention Mechanism', 'Level Characteristics', 'Attention Module', 'Segmentation Results', 'Spatial Attention', 'Pose Estimation', 'Object Parts', 'Multi-task Learning', 'Part Segmentation', 'Atrous Spatial Pyramid Pooling', 'Channel-wise Attention', 'Object Appearance', 'Different Levels Of Features', 'Attention Vector', 'Object Labels', 'Feature Concatenation', 'Boundary Prediction', 'Graph Matching', 'Global Average Pooling']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",4,"Multi-class part parsing is a dense prediction task that seeks to simultaneously detect multiple objects and the semantic parts within these objects in the scene. This problem is important in providing detailed object understanding, but is challenging due to the existence of both class-level and part-level ambiguities. In this paper, we propose to integrate an attention refinement module and a feature fusion module to tackle the part-level ambiguity. The attention refinement module aims to enhance the feature representations by focusing on important features. The feature fusion module aims to improve the fusion operation for different scales of features. We also propose an object-to-part training strategy to tackle the class-level ambiguity, which improves the localization of parts by exploiting prior knowledge of objects. The experimental results demonstrated the effectiveness of the proposed modules and the training strategy, and showed that our proposed method achieved state-of-the-art performance on the benchmark datasets."
ALPINE: Improving Remote Heart Rate Estimation Using Contrastive Learning,"Lokendra Birla, Sneha Shukla, Anup Kumar Gupta, Puneet Gupta",Indian Institute of Technology Indore,100,India,0,,"Heart rate (HR) is a crucial physiological indicator of human health and can be used to detect cardiovascular disorders. The traditional HR estimation methods, such as electrocardiograms (ECG) and photoplethysmographs, require skin contact. Due to the increased risk of viral infection from skin contact, these approaches are avoided in the ongoing COVID-19 pandemic. Alternatively, one can use the non-contact HR estimation technique, remote photoplethysmography (rPPG), wherein HR is estimated from the facial videos of a person. Unfortunately, the existing rPPG methods perform poorly in the presence of facial deformations. Recently, there has been a proliferation of deep learning networks for rPPG. However, these networks require large-scale labelled data for better generalization. To alleviate these shortcomings, we propose a method ALPINE, that is, A noveL rPPG technique for Improving the remote heart rate estimatioN using contrastive lEarning. ALPINE utilizes the contrastive learning framework during training to address the issue of limited labelled data and introduces diversity in the data samples for better network generalization. Additionally, we introduce a novel hybrid loss comprising contrastive loss, signal-to-noise ratio (SNR) loss and data fidelity loss. Our novel contrastive loss maximizes the similarity between the rPPG information from different facial regions, thereby minimizing the effect of local noise. The SNR loss improves the quality of temporal signals, and the data fidelity loss ensures that the correct rPPG signal is extracted. Our extensive experiments on publicly available datasets demonstrate that the proposed method, ALPINE outperforms the previous well-known rPPG methods.",https://openaccess.thecvf.com/content/WACV2023/html/Birla_ALPINE_Improving_Remote_Heart_Rate_Estimation_Using_Contrastive_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Birla_ALPINE_Improving_Remote_Heart_Rate_Estimation_Using_Contrastive_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030106/,"['Heart rate', 'Training', 'Diversity reception', 'Estimation', 'Transformers', 'Skin', 'Plethysmography']","['Self-supervised Learning', 'Heart Rate Estimation', 'Deep Learning', 'Plethysmography', 'Skin Contact', 'Temporal Signal', 'Contrastive Loss', 'Ongoing COVID-19 Pandemic', 'Photoplethysmography', 'Facial Deformity', 'Video Of A Person', 'Loss Function', 'Training Set', 'Convolutional Neural Network', 'Denoising', 'Facial Expressions', 'Long Short-term Memory', 'Data Augmentation', 'Independent Component Analysis', 'Color Variation', 'Augmented Signaling', 'Pulse Signal', 'Heart Rate Range', 'Original Signal', 'Deep Learning-based Methods', 'Learning-based Methods', 'Triplet Loss', 'Illumination Variations', 'Motor Variability', '1D Convolutional Neural Network']",['Applications: Biomedical/healthcare/medicine'],7,"Heart rate (HR) is a crucial physiological indicator of human health and can be used to detect cardiovascular disorders. The traditional HR estimation methods, such as electrocardiograms (ECG) and photoplethysmographs, require skin contact. Due to the increased risk of viral in- fection from skin contact, these approaches are avoided in the ongoing COVID-19 pandemic. Alternatively, one can use the non-contact HR estimation technique, remote photo- plethysmography (rPPG), wherein HR is estimated from the facial videos of a person. Unfortunately, the existing rPPG methods perform poorly in the presence of facial deformations. Recently, there has been a proliferation of deep learning networks for rPPG. However, these networks require large-scale labelled data for better generalization. To alleviate these shortcomings, we propose a method ALPINE, that is, A noveL rPPG technique for Improving the remote heart rate estimatioN using contrastive lEarning. ALPINE utilizes the contrastive learning framework during training to address the issue of limited labelled data and introduces diversity in the data samples for better network generalization. Additionally, we introduce a novel hybrid loss comprising contrastive loss, signal-to-noise ratio (SNR) loss and data fidelity loss. Our novel contrastive loss maximizes the similarity between the rPPG information from different facial regions, thereby minimizing the effect of local noise. The SNR loss improves the quality of temporal signals, and the data fidelity loss ensures that the correct rPPG signal is extracted. Our extensive experiments on publicly available datasets demonstrate that the proposed method, ALPINE outperforms the previous well-known rPPG methods."
ARUBA: An Architecture-Agnostic Balanced Loss for Aerial Object Detection,"Rebbapragada V. C. Sairam, Monish Keswani, Uttaran Sinha, Nishit Shah, Vineeth N. Balasubramanian",Indian Institute of Technology Hyderabad,100,India,0,,"Deep neural networks tend to reciprocate the bias of their training dataset. In object detection, the bias exists in the form of various imbalances such as class, background-foreground, and object size. In this paper, we denote size of an object as the number of pixels it covers in an image and size imbalance as the over-representation of certain sizes of objects in a dataset. We aim to address the problem of size imbalance in drone-based aerial image datasets. Existing methods for solving size imbalance are based on architectural changes that utilize multiple scales of images or feature maps for detecting objects of different sizes. We, on the other hand, propose a novel ARchitectUre-agnostic BAlanced Loss (ARUBA) that can be applied as a plugin on top of any object detection model. It follows a neighborhood-driven approach inspired by the ordinality of object size. We evaluate the effectiveness of our approach through comprehensive experiments on aerial datasets such as HRSC2016, DOTAv1.0, DOTAv1.5 and VisDrone and obtain consistent improvement in performance.",https://openaccess.thecvf.com/content/WACV2023/html/Sairam_ARUBA_An_Architecture-Agnostic_Balanced_Loss_for_Aerial_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sairam_ARUBA_An_Architecture-Agnostic_Balanced_Loss_for_Aerial_Object_Detection_WACV_2023_paper.pdf,,,2210.04574,main,Poster,https://ieeexplore.ieee.org/document/10030576/,"['Training', 'Deep learning', 'Computer vision', 'Neural networks', 'Object detection', 'Feature extraction']","['Object Detection', 'Loss Of Balance', 'Aerial Object', 'Aerial Object Detection', 'Deep Neural Network', 'Ordination', 'Feature Maps', 'Aerial Images', 'Object Size', 'Imbalance Problem', 'Imbalanced Datasets', 'Object Dataset', 'Objects Of Different Sizes', 'Aerial Image Dataset', 'Size Distribution', 'Validation Set', 'Ordinal Scale', 'Bounding Box', 'Baseline Methods', 'Small Objects', 'Object Instances', 'Severe Imbalance', 'Neighborhood Effects', 'Popular Datasets', 'Variation In Orientation', 'Object Detection Dataset', 'Class Imbalance', 'Small Instances', 'Feature Pyramid Network', 'Data Generation Method']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",6,"Deep neural networks tend to reciprocate the bias of their training dataset. In object detection, the bias exists in the form of various imbalances such as class, background-foreground, and object size. In this paper, we denote size of an object as the number of pixels it covers in an image and size imbalance as the over-representation of certain sizes of objects in a dataset. We aim to address the problem of size imbalance in drone-based aerial image datasets. Existing methods for solving size imbalance are based on architectural changes that utilize multiple scales of images or feature maps for detecting objects of different sizes. We, on the other hand, propose a novel ARchitectUre-agnostic BAlanced Loss (ARUBA) that can be applied as a plugin on top of any object detection model. It follows a neighborhood-driven approach inspired by the ordinality of object size. We evaluate the effectiveness of our approach through comprehensive experiments on aerial datasets such as HRSC2016, DOTAv1.0, DOTAv1.5 and VisDrone and obtain consistent improvement in performance."
AT-DDPM: Restoring Faces Degraded by Atmospheric Turbulence Using Denoising Diffusion Probabilistic Models,"Nithin Gopalakrishnan Nair, Kangfu Mei, Vishal M. Patel",Johns Hopkins University,100,USA,0,,"Although many long-range imaging systems are designed to support extended vision applications, a natural obstacle to their operation is degradation due to atmospheric turbulence. Atmospheric turbulence causes significant degradation to image quality by introducing blur and geometric distortion. In recent years, various deep learning-based single image atmospheric turbulence mitigation methods, including CNN-based and GAN inversion-based, have been proposed in the literature which attempt to remove the distortion in the image. However, some of these methods are difficult to train and often fail to reconstruct facial features and produce unrealistic results, especially in the case of high turbulence. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained some traction because of their stable training process and their ability to generate high quality images. In this paper, we propose the first DDPM-based solution for the problem of atmospheric turbulence mitigation. We also propose a fast sampling technique for reducing the inference times for conditional DDPMs. Extensive experiments are conducted on synthetic and real-world data to show the significance of our model. To facilitate further research, all codes and pretrained models are publically available at http://github.com/Nithin-GK/AT-DDPM",https://openaccess.thecvf.com/content/WACV2023/html/Nair_AT-DDPM_Restoring_Faces_Degraded_by_Atmospheric_Turbulence_Using_Denoising_Diffusion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nair_AT-DDPM_Restoring_Faces_Degraded_by_Atmospheric_Turbulence_Using_Denoising_Diffusion_WACV_2023_paper.pdf,,http://github.com/Nithin-GK/AT-DDPM,,main,Poster,https://ieeexplore.ieee.org/document/10030292/,['Portable document format'],,"['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Low-level and physics-based vision']",19,"Although many long-range imaging systems are designed to support extended vision applications, a natural obstacle to their operation is degradation due to atmospheric turbulence. Atmospheric turbulence causes significant degradation to image quality by introducing blur and geometric distortion. In recent years, various deep learning-based single image atmospheric turbulence mitigation methods, including CNN-based and GAN inversion-based, have been proposed in the literature which attempt to remove the distortion in the image. However, some of these methods are difficult to train and often fail to reconstruct facial features and produce unrealistic results, especially in the case of high turbulence. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained some traction because of their stable training process and their ability to generate high quality images. In this paper, we propose the first DDPM-based solution for the problem of atmospheric turbulence mitigation. We also propose a fast sampling technique for reducing the inference times for conditional DDPMs. Extensive experiments are conducted on synthetic and real-world data to show the significance of our model. To facilitate further research, all codes and pretrained models are publically available at http://github.com/Nithin-GK/AT-DDPM"
ATCON: Attention Consistency for Vision Models,"Ali Mirzazadeh, Florian Dubost, Maxwell Pike, Krish Maniar, Max Zuo, Christopher Lee-Messer, Daniel Rubin","Georgia Institute of Technology; Stanford University; Stanford University, Georgia Institute of Technology",100,USA,0,,"Attention--or attribution--maps methods are methods designed to highlight regions of the model's input that were discriminative for its predictions. However, different attention maps methods can highlight different regions of the input, with sometimes contradictory explanations for a prediction. This effect is exacerbated when the training set is small. This indicates that either the model learned incorrect representations or that the attention maps methods did not accurately estimate the model's representations. We propose an unsupervised fine-tuning method that optimizes the consistency of attention maps and show that it improves both classification performance and the quality of attention maps. We propose an implementation for two state-of-the-art attention computation methods, Grad-CAM and Guided Backpropagation, which relies on an input masking technique. We also show results on Grad-CAM and Integrated Gradients in an ablation study. We evaluate this method on our own dataset of event detection in continuous video recordings of hospital patients aggregated and curated for this work. As a sanity check, we also evaluate the proposed method on PASCAL VOC and SVHN. With the proposed method, with small training sets, we achieve a 6.6 points lift of F1 score over the baselines on our video dataset, a 2.9 point lift of F1 score on PASCAL, and a 1.8 points lift of mean Intersection over Union over Grad-CAM for weakly supervised detection on PASCAL. Those improved attention maps may help clinicians better understand vision model predictions and ease the deployment of machine learning systems into clinical care. We share part of the code for this article at the following repository: https://github.com/alimirzazadeh/SemisupervisedAttention.",https://openaccess.thecvf.com/content/WACV2023/html/Mirzazadeh_ATCON_Attention_Consistency_for_Vision_Models_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mirzazadeh_ATCON_Attention_Consistency_for_Vision_Models_WACV_2023_paper.pdf,,https://github.com/alimirzazadeh/SemisupervisedAttention,2210.09705,main,Poster,https://ieeexplore.ieee.org/document/10030496/,"['Training', 'Backpropagation', 'Computer vision', 'Codes', 'Hospitals', 'Event detection', 'Computational modeling']","['Training Set', 'Classification Performance', 'F1 Score', 'Patient Records', 'Intersection Over Union', 'Continuous Recording', 'Attention Map', 'Video Dataset', 'Sanity Check', 'Input Regions', 'Small Training Set', 'Validation Set', 'Data Augmentation', 'Multi-label', 'Training Strategy', 'Bounding Box', 'Video Clips', 'Correlation Measures', 'Representation Learning', 'Attention Function', 'Self-supervised Learning', 'Few-shot Learning', 'Semi-supervised Learning', 'Consistency Loss', 'Structural Similarity Index Measure', 'Semi-supervised Methods', 'Mean Average Precision', 'Improve Classification Performance', 'Few-shot Classification', 'Medical Image Analysis']","['Applications: Biomedical/healthcare/medicine', 'Social good']",,"Attention–or attribution–maps methods are methods designed to highlight regions of the model’s input that were discriminative for its predictions. However, different attention maps methods can highlight different regions of the input, with sometimes contradictory explanations for a prediction. This effect is exacerbated when the training set is small. This indicates that either the model learned incorrect representations or that the attention maps methods did not accurately estimate the model’s representations. We propose an unsupervised fine-tuning method that optimizes the consistency of attention maps and show that it improves both classification performance and the quality of attention maps. We propose an implementation for two state-of-the-art attention computation methods, Grad-CAM and Guided Backpropagation, which relies on an input masking technique. We also show results on Grad-CAM and Integrated Gradients in an ablation study. We evaluate this method on our own dataset of event detection in continuous video recordings of hospital patients aggregated and curated for this work. As a sanity check, we also evaluate the proposed method on PASCAL VOC and SVHN. With the proposed method, with small training sets, we achieve a 6.6 points lift of F1 score over the baselines on our video dataset, a 2.9 point lift of F1 score on PASCAL, and a 1.8 points lift of mean Intersection over Union over Grad-CAM for weakly supervised detection on PASCAL. Those improved attention maps may help clinicians better understand vision model predictions and ease the deployment of machine learning systems into clinical care. We share part of the code for this article at the following repository: https://github.com/alimirzazadeh/SemisupervisedAttention."
AVE-CLIP: AudioCLIP-Based Multi-Window Temporal Transformer for Audio Visual Event Localization,"Tanvir Mahmud, Diana Marculescu",The University of Texas at Austin,100,USA,0,,"An audio-visual event (AVE) is denoted by the correspondence of the visual and auditory signals in a video segment. Precise localization of the AVEs is very challenging since it demands effective multi-modal feature correspondence to ground the short and long range temporal interactions. Existing approaches struggle in capturing the different scales of multi-modal interaction due to ineffective multi-modal training strategies. To overcome this limitation, we introduce AVE-CLIP, a novel framework that integrates the AudioCLIP pre-trained on large-scale audio-visual data with a multi-window temporal transformer to effectively operate on different temporal scales of video frames. Our contributions are three-fold: (1) We introduce a multi-stage training framework to incorporate AudioCLIP pre-trained with audio-image pairs into the AVE localization task on video frames through contrastive fine-tuning, effective mean video feature extraction, and multi-scale training phases. (2) We propose a multi-domain attention mechanism that operates on both temporal and feature domains over varying timescales to fuse the local and global feature variations. (3) We introduce a temporal refining scheme with event-guided attention followed by a simple-yet-effective post processing step to handle significant variations of the background over diverse events. Our method achieves state-of-the-art performance on the publicly available AVE dataset with 5.9% mean accuracy improvement which proves its superiority over existing approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Mahmud_AVE-CLIP_AudioCLIP-Based_Multi-Window_Temporal_Transformer_for_Audio_Visual_Event_Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mahmud_AVE-CLIP_AudioCLIP-Based_Multi-Window_Temporal_Transformer_for_Audio_Visual_Event_Localization_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030779/,"['Location awareness', 'Training', 'Visualization', 'Computer vision', 'Fuses', 'Refining', 'Computer architecture']","['Audiovisual Events', 'Local Variations', 'Attention Mechanism', 'Video Frames', 'Corresponding Points', 'Global Variables', 'Localization Task', 'Post Processing', 'Auditory Signals', 'Video Segments', 'Video Features', 'Multimodal Features', 'Multimodal Strategy', 'Multimodal Interaction', 'Feature Maps', 'Long Short-term Memory', 'Temporal Features', 'Large-scale Datasets', 'Global Context', 'Window Length', 'Image Encoder', 'Sound Localization', 'Temporal Attention', 'Event Categories', 'Attention Map', 'Entire Video', 'Small Window', 'Effects Of Labels', 'Audio Segments', 'Fusion Strategy']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",11,"An audio-visual event (AVE) is denoted by the correspondence of the visual and auditory signals in a video segment. Precise localization of the AVEs is very challenging since it demands effective multi-modal feature correspondence to ground the short and long range temporal interactions. Existing approaches struggle in capturing the different scales of multi-modal interaction due to ineffective multi-modal training strategies. To overcome this limitation, we introduce AVE-CLIP, a novel framework that integrates the AudioCLIP pre-trained on large-scale audio-visual data with a multi-window temporal transformer to effectively operate on different temporal scales of video frames. Our contributions are three-fold: (1) We introduce a multi-stage training framework to incorporate AudioCLIP pre-trained with audio-image pairs into the AVE localization task on video frames through contrastive fine-tuning, effective mean video feature extraction, and multi-scale training phases. (2) We propose a multi-domain attention mechanism that operates on both temporal and feature domains over varying timescales to fuse the local and global feature variations. (3) We introduce a temporal refining scheme with event-guided attention followed by a simple-yet-effective post processing step to handle significant variations of the background over diverse events. Our method achieves state-of-the-art performance on the publicly available AVE dataset with 5.9% mean accuracy improvement which proves its superiority over existing approaches."
Accelerating Self-Supervised Learning via Efficient Training Strategies,"Mustafa Taha Koçyiğit, Timothy M. Hospedales, Hakan Bilen","University of Edinburgh, UK",100,UK,0,,"Recently the focus of the computer vision community has shifted from expensive supervised learning towards self-supervised learning of visual representations. While the performance gap between supervised and self-supervised has been narrowing, the time for training self-supervised deep networks remains an order of magnitude larger than its supervised counterparts, which hinders progress, imposes carbon cost, and limits societal benefits to institutions with substantial resources. Motivated by these issues, this paper investigates reducing the training time of recent self-supervised methods by various model-agnostic strategies that have not been used for this problem. In particular, we study three strategies: an extendable cyclic learning rate schedule, a matching progressive augmentation magnitude and image resolutions schedule, and a hard positive mining strategy based on augmentation difficulty. We show that all three methods combined lead up to 2.7 times speed-up in the training time of several self-supervised methods while retaining comparable performance to the standard self-supervised learning setting.",https://openaccess.thecvf.com/content/WACV2023/html/Kocyigit_Accelerating_Self-Supervised_Learning_via_Efficient_Training_Strategies_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kocyigit_Accelerating_Self-Supervised_Learning_via_Efficient_Training_Strategies_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030918/,"['Training', 'Schedules', 'Computer vision', 'Visualization', 'Costs', 'Self-supervised learning', 'Transformers']","['Training Efficiency', 'Self-supervised Learning', 'Efficient Training Strategy', 'Learning Rate', 'Image Resolution', 'Supervised Learning', 'Training Time', 'Representation Learning', 'Learning Rate Schedule', 'Number Of Steps', 'Additive Noise', 'Training Costs', 'ImageNet Dataset', 'Fast Training', 'Input Resolution', 'Stochastic Gradient Descent Optimizer', 'Minimum Resolution', 'Loss Of Order', 'ResNet-50 Backbone', 'Small Learning Rate', 'Warm-up Phase', 'Progressive Learning', 'Self-supervised Learning Methods', 'Large Learning Rate', 'Noise Scale', 'Cosine Annealing', 'Self-supervised Training', 'Pretext Task', 'Longer Training Time', 'Highest Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Recently the focus of the computer vision community has shifted from expensive supervised learning towards self-supervised learning of visual representations. While the performance gap between supervised and self-supervised has been narrowing, the time for training self-supervised deep networks remains an order of magnitude larger than its supervised counterparts, which hinders progress, imposes carbon cost, and limits societal benefits to institutions with substantial resources. Motivated by these issues, this paper investigates reducing the training time of recent self-supervised methods by various model-agnostic strategies that have not been used for this problem. In particular, we study three strategies: an extendable cyclic learning rate schedule, a matching progressive augmentation magnitude and image resolutions schedule, and a hard positive mining strategy based on augmentation difficulty. We show that all three methods combined lead up to 2.7 times speed-up in the training time of several self-supervised methods while retaining comparable performance to the standard self-supervised learning setting."
Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets,"Xiangyu Chen, Qinghao Hu, Kaidong Li, Cuncong Zhong, Guanghui Wang","Institute of Automation, Chinese Academy of Sciences, China; Department of EECS, University of Kansas, KS, USA; Department of CS, Toronto Metropolitan University, Toronto, ON, Canada",100,"Canada, China, USA",0,,"Vision Transformers has demonstrated competitive performance on computer vision tasks benefiting from their ability to capture long-range dependencies with multi-head self-attention modules and multi-layer perceptron. However, calculating global attention brings another disadvantage compared with convolutional neural networks, i.e. requiring much more data and computations to converge, which makes it difficult to generalize well on small datasets, which is common in practical applications. Previous works are either focusing on transferring knowledge from large datasets or adjusting the structure for small datasets. After carefully examining the self-attention modules, we discover that the number of trivial attention weights is far greater than the important ones and the accumulated trivial weights are dominating the attention in Vision Transformers due to their large quantity, which is not handled by the attention itself. This will cover useful non-trivial attention and harm the performance when trivial attention includes more noise, e.g. in shallow layers for some backbones. To solve this issue, we proposed to divide attention weights into trivial and non-trivial ones by thresholds, then Suppressing Accumulated Trivial Attention (SATA) weights by proposed Trivial WeIghts Suppression Transformation (TWIST) to reduce attention noise. Extensive experiments on CIFAR-100 and Tiny-ImageNet datasets show that our suppressing method boosts the accuracy of Vision Transformers by up to 2.3%. Code is available at https://github.com/xiangyu8/SATA.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Accumulated_Trivial_Attention_Matters_in_Vision_Transformers_on_Small_Datasets_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Accumulated_Trivial_Attention_Matters_in_Vision_Transformers_on_Small_Datasets_WACV_2023_paper.pdf,,https://github.com/xiangyu8/SATA,2210.12333,main,Poster,https://ieeexplore.ieee.org/document/10030900/,"['Computer vision', 'Codes', 'Focusing', 'Transformers', 'Convolutional neural networks', 'Task analysis']","['Small Datasets', 'Vision Transformer', 'Large Datasets', 'Convolutional Neural Network', 'Multilayer Perceptron', 'Global Attention', 'Shallow Layers', 'Attention Weights', 'Long-range Dependencies', 'Multi-head Self-attention', 'Self-attention Module', 'CIFAR-100 Dataset', 'Learning Rate', 'Sequence Length', 'Transfer Learning', 'Receptive Field', 'Weight Distribution', 'Grid Search', 'Convolution Operation', 'Attention Function', 'Maximum Weight', 'Attention Module', 'Softmax Function', 'Sum Of Weights', 'Self-supervised Learning', 'Relative Threshold', 'Multilayer Perceptron Layer', 'Distribution Of Attention']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",3,"Vision Transformers has demonstrated competitive performance on computer vision tasks benefiting from their ability to capture long-range dependencies with multi-head self-attention modules and multi-layer perceptron. However, calculating global attention brings another disadvantage compared with convolutional neural networks, i.e. requiring much more data and computations to converge, which makes it difficult to generalize well on small datasets, which is common in practical applications. Previous works are either focusing on transferring knowledge from large datasets or adjusting the structure for small datasets. After carefully examining the self-attention modules, we discover that the number of trivial attention weights is far greater than the important ones and the accumulated trivial weights are dominating the attention in Vision Transformers due to their large quantity, which is not handled by the attention itself. This will cover useful non-trivial attention and harm the performance when trivial attention includes more noise, e.g. in shallow layers for some backbones. To solve this issue, we proposed to divide attention weights into trivial and non-trivial ones by thresholds, then Suppressing Accumulated Trivial Attention (SATA) weights by proposed Trivial WeIghts Suppression Transformation (TWIST) to reduce attention noise. Extensive experiments on CIFAR-100 and Tiny-ImageNet datasets show that our suppressing method boosts the accuracy of Vision Transformers by up to 2.3%. Code is available at https://github.com/xiangyu8/SATA."
Action-Aware Masking Network With Group-Based Attention for Temporal Action Localization,"Tae-Kyung Kang, Gun-Hee Lee, Kyung-Min Jin, Seong-Whan Lee","Dept. of Computer Science and Engineering, Korea University, South Korea; Dept. of Artificial Intelligence, Korea University, South Korea",100,South Korea,0,,"Temporal Action Localization (TAL) is a significant and challenging task that searches for subtle human activities in an untrimmed video. To extract snippet-level video features, existing TAL methods commonly use video encoders pre-trained on short-video classification datasets. However, the snippet-level features can incur ambiguity between consecutive frames due to short and poor temporal information, disrupting the precise prediction of action instances. Several methods incorporating temporal relations have been proposed to mitigate this problem; however, they still suffer from poor video features. To address this issue, we propose a novel temporal action localization framework called an Action-aware Masking Network (AMNet). Our method simultaneously refines video features using action-aware attention and considers inherent temporal relations using self-attention and cross-attention mechanisms. First, we present an Action Masking Encoder (AME) that generates an action-aware mask to represent positive characteristics, which is then used to refine snippet-level features to be more salient around actions. Second, we design a Group Attention Module (GAM), which models relations of temporal information and exchanges mutual information by dividing the features into two groups, i.e., long and short-groups. Extensive experiments and ablation studies on two primary benchmark datasets demonstrate the effectiveness of AMNet, and our method achieves state-of-the-art performances on THUMOS-14 and ActivityNet1.3.",https://openaccess.thecvf.com/content/WACV2023/html/Kang_Action-Aware_Masking_Network_With_Group-Based_Attention_for_Temporal_Action_Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kang_Action-Aware_Masking_Network_With_Group-Based_Attention_for_Temporal_Action_Localization_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030208/,"['Location awareness', 'Computer vision', 'Computational modeling', 'Semantics', 'Benchmark testing', 'Feature extraction', 'Task analysis']","['Temporal Localization', 'Action Localization', 'Temporal Action Localization', 'Extensive Experiments', 'Temporal Information', 'Consecutive Frames', 'Primary Dataset', 'Video Features', 'Video Encoding', 'Sigmoid Function', 'Temporal Dimension', 'Confidence Score', 'Hyperbolic Tangent', 'Action Recognition', 'Multi-scale Features', 'Action Classes', 'Matching Score', 'Action Areas', 'Feature Pyramid Network', 'Triplet Loss', 'Temporal Boundaries', 'Matching Loss', '2nd Row', 'Prediction Head', '1st Row', 'Action Detection', 'Dynamic Datasets', 'Absolute Improvement', 'AdamW Optimizer', 'End Time']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Temporal Action Localization (TAL) is a significant and challenging task that searches for subtle human activities in an untrimmed video. To extract snippet-level video features, existing TAL methods commonly use video encoders pre-trained on short-video classification datasets. However, the snippet-level features can incur ambiguity between consecutive frames due to short and poor temporal information, disrupting the precise prediction of action instances. Several methods incorporating temporal relations have been proposed to mitigate this problem; however, they still suffer from poor video features. To address this issue, we propose a novel temporal action localization framework called an Action-aware Masking Network (AMNet). Our method simultaneously refines video features using action-aware attention and considers inherent temporal relations using self-attention and cross-attention mechanisms. First, we present an Action Masking Encoder (AME) that generates an action-aware mask to represent positive characteristics, which is then used to refine snippet-level features to be more salient around actions. Second, we design a Group Attention Module (GAM), which models relations of temporal information and exchanges mutual information by dividing the features into two groups, i.e., long and short-groups. Extensive experiments and ablation studies on two primary benchmark datasets demonstrate the effectiveness of AM-Net, and our method achieves state-of-the-art performances on THUMOS-14 and ActivityNet1.3."
AdaNorm: Adaptive Gradient Norm Correction Based Optimizer for CNNs,"Shiv Ram Dubey, Satish Kumar Singh, Bidyut Baran Chaudhuri","Techno India University, Kolkata, India and Indian Statistical Institute, Kolkata, India; Computer Vision and Biometrics Lab, Indian Institute of Information Technology, Allahabad",100,India,0,,"The stochastic gradient descent (SGD) optimizers are generally used to train the convolutional neural networks (CNNs). In recent years, several adaptive momentum based SGD optimizers have been introduced, such as Adam, diffGrad, Radam and AdaBelief. However, the existing SGD optimizers do not exploit the gradient norm of past iterations and lead to poor convergence and performance. In this paper, we propose a novel AdaNorm based SGD optimizers by correcting the norm of gradient in each iteration based on the adaptive training history of gradient norm. By doing so, the proposed optimizers are able to maintain high and representive gradient throughout the training and solves the low and atypical gradient problems. The proposed concept is generic and can be used with any existing SGD optimizer. We show the efficacy of the proposed AdaNorm with four state-of-the-art optimizers, including Adam, diffGrad, Radam and AdaBelief. We depict the performance improvement due to the proposed optimizers using three CNN models, including VGG16, ResNet18 and ResNet50, on three benchmark object recognition datasets, including CIFAR10, CIFAR100 and TinyImageNet.",https://openaccess.thecvf.com/content/WACV2023/html/Dubey_AdaNorm_Adaptive_Gradient_Norm_Correction_Based_Optimizer_for_CNNs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dubey_AdaNorm_Adaptive_Gradient_Norm_Correction_Based_Optimizer_for_CNNs_WACV_2023_paper.pdf,,,2210.06364,main,Poster,https://ieeexplore.ieee.org/document/10030799/,"['Training', 'Computer vision', 'Stochastic processes', 'Benchmark testing', 'Boosting', 'History', 'Convolutional neural networks']","['Convolutional Neural Network', 'Gradient Norm', 'Object Recognition', 'Stochastic Gradient Descent', 'Convolutional Neural Network Model', 'Stochastic Gradient Descent Optimizer', 'Iterative Gradient', 'Learning Rate', 'Hyperparameters', 'Batch Size', 'Adam Optimizer', 'Training Images', 'Second Moment', 'Bias Correction', 'Training Loss', 'Training Iterations', 'RMSprop', 'Intuitive Explanation', 'Start Of Training', 'Adaptive Optimization', 'CIFAR-100 Dataset', 'ResNet-18 Model', 'VGG16 Model', 'ResNet-50 Model', 'Small Learning Rate', 'Current Gradient', 'Impact Of Correction', 'Training Epochs', 'Previous Iteration', 'Object Classification']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"The stochastic gradient descent (SGD) optimizers are generally used to train the convolutional neural networks (CNNs). In recent years, several adaptive momentum based SGD optimizers have been introduced, such as Adam, diffGrad, Radam and AdaBelief. However, the existing SGD optimizers do not exploit the gradient norm of past iterations and lead to poor convergence and performance. In this paper, we propose a novel AdaNorm based SGD optimizers by correcting the norm of gradient in each iteration based on the adaptive training history of gradient norm. By doing so, the proposed optimizers are able to maintain high and representive gradient throughout the training and solves the low and atypical gradient problems. The proposed concept is generic and can be used with any existing SGD optimizer. We show the efficacy of the proposed AdaNorm with four state-of-the-art optimizers, including Adam, diffGrad, Radam and AdaBelief. We depict the performance improvement due to the proposed optimizers using three CNN models, including VGG16, ResNet18 and ResNet50, on three benchmark object recognition datasets, including CIFAR10, CIFAR100 and TinyImageNet."
Adaptive Feature Fusion for Cooperative Perception Using LiDAR Point Clouds,"Donghao Qiao, Farhana Zulkernine","Queen’s University, Canada",100,Canada,0,,"Cooperative perception allows a Connected Autonomous Vehicle (CAV) to interact with the other CAVs in the vicinity to enhance perception of surrounding objects to increase safety and reliability. It can compensate for the limitations of the conventional vehicular perception such as blind spots, low resolution, and weather effects. An effective feature fusion model for the intermediate fusion methods of cooperative perception can improve feature selection and information aggregation to further enhance the perception accuracy. We propose adaptive feature fusion models with trainable feature selection modules. One of our proposed models Spatial-wise Adaptive feature Fusion (S-AdaFusion) outperforms all other State-of-the-Arts (SOTAs) on two subsets of the OPV2V dataset: Default CARLA Towns for vehicle detection and the Culver City for domain adaptation. In addition, previous studies have only tested cooperative perception for vehicle detection. A pedestrian, however, is much more likely to be seriously injured in a traffic accident. We evaluate the performance of cooperative perception for both vehicle and pedestrian detection using the CODD dataset. Our architecture achieves higher Average Precision (AP) than other existing models for both vehicle and pedestrian detection on the CODD dataset. The experiments demonstrate that cooperative perception also improves the pedestrian detection accuracy compared to the conventional single vehicle perception process.",https://openaccess.thecvf.com/content/WACV2023/html/Qiao_Adaptive_Feature_Fusion_for_Cooperative_Perception_Using_LiDAR_Point_Clouds_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qiao_Adaptive_Feature_Fusion_for_Cooperative_Perception_Using_LiDAR_Point_Clouds_WACV_2023_paper.pdf,,,2208.00116,main,Poster,https://ieeexplore.ieee.org/document/10030450/,"['Point cloud compression', 'Adaptation models', 'Laser radar', 'Adaptive systems', 'Vehicle detection', 'Neural networks', 'Urban areas']","['Point Cloud', 'Feature Fusion', 'LiDAR Point Clouds', 'Cooperative Perception', 'Adaptive Feature Fusion', 'Perceptual Processing', 'Adaptive Model', 'Autonomous Vehicles', 'Average Precision', 'Fusion Method', 'Blind Spot', 'Domain Adaptation', 'Vehicle Detection', 'Pedestrian Detection', 'Convolutional Neural Network', 'Feature Maps', 'Object Detection', 'Data Fusion', 'Feature Integration', 'Intermediate Features', 'Early Fusion', '3D Object Detection', 'Intermediate Feature Maps', 'Late Fusion', '3D Bounding Box', 'Traditional Perception', 'Point Cloud Data', 'Camera Data', 'Graph Neural Networks', 'Single Shot Detector']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Robotics']",19,"Cooperative perception allows a Connected Autonomous Vehicle (CAV) to interact with the other CAVs in the vicinity to enhance perception of surrounding objects to increase safety and reliability. It can compensate for the limitations of the conventional vehicular perception such as blind spots, low resolution, and weather effects. An effective feature fusion model for the intermediate fusion methods of cooperative perception can improve feature selection and information aggregation to further enhance the perception accuracy. We propose adaptive feature fusion models with trainable feature selection modules. One of our proposed models Spatial-wise Adaptive feature Fusion (S-AdaFusion) outperforms all other State-of-the-Arts (SO-TAs) on two subsets of the OPV2V dataset: Default CARLA Towns for vehicle detection and the Culver City for domain adaptation. In addition, previous studies have only tested cooperative perception for vehicle detection. A pedestrian, however, is much more likely to be seriously injured in a traffic accident. We evaluate the performance of cooperative perception for both vehicle and pedestrian detection using the CODD dataset. Our architecture achieves higher Average Precision (AP) than other existing models for both vehicle and pedestrian detection on the CODD dataset. The experiments demonstrate that cooperative perception also improves the pedestrian detection accuracy compared to the conventional single vehicle perception process."
Adaptive Local-Component-Aware Graph Convolutional Network for One-Shot Skeleton-Based Action Recognition,"Anqi Zhu, Qiuhong Ke, Mingming Gong, James Bailey","School of Computing and Information Systems, The University of Melbourne; Department of Data Science & AI, Monash University; School of Mathematics and Statistics, The University of Melbourne",100,Australia,0,,"Skeleton-based action recognition receives increasing attention because skeleton sequences reduce training complexity by eliminating visual information irrelevant to actions. To further improve sample efficiency, meta-learning-based one-shot learning solutions were developed for skeleton-based action recognition. These methods predict by finding the nearest neighbors according to the similarity between instance-level global embedding. However, such measurement holds unstable representativity due to inadequate generalized learning on the averaged local invariant and noisy features, while intuitively, steady and fine-grained recognition relies on determining key local body movements. To address this limitation, we present the Adaptive Local-Component-aware Graph Convolutional Network, which replaces the comparison metric with a focused sum of similarity measurements on aligned local embedding of action-critical spatial/temporal segments. Comprehensive one-shot experiments on the public benchmark of NTU-RGB+D 120 indicate that our method provides a stronger representation than the global embedding and helps our model reach state-of-the-art.",https://openaccess.thecvf.com/content/WACV2023/html/Zhu_Adaptive_Local-Component-Aware_Graph_Convolutional_Network_for_One-Shot_Skeleton-Based_Action_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhu_Adaptive_Local-Component-Aware_Graph_Convolutional_Network_for_One-Shot_Skeleton-Based_Action_Recognition_WACV_2023_paper.pdf,,,2209.10073,main,Poster,,,,,,
Adaptive Sample Selection for Robust Learning Under Label Noise,"Deep Patel, P. S. Sastry","Indian Institute of Science, Bangalore, India - 560012",100,India,0,,"Deep Neural Networks (DNNs) have been shown to be susceptible to memorization or overfitting in the presence of noisily-labelled data. For the problem of robust learning under such noisy data, several algorithms have been proposed. A prominent class of algorithms rely on sample selection strategies wherein, essentially, a fraction of samples with loss values below a certain threshold are selected for training. These algorithms are sensitive to such thresholds, and it is difficult to fix or learn these thresholds. Often, these algorithms also require information such as label noise rates which are typically unavailable in practice. In this paper, we propose an adaptive sample selection strategy that relies only on batch statistics of a given mini-batch to provide robustness against label noise. The algorithm does not have any additional hyperparameters for sample selection, does not need any information on noise rates and does not need access to separate data with clean labels. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Patel_Adaptive_Sample_Selection_for_Robust_Learning_Under_Label_Noise_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Patel_Adaptive_Sample_Selection_for_Robust_Learning_Under_Label_Noise_WACV_2023_paper.pdf,,https://github.com/dbp1994/masters_thesis_codes/tree/main/BARE11,2106.15292,main,Poster,https://ieeexplore.ieee.org/document/10030984/,"['Training', 'Knowledge engineering', 'Deep learning', 'Computer vision', 'Heuristic algorithms', 'Neural networks', 'Benchmark testing']","['Adaptive Selection', 'Label Noise', 'Robust Learning', 'Adaptive Sample Selection', 'Neural Network', 'Hyperparameters', 'Deep Neural Network', 'Loss Value', 'Fraction Of Samples', 'Noisy Data', 'Noise Rate', 'Training Set', 'Learning Algorithms', 'Posterior Probability', 'Validation Set', 'Test Accuracy', 'Clean Data', 'Precision And Recall', 'Cognitive Evaluation', 'Small Loss', 'Types Of Noise', 'Noisy Labels', 'Curriculum Learning', 'Self-paced Learning', 'Set Of Classes', 'Clean Samples', 'Baseline Schemes', 'Self-supervised Learning', 'Extra Data', 'Warm-up Period']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",18,"Deep Neural Networks (DNNs) have been shown to be susceptible to memorization or overfitting in the presence of noisily-labelled data. For the problem of robust learning under such noisy data, several algorithms have been proposed. A prominent class of algorithms rely on sample selection strategies wherein, essentially, a fraction of samples with loss values below a certain threshold are selected for training. These algorithms are sensitive to such thresholds, and it is difficult to fix or learn these thresholds. Often, these algorithms also require information such as label noise rates which are typically unavailable in practice. In this paper, we propose an adaptive sample selection strategy that relies only on batch statistics of a given mini-batch to provide robustness against label noise. The algorithm does not have any additional hyperparameters for sample selection, does not need any information on noise rates and does not need access to separate data with clean labels. We empirically demonstrate the effectiveness of our algorithm on benchmark datasets.
<sup>1</sup>"
Adaptively-Realistic Image Generation From Stroke and Sketch With Diffusion Model,"Shin-I Cheng, Yu-Jie Chen, Wei-Chen Chiu, Hung-Yu Tseng, Hsin-Ying Lee","National Chiao Tung University, Taiwan; Meta; Snap Inc.",33.33333333,Taiwan,66.66666667,USA,"Generating images from hand-drawings is a crucial and fundamental task in content creation. The translation is difficult as there exist infinite possibilities and the different users usually expect different outcomes. Therefore, we propose a unified framework supporting a three-dimensional control over the image synthesis from sketches and strokes based on diffusion models. Users can not only decide the level of faithfulness to the input strokes and sketches, but also the degree of realism, as the user inputs are usually not consistent with the real images. Qualitative and quantitative experiments demonstrate that our framework achieves state-of-the-art performance while providing flexibility in generating customized images with control over shape, color, and realism. Moreover, our method unleashes applications such as editing on real images, generation with partial sketches and strokes, and multi-domain multi-modal synthesis.",https://openaccess.thecvf.com/content/WACV2023/html/Cheng_Adaptively-Realistic_Image_Generation_From_Stroke_and_Sketch_With_Diffusion_Model_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cheng_Adaptively-Realistic_Image_Generation_From_Stroke_and_Sketch_With_Diffusion_Model_WACV_2023_paper.pdf,https://cyj407.github.io/DiSS/,,2208.12675,main,Poster,https://ieeexplore.ieee.org/document/10030137/,"['Solid modeling', 'Computer vision', 'Adaptation models', 'Image synthesis', 'Shape', 'Image color analysis', 'Process control']","['Diffusion Model', 'Image Generation', 'Content Creation', 'Image Synthesis', 'Degree Of Realism', 'Markov Chain', 'Generation Process', 'Interest For Applications', 'Generative Adversarial Networks', 'Perception Of Quality', 'Reference Image', 'Target Distribution', 'Color Information', 'Iterative Refinement', 'Isotropic Gaussian', 'Supplement For Details', 'Stroke Conditions', 'Degree Of Fidelity', 'Inception Distance', 'Fréchet Inception Distance', 'Contour Images']","['Algorithms: Computational photography', 'image and video synthesis']",30,"Generating images from hand-drawings is a crucial and fundamental task in content creation. The translation is difficult as there exist infinite possibilities and the different users usually expect different outcomes. Therefore, we propose a unified framework supporting a three-dimensional control over the image synthesis from sketches and strokes based on diffusion models. Users can not only decide the level of faithfulness to the input strokes and sketches, but also the degree of realism, as the user inputs are usually not consistent with the real images. Qualitative and quantitative experiments demonstrate that our framework achieves state-of-the-art performance while providing flexibility in generating customized images with control over shape, color, and realism. Moreover, our method unleashes applications such as editing on real images, generation with partial sketches and strokes, and multi-domain multi-modal synthesis."
Addressing Feature Suppression in Unsupervised Visual Representations,"Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, Rogerio Feris, Piotr Indyk, Dina Katabi",MIT CSAIL; MIT-IBM Watson AI Lab,100,USA,0,,"Contrastive learning is one of the fastest growing research areas in machine learning due to its ability to learn useful representations without labeled data. However, contrastive learning is susceptible to feature suppression - i.e., it may discard important information relevant to the task of interest, and learn irrelevant features. Past work has addressed this limitation via handcrafted data augmentations that eliminate irrelevant information. This approach however does not work across all datasets and tasks. Further, data augmentations fail in addressing feature suppression in multi-attribute classification when one attribute can suppress features relevant to other attributes. In this paper, we analyze the objective function of contrastive learning and formally prove that it is vulnerable to feature suppression. We then present Predictive Contrastive Learning (PrCL), a framework for learning unsupervised representations that are robust to feature suppression. The key idea is to force the learned representation to predict the input, and hence prevent it from discarding important information. Extensive experiments verify that PrCL is robust to feature suppression and outperforms state-of-the-art contrastive learning methods on a variety of datasets and tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Addressing_Feature_Suppression_in_Unsupervised_Visual_Representations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Addressing_Feature_Suppression_in_Unsupervised_Visual_Representations_WACV_2023_paper.pdf,,,2012.09962,main,Poster,https://ieeexplore.ieee.org/document/10030871/,"['Learning systems', 'Visualization', 'Computer vision', 'Force', 'Machine learning', 'Linear programming', 'Task analysis']","['Unsupervised Representation', 'Data Augmentation', 'Representation Learning', 'Self-supervised Learning', 'Irrelevant Features', 'Area Of Machine Learning', 'Local Minima', 'Unsupervised Learning', 'Feature Learning', 'Global Minimum', 'Gender Binary', 'Prediction Task', 'Input Information', 'Saddle Point', 'Simple Features', 'Pose Estimation', 'Prediction Loss', 'Contrastive Loss', 'Formal Proof', 'Color Distribution', 'Unsupervised Representation Learning', 'Latent Code', 'Diverse Tasks', 'Digit Classification', 'Pretext Task', 'Background Objects', 'Human Supervision', 'Supplemental Material', 'Positive Samples', 'Reconstruction Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",3,"Contrastive learning is one of the fastest growing research areas in machine learning due to its ability to learn useful representations without labeled data. However, contrastive learning is susceptible to feature suppression – i.e., it may discard important information relevant to the task of interest, and learn irrelevant features. Past work has addressed this limitation via handcrafted data augmentations that eliminate irrelevant information. This approach however does not work across all datasets and tasks. Further, data augmentations fail in addressing feature suppression in multi-attribute classification when one attribute can suppress features relevant to other attributes. In this paper, we analyze the objective function of contrastive learning and formally prove that it is vulnerable to feature suppression. We then present Predictive Contrastive Learning (PrCL), a framework for learning unsupervised representations that are robust to feature suppression. The key idea is to force the learned representation to predict the input, and hence prevent it from discarding important information. Extensive experiments verify that PrCL is robust to feature suppression and outperforms state-of-the-art contrastive learning methods on a variety of datasets and tasks."
Adversarial Local Distribution Regularization for Knowledge Distillation,"Thanh Nguyen-Duc, Trung Le, He Zhao, Jianfei Cai, Dinh Phung","Department of Data Science and AI, Monash University; CSIRO’s Data61",100,Australia,0,,"Knowledge distillation is a process of distilling information from a large model with significant knowledge capacity (teacher) to enhance a smaller model (student). Therefore, exploring the properties of the teacher is the key to improving student performance (e.g., teacher decision boundaries). One decision boundary exploring technique is to leverage adversarial attack methods, which add crafted perturbations within a ball constraint to clean inputs to create attack examples of the teacher called adversarial examples. These adversarial examples are informative examples because they are near decision boundaries. In this paper, we formulate a teacher adversarial local distribution, a set of all adversarial examples within the ball constraint given an input. This distribution is used to sufficiently explore the decision boundaries of the teacher by covering the full spectrum of possible teacher model perturbations. The student model is then regularized by matching the loss between teacher and student using these adversarial example inputs. We conducted a number of experiments on CIFAR-100 and Imagenet datasets to illustrate this teacher adversarial local distribution regularization (TALD) can be applied to improve performance of many existing knowledge distillation methods (e.g., KD, FitNet, CRD, VID, FT, etc.).",https://openaccess.thecvf.com/content/WACV2023/html/Nguyen-Duc_Adversarial_Local_Distribution_Regularization_for_Knowledge_Distillation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nguyen-Duc_Adversarial_Local_Distribution_Regularization_for_Knowledge_Distillation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030150/,"['Computer vision', 'Perturbation methods']","['Adversarial Regularization', 'Teacher Model', 'Small Model', 'Decision Boundary', 'Student Model', 'Adversarial Attacks', 'Adversarial Examples', 'Adversary Model', 'Attack Methods', 'CIFAR-100 Dataset', 'Gradient Descent', 'Deep Learning Models', 'Markov Chain Monte Carlo', 'Kullback-Leibler', 'Additional Loss', 'Vanilla', 'Clean Samples', 'Random Initialization', 'Intermediate Representation', 'Iterative Update', 'Architectural Style', 'Fast Gradient Sign Method', 'Regularization Loss', 'Projected Gradient Descent']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods']",4,"Knowledge distillation is a process of distilling information from a large model with significant knowledge capacity (teacher) to enhance a smaller model (student). Therefore, exploring the properties of the teacher is the key to improving student performance (e.g., teacher decision boundaries). One decision boundary exploring technique is to leverage adversarial attack methods, which add crafted perturbations within a ball constraint to clean inputs to create attack examples of the teacher called adversarial examples. These adversarial examples are informative examples because they are near decision boundaries. In this paper, we formulate a teacher adversarial local distribution, a set of all adversarial examples within the ball constraint given an input. This distribution is used to sufficiently explore the decision boundaries of the teacher by covering the full spectrum of possible teacher model perturbations. The student model is then regularized by matching the loss between teacher and student using these adversarial example inputs. We conducted a number of experiments on CIFAR-100 and Imagenet datasets to illustrate this teacher adversarial local distribution regularization (TALD) can be applied to improve performance of many existing knowledge distillation methods (e.g., KD, FitNet, CRD, VID, FT, etc.)."
Adversarial Robustness in Discontinuous Spaces via Alternating Sampling & Descent,"Rahul Venkatesh, Eric Wong, Zico Kolter","University of Pennsylvania, Philadelphia, PA, USA; Stanford University, Stanford, CA, USA; Carnegie Mellon University and Bosch Center for AI, Pittsburgh, PA, USA",100,USA,0,,"Several works have shown that deep learning models are vulnerable to adversarial attacks where seemingly simple label-preserving changes to the input image lead to incorrect predictions. To combat this, gradient based adversarial training is generally employed as a standard defense mechanism. However, in cases where the loss landscape is discontinuous with respect to a given perturbation set, first order methods get stuck in local optima, and fail to defend against threat. This is often a problem for many physically realizable perturbation sets such as 2D affine transformations and 3D scene parameters. To work in such settings, we introduce a new optimization framework that alternates between global zeroth order sampling and local gradient updates to compute strong adversaries that can be used to harden the model against attack. Further, we design a powerful optimization algorithm using this framework, called Alternating Evolutionary Sampling and Descent (ASD), which combines an evolutionary search strategy (viz. covariance matrix adaptation) with gradient descent. We consider two settings with discontinuous/discrete and non-convex loss landscapes to evaluate ASD: a) 3D scene parameters and b) 2D patch attacks, and find that it achieves state-of-the-art results on adversarial robustness.",https://openaccess.thecvf.com/content/WACV2023/html/Venkatesh_Adversarial_Robustness_in_Discontinuous_Spaces_via_Alternating_Sampling__Descent_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Venkatesh_Adversarial_Robustness_in_Discontinuous_Spaces_via_Alternating_Sampling__Descent_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030832/,"['Training', 'Solid modeling', 'Three-dimensional displays', 'Perturbation methods', 'Pipelines', 'Predictive models', 'Search problems']","['Joint Space', 'Adversarial Robustness', 'Deep Learning', 'Covariance Matrix', 'Optimization Algorithm', 'Local Optimum', 'Evolutionary Strategy', 'Optimization Framework', 'Adversarial Training', '3D Scene', 'Adversarial Attacks', 'Gradient Update', 'Evolutionary Search', 'Optimization Method', 'Dimensional Space', 'Light Signal', 'Generative Adversarial Networks', 'Feasible Set', 'Types Of Attacks', 'Search Optimization', 'Projected Gradient Descent', 'Adversarial Examples', 'Attack Methods', 'Random Search', 'Candidate Solutions', 'HSV Color', 'Low-dimensional Settings', 'Threat Model', 'Gradient Information', 'Search Volume']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",,"Several works have shown that deep learning models are vulnerable to adversarial attacks where seemingly simple label-preserving changes to the input image lead to incorrect predictions. To combat this, gradient based adversarial training is generally employed as a standard defense mechanism. However, in cases where the loss landscape is discontinuous with respect to a given perturbation set, first order methods get stuck in local optima, and fail to defend against threat. This is often a problem for many physically realizable perturbation sets such as 2D affine transformations and 3D scene parameters. To work in such settings, we introduce a new optimization framework that alternates between global zeroth order sampling and local gradient updates to compute strong adversaries that can be used to harden the model against attack. Further, we design a powerful optimization algorithm using this framework, called Alternating Evolutionary Sampling and Descent (ASD), which combines an evolutionary search strategy (viz. covariance matrix adaptation) with gradient descent. We consider two settings with discontinuous/discrete and non-convex loss landscapes to evaluate ASD: a) 3D scene parameters and b) 2D patch attacks, and find that it achieves state-of-the-art results on adversarial robustness."
AdvisIL - A Class-Incremental Learning Advisor,"Eva Feillet, Grégoire Petit, Adrian Popescu, Marina Reyboz, Céline Hudelot","Universitè Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France; Universitè Paris-Saclay, CentraleSup´elec, MICS, France; Universitè Grenoble Alpes, CEA, LIST, F-38000 Grenoble, France; LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France",100,France,0,,"Recent class-incremental learning methods combine deep neural architectures and learning algorithms to handle streaming data under memory and computational constraints. The performance of existing methods varies depending on the characteristics of the incremental process. To date, there is no other approach than to test all pairs of learning algorithms and neural architectures on the training data available at the start of the learning process to select a suited algorithm-architecture combination. To tackle this problem, in this article, we introduce AdvisIL, a method which takes as input the main characteristics of the incremental process (memory budget for the deep model, initial number of classes, size of incremental steps) and recommends an adapted pair of learning algorithm and neural architecture. The recommendation is based on a similarity between the user-provided settings and a large set of pre-computed experiments. AdvisIL makes class-incremental learning easier, since users do not need to run cumbersome experiments to design their system. We evaluate our method on four datasets under six incremental settings and three deep model sizes. We compare six algorithms and three deep neural architectures. Results show that AdvisIL has better overall performance than any of the individual combinations of a learning algorithm and a neural architecture. AdvisIL's code is available at https://github.com/EvaJF/AdvisIL.",https://openaccess.thecvf.com/content/WACV2023/html/Feillet_AdvisIL_-_A_Class-Incremental_Learning_Advisor_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Feillet_AdvisIL_-_A_Class-Incremental_Learning_Advisor_WACV_2023_paper.pdf,,https://github.com/EvaJF/AdvisIL,,main,Poster,https://ieeexplore.ieee.org/document/10030995/,"['Learning systems', 'Computer vision', 'Adaptation models', 'Codes', 'Memory management', 'Training data']","['Class-incremental Learning', 'Neural Architecture', 'Deep Architecture', 'Incremental Steps', 'Memory Constraints', 'Pairing Algorithm', 'Neural Network', 'Model Performance', 'Classification Performance', 'Test Dataset', 'Number Of Steps', 'Subsequent Steps', 'Incremental Learning', 'Backbone Network', 'Inference Time', 'Combination Of Algorithms', 'Test Scenarios', 'Combined Network', 'Single Algorithm', 'Choice Of Algorithm', 'Reference Configuration', 'Recommendation Method', 'Neural Architecture Search', 'Catastrophic Forgetting', 'Choice Of Network', 'Best-performing Algorithm', 'Training Time', 'Quadruplet', 'Less Than Or Equal', 'Sequential Process']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Recent class-incremental learning methods combine deep neural architectures and learning algorithms to handle streaming data under memory and computational constraints. The performance of existing methods varies depending on the characteristics of the incremental process. To date, there is no other approach than to test all pairs of learning algorithms and neural architectures on the training data available at the start of the learning process to select a suited algorithm-architecture combination. To tackle this problem, in this article, we introduce AdvisIL, a method which takes as input the main characteristics of the incremental process (memory budget for the deep model, initial number of classes, size of incremental steps) and recommends an adapted pair of learning algorithm and neural architecture. The recommendation is based on a similarity between the user-provided settings and a large set of pre-computed experiments. AdvisIL makes class-incremental learning easier, since users do not need to run cumbersome experiments to design their system. We evaluate our method on four datasets under six incremental settings and three deep model sizes. We compare six algorithms and three deep neural architectures. Results show that AdvisIL has better overall performance than any of the individual combinations of a learning algorithm and a neural architecture. AdvisIL’s code is available at https://github.com/EvaJF/AdvisIL."
Aerial Image Dehazing With Attentive Deformable Transformers,"Ashutosh Kulkarni, Subrahmanyam Murala","CVPR Lab, Indian Institute of Technology Ropar, INDIA",100,India,0,,"Aerial imagery is widely utilized in visual data dependent applications such as military surveillance, earthquake assessment, etc. For these applications, minute texture in the aerial image are essential as any disturbance can cause inaccurate prediction. However, atmospheric haze severely reduces the visibility of the scene to be analysed, and hence takes a toll on accuracy of higher level applications. Existing methods either utilize additional prior while training, or produce sub-optimal outputs on different densities of haze degradation, due to absence of local and global dependencies in the extracted features. Therefore, it is essential to have a texture preserving algorithm for aerial image dehazing. In light of this, we propose a work that introduces a novel deformable multi-head attention with spatially attentive offset extraction based solution for aerial image dehazing. Here, the deformable multi-head attention is introduced to reconstruct fine level texture in the restored image. We also introduce spatially attentive offset extractor in the deformable convolution for focusing on relevant contextual information. Further, edge boosting skip connections are proposed for effectively passing edge features from shallow layers to deeper layers of the network. Thorough experimentation on synthetic as well as real-world data, along with extensive ablation study, demonstrate that the proposed method outperforms the prevailing works on aerial image dehazing. The code is provided at https://github.com/ AshutoshKulkarni4998/AIDTransformer.",https://openaccess.thecvf.com/content/WACV2023/html/Kulkarni_Aerial_Image_Dehazing_With_Attentive_Deformable_Transformers_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kulkarni_Aerial_Image_Dehazing_With_Attentive_Deformable_Transformers_WACV_2023_paper.pdf,,https://github.com/AshutoshKulkarni4998/AIDTransformer,,main,Poster,https://ieeexplore.ieee.org/document/10030985/,"['Training', 'Visualization', 'Convolution', 'Image edge detection', 'Surveillance', 'Feature extraction', 'Transformers']","['Aerial Images', 'Image Dehazing', 'Network Layer', 'Attention Mechanism', 'Skip Connections', 'Local Dependence', 'Global Dependencies', 'Deformable Convolution', 'Loss Function', 'Comparative Analysis', 'Convolutional Neural Network', 'Quantitative Results', 'Input Features', 'Generative Adversarial Networks', 'Feed-forward Network', 'Synthetic Aperture Radar', 'Spatial Attention', 'Relevant Regions', 'Training Details', 'Edge Information', 'Multi-head Self-attention', 'Long-range Dependencies', 'Real-world Images', 'Natural Scene Images', 'Vision Transformer', 'Transmission Map', 'Floating-point Operations', 'Transformer Block', 'Quantitative Analysis', 'Unsupervised Learning']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Low-level and physics-based vision']",26,"Aerial imagery is widely utilized in visual data dependent applications such as military surveillance, earthquake assessment, etc. For these applications, minute texture in the aerial image are essential as any disturbance can cause inaccurate prediction. However, atmospheric haze severely reduces the visibility of the scene to be analysed, and hence takes a toll on accuracy of higher level applications. Existing methods either utilize additional prior while training, or produce sub-optimal outputs on different densities of haze degradation, due to absence of local and global dependencies in the extracted features. Therefore, it is essential to have a texture preserving algorithm for aerial image dehazing. In light of this, we propose a work that introduces a novel deformable multi-head attention with spatially attentive offset extraction based solution for aerial image dehazing. Here, the deformable multi-head attention is introduced to reconstruct fine level texture in the restored image. We also introduce spatially attentive offset extractor in the deformable convolution for focusing on relevant contextual information. Further, edge boosting skip connections are proposed for effectively passing edge features from shallow layers to deeper layers of the network. Thorough experimentation on synthetic as well as real-world data, along with extensive ablation study, demonstrate that the proposed method outperforms the prevailing works on aerial image dehazing. The code is provided at https://github.com/AshutoshKulkarni4998/AIDTransformer."
Aggregating Bilateral Attention for Few-Shot Instance Localization,"He-Yen Hsieh, Ding-Jie Chen, Cheng-Wei Chang, Tyng-Luh Liu","Institute of Information Science, Academia Sinica, Taiwan",100,Taiwan,0,,"Attention filtering under various learning scenarios has proven advantageous in enhancing the performance of many neural network architectures. The mainstream attention mechanism is established upon the non-local block, also known as an essential component of the prominent Transformer networks, to catch long-range correlations. However, such unilateral attention is often hampered by sparse and obscure responses, revealing insufficient dependencies across images/patches, and high computational cost, especially for those employing the multi-head design. To overcome these issues, we introduce a novel mechanism of aggregating bilateral attention (ABA) and validate its usefulness in tackling the task of few-shot instance localization, reflecting the underlying query-support dependency. Specifically, our method facilitates uncovering informative features via assessing: i) an embedding norm for exploring the semantically-related cues; ii) context awareness for correlating the query data and support regions. ABA is then carried out by integrating the affinity relations derived from the two measurements to serve as a lightweight but effective query-support attention mechanism with high localization recall. We evaluate ABA on two localization tasks, namely, few-shot action localization and one-shot object detection. Extensive experiments demonstrate that the proposed ABA achieves superior performances over existing methods.",https://openaccess.thecvf.com/content/WACV2023/html/Hsieh_Aggregating_Bilateral_Attention_for_Few-Shot_Instance_Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Aggregating_Bilateral_Attention_for_Few-Shot_Instance_Localization_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030110/,"['Location awareness', 'Filtering', 'Computational modeling', 'Neural networks', 'Focusing', 'Context awareness', 'Object detection']","['Object Detection', 'Attention Mechanism', 'Localization Task', 'Context-aware', 'Query Data', 'Non-local Block', 'Convolution', 'Experimental Methods', 'Paired Data', 'Object Classification', 'Average Precision', 'Dot Product', 'Feature Points', 'Video Sequences', 'Global Average Pooling', 'Self-attention Mechanism', 'Long-range Dependencies', 'Affinity Matrix', 'Support Set', 'Supportive Context', 'Unseen Classes', 'Query Image', 'Dot Product Operation', 'PASCAL VOC Dataset', 'Action Proposals', 'Linear Embedding', 'Spatial Neighborhood', 'Latent Space', 'Test Split', 'Issue Attention']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",4,"Attention filtering under various learning scenarios has proven advantageous in enhancing the performance of many neural network architectures. The mainstream attention mechanism is established upon the non-local block, also known as an essential component of the prominent Transformer networks, to catch long-range correlations. However, such unilateral attention is often hampered by sparse and obscure responses, revealing insufficient dependencies across images/patches, and high computational cost, especially for those employing the multi-head design. To overcome these issues, we introduce a novel mechanism of aggregating bilateral attention (ABA) and validate its usefulness in tackling the task of few-shot instance localization, reflecting the underlying query-support dependency. Specifically, our method facilitates uncovering informative features via assessing: i) an embedding norm for exploring the semantically-related cues; ii) context awareness for correlating the query data and support regions. ABA is then carried out by integrating the affinity relations derived from the two measurements to serve as a lightweight but effective query-support attention mechanism with high localization recall. We evaluate ABA on two localization tasks, namely, few-shot action localization and one-shot object detection. Extensive experiments demonstrate that the proposed ABA achieves superior performances over existing methods."
An Embedding-Dynamic Approach to Self-Supervised Learning,"Suhong Moon, Domas Buracas, Seunghyun Park, Jinkyu Kim, John Canny","Clova AI Research, NAVER Corp; UC Berkeley; Korea University",66.66666667,"South Korea, USA",33.33333333,South Korea,"A number of recent self-supervised learning methods have shown impressive performance on image classification and other tasks. A somewhat bewildering variety of techniques have been used, not always with a clear understanding of the reasons for their benefits, especially when used in combination. Here we treat the embeddings of images as point particles and consider model optimization as a dynamic process on this system of particles. Our dynamic model combines an attractive force for similar images, a locally dispersive force to avoid local collapse, and a global dispersive force to achieve a globally-homogeneous distribution of particles. The dynamic perspective highlights the advantage of using a delayed-parameter image embedding (a la BYOL) together with multiple views of the same image. It also uses a purely-dynamic local dispersive force (Brownian motion) that shows improved performance over other methods and does not require knowledge of other particle coordinates. The method is called MSBReg which stands for (i) a Multiview centroid loss, which applies an attractive force to pull different image view embeddings toward their centroid, (ii) a Singular value loss, which pushes the particle system toward spatially homogeneous density, (iii) a Brownian diffusive loss. We evaluate downstream classification performance of MSBReg on ImageNet as well as transfer learning tasks including fine-grained classification, multi-class object classification, object detection, and instance segmentation. In addition, we also show that applying our regularization term to other methods further improves their performance and stabilize the training by preventing a mode collapse.",https://openaccess.thecvf.com/content/WACV2023/html/Moon_An_Embedding-Dynamic_Approach_to_Self-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Moon_An_Embedding-Dynamic_Approach_to_Self-Supervised_Learning_WACV_2023_paper.pdf,,,2207.03552,main,Poster,https://ieeexplore.ieee.org/document/10030235/,"['Training', 'Semantic segmentation', 'Force', 'Dynamics', 'Transfer learning', 'Self-supervised learning', 'Object detection']","['Self-supervised Learning', 'Image Classification', 'Object Detection', 'Brownian Motion', 'Singular Value', 'Transfer Learning', 'Multi-label', 'Attractive Forces', 'Image Classification Tasks', 'Dispersion Forces', 'Image Embedding', 'Low Resolution', 'Covariance Matrix', 'Supervised Learning', 'Batch Normalization', 'Latent Space', 'Performance Gain', 'Loss Term', 'Target Network', 'Cholesky Decomposition', 'Quality Of Representations', 'Top-1 Accuracy', 'Local Diffusion', 'Representative View', '3rd Row', '2nd Row', 'Global Dissemination', 'Hypersphere', 'Image Augmentation', 'Singular Values Of Matrix']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",1,"A number of recent self-supervised learning methods have shown impressive performance on image classification and other tasks. A somewhat bewildering variety of techniques have been used, not always with a clear understanding of the reasons for their benefits, especially when used in combination. Here we treat the embeddings of images as point particles and consider model optimization as a dynamic process on this system of particles. Our dynamic model combines an attractive force for similar images, a locally dispersive force to avoid local collapse, and a global dispersive force to achieve a globally-homogeneous distribution of particles. The dynamic perspective highlights the advantage of using a delayed-parameter image embedding (a la BYOL) together with multiple views of the same image. It also uses a purely-dynamic local dispersive force (Brownian motion) that shows improved performance over other methods and does not require knowledge of other particle coordinates. The method is called MSBReg which stands for (i) a Multiview centroid loss, which applies an attractive force to pull different image view embeddings toward their centroid, (ii) a Singular value loss, which pushes the particle system toward spatially homogeneous density, (iii) a Brownian diffusive loss. We evaluate downstream classification performance of MSBReg on ImageNet as well as transfer learning tasks including fine-grained classification, multi-class object classification, object detection, and instance segmentation. In addition, we also show that applying our regularization term to other methods further improves their performance and stabilize the training by preventing a mode collapse."
An Unified Framework for Language Guided Image Completion,"Jihyun Kim, Seong-Hun Jeong, Kyeongbo Kong, Suk-Ju Kang",Pukyong National University; Sogang University,100,South Korea,0,,"Image completion is a research field which aims to generate visual contents for unknown regions of an image. Image outpainting and wide-range image blending, which we refer to as extensive painting, are considered challenging because compared to the large unknown regions, relatively less context is provided. Some recent studies have tried to decrease the complexity of extensive painting by generating image hints for the missing regions. In this paper, we introduce a novel modality of hints, the natural language. Moreover, we propose a Captioning-based Extensive Painting (CEP) module, which combines models for two different multi-modal tasks: image captioning and text-guided image completion. In order to generate appropriate captions for masked images, the image captioning model is optimized using self-critical sequence training (SCST) method with random masks. The biggest benefit of our methodology is the accessibility to well-designed image captioning and text-guided image manipulation models such as OFA and GLIDE without the need for additional architectural changes. In evaluation, our model demonstrates remarkable performance even with complicated image datasets both quantitatively and qualitatively.",https://openaccess.thecvf.com/content/WACV2023/html/Kim_An_Unified_Framework_for_Language_Guided_Image_Completion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_An_Unified_Framework_for_Language_Guided_Image_Completion_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030522/,"['Training', 'Visualization', 'Computer vision', 'Image synthesis', 'Computational modeling', 'Natural languages', 'Complexity theory']","['Natural Language', 'Image Regions', 'Training Methods', 'Masked Images', 'Image Captioning', 'Missing Regions', 'Unknown Regions', 'Multimodal Tasks', 'Large Datasets', 'Challenging Task', 'Input Image', 'Large-scale Datasets', 'Training Images', 'Image Generation', 'Large-scale Models', 'Complete Image', 'Image Inpainting', 'Extract Visual Features']","['Algorithms: Low-level and physics-based vision', 'Vision + language and/or other modalities']",1,"Image completion is a research field which aims to generate visual contents for unknown regions of an image. Image outpainting and wide-range image blending, which we refer to as extensive painting, are considered challenging because compared to the large unknown regions, relatively less context is provided. Some recent studies have tried to decrease the complexity of extensive painting by generating image hints for the missing regions. In this paper, we introduce a novel modality of hints, the natural language. Moreover, we propose a Captioning-based Extensive Painting (CEP) module, which combines models for two different multi-modal tasks: image captioning and text-guided image completion. In order to generate appropriate captions for masked images, the image captioning model is optimized using self-critical sequence training (SCST) method with random masks. The biggest benefit of our methodology is the accessibility to well-designed image captioning and text-guided image manipulation models such as OFA and GLIDE without the need for additional architectural changes. In evaluation, our model demonstrates remarkable performance even with complicated image datasets both quantitatively and qualitatively."
Analysis of Master Vein Attacks on Finger Vein Recognition Systems,"Huy H. Nguyen, Trung-Nghia Le, Junichi Yamagishi, Isao Echizen","National Institute of Informatics, Tokyo, Japan; University of Science, VNU-HCM, Vietnam; The University of Tokyo, Tokyo, Japan",100,"Japan, Vietnam",0,,"Finger vein recognition (FVR) systems have been commercially used, especially in ATMs, for customer verification. Thus, it is essential to measure their robustness against various attack methods, especially when a hand-crafted FVR system is used without any countermeasure methods. In this paper, we are the first in the literature to introduce master vein attacks in which we craft a vein-looking image so that it can falsely match with as many identities as possible by the FVR systems. We present two methods for generating master veins for use in attacking these systems. The first uses an adaptation of the latent variable evolution algorithm with a proposed generative model (a multi-stage combination of beta-VAE and WGAN-GP models). The second uses an adversarial machine learning attack method to attack a strong surrogate CNN-based recognition system. The two methods can be easily combined to boost their attack ability. Experimental results demonstrated that the proposed methods alone and together achieved false acceptance rates up to 73.29% and 88.79%, respectively, against Miura's hand-crafted FVR system. We also point out that Miura's system is easily compromised by non-vein-looking samples generated by a WGAN-GP model with false acceptance rates up to 94.21%. The results raise the alarm about the robustness of such systems and",https://openaccess.thecvf.com/content/WACV2023/html/Nguyen_Analysis_of_Master_Vein_Attacks_on_Finger_Vein_Recognition_Systems_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nguyen_Analysis_of_Master_Vein_Attacks_on_Finger_Vein_Recognition_Systems_WACV_2023_paper.pdf,,,2210.10667,main,Poster,https://ieeexplore.ieee.org/document/10030345/,"['Adaptation models', 'Computer vision', 'Machine learning algorithms', 'Veins', 'Computational modeling', 'Robustness', 'Adversarial machine learning']","['Recognition System', 'Finger Vein', 'Finger Vein Recognition', 'Evolutionary Algorithms', 'Generative Adversarial Networks', 'Adversarial Attacks', 'Attack Methods', 'False Acceptance', 'False Acceptance Rate', 'Training Set', 'Convolutional Neural Network', 'Gaussian Blur', 'Variational Autoencoder', 'Normal Dataset', 'CNN-based Methods', 'Target Label', 'Partial Match', 'Generative Adversarial Networks Model', 'Attack Scenarios', 'Biometric Systems', 'Mode Matching', 'Black-box Attacks', 'Master Image', 'Partial Mode', 'Blur Kernel', 'White-box Attack', 'Disentangled Representation']","['Applications: Biomedical/healthcare/medicine', 'Adversarial learning', 'adversarial attack and defense methods', 'Computational photography', 'image and video synthesis']",2,"Finger vein recognition (FVR) systems have been commercially used, especially in ATMs, for customer verification. Thus, it is essential to measure their robustness against various attack methods, especially when a handcrafted FVR system is used without any countermeasure methods. In this paper, we are the first in the literature to introduce master vein attacks in which we craft a vein-looking image so that it can falsely match with as many identities as possible by the FVR systems. We present two methods for generating master veins for use in attacking these systems. The first uses an adaptation of the latent variable evolution algorithm with a proposed generative model (a multi-stage combination of β-VAE and WGAN-GP models). The second uses an adversarial machine learning attack method to attack a strong surrogate CNN-based recognition system. The two methods can be easily combined to boost their attack ability. Experimental results demonstrated that the proposed methods alone and together achieved false acceptance rates up to 73.29% and 88.79%, respectively, against Miura’s hand-crafted FVR system. We also point out that Miura’s system is easily compromised by non-vein-looking samples generated by a WGAN-GP model with false acceptance rates up to 94.21%. The results raise the alarm about the robustness of such systems and suggest that master vein attacks should be considered an important security measure."
Ancestor Search: Generalized Open Set Recognition via Hyperbolic Side Information Learning,"Xiwen Dengxiong, Yu Kong","Department of Computer Science and Engineering, Michigan State University; Golisano College of Computing and Information Sciences, Rochester Institute of Technology",100,USA,0,,"Different from the open set recognition, generalized open set recognition learns the most similar known classes for unseen samples using known classes samples and side information of known classes. It is challenging because hierarchically structured side information is distorted when features are embedded in the Euclidean space in existing literature, which incurs the difficulty of identifying the unseen samples. In this paper, we introduce a side information learning algorithm for generalized open set recognition based on the hyperbolic space to alleviate the distortion and accurately identify the unknown samples. Specifically, we propose a hyperbolic side information learning framework to identify the unseen samples and an ancestor search algorithm to search the most similar ancestor from the taxonomy of selected known classes. Experiments on CUB-200 and AWA 2 datasets show that our method improves the performance of generalized open set recognition by a large margin.",https://openaccess.thecvf.com/content/WACV2023/html/Dengxiong_Ancestor_Search_Generalized_Open_Set_Recognition_via_Hyperbolic_Side_Information_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dengxiong_Ancestor_Search_Generalized_Open_Set_Recognition_via_Hyperbolic_Side_Information_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030431/,"['Computer vision', 'Taxonomy', 'Search problems', 'Distortion', 'Indexes']","['Open Set', 'Open Set Recognition', 'Hierarchical Structure', 'Classification Of Samples', 'Euclidean Space', 'Unknown Samples', 'Hyperbolic Space', 'Neural Network', 'Deep Neural Network', 'Sample Variance', 'Confidence Score', 'Root Node', 'Taxonomic Annotation', 'non-Euclidean', 'Lowest Common Ancestor', 'Recognition Capability', 'Zero-shot', 'Exponential Map', 'Unseen Classes', 'Hyperbolic Geometry', 'Disk Model']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",6,"Different from the open set recognition, generalized open set recognition learns the most similar known classes for unseen samples using known classes samples and side in-formation of known classes. It is challenging because hierarchically structured side information is distorted when features are embedded in the Euclidean space in existing literature, which incurs the difficulty of identifying the unseen samples. In this paper, we introduce a side information learning algorithm for generalized open set recognition based on the hyperbolic space to alleviate the distortion and accurately identify the unknown samples. Specifically, we propose a hyperbolic side information learning framework to identify the unseen samples and an ancestor search algorithm to search the most similar ancestor from the taxonomy of selected known classes. Experiments on CUB-200 and AWA 2 datasets show that our method improves the performance of generalized open set recognition by a large margin."
Anisotropic Multi-Scale Graph Convolutional Network for Dense Shape Correspondence,"Mohammad Farazi, Wenhui Zhu, Zhangsihao Yang, Yalin Wang",Arizona State University,100,USA,0,,"This paper studies 3D dense shape correspondence, a key shape analysis application in computer vision and graphics. We introduce a novel hybrid geometric deep learning-based model that learns geometrically meaningful and discretization-independent features. The proposed framework has a U-Net model as the primary node feature extractor, followed by a successive spectral-based graph convolutional network. To create a diverse set of filters, we use anisotropic wavelet basis filters, being sensitive to both different directions and band-passes. This filter set overcomes the common over-smoothing behavior of conventional graph neural networks. To further improve the model's performance, we add a function that perturbs the feature maps in the last layer ahead of fully connected layers, forcing the network to learn more discriminative features overall. The resulting correspondence maps show state-of-the-art performance on the benchmark datasets based on average geodesic errors and superior robustness to discretization in 3D meshes. Our approach provides new insights and practical solutions to the dense shape correspondence research.",https://openaccess.thecvf.com/content/WACV2023/html/Farazi_Anisotropic_Multi-Scale_Graph_Convolutional_Network_for_Dense_Shape_Correspondence_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Farazi_Anisotropic_Multi-Scale_Graph_Convolutional_Network_for_Dense_Shape_Correspondence_WACV_2023_paper.pdf,,,2210.09466,main,Poster,https://ieeexplore.ieee.org/document/10030136/,"['Computer vision', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Benchmark testing', 'Feature extraction', 'Robustness']","['Convolutional Network', 'Graph Convolutional Network', 'Graph Convolution', 'Multi-scale Network', 'Discretion', 'Feature Maps', 'Discriminative Features', '3D Mesh', '3D Shape', 'Graph Neural Networks', 'Computer Vision Applications', 'U-Net Model', 'Wavelet Basis', 'Deep Learning', 'Convolutional Layers', 'Margin Of Error', 'Feature Learning', 'Point Cloud', 'Spatial Domain', 'Laplace-Beltrami Operator', 'Graph Neural Network Model', 'Wavelet Function', 'Functional Scale', 'Global Brain', 'Robust Features', 'Eigendecomposition', 'Heat Diffusion', 'Unstructured Data', 'Number Of Filters']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Visualization']",2,"This paper studies 3D dense shape correspondence, a key shape analysis application in computer vision and graphics. We introduce a novel hybrid geometric deep learning-based model that learns geometrically meaningful and discretization-independent features. The proposed framework has a U-Net model as the primary node feature extractor, followed by a successive spectral-based graph convolutional network. To create a diverse set of filters, we use anisotropic wavelet basis filters, being sensitive to both different directions and band-passes. This filter set overcomes the common over-smoothing behavior of conventional graph neural networks. To further improve the model’s performance, we add a function that perturbs the feature maps in the last layer ahead of fully connected layers, forcing the network to learn more discriminative features overall. The resulting correspondence maps show state-of-the-art performance on the benchmark datasets based on average geodesic errors and superior robustness to discretization in 3D meshes. Our approach provides new insights and practical solutions to the dense shape correspondence research."
AnoLeaf: Unsupervised Leaf Disease Segmentation via Structurally Robust Generative Inpainting,"Swati Bhugra, Vinay Kaushik, Amit Gupta, Brejesh Lall, Santanu Chaudhury",IIT Jodhpur; IIT Delhi; DVIEW,66.66666667,India,33.33333333,USA,"Plant diseases severely limits agriculture production, necessitating the high-throughput monitoring of plant leaves. Currently, this is formulated as an automatic disease segmentation task addressed via deep learning frameworks. These deep leaning frameworks trained with leaf image data in a supervised paradigm have few limitations, mainly: (1) training datasets are heavily imbalanced towards healthy leaf images, (2) disease region annotation is labour-intensive and (3) due to the heterogeneity of disease symptoms, these frameworks lacks generalisability. In this paper, we reformulate disease segmentation as an anomaly localisation task. Specifically, we introduce a novel unsupervised framework (AnoLeaf) based on an edge-guided inpainting that optimises the learning of contextual attention on only healthy leaf images. The network utilisation on diseased leaf images results in reconstruction of its healthy counterparts, generating an inpainting error. The contextual attention maps reinforce the inpainting error to effectively localise the disease. Thus, AnoLeaf alleviates the acquisition and annotation of rare disease images. Additional experiments on MVTec anomaly detection dataset further demonstrate its generalisability.",https://openaccess.thecvf.com/content/WACV2023/html/Bhugra_AnoLeaf_Unsupervised_Leaf_Disease_Segmentation_via_Structurally_Robust_Generative_Inpainting_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bhugra_AnoLeaf_Unsupervised_Leaf_Disease_Segmentation_via_Structurally_Robust_Generative_Inpainting_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030320/,"['Training', 'Image segmentation', 'Annotations', 'Plants (biology)', 'Image edge detection', 'Production', 'Task analysis']","['Leaf Diseases', 'Generative Inpainting', 'Deep Learning', 'Disease Symptoms', 'Plant Disease', 'Anomaly Detection', 'Attention Map', 'Healthy Counterparts', 'Network Utility', 'Heterogeneity Of Symptoms', 'Unsupervised Framework', 'Healthy Leaf', 'Contextual Attention', 'Leaf Images', 'Convolutional Network', 'Convolutional Neural Network', 'K-means', 'Color Space', 'Activation Maps', 'Image Inpainting', 'Normal Images', 'Layer Of The Discriminator', 'Pixel-level Annotations', 'Refinement Network', 'Missing Regions', 'HSV Color', 'Peak Signal-to-noise Ratio', 'Variational Autoencoder', 'Structural Similarity Index']","['Applications: Agriculture', 'Biomedical/healthcare/medicine']",2,"Plant diseases severely limits agriculture production, necessitating the high-throughput monitoring of plant leaves. Currently, this is formulated as an automatic disease segmentation task addressed via deep learning frameworks. These deep leaning frameworks trained with leaf image data in a supervised paradigm have few limitations, mainly: (1) training datasets are heavily imbalanced towards healthy leaf images, (2) disease region annotation is labour-intensive and (3) due to the heterogeneity of disease symptoms, these frameworks lacks generalisability. In this paper, we reformulate disease segmentation as an anomaly localisation task. Specifically, we introduce a novel unsupervised framework (AnoLeaf) based on an edge-guided in-painting that optimises the learning of contextual attention on only healthy leaf images. The network utilisation on diseased leaf images results in reconstruction of its healthy counterparts, generating an inpainting error. The contextual attention maps reinforce the inpainting error to effectively localise the disease. Thus, AnoLeaf alleviates the acquisition and annotation of rare disease images. Additional experiments on MVTec anomaly detection dataset further demonstrate its generalisability."
Anomaly Clustering: Grouping Images Into Coherent Clusters of Anomaly Types,"Kihyuk Sohn, Jinsung Yoon, Chun-Liang Li, Chen-Yu Lee, Tomas Pfister",Google Cloud AI Research,0,,100,USA,"We study anomaly clustering, grouping data into coherent clusters of anomaly types. This is different from anomaly detection that aims to divide anomalies from normal data. Unlike object-centered image clustering, anomaly clustering is particularly challenging as anomalous patterns are subtle and local. We present a simple yet effective clustering framework using a patch-based pretrained deep embeddings and off-the-shelf clustering methods. We define a distance function between images, each of which is represented as a bag of embeddings, by the Euclidean distance between weighted averaged embeddings. The weight defines the importance of instances (i.e., patch embeddings) in the bag, which may highlight defective regions. We compute weights in an unsupervised way or in a semi-supervised way when labeled normal data is available. Extensive experimental studies show the effectiveness of the proposed clustering framework along with a novel distance function upon exist-ing multiple instance or deep clustering frameworks. Over-all, our framework achieves 0.451 and 0.674 normalized mutual information scores on MVTec object and texture categories and further improve with a few labeled normal data (0.577, 0.669), far exceeding the baselines (0.244, 0.273) or state-of-the-art deep clustering methods (0.176, 0.277).",https://openaccess.thecvf.com/content/WACV2023/html/Sohn_Anomaly_Clustering_Grouping_Images_Into_Coherent_Clusters_of_Anomaly_Types_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sohn_Anomaly_Clustering_Grouping_Images_Into_Coherent_Clusters_of_Anomaly_Types_WACV_2023_paper.pdf,,,2112.11573,main,Poster,https://ieeexplore.ieee.org/document/10030348/,"['Computer vision', 'Clustering methods', 'Euclidean distance', 'Mutual information', 'Anomaly detection']","['Types Of Anomalies', 'Data Normalization', 'Extensive Studies', 'Clustering Method', 'Distance Function', 'Multiple Clusters', 'Anomaly Detection', 'Defect Region', 'Unsupervised Way', 'Anomalous Patterns', 'Clustering Framework', 'Hierarchical Clustering', 'Distancing Measures', 'Unsupervised Clustering', 'Weight Vector', 'Types Of Defects', 'Gaussian Mixture Model', 'Complete Linkage', 'Pre-trained Network', 'Clustering Performance', 'Holistic Representation', 'Hausdorff Distance', 'Deep Representation', 'Clustering Problem', 'Spectral Clustering', 'Adjusted Rand Index', 'Defect Area', 'Vision Transformer', 'Anomalous Data', 'Clustering Accuracy']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",7,"We study anomaly clustering, grouping data into coherent clusters of anomaly types. This is different from anomaly detection that aims to divide anomalies from normal data. Unlike object-centered image clustering, anomaly clustering is particularly challenging as anomalous patterns are subtle and local. We present a simple yet effective clustering framework using a patch-based pretrained deep embeddings and off-the-shelf clustering methods. We define a distance function between images, each of which is represented as a bag of embeddings, by the Euclidean distance between weighted averaged embeddings. The weight defines the importance of instances (i.e., patch embeddings) in the bag, which may highlight defective regions. We compute weights in an unsupervised way or in a semi-supervised way when labeled normal data is available. Extensive experimental studies show the effectiveness of the proposed clustering framework along with a novel distance function upon existing multiple instance or deep clustering frameworks. Overall, our framework achieves 0.451 and 0.674 normalized mutual information scores on MVTec object and texture categories and further improve with a few labeled normal data (0.577, 0.669), far exceeding the baselines (0.244, 0.273) or state-of-the-art deep clustering methods (0.176, 0.277)."
Anomaly Detection in 3D Point Clouds Using Deep Geometric Descriptors,"Paul Bergmann, David Sattlegger","MVTec Software GmbH, Technical University of Munich; MVTec Software GmbH",50,Germany,50,Germany,"We present a new method for the unsupervised detection of geometric anomalies in high-resolution 3D point clouds. In particular, we propose an adaptation of the established student-teacher anomaly detection framework to three dimensions. A student network is trained to match the output of a pretrained teacher network on anomaly-free point clouds. When applied to test data, regression errors between the teacher and the student allow reliable localization of anomalous structures. To construct an expressive teacher network that extracts dense local geometric descriptors, we introduce a novel self-supervised pretraining strategy. The teacher is trained by reconstructing local receptive fields and does not require annotations. Extensive experiments on the comprehensive MVTec 3D Anomaly Detection dataset highlight the effectiveness of our approach, which outperforms the existing methods by a large margin. Ablation studies show that our approach meets the requirements of practical applications regarding performance, runtime, and memory consumption.",https://openaccess.thecvf.com/content/WACV2023/html/Bergmann_Anomaly_Detection_in_3D_Point_Clouds_Using_Deep_Geometric_Descriptors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bergmann_Anomaly_Detection_in_3D_Point_Clouds_Using_Deep_Geometric_Descriptors_WACV_2023_paper.pdf,,,2202.1166,main,Poster,https://ieeexplore.ieee.org/document/10030323/,"['Point cloud compression', 'Training', 'Location awareness', 'Three-dimensional displays', 'Runtime', 'Protocols', 'Memory management']","['Point Cloud', 'Anomaly Detection', '3D Point', '3D Point Cloud', 'Geometric Description', 'Detection In Point Clouds', 'Receptive Field', 'Large Margin', 'Regression Residuals', 'Memory Consumption', 'Teacher Network', 'Local Descriptors', 'Student Network', 'Local Receptive Field', 'Local Features', 'Geometric Features', 'Generative Adversarial Networks', '3D Data', 'Residual Block', 'Anomaly Score', 'Input Point Cloud', 'Pre-training Dataset', 'Input Point', 'Random Weight Initialization', 'Absolute Coordinates', 'Nearest Neighbor Graph', 'Convolutional Autoencoder', 'Human Pose Estimation', 'Established Protocol']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', '3D computer vision']",20,"We present a new method for the unsupervised detection of geometric anomalies in high-resolution 3D point clouds. In particular, we propose an adaptation of the established student-teacher anomaly detection framework to three dimensions. A student network is trained to match the output of a pretrained teacher network on anomaly-free point clouds. When applied to test data, regression errors between the teacher and the student allow reliable localization of anomalous structures. To construct an expressive teacher network that extracts dense local geometric descriptors, we introduce a novel self-supervised pretraining strategy. The teacher is trained by reconstructing local receptive fields and does not require annotations. Extensive experiments on the comprehensive MVTec 3D Anomaly Detection dataset highlight the effectiveness of our approach, which outperforms the existing methods by a large margin. Ablation studies show that our approach meets the requirements of practical applications regarding performance, runtime, and memory consumption."
Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation,"Zeyun Zhong, David Schneider, Michael Voit, Rainer Stiefelhagen, Jürgen Beyerer","Fraunhofer IOSB, Karlsruhe; Fraunhofer IOSB, Karlsruhe and Karlsruhe Institute of Technology (KIT); Karlsruhe Institute of Technology (KIT)",66.66666667,Germany,33.33333333,Germany,"Although human action anticipation is a task which is inherently multi-modal, state-of-the-art methods on well known action anticipation datasets leverage this data by applying ensemble methods and averaging scores of uni-modal anticipation networks. In this work we introduce transformer based modality fusion techniques, which unify multi-modal data at an early stage. Our Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular score fusion approaches and presents state-of-the-art results outperforming previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily extensible and allows for adding new modalities without architectural changes. Consequently, we extracted audio features on EpicKitchens-100 which we add to the set of commonly used features in the community.",https://openaccess.thecvf.com/content/WACV2023/html/Zhong_Anticipative_Feature_Fusion_Transformer_for_Multi-Modal_Action_Anticipation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhong_Anticipative_Feature_Fusion_Transformer_for_Multi-Modal_Action_Anticipation_WACV_2023_paper.pdf,,https://github.com/zeyun-zhong/AFFT,2210.12649,main,Poster,https://ieeexplore.ieee.org/document/10030735/,"['Computer vision', 'Computational modeling', 'Feature extraction', 'Transformers', 'Ensemble learning', 'Task analysis']","['Feature Fusion', 'Activity Prediction', 'Multimodal Activity', 'Time Step', 'Hyperparameters', 'Fusion Method', 'Action Recognition', 'Optical Flow', 'Temporal Context', 'Fusion Strategy', 'Linear Layer', 'Linear Projection', 'Multimodal Features', 'Position Embedding', 'Attention Heads', 'Modal Features', 'Temporal Attention', 'Encoder Block', 'Validation Split', 'Time Ts', 'RGB Features', 'Past Frames', 'Action Recognition Model', 'Observation Time', 'Attention Mechanism', 'Visual Modality', 'Default Hyperparameters']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",21,"Although human action anticipation is a task which is inherently multi-modal, state-of-the-art methods on well known action anticipation datasets leverage this data by applying ensemble methods and averaging scores of uni-modal anticipation networks. In this work we introduce transformer based modality fusion techniques, which unify multi-modal data at an early stage. Our Anticipative Feature Fusion Transformer (AFFT) proves to be superior to popular score fusion approaches and presents state-of-the-art results outperforming previous methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily extensible and allows for adding new modalities without architectural changes. Consequently, we extracted audio features on EpicKitchens-100 which we add to the set of commonly used features in the community. 
<sup>1</sup>"
Arbitrary Style Guidance for Enhanced Diffusion-Based Text-to-Image Generation,"Zhihong Pan, Xin Zhou, Hao Tian",Baidu Research (USA),0,,100,USA,"Diffusion-based text-to-image generation models like GLIDE and DALLE-2 have gained wide success recently for their superior performance in turning complex text inputs into images of high quality and wide diversity. In particular, they are proven to be very powerful in creating graphic arts of various formats and styles. Although current models supported specifying style formats like oil painting or pencil drawing, fine-grained style features like color distributions and brush strokes are hard to specify as they are randomly picked from a conditional distribution based on the given text input. Here we propose a novel style guidance method to support generating images using arbitrary style guided by a reference image. The generation method does not require a separate style transfer model to generate desired styles while maintaining image quality in generated content as controlled by the text input. Additionally, the guidance method can be applied without a style reference, denoted as self style guidance, to generate images of more diverse styles. Comprehensive experiments prove that the proposed method remains robust and effective in a wide range of conditions, including diverse graphic art forms, image content types and diffusion models.",https://openaccess.thecvf.com/content/WACV2023/html/Pan_Arbitrary_Style_Guidance_for_Enhanced_Diffusion-Based_Text-to-Image_Generation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pan_Arbitrary_Style_Guidance_for_Enhanced_Diffusion-Based_Text-to-Image_Generation_WACV_2023_paper.pdf,,https://github.com/openai/glide-text2im,2211.07751,main,Poster,https://ieeexplore.ieee.org/document/10030821/,"['Graphics', 'Training', 'Technological innovation', 'Adaptation models', 'Adaptive systems', 'Art', 'Navigation']","['Arbitrary Style', 'Style Guidance', 'High-quality', 'Image Quality', 'Diffusion Model', 'Reference Image', 'Variety Of Styles', 'Input Text', 'Color Distribution', 'Style Transfer', 'Style Features', 'Oil Paint', 'Guidance Methods', 'Sample Processing', 'Iterative Process', 'Gaussian Noise', 'Super-resolution', 'Autoregressive Model', 'Class Labels', 'Reversible Process', 'Broad Range Of Applications', 'Noisy Images', 'Generative Adversarial Networks', 'Style Image', 'Image Generation', 'Language Model', 'Realistic Images', 'Inference Time', 'Image X', 'Two-step Method']","['Algorithms: Computational photography', 'image and video synthesis', 'Vision + language and/or other modalities']",6,"Diffusion-based text-to-image generation models like GLIDE and DALLE-2 have gained wide success recently for their superior performance in turning complex text inputs into images of high quality and wide diversity. In particular, they are proven to be very powerful in creating graphic arts of various formats and styles. Although current models supported specifying style formats like oil painting or pencil drawing, fine-grained style features like color distributions and brush strokes are hard to specify as they are randomly picked from a conditional distribution based on the given text input. Here we propose a novel style guidance method to support generating images using arbitrary style guided by a reference image. The generation method does not require a separate style transfer model to generate desired styles while maintaining image quality in generated content as controlled by the text input. Additionally, the guidance method can be applied without a style reference, denoted as self style guidance, to generate images of more diverse styles. Comprehensive experiments prove that the proposed method remains robust and effective in a wide range of conditions, including diverse graphic art forms, image content types and diffusion models."
Are Straight-Through Gradients and Soft-Thresholding All You Need for Sparse Training?,"Antoine Vanderschueren, Christophe De Vleeschouwer",Second Author's Affiliation; First Author's Affiliation,0,,100,,"Turning the weights to zero when training a neural network helps in reducing the computational complexity at inference. To progressively increase the sparsity ratio in the network without causing sharp weight discontinuities during training, our work combines soft-thresholding and straight-through gradient estimation to update the raw, i.e. non-thresholded, version of zeroed weights. Our method, named ST-3 for straight-through/soft-thresholding/sparse-training, obtains SoA results, both in terms of accuracy/sparsity and accuracy/FLOPS trade-offs, when progressively increasing the sparsity ratio in a single training cycle. In particular, despite its simplicity, ST-3 favorably compares to the most recent methods, adopting differentiable formulations or bio-inspired neuroregeneration principles. This suggests that the key ingredients for effective sparsification primarily lie in the ability to give the weights the freedom to evolve smoothly across the zero state while progressively increasing the sparsity ratio. Source code and weights available at https://github.com/vanderschuea/stthree.",https://openaccess.thecvf.com/content/WACV2023/html/Vanderschueren_Are_Straight-Through_Gradients_and_Soft-Thresholding_All_You_Need_for_Sparse_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Vanderschueren_Are_Straight-Through_Gradients_and_Soft-Thresholding_All_You_Need_for_Sparse_WACV_2023_paper.pdf,Project Link (if available),GitHub Link (if available),2212.01076,main,Poster,https://ieeexplore.ieee.org/document/10030853/,"['Training', 'Computer vision', 'Neural networks', 'Estimation', 'Computer architecture', 'Turning', 'Computational complexity']","['Sparse Training', 'Neural Network', 'Progressive Increase', 'Key Ingredient', 'Training Cycle', 'Sharp Discontinuity', 'Learning Rate', 'Gradient Descent', 'Dense Network', 'Increase In Ratio', 'Data Augmentation', 'ImageNet', 'Inactive State', 'Training Iterations', 'Weights Of Layer', 'Soft Threshold', 'Global Threshold', 'Sparse Method', 'Longer Training', 'Weight Development', 'Stable Training', 'Hard Threshold', 'Beginning Of Training', 'Switching Pattern', 'Magnitude Of Weights', 'Training Round']",['Applications: Embedded sensing/real-time techniques'],1,"Turning the weights to zero when training a neural network helps in reducing the computational complexity at inference. To progressively increase the sparsity ratio in the network without causing sharp weight discontinuities during training, our work combines soft-thresholding and straight-through gradient estimation to update the raw, i.e. non-thresholded, version of zeroed weights. Our method, named ST-3 for straight-through/soft-thresholding/sparse-training
<sup>2</sup>
, obtains SoA results, both in terms of accuracy/sparsity and accuracy/FLOPS trade-offs, when progressively increasing the sparsity ratio in a single training cycle. In particular, despite its simplicity, ST-3 favorably compares to the most recent methods, adopting differentiable formulations [42] or bio-inspired neuroregeneration principles [25]. This suggests that the key ingredients for effective sparsification primarily lie in the ability to give the weights the freedom to evolve smoothly across the zero state while progressively increasing the sparsity ratio."
Asymmetric Student-Teacher Networks for Industrial Anomaly Detection,"Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, Bastian Wandt","L3S / Leibniz University Hannover, Germany; Linköping University, Sweden",100,"Germany, Sweden",0,,"Industrial defect detection is commonly addressed with anomaly detection (AD) methods where no or only incomplete data of potentially occurring defects is available. This work discovers previously unknown problems of student-teacher approaches for AD and proposes a solution, where two neural networks are trained to produce the same output for the defect-free training examples. The core assumption of student-teacher networks is that the distance between the outputs of both networks is larger for anomalies since they are absent in training. However, previous methods suffer from the similarity of student and teacher architecture, such that the distance is undesirably small for anomalies. For this reason, we propose asymmetric student-teacher networks (AST). We train a normalizing flow for density estimation as a teacher and a conventional feed-forward network as a student to trigger large distances for anomalies: The bijectivity of the normalizing flow enforces a divergence of teacher outputs for anomalies compared to normal data. Outside the training distribution the student cannot imitate this divergence due to its fundamentally different architecture. Our AST network compensates for wrongly estimated likelihoods by a normalizing flow, which was alternatively used for anomaly detection in previous work. We show that our method produces state-of-the-art results on the two currently most relevant defect detection datasets MVTec AD and MVTec 3D-AD regarding image-level anomaly detection on RGB and 3D data.",https://openaccess.thecvf.com/content/WACV2023/html/Rudolph_Asymmetric_Student-Teacher_Networks_for_Industrial_Anomaly_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rudolph_Asymmetric_Student-Teacher_Networks_for_Industrial_Anomaly_Detection_WACV_2023_paper.pdf,,,2210.07829,main,Poster,https://ieeexplore.ieee.org/document/10030168/,"['Training', 'Location awareness', 'Computer vision', 'Three-dimensional displays', 'Neural networks', 'Estimation', 'Computer architecture']","['Anomaly Detection', 'Asymmetric Network', 'Neural Network', 'Density Estimation', '3D Data', 'Normal Flow', 'Conventional Network', 'Training Distribution', 'RGB Data', 'Hidden Layer', 'Feature Maps', 'Point Cloud', 'RGB Images', 'Data Modalities', 'Depth Map', 'Residual Block', 'Regression Residuals', 'Pre-trained Network', 'Teacher Network', 'Anomaly Score', 'Student Network', 'Positional Encoding', 'Random Projection', 'Resolution Of The Feature Map', 'Negative Log-likelihood', 'Anomalous Data', 'Pretext Task', 'Pixel Position', 'ImageNet']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",61,"Industrial defect detection is commonly addressed with anomaly detection (AD) methods where no or only incomplete data of potentially occurring defects is available. This work discovers previously unknown problems of student-teacher approaches for AD and proposes a solution, where two neural networks are trained to produce the same output for the defect-free training examples. The core assumption of student-teacher networks is that the distance between the outputs of both networks is larger for anomalies since they are absent in training. However, previous methods suffer from the similarity of student and teacher architecture, such that the distance is undesirably small for anomalies. For this reason, we propose asymmetric student-teacher networks (AST). We train a normalizing flow for density estimation as a teacher and a conventional feed-forward network as a student to trigger large distances for anomalies: The bijectivity of the normalizing flow enforces a divergence of teacher outputs for anomalies compared to normal data. Outside the training distribution the student cannot imitate this divergence due to its fundamentally different architecture. Our AST network compensates for wrongly estimated likelihoods by a normalizing flow, which was alternatively used for anomaly detection in previous work. We show that our method produces state-of-the-art results on the two currently most relevant defect detection datasets MVTec AD and MVTec 3D-AD regarding image-level anomaly detection on RGB and 3D data."
AttTrack: Online Deep Attention Transfer for Multi-Object Tracking,"Keivan Nalaie, Rong Zheng",McMaster University,100,Canada,0,,"Multi-object tracking (MOT) is a vital component of intelligent video analytics applications such as surveillance and autonomous driving. The time and storage complexity required to execute deep learning models for visual object tracking hinder their adoption on embedded devices with limited computing power. In this paper, we aim to accelerate MOT by transferring the knowledge from high-level features of a complex network (teacher) to a lightweight network (student) at both training and inference times. The proposed AttTrack framework has three key components: 1) cross-model feature learning to align intermediate representations from the teacher and student models, 2) interleaving the execution of the two models at inference time, and 3) incorporating the updated predictions from the teacher model as prior knowledge to assist the student model. Experiments on pedestrian tracking tasks are conducted on the MOT17 and MOT15 datasets using two different object detection backbones YOLOv5 and DLA34 show that AttTrack can significantly improve student model tracking performance while sacrificing only minor degradation of tracking speed.",https://openaccess.thecvf.com/content/WACV2023/html/Nalaie_AttTrack_Online_Deep_Attention_Transfer_for_Multi-Object_Tracking_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nalaie_AttTrack_Online_Deep_Attention_Transfer_for_Multi-Object_Tracking_WACV_2023_paper.pdf,,,2210.08648,main,Poster,https://ieeexplore.ieee.org/document/10030881/,"['Training', 'Representation learning', 'Knowledge engineering', 'Degradation', 'Visual analytics', 'Surveillance', 'Object detection']","['Multi-object Tracking', 'Attention Transfer', 'Training Time', 'Object Detection', 'Teacher Model', 'Tracking Performance', 'Inference Time', 'Object Tracking', 'Student Model', 'Tracking Task', 'Neural Network', 'Spatial Features', 'Knowledge Transfer', 'Visual Features', 'Attention Mechanism', 'Model Size', 'Bounding Box', 'Small Model', 'Attention Module', 'Tracking Accuracy', 'Frames Per Second', 'Channel Layer', 'Inference Stage', 'Attention Feature', 'COCO Dataset', 'Input Frames', 'One-hot Vector', 'Object Boxes', 'Current Frame', 'Quantification Model']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Multi-object tracking (MOT) is a vital component of intelligent video analytics applications such as surveillance and autonomous driving. The time and storage complexity required to execute deep learning models for visual object tracking hinder their adoption on embedded devices with limited computing power. In this paper, we aim to accelerate MOT by transferring the knowledge from high-level features of a complex network (teacher) to a lightweight network (student) at both training and inference times. The proposed AttTrack framework has three key components: 1) cross-model feature learning to align intermediate representations from the teacher and student models, 2) interleaving the execution of the two models at inference time, and 3) incorporating the updated predictions from the teacher model as prior knowledge to assist the student model. Experiments on pedestrian tracking tasks are conducted on the MOT17 and MOT15 datasets using two different object detection backbones YOLOv5 and DLA34 show that AttTrack can significantly improve student model tracking performance while sacrificing only minor degradation of tracking speed."
Attend Who Is Weak: Pruning-Assisted Medical Image Localization Under Sophisticated and Implicit Imbalances,"Ajay Jaiswal, Tianlong Chen, Justin F. Rousseau, Yifan Peng, Ying Ding, Zhangyang Wang",The University of Texas at Austin; Weill Cornell Medicine,100,USA,0,,"Deep neural networks (DNNs) have rapidly become a de facto choice to medical image understanding tasks. However, DNNs are notoriously fragile to the class imbalance in image classification. We further point out that such imbalance fragility can be amplified when it comes to more sophisticated tasks such as pathology localization, as imbalances in such problems can have highly complex and often implicit forms of presence. For example, different pathology can have different sizes or colors (w.r.t.the background), different underlying demographic distributions, and in general different difficulty levels to recognize, even in a meticulously curated balanced distribution of training data. In this paper, we propose to use pruning to automatically and adaptively identify hard-to-learn (HTL) training samples, and improve pathology localization by attending them explicitly, during training in supervised, semi-supervised, and weakly-supervised settings. Our main inspiration is drawn from the recent finding that deep classification models have difficult-to-memorize samples and those may be effectively exposed through network pruning - and we extend such observation beyond classification for the first time. We also present interesting demographic analysis which illustrates HTLs ability to capture complex demographic imbalances. Our extensive experiments on the Skin Lesion Localization task in multiple training settings by paying additional attention to HTLs show significant improvement of localization performance by  2-3%.",https://openaccess.thecvf.com/content/WACV2023/html/Jaiswal_Attend_Who_Is_Weak_Pruning-Assisted_Medical_Image_Localization_Under_Sophisticated_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jaiswal_Attend_Who_Is_Weak_Pruning-Assisted_Medical_Image_Localization_Under_Sophisticated_WACV_2023_paper.pdf,,,2212.02675,main,Poster,https://ieeexplore.ieee.org/document/10030493/,"['Location awareness', 'Training', 'Pathology', 'Image color analysis', 'Neural networks', 'Training data', 'Skin']","['Medical Imaging', 'Training Set', 'Training Data', 'Deep Neural Network', 'Image Classification', 'Extensive Experiments', 'Localization Performance', 'Class Imbalance', 'Localization Task', 'Demographic Analysis', 'Implicit Form', 'Network Pruning', 'In-depth Analysis', 'Network Training', 'Gender Distribution', 'Bounding Box', 'Class Distribution', 'Real-world Datasets', 'Performance Gain', 'Imbalanced Data', 'Medical Datasets', 'Pruning Method', 'Training Examples', 'Medical Image Datasets', 'Saliency Map', 'Minority Samples', 'Individual Demographics', 'Disproportionate Impact', 'Final Network', 'Labeled Training Data']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Biomedical/healthcare/medicine']",3,"Deep neural networks (DNNs) have rapidly become a de facto choice for medical image understanding tasks. However, DNNs are notoriously fragile to the class imbalance in image classification. We further point out that such imbalance fragility can be amplified when it comes to more sophisticated tasks such as pathology localization, as imbalances in such problems can have highly complex and often implicit forms of presence. For example, different pathology can have different sizes or colors (w.r.t.the background), different underlying demographic distributions, and in general different difficulty levels to recognize, even in a meticulously curated balanced distribution of training data. In this paper, we propose to use pruning to automatically and adaptively identify hard-to-learn (HTL) training samples, and improve pathology localization by attending them explicitly, during training in supervised, semi-supervised, and weakly-supervised settings. Our main inspiration is drawn from the recent finding that deep classification models have difficult-to-memorize samples and those may be effectively exposed through network pruning [15] - and we extend such observation beyond classification for the first time. We also present an interesting demographic analysis which illustrates HTLs ability to capture complex demographic imbalances. Our extensive experiments on the Skin Lesion Localization task in multiple training settings by paying additional attention to HTLs show significant improvement of localization performance by ∼2-3%."
Attention Attention Everywhere: Monocular Depth Prediction With Skip Attention,"Ashutosh Agarwal, Chetan Arora",Indian Institute of Technology Delhi,100,India,0,,"Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a single RGB image. For both, the convolutional as well as the recent attention-based models, encoder-decoder-based architectures have been found to be useful due to the simultaneous requirement of global context and pixel-level resolution. Typically, a skip connection module is used to fuse the encoder and decoder features, which comprises of feature map concatenation followed by a convolution operation. Inspired by the demonstrated benefits of attention in a multitude of computer vision problems, we propose an attention-based fusion of encoder and decoder features. We pose MDE as a pixel query refinement problem, where coarsest-level encoder features are used to initialize pixel-level queries, which are then refined to higher resolutions by the proposed Skip Attention Module (SAM). We formulate the prediction problem as ordinal regression over the bin centers that discretize the continuous depth range and introduce a Bin Center Predictor (BCP) module that predicts bins at the coarsest level using pixel queries. Apart from the benefit of image adaptive depth binning, the proposed design helps learn improved depth embedding in initial pixel queries via direct supervision from the ground truth. Extensive experiments on the two canonical datasets, NYUV2 and KITTI, show that our architecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively, along with an improved generalization performance by 9.4% on the SUNRGBD dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Attention_Attention_Everywhere_Monocular_Depth_Prediction_With_Skip_Attention_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Attention_Attention_Everywhere_Monocular_Depth_Prediction_With_Skip_Attention_WACV_2023_paper.pdf,,https://github.com/ashutosh1807/PixelFormer.git,2210.09071,main,Poster,https://ieeexplore.ieee.org/document/10030542/,"['Convolutional codes', 'Computer vision', 'Image resolution', 'Fuses', 'Convolution', 'Semantic segmentation', 'Estimation']","['Depth Prediction', 'Monocular Depth Prediction', 'Contralateral', 'Feature Maps', 'Single Image', 'Convolution Operation', 'RGB Images', 'Feature Fusion', 'Depth Range', 'Skip Connections', 'Depth Estimation', 'Direct Supervision', 'Feature Encoder', 'Central Bin', 'Decoder Features', 'Monocular Depth Estimation', 'Improve Generalization Performance', 'Root Mean Square Error', 'Convolutional Neural Network', 'Decoding', 'Vision Transformer', 'Global Information', 'KITTI Dataset', 'Ground Truth Depth', 'Depth Map', 'Input Image', 'Global Average Pooling', 'Semantic Gap', 'Receptive Field', 'Global Pooling']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",71,"Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a single RGB image. For both, the convolutional as well as the recent attention-based models, encoder-decoder-based architectures have been found to be useful due to the simultaneous requirement of global context and pixel-level resolution. Typically, a skip connection module is used to fuse the encoder and decoder features, which comprises of feature map concatenation followed by a convolution operation. Inspired by the demonstrated benefits of attention in a multitude of computer vision problems, we propose an attention-based fusion of encoder and decoder features. We pose MDE as a pixel query refinement problem, where coarsest-level encoder features are used to initialize pixel-level queries, which are then refined to higher resolutions by the proposed Skip Attention Module (SAM). We formulate the prediction problem as ordinal regression over the bin centers that discretize the continuous depth range and introduce a Bin Center Predictor (BCP) module that predicts bins at the coarsest level using pixel queries. Apart from the benefit of image adaptive depth binning, the proposed design helps learn improved depth embedding in initial pixel queries via direct supervision from the ground truth. Extensive experiments on the two canonical datasets, NYUV2 and KITTI, show that our architecture outperforms the state-of-the-art by 5.3% and 3.9%, respectively, along with an improved generalization performance by 9.4% on the SUNRGBD dataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git."
Attribution-Aware Weight Transfer: A Warm-Start Initialization for Class-Incremental Semantic Segmentation,"Dipam Goswami, René Schuster, Joost van de Weijer, Didier Stricker","‡Computer Vision Center, Barcelona; †DFKI - German Research Center for Artificial Intelligence, Kaiserslautern; †DFKI - German Research Center for Artificial Intelligence, Kaiserslautern §Birla Institute of Technology and Science, Pilani",100,"Germany, Spain",0,,"In class-incremental semantic segmentation (CISS), deep learning architectures suffer from the critical problems of catastrophic forgetting and semantic background shift. Although recent works focused on these issues, existing classifier initialization methods do not address the background shift problem and assign the same initialization weights to both background and new foreground class classifiers. We propose to address the background shift with a novel classifier initialization method which employs gradient-based attribution to identify the most relevant weights for new classes from the classifier's weights for the previous background and transfers these weights to the new classifier. This warm-start weight initialization provides a general solution applicable to several CISS methods. Furthermore, it accelerates learning of new classes while mitigating forgetting. Our experiments demonstrate significant improvement in mIoU compared to the state-of-the-art CISS methods on the Pascal-VOC 2012, ADE20K and Cityscapes datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Goswami_Attribution-Aware_Weight_Transfer_A_Warm-Start_Initialization_for_Class-Incremental_Semantic_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Goswami_Attribution-Aware_Weight_Transfer_A_Warm-Start_Initialization_for_Class-Incremental_Semantic_Segmentation_WACV_2023_paper.pdf,,,2210.07207,main,Poster,https://ieeexplore.ieee.org/document/10030539/,"['Computer vision', 'Semantic segmentation', 'Semantics', 'Deep architecture']","['Semantic Segmentation', 'Weight Transfer', 'Class Weights', 'Background Class', 'Semantic Change', 'Catastrophic Forgetting', 'Training Set', 'Training Data', 'Previous Step', 'Spatial Dimensions', 'Object Classification', 'Incremental Learning', 'Set Of Classes', 'Ablation Experiments', 'Classification Layer', 'Channel Selection', 'Pixel Classification', 'Incremental Steps', 'Fully Convolutional Network', 'Ground Truth Map', 'Attribution Methods', 'Real Class']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"In class-incremental semantic segmentation (CISS), deep learning architectures suffer from the critical problems of catastrophic forgetting and semantic background shift. Although recent works focused on these issues, existing classifier initialization methods do not address the background shift problem and assign the same initialization weights to both background and new foreground class classifiers. We propose to address the background shift with a novel classifier initialization method which employs gradient-based attribution to identify the most relevant weights for new classes from the classifier’s weights for the previous background and transfers these weights to the new classifier. This warm-start weight initialization provides a general solution applicable to several CISS methods. Furthermore, it accelerates learning of new classes while mitigating forgetting. Our experiments demonstrate significant improvement in mIoU compared to the state-of-the-art CISS methods on the Pascal-VOC 2012, ADE20K and Cityscapes datasets."
Audio-Visual Efficient Conformer for Robust Speech Recognition,"Maxime Burchi, Radu Timofte","Computer Vision Lab, CAIDAS, IFI, University of Würzburg, Germany",100,Germany,0,,"End-to-end Automatic Speech Recognition (ASR) systems based on neural networks have seen large improvements in recent years. The availability of large scale hand-labeled datasets and sufficient computing resources made it possible to train powerful deep neural networks, reaching very low Word Error Rate (WER) on academic benchmarks. However, despite impressive performance on clean audio samples, a drop of performance is often observed on noisy speech. In this work, we propose to improve the noise robustness of the recently proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based architecture by processing both audio and visual modalities. We improve previous lip reading methods using an Efficient Conformer back-end on top of a ResNet-18 visual front-end and by adding intermediate CTC losses between blocks. We condition intermediate block features on early predictions using Inter CTC residual modules to relax the conditional independence assumption of CTC-based models. We also replace the Efficient Conformer grouped attention by a more efficient and simpler attention mechanism that we call patch attention. We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and visual modalities allows to better recognize speech in the presence of environmental noise and significantly accelerate training, reaching lower WER with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC) model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on LRS2 and LRS3 test sets. Code and pretrained models are available at https://github.com/burchim/AVEC.",https://openaccess.thecvf.com/content/WACV2023/html/Burchi_Audio-Visual_Efficient_Conformer_for_Robust_Speech_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Burchi_Audio-Visual_Efficient_Conformer_for_Robust_Speech_Recognition_WACV_2023_paper.pdf,,https://github.com/burchim/AVEC,,main,Poster,https://ieeexplore.ieee.org/document/10030963/,"['Training', 'Visualization', 'Error analysis', 'Lips', 'Working environment noise', 'Neural networks', 'Computer architecture']","['Speech Recognition', 'Robust Speech Recognition', 'Neural Network', 'Deep Neural Network', 'Attention Mechanism', 'Early Prediction', 'Presence Of Noise', 'Visual Modality', 'Training Step', 'Intermediate Features', 'Availability Of Large Datasets', 'Speech In The Presence', 'Speech Recognition Systems', 'Word Error Rate', 'Automatic Speech Recognition System', 'Convolution', 'Utterances', 'Recurrent Neural Network', 'Feature Dimension', 'Visual Encoding', 'Auxiliary Loss', 'Babble Noise', 'Language Model', 'Multi-head Self-attention', 'Recognition Performance', 'Speech Recognition Performance', 'Global Relations', 'Reduce Model Complexity', 'Lip Movements']","['Applications: Embedded sensing/real-time techniques', 'Vision + language and/or other modalities']",13,"End-to-end Automatic Speech Recognition (ASR) systems based on neural networks have seen large improvements in recent years. The availability of large scale hand-labeled datasets and sufficient computing resources made it possible to train powerful deep neural networks, reaching very low Word Error Rate (WER) on academic benchmarks. However, despite impressive performance on clean audio samples, a drop of performance is often observed on noisy speech. In this work, we propose to improve the noise robustness of the recently proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based architecture by processing both audio and visual modalities. We improve previous lip reading methods using an Efficient Conformer back-end on top of a ResNet-18 visual front-end and by adding intermediate CTC losses between blocks. We condition intermediate block features on early predictions using Inter CTC residual modules to relax the conditional independence assumption of CTC-based models. We also replace the Efficient Conformer grouped attention by a more efficient and simpler attention mechanism that we call patch attention. We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and visual modalities allows to better recognize speech in the presence of environmental noise and significantly accelerate training, reaching lower WER with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC) model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on LRS2 and LRS3 test sets. Code and pretrained models are available at https://github.com/burchim/AVEC."
Audio-Visual Face Reenactment,"Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C. V. Jawahar","IIIT, Hyderabad; University of Bath",100,"India, UK",0,,"This work proposes a novel method to generate realistic talking head videos using audio and visual streams. We animate a source image by transferring head motion from a driving video using a dense motion field generated using learnable keypoints. We improve the quality of lip sync using audio as an additional input, helping the network to attend to the mouth region. We use additional priors using face segmentation and face mesh to improve the structure of the reconstructed faces. Finally, we improve the visual quality of the generations by incorporating a carefully designed identity-aware generator module. The identity-aware generator takes the source image and the warped motion features as input to generate a high-quality output with fine-grained details. Our method produces state-of-the-art results and generalizes well to unseen faces, languages, and voices. We comprehensively evaluate our approach using multiple metrics and outperforming the current techniques both qualitative and quantitatively. Our work opens up several applications, including enabling low bandwidth video calls. We release a demo video and additional information at http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Audio-Visual_Face_Reenactment_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Audio-Visual_Face_Reenactment_WACV_2023_paper.pdf,http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr,,2210.02755,main,Poster,https://ieeexplore.ieee.org/document/10031022/,"['Measurement', 'Visualization', 'Motion segmentation', 'Lips', 'Mouth', 'Streaming media', 'Generators']","['Videoconferencing', 'Visual Quality', 'Source Images', 'Motion Features', 'Motion Field', 'Talking Head', 'Lip-sync', 'Contralateral', 'Feature Maps', 'Final Output', 'Head Movements', 'Identity Information', 'Latent Space', 'Facial Features', 'Face Images', 'Motion Information', 'Keypoint Detection', 'Head Pose']","['Algorithms: Vision + language and/or other modalities', 'Biometrics', 'face', 'gesture', 'body pose', 'Commercial/retail']",14,"This work proposes a novel method to generate realistic talking head videos using audio and visual streams. We animate a source image by transferring head motion from a driving video using a dense motion field generated using learnable keypoints. We improve the quality of lip sync using audio as an additional input, helping the network to attend to the mouth region. We use additional priors using face segmentation and face mesh to improve the structure of the reconstructed faces. Finally, we improve the visual quality of the generations by incorporating a carefully designed identity-aware generator module. The identity-aware generator takes the source image and the warped motion features as input to generate a high-quality output with fine-grained details. Our method produces state-of-the-art results and generalizes well to unseen faces, languages, and voices. We comprehensively evaluate our approach using multiple metrics and outperforming the current techniques both qualitative and quantitatively. Our work opens up several applications, including enabling low bandwidth video calls. We release a demo video and additional information at http://cvit.iiit.ac.in/research/projects/cvit-projects/avfr."
AudioViewer: Learning To Visualize Sounds,"Chunjin Song, Yuchi Zhang, Willis Peng, Parmis Mohaghegh, Bastian Wandt, Helge Rhodin",University of British Columbia,100,Canada,0,,"A long-standing goal in the field of sensory substitution is enabling sound perception for deaf and hard of hearing (DHH) people by visualizing audio content. Different from existing models that translate to hand sign language, between speech and text, or text and images, we target immediate and low-level audio to video translation that applies to generic environment sounds as well as human speech. Since such a substitution is artificial, without labels for supervised learning, our core contribution is to build a mapping from audio to video that learns from unpaired examples via high-level constraints. For speech, we additionally disentangle content from style, such as gender and dialect. Qualitative and quantitative results, including a human study, demonstrate that our unpaired translation approach maintains important audio features in the generated video and that videos of faces and numbers are well suited for visualizing high-dimensional audio features that can be parsed by humans to match and distinguish between sounds and words. Project website: https://chunjinsong.github.io/audioviewer",https://openaccess.thecvf.com/content/WACV2023/html/Song_AudioViewer_Learning_To_Visualize_Sounds_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Song_AudioViewer_Learning_To_Visualize_Sounds_WACV_2023_paper.pdf,https://chunjinsong.github.io/audioviewer,,2012.13341,main,Poster,https://ieeexplore.ieee.org/document/10030740/,"['Visualization', 'Computer vision', 'Lips', 'Supervised learning', 'Gesture recognition', 'Auditory system', 'Real-time systems']","['Human Studies', 'Environmentally Friendly', 'Hearing Loss', 'Sign Language', 'Human Speech', 'Field Goal', 'Audio Content', 'Signal-to-noise', 'Kullback-Leibler', 'Acoustic Waves', 'Latent Space', 'Image Generation', 'Visual Space', 'Audio Data', 'Latent Dimensions', 'Latent Code', 'Temporal Smoothing', 'Video Modeling', 'Audio Input', 'Lip-sync', 'Translation Network']","['Applications: Visualization', 'Psychology and cognitive science', 'Social good']",2,"A long-standing goal in the field of sensory substitution is enabling sound perception for deaf and hard of hearing (DHH) people by visualizing audio content. Different from existing models that translate to hand sign language, between speech and text, or text and images, we target immediate and low-level audio to video translation that applies to generic environment sounds as well as human speech. Since such a substitution is artificial, with-out labels for supervised learning, our core contribution is to build a mapping from audio to video that learns from unpaired examples via high-level constraints. For speech, we additionally disentangle content from style, such as gender and dialect. Qualitative and quantitative results, including a human study, demonstrate that our unpaired translation approach maintains important audio features in the generated video and that videos of faces and numbers are well suited for visualizing high-dimensional audio features that can be parsed by humans to match and distinguish between sounds and words. Project website: https://chunjinsong.github.io/audioviewer"
Augmentation by Counterfactual Explanation - Fixing an Overconfident Classifier,"Sumedha Singla, Nihal Murali, Forough Arabshahi, Sofia Triantafyllou, Kayhan Batmanghelich",University of Crete; Meta AI; University of Pittsburgh,66.66666667,"Greece, USA",33.33333333,USA,"A highly accurate but overconfident model is ill-suited for deployment in critical applications such as healthcare and autonomous driving. The classification outcome should reflect a high uncertainty on ambiguous in-distribution samples that lie close to the decision boundary. The model should also refrain from making overconfident decisions on samples that lie far outside its training distribution, far-out-of-distribution (far-OOD), or on unseen samples from novel classes that lie near its training distribution (near-OOD). This paper proposes an application of counterfactual explanations in fixing an over-confident classifier. Specifically, we propose to fine-tune a given pre-trained classifier using augmentations from a counterfactual explainer (ACE) to fix its uncertainty characteristics while retaining its predictive performance. We perform extensive experiments with detecting far-OOD, near-OOD, and ambiguous samples. Our empirical results show that the revised model have improved uncertainty measures, and its performance is competitive to the state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Singla_Augmentation_by_Counterfactual_Explanation_-_Fixing_an_Overconfident_Classifier_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Singla_Augmentation_by_Counterfactual_Explanation_-_Fixing_an_Overconfident_Classifier_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030352/,"['Training', 'Computer vision', 'Uncertainty', 'Measurement uncertainty', 'Medical services', 'Robustness', 'Data models']","['Counterfactual Explanations', 'Decision Boundary', 'Training Distribution', 'Training Data', 'Deep Neural Network', 'Density Estimation', 'Uncertainty Estimation', 'Training Procedure', 'Real Samples', 'Latent Space', 'Input Space', 'Uncertainty Quantification', 'Adversarial Attacks', 'Image X', 'Negative Log-likelihood', 'Discriminator Network', 'Fine-tuned Model', 'Epistemic Uncertainty', 'Query Image', 'Deep Neural Network Classifier', 'Pre-trained Deep Neural Networks', 'Fast Gradient Sign Method', 'Soft Labels', 'Deep Neural Network Model', 'Aleatory', 'Objective Function']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Adversarial learning', 'adversarial attack and defense methods']",1,"A highly accurate but overconfident model is ill-suited for deployment in critical applications such as healthcare and autonomous driving. The classification outcome should reflect a high uncertainty on ambiguous in-distribution samples that lie close to the decision boundary. The model should also refrain from making overconfident decisions on samples that lie far outside its training distribution, far-out-of-distribution (far-OOD), or on unseen samples from novel classes that lie near its training distribution (near-OOD). This paper proposes an application of counterfactual explanations in fixing an over-confident classifier. Specifically, we propose to fine-tune a given pre-trained classifier using augmentations from a counterfactual explainer (ACE) to fix its uncertainty characteristics while retaining its predictive performance. We perform extensive experiments with detecting far-OOD, near-OOD, and ambiguous samples. Our empirical results show that the revised model have improved uncertainty measures, and its performance is competitive to the state-of-the-art methods."
Autoencoder-Based Background Reconstruction and Foreground Segmentation With Background Noise Estimation,"Bruno Sauvalle, Arnaud de La Fortelle","Centre de Robotique, Mines ParisTech PSL University",100,France,0,,"Even after decades of research, dynamic scene background reconstruction and foreground object segmentation are still considered as open problems due to various challenges such as illumination changes, camera movements, or background noise caused by air turbulence or moving trees. We propose in this paper to model the background of a frame sequence as a low dimensional manifold using an autoencoder and compare the reconstructed background provided by this autoencoder with the original image to compute the foreground/background segmentation masks. The main novelty of the proposed model is that the autoencoder is also trained to predict the background noise, which allows to compute for each frame a pixel-dependent threshold to perform the foreground segmentation. Although the proposed model does not use any temporal or motion information, it exceeds the state of the art for unsupervised background subtraction on the CDnet 2014 and LASIESTA datasets, with a significant improvement on videos where the camera is moving. It is also able to perform background reconstruction on some non-video image datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Sauvalle_Autoencoder-Based_Background_Reconstruction_and_Foreground_Segmentation_With_Background_Noise_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sauvalle_Autoencoder-Based_Background_Reconstruction_and_Foreground_Segmentation_With_Background_Noise_Estimation_WACV_2023_paper.pdf,,,2112.08001,main,Poster,https://ieeexplore.ieee.org/document/10030273/,"['Image segmentation', 'Adaptation models', 'Computational modeling', 'Motion segmentation', 'Predictive models', 'Cameras', 'Real-time systems']","['Background Noise', 'Foreground Segmentation', 'Background Reconstruction', 'Background Noise Estimation', 'Autoencoder', 'Background Subtraction', 'Temporal Information', 'Sequence Of Frames', 'Motion Information', 'Illumination Changes', 'Foreground Objects', 'Low-dimensional Manifold', 'Background Scene', 'Air Turbulence', 'Dynamic Background', 'Dynamic Reconstruction', 'Loss Function', 'Deep Learning', 'Deep Learning Models', 'Image Size', 'Unsupervised Model', 'Reconstruction Model', 'Simple Background', 'Reconstruction Loss', 'Complex Background', 'Presence Of Shadows', 'Unsupervised Methods', 'L1 Loss', 'Real Applications', 'Risk Of Overfitting']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",7,"Even after decades of research, dynamic scene background reconstruction and foreground object segmentation are still considered as open problems due to various challenges such as illumination changes, camera movements, or background noise caused by air turbulence or moving trees. We propose in this paper to model the background of a frame sequence as a low dimensional manifold using an autoencoder and compare the reconstructed background provided by this autoencoder with the original image to compute the foreground/background segmentation masks. The main novelty of the proposed model is that the autoencoder is also trained to predict the background noise, which allows to compute for each frame a pixel-dependent threshold to perform the foreground segmentation. Although the proposed model does not use any temporal or motion information, it exceeds the state of the art for unsupervised background subtraction on the CDnet 2014 and LASIESTA datasets, with a significant improvement on videos where the camera is moving. It is also able to perform background reconstruction on some non-video image datasets."
Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification,"Matthias Rottmann, Marco Reese","School of Mathematics and Natural Sciences, IZMD, University of Wuppertal, Germany; School of Computer and Communication Sciences, CVLab, EPFL, Switzerland; School of Mathematics and Natural Sciences, IZMD, University of Wuppertal, Germany",100,"Germany, Switzerland",0,,"In this work, we for the first time present a method for detecting labeling errors in image datasets with semantic segmentation, i.e., pixel-wise class labels. Annotation acquisition for semantic segmentation datasets is time-consuming and requires plenty of human labor. In particular, review processes are time consuming and label errors can easily be overlooked by humans. The consequences are biased benchmarks and in extreme cases also performance degradation of deep neural networks (DNNs) trained on such datasets. DNNs for semantic segmentation yield pixel-wise predictions, which makes detection of labeling errors via uncertainty quantification a complex task. Uncertainty is particularly pronounced at the transitions between connected components of the prediction. By lifting the consideration of uncertainty to the level of predicted components, we enable the usage of DNNs together with component-level uncertainty quantification for the detection of labeling errors. We present a principled approach to benchmarking the task of label error detection by dropping labels from the Cityscapes dataset as well from a dataset extracted from the CARLA driving simulator, where in the latter case we have the labels under control. Our experiments show that our approach is able to detect the vast majority of labeling errors while controlling the number of false label error detections. Furthermore, we apply our method to semantic segmentation datasets frequently used by the computer vision community and present a collection of labeling errors along with sample statistics.",https://openaccess.thecvf.com/content/WACV2023/html/Rottmann_Automated_Detection_of_Label_Errors_in_Semantic_Segmentation_Datasets_via_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rottmann_Automated_Detection_of_Label_Errors_in_Semantic_Segmentation_Datasets_via_WACV_2023_paper.pdf,,,2207.06104,main,Poster,https://ieeexplore.ieee.org/document/10030499/,"['Deep learning', 'Degradation', 'Computer vision', 'Uncertainty', 'Annotations', 'Semantic segmentation', 'Neural networks']","['Deep Learning', 'Semantic Segmentation', 'Error Detection', 'Uncertainty Quantification', 'Errors In Dataset', 'Labeling Errors', 'Semantic Segmentation Datasets', 'Benchmark', 'Deep Neural Network', 'Detection Task', 'False Labels', 'False Positive', 'Training Set', 'Inter-rater', 'Medical Imaging', 'True Positive', 'Validation Set', 'Active Learning', 'Image Classification', 'Intersection Over Union', 'Label Noise', 'Pixel Level', 'Real Error', 'Baseline Methods', 'Original Image Size', 'Traffic Light', 'Quantization Error', 'Street Scenes', 'Perturbation Level', 'Handcrafted Features']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",11,"In this work, we present for the first time a method for detecting label errors in image datasets with semantic segmentation, i.e., pixel-wise class labels. Annotation acquisition for semantic segmentation datasets is time-consuming and requires plenty of human labor. In particular, review processes are time consuming and label errors can easily be overlooked by humans. The consequences are biased benchmarks and in extreme cases also performance degradation of deep neural networks (DNNs) trained on such datasets. DNNs for semantic segmentation yield pixel-wise predictions, which makes detection of label errors via uncertainty quantification a complex task. Uncertainty is particularly pronounced at the transitions between connected components of the prediction. By lifting the consideration of uncertainty to the level of predicted components, we enable the usage of DNNs together with component-level uncertainty quantification for the detection of label errors. We present a principled approach to benchmark the task of label error detection by dropping labels from the Cityscapes dataset as well as from a dataset extracted from the CARLA driving simulator, where in the latter case we have the labels under control. Our experiments show that our approach is able to detect the vast majority of label errors while controlling the number of false label error detections. Furthermore, we apply our method to semantic segmentation datasets frequently used by the computer vision community and present a collection of label errors along with sample statistics."
Automated Line Labelling: Dataset for Contour Detection and 3D Reconstruction,"Hari Santhanam, Nehal Doiphode, Jianbo Shi",University of Pennsylvania,100,USA,0,,"Understanding the finer details of a 3D object, its contours, is the first step toward a physical understanding of an object. Many real-world application domains require adaptable 3D object shape recognition models, usually with little training data. For this purpose, we develop the first automatically generated contour labeled dataset, bypassing manual human labeling. Using this dataset, we study the performance of current state-of-the-art instance segmentation algorithms on detecting and labeling the contours. We produce promising visual results with accurate contour prediction and labeling. We demonstrate that our finely labeled contours can help downstream tasks in computer vision, such as 3D reconstruction from a 2D image.",https://openaccess.thecvf.com/content/WACV2023/html/Santhanam_Automated_Line_Labelling_Dataset_for_Contour_Detection_and_3D_Reconstruction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Santhanam_Automated_Line_Labelling_Dataset_for_Contour_Detection_and_3D_Reconstruction_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030405/,"['Solid modeling', 'Computer vision', 'Visualization', 'Three-dimensional displays', 'Shape', 'Training data', 'Manuals']","['3D Reconstruction', 'Contour Detection', 'Computer Vision', '2D Images', 'Segmentation Algorithm', 'Vision Tasks', 'Objective Understanding', 'Human Labeling', 'Transformer', '3D Printing', 'Grid Cells', 'Intersection Over Union', 'Input Channels', 'Segmentation Performance', 'Mean Average Precision', 'Local Curvature', 'Segmentation Problem', '3D Segmentation', 'Early Fusion', 'Visual Point', 'Bezier Curve', '3D Contour', 'Unseen Objects', '2D Contour', '3D CAD Models', 'Ray Casting', 'Single Viewpoint', '3D Tasks', 'Surface Normals']",['Algorithms: 3D computer vision'],4,"Understanding the finer details of a 3D object, its contours, is the first step toward a physical understanding of an object. Many real-world application domains require adaptable 3D object shape recognition models, usually with little training data. For this purpose, we develop the first automatically generated contour labeled dataset, bypassing manual human labeling. Using this dataset, we study the performance of current state-of-the-art instance segmentation algorithms on detecting and labeling the contours. We produce promising visual results with accurate contour prediction and labeling. We demonstrate that our finely labeled contours can help downstream tasks in computer vision, such as 3D reconstruction from a 2D image."
Automatically Annotating Indoor Images With CAD Models via RGB-D Scans,"Stefan Ainetter, Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit","Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria; LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France",100,"Austria, France",0,,"We present an automatic method for annotating images of indoor scenes with the CAD models of the objects by relying on RGB-D scans. Through a visual evaluation by 3D experts, we show that our method retrieves annotations that are at least as accurate as manual annotations, and can thus be used as ground truth without the burden of manually annotating 3D data. We do this using an analysis-by-synthesis approach, which compares renderings of the CAD models with the captured scene. We introduce a 'cloning procedure' that identifies objects that have the same geometry, to annotate these objects with the same CAD models. This allows us to obtain complete annotations for the ScanNet dataset and the recent ARKitScenes dataset. We will release these annotations publicly, as we believe they will be very useful for the computer vision community.",https://openaccess.thecvf.com/content/WACV2023/html/Ainetter_Automatically_Annotating_Indoor_Images_With_CAD_Models_via_RGB-D_Scans_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ainetter_Automatically_Annotating_Indoor_Images_With_CAD_Models_via_RGB-D_Scans_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030501/,"['Geometry', 'Solid modeling', 'Computer vision', 'Visualization', 'Three-dimensional displays', 'Annotations', 'Computational modeling']","['CAD Model', 'RGB-D Scans', 'Computer Vision', 'Manual Annotation', 'Visual Evaluation', 'Cloning Procedures', 'Objective Function', 'Point Cloud', 'Bounding Box', 'Image Pairs', 'Latent Space', 'Exhaustive Search', 'Target Object', 'Depth Map', '3D Scanning', '3D Mesh', 'Objects In The Scene', 'Instance Segmentation', 'Top Image', '3D Scene', '3D Bounding Box', '3D Orientation', 'Chamfer Distance', 'Clustering Objective', 'Reprojection', 'Bottom Image', 'Semantic Segmentation', 'Similar Characteristics']",['Algorithms: 3D computer vision'],4,"We present an automatic method for annotating images of indoor scenes with the CAD models of the objects by relying on RGB-D scans. Through a visual evaluation by 3D experts, we show that our method retrieves annotations that are at least as accurate as manual annotations, and can thus be used as ground truth without the burden of manually annotating 3D data. We do this using an analysis-by-synthesis approach, which compares renderings of the CAD models with the captured scene. We introduce a ’cloning procedure’ that identifies objects that have the same geometry, to annotate these objects with the same CAD models. This allows us to obtain complete annotations for the Scan-Net dataset and the recent ARKitScenes dataset. We will release these annotations publicly, as we believe they will be very useful for the computer vision community."
Auxiliary Task-Guided CycleGAN for Black-Box Model Domain Adaptation,"Michael Essich, Markus Rehmann, Cristóbal Curio","Cognitive Systems Group, Reutlingen University, Germany",100,Germany,0,,"The research area of domain adaptation investigates methods that enable the transfer of existing models across different domains, e.g., addressing environmental changes or the transfer from synthetic to real data. Especially unsupervised domain adaptation is beneficial because it does not require any labeled target domain data. Usually, existing methods are targeted at specific tasks and require access or even modifications to the source model and its parameters which is a major drawback when only a black-box model is available. Therefore, we propose a CycleGAN-based approach suitable for black-box source models to translate target domain data into the source domain on which the source model can operate. Inspired by multi-task learning, we extend CycleGAN with an additional auxiliary task that can be arbitrarily chosen to support the transfer of task-related information across domains without the need for having access to a differentiable source model or its parameters. In this work, we focus on the regression task of 2D human pose estimation and compare our results in four different domain adaptation settings to CycleGAN and RegDA, a state-of-the-art method for unsupervised domain adaptation for keypoint detection.",https://openaccess.thecvf.com/content/WACV2023/html/Essich_Auxiliary_Task-Guided_CycleGAN_for_Black-Box_Model_Domain_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Essich_Auxiliary_Task-Guided_CycleGAN_for_Black-Box_Model_Domain_Adaptation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030089/,"['Training', 'Adaptation models', 'Computer vision', 'Pose estimation', 'Closed box', 'Benchmark testing', 'Multitasking']","['Domain Adaptation', 'Specific Tasks', 'Information Transfer', 'Target Domain', 'Source Model', 'Additional Tasks', 'Pose Estimation', 'Multi-task Learning', 'Source Domain', 'Human Pose Estimation', 'Human Pose', 'Auxiliary Task', 'Keypoint Detection', 'Target Domain Data', 'Unsupervised Domain Adaptation Methods', 'Adaptation In Areas', 'Feature Space', 'Paired Data', 'Bounding Box', 'Generative Adversarial Networks', 'Source Domain Data', 'Domain Adaptation Methods', 'Synthetic Images', 'Domain Shift', 'Motion Capture', 'Extrinsic Parameters', 'Unlabeled Data', '3D Scanning', 'Semantic Segmentation', 'Imaging Buffer']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",2,"The research area of domain adaptation investigates methods that enable the transfer of existing models across different domains, e.g., addressing environmental changes or the transfer from synthetic to real data. Especially unsupervised domain adaptation is beneficial because it does not require any labeled target domain data. Usually, existing methods are targeted at specific tasks and require access or even modifications to the source model and its parameters which is a major drawback when only a black-box model is available. Therefore, we propose a CycleGAN-based approach suitable for black-box source models to translate target domain data into the source domain on which the source model can operate. Inspired by multi-task learning, we extend CycleGAN with an additional auxiliary task that can be arbitrarily chosen to support the transfer of task-related information across domains without the need for having access to a differentiable source model or its parameters. In this work, we focus on the regression task of 2D human pose estimation and compare our results in four different domain adaptation settings to CycleGAN and RegDA, a state-of-the-art method for unsupervised domain adaptation for keypoint detection."
Avoiding Lingering in Learning Active Recognition by Adversarial Disturbance,"Lei Fan, Ying Wu",Northwestern University,100,USA,0,,"This paper considers the active recognition scenario, where the agent is empowered to intelligently acquire observations for better recognition. The agents usually compose two modules, i.e., the policy and the recognizer, to select actions and predict the category. While using ground-truth class labels to supervise the recognizer, the policy is typically updated with rewards determined by the current in-training recognizer, like whether achieving correct predictions. However, this joint learning process could lead to unintended solutions, like a collapsed policy that only visits views that the recognizer is already sufficiently trained to obtain rewards, which harms the generalization ability. We call this phenomenon lingering to depict the agent being reluctant to explore challenging views during training. Existing approaches to tackle the exploration-exploitation trade-off could be ineffective as they usually assume reliable feedback during exploration to update the estimate of rarely-visited states. This assumption is invalid here as the reward from the recognizer could be insufficiently trained. To this end, our approach integrates another adversarial policy to constantly disturb the recognition agent during training, forming a competing game to promote active explorations and avoid lingering. The reinforced adversary, rewarded when the recognition fails, contests the recognition agent by turning the camera to challenging observations. Extensive experiments across two datasets validate the effectiveness of the proposed approach regarding its recognition performances, learning efficiencies, and especially robustness in managing environmental noises.",https://openaccess.thecvf.com/content/WACV2023/html/Fan_Avoiding_Lingering_in_Learning_Active_Recognition_by_Adversarial_Disturbance_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Fan_Avoiding_Lingering_in_Learning_Active_Recognition_by_Adversarial_Disturbance_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030827/,"['Training', 'Computer vision', 'Working environment noise', 'Perturbation methods', 'Games', 'Turning', 'Cameras']","['Action Recognition', 'Adversarial Disturbances', 'Recognizable', 'Generalization Ability', 'Recognition Performance', 'Environmental Noise', 'Active Learning', 'Recurrent Neural Network', 'Active Agents', 'Object Recognition', 'Generative Adversarial Networks', 'Visual Observation', 'Target Object', 'Reward Function', 'Markov Decision Process', 'Gated Recurrent Unit', 'Policy Learning', 'Weight Balance', 'Joint Training', 'Visual Encoding', 'Hidden Vector', 'Camera Viewpoint', 'Object Instances', 'Policy Update', 'Limited View', 'Loss Term', 'Recognition Accuracy', 'Proprioceptive', 'Training Iterations']","['Algorithms: Vision + language and/or other modalities', 'Robotics']",2,"This paper considers the active recognition scenario, where the agent is empowered to intelligently acquire observations for better recognition. The agents usually compose two modules, i.e., the policy and the recognizer, to select actions and predict the category. While using ground-truth class labels to supervise the recognizer, the policy is typically updated with rewards determined by the current in-training recognizer, like whether achieving correct predictions. However, this joint learning process could lead to unintended solutions, like a collapsed policy that only visits views that the recognizer is already sufficiently trained to obtain rewards, which harms the generalization ability. We call this phenomenon lingering to depict the agent being reluctant to explore challenging views during training. Existing approaches to tackle the exploration-exploitation trade-off could be ineffective as they usually assume reliable feedback during exploration to update the estimate of rarely-visited states. This assumption is invalid here as the reward from the recognizer could be insufficiently trained.To this end, our approach integrates another adversarial policy to constantly disturb the recognition agent during training, forming a competing game to promote active explorations and avoid lingering. The reinforced adversary, rewarded when the recognition fails, contests the recognition agent by turning the camera to challenging observations. Extensive experiments across two datasets validate the effectiveness of the proposed approach regarding its recognition performances, learning efficiencies, and especially robustness in managing environmental noises."
BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs,"Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang, Erkang Cheng",Zhengzhou University; Nullmax,50,China,50,USA,"Semantic segmentation in bird's eye view (BEV) is an important task for autonomous driving. Though this task has attracted a large amount of research efforts, it is still challenging to flexibly cope with arbitrary (single or multiple) camera sensors equipped on the autonomous vehicle. In this paper, we present BEVSegFormer, an effective transformer-based method for BEV semantic segmentation from arbitrary camera rigs. Specifically, our method first encodes image features from arbitrary cameras with a shared backbone. These image features are then enhanced by a deformable transformer-based encoder. Moreover, we introduce a BEV transformer decoder module to parse BEV semantic segmentation results. An efficient multi-camera deformable attention unit is designed to carry out the BEV-to-image view transformation. Finally, the queries are reshaped according the layout of grids in the BEV, and upsampled to produce the semantic segmentation result in a supervised manner. We evaluate the proposed algorithm on the public nuScenes dataset and a self-collected dataset. Experimental results show that our method achieves promising performance on BEV semantic segmentation from arbitrary camera rigs. We also demonstrate the effectiveness of each component via ablation study.",https://openaccess.thecvf.com/content/WACV2023/html/Peng_BEVSegFormer_Birds_Eye_View_Semantic_Segmentation_From_Arbitrary_Camera_Rigs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Peng_BEVSegFormer_Birds_Eye_View_Semantic_Segmentation_From_Arbitrary_Camera_Rigs_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030199/,"['Computer vision', 'Semantic segmentation', 'Semantics', 'Layout', 'Transformers', 'Cameras', 'Decoding']","['Semantic Segmentation', 'Bird’s Eye', 'Arbitrary Camera', 'Image Features', 'Attention Mechanism', 'Autonomous Vehicles', 'Segmentation Results', 'Multiple Cameras', 'Semantic Segmentation Methods', 'Transformer-based Methods', 'Semantic Segmentation Results', 'Transformer Decoder', 'Feature Maps', '3D Space', 'Temporal Information', 'Multiple Images', 'Straightforward Way', 'Attention Module', 'Image Space', 'Multi-scale Features', 'Transformer Encoder', 'Front Camera', 'Standard Cross', 'Position Embedding', 'Standard Module', 'Extrinsic Parameters', 'Depth Estimation', 'Attention Weights', 'Traffic Lanes']","['Applications: Robotics', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",56,"Semantic segmentation in bird's eye view (BEV) is an important task for autonomous driving. Though this task has attracted a large amount of research efforts, it is still challenging to flexibly cope with arbitrary (single or multiple) camera sensors equipped on the autonomous vehicle. In this paper, we present BEVSegFormer, an effective transformer-based method for BEV semantic segmentation from arbitrary camera rigs. Specifically, our method first encodes image features from arbitrary cameras with a shared backbone. These image features are then enhanced by a deformable transformer-based encoder. Moreover, we introduce a BEV transformer decoder module to parse BEV semantic segmentation results. An efficient multi-camera deformable attention unit is designed to carry out the BEV-to-image view transformation. Finally, the queries are reshaped according to the layout of grids in the BEV, and upsampled to produce the semantic segmentation result in a supervised manner. We evaluate the proposed algorithm on the public nuScenes dataset and a self-collected dataset. Experimental results show that our method achieves promising performance on BEV semantic segmentation from arbitrary camera rigs. We also demonstrate the effectiveness of each component via ablation study."
"BURST: A Benchmark for Unifying Object Recognition, Segmentation and Tracking in Video","Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, Deva Ramanan","Amazon; Carnegie Mellon University, USA; Google; RWTH Aachen University, Germany",50,"Germany, USA",50,USA,"Multiple existing benchmarks involve tracking and segmenting objects in video e.g., Video Object Segmentation (VOS) and Multi-Object Tracking and Segmentation (MOTS), but there is little interaction between them due to the use of disparate benchmark datasets and metrics (e.g. \JnF, mAP, sMOTSA). As a result, published works usually target a particular benchmark, and are not easily comparable to each another. We believe that the development of generalized methods that can tackle multiple tasks requires greater cohesion among these research sub-communities. In this paper, we aim to facilitate this by proposing BURST, a dataset which contains thousands of diverse videos with high-quality object masks, and an associated benchmark with six tasks involving object tracking and segmentation in video. All tasks are evaluated using the same data and comparable metrics, which enables researchers to consider them in unison, and hence, more effectively pool knowledge from different methods across different tasks. Additionally, we demonstrate several baselines for all tasks and show that approaches for one task can be applied to another with a quantifiable and explainable performance difference. Dataset annotations are available at: https://github.com/Ali2500/BURST-benchmark.",https://openaccess.thecvf.com/content/WACV2023/html/Athar_BURST_A_Benchmark_for_Unifying_Object_Recognition_Segmentation_and_Tracking_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Athar_BURST_A_Benchmark_for_Unifying_Object_Recognition_Segmentation_and_Tracking_WACV_2023_paper.pdf,,https://github.com/Ali2500/BURST-benchmark,2209.12118,main,Poster,https://ieeexplore.ieee.org/document/10030387/,"['Measurement', 'Training', 'Vocabulary', 'Target tracking', 'Annotations', 'Taxonomy', 'Benchmark testing']","['Benchmark', 'Video Tracking', 'Object Segmentation', 'Object Tracking', 'Video Segments', 'Video Recognition', 'Video Object', 'Multi-object Tracking', 'Training Set', 'False Negative', 'Detection Accuracy', 'Object Detection', 'Bounding Box', 'Object Classification', 'Multiple Objects', 'Video Frames', 'Target Object', 'Positive Detection', 'Set Of Classes', 'Tracking Accuracy', 'True Positive Detection', 'Common Tasks', 'False Positive Detection', 'Outdoor Scenes', 'Human Effort', 'Internet Video', 'Object In Frame', 'Instance Segmentation', 'Validation Set', 'Final Metric']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",20,"Multiple existing benchmarks involve tracking and segmenting objects in video e.g., Video Object Segmentation (VOS) and Multi-Object Tracking and Segmentation (MOTS), but there is little interaction between them due to the use of disparate benchmark datasets and metrics (e.g. $\mathcal{J}\& {\mathcal{F}}$, mAP, sMOTSA). As a result, published works usually target a particular benchmark, and are not easily comparable to each another. We believe that the development of generalized methods that can tackle multiple tasks requires greater cohesion among these research sub-communities. In this paper, we aim to facilitate this by proposing BURST, a dataset which contains thousands of diverse videos with high-quality object masks, and an associated benchmark with six tasks involving object tracking and segmentation in video. All tasks are evaluated using the same data and comparable metrics, which enables researchers to consider them in unison, and hence, more effectively pool knowledge from different methods across different tasks. Additionally, we demonstrate several baselines for all tasks and show that approaches for one task can be applied to another with a quantifiable and explainable performance difference. Dataset annotations are available at: https://github.com/Ali2500/BURST-benchmark."
Back to MLP: A Simple Baseline for Human Motion Prediction,"Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier Alameda-Pineda, Francesc Moreno-Noguer","Tencent AI Lab; LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France; Inria, Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France; Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Barcelona, Spain",75,"France, Spain",25,China,"This paper tackles the problem of human motion prediction, consisting in forecasting future body poses from historically observed sequences. State-of-the-art approaches provide good results, however, they rely on deep learning architectures of arbitrary complexity, such as Recurrent Neural Networks(RNN), Transformers or Graph Convolutional Networks(GCN), typically requiring multiple training stages and more than 2 million parameters. In this paper, we show that, after combining with a series of standard practices, such as applying Discrete Cosine Transform (DCT), predicting residual displacement of joints and optimizing velocity as an auxiliary loss, a light-weight network based on multi-layer perceptrons (MLPs) with only 0.14 million parameters can surpass the state-of-the-art performance. An exhaustive evaluation on the Human3.6M, AMASS, and 3DPW datasets shows that our method, named siMLPe, consistently outperforms all other approaches. We hope that our simple method could serve as a strong baseline for the community and allow re-thinking of the human motion prediction problem. The code is publicly available at https://github.com/dulucas/siMLPe.",https://openaccess.thecvf.com/content/WACV2023/html/Guo_Back_to_MLP_A_Simple_Baseline_for_Human_Motion_Prediction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Guo_Back_to_MLP_A_Simple_Baseline_for_Human_Motion_Prediction_WACV_2023_paper.pdf,,https://github.com/dulucas/siMLPe,2207.01567,main,Poster,https://ieeexplore.ieee.org/document/10030747/,"['Training', 'Computer vision', 'Recurrent neural networks', 'Deep architecture', 'Transformers', 'Discrete cosine transforms', 'Convolutional neural networks']","['Multilayer Perceptron', 'Human Motion', 'Motion Prediction', 'Human Motion Prediction', 'Deep Learning', 'Transformer', 'Recurrent Neural Network', 'Graph Convolutional Network', 'Discrete Cosine Transform', 'Auxiliary Loss', 'Residual Displacement', 'Training Set', 'Training Data', 'Time Step', 'Validation Set', 'Temporal Dimension', 'Data Augmentation', 'Temporal Information', 'Normalization Layer', 'Exhaustive Search', '3D Pose', 'Long-term Prediction', 'Human Pose', 'Motion Sequences', 'Short-term Prediction', 'Complex Motion', 'Transpose Operator', 'Number Of Joints', 'Fewer Parameters', 'Gaussian Process']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision']",62,"This paper tackles the problem of human motion prediction, consisting in forecasting future body poses from historically observed sequences. State-of-the-art approaches provide good results, however, they rely on deep learning architectures of arbitrary complexity, such as Recurrent Neural Networks (RNN), Transformers or Graph Convolutional Networks (GCN), typically requiring multiple training stages and more than 2 million parameters. In this paper, we show that, after combining with a series of standard practices, such as applying Discrete Cosine Transform (DCT), predicting residual displacement of joints and optimizing velocity as an auxiliary loss, a light-weight network based on multi-layer perceptrons (MLPs) with only 0.14 million parameters can surpass the state-of-the-art performance. An exhaustive evaluation on the Human3.6M, AMASS, and 3DPW datasets shows that our method, named siMLpe, consistently outperforms all other approaches. We hope that our simple method could serve as a strong baseline for the community and allow re-thinking of the human motion prediction problem. The code is publicly available at https://github.com/dulucas/siMLPe."
Backprop Induced Feature Weighting for Adversarial Domain Adaptation With Iterative Label Distribution Alignment,"Thomas Westfechtel, Hao-Wei Yeh, Qier Meng, Yusuke Mukuta, Tatsuya Harada","The University of Tokyo; The University of Tokyo, RIKEN",100,Japan,0,,"The requirement for large labeled datasets is one of the limiting factors for training accurate deep neural networks. Unsupervised domain adaptation tackles this problem of limited training data by transferring knowledge from one domain, which has many labeled data, to a different domain for which little to no labeled data is available. One common approach is to learn domain-invariant features for example with an adversarial approach. Previous methods often train the domain classifier and label classifier network separately, where both classification networks have little interaction with each other. In this paper, we introduce a classifier-based backprop-induced weighting of the feature space. This approach has two main advantages. Firstly, it lets the domain classifier focus on features that are important for the classification, and, secondly, it couples the classification and adversarial branch more closely. Furthermore, we introduce an iterative label distribution alignment method, that employs results of previous runs to approximate a class-balanced dataloader. We conduct experiments and ablation studies on three benchmarks Office-31, OfficeHome, and DomainNet to show the effectiveness of our proposed algorithm.",https://openaccess.thecvf.com/content/WACV2023/html/Westfechtel_Backprop_Induced_Feature_Weighting_for_Adversarial_Domain_Adaptation_With_Iterative_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Westfechtel_Backprop_Induced_Feature_Weighting_for_Adversarial_Domain_Adaptation_With_Iterative_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030755/,"['Training', 'Deep learning', 'Computer vision', 'Limiting', 'Neural networks', 'Training data', 'Benchmark testing']","['Domain Adaptation', 'Label Distribution', 'Iterative Alignment', 'Iterative Label', 'Feature Space', 'Domain Classifier', 'Domain-invariant Features', 'Adversarial Approach', 'Average Accuracy', 'Attention Mechanism', 'Domain Shift', 'Version Of Task', 'Target Domain', 'Target Data', 'Iterative Scheme', 'Representative Class', 'Source Domain', 'Target Dataset', 'Source Dataset', 'Pseudo Labels', 'Bootstrapping Strategy', 'Domain Gap', 'Beginning Of Training']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",13,"The requirement for large labeled datasets is one of the limiting factors for training accurate deep neural networks. Unsupervised domain adaptation tackles this problem of limited training data by transferring knowledge from one domain, which has many labeled data, to a different domain for which little to no labeled data is available. One common approach is to learn domain-invariant features for example with an adversarial approach. Previous methods often train the domain classifier and label classifier network separately, where both classification networks have little interaction with each other. In this paper, we introduce a classifier-based backprop-induced weighting of the feature space. This approach has two main advantages. Firstly, it lets the domain classifier focus on features that are important for the classification, and, secondly, it couples the classification and adversarial branch more closely. Furthermore, we introduce an iterative label distribution alignment method, that employs results of previous runs to approximate a class-balanced dataloader. We conduct experiments and ablation studies on three benchmarks Office-31, Office-Home, and DomainNet to show the effectiveness of our proposed algorithm."
Barlow Constrained Optimization for Visual Question Answering,"Abhishek Jha, Badri Patro, Luc Van Gool, Tinne Tuytelaars","ESAT-PSI, KU Leuven, CVL, ETH Zürich; ESAT-PSI, KU Leuven",100,"Belgium, Switzerland",0,,"Visual question answering is a vision-and-language multimodal task, that aims at predicting answers given samples from the question and image modalities. Most recentmethods focus on learning a good joint embedding space ofimages and questions, either by improving the interactionbetween these two modalities, or by making it a more discriminant space. However, how informative this joint space is, has not been well explored. In this paper, we propose a novel regularization for VQA models, Constrained Optimization using Barlow's theory (COB), that improves the information content of the joint space by minimizing the redundancy. It reduces the correlation between the learned feature components and thereby disentangles semantic concepts. Our model also aligns the joint space with the answer embedding space, where we consider the answer and image+question as two different 'views' of what in essence is the same semantic information. We propose a constrained optimization policy to balance the categorical and redundancy minimization forces. When built on the state-of-the-art GGE model, the resulting model improves VQA accuracy by 1.4% and 4% on the VQA-CP v2 and VQA v2 datasets respectively. The model also exhibits better interpretability. Code is made available: https://github.com/abskjha/Barlow-constrained-VQA",https://openaccess.thecvf.com/content/WACV2023/html/Jha_Barlow_Constrained_Optimization_for_Visual_Question_Answering_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jha_Barlow_Constrained_Optimization_for_Visual_Question_Answering_WACV_2023_paper.pdf,,https://github.com/abskjha/Barlow-constrained-VQA,2203.03727,main,Poster,https://ieeexplore.ieee.org/document/10030487/,"['Training', 'Visualization', 'Computational modeling', 'Redundancy', 'Semantics', 'Minimization', 'Question answering (information retrieval)']","['Constrained Optimization', 'Visual Question Answering', 'Information Content', 'Latent Space', 'Joint Space', 'Joint Embedding', 'V2 Dataset', 'Objective Function', 'Feature Space', 'Feature Information', 'Visual Features', 'Cross-entropy Loss', 'Projector', 'Model Interpretation', 'Canonical Correlation Analysis', 'Loss Term', 'Decorrelation', 'Biased Distribution', 'Form Of Constraints', 'Dataset Bias', 'Language Bias', 'Joint Representation', 'Redundancy Reduction', 'Constraint Loss', 'Joint Feature', 'Constraint Term', 'Training Policy', 'Latent Features', 'Projective Space', 'Improvements In Outcomes']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"Visual question answering is a vision-and-language multimodal task, that aims at predicting answers given samples from the question and image modalities. Most recent methods focus on learning a good joint embedding space of images and questions, either by improving the interaction between these two modalities, or by making it a more discriminant space. However, how informative this joint space is, has not been well explored. In this paper, we propose a novel regularization for VQA models, Constrained Optimization using Barlow’s theory (COB), that improves the information content of the joint space by minimizing the redundancy. It reduces the correlation between the learned feature components and thereby disentangles semantic concepts. Our model also aligns the joint space with the answer embedding space, where we consider the answer and image+question as two different ‘views’ of what in essence is the same semantic information. We propose a constrained optimization policy to balance the categorical and redundancy minimization forces. When built on the state-of-the-art GGE model, the resulting model improves VQA accuracy by 1.4% and 4% on the VQA-CP v2 and VQA v2 datasets respectively. The model also exhibits better interpretability. Code is made available: https://github.com/abskjha/Barlow-constrained-VQA"
Benchmarking Visual Localization for Autonomous Navigation,"Lauri Suomela, Jussi Kalliola, Atakan Dag, Harry Edelman, Joni-Kristian Kämäräinen","Tampere University, Finland",100,Finland,0,,"This work introduces a simulator-based benchmark for visual localization in the autonomous navigation context. The dynamic benchmark enables investigation of how variables such as the time of day, weather, and camera perspective affect the navigation performance of autonomous agents that utilize visual localization for closed-loop control. The experimental part of the paper studies the effects of four such variables by evaluating state-of-the-art visual localization methods as part of the motion planning module of an autonomous navigation stack. The results show major variation in the suitability of the different methods for vision-based navigation. To the authors' best knowledge, the proposed benchmark is the first to study modern visual localization methods as part of a complete navigation stack. We make the benchmark available at https://github.com/lasuomela/carla_vloc_benchmark.",https://openaccess.thecvf.com/content/WACV2023/html/Suomela_Benchmarking_Visual_Localization_for_Autonomous_Navigation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Suomela_Benchmarking_Visual_Localization_for_Autonomous_Navigation_WACV_2023_paper.pdf,,https://github.com/lasuomela/carla_vloc_benchmark,2203.13048,main,Poster,https://ieeexplore.ieee.org/document/10030250/,"['Location awareness', 'Measurement', 'Visualization', 'Computer vision', 'Navigation', 'Benchmark testing', 'Cameras']","['Visual Localization', 'Autonomous Navigation', 'Local Method', 'Visual Methods', 'Path Planning', 'Autonomous Agents', 'Suitable Variables', 'Navigation Performance', 'Local Features', 'Performance Metrics', 'Changes In Appearance', 'Illumination Conditions', 'Pose Estimation', 'Elevation Angle', 'None Of These Methods', 'Recall Rate', 'Weather Changes', 'Extended Kalman Filter', 'Illumination Changes', 'Solar Zenith Angle', 'Query Image', 'Gallery Set', 'Nonexpansive Mapping', 'Gallery Images', 'Viewpoint Changes', 'Benchmark Simulation', 'Place Recognition', 'Camera Pose', 'Performance Of Method', 'Localization Performance']","['Applications: Robotics', '3D computer vision']",,"This work introduces a simulator-based benchmark for visual localization in the autonomous navigation context. The dynamic benchmark enables investigation of how variables such as the time of day, weather, and camera perspective affect the navigation performance of autonomous agents that utilize visual localization for closed-loop control. The experimental part of the paper studies the effects of four such variables by evaluating state-of-the-art visual localization methods as part of the motion planning module of an autonomous navigation stack. The results show major variation in the suitability of the different methods for vision-based navigation. To the authors' best knowledge, the proposed benchmark is the first to study modern visual localization methods as part of a complete navigation stack. We make the benchmark available at https://github.com/lasuomela/carla_vloc_benchmark."
Bent & Broken Bicycles: Leveraging Synthetic Data for Damaged Object Re-Identification,"Luca Piano, Filippo Gabriele Pratticò, Alessandro Sebastian Russo, Lorenzo Lanari, Lia Morra, Fabrizio Lamberti","Department of Control and Computer Engineering, Politecnico di Torino, Torino, Italy",100,Italy,0,,"Instance-level object re-identification is a fundamental computer vision task, with applications from image retrieval to intelligent monitoring and fraud detection. In this work, we propose the novel task of damaged object re-identification, which aims at distinguishing changes in visual appearance due to deformations or missing parts from subtle intra-class variations. To explore this task, we leverage the power of computer-generated imagery to create, in a semi-automatic fashion, high-quality synthetic images of the same bike before and after a damage occurs. The resulting dataset, Bent & Broken Bicycles (BBBicycles), contains 39,200 images and 2,800 unique bike instances spanning 20 different bike models. As a baseline for this task, we propose TransReI3D, a multi-task, transformer-based deep network unifying damage detection (framed as a multi-label classification task) with object re-identification.",https://openaccess.thecvf.com/content/WACV2023/html/Piano_Bent__Broken_Bicycles_Leveraging_Synthetic_Data_for_Damaged_Object_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Piano_Bent__Broken_Bicycles_Leveraging_Synthetic_Data_for_Damaged_Object_WACV_2023_paper.pdf,https://tinyurl.com/37tepf7m,,,main,Poster,https://ieeexplore.ieee.org/document/10030335/,"['Image segmentation', 'Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Image retrieval', 'Computer architecture', 'Bicycles']","['Object Re-identification', 'Synthetic Images', 'Image Retrieval', 'Fraud Detection', 'Missing Parts', 'Baseline Task', 'Multi-label Classification Task', 'Training Set', 'Deep Neural Network', 'Validation Set', 'Local Features', 'Training Time', 'Data Augmentation', 'Global Features', 'Stochastic Gradient Descent', 'Multiple Images', 'Domain Shift', 'Stress Test', 'Latent Space', 'Domain Adaptation', 'High Dynamic Range Image', 'Re-identification Task', 'Query Image', 'Vision Transformer', 'Synthetic Generation', 'Multi-task Learning', 'Gallery Images', '3D Mesh', 'Deep Learning']","['instance-level retrieval', 're-identification', 'synthetic data', 'damage detection', 'transformers']",4,"Instance-level object re-identification is a fundamental computer vision task, with applications from image retrieval to intelligent monitoring and fraud detection. In this work, we propose the novel task of damaged object re-identification, which aims at distinguishing changes in visual appearance due to deformations or missing parts from subtle intra-class variations. To explore this task, we leverage the power of computer-generated imagery to create, in a semi-automatic fashion, high-quality synthetic images of the same bike before and after a damage occurs. The resulting dataset, Bent & Broken Bicycles (BB-Bicycles), contains 39,200 images and 2,800 unique bike instances spanning 20 different bike models. As a baseline for this task, we propose TransReI3D, a multi-task, transformer-based deep network unifying damage detection (framed as a multi-label classification task) with object re-identification. The BBBicycles dataset is available at https://tinyurl.com/37tepf7m"
Beyond RGB: Scene-Property Synthesis With Neural Radiance Fields,"Mingtong Zhang, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Yu-Xiong Wang",Carnegie Mellon University; University of Illinois Urbana-Champaign,100,USA,0,,"Comprehensive 3D scene understanding, both geometrically and semantically, is important for real-world applications such as robot perception. Most of the existing work has focused on developing data-driven discriminative models for scene understanding. This paper provides a new approach to scene understanding, from a synthesis model perspective, by leveraging the recent progress on implicit scene representation and neural rendering. Building upon the great success of Neural Radiance Fields (NeRFs), we introduce Scene-Property Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic RGB images from novel viewpoints, but also render various accurate scene properties (e.g., appearance, geometry, and semantics). By doing so, we facilitate addressing a variety of scene understanding tasks under a unified framework, including semantic segmentation, surface normal estimation, reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be a powerful tool for bridging generative learning and discriminative learning, and thus be beneficial to the investigation of a wide range of interesting problems, such as studying task relationships within a synthesis paradigm, transferring knowledge to novel tasks, facilitating downstream discriminative tasks as ways of data augmentation, and serving as auto-labeller for data creation. Our code is available at https://github.com/zsh2000/SS-NeRF.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Beyond_RGB_Scene-Property_Synthesis_With_Neural_Radiance_Fields_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Beyond_RGB_Scene-Property_Synthesis_With_Neural_Radiance_Fields_WACV_2023_paper.pdf,,https://github.com/zsh2000/SS-NeRF,2206.04669,main,Poster,https://ieeexplore.ieee.org/document/10030817/,"['Geometry', 'Visualization', 'Three-dimensional displays', 'Computational modeling', 'Semantic segmentation', 'Semantics', 'Rendering (computer graphics)']","['Neural Radiance Fields', 'Data Augmentation', 'Semantic Segmentation', 'RGB Images', 'Model Discrimination', 'Edge Detection', 'Discriminative Learning', '3D Scene', 'Scene Understanding', 'Scene Representation', 'Keypoint Detection', 'Perception Of The Robot', 'Surface Normals', 'Photo-realistic Images', 'Scene Properties', 'Knowledge Transfer', 'Object Detection', 'Transfer Learning', 'Visual Task', 'Additional Properties', 'Implicit Representation', 'Semantic Representations', 'Multi-task Learning', 'Semantic Labels', 'View Direction', 'RGB Color', 'Geometric Representation', 'Camera Pose', 'Implicit Learning', 'Inductive Bias']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"Comprehensive 3D scene understanding, both geometrically and semantically, is important for real-world applications such as robot perception. Most of the existing work has focused on developing data-driven discriminative models for scene understanding. This paper provides a new approach to scene understanding, from a synthesis model perspective, by leveraging the recent progress on implicit scene representation and neural rendering. Building upon the great success of Neural Radiance Fields (NeRFs), we introduce Scene-Property Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic RGB images from novel viewpoints, but also render various accurate scene properties (e.g., appearance, geometry, and semantics). By doing so, we facilitate addressing a variety of scene understanding tasks under a unified framework, including semantic segmentation, surface normal estimation, reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be a powerful tool for bridging generative learning and discriminative learning, and thus be beneficial to the investigation of a wide range of interesting problems, such as studying task relationships within a synthesis paradigm, transferring knowledge to novel tasks, facilitating downstream discriminative tasks as ways of data augmentation, and serving as auto-labeller for data creation. Our code is available at https://github.com/zsh2000/SS-NeRF."
Bi-Directional Frame Interpolation for Unsupervised Video Anomaly Detection,"Hanqiu Deng, Zhaoxiang Zhang, Shihao Zou, Xingyu Li",University of Alberta,100,Canada,0,,"Anomaly detection in video surveillance aims to detect anomalous frames whose properties significantly differ from normal patterns. Anomalies in videos can occur in both spatial appearance and temporal motion, making unsupervised video anomaly detection challenging. To tackle this problem, we investigate forward and backward motion continuity between adjacent frames and propose a new video anomaly detection paradigm based on bi-directional frame interpolation. The proposed framework consists of an optical flow estimation network and an interpolation network jointly optimized end-to-end to synthesize a middle frame from its nearest two frames. We further introduce a novel dynamic memory mechanism to balance memory sparsity and normality representation diversity, which attenuates abnormal features in frame interpolation without affecting normal prototypes. In inference, interpolation error and dynamic memory error are fused as anomaly scores. The proposed bi-directional interpolation design improves normal frame synthesis, lowering the false alarm rate of anomaly appearance; meanwhile, the implicit ""regular"" motion constraint in our optical flow estimation and the novel dynamic memory mechanism play blocking roles in interpolating abnormal frames, increasing the system's sensitivity to anomalies. Extensive experiments on public benchmarks demonstrate the superiority of the proposed framework over prior arts.",https://openaccess.thecvf.com/content/WACV2023/html/Deng_Bi-Directional_Frame_Interpolation_for_Unsupervised_Video_Anomaly_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Deng_Bi-Directional_Frame_Interpolation_for_Unsupervised_Video_Anomaly_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030200/,"['Interpolation', 'Visualization', 'Dynamics', 'Video sequences', 'Estimation', 'Bidirectional control', 'Benchmark testing']","['Anomaly Detection', 'Frame Interpolation', 'Video Anomaly', 'Video Anomaly Detection', 'Dynamic Mechanism', 'Normal Pattern', 'Optical Flow', 'Flow Estimation', 'Video Surveillance', 'Memory Mechanisms', 'Adjacent Frames', 'Optical Networks', 'Optical Flow Estimation', 'Interpolation Error', 'Motion Constraints', 'Anomaly Score', 'Large Errors', 'Detailed Comparison', 'Recurrent Neural Network', 'Video Frames', 'Memory Module', 'Abnormal Events', 'Future Frames', 'Memory Pool', 'Anomalous Events', 'Temporal Continuity', 'Video Sequences', 'Motion Patterns', 'Abnormal Appearance', 'Memory Capacity']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",16,"Anomaly detection in video surveillance aims to detect anomalous frames whose properties significantly differ from normal patterns. Anomalies in videos can occur in both spatial appearance and temporal motion, making unsupervised video anomaly detection challenging. To tackle this problem, we investigate forward and backward motion continuity between adjacent frames and propose a new video anomaly detection paradigm based on bi-directional frame interpolation. The proposed framework consists of an optical flow estimation network and an interpolation network jointly optimized end-to-end to synthesize a middle frame from its nearest two frames. We further introduce a novel dynamic memory mechanism to balance memory sparsity and normality representation diversity, which attenuates abnormal features in frame interpolation without affecting normal prototypes. In inference, interpolation error and dynamic memory error are fused as anomaly scores. The proposed bi-directional interpolation design improves normal frame synthesis, lowering the false alarm rate of anomaly appearance; meanwhile, the implicit ""regular"" motion constraint in our optical flow estimation and the novel dynamic memory mechanism play blocking roles in interpolating abnormal frames, increasing the system&#x2019;s sensitivity to anomalies. Extensive experiments on public benchmarks demonstrates the superiority of the proposed framework over prior arts."
BirdSoundsDenoising: Deep Visual Audio Denoising for Bird Sounds,"Youshan Zhang, Jialu Li","Yeshiva University, NYC, NY; Cornell University, Ithaca, NY",100,USA,0,,"Audio denoising has been explored for decades using both traditional and deep learning-based methods. However, these methods are still limited to either manually added artificial noise or lower denoised audio quality. To overcome these challenges, we collect a large-scale natural noise bird sound dataset. We are the first to transfer the audio denoising problem into an image segmentation problem and propose a deep visual audio denoising (DVAD) model. With a total of 14,120 audio images, we develop an audio ImageMask tool and propose to use a few-shot generalization strategy to label these images. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance. We also show that our method can be easily generalized to speech denoising, audio separation, audio enhancement, and noise estimation.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_BirdSoundsDenoising_Deep_Visual_Audio_Denoising_for_Bird_Sounds_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_BirdSoundsDenoising_Deep_Visual_Audio_Denoising_for_Bird_Sounds_WACV_2023_paper.pdf,,,2210.10196,main,Poster,https://ieeexplore.ieee.org/document/10030171/,"['Learning systems', 'Visualization', 'Image segmentation', 'Computer vision', 'Computational modeling', 'Noise reduction', 'Estimation']","['Bird Sounds', 'Image Segmentation', 'Noise Estimation', 'Artificial Noise', 'Audio Quality', 'Natural Noise', 'Neural Network', 'Deep Learning', 'Convolutional Neural Network', 'Deep Neural Network', 'Clear Signal', 'Validation Dataset', 'Long Short-term Memory', 'Raw Images', 'Deep Learning Approaches', 'Segmentation Model', 'Hearing Aid', 'Noise Signal', 'Minimum Mean Square Error', 'Sound Detection', 'Clear Areas', 'Short-time Fourier Transform', 'Convolutional Recurrent Neural Network', 'Breath Sounds', 'Wiener Filter', 'Heart Sound', 'Window Function', 'Discrete Fourier Transform', 'Transformer', 'Recurrent Neural Network']",['Applications: Animals/Insects'],9,"Audio denoising has been explored for decades using both traditional and deep learning-based methods. However, these methods are still limited to either manually added artificial noise or lower denoised audio quality. To overcome these challenges, we collect a large-scale natural noise bird sound dataset. We are the first to transfer the audio denoising problem into an image segmentation problem and propose a deep visual audio denoising (DVAD) model. With a total of 14,120 audio images, we develop an audio ImageMask tool and propose to use a few-shot generalization strategy to label these images. Extensive experimental results demonstrate that the proposed model achieves state-of-the-art performance. We also show that our method can be easily generalized to speech denoising, audio separation, audio enhancement, and noise estimation."
Body Part-Based Representation Learning for Occluded Person Re-Identification,"Vladimir Somers, Christophe De Vleeschouwer, Alexandre Alahi","EPFL & UCLouvain & Sportradar; EPFL, Switzerland; UCLouvain, Belgium",100,"Belgium, Switzerland",0,,"Occluded person re-identification (ReID) is a person retrieval task which aims at matching occluded person images with holistic ones. For addressing occluded ReID, part-based methods have been shown beneficial as they offer fine-grained information and are well suited to represent partially visible human bodies. However, training a part-based model is a challenging task for two reasons. Firstly, individual body part appearance is not as discriminative as global appearance (two distinct IDs might have the same local appearance), this means standard ReID training objectives using identity labels are not adapted to local feature learning. Secondly, ReID datasets are not provided with human topographical annotations. In this work, we propose BPBreID, a body part-based ReID model for solving the above issues. We first design two modules for predicting body part attention maps and producing body part-based features of the ReID target. We then propose GiLt, a novel training scheme for learning part-based representations that is robust to occlusions and non-discriminative local appearance. Extensive experiments on popular holistic and occluded datasets show the effectiveness of our proposed method, which outperforms state-of-the-art methods by 0.7% mAP and 5.6% rank-1 accuracy on the challenging Occluded-Duke dataset. Our code is available at https://github.com/VlSomers/bpbreid.",https://openaccess.thecvf.com/content/WACV2023/html/Somers_Body_Part-Based_Representation_Learning_for_Occluded_Person_Re-Identification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Somers_Body_Part-Based_Representation_Learning_for_Occluded_Person_Re-Identification_WACV_2023_paper.pdf,,https://github.com/VlSomers/bpbreid,2211.03679,main,Poster,https://ieeexplore.ieee.org/document/10030944/,"['Training', 'Representation learning', 'Adaptation models', 'Computer vision', 'Codes', 'Computational modeling', 'Biological system modeling']","['Representation Learning', 'Parts-based Representation', 'Occluded Person', 'Occluded Person Re-identification', 'Body Parts', 'Local Features', 'Training Strategy', 'Attention Map', 'Person Image', 'Identity Labels', 'Feature Maps', 'Pairwise Distances', 'Attention Mechanism', 'Training Procedure', 'Discriminative Features', 'Visual Score', 'Pose Estimation', 'Appearance Features', 'Loss Of Identity', 'Triplet Loss', 'Re-identification Task', 'Human Labeling', 'Background Clutter', 'Feature Alignment', 'Parallel Branches', 'Visible Part']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Biometrics', 'face', 'gesture', 'body pose', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",65,"Occluded person re-identification (ReID) is a person retrieval task which aims at matching occluded person images with holistic ones. For addressing occluded ReID, part-based methods have been shown beneficial as they offer fine-grained information and are well suited to represent partially visible human bodies. However, training a part-based model is a challenging task for two reasons. Firstly, individual body part appearance is not as discriminative as global appearance (two distinct IDs might have the same local appearance), this means standard ReID training objectives using identity labels are not adapted to local feature learning. Secondly, ReID datasets are not provided with human topographical annotations. In this work, we propose BPBreID, a body part-based ReID model for solving the above issues. We first design two modules for predicting body part attention maps and producing body part-based features of the ReID target. We then propose GiLt, a novel training scheme for learning part-based representations that is robust to occlusions and non-discriminative local appearance. Extensive experiments on popular holistic and occluded datasets show the effectiveness of our proposed method, which outperforms state-of-the-art methods by 0.7% mAP and 5.6% rank-1 accuracy on the challenging Occluded-Duke dataset. Our code is available at https://github.com/VlSomers/bpbreid."
Boosting Neural Video Codecs by Exploiting Hierarchical Redundancy,"Reza Pourreza, Hoang Le, Amir Said, Guillaume Sautière, Auke Wiggers",Qualcomm AI Research,0,,100,USA,"In video compression, coding efficiency is improved by reusing pixels from previously decoded frames via motion and residual compensation. We define two levels of hierarchical redundancy in video frames: 1) first-order: redundancy in pixel space, i.e, similarities in pixel values across neighboring frames, which is effectively captured using motion and residual compensation, 2) second-order: redundancy in motion and residual maps due to smooth motion in natural videos. While most of the existing neural video coding literature addresses first-order redundancy, we tackle the problem of capturing second-order redundancy in neural video codecs via predictors. We introduce generic motion and residual predictors that learn to extrapolate from previously decoded data. These predictors are lightweight, and can be employed with most neural video codecs in order to improve their rate-distortion performance. Moreover, while RGB is the dominant colorspace in neural video coding literature, we introduce general modifications for neural video codecs to embrace the YUV420 colorspace and report YUV420 results. Our experiments show that using our predictors with a well-known neural video codec leads to 38% and 34% bitrate saving in RGB and YUV420 colorspaces measured on the UVG dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Pourreza_Boosting_Neural_Video_Codecs_by_Exploiting_Hierarchical_Redundancy_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pourreza_Boosting_Neural_Video_Codecs_by_Exploiting_Hierarchical_Redundancy_WACV_2023_paper.pdf,,,2208.04303,main,Poster,https://ieeexplore.ieee.org/document/10030298/,"['Video coding', 'Visualization', 'Bit rate', 'Termination of employment', 'Rate-distortion', 'Training data', 'Video compression']","['Video Codec', 'Decoding', 'Video Frames', 'Coding Efficiency', 'Motion Compensation', 'RGB Color Space', 'Video Compression', 'Motion Video', 'Autoencoder', 'Latent Space', 'Optical Flow', 'Consecutive Frames', 'Bitstream', 'Current Frame', 'Previous Frame', 'Least Significant Bit', 'Flow Prediction', 'Flow Map', 'Input Resolution', 'Motion Vector', 'Gradient Update', 'Input RGB', 'Set Of Videos']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",6,"In video compression, coding efficiency is improved by reusing pixels from previously decoded frames via motion and residual compensation. We define two levels of hierarchical redundancy in video frames: 1) first-order: redundancy in pixel space, i.e., similarities in pixel values across neighboring frames, which is effectively captured using motion and residual compensation, 2) second-order: redundancy in motion and residual maps due to smooth motion in natural videos. While most of the existing neural video coding literature addresses first-order redundancy, we tackle the problem of capturing second-order redundancy in neural video codecs via predictors. We introduce generic motion and residual predictors that learn to extrapolate from previously decoded data. These predictors are lightweight, and can be employed with most neural video codecs in order to improve their rate-distortion performance. Moreover, while RGB is the dominant colorspace in neural video coding literature, we introduce general modifications for neural video codecs to embrace the YUV420 colorspace and report YUV420 results. Our experiments show that using our predictors with a well-known neural video codec leads to 38% and 34% bitrate savings in RGB and YUV420 colorspaces measured on the UVG dataset."
Boosting Vision Transformers for Image Retrieval,"Chull Hwan Song, Jooyoung Yoon, Shunghyun Choi, Yannis Avrithis",Athena RC; Institute of Advanced Research on Artificial Intelligence (IARAI); Dealicious Inc.,33.33333333,Austria,66.66666667,Greece,"The explosive increase in vision transformers studies has shown remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image epresentation. (3) In each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation.",https://openaccess.thecvf.com/content/WACV2023/html/Song_Boosting_Vision_Transformers_for_Image_Retrieval_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Song_Boosting_Vision_Transformers_for_Image_Retrieval_WACV_2023_paper.pdf,,https://github.com/dealicious-inc/DToP,2210.11909,main,Poster,https://ieeexplore.ieee.org/document/10030228/,"['Training', 'Location awareness', 'Image retrieval', 'Computer architecture', 'Self-supervised learning', 'Image representation', 'Transformers']","['Transformer', 'Image Retrieval', 'Vision Transformer', 'Training Set', 'Convolutional Network', 'State Of The Art', 'Image Classification', 'Large Margin', 'Image Representation', 'Skip Connections', 'Global Representation', 'Hybrid Architecture', 'Transformer Encoder', 'Multilayer Feature', 'Deep Learning', 'Local Features', 'Object Detection', 'Spatial Dimensions', 'Global Features', 'Large-scale Datasets', 'Global Descriptors', 'Mean Average Precision', 'Token Embedding', 'Local Descriptors', 'Position Embedding', 'Inductive Bias', 'Set Of Literature', 'Open Dataset', 'Hybrid Model', 'Patch Features']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",18,"Vision transformers have achieved remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image representation. (3) In each branch, we collect multilayer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation. Public code is available at https://github.com/dealicious-inc/DToP."
Bootstrapping the Relationship Between Images and Their Clean and Noisy Labels,"Brandon Smart, Gustavo Carneiro","Australian Institute for Machine Learning, University of Adelaide, Australia; Centre for Vision, Speech and Signal Processing, University of Surrey, United Kingdom",100,"Australia, UK",0,,"Many state-of-the-art noisy-label learning methods rely on learning mechanisms that estimate the samples' clean labels during training and discard their original noisy labels. However, this approach prevents the learning of the relationship between images, noisy labels and clean labels, which has been shown to be useful when dealing with instance-dependent label noise problems. Furthermore, methods that do aim to learn this relationship require cleanly annotated subsets of data, as well as distillation or multi-faceted models for training. In this paper, we propose a new training algorithm that relies on a simple model to learn the relationship between clean and noisy labels without the need for a cleanly labelled subset of data. Our algorithm follows a 3-stage process, namely: 1) self-supervised pretraining followed by an early-stopping training of the classifier to confidently predict clean labels for a subset of the training set; 2) use the clean set from stage (1) to bootstrap the relationship between images, noisy labels and clean labels, which we exploit for effective relabelling of the remaining training set using semi-supervised learning; and 3) supervised training of the classifier with all relabelled samples from stage (2). By learning this relationship, we achieve state-of-the-art performance in asymmetric and instance-dependent label noise problems. Code is available at https://github.com/btsmart/bootstrapping-label-noise",https://openaccess.thecvf.com/content/WACV2023/html/Smart_Bootstrapping_the_Relationship_Between_Images_and_Their_Clean_and_Noisy_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Smart_Bootstrapping_the_Relationship_Between_Images_and_Their_Clean_and_Noisy_WACV_2023_paper.pdf,,https://github.com/btsmart/bootstrapping-label-noise,2210.08826,main,Poster,https://ieeexplore.ieee.org/document/10030919/,"['Training', 'Computer vision', 'Computational modeling', 'Computer architecture', 'Semisupervised learning', 'Prediction algorithms', 'Data models']","['Noisy Labels', 'Training Set', 'Subset Of Data', 'Classifier Training', 'Semi-supervised Learning', 'Clear Set', 'Label Noise', 'Image Features', 'Feature Space', 'Data Augmentation', 'Training Stage', 'Real-world Datasets', 'Truth Labels', 'Labeled Samples', 'Decision Boundary', 'Clean Samples', 'Prediction Confidence', 'Horizontal Flip', 'Error Set', 'Minimum Entropy', 'Samples In The Feature Space', 'Consistency Regularization', 'Random Cropping', 'Noise In The Dataset']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Many state-of-the-art noisy-label learning methods rely on learning mechanisms that estimate the samples’ clean labels during training and discard their original noisy labels. However, this approach prevents the learning of the relationship between images, noisy labels and clean labels, which has been shown to be useful when dealing with instance-dependent label noise problems. Further-more, methods that do aim to learn this relationship re-quire cleanly annotated subsets of data, as well as distillation or multi-faceted models for training. In this paper, we propose a new training algorithm that relies on a simple model to learn the relationship between clean and noisy labels without the need for a cleanly labelled subset of data. Our algorithm follows a 3-stage process, namely: 1) self-supervised pre-training followed by an early-stopping training of the classifier to confidently predict clean labels for a subset of the training set; 2) use the clean set from stage (1) to bootstrap the relationship between images, noisy labels and clean labels, which we exploit for effective relabelling of the remaining training set using semi-supervised learning; and 3) supervised training of the classifier with all relabelled samples from stage (2). By learning this relationship, we achieve state-of-the-art performance in asymmetric and instance-dependent label noise problems
<sup>1</sup>
. Code is available at https://github.com/btsmart/bootstrapping-label-noise."
BoxMask: Revisiting Bounding Box Supervision for Video Object Detection,"Khurram Azeem Hashmi, Alain Pagani, Didier Stricker, Muhammad Zeshan Afzal","DFKI - German Research Center for Artificial Intelligence, Kaiserslautern",100,Germany,0,,"We present a new, simple yet effective approach to uplift video object detection. We observe that prior works operate on instance-level feature aggregation that imminently neglects the refined pixel-level representation, resulting in confusion among objects sharing similar appearance or motion characteristics. To address this limitation, we pro- pose BoxMask, which effectively learns discriminative representations by incorporating class-aware pixel-level information. We simply consider bounding box-level annotations as a coarse mask for each object to supervise our method. The proposed module can be effortlessly integrated into any region-based detector to boost detection. Extensive experiments on ImageNet VID and EPIC KITCHENS datasets demonstrate consistent and significant improvement when we plug our BoxMask module into numerous recent state-of-the-art methods. The code will be available at https://github.com/khurramHashmi/BoxMask.",https://openaccess.thecvf.com/content/WACV2023/html/Hashmi_BoxMask_Revisiting_Bounding_Box_Supervision_for_Video_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hashmi_BoxMask_Revisiting_Bounding_Box_Supervision_for_Video_Object_Detection_WACV_2023_paper.pdf,,https://github.com/khurramHashmi/BoxMask,2210.06008,main,Poster,https://ieeexplore.ieee.org/document/10030892/,"['Location awareness', 'Computer vision', 'Upper bound', 'Codes', 'Annotations', 'Object detection', 'Detectors']","['Object Detection', 'Bounding Box', 'Video Object Detection', 'Box Supervision', 'Motion Features', 'Similar Appearance', 'Feature Aggregation', 'Similar Motion', 'Convolutional Layers', 'Feature Maps', 'Temporal Features', 'Class Labels', 'Temporal Information', 'Video Frames', 'Semantic Segmentation', 'Optical Flow', 'Backbone Network', 'Still Images', 'Instance Segmentation', 'Region Proposal Network', 'Target Frame', 'Support Frame', 'Object Proposals', 'Detection Head', 'Region-based Methods', 'Detection In Videos', 'Foreground Regions', 'Entire Video', 'Partial Occlusion', 'Detection Results']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",7,"We present a new, simple yet effective approach to up- lift video object detection. We observe that prior works operate on instance-level feature aggregation that imminently neglects the refined pixel-level representation, resulting in confusion among objects sharing similar appearance or motion characteristics. To address this limitation, we propose BoxMask, which effectively learns discriminative representations by incorporating class-aware pixel-level information. We simply consider bounding box-level annotations as a coarse mask for each object to supervise our method. The proposed module can be effortlessly integrated into any region-based detector to boost detection. Extensive experiments on ImageNet VID and EPIC KITCHENS datasets demonstrate consistent and significant improvement when we plug our BoxMask module into numerous recent state-of-the-art methods. The code will be available at https://github.com/khurramHashmi/BoxMask."
BrightFlow: Brightness-Change-Aware Unsupervised Learning of Optical Flow,"Rémi Marsal, Florian Chabot, Angélique Loesch, Hichem Sahbi","Sorbonne University, CNRS, LIP6 F-75005, Paris, France; Université Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France",100,France,0,,"Unsupervised optical flow estimation relies on the assumption that pixels characterizing the same observed object should exhibit a stable appearance across video frames. With this assumption, the long-standing principle behind flow estimation consists in optimizing a photometric loss that maximizes the similarity between paired pixels in successive frames. However, these frames could be subject to strong brightness changes due to the radiometric properties of scenes as well as their viewing conditions. In this paper, we present BrightFlow, a new method to train any optical flow estimation network in an unsupervised manner. It consists in training two networks that jointly estimate optical flow and brightness changes. These changes are then compensated in the photometric loss so that reconstruction errors due to shadows or reflections will not affect negatively the training. As this compensation mechanism is only used at training stage, our method does not impact the number of parameters or the complexity at inference. Extensive experiments conducted on standard datasets and optical flow architectures show a consistent gain of our method. Source code is available at https://github.com/CEA-LIST/BrightFlow.",https://openaccess.thecvf.com/content/WACV2023/html/Marsal_BrightFlow_Brightness-Change-Aware_Unsupervised_Learning_of_Optical_Flow_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Marsal_BrightFlow_Brightness-Change-Aware_Unsupervised_Learning_of_Optical_Flow_WACV_2023_paper.pdf,,https://github.com/CEA-LIST/BrightFlow,,main,Poster,https://ieeexplore.ieee.org/document/10030140/,"['Training', 'Optical losses', 'Source coding', 'Brightness', 'Estimation', 'Radiometry', 'Reflection']","['Optical Flow', 'Video Frames', 'Strong Changes', 'Standard Datasets', 'Flow Estimation', 'Successive Frames', 'Optical Networks', 'Brightness Changes', 'Optical Flow Estimation', 'Neural Network', 'Estimation Error', 'Deep Network', 'Deep Neural Network', 'Input Image', 'Data Augmentation', 'Unsupervised Methods', 'Backward Direction', 'Flow Prediction', 'Correct Mapping', 'Backward Flow', 'Cost Volume', 'Smoothness Loss', 'Video Collection']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"Unsupervised optical flow estimation relies on the assumption that pixels characterizing the same observed object should exhibit a stable appearance across video frames. With this assumption, the long-standing principle behind flow estimation consists in optimizing a photometric loss that maximizes the similarity between paired pixels in successive frames. However, these frames could be subject to strong brightness changes due to the radiometric properties of scenes as well as their viewing conditions.In this paper, we present BrightFlow, a new method to train any optical flow estimation network in an unsupervised manner. It consists in training two networks that jointly estimate optical flow and brightness changes. These changes are then compensated in the photometric loss so that reconstruction errors due to shadows or reflections will not affect negatively the training. As this compensation mechanism is only used at training stage, our method does not impact the number of parameters or the complexity at inference. Extensive experiments conducted on standard datasets and optical flow architectures show a consistent gain of our method. Source code is available at https://github.com/CEA-LIST/BrightFlow."
Burst Reflection Removal Using Reflection Motion Aggregation Cues,"B. H. Pawan Prasad, Green Rosh K. S., Lokesh R. B., Kaushik Mitra","Samsung R&D Institute Bangalore, India; IIT Madras, Chennai, India",100,India,0,,"Single image reflection removal has attracted lot of interest in the recent past with data driven approaches demonstrating significant improvements. However deep learning based approaches for multi-image reflection removal remains relatively less explored. The existing multi-image methods require input images to be captured at sufficiently different view points with wide baselines. This makes it cumbersome for the user who is required to capture the scene by moving the camera in multiple directions. A more convenient way is to capture a burst of images in a short time duration without providing any specific instructions to the user. A burst of images captured on a hand-held device provide crucial cues that rely on the subtle handshakes created during the capture process to separate the reflection and the transmission layers. In this paper, we propose a multi-stage deep learning based approach for burst reflection removal. In the first stage, we perform reflection suppression on the individual images. In the second stage, a novel reflection motion aggregation (RMA) cue is extracted that emphasizes the transmission layer more than the reflection layer to aid better layer separation. In our final stage we use this RMA cue as a guide to remove reflections from the input. We provide the first real world burst images dataset along with ground truth for reflection removal that can enable future benchmarking. We evaluate both qualitatively and quantitatively to demonstrate the superiority of the proposed approach. Our method achieves  2 dB improvement in PSNR over single image based methods and  1 dB over multi-image based methods.",https://openaccess.thecvf.com/content/WACV2023/html/Prasad_Burst_Reflection_Removal_Using_Reflection_Motion_Aggregation_Cues_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Prasad_Burst_Reflection_Removal_Using_Reflection_Motion_Aggregation_Cues_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030406/,"['Deep learning', 'Performance evaluation', 'Computer vision', 'Benchmark testing', 'Cameras', 'Reflection', 'Optimization']","['Reflection Removal', 'Deep Learning', 'Input Image', 'Single Image', 'Separate Layers', 'Single Reflection', 'Approach For Removal', 'Reflective Layer', 'Degrees Of Freedom', 'Large Datasets', 'Fine-tuned', 'Smartphone', 'Imaging Methods', 'State Of The Art', 'Quantitative Evaluation', 'Multiple Images', 'Reference Image', 'Video Sequences', 'High Dynamic Range', 'Differences In Motion', 'Online Optimization', 'Stages Of Aggregation', 'Perfect Alignment', 'Reflection Of Reality', 'Smartphone Camera', 'Small Misalignment', 'Motion Vector', 'Convolutional Block', 'Adam Optimizer']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"Single image reflection removal has attracted lot of interest in the recent past with data driven approaches demonstrating significant improvements. However deep learning based approaches for multi-image reflection removal remains relatively less explored. The existing multi-image methods require input images to be captured at sufficiently different view points with wide baselines. This makes it cumbersome for the user who is required to capture the scene by moving the camera in multiple directions. A more convenient way is to capture a burst of images in a short time duration without providing any specific instructions to the user. A burst of images captured on a hand-held device provide crucial cues that rely on the subtle handshakes created during the capture process to separate the reflection and the transmission layers. In this paper, we propose a multi-stage deep learning based approach for burst reflection removal. In the first stage, we perform reflection suppression on the individual images. In the second stage, a novel reflection motion aggregation (RMA) cue is extracted that emphasizes the transmission layer more than the reflection layer to aid better layer separation. In our final stage we use this RMA cue as a guide to remove reflections from the input. We provide the first real world burst images dataset along with ground truth for reflection removal that can enable future benchmarking. We evaluate both qualitatively and quantitatively to demonstrate the superiority of the proposed approach. Our method achieves ~ 2dB improvement in PSNR over single image based methods and ~ 1dB over multi-image based methods."
Burst Vision Using Single-Photon Cameras,"Sizhuo Ma, Paul Mos, Edoardo Charbon, Mohit Gupta","École Polytechnique Fédérale de Lausanne, Switzerland; University of Wisconsin-Madison, USA",100,"Switzerland, USA",0,,"Single-photon avalanche diodes (SPADs) are novel image sensors that record the arrival of individual photons at extremely high temporal resolution. In the past, they were only available as single pixels or small-format arrays, for various active imaging applications such as LiDAR and microscopy. Recently, high-resolution SPAD arrays up to 3.2 megapixel have been realized, which for the first time may be able to capture sufficient spatial details for general computer vision tasks, purely as a passive sensor. However, existing vision algorithms are not directly applicable on the binary data captured by SPADs. In this paper, we propose developing quanta vision algorithms based on burst processing for extracting scene information from SPAD photon streams. With extensive real-world data, we demonstrate that current SPAD arrays, along with burst processing as an example plug-and-play algorithm, are capable of a wide range of downstream vision tasks in extremely challenging imaging conditions including fast motion, low light (<5 lux) and high dynamic range. To our knowledge, this is the first attempt to demonstrate the capabilities of SPAD sensors for a wide gamut of real-world computer vision tasks including object detection, pose estimation, SLAM, and text recognition. We hope this work will inspire future research into developing computer vision algorithms for robust scene inference in extreme scenarios using single-photon cameras.",https://openaccess.thecvf.com/content/WACV2023/html/Ma_Burst_Vision_Using_Single-Photon_Cameras_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ma_Burst_Vision_Using_Single-Photon_Cameras_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030324/,"['Computer vision', 'Simultaneous localization and mapping', 'Text recognition', 'Heuristic algorithms', 'Streaming media', 'Cameras', 'Task analysis']","['Single-photon Cameras', 'Dynamic Range', 'Computer Vision', 'Object Detection', 'Low Light', 'Image Sensor', 'Vision Tasks', 'Range Of Tasks', 'Challenging Conditions', 'Pose Estimation', 'Single Pixel', 'High Dynamic Range', 'Optical Character Recognition', 'Wide Range Of Tasks', 'Passive Sensors', 'Sensory Capabilities', 'Computer Vision Algorithms', 'General Vision', 'Single-photon Avalanche Diode', 'Illumination', 'Conventional Camera', 'Integration Window', 'Multiple Frames', 'Passive Imaging', 'Bitstream', 'Time-to-digital Converter', 'Exposure Time', 'Clear Image', 'Frame Rate', 'Photon Flux']","['Algorithms: Low-level and physics-based vision', 'Computational photography', 'image and video synthesis']",8,"Single-photon avalanche diodes (SPADs) are novel image sensors that record the arrival of individual photons at extremely high temporal resolution. In the past, they were only available as single pixels or small-format arrays, for various active imaging applications such as LiDAR and microscopy. Recently, high-resolution SPAD arrays up to 3.2 megapixel have been realized, which for the first time may be able to capture sufficient spatial details for general computer vision tasks, purely as a passive sensor. However, existing vision algorithms are not directly applicable on the binary data captured by SPADs. In this paper, we propose developing quanta vision algorithms based on burst processing for extracting scene information from SPAD photon streams. With extensive real-world data, we demonstrate that current SPAD arrays, along with burst processing as an example plug-and-play algorithm, are capable of a wide range of downstream vision tasks in extremely challenging imaging conditions including fast motion, low light (< 5 lux) and high dynamic range. To our knowledge, this is the first attempt to demonstrate the capabilities of SPAD sensors for a wide gamut of real-world computer vision tasks including object detection, pose estimation, SLAM, and text recognition. We hope this work will inspire future research into developing computer vision algorithms in extreme scenarios using single-photon cameras."
CAST: Conditional Attribute Subsampling Toolkit for Fine-Grained Evaluation,"Wes Robbins, Steven Zhou, Aman Bhatta, Chad Mello, Vítor Albiero, Kevin W. Bowyer, Terrance E. Boult","University of Notre Dame; University of Colorado, Colorado Springs",100,USA,0,,"Thorough evaluation is critical for developing models that are fair and robust. In this work, we describe the Conditional Attribute Subsampling Toolkit (CAST) for selecting data subsets for fine-grained scientific evaluations. Our toolkit efficiently filters data given an arbitrary number of conditions for metadata attributes. The purpose of the toolkit is to allow researchers to easily to evaluate models on targeted test distributions. The functionality of CAST is demonstrated on the WebFace42M face Recognition dataset. We calculate over 50 attributes for this dataset including race, image quality, facial features, and accessories. Using our toolkit, we create over a hundred test sets conditioned on one or multiple attributes. Results are presented for subsets of various demographics and image quality ranges. Using eleven different subsets, we build a face recognition 1:1 verification benchmark called C11 that exclusively contains pairs that are near the decision threshold. Evaluation on C11 with state-of-the-art methods demonstrates the suitability of the proposed benchmark. The toolkit is publicly available at https://github.com/WesRobbins/CAST.",https://openaccess.thecvf.com/content/WACV2023/html/Robbins_CAST_Conditional_Attribute_Subsampling_Toolkit_for_Fine-Grained_Evaluation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Robbins_CAST_Conditional_Attribute_Subsampling_Toolkit_for_Fine-Grained_Evaluation_WACV_2023_paper.pdf,,https://github.com/WesRobbins/CAST,,main,Poster,https://ieeexplore.ieee.org/document/10030177/,"['Training', 'Image quality', 'Deep learning', 'Computer vision', 'Face recognition', 'Computational modeling', 'Benchmark testing']","['Fine-grained Evaluation', 'Image Quality', 'Face Recognition', 'Facial Features', 'Decision Threshold', 'Training Set', 'Deep Learning', 'Validation Set', 'Low Range', 'Stochastic Gradient Descent', 'Benchmark Datasets', 'Middle Eastern', 'Rate Set', 'Demographic Groups', 'Probability Rate', 'Dataset In Order', 'Quartile Range', 'Percentile Range', 'ResNet-50 Model', 'Face Recognition Performance', 'Previous Benchmark', 'Image Quality Metrics', 'Categorical Attributes', 'Backbone Architecture', 'List Of Files', 'Softmax', 'Disjoint Sets', 'Deep Learning Models', 'Face Recognition Model']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Thorough evaluation is critical for developing models that are fair and robust. In this work, we describe the Conditional Attribute Subsampling Toolkit (CAST) for selecting data subsets for fine-grained scientific evaluations. Our toolkit efficiently filters data given an arbitrary number of conditions for metadata attributes. The purpose of the toolkit is to allow researchers to easily to evaluate models on targeted test distributions. The functionality of CAST is demonstrated on the WebFace42M face Recognition dataset. We calculate over 50 attributes for this dataset including race, image quality, facial features, and accessories. Using our toolkit, we create over a hundred test sets conditioned on one or multiple attributes. Results are presented for subsets of various demographics and image quality ranges. Using eleven different subsets, we build a face recognition 1:1 verification benchmark called C11 that exclusively contains pairs that are near the decision threshold. Evaluation on C11 with state-of-the-art methods demonstrates the suitability of the proposed benchmark. The toolkit is publicly available at https://github.com/WesRobbins/CAST."
CFL-Net: Image Forgery Localization Using Contrastive Learning,"Fahim Faisal Niloy, Kishor Kumar Bhaumik, Simon S. Woo","Computer Science and Engineering Department, Sungkyunkwan University, Suwon, South Korea; Center for Computational & Data Sciences, Independent University, Bangladesh",100,"Bangladesh, South Korea",0,,"Conventional forgery localizing methods usually rely on different forgery footprints such as JPEG artifacts, edge inconsistency, camera noise, etc., with cross-entropy loss to locate manipulated regions. However, these methods have the disadvantage of over-fitting and focusing on only a few specific forgery footprints. On the other hand, real-life manipulated images are generated via a wide variety of forgery operations and thus, leave behind a wide variety of forgery footprints. Therefore, we need a more general approach for image forgery localization that can work well on a variety of forgery conditions. A key assumption in underlying forged region localization is that there remains a difference of feature distribution between untampered and manipulated regions in each forged image sample, irrespective of the forgery type. In this paper, we aim to leverage this difference of feature distribution to aid in image forgery localization. Specifically, we use contrastive loss to learn mapping into a feature space where the features between untampered and manipulated regions are well-separated for each image. Also, our method has the advantage of localizing manipulated region without requiring any prior knowledge or assumption about the forgery type. We demonstrate that our work outperforms several existing methods on three benchmark image manipulation datasets. Code is available at https://github.com/niloy193/CFLNet.",https://openaccess.thecvf.com/content/WACV2023/html/Niloy_CFL-Net_Image_Forgery_Localization_Using_Contrastive_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Niloy_CFL-Net_Image_Forgery_Localization_Using_Contrastive_Learning_WACV_2023_paper.pdf,,https://github.com/niloy193/CFLNet,,main,Poster,https://ieeexplore.ieee.org/document/10030939/,"['Location awareness', 'Measurement', 'Fuses', 'Image edge detection', 'Transform coding', 'Focusing', 'Benchmark testing']","['Self-supervised Learning', 'Image Forgery', 'Forgery Localization', 'Footprint', 'Feature Space', 'Cross-entropy Loss', 'Contrastive Loss', 'Feature Maps', 'Unsupervised Learning', 'High-pass Filter', 'RGB Images', 'Area Under Curve', 'Segmentation Task', 'Color In Fig', 'Rest Of The Dataset', 'Real-life Datasets', 'JPEG Compression']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Social good']",19,"Conventional forgery localizing methods usually rely on different forgery footprints such as JPEG artifacts, edge inconsistency, camera noise, etc., with cross-entropy loss to locate manipulated regions. However, these methods have the disadvantage of over-fitting and focusing on only a few specific forgery footprints. On the other hand, real-life manipulated images are generated via a wide variety of forgery operations and thus, leave behind a wide variety of forgery footprints. Therefore, we need a more general approach for image forgery localization that can work well on a variety of forgery conditions. A key assumption in underlying forged region localization is that there remains a difference of feature distribution between untampered and manipulated regions in each forged image sample, irrespective of the forgery type. In this paper, we aim to leverage this difference of feature distribution to aid in image forgery localization. Specifically, we use contrastive loss to learn mapping into a feature space where the features between un-tampered and manipulated regions are well-separated for each image. Also, our method has the advantage of localizing manipulated region without requiring any prior knowledge or assumption about the forgery type. We demonstrate that our work outperforms several existing methods on three benchmark image manipulation datasets. Code is available at https://github.com/niloy193/CFLNet"
CG-NeRF: Conditional Generative Neural Radiance Fields for 3D-Aware Image Synthesis,"Kyungmin Jo, Gyumin Shim, Sanghun Jung, Soyoung Yang, Jaegul Choo",Korea Advanced Institute of Science and Technology (KAIST),100,South Korea,0,,"Recent generative models based on neural radiance fields (NeRF) achieve the generation of diverse 3D-aware images. Despite the success, their applicability can be further expanded by incorporating with various types of user-specified conditions such as text and images. In this paper, we propose a novel approach called the conditional generative neural radiance fields (CG-NeRF), which generates multi-view images that reflect multimodal input conditions such as images or text. However, generating 3D-aware images from multimodal conditions bears several challenges. First, each condition type has different amount of information - e.g., the amount of information in text and color images are significantly different. Furthermore, the pose-consistency is often violated when diversifying the generated images from input conditions. Addressing such challenges, we propose 1) a unified architecture that effectively handles multiple types of conditions, and 2) the pose-consistent diversity loss for generating various images while maintaining the view consistency. Experimental results show that the proposed method maintains consistent image quality on various multimodal condition types and achieves superior fidelity and diversity compared to the existing NeRF-based generative models.",https://openaccess.thecvf.com/content/WACV2023/html/Jo_CG-NeRF_Conditional_Generative_Neural_Radiance_Fields_for_3D-Aware_Image_Synthesis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jo_CG-NeRF_Conditional_Generative_Neural_Radiance_Fields_for_3D-Aware_Image_Synthesis_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030346/,"['Measurement', 'Image quality', 'Computer vision', 'Codes', 'Shape', 'Image synthesis', 'Computer architecture']","['Radiance Field', 'Neural Radiance Fields', 'Amount Of Information', 'Typical Conditions', 'Color Images', 'Input State', 'Diverse Images', 'Multi-view Images', 'Multimodal Input', 'Neural Field', 'Unified Architecture', 'Convolutional Neural Network', 'Quantitative Comparison', 'Global Features', 'Precision And Recall', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Visual Quality', '3D Coordinates', 'Output Image', 'Camera Pose', 'Fréchet Inception Distance', 'Pose Estimation', 'Vector C', 'View Synthesis', 'Fake Images', 'Global Vector', 'Gradient Penalty', 'Low-resolution Images', 'Matching Condition']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision']",3,"Recent generative models based on neural radiance fields (NeRF) achieve the generation of diverse 3D-aware images. Despite the success, their applicability can be further expanded by incorporating with various types of user-specified conditions such as text and images. In this paper, we propose a novel approach called the conditional generative neural radiance fields (CG-NeRF), which generates multi-view images that reflect multimodal input conditions such as images or text. However, generating 3D-aware images from multimodal conditions bears several challenges. First, each condition type has different amount of information - e.g., the amount of information in text and color images are significantly different. Furthermore, the pose-consistency is often violated when diversifying the generated images from input conditions. Addressing such challenges, we propose 1) a unified architecture that effectively handles multiple types of conditions, and 2) the pose-consistent diversity loss for generating various images while maintaining the view consistency. Experimental results show that the proposed method maintains consistent image quality on various multimodal condition types and achieves superior fidelity and diversity compared to the existing NeRF-based generative models."
CNN2Graph: Building Graphs for Image Classification,"Vivek Trivedy, Longin Jan Latecki","Department of Computer and Information Sciences, Temple University, Philadelphia, USA",100,USA,0,,"Neural Network classifiers generally operate via the i.i.d. assumption where examples are passed through independently during training. We propose CNN2GNN and CNN2Transformer which instead leverage inter-example information for classification. We use Graph Neural Networks (GNNs) to build a latent space bipartite graph and compute cross-attention scores between input images and a proxy set. Our approach addresses several challenges of existing methods. Firstly, it is end-to-end differentiable despite the generally discrete nature of graph construction. Secondly, it allows inductive inference at no extra cost. Thirdly, it presents a simple method to construct graphs from arbitrary datasets that captures both example level and class level information. Finally, it addresses the proxy collapse problem by combining contrastive and cross-entropy losses rather than separate clustering algorithms. Our results increase classification performance over baseline experiments and outperform other methods. We also conduct an empirical investigation showing that Transformer style attention scales better than GAT attention with dataset size.",https://openaccess.thecvf.com/content/WACV2023/html/Trivedy_CNN2Graph_Building_Graphs_for_Image_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Trivedy_CNN2Graph_Building_Graphs_for_Image_Classification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030253/,"['Training', 'Representation learning', 'Costs', 'Transformers', 'Inference algorithms', 'Graph neural networks', 'Data models']","['Image Classification', 'Neural Network', 'Transformer', 'Input Image', 'Cross-entropy Loss', 'Latent Space', 'Inductive Reasoning', 'Graph Neural Networks', 'Graph Construction', 'Contrastive Loss', 'Graph Attention Network', 'Training Set', 'Support Vector Machine', 'K-nearest Neighbor', 'Classification Datasets', 'Graph Structure', 'Training Examples', 'Graph Convolutional Network', 'Kernel Methods', 'Attention Weights', 'Triplet Loss', 'Global Representation', 'Complete Bipartite Graph', 'Arbitrary Data', 'CNN Backbone', 'Vision Transformer', 'Learnable Weight Matrix', 'Aggregation Function', 'Margin Parameter', 'Forward Pass']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"Neural Network classifiers generally operate via the i.i.d. assumption where examples are passed through independently during training. We propose CNN2GNN and CNN2Transformer which instead leverage inter-example information for classification. We use Graph Neural Networks (GNNs) to build a latent space bipartite graph and compute cross-attention scores between input images and a proxy set. Our approach addresses several challenges of existing methods. Firstly, it is end-to-end differentiable despite the generally discrete nature of graph construction. Secondly, it allows inductive inference at no extra cost. Thirdly, it presents a simple method to construct graphs from arbitrary datasets that captures both example level and class level information. Finally, it addresses the proxy collapse problem by combining contrastive and cross-entropy losses rather than separate clustering algorithms. Our results increase classification performance over baseline experiments and outperform other methods. We also conduct an empirical investigation showing that Transformer style attention scales better than GAT attention with dataset size."
COPE: End-to-End Trainable Constant Runtime Object Pose Estimation,"Stefan Thalhammer, Timothy Patten, Markus Vincze","Robotics Institute, UTS; Automation and Control Institute, TU Vienna",100,"Australia, Austria",0,,"State-of-the-art object pose estimation handles multiple instances in a test image by using multi-model formulations: detection as a first stage and then separately trained networks per object for 2D-3D geometric correspondence prediction as a second stage. Poses are subsequently estimated using the Perspective-n-Points algorithm at runtime. Unfortunately, multi-model formulations are slow and do not scale well with the number of object instances involved. Recent approaches show that direct 6D object pose estimation is feasible when derived from the aforementioned geometric correspondences. We present an approach that learns an intermediate geometric representation of multiple objects to directly regress 6D poses of all instances in a test image. The inherent end-to-end trainability overcomes the requirement of separately processing individual object instances. By calculating the mutual Intersection-over-Unions, pose hypotheses are clustered into distinct instances, which achieves negligible runtime overhead with respect to the number of object instances. Results on multiple challenging standard datasets show that the pose estimation performance is superior to single-model state-of-the-art approaches despite being more than  35 times faster. We additionally provide an analysis showing real-time applicability (>24 fps) for images where more than 90 object instances are present. Further results show the advantage of supervising geometric correspondence-based object pose estimation with the 6D pose.",https://openaccess.thecvf.com/content/WACV2023/html/Thalhammer_COPE_End-to-End_Trainable_Constant_Runtime_Object_Pose_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Thalhammer_COPE_End-to-End_Trainable_Constant_Runtime_Object_Pose_Estimation_WACV_2023_paper.pdf,,,2208.08807,main,Poster,https://ieeexplore.ieee.org/document/10030977/,"['Computer vision', 'Runtime', 'Pose estimation', 'Prediction algorithms', 'Real-time systems', 'Task analysis', 'Standards']","['Pose Estimation', 'Human Pose Estimation', 'Constant Runtime', 'Recent Approaches', 'Direct Estimates', 'Challenging Dataset', 'Intermediate Representation', 'Object Instances', 'Direct Regression', 'Standardised', 'Training Data', 'Hyperparameters', 'Feature Maps', 'Single Image', 'Object Detection', 'Bounding Box', 'Object Classification', 'Local Image', 'Image Space', 'True Location', 'Single Input Image', 'Negligible Increase', 'Pyramid Level', 'Multi-scale Feature Maps', 'Feature Pyramid', 'Resolution Of The Feature Map', 'Target Training', 'Coordinate Frame', 'CPU Intel']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",7,"State-of-the-art object pose estimation handles multiple instances in a test image by using multi-model formulations: detection as a first stage and then separately trained networks per object for 2D-3D geometric correspondence prediction as a second stage. Poses are subsequently estimated using the Perspective-n-Points algorithm at runtime. Unfortunately, multi-model formulations are slow and do not scale well with the number of object instances involved. Recent approaches show that direct 6D object pose estimation is feasible when derived from the aforementioned geometric correspondences. We present an approach that learns an intermediate geometric representation of multiple objects to directly regress 6D poses of all instances in a test image. The inherent end-to-end trainability overcomes the requirement of separately processing individual object instances. By calculating the mutual Intersection-over-Unions, pose hypotheses are clustered into distinct instances, which achieves negligible runtime overhead with respect to the number of object instances. Results on multiple challenging standard datasets show that the pose estimation performance is superior to single-model state-of-the-art approaches despite being more than ~35 times faster. We additionally provide an analysis showing real-time applicability (> 24 fps) for images where more than 90 object instances are present. Further results show the advantage of supervising geometric correspondence-based object pose estimation with the 6D pose."
CORL: Compositional Representation Learning for Few-Shot Classification,"Ju He, Adam Kortylewski, Alan Yuille","Johns Hopkins University, Max Planck Institute for Informatics, University of Freiburg; Johns Hopkins University",100,"Germany, USA",0,,"Few-shot image classification consists of two consecutive learning processes: 1) In the meta-learning stage, the model acquires a knowledge base from a set of training classes. 2) During meta-testing, the acquired knowledge is used to recognize unseen classes from very few examples. Inspired by the compositional representation of objects in humans, we train a neural network architecture that explicitly represents objects as a dictionary of shared components and their spatial composition. In particular, during meta-learning, we train a knowledge base that consists of a dictionary of component representations and a dictionary of component activation maps that encode common spatial activation patterns of components. The elements of both dictionaries are shared among the training classes. During meta-testing, the representation of unseen classes is learned using the component representations and the component activation maps from the knowledge base. Finally, an attention mechanism is used to strengthen those components that are most important for each category. We demonstrate the value of our compositional learning framework for a few-shot classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where we achieve comparable performance.",https://openaccess.thecvf.com/content/WACV2023/html/He_CORL_Compositional_Representation_Learning_for_Few-Shot_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/He_CORL_Compositional_Representation_Learning_for_Few-Shot_Classification_WACV_2023_paper.pdf,,,2101.11878,main,Poster,https://ieeexplore.ieee.org/document/10030297/,"['Training', 'Representation learning', 'Computer vision', 'Dictionaries', 'Image recognition', 'Knowledge based systems', 'Neural networks']","['Representation Learning', 'Representation Of Composition', 'Few-shot Classification', 'Neural Network', 'Spatial Patterns', 'Knowledge Base', 'Image Classification', 'Attention Mechanism', 'Common Activities', 'Set Of Classes', 'Shared Component', 'Human Object', 'Consecutive Processes', 'Spatial Patterns Of Activity', 'Spatial Composition', 'Common Spatial Pattern', 'Unseen Classes', 'Training Data', 'Deep Neural Network', 'Output Layer', 'Deep Convolutional Neural Network', 'Composite Model', 'Feature Maps', 'Clustering Loss', 'Few-shot Learning', 'Training Images', 'Stochastic Gradient Descent', 'Base Classes', 'Image Pattern']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",6,"Few-shot image classification consists of two consecutive learning processes: 1) In the meta-learning stage, the model acquires a knowledge base from a set of training classes. 2) During meta-testing, the acquired knowledge is used to recognize unseen classes from very few examples. Inspired by the compositional representation of objects in humans, we train a neural network architecture that explicitly represents objects as a dictionary of shared components and their spatial composition. In particular, during meta-learning, we train a knowledge base that consists of a dictionary of component representations and a dictionary of component activation maps that encode common spatial activation patterns of components. The elements of both dictionaries are shared among the training classes. During meta-testing, the representation of unseen classes is learned using the component representations and the component activation maps from the knowledge base. Finally, an attention mechanism is used to strengthen those components that are most important for each category. We demonstrate the value of our interpretable compositional learning framework for a few-shot classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where we achieve comparable performance."
CRT-6D: Fast 6D Object Pose Estimation With Cascaded Refinement Transformers,"Pedro Castro, Tae-Kyun Kim","Imperial College London, KAIST; Imperial College London",100,"South Korea, UK",0,,"Learning based 6D object pose estimation methods rely on computing large intermediate pose representations and/or iteratively refining an initial estimation with a slow render-compare pipeline. This paper introduces a novel method we call Cascaded Pose Refinement Transformers, or CRT-6D. We replace the commonly used dense intermediate representation with a sparse set of features sampled from the feature pyramid we call OSKFs(Object Surface Keypoint Features) where each element corresponds to an object keypoint. We employ lightweight deformable transformers and chain them together to iteratively refine proposed poses over the sampled OSKFs. We achieve inference runtimes 2x faster than the closest real-time state of the art methods while supporting up to 21 objects on a single model. We demonstrate the effectiveness of CRT-6D by performing extensive experiments on the LM-O and YCBV datasets. Compared to real-time methods, we achieve state of the art on LM-O and YCB-V, falling slightly behind methods with inference runtimes one order of magnitude higher. The source code is available at: https://github.com/PedroCastro/CRT-6D",https://openaccess.thecvf.com/content/WACV2023/html/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.pdf,,https://github.com/PedroCastro/CRT-6D,,main,Poster,https://ieeexplore.ieee.org/document/10030886/,"['Computer vision', 'Runtime', 'Source coding', 'Pose estimation', 'Refining', 'Pipelines', 'Transformers']","['Transformer', 'Pose Estimation', 'Human Pose Estimation', 'Object Pose', '6D Object Pose', '6D Object Pose Estimation', 'State Of The Art', 'Real-time Method', 'Iterative Refinement', 'Feature Pyramid', 'Intermediate Representation', 'Pose Estimation Methods', 'Convolutional Neural Network', 'Object Detection', 'Attention Mechanism', 'Bounding Box', 'Target Object', 'Inference Time', 'Binding Pose', 'Refiner', 'Prior Methods', 'Iterative Closest Point', 'Prior Art', 'Refinement Step', 'Refinement Method', '3D Bounding Box', 'Camera Pose Estimation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', '3D computer vision']",23,"Learning based 6D object pose estimation methods rely on computing large intermediate pose representations and/or iteratively refining an initial estimation with a slow render-compare pipeline. This paper introduces a novel method we call Cascaded Pose Refinement Transformers, or CRT-6D. We replace the commonly used dense intermediate representation with a sparse set of features sampled from the feature pyramid we call OSKFs(Object Surface Keypoint Features) where each element corresponds to an object keypoint. We employ lightweight deformable transformers and chain them together to iteratively refine proposed poses over the sampled OSKFs. We achieve inference runtimes 2× faster than the closest real-time state of the art methods while supporting up to 21 objects on a single model. We demonstrate the effectiveness of CRT-6D by performing extensive experiments on the LM-O and YCBV datasets. Compared to real-time methods, we achieve state of the art on LM-O and YCB-V, falling slightly behind methods with inference runtimes one order of magnitude higher. The source code is available at: https://github.com/PedroCastro/CRT-6D"
CTrGAN: Cycle Transformers GAN for Gait Transfer,"Shahar Mahpod, Noam Gaash, Hay Hoffman, Gil Ben-Artzi","Ariel University, Israel",100,Israel,0,,"We introduce a novel approach for gait transfer from unconstrained videos in-the-wild. In contrast to motion transfer, the objective here is not to imitate the source's motions by the target, but rather to replace the walking source with the target, while transferring the target's typical gait. Our approach can be trained only once with multiple sources and is able to transfer the gait of the target from unseen sources, eliminating the need for retraining for each new source independently. Furthermore, we propose a novel metrics for gait transfer based on gait recognition models that enable to quantify the quality of the transferred gait, and show that existing techniques yield a discrepancy that can be easily detected.\nWe introduce Cycle Transformers GAN (CTrGAN), that consist of a decoder and encoder, both Transformers, where the attention is on the temporal domain between complete images rather than the spatial domain between patches. Using a widely-used gait recognition dataset, we demonstrate that our approach is capable of producing over an order of magnitude more realistic personalized gaits than existing methods, even when used with sources that were not available during training. As part of our solution, we present a detector that determines whether a video is real or generated by our model.",https://openaccess.thecvf.com/content/WACV2023/html/Mahpod_CTrGAN_Cycle_Transformers_GAN_for_Gait_Transfer_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mahpod_CTrGAN_Cycle_Transformers_GAN_for_Gait_Transfer_WACV_2023_paper.pdf,http://gil-ba.com,,2206.15248,main,Poster,https://ieeexplore.ieee.org/document/10030796/,"['Measurement', 'Training', 'Legged locomotion', 'Computer vision', 'Detectors', 'Computer architecture', 'Transformers']","['Temporal Domain', 'Loss Function', 'Training Set', 'Series Of Images', 'Intersection Over Union', 'Transformer Model', 'Image Collection', 'Video Sequences', 'Gait Pattern', 'Gait Cycle', 'Ground Truth Image', 'Feature Encoder', 'Subjects In Dataset', 'Person Walking', 'Fréchet Inception Distance', 'Alpha Matte', 'Target Pose', 'Appearance Quality', 'Source Motion']","['Algorithms: Computational photography', 'image and video synthesis', 'Biometrics', 'face', 'gesture', 'body pose']",1,"We introduce a novel approach for gait transfer from unconstrained videos in-the-wild. In contrast to motion transfer, the objective here is not to imitate the source’s motions by the target, but rather to replace the walking source with the target, while transferring the target’s typical gait. Our approach can be trained only once with multiple sources and is able to transfer the gait of the target from unseen sources, eliminating the need for retraining for each new source independently. Furthermore, we propose a novel metrics for gait transfer based on gait recognition models that enable to quantify the quality of the transferred gait, and show that existing techniques yield a discrepancy that can be easily detected.We introduce Cycle Transformers GAN (CTrGAN), that consist of a decoder and encoder, both Transformers, where the attention is on the temporal domain between complete images rather than the spatial domain between patches. Using a widely-used gait recognition dataset, we demonstrate that our approach is capable of producing over an order of magnitude more realistic personalized gaits than existing methods, even when used with sources that were not available during training. As part of our solution, we present a detector that determines whether a video is real or generated by our model."
CUDA-GHR: Controllable Unsupervised Domain Adaptation for Gaze and Head Redirection,"Swati Jindal, Xin Eric Wang","University of California, Santa Cruz",100,USA,0,,"The robustness of gaze and head pose estimation models is highly dependent on the amount of labeled data. Recently, generative modeling has shown excellent results in generating photo-realistic images, which can alleviate the need for annotations. However, adopting such generative models to new domains while maintaining their ability to provide fine-grained control over different image attributes, e.g., gaze and head pose directions, has been a challenging problem. This paper proposes CUDA-GHR, an unsupervised domain adaptation framework that enables fine-grained control over gaze and head pose directions while preserving the appearance-related factors of the person. Our framework simultaneously learns to adapt to new domains and disentangle visual attributes such as appearance, gaze direction, and head orientation by utilizing a label-rich source domain and an unlabeled target domain. Extensive experiments on the benchmarking datasets show that the proposed method can outperform state-of-the-art techniques on both quantitative and qualitative evaluations. Furthermore, we demonstrate the effectiveness of generated image-label pairs in the target domain for pretraining networks for the downstream task of gaze and head pose estimation. The source code and pre-trained models are available at https://github.com/jswati31/cuda-ghr.",https://openaccess.thecvf.com/content/WACV2023/html/Jindal_CUDA-GHR_Controllable_Unsupervised_Domain_Adaptation_for_Gaze_and_Head_Redirection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jindal_CUDA-GHR_Controllable_Unsupervised_Domain_Adaptation_for_Gaze_and_Head_Redirection_WACV_2023_paper.pdf,,https://github.com/jswati31/cuda-ghr,,main,Poster,https://ieeexplore.ieee.org/document/10030912/,"['Adaptation models', 'Visualization', 'Head', 'Annotations', 'Source coding', 'Pose estimation', 'Motion pictures']","['Domain Adaptation', 'Quantitative Evaluation', 'Labeled Data', 'Target Domain', 'Pre-trained Network', 'Visual Properties', 'Source Domain', 'Gaze Direction', 'Head Orientation', 'Head Pose', 'Unlabeled Target Domain', 'Need For Annotation', 'Paired Samples', 'Input Image', 'Latent Factors', 'Generative Adversarial Networks', 'Unsupervised Methods', 'Latent Space', 'Output Image', 'Label Information', 'Generative Adversarial Networks Loss', 'Yaw Angle', 'Consistency Loss', 'Pseudo Labels', 'Pitch Angle', 'Reconstruction Loss', 'Task Network', 'Perceptual Similarity', 'Disentangled Representation', 'Perceptual Loss']","['Applications: Smartphones/end user devices', 'Biometrics', 'face', 'gesture', 'body pose']",3,"The robustness of gaze and head pose estimation models is highly dependent on the amount of labeled data. Recently, generative modeling has shown excellent results in generating photo-realistic images, which can alleviate the need for annotations. However, adopting such generative models to new domains while maintaining their ability to provide fine-grained control over different image attributes, e.g., gaze and head pose directions, has been a challenging problem. This paper proposes CUDA-GHR, an unsupervised domain adaptation framework that enables fine-grained control over gaze and head pose directions while preserving the appearance-related factors of the person. Our framework simultaneously learns to adapt to new domains and disentangle visual attributes such as appearance, gaze direction, and head orientation by utilizing a label-rich source domain and an unlabeled target domain. Extensive experiments on the benchmarking datasets show that the proposed method can outperform state-of-the-art techniques on both quantitative and qualitative evaluations. Furthermore, we demonstrate the effectiveness of generated image-label pairs in the target domain for pretraining networks for the downstream task of gaze and head pose estimation. The source code and pre-trained models are available at https://github.com/jswati31/cuda-ghr."
CYBORG: Blending Human Saliency Into the Loss Improves Deep Learning-Based Synthetic Face Detection,"Aidan Boyd, Patrick Tinsley, Kevin W. Bowyer, Adam Czajka","University of Notre Dame, Notre Dame IN 46556, USA",100,USA,0,,"Can deep learning models achieve greater generalization if their training is guided by reference to human perceptual abilities? And how can we implement this in a practical manner? This paper proposes a training strategy to ConveY Brain Oversight to Raise Generalization (CYBORG). This new approach incorporates human-annotated saliency maps into a loss function that guides the model's learning to focus on image regions that humans deem salient for the task. The Class Activation Mapping (CAM) mechanism is used to probe the model's current saliency in each training batch, juxtapose this model saliency with human saliency, and penalize large differences. Results on the task of synthetic face detection, selected to illustrate the effectiveness of the approach, show that CYBORG leads to significant improvement in accuracy on unseen samples consisting of face images generated from six Generative Adversarial Networks across multiple classification network architectures. We also show that scaling to even seven times the training data, or using non-human-saliency auxiliary information, such as segmentation masks, and standard loss cannot beat the performance of CYBORG-trained models. As a side effect of this work, we observe that the addition of explicit region annotation to the task of synthetic face detection increased human classification accuracy. This work opens a new area of research on how to incorporate human visual saliency into loss functions in practice. All data, code and pre-trained models used in this work are offered with this paper.",https://openaccess.thecvf.com/content/WACV2023/html/Boyd_CYBORG_Blending_Human_Saliency_Into_the_Loss_Improves_Deep_Learning-Based_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Boyd_CYBORG_Blending_Human_Saliency_Into_the_Loss_Improves_Deep_Learning-Based_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030287/,"['Training', 'Visualization', 'Training data', 'Brain modeling', 'Data models', 'Face detection', 'Task analysis']","['Synthetic Faces', 'Human Saliency', 'Loss Function', 'Training Data', 'Deep Learning', 'Image Regions', 'Training Strategy', 'Detection Task', 'Generative Adversarial Networks', 'Image Generation', 'Face Images', 'Saliency Map', 'Training Batch', 'Class Activation Maps', 'Training Set', 'Test Data', 'Validation Set', 'Attention Mechanism', 'Stochastic Gradient Descent', 'Image Pairs', 'Classical Training', 'Deepfake', 'Synthetic Images', 'Salient Regions', 'Correct Annotation', 'Batch Of Samples', 'Ensemble Method', 'Style Transfer', 'Classification Loss']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Psychology and cognitive science']",14,"Can deep learning models achieve greater generalization if their training is guided by reference to human perceptual abilities? And how can we implement this in a practical manner? This paper proposes a training strategy to ConveY Brain Oversight to Raise Generalization (CYBORG). This new approach incorporates human-annotated saliency maps into a loss function that guides the model’s learning to focus on image regions that humans deem salient for the task. The Class Activation Mapping (CAM) mechanism is used to probe the model’s current saliency in each training batch, juxtapose this model saliency with human saliency, and penalize large differences. Results on the task of synthetic face detection, selected to illustrate the effectiveness of the approach, show that CYBORG leads to significant improvement in accuracy on unseen samples consisting of face images generated from six Generative Adversarial Networks across multiple classification network architectures. We also show that scaling to even seven times the training data, or using non-human-saliency auxiliary information, such as segmentation masks, and standard loss cannot beat the performance of CYBORG-trained models. As a side effect of this work, we observe that the addition of explicit region annotation to the task of synthetic face detection increased human classification accuracy. This work opens a new area of research on how to incorporate human visual saliency into loss functions in practice. All data, code and trained models used in this work are offered with this paper."
Calibrating Deep Neural Networks Using Explicit Regularisation and Dynamic Data Pruning,"Rishabh Patra, Ramya Hebbalaguppe, Tirtharaj Dash, Gautam Shroff, Lovekesh Vig","TCS Research, New Delhi; APPCAIR, BITS Pilani, Goa Campus",50,India,50,India,"Deep neural networks are prone to miscalibrated predictions, often exhibiting a mismatch between the target output and the generated sample confidence scores. Contemporary model calibration techniques mitigate the problem of overconfident predictions by pushing down the confidence of the winning class while increasing the confidence of the remaining classes across all test samples. However, from a deployment perspective, an ideal model would (i) generate well-calibrated predictions for high-confidence samples (say, Prob > 0.95) and (ii) generate a higher proportion of legitimate high-confidence samples. To this end, we propose a novel regularization technique that can be used with classification losses, leading to state-of-the-art calibrated predictions at test time; From a deployment standpoint in safety-critical applications, only high-confidence samples from a well-calibrated model are of interest, as the remaining samples have to undergo manual inspection. Predictive confidence reduction of these potentially ""high-confidence samples"" is a downside of existing calibration approaches. To mitigate this, we propose a dynamic train-time data pruning strategy which prunes low confidence samples every few epochs, providing an increase in confident yet calibrated samples. We demonstrate state-of-the-art calibration performance across image classification benchmarks, reducing training time without much compromise in accuracy. We provide insights into our dynamic pruning strategy showing that pruning low-confidence training samples lead to an increase in high-confidence samples at test time.",https://openaccess.thecvf.com/content/WACV2023/html/Patra_Calibrating_Deep_Neural_Networks_Using_Explicit_Regularisation_and_Dynamic_Data_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Patra_Calibrating_Deep_Neural_Networks_Using_Explicit_Regularisation_and_Dynamic_Data_WACV_2023_paper.pdf,,,2212.10005,main,Poster,https://ieeexplore.ieee.org/document/10030246/,"['Training', 'Deep learning', 'Neural networks', 'Manuals', 'Predictive models', 'Inspection', 'Calibration']","['Neural Network', 'Deep Neural Network', 'Training Time', 'Low Confidence', 'Prediction Probability', 'Classification Loss', 'Dynamic Strategy', 'Prediction Confidence', 'Calibration Technique', 'Calibration Performance', 'Reduce Training Time', 'Safety-critical Applications', 'Pruning Strategy', 'Training Dataset', 'Entire Dataset', 'Stochastic Gradient Descent', 'Kullback-Leibler', 'Correct Predictions', 'Gradient-based Optimization', 'Loss Term', 'Auxiliary Loss', 'Correct Probability', 'Huber Loss', 'High-confidence Predictions', 'Focal Loss', 'Deep Neural Network Classifier', 'Data Instances', 'L2 Loss', 'Negative Log-likelihood', 'Incorrect Predictions']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Social good']",4,"Deep neural networks (DNNS) are prone to miscalibrated predictions, often exhibiting a mismatch between the predicted output and the associated confidence scores. Contemporary model calibration techniques mitigate the problem of overconfident predictions by pushing down the confidence of the winning class while increasing the confidence of the remaining classes across all test samples. However, from a deployment perspective an ideal model is desired to (i) generate well calibrated predictions for high-confidence samples with predicted probability say > 0.95 and (ii) generate a higher proportion of legitimate high-confidence samples. To this end, we propose a novel regularization technique that can be used with classification losses, leading to state-of-the-art calibrated predictions at test time; From a deployment standpoint in safety critical applications, only high-confidence samples from a well-calibrated model are of interest, as the remaining samples have to undergo manual inspection. Predictive confidence reduction of these potentially ""high-confidence samples"" is a downside of existing calibration approaches. We mitigate this via proposing a dynamic traintime data pruning strategy which prunes low confidence samples every few epochs, providing an increase in confident yet calibrated samples. We demonstrate state-of-the-art calibration performance across image classification benchmarks, reducing training time without much compromise in accuracy. We provide insights into why our dynamic pruning strategy that prunes low confidence training samples leads to an increase in high-confidence samples at test time."
Camera Alignment and Weighted Contrastive Learning for Domain Adaptation in Video Person ReID,"Djebril Mekhazni, Maximilien Dufau, Christian Desrosiers, Marco Pedersoli, Eric Granger","Dept. of Systems Engineering, ETS Montreal, Canada; LIVIA, Dept. of Systems Engineering, ETS Montreal, Canada",100,Canada,0,,"Systems for person re-identification (ReID) can achieve a high level of accuracy when trained on large fully-labeled image datasets. However, the domain shift typically associated with diverse operational capture conditions (e.g., camera viewpoints and lighting) may translate to a significant decline in performance. This paper focuses on unsupervised domain adaptation (UDA) for video-based ReID -- a relevant scenario that is less explored in the literature. In this scenario, the ReID model must adapt to a complex target domain defined by a network of diverse video cameras based on tracklet information. State-of-art methods cluster unlabeled target data, yet domain shifts across target cameras (sub-domains) can lead to poor initialization of clustering methods that propagates noise across epochs, and the ReID model cannot accurately associate samples of the same identity. In this paper, an UDA method is introduced for video person ReID that leverages knowledge on video tracklets, and on the distribution of frames captured over target cameras to improve the performance of CNN backbones trained using pseudo-labels. Our method relies on an adversarial approach, where a camera-discriminator network is introduced to extract discriminant camera-independent representations, facilitating the subsequent clustering. In addition, a weighted contrastive loss is proposed to leverage the confidence of clusters, and mitigate the risk of incorrect identity associations. Experimental results obtained on three challenging video-based person ReID datasets -- PRID2011, iLIDS-VID, and MARS -- indicate that our proposed method can outperform related state-of-the-art methods. The code is available at: https://github.com/wacv23775/775.",https://openaccess.thecvf.com/content/WACV2023/html/Mekhazni_Camera_Alignment_and_Weighted_Contrastive_Learning_for_Domain_Adaptation_in_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mekhazni_Camera_Alignment_and_Weighted_Contrastive_Learning_for_Domain_Adaptation_in_WACV_2023_paper.pdf,,https://github.com/dmekhazni/CAWCL-ReID,2211.03626,main,Poster,https://ieeexplore.ieee.org/document/10030893/,"['Training', 'Representation learning', 'Adaptation models', 'Mars', 'Target tracking', 'Lighting', 'Cameras']","['Domain Adaptation', 'Self-supervised Learning', 'Camera Alignment', 'Video Person Re-identification', 'Video Camera', 'Domain Shift', 'Target Domain', 'Unlabeled Data', 'Target Data', 'Contrastive Loss', 'Relevant Scenarios', 'Adversarial Approach', 'Unsupervised Domain Adaptation Methods', 'Camera Network', 'Data Sources', 'Batch Size', 'K-nearest Neighbor', 'Recurrent Neural Network', 'Attention Network', 'Batch Of Samples', 'Source Domain', 'Source Dataset', 'Target Dataset', 'Self-paced Learning', 'Temporal Attention', 'Memory Bank', 'Combined Loss', 'Learning Pace', 'Labeled Source Domain']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",8,"Systems for person re-identification (ReID) can achieve a high accuracy when trained on large fully-labeled image datasets. However, the domain shift typically associated with diverse operational capture conditions (e.g., camera viewpoints and lighting) may translate to a significant decline in performance. This paper focuses on unsupervised domain adaptation (UDA) for video-based ReID – a relevant scenario that is less explored in the literature. In this scenario, the ReID model must adapt to a complex target domain defined by a network of diverse video cameras based on track-let information. State-of-art methods cluster unlabeled target data, yet domain shifts across target cameras (sub-domains) can lead to poor initialization of clustering methods that propagates noise across epochs, thus preventing the ReID model to accurately associate samples of same identity. In this paper, an UDA method is introduced for video person ReID that leverages knowledge on video tracklets, and on the distribution of frames captured over target cameras to improve the performance of CNN backbones trained using pseudo-labels. Our method relies on an adversarial approach, where a camera-discriminator network is introduced to extract discriminant camera-independent representations, facilitating the subsequent clustering. In addition, a weighted contrastive loss is proposed to leverage the confidence of clusters, and mitigate the risk of incorrect identity associations. Experimental results obtained on three challenging video-based person ReID datasets – PRID2011, iLIDS-VID, and MARS – indicate that our proposed method can outperform related state-of-the-art methods. Our code is available at: https://github.com/dmekhazni/CAWCL-ReID"
CameraPose: Weakly-Supervised Monocular 3D Human Pose Estimation by Leveraging In-the-Wild 2D Annotations,"Cheng-Yen Yang, Jiajia Luo, Lu Xia, Yuyin Sun, Nan Qiao, Ke Zhang, Zhongyu Jiang, Jenq-Neng Hwang, Cheng-Hao Kuo","Department of Electrical and Computer Engineering, University of Washington, WA, USA; Amazon Lab126, USA",50,USA,50,USA,"To improve the generalization of 3D human pose estimators, many existing deep learning based models focus on adding different augmentations to training poses. However, data augmentation techniques are limited to the ""seen"" pose combinations and hard to infer poses with rare ""unseen"" joint positions. To address this problem, we present CameraPose, a weakly-supervised framework for 3D human pose estimation from a single image, which can not only be applied on 2D-3D pose pairs but also on 2D alone annotations. By adding a camera parameter branch, any in-the-wild 2D annotations can be fed into our pipeline to boost the training diversity and the 3D poses can be implicitly learned by reprojecting back to 2D. Moreover, CameraPose introduces a refinement network module with confidence-guided loss to further improve the quality of noisy 2D keypoints extracted by 2D pose estimators. Experimental results demonstrate that the CameraPose brings in clear improvements on cross-scenario datasets. Notably, it outperforms the baseline method by 3mm on the most challenging dataset 3DPW. In addition, by combining our proposed refinement network module with existing 3D pose estimators, their performance can be improved in cross-scenario evaluation.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_CameraPose_Weakly-Supervised_Monocular_3D_Human_Pose_Estimation_by_Leveraging_In-the-Wild_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_CameraPose_Weakly-Supervised_Monocular_3D_Human_Pose_Estimation_by_Leveraging_In-the-Wild_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
Can Shadows Reveal Biometric Information?,"Safa C. Medin, Amir Weiss, Frédo Durand, William T. Freeman, Gregory W. Wornell",Massachusetts Institute of Technology,100,USA,0,,"We study the problem of extracting biometric information of individuals by looking at shadows of objects cast on diffuse surfaces. We show that the biometric information leakage from shadows can be sufficient for reliable identity inference under representative scenarios via a maximum likelihood analysis. We then develop a learning-based method that demonstrates this phenomenon in real settings, exploiting the subtle cues in the shadows that are the source of the leakage without requiring any labeled real data. In particular, our approach relies on building synthetic scenes composed of 3D face models obtained from a single photograph of each identity. We transfer what we learn from the synthetic data to the real data using domain adaptation in a completely unsupervised way. Our model is able to generalize well to the real domain and is robust to several variations in the scenes. We report high classification accuracies in an identity classification task that takes place in a scene with unknown geometry and occluding objects.",https://openaccess.thecvf.com/content/WACV2023/html/Medin_Can_Shadows_Reveal_Biometric_Information_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Medin_Can_Shadows_Reveal_Biometric_Information_WACV_2023_paper.pdf,,,2209.10077,main,Poster,https://ieeexplore.ieee.org/document/10030926,"['Learning systems', 'Geometry', 'Adaptation models', 'Solid modeling', 'Three-dimensional displays', 'Biometrics (access control)', 'Reliability']","['Classification Accuracy', 'Maximum Likelihood Analyses', 'Information Leakage', 'Domain Adaptation', 'Surface Diffusion', 'Face Model', 'Unsupervised Way', 'Representative Scenarios', 'Occluded Objects', 'Real Domain', 'Identity Classification', '3D Face', 'Biometric Information', 'Source Leakage', 'Data Sources', 'Training Set', 'Light Source', 'Identification Number', 'Light Conditions', 'Facial Expressions', 'Source Domain', 'Target Domain', 'Face Position', 'Head Pose', 'Variety Of Scenes', 'Object Classification', 'Neural Network Classifier', 'Cast Shadows', 'Facial Shape', 'Sample Covariance']","['Algorithms: Computational photography', 'image and video synthesis', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"We study the problem of extracting biometric information of individuals by looking at shadows of objects cast on diffuse surfaces. We show that the biometric information leakage from shadows can be sufficient for reliable identity inference under representative scenarios via a maximum likelihood analysis. We then develop a learning-based method that demonstrates this phenomenon in real settings, exploiting the subtle cues in the shadows that are the source of the leakage without requiring any labeled real data. In particular, our approach relies on building synthetic scenes composed of 3D face models obtained from a single photograph of each identity. We transfer what we learn from the synthetic data to the real data using domain adaptation in a completely unsupervised way. Our model is able to generalize well to the real domain and is robust to several variations in the scenes. We report high classification accuracies in an identity classification task that takes place in a scene with unknown geometry and occluding objects."
CellTranspose: Few-Shot Domain Adaptation for Cellular Instance Segmentation,"Matthew R. Keaton, Ram J. Zaveri, Gianfranco Doretto",West Virginia University,100,USA,0,,"Automated cellular instance segmentation is a process utilized for accelerating biological research for the past two decades, and recent advancements have produced higher quality results with less effort from the biologist. Most current endeavors focus on completely cutting the researcher out of the picture by generating highly generalized models. However, these models invariably fail when faced with novel data, distributed differently than the ones used for training. Rather than approaching the problem with methods that presume the availability of large amounts of target data and computing power for retraining, in this work we address the even greater challenge of designing an approach that requires minimal amounts of new annotated data as well as training time. We do so by designing specialized contrastive losses that leverage the few annotated samples very efficiently. A large set of results show that 3 to 5 annotations lead to models with accuracy that: 1) significantly mitigate the covariate shift effects; 2) matches or surpasses other adaptation methods; 3) even approaches methods that have been fully retrained on the target distribution. The adaptation training is only a few minutes, paving a path towards a balance between model performance, computing requirements and expert-level annotation needs.",https://openaccess.thecvf.com/content/WACV2023/html/Keaton_CellTranspose_Few-Shot_Domain_Adaptation_for_Cellular_Instance_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Keaton_CellTranspose_Few-Shot_Domain_Adaptation_for_Cellular_Instance_Segmentation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030214/,"['Training', 'Adaptation models', 'Computer vision', 'Annotations', 'Computational modeling', 'Biological system modeling', 'Graphics processing units']","['Domain Adaptation', 'Instance Segmentation', 'Few-shot Domain Adaptation', 'Target Data', 'Target Distribution', 'Contrastive Loss', 'Annotated Samples', 'Covariate Shift', 'Cell Size', 'Tissue Types', 'Unsupervised Learning', 'Intersection Over Union', 'Target Sample', 'Positive Features', 'Broad Institute', 'Adaptive Model', 'Semantic Segmentation', 'Source Images', 'Negative Characteristics', 'Target Domain', 'Source Domain', 'Target Dataset', 'Source Dataset', 'Low Budget', 'Few-shot Learning', 'Sample Patches', 'Watershed Segmentation', 'Adaptive Loss', 'Gradient Flow', 'Unsupervised Manner']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",12,"Automated cellular instance segmentation is a process utilized for accelerating biological research for the past two decades, and recent advancements have produced higher quality results with less e ort from the biologist. Most current endeavors focus on completely cutting the researcher out of the picture by generating highly generalized models. However, these models in-variably fail when faced with novel data, distributed differently than the ones used for training. Rather than approaching the problem with methods that presume the availability of large amounts of target data and computing power for retraining, in this work we address the even greater challenge of designing an approach that requires minimal amounts of new annotated data as well as training time. We do so by designing specialized contrastive losses that leverage the few annotated samples very efficiently. A large set of results show that 3 to 5 annotations lead to models with accuracy that: 1) significantly mitigate the covariate shift effects; 2) matches or surpasses other adaptation methods; 3) even approaches methods that have been fully retrained on the target distribution. The adaptation training is only a few minutes, paving a path towards a balance be-tween model performance, computing requirements and expert-level annotation needs."
Center-Aware Adversarial Augmentation for Single Domain Generalization,"Tianle Chen, Mahsa Baktashmotlagh, Zijian Wang, Mathieu Salzmann",EPFL; The University of Queensland,100,"Australia, Switzerland",0,,"Domain generalization (DG) aims to learn a model from multiple training (i.e., source) domains that can generalize well to the unseen test (i.e., target) data coming from a different distribution. Single domain generalization (Single-DG) has recently emerged to tackle a more challenging, yet realistic setting, where only one source domain is available at training time. The existing Single-DG approaches typically are based on data augmentation strategies and aim to expand the span of source data by augmenting out-of-domain samples. Generally speaking, they aim to generate hard examples to confuse the classifier. While this may make the classifier robust to small perturbation, the generated samples are typically not diverse enough to mimic a large domain shift, resulting in sub-optimal generalization performance To alleviate this, we propose a center-aware adversarial augmentation technique that expands the source distribution by altering the source samples so as to push them away from the class centers via a novel angular center loss. We conduct extensive experiments to demonstrate the effectiveness of our approach on several benchmark datasets for Single-DG and show that our method outperforms the state-of-the-art in most cases.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Center-Aware_Adversarial_Augmentation_for_Single_Domain_Generalization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Center-Aware_Adversarial_Augmentation_for_Single_Domain_Generalization_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031006/,"['Training', 'Computer vision', 'Perturbation methods', 'Semantics', 'Training data', 'Benchmark testing', 'Feature extraction']","['Domain Generalization', 'Adversarial Augmentation', 'Single Domain Generalization', 'Data Sources', 'Test Data', 'Training Time', 'Data Augmentation', 'Benchmark Datasets', 'Domain Shift', 'Real Sets', 'Target Data', 'Source Domain', 'Central Loss', 'Training Domain', 'Class Center', 'Span Of Data', 'Hard Examples', 'Training Data', 'Learning Rate', 'Stochastic Gradient Descent', 'Unseen Domains', 'Angular Distance', 'Target Domain', 'Types Of Corruption', 'Adversarial Examples', 'Latent Space', 'Vector Of Length', 'Class Activation Maps', 'Augmented Samples', 'Iterative Sampling']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",10,"Domain generalization (DG) aims to learn a model from multiple training (i.e., source) domains that can generalize well to the unseen test (i.e., target) data coming from a different distribution. Single domain generalization (Single-DG) has recently emerged to tackle a more challenging, yet realistic setting, where only one source domain is available at training time. The existing Single-DG approaches typically are based on data augmentation strategies and aim to expand the span of source data by augmenting out-of-domain samples. Generally speaking, they aim to generate hard examples to confuse the classifier. While this may make the classifier robust to small perturbation, the generated samples are typically not diverse enough to mimic a large domain shift, resulting in sub-optimal generalization performance. To alleviate this, we propose a center-aware adversarial augmentation technique that expands the source distribution by altering the source samples so as to push them away from the class centers via a novel angular center loss. We conduct extensive experiments to demonstrate the effectiveness of our approach on several benchmark datasets for Single-DG and show that our method outperforms the state-of-the-art in most cases."
Centroid Distance Keypoint Detector for Colored Point Clouds,"Hanzhe Teng, Dimitrios Chatziparaschis, Xinyue Kan, Amit K. Roy-Chowdhury, Konstantinos Karydis","University of California, Riverside",100,USA,0,,"Keypoint detection serves as the basis for many computer vision and robotics applications. Despite the fact that colored point clouds can be readily obtained, most existing keypoint detectors extract only geometry-salient keypoints, which can impede the overall performance of systems that intend to (or have the potential to) leverage color information. To promote advances in such systems, we propose an efficient multi-modal keypoint detector that can extract both geometry-salient and color-salient keypoints in colored point clouds. The proposed CEntroid Distance (CED) keypoint detector comprises an intuitive and effective saliency measure, the centroid distance, that can be used in both 3D space and color space, and a multi-modal non-maximum suppression algorithm that can select keypoints with high saliency in two or more modalities. The proposed saliency measure leverages directly the distribution of points in a local neighborhood and does not require normal estimation or eigenvalue decomposition. We evaluate the proposed method in terms of repeatability and computational efficiency (i.e. running time) against state-of-the-art keypoint detectors on both synthetic and real-world datasets. Results demonstrate that our proposed CED keypoint detector requires minimal computational time while attaining high repeatability. To showcase one of the potential applications of the proposed method, we further investigate the task of colored point cloud registration. Results suggest that our proposed CED detector outperforms state-of-the-art handcrafted and learning-based keypoint detectors in the evaluated scenes. The C++ implementation of the proposed method is made publicly available at https://github.com/UCR-Robotics/CED_Detector.",https://openaccess.thecvf.com/content/WACV2023/html/Teng_Centroid_Distance_Keypoint_Detector_for_Colored_Point_Clouds_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Teng_Centroid_Distance_Keypoint_Detector_for_Colored_Point_Clouds_WACV_2023_paper.pdf,,https://github.com/UCR-Robotics/CED_Detector,2210.01298,main,Poster,https://ieeexplore.ieee.org/document/10030902/,"['Point cloud compression', 'Computer vision', 'Three-dimensional displays', 'Navigation', 'Estimation', 'Detectors', 'Color']","['Point Cloud', 'Keypoint Detection', 'Colored Point Cloud', 'System Performance', 'Computational Efficiency', 'Running Time', 'Computer Vision', '3D Space', 'Color Space', 'Real-world Datasets', 'Local Neighborhood', 'Normal Approximation', 'Point Detection', 'Robotic Applications', 'Color Information', 'Non-maximum Suppression', 'High Salience', 'Point Cloud Registration', 'Intuitive Measure', 'Measure Of Salience', 'Geometric Information', 'Query Point', 'Spherical Region', 'Position Of Point', 'RGB Color Space', 'Indoor Environments', 'Neighboring Points', '3D Point Cloud', 'Cloud Properties', 'Descriptive Characteristics']","['Algorithms: 3D computer vision', 'Robotics']",4,"Keypoint detection serves as the basis for many computer vision and robotics applications. Despite the fact that colored point clouds can be readily obtained, most existing keypoint detectors extract only geometry-salient keypoints, which can impede the overall performance of systems that intend to (or have the potential to) leverage color information. To promote advances in such systems, we propose an efficient multi-modal keypoint detector that can extract both geometry-salient and color-salient keypoints in colored point clouds. The proposed CEntroid Distance (CED) key- point detector comprises an intuitive and effective saliency measure, the centroid distance, that can be used in both 3D space and color space, and a multi-modal non-maximum suppression algorithm that can select keypoints with high saliency in two or more modalities. The proposed saliency measure leverages directly the distribution of points in a local neighborhood and does not require normal estimation or eigenvalue decomposition. We evaluate the proposed method in terms of repeatability and computational efficiency (i.e. running time) against state-of-the-art key- point detectors on both synthetic and real-world datasets. Results demonstrate that our proposed CED keypoint detector requires minimal computational time while attaining high repeatability. To showcase one of the potential applications of the proposed method, we further investigate the task of colored point cloud registration. Results suggest that our proposed CED detector outperforms state-of- the-art handcrafted and learning-based keypoint detectors in the evaluated scenes. The C++ implementation of the proposed method is made publicly available at https://github.com/UCR-Robotics/CED_Detector."
Certified Defense for Content Based Image Retrieval,"Kazuya Kakizaki, Kazuto Fukuchi, Jun Sakuma","University of Tsukuba, RIKEN AIP, Tsukuba, Japan; NEC Corporation, University of Tsukuba, Kawasaki, Japan",100,Japan,0,,"This paper develops a certified defense for deep neural network (DNN) based content based image retrieval (CBIR) against adversarial examples (AXs). Previous works put their effort into certified defense for classification to improve certified robustness, which guarantees that no AX to cause misclassification exists around the sample. Such certified defense, however, could not be applied to CBIR directly because the goals of adversarial attack against classification and CBIR are completely different. To develop the certified defense for CBIR, we first define new certified robustness of CBIR, which guarantees that no AX that changes the ranking of CBIR exists around the query or candidate images. Then, we propose computationally tractable verification algorithms that verify whether the certified robustness of CBIR is achieved by utilizing upper and lower bounds of distances between feature representations of perturbed and non-perturbed images. Finally, we propose new objective functions for training feature extraction DNNs that increases the number of inputs that satisfy the certified robustness of CBIR by tightening the upper and lower bounds. Experimental results show that our objective functions significantly improve the certified robustness of CBIR than existing methods.",https://openaccess.thecvf.com/content/WACV2023/html/Kakizaki_Certified_Defense_for_Content_Based_Image_Retrieval_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kakizaki_Certified_Defense_for_Content_Based_Image_Retrieval_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030859/,"['Training', 'Deep learning', 'Computer vision', 'Image retrieval', 'Neural networks', 'Linear programming', 'Feature extraction']","['Content-based Image Retrieval', 'Lower Bound', 'Objective Function', 'Deep Neural Network', 'Adversarial Attacks', 'Query Image', 'Adversarial Examples', 'Verification Algorithm', 'Training Set', 'Training Dataset', 'Hyperparameters', 'Step Size', 'Sufficient Conditions', 'Feature Space', 'Adversarial Training', 'Metric Learning', 'Triplet Loss', 'Robust Training', 'Adversarial Perturbations', 'Defense Methods']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"This paper develops a certified defense for deep neural network (DNN) based content based image retrieval (CBIR) against adversarial examples (AXs). Previous works put their effort into certified defense for classification to improve certified robustness, which guarantees that no AX to cause misclassification exists around the sample. Such certified defense, however, could not be applied to CBIR directly because the goals of adversarial attack against classification and CBIR are completely different. To develop the certified defense for CBIR, we first define new certified robustness of CBIR, which guarantees that no AX that changes the ranking of CBIR exists around the query or candidate images. Then, we propose computationally tractable verification algorithms that verify whether the certified robustness of CBIR is achieved by utilizing upper and lower bounds of distances between feature representations of perturbed and non-perturbed images. Finally, we propose new objective functions for training feature extraction DNNs that increases the number of inputs that satisfy the certified robustness of CBIR by tightening the upper and lower bounds. Experimental results show that our objective functions significantly improve the certified robustness of CBIR than existing methods."
Class-Level Confidence Based 3D Semi-Supervised Learning,"Zhimin Chen, Longlong Jing, Liang Yang, Yingwei Li, Bing Li",The City University of New York; Johns Hopkins University; Clemson University,100,USA,0,,"Current pseudo-labeling strategies in 3D semi-supervised learning (SSL) fail to dynamically incorporate the variance of learning status which is affected by each class's learning difficulty and data imbalance. To address this problem, we practically demonstrate that 3D unlabeled data class-level confidence can represent the learning status. Based on this finding, we present a novel class-level confidence based 3D SSL method. Firstly, a dynamic thresholding strategy is proposed to utilize more unlabeled data, especially for low learning status classes. Then, a re-sampling strategy is designed to avoid biasing toward high learning status classes, which dynamically changes the sampling probability of each class. Unlike the latest state-of-the-art SSL method FlexMatch which also utilizes dynamic threshold, our method can be applied to the inherently imbalanced dataset and thus is more general. To show the effectiveness of our method in 3D SSL tasks, we conduct extensive experiments on 3D SSL classification and detection tasks. Our method significantly outperforms state-of-the-art counterparts for both 3D SSL classification and detection tasks in all datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Class-Level_Confidence_Based_3D_Semi-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Class-Level_Confidence_Based_3D_Semi-Supervised_Learning_WACV_2023_paper.pdf,,,2210.10138,main,Poster,https://ieeexplore.ieee.org/document/10030420/,"['Computer vision', 'Three-dimensional displays', 'Estimation', 'Semisupervised learning', 'Task analysis']","['Semi-supervised Learning', 'Classification Task', 'Probability Sampling', 'Detection Task', 'Lower Class', 'Class Probabilities', 'Learning Conditions', 'Unlabeled Data', 'Imbalanced Data', 'Dynamic Strategy', '3D Classification', 'Secret Sharing', 'Dynamic Threshold', '3D Detection', '3D Tasks', 'Resampling Strategy', 'Nonlinear Function', 'Classification Results', 'Higher Threshold', 'Learning Disabilities', '3D Object Detection', 'Pseudo Labels', 'Unlabeled Set', 'Object Detection', 'Semi-supervised Methods', 'Point Cloud', 'Mean Average Precision', 'Low Difficulty', 'Prediction Confidence']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', '3D computer vision']",4,"Recent state-of-the-art method FlexMatch firstly demonstrated that correctly estimating learning status is crucial for semi-supervised learning (SSL). However, the estimation method proposed by FlexMatch does not take into account imbalanced data, which is the common case for 3D semi-supervised learning. To address this problem, we practically demonstrate that unlabeled data class-level confidence can represent the learning status in the 3D imbalanced dataset. Based on this finding, we present a novel class-level confidence based 3D SSL method. Firstly, a dynamic thresholding strategy is proposed to utilize more unlabeled data, especially for low learning status classes. Then, a re-sampling strategy is designed to avoid biasing toward high learning status classes, which dynamically changes the sampling probability of each class. To show the effectiveness of our method in 3D SSL tasks, we conduct extensive experiments on 3D SSL classification and detection tasks. Our method significantly outperforms state-of-the-art counterparts for both 3D SSL classification and detection tasks in all datasets."
Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently,"Futa Waseda, Sosuke Nishikawa, Trung-Nghia Le, Huy H. Nguyen, Isao Echizen","National Institute of Informatics, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan; University of Science, VNU-HCM, Vietnam; Vietnam National University, Ho Chi Minh City, Vietnam; The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan",100,"Japan, Vietnam",0,,"Deep neural networks are vulnerable to adversarial examples (AEs), which have adversarial transferability: AEs generated for the source model can mislead another (target) model's predictions. However, the transferability has not been understood in terms of to which class target model's predictions were misled (i.e., class-aware transferability). In this paper, we differentiate the cases in which a target model predicts the same wrong class as the source model (""same mistake"") or a different wrong class (""different mistake"") to analyze and provide an explanation of the mechanism. We find that (1) AEs tend to cause same mistakes, which correlates with ""non-targeted transferability""; however, (2) different mistakes occur even between similar models, regardless of the perturbation size. Furthermore, we present evidence that the difference between same mistakes and different mistakes can be explained by non-robust features, predictive but human-uninterpretable patterns: different mistakes occur when non-robust features in AEs are used differently by models. Non-robust features can thus provide consistent explanations for the class-aware transferability of AEs.",https://openaccess.thecvf.com/content/WACV2023/html/Waseda_Closer_Look_at_the_Transferability_of_Adversarial_Examples_How_They_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Waseda_Closer_Look_at_the_Transferability_of_Adversarial_Examples_How_They_WACV_2023_paper.pdf,,,2112.14337,main,Poster,https://ieeexplore.ieee.org/document/10030095/,"['Deep learning', 'Analytical models', 'Computer vision', 'Perturbation methods', 'Neural networks', 'Predictive models']","['Adversarial Examples', 'Transferability Of Adversarial Examples', 'Deep Neural Network', 'Source Model', 'Target Model', 'Wrong Classification', 'Training Set', 'Step Size', 'Specific Categories', 'Feature Classification', 'Feature Learning', 'Generative Adversarial Networks', 'Ensemble Model', 'Target Class', 'Linear Classifier', 'Decision Boundary', 'Dropout Layer', 'True Class', 'Adversarial Attacks', 'Image X', 'Projected Gradient Descent', 'Black-box Attacks', 'Original Test Set', 'Large Perturbations', 'Usage Of Features', 'Adversarial Perturbations']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",19,"Deep neural networks are vulnerable to adversarial examples (AEs), which have adversarial transferability: AEs generated for the source model can mislead another (target) model’s predictions. However, the transferability has not been understood in terms of to which class target model’s predictions were misled (i.e., class-aware transferability). In this paper, we differentiate the cases in which a target model predicts the same wrong class as the source model (""same mistake"") or a different wrong class (""different mistake"") to analyze and provide an explanation of the mechanism. We find that (1) AEs tend to cause same mistakes, which correlates with ""non-targeted transferability""; how-ever, (2) different mistakes occur even between similar models, regardless of the perturbation size. Furthermore, we present evidence that the difference between same mistakes and different mistakes can be explained by non-robust features, predictive but human-uninterpretable patterns: different mistakes occur when non-robust features in AEs are used differently by models. Non-robust features can thus provide consistent explanations for the class-aware transferability of AEs."
CoKe: Contrastive Learning for Robust Keypoint Detection,"Yutong Bai, Angtian Wang, Adam Kortylewski, Alan Yuille",University of Freiburg; Max-Planck-Institute for Informatics; Johns Hopkins University,100,"Germany, USA",0,,"In this paper, we introduce a contrastive learning framework for keypoint detection (CoKe). Keypoint detection differs from other visual tasks where contrastive learning has been applied because the input is a set of images in which multiple keypoints are annotated. This requires the contrastive learning to be extended such that the keypoints are represented and detected independently, which enables the contrastive loss to make the keypoint features different from each other and from the background. Our approach has two benefits: It enables us to exploit the power of contrastive learning for keypoint detection, and by detecting each keypoint independently the detection becomes more robust to occlusion compared to holistic methods, such as stacked hourglass networks, which attempt to detect all keypoints jointly. Our CoKe framework introduces several technical innovations. In particular, we introduce: (i) A clutter bank to represent non-keypoint features; (ii) a keypoint bank that stores prototypical representations of keypoints to approximate the contrastive loss between keypoints; and (iii) a cumulative moving average update to learn the keypoint prototypes while training the feature extractor. Our experiments on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D) show that our approach works as well, or better than, alternative methods for keypoint detection, even for human keypoints, for which the literature is vast. Moreover, we observe that CoKe is exceptionally robust to partial occlusion and previously unseen object poses.",https://openaccess.thecvf.com/content/WACV2023/html/Bai_CoKe_Contrastive_Learning_for_Robust_Keypoint_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bai_CoKe_Contrastive_Learning_for_Robust_Keypoint_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031011/,"['Training', 'Representation learning', 'Visualization', 'Technological innovation', 'Computer vision', 'Prototypes', 'Feature extraction']","['Self-supervised Learning', 'Keypoint Detection', 'Visual Task', 'Contrastive Loss', 'Training Data', 'Feature Space', 'Feature Maps', 'Rigid Body', 'Receptive Field', 'Representation Learning', 'Memory Consumption', 'Local Descriptors', 'Large Number Of Images', 'Challenging Scenarios', 'Regression Loss', 'Human Pose', 'Gradient Step', 'Occlusion Effect', 'Occlusion Level']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",8,"In this paper, we introduce a contrastive learning framework for keypoint detection (CoKe). Keypoint detection differs from other visual tasks where contrastive learning has been applied because the input is a set of images in which multiple keypoints are annotated. This requires the contrastive learning to be extended such that the keypoints are represented and detected independently, which enables the contrastive loss to make the keypoint features different from each other and from the background. Our approach has two benefits: It enables us to exploit contrastive learning for keypoint detection, and by detecting each key-point independently the detection becomes more robust to occlusion compared to holistic methods, such as stacked hourglass networks, which attempt to detect all keypoints jointly. Our CoKe framework introduces several technical innovations. In particular, we introduce: (i) A clutter bank to represent non-keypoint features; (ii) a keypoint bank that stores prototypical representations of keypoints to approximate the contrastive loss between keypoints; and (iii) a cumulative moving average update to learn the key-point prototypes while training the feature extractor. Our experiments on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D) show that our approach works as well, or better than, alternative methods for keypoint detection, even for human keypoints, for which the literature is vast. Moreover, we observe that CoKe is exceptionally robust to partial occlusion and previously unseen object poses."
CoNMix for Source-Free Single and Multi-Target Domain Adaptation,"Vikash Kumar, Rohit Lal, Himanshu Patil, Anirban Chakraborty","Indian Institute of Science, Bengaluru, India",100,India,0,,"This work introduces the novel task of Source-free Multi-target Domain Adaptation and proposes adaptation framework comprising of Consistency with Nuclear-Norm Maximization and MixUp knowledge distillation (CoNMix) as a solution to this problem. The main motive of this work is to solve for Single and Multi target Domain Adaptation (SMTDA) for the source-free paradigm, which enforces a constraint where the labeled source data is not available during target adaptation due to various privacy-related restrictions on data sharing. The source-free approach leverages target pseudo labels, which can be noisy, to improve the target adaptation. We introduce consistency between label preserving augmentations and utilize pseudo label refinement methods to reduce noisy pseudo labels. Further, we propose novel MixUp Knowledge Distillation (MKD) for better generalization on multiple target domains using various source-free STDA models. We also show that the Vision Transformer (VT) backbone gives better feature representation with improved domain transferability and class discriminability. Our proposed framework achieves the state-of-the-art (SOTA) results in various paradigms of source-free STDA and MTDA settings on popular domain adaptation datasets like Office-Home, Office-Caltech, and DomainNet. Project Page: https://sites.google.com/view/conmix-vcl",https://openaccess.thecvf.com/content/WACV2023/html/Kumar_CoNMix_for_Source-Free_Single_and_Multi-Target_Domain_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kumar_CoNMix_for_Source-Free_Single_and_Multi-Target_Domain_Adaptation_WACV_2023_paper.pdf,https://sites.google.com/view/conmix-vcl,,2211.03876,main,Poster,https://ieeexplore.ieee.org/document/10030885/,"['Computer vision', 'Adaptation models', 'Uncertainty', 'Heuristic algorithms', 'Feature extraction', 'Transformers', 'Noise measurement']","['Single Domain', 'Domain Adaptation', 'Multi-target Domain Adaptation', 'Multi-domain', 'Feature Representation', 'Single Target', 'Target Domain', 'Popular Datasets', 'Nuclear Norm', 'Pseudo Labels', 'Domain Transfer', 'Noisy Labels', 'Vision Transformer', 'Target Image', 'Representation Learning', 'Version Of Task', 'Unlabeled Data', 'Frobenius Norm', 'Source Domain', 'Student Model', 'Intermediate Domain', 'Unlabeled Target Data', 'Student Network', 'Source Dataset', 'Unlabeled Target Domain', 'Consistency Constraint', 'Inductive Bias', 'Consistency Loss', 'Labeled Source Domain', 'Domain Adaptation Methods']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Vision + language and/or other modalities']",9,"This work introduces the novel task of Source-free Multi-target Domain Adaptation and proposes adaptation framework comprising of Consistency with Nuclear-Norm Maximization and MixUp knowledge distillation (CoNMix) as a solution to this problem. The main motive of this work is to solve for Single and Multi target Domain Adaptation (SMTDA) for the source-free paradigm, which enforces a constraint where the labeled source data is not available during target adaptation due to various privacy-related restrictions on data sharing. The source-free approach leverages target pseudo labels, which can be noisy, to improve the target adaptation. We introduce consistency between label preserving augmentations and utilize pseudo label refinement methods to reduce noisy pseudo labels. Further, we propose novel MixUp Knowledge Distillation (MKD) for better generalization on multiple target domains using various source-free STDA models. We also show that the Vision Transformer (VT) backbone gives better feature representation with improved domain transferability and class discriminability. Our proposed framework achieves the state-of-the-art (SOTA) results in various paradigms of source-free STDA and MTDA settings on popular domain adaptation datasets like Office-Home, Office-Caltech, and DomainNet. Project Page: https://sites.google.com/view/conmix-vcl"
Collaborative Multi-Teacher Knowledge Distillation for Learning Low Bit-Width Deep Neural Networks,"Cuong Pham, Tuan Hoang, Thanh-Toan Do","Department of Data Science and AI, Monash University, Australia; Bytedance",50,Australia,50,China,"Knowledge distillation which learns a lightweight student model by distilling knowledge from a cumbersome teacher model is an attractive approach for learning compact deep neural networks (DNNs). Recent works further improve student network performance by leveraging multiple teacher networks. However, most of the existing knowledge distillation-based multi-teacher methods use separately pretrained teachers. This limits the collaborative learning between teachers and the mutual learning between teachers and student. Network quantization is another at- tractive approach for learning compact DNNs. However, most existing network quantization methods are developed and evaluated without considering multi-teacher support to enhance the performance of quantized student model. In this paper, we propose a novel framework that leverages both multi-teacher knowledge distillation and network quantization for learning low bit-width DNNs. The proposed method encourages both collaborative learning between quantized teachers and mutual learning between quantized teachers and quantized student. During learning process, at corresponding layers, knowledge from teachers will form an importance-aware shared knowledge which will be used as input for teachers at subsequent layers and also be used to guide student. Our experimental results on CIFAR100 and ImageNet datasets show that the compact quantized student models trained with our method achieve competitive results compared to other state-of-the-art methods, and in some cases, indeed surpass the full precision models.",https://openaccess.thecvf.com/content/WACV2023/html/Pham_Collaborative_Multi-Teacher_Knowledge_Distillation_for_Learning_Low_Bit-Width_Deep_Neural_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pham_Collaborative_Multi-Teacher_Knowledge_Distillation_for_Learning_Low_Bit-Width_Deep_Neural_WACV_2023_paper.pdf,,,2210.16103,main,Poster,https://ieeexplore.ieee.org/document/10030584/,"['Knowledge engineering', 'Deep learning', 'Training', 'Computer vision', 'Quantization (signal)', 'Federated learning', 'Computational modeling']","['Deep Neural Network', 'Learning Process', 'Quantification Method', 'Teacher Model', 'Knowledge Sharing', 'ImageNet Dataset', 'Student Model', 'Teacher Network', 'Mutual Learning', 'Student Network', 'Multiple Teachers', 'CIFAR-100 Dataset', 'Network Quantization', 'Quantum', 'Learning Rate', 'Convolutional Neural Network', 'Convolutional Layers', 'Feature Maps', 'Educational Factors', 'Top-1 Accuracy', 'Attention Loss', 'Top-5 Accuracy', 'Uniform Quantization', 'Quantification Model', 'Attention Map', 'Distillation Method']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Embedded sensing/real-time techniques']",9,"Knowledge distillation which learns a lightweight student model by distilling knowledge from a cumbersome teacher model is an attractive approach for learning compact deep neural networks (DNNs). Recent works further improve student network performance by leveraging multiple teacher networks. However, most of the existing knowledge distillation-based multi-teacher methods use separately pretrained teachers. This limits the collaborative learning between teachers and the mutual learning between teachers and student. Network quantization is another attractive approach for learning compact DNNs. However, most existing network quantization methods are developed and evaluated without considering multi-teacher support to enhance the performance of quantized student model. In this paper, we propose a novel framework that leverages both multi-teacher knowledge distillation and network quantization for learning low bit-width DNNs. The proposed method encourages both collaborative learning between quantized teachers and mutual learning between quantized teachers and quantized student. During learning process, at corresponding layers, knowledge from teachers will form an importance-aware shared knowledge which will be used as input for teachers at subsequent layers and also be used to guide student. Our experimental results on CIFAR-100 and ImageNet datasets show that the compact quantized student models trained with our method achieve competitive results compared to other state-of-the-art methods, and in some cases, surpass the full precision models."
Color Recommendation for Vector Graphic Documents Based on Multi-Palette Representation,"Qianru Qiu, Xueting Wang, Mayu Otani, Yuki Iwazaki","CyberAgent, Shibuya, Tokyo, Japan",100,Japan,0,,"Vector graphic documents present multiple visual elements, such as images, shapes, and texts. Choosing appropriate colors for multiple visual elements is a difficult but crucial task for both amateurs and professional designers. Instead of creating a single color palette for all elements, we extract multiple color palettes from each visual element in a graphic document, and then combine them into a color sequence. We propose a masked color model for color sequence completion and recommend the specified colors based on color context in multi-palette with high probability. We train the model and build a color recommendation system on a large-scale dataset of vector graphic documents. The proposed color recommendation method outperformed other state-of-the-art methods by both quantitative and qualitative evaluations on color prediction and our color recommendation system received positive feedback from professional designers in an interview study.",https://openaccess.thecvf.com/content/WACV2023/html/Qiu_Color_Recommendation_for_Vector_Graphic_Documents_Based_on_Multi-Palette_Representation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qiu_Color_Recommendation_for_Vector_Graphic_Documents_Based_on_Multi-Palette_Representation_WACV_2023_paper.pdf,,https://github.com/CyberAgentAILab/multipalette,2209.1082,main,Poster,https://ieeexplore.ieee.org/document/10030568/,"['Graphics', 'Visualization', 'Computer vision', 'Image color analysis', 'Shape', 'Interactive systems', 'Computational modeling']","['Vector Graphics', 'Quantitative Evaluation', 'Recommender Systems', 'Visual Elements', 'Color Model', 'Color Palette', 'Design Professionals', 'Deep Learning', 'Test Dataset', 'Representation Learning', 'Color Space', 'Good And Bad', 'Convex Hull', 'Handcrafted Features', 'Word Embedding', 'Graphic Design', 'Template Design', 'Textual Elements', 'Position Embedding', 'RGB Color Space', 'Text Color', 'Neutral Color', 'Word2vec Model', 'Word Embedding Model', 'Color Representation', 'CIELAB Color Space']","['Applications: Arts/games/social media', 'Visualization']",1,"Vector graphic documents present multiple visual elements, such as images, shapes, and texts. Choosing appropriate colors for multiple visual elements is challenging but crucial for amateurs and professional designers. Instead of creating a single color palette for all elements, we extract multiple color palettes from each visual element of a graphic document, and then combine them into a color sequence. We propose a masked color model for color sequence completion and recommend the specified colors based on the color context in multi-palette with high probability. We train the model and build a color recommendation system on a large-scale dataset of vector graphic documents. The proposed color recommendation method outperformed other state-of-the-art methods by quantitative and qualitative evaluations on color prediction and our color recommendation system received positive feedback from professional designers in an interview study. Our code and trained model are available at https://github.com/CyberAgentAILab/multipalette."
Compact and Optimal Deep Learning With Recurrent Parameter Generators,"Jiayun Wang, Yubei Chen, Stella X. Yu, Brian Cheung, Yann LeCun",MIT CSAIL & BCS; UC Berkeley / ICSI / University of Michigan; Meta AI / New York University; UC Berkeley / ICSI,100,"Canada, USA",0,,"Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of an arbitrary architecture, in one-stage end-to-end learning. Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with in fact faster convergence. Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction, and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18's performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34's performance with only 0.25% DoF! Our work shows significant potential of constrained neural optimization in compact and optimal deep learning.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_Compact_and_Optimal_Deep_Learning_With_Recurrent_Parameter_Generators_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Compact_and_Optimal_Deep_Learning_With_Recurrent_Parameter_Generators_WACV_2023_paper.pdf,,,2107.0711,main,Poster,https://ieeexplore.ieee.org/document/10030806/,"['Deep learning', 'Training', 'Computer vision', 'Computer architecture', 'Performance gain', 'Generators', 'Decorrelation']","['Optimal Deep Learning', 'Model Parameters', 'Degrees Of Freedom', 'Gradient Descent', 'Convolutional Layers', 'Random Permutations', 'Linear Constraints', 'Top-1 Accuracy', 'Log-linear Relationship', 'Sign Flip', 'Neural Network', 'Deep Neural Network', 'Power-law', 'Model Size', 'Deep Convolutional Neural Network', 'Batch Normalization', 'Convolution Kernel', 'Equilibrium Model', 'Normal Approximation', 'Neural Architecture Search', 'Pose Estimation', 'Permutation Matrix', 'Recurrent Model', 'Generator Matrix', 'Shared Parameters', 'Pruning Method', 'Network Quantization', 'Depth Estimation', 'Monocular Depth Estimation']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",1,"Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of an arbitrary architecture, in one-stage end-to-end learning.Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with in fact faster convergence.Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction, and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18’s performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34’s performance with only 0.25% DoF! Our work shows significant potential of constrained neural opti-mization in compact and optimal deep learning."
Complementary Bi-Directional Feature Compression for Indoor 360deg Semantic Segmentation With Self-Distillation,"Zishuo Zheng, Chunyu Lin, Lang Nie, Kang Liao, Zhijie Shen, Yao Zhao","Institute of Information Science, Beijing Jiaotong University, Beijing, China",100,China,0,,"Semantic segmentation on 360deg images is a vital component of scene understanding due to the rich surrounding information. Recently, horizontal representation-based approaches outperform projection-based solutions, because the distortions can be effectively removed by compressing the spherical data in the vertical direction. However, these methods ignore the distortion distribution prior and are limited to unbalanced receptive fields, e.g., the receptive fields are sufficient in the vertical direction and insufficient in the horizontal direction. Differently, a vertical representation compressed in another direction can offer implicit distortion prior and enlarge horizontal receptive fields. In this paper, we combine the two different representations and propose a novel 360deg semantic segmentation solution from a complementary perspective. Our network comprises three modules: a feature extraction module, a bi-directional compression module, and an ensemble decoding module. First, we extract multi-scale features from a panorama. Then, a bi-directional compression module is designed to compress features into two complementary low-dimensional representations, which provide content perception and distortion prior. Furthermore, to facilitate the fusion of bi-directional features, we design a unique self distillation strategy in the ensemble decoding module to enhance the interaction of different features and further improve the performance. Experimental results show that our approach outperforms the state-of-the-art solutions on quantitative evaluations while displaying the best performance on visual appearance.",https://openaccess.thecvf.com/content/WACV2023/html/Zheng_Complementary_Bi-Directional_Feature_Compression_for_Indoor_360deg_Semantic_Segmentation_With_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_Complementary_Bi-Directional_Feature_Compression_for_Indoor_360deg_Semantic_Segmentation_With_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
Complementary Cues From Audio Help Combat Noise in Weakly-Supervised Object Detection,"Cagri Gungor, Adriana Kovashka","Intelligent Systems Program, University of Pittsburgh; Department of Computer Science, University of Pittsburgh",100,USA,0,,"We tackle the problem of learning object detectors in a noisy environment, which is one of the significant challenges for weakly-supervised learning. We use multimodal learning to help localize objects of interest, but unlike other methods, we treat audio as an auxiliary modality that assists to tackle noise in detection from visual regions. First, we use the audio-visual model to generate new ""ground-truth"" labels for the training set to remove noise between the visual features and noisy supervision. Second, we propose an ""indirect path"" between audio and class predictions, which combines the link between visual and audio regions, and the link between visual features and predictions. Third, we propose a sound-based ""attention path"" which uses the benefit of complementary audio cues to identify important visual regions. We use contrastive learning to perform region-based audio-visual instance discrimination, which serves as an intermediate task and benefits from the complementary cues from audio to boost object classification and detection performance. We show that our methods, which update noisy ground truth and provide indirect and attention paths, greatly boosting performance on the AudioSet and VGGSound datasets compared to single-modality predictions, even ones that use contrastive learning. Our method outperforms previous weakly-supervised detectors for the task of object detection by reaching the state-of-art on AudioSet, and our sound localization module performs better than several state-of-art methods on AudioSet and MUSIC.",https://openaccess.thecvf.com/content/WACV2023/html/Gungor_Complementary_Cues_From_Audio_Help_Combat_Noise_in_Weakly-Supervised_Object_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gungor_Complementary_Cues_From_Audio_Help_Combat_Noise_in_Weakly-Supervised_Object_WACV_2023_paper.pdf,https://cagrigungor.github.io/AudioVisualWSOD/,https://github.com/cagrigungor/AudioVisualWSOD,,main,Poster,https://ieeexplore.ieee.org/document/10030521/,"['Training', 'Location awareness', 'Visualization', 'Computer vision', 'Music', 'Object detection', 'Detectors']","['Object Detection', 'Detector Noise', 'Complementary Cues', 'Weakly Supervised Object Detection', 'Training Set', 'Detection Performance', 'Visual Features', 'Noisy Environments', 'Indirect Path', 'Self-supervised Learning', 'Sound Localization', 'Object Detection Task', 'Multimodal Learning', 'Labeled Training Set', 'Set Of Results', 'Attention Mechanism', 'Image Pairs', 'Video Clips', 'Object Location', 'Classification Score', 'Noisy Labels', 'Label Noise', 'Noise Set', 'Image-level Labels', 'Detection Module', 'Pseudo Labels', 'Combination Of Paths', 'Object Labels', 'Noisy Set', 'Visual Modality']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Vision + language and/or other modalities']",3,"We tackle the problem of learning object detectors in a noisy environment, which is one of the significant challenges for weakly-supervised learning. We use multimodal learning to help localize objects of interest, but unlike other methods, we treat audio as an auxiliary modality that assists to tackle noise in detection from visual regions. First, we use the audio-visual model to generate new ""ground-truth"" labels for the training set to remove noise between the visual features and noisy supervision. Second, we propose an ""indirect path"" between audio and class predictions, which combines the link between visual and audio regions, and the link between visual features and predictions. Third, we propose a sound-based ""attention path"" which uses the benefit of complementary audio cues to identify important visual regions. We use contrastive learning to perform region-based audio-visual instance discrimination, which serves as an intermediate task and benefits from the complementary cues from audio to boost object classification and detection performance. We show that our methods, which update noisy ground truth and provide indirect and attention paths, greatly boosting performance on the AudioSet and VGGSound datasets compared to single-modality predictions, even ones that use contrastive learning. Our method outperforms previous weakly-supervised detectors for the task of object detection by reaching the state-of-art on AudioSet, and our sound localization module performs better than several state-of-art methods on AudioSet and MUSIC."
Composite Learning for Robust and Effective Dense Predictions,"Menelaos Kanakis, Thomas E. Huang, David Brüggemann, Fisher Yu, Luc Van Gool","ETH Zürich; ETH Zürich, KU Leuven",100,"Belgium, Switzerland",0,,"Multi-task learning promises better model generalization on a target task by jointly optimizing it with an auxiliary task. However, the current practice requires additional labeling efforts for the auxiliary task, while not guaranteeing better model performance. In this paper, we find that jointly training a dense prediction (target) task with a self-supervised (auxiliary) task can consistently improve the performance of the target task, while eliminating the need for labeling auxiliary tasks. We refer to this joint training as Composite Learning (CompL). Experiments of CompL on monocular depth estimation, semantic segmentation, and boundary detection show consistent performance improvements in fully and partially labeled datasets. Further analysis on depth estimation reveals that joint training with self-supervision outperforms most labeled auxiliary tasks. We also find that CompL can improve model robustness when the models are evaluated in new domains. These results demonstrate the benefits of self-supervision as an auxiliary task, and establish the design of novel task-specific self-supervised methods as a new axis of investigation for future multi-task learning research.",https://openaccess.thecvf.com/content/WACV2023/html/Kanakis_Composite_Learning_for_Robust_and_Effective_Dense_Predictions_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kanakis_Composite_Learning_for_Robust_and_Effective_Dense_Predictions_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030424/,"['Training', 'Computer vision', 'Semantic segmentation', 'Estimation', 'Multitasking', 'Robustness', 'Labeling']","['Dense Prediction', 'Task Performance', 'Semantic Segmentation', 'Prediction Task', 'Joint Optimization', 'Consistent Improvement', 'Depth Estimation', 'Multi-task Learning', 'Target Task', 'Joint Training', 'Segmentation Detection', 'Boundary Detection', 'Auxiliary Task', 'Monocular Depth Estimation', 'Self-supervised Task', 'Root Mean Square Error', 'Training Set', 'Convolutional Network', 'Convolutional Neural Network', 'Input Image', 'Transfer Learning', 'Inductive Bias', 'Self-supervised Learning', 'Performance Gain', 'Baseline Performance', 'Prediction Head', 'Joint Learning', 'Domain Shift', 'Feature Representation', 'Representation Learning']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",3,"Multi-task learning promises better model generalization on a target task by jointly optimizing it with an auxiliary task. However, the current practice requires additional labeling efforts for the auxiliary task, while not guaranteeing better model performance. In this paper, we find that jointly training a dense prediction (target) task with a self-supervised (auxiliary) task can consistently improve the performance of the target task, while eliminating the need for labeling auxiliary tasks. We refer to this joint training as Composite Learning (CompL). Experiments of CompL on monocular depth estimation, semantic segmentation, and boundary detection show consistent performance improvements in fully and partially labeled datasets. Further analysis on depth estimation reveals that joint training with self-supervision outperforms most labeled auxiliary tasks. We also find that CompL can improve model robustness when the models are evaluated in new domains. These results demonstrate the benefits of self-supervision as an auxiliary task, and establish the design of novel task-specific self-supervised methods as a new axis of investigation for future multi-task learning research."
Composite Relationship Fields With Transformers for Scene Graph Generation,"George Adaimi, David Mizrahi, Alexandre Alahi","EPFL, VITA",100,Switzerland,0,,"Scene graph generation (SGG) methods extract relationships between objects. While most methods focus on improving top-down approaches, which build a scene graph based on detected objects from an off-the-shelf object detector, there is a limited amount of work on bottom-up approaches, which jointly detect objects and their relationships in a single stage. In this work, we present a novel bottom-up SGG approach by representing relationships using Composite Relationship Fields (CoRF). CoRF turns relationship detection into a dense regression and classification task, where each cell of the output feature map identifies surrounding objects and their relationships. Furthermore, we propose a refinement head that leverages Transformers for global scene reasoning, resulting in more meaningful relationship predictions. By combining both contributions, our method outperforms previous bottom-up methods on the Visual Genome dataset by 26% while preserving real-time performance.",https://openaccess.thecvf.com/content/WACV2023/html/Adaimi_Composite_Relationship_Fields_With_Transformers_for_Scene_Graph_Generation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Adaimi_Composite_Relationship_Fields_With_Transformers_for_Scene_Graph_Generation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030807/,"['Visualization', 'Image recognition', 'Image synthesis', 'Semantics', 'Genomics', 'Transformers', 'Real-time systems']","['Composite Field', 'Scene Graph', 'Feature Maps', 'Object Detection', 'Top-down Approach', 'Bottom-up Approach', 'Output Feature Map', 'Bottom-up Methods', 'Convolutional Neural Network', 'Performance Of Method', 'Convolutional Layers', 'Bounding Box', 'Top-down And Bottom-up', 'Action Recognition', 'Subject Positions', 'Subject And Object', 'Objects In The Scene', 'Mean Of Recall', 'L1 Loss', 'Yellow Box', 'Top-down Methods', 'Detection Head', 'Transformer Encoder', 'Positional Encoding', 'Total Direct Effect', 'Deformable Convolution', 'Ground Truth Annotations', 'Objective Vector', 'Receptive Field', 'Output Position']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Arts/games/social media', 'Commercial/retail']",2,"Scene graph generation (SGG) methods extract relationships between objects. While most methods focus on improving top-down approaches, which build a scene graph based on detected objects from an off-the-shelf object detector, there is a limited amount of work on bottom-up approaches, which jointly detect objects and their relationships in a single stage.In this work, we present a novel bottom-up SGG approach by representing relationships using Composite Relationship Fields (CoRF). CoRF turns relationship detection into a dense regression and classification task, where each cell of the output feature map identifies surrounding objects and their relationships. Furthermore, we propose a refinement head that leverages Transformers for global scene reasoning, resulting in more meaningful relationship predictions. By combining both contributions, our method outperforms previous bottom-up methods on the Visual Genome dataset by 26% while preserving real-time performance."
Compressing Explicit Voxel Grid Representations: Fast NeRFs Become Also Small,"Chenxi Lola Deng, Enzo Tartaglione","LTCI, Télécum Paris, Institut Polytechnique de Paris",100,France,0,,"NeRFs have revolutionized the world of per-scene radiance field reconstruction because of their intrinsic compactness. One of the main limitations of NeRFs is their slow rendering speed, both at training and inference time. Recent research addressing this issue focuses on optimisation of an explicit voxel grid (EVG) that represents the scene, which can be paired with neural networks to learn radiance fields. This approach significantly enhances the speed both at train and inference time, but at the cost of large memory occupation. In this work we propose Re:NeRF, an approach specifically designed for targeting EVG-NeRFs compressibility, which aims to reduce memory storage of NeRF models while maintaining comparable performance. We benchmark our approach with three different EVG-NeRF architectures on three popular benchmarks, showing Re:NeRF's broad usability and effectiveness.",https://openaccess.thecvf.com/content/WACV2023/html/Deng_Compressing_Explicit_Voxel_Grid_Representations_Fast_NeRFs_Become_Also_Small_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Deng_Compressing_Explicit_Voxel_Grid_Representations_Fast_NeRFs_Become_Also_Small_WACV_2023_paper.pdf,,,2210.12782,main,Poster,https://ieeexplore.ieee.org/document/10030561/,"['Training', 'Computer vision', 'Costs', 'Computational modeling', 'Neural networks', 'Computer architecture', 'Benchmark testing']","['Explicit Representation', 'Voxel Grid', 'Neural Radiance Fields', 'Training Time', 'Memory Storage', 'Inference Time', 'Popularity', 'Model Parameters', 'Deep Neural Network', 'Computer Vision', '3D Reconstruction', 'Model Size', 'Multilayer Perceptron', 'Ergogenic', 'Lower Magnitude', 'High Compression', 'Quantile Function', 'Popular Datasets', 'Subset Of Parameters', 'Scene Representation', 'Popular Research Area', 'Computer Vision Research', 'Neural Field']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",22,"NeRFs have revolutionized the world of per-scene radiance field reconstruction because of their intrinsic compactness. One of the main limitations of NeRFs is their slow rendering speed, both at training and inference time. Recent research focuses on the optimization of an explicit voxel grid (EVG) that represents the scene, which can be paired with neural networks to learn radiance fields. This approach significantly enhances the speed both at train and inference time, but at the cost of large memory occupation. In this work we propose Re:NeRF, an approach that specifically targets EVG-NeRFs compressibility, aiming to reduce memory storage of NeRF models while maintaining comparable performance. We benchmark our approach with three different EVG-NeRF architectures on four popular benchmarks, showing Re:NeRF’s broad usability and effectiveness."
Computer Vision for International Border Legibility,"Trevor Ortega, Thomas Nelson, Skyler Crane, Josh Myers-Dean, Scott Wehrwein",University of Colorado Boulder; Western Washington University,100,USA,0,,"Key aspects of international policy, such as those pertaining to migration and trade, manifest in the physical world at international political borders; for this reason, borders are of interest to political science studying the impacts and implications of these policies. While some prior efforts have worked to characterize features of borders using trained human coders and crowdsourcing, these are limited in scale by the need for manual annotations. In this paper, we present a new task, dataset, and baseline approaches for estimating the legibility of international political borders automatically and on a global scale. Our contributions are to (1) define the border legibility estimation task; (2) collect a dataset of overhead (aerial) imagery for the entire world's international borders, (3) propose several classical and deep-learning-based approaches to establish a baseline for the task, and (4) evaluate our algorithms against a validation dataset of crowdsourced legibility comparisons. Our results on this challenging task confirm that while low-level features can often explain border legibility, mid- and high-level features are also important. Finally, we show preliminary results of a global analysis of legibility, confirming some of the political and geographic influences of legibility.",https://openaccess.thecvf.com/content/WACV2023/html/Ortega_Computer_Vision_for_International_Border_Legibility_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ortega_Computer_Vision_for_International_Border_Legibility_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030884/,"['Geography', 'Crowdsourcing', 'Computer vision', 'Annotations', 'Computational modeling', 'Estimation', 'Manuals']","['Computer Vision', 'International Borders', 'Political Science', 'Aerial Images', 'High-level Features', 'Low-level Features', 'Physical World', 'Political Borders', 'Pairwise Comparisons', 'Land Use', 'Validation Set', 'Cross-border', 'Image Pairs', 'Cluster Assignment', 'Ground Truth Labels', 'Global Dataset', 'Collection Quality', 'Self-supervised Learning', 'Side Of The Border', 'Local Border', 'Feature Pairs', 'Image Tiles', 'Self-supervised Approach', 'Single Tile', 'Siamese Network', 'RGB Pixel', 'Human Influence', 'Visual Features']","['Applications: Remote Sensing', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"Key aspects of international policy, such as those pertaining to migration and trade, manifest in the physical world at international political borders; for this reason, borders are of interest to political science studying the impacts and implications of these policies. While some prior efforts have worked to characterize features of borders using trained human coders and crowdsourcing, these are limited in scale by the need for manual annotations. In this paper, we present a new task, dataset, and baseline approaches for estimating the legibility of international political borders automatically and on a global scale. Our contributions are to (1) define the border legibility estimation task; (2) collect a dataset of overhead (aerial) imagery for the entire world’s international borders, (3) propose several classical and deep-learning-based approaches to establish a baseline for the task, and (4) evaluate our algorithms against a validation dataset of crowdsourced legibility comparisons. Our results on this challenging task confirm that while low-level features can often explain border legibility, mid- and high-level features are also important. Finally, we show preliminary results of a global analysis of legibility, confirming some of the political and geographic influences of legibility."
Computer Vision for Ocean Eddy Detection in Infrared Imagery,"Evangelos Moschos, Alisa Kugusheva, Paul Coste, Alexandre Stegner","AMPHITRITE, X-novation Center, Ecole Polytechnique, Palaiseau, France; IMT Atlantique, Avenue du Technopôle, Plouzané, France; AMPHITRITE, Ecole Polytechnique, LMD/CNRS, Avenue Coriolis, Palaiseau, France",66.66666667,France,33.33333333,France,"Reliable and precise detection of ocean eddies can significantly improve the monitoring of the ocean surface and subsurface dynamics, besides the characterization of local hydrographical and biological properties, or the concentration pelagic species. Today, most of the eddy detection algorithms operate on satellite altimetry gridded observations, which provide daily maps of sea surface height and surface geostrophic velocity. However, the reliability and the spatial resolution of altimetry products is limited by the strong spatio-temporal averaging of the mapping procedure. Yet, the availability of high-resolution satellite imagery makes real-time object detection possible at a much finer scale, via advanced computer vision methods. We propose a novel eddy detection method via a transfer learning schema, using the ground truth of high-resolution ocean numerical models to link the characteristic streamlines of eddies with their signature (gradients, swirls, and filaments) on Sea Surface Temperature (SST). A trained, multi-task convolutional neural network is then employed to segment infrared satellite imagery of SST in order to retreive the accurate position, size, and form of each detected eddy. The EddyScan-SST is an operational oceanographic module that provides, in real-time, key information on the ocean dynamics to maritime stakeholders.",https://openaccess.thecvf.com/content/WACV2023/html/Moschos_Computer_Vision_for_Ocean_Eddy_Detection_in_Infrared_Imagery_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Moschos_Computer_Vision_for_Ocean_Eddy_Detection_in_Infrared_Imagery_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030897/,"['Sea surface', 'Computer vision', 'Satellites', 'Neural networks', 'Real-time systems', 'Numerical models', 'Reliability']","['Computer Vision', 'Ocean Eddies', 'Eddy Detection', 'Neural Network', 'Numerical Simulations', 'Convolutional Neural Network', 'Object Detection', 'Sea Surface', 'Sea Surface Temperature', 'Satellite Imagery', 'Sea Surface Height', 'Ocean Dynamics', 'High-resolution Satellite Imagery', 'Satellite Altimetry', 'Dynamical', 'Flow Velocity', 'Intersection Over Union', 'Precision And Recall', 'Satellite Data', 'Synthetic Aperture Radar', 'Small Eddies', 'Sea Surface Temperature Data', 'Standard Method For Detection', 'Anticyclonic', 'Altimeter Data', 'Mesoscale Eddies', 'Position Error', 'Multi-task Learning', 'Contour Detection', 'Missing Rate']","['Applications: Remote Sensing', 'Environmental monitoring/climate change/ecology']",4,"Reliable and precise detection of ocean eddies can significantly improve the monitoring of the ocean surface and subsurface dynamics, besides the characterization of local hydrographical and biological properties, or the concentration pelagic species. Today, most of the eddy detection algorithms operate on satellite altimetry gridded observations, which provide daily maps of sea surface height and surface geostrophic velocity. However, the reliability and the spatial resolution of altimetry products is limited by the strong spatio-temporal averaging of the mapping procedure. Yet, the availability of high-resolution satellite imagery makes real-time object detection possible at a much finer scale, via advanced computer vision methods. We propose a novel eddy detection method via a transfer learning schema, using the ground truth of high-resolution ocean numerical models to link the characteristic streamlines of eddies with their signature (gradients, swirls, and filaments) on Sea Surface Temperature (SST). A trained, multi-task convolutional neural network is then employed to segment infrared satellite imagery of SST in order to retrieve the accurate position, size, and form of each detected eddy. The EddyScan-SST is an operational oceanographic module that provides, in real-time, key information on the ocean dynamics to maritime stakeholders."
Computer Vision to the Rescue: Infant Postural Symmetry Estimation From Incongruent Annotations,"Xiaofei Huang, Michael Wan, Lingfei Luan, Bethany Tunik, Sarah Ostadabbas","Board-Certified Clinical Specialist in Pediatric Physical Therapy; Augmented Cognition Lab (ACLab), Northeastern University, Boston, MA, USA",50,USA,50,USA,"Bilateral postural symmetry plays a key role as a potential risk marker for autism spectrum disorder (ASD) and as a symptom of congenital muscular torticollis (CMT) in infants, but current methods of assessing symmetry require laborious clinical expert assessments. In this paper, we develop a computer vision based infant symmetry assessment system, leveraging 3D human pose estimation for infants. Evaluation and calibration of our system against ground truth assessments is complicated by our findings from a survey of human ratings of angle and symmetry, that such ratings exhibit low inter-rater reliability. To rectify this, we develop a Bayesian estimator of the ground truth derived from a probabilistic graphical model of fallible human raters. We show that the 3D infant pose estimation model can achieve 68% area under the receiver operating characteristic curve performance in predicting the Bayesian aggregate labels, compared to only 61% from a 2D infant pose estimation model and 60% from a 3D adult pose estimation model, highlighting the importance of 3D poses and infant domain knowledge in assessing infant body symmetry. Our survey analysis also suggests that human ratings are susceptible to higher levels of bias and inconsistency, and hence our final 3D pose-based symmetry assessment system is calibrated but not directly supervised by Bayesian aggregate human ratings, yielding higher levels of consistency and lower levels of inter-limb assessment bias.",https://openaccess.thecvf.com/content/WACV2023/html/Huang_Computer_Vision_to_the_Rescue_Infant_Postural_Symmetry_Estimation_From_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Huang_Computer_Vision_to_the_Rescue_Infant_Postural_Symmetry_Estimation_From_WACV_2023_paper.pdf,,https://github.com/ostadabbas/Infant-Postural-Symmetry,2207.09352,main,Poster,https://ieeexplore.ieee.org/document/10030329/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Aggregates', 'Computational modeling', 'Pose estimation', 'Predictive models']","['Computer Vision', 'Human Evaluation', 'Pose Estimation', 'Aggregation Rate', 'Level Of Bias', 'Human Pose Estimation', 'Human Pose', 'Probabilistic Graphical Models', '3D Pose', 'Infant Body', 'Low Inter-rater Reliability', 'Body Symmetry', 'Internal Consistency', 'Lower Limb', 'Upper Arm', 'High Internal Consistency', 'Angle Difference', '3D Data', 'Majority Voting', 'Lower Arm', '2D Pose', 'Upper Leg', '3D Skeleton', 'Symmetry Measures', 'Ground Truth 3D', 'Collective Agreements', 'Weak Labels', 'Body Pose', 'Part Of The Limb', 'Angle Threshold']","['Applications: Biomedical/healthcare/medicine', '3D computer vision', 'Psychology and cognitive science']",8,"Bilateral postural symmetry plays a key role as a potential risk marker for autism spectrum disorder (ASD) and as a symptom of congenital muscular torticollis (CMT) in infants, but current methods of assessing symmetry require laborious clinical expert assessments. In this paper, we develop a computer vision based infant symmetry assessment system, leveraging 3D human pose estimation for infants. Evaluation and calibration of our system against ground truth assessments is complicated by our findings from a survey of human ratings of angle and symmetry, that such ratings exhibit low inter-rater reliability. To rectify this, we develop a Bayesian estimator of the ground truth derived from a probabilistic graphical model of fallible human raters. We show that the 3D infant pose estimation model can achieve 68% area under the receiver operating characteristic curve performance in predicting the Bayesian aggregate labels, compared to only 61% from a 2D infant pose estimation model and 60% from a 3D adult pose estimation model, highlighting the importance of 3D poses and infant domain knowledge in assessing infant body symmetry. Our survey analysis also suggests that human ratings are susceptible to higher levels of bias and inconsistency, and hence our final 3D pose-based symmetry assessment system is calibrated but not directly supervised by Bayesian aggregate human ratings, yielding higher levels of consistency and lower levels of inter-limb assessment bias
<sup>1</sup>
."
Concept Correlation and Its Effects on Concept-Based Models,"Lena Heidemann, Maureen Monnet, Karsten Roscher","Fraunhofer IKS, Munich, Germany",0,,100,Germany,"Concept-based learning approaches for image classification, such as Concept Bottleneck Models, aim to enable interpretation and increase robustness by directly learning high-level concepts which are used for predicting the main class. They achieve competitive test accuracies compared to standard end-to-end models. However, with multiple concepts per image and binary concept annotations (without concept localization), it is not evident if the output of the concept model is truly based on the predicted concepts or other features in the image. Additionally, high correlations between concepts would allow a model to predict a concept with high test accuracy by simply using a correlated concept as a proxy. In this paper, we analyze these correlations between concepts in the CUB and GTSRB datasets and propose methods beyond test accuracy for evaluating their effects on the performance of a concept-based model trained on this data. To this end, we also perform a more detailed analysis on the effects of concept correlation using synthetically generated datasets of 3D shapes. We see that high concept correlation increases the risk of a model's inability to distinguish these concepts. Yet simple techniques, like loss weighting, show promising initial results for mitigating this issue.",https://openaccess.thecvf.com/content/WACV2023/html/Heidemann_Concept_Correlation_and_Its_Effects_on_Concept-Based_Models_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Heidemann_Concept_Correlation_and_Its_Effects_on_Concept-Based_Models_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031009/,"['Location awareness', 'Correlation', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Predictive models', 'Robustness']","['Image Features', 'Test Accuracy', '3D Shape', 'Multiple Concepts', 'Higher Test Accuracy', 'Training Data', 'Independent Samples', 'Convolutional Neural Network', 'Highly Correlated', 'Low Accuracy', 'Highest Correlation', 'Difference In Accuracy', 'Pairwise Correlations', 'Multilayer Perceptron', 'Model Interpretation', 'Latent Space', 'Real-world Datasets', 'Sample Loss', 'Balanced Accuracy', 'Saliency Map', 'Impact Of Correlation', 'Concept Pairs', 'Concept Of Image', 'Accuracy Drop', 'Post-hoc Method', 'Deep Neural Network', '3D Datasets', 'Concept Of Learning', 'Side Effects']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"Concept-based learning approaches for image classification, such as Concept Bottleneck Models, aim to enable interpretation and increase robustness by directly learning high-level concepts which are used for predicting the main class. They achieve competitive test accuracies compared to standard end-to-end models. However, with multiple concepts per image and binary concept annotations (without concept localization), it is not evident if the output of the concept model is truly based on the predicted concepts or other features in the image. Additionally, high correlations between concepts would allow the model to predict a concept with high test accuracy by simply using a correlated concept as a proxy. In this paper, we analyze these correlations between concepts in the CUB and GTSRB datasets and propose methods beyond test accuracy for evaluating their effects on the performance of a concept-based model trained on this data. To this end, we also perform a more detailed analysis on the effects of concept correlation using synthetically generated datasets of 3D shapes. We see that high concept correlation increases the risk of a model’s inability to distinguish these concepts. Yet simple techniques, like loss weighting, show promising initial results for mitigating this issue."
ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-Based Mixing,"Giulio Mattolin, Luca Zanella, Elisa Ricci, Yiming Wang","University of Trento, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy",100,Italy,0,,"Unsupervised Domain Adaptation (UDA) for object detection aims to adapt a model trained on a source domain to detect instances from a new target domain for which annotations are not available. Different from traditional approaches, we propose ConfMix, the first method that introduces a sample mixing strategy based on region-level detection confidence for adaptive object detector learning. We mix the local region of the target sample that corresponds to the most confident pseudo detections with a source image, and apply an additional consistency loss term to gradually adapt towards the target data distribution. In order to robustly define a confidence score for a region, we exploit the confidence score per pseudo detection that accounts for both the detector-dependent confidence and the bounding box uncertainty. Moreover, we propose a novel pseudo labelling scheme that progressively filters the pseudo target detections using the confidence metric that varies from a loose to strict manner along the training. We perform extensive experiments with three datasets, achieving state-of-the-art performance in two of them and approaching the supervised target model performance in the other. Code is available at https://github.com/giuliomattolin/ConfMix.",https://openaccess.thecvf.com/content/WACV2023/html/Mattolin_ConfMix_Unsupervised_Domain_Adaptation_for_Object_Detection_via_Confidence-Based_Mixing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mattolin_ConfMix_Unsupervised_Domain_Adaptation_for_Object_Detection_via_Confidence-Based_Mixing_WACV_2023_paper.pdf,,https://github.com/giuliomattolin/ConfMix,2210.11539,main,Poster,https://ieeexplore.ieee.org/document/10030556/,"['Measurement', 'Training', 'Adaptation models', 'Computer vision', 'Uncertainty', 'Codes', 'Object detection']","['Object Detection', 'Domain Adaptation', 'Target Sample', 'Bounding Box', 'Confidence Score', 'Source Images', 'Target Domain', 'Target Data', 'Adaptive Learning', 'Mixed Strategy', 'Source Domain', 'Consistency Loss', 'Pseudo Labels', 'Detection Confidence', 'Computer Vision', 'Target Region', 'Data Augmentation', 'Mixed Samples', 'Generative Adversarial Networks', 'Target Image', 'Non-maximum Suppression', 'Training Adaptations', 'Confidence Region', 'Detection In Order', 'Reliable Detection', 'Faster R-CNN', 'Object Detection Approaches', 'Two-stage Object Detection', 'Detector Set', 'Confidence Threshold']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",31,"Unsupervised Domain Adaptation (UDA) for object detection aims to adapt a model trained on a source domain to detect instances from a new target domain for which annotations are not available. Different from traditional approaches, we propose ConfMix, the first method that introduces a sample mixing strategy based on region-level detection confidence for adaptive object detector learning. We mix the local region of the target sample that corresponds to the most confident pseudo detections with a source image, and apply an additional consistency loss term to gradually adapt towards the target data distribution. In order to robustly define a confidence score for a region, we exploit the confidence score per pseudo detection that accounts for both the detector-dependent confidence and the bounding box uncertainty. Moreover, we propose a novel pseudo labelling scheme that progressively filters the pseudo target detections using the confidence metric that varies from a loose to strict manner along the training. We perform extensive experiments with three datasets, achieving state-of-the-art performance in two of them and approaching the supervised target model performance in the other. Code is available at https://github.com/giuliomattolin/ConfMix."
Content-Based Music-Image Retrieval Using Self- and Cross-Modal Feature Embedding Memory,"Takayuki Nakatsuka, Masahiro Hamasaki, Masataka Goto",National Institute of Advanced Industrial Science and Technology (AIST),100,Japan,0,,"This paper describes a method based on deep metric learning for content-based cross-modal retrieval of a piece of music and its representative image (i.e., a music audio signal and its cover art image). We train music and image encoders so that the embeddings of a positive music-image pair lie close to each other, while those of a random pair lie far from each other, in a shared embedding space. Furthermore, we propose a mechanism called self- and cross-modal feature embedding memory, which stores both the music and image embeddings of any previous iterations in memory and enables the encoders to mine informative pairs for training. To perform such training, we constructed a dataset containing 78,325 music-image pairs. We demonstrate the effectiveness of the proposed mechanism on this dataset: specifically, our mechanism outperforms baseline methods by 1.93 3.38 times for the mean reciprocal rank, 2.19 3.56 times for recall@50, and 528 891 ranks for the median rank.",https://openaccess.thecvf.com/content/WACV2023/html/Nakatsuka_Content-Based_Music-Image_Retrieval_Using_Self-_and_Cross-Modal_Feature_Embedding_Memory_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nakatsuka_Content-Based_Music-Image_Retrieval_Using_Self-_and_Cross-Modal_Feature_Embedding_Memory_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031003/,"['Training', 'Measurement', 'Computer vision', 'Art', 'Multiple signal classification', 'Task analysis']","['Baseline Methods', 'Metric Learning', 'Median Rank', 'Image Encoder', 'Deep Metric Learning', 'Loss Function', 'Training Set', 'Validation Set', 'Metadata', 'Comparative Experiments', 'Data Augmentation', 'Similarity Matrix', 'RGB Images', 'Backbone Network', 'Current Iteration', 'Types Of Pairs', 'Short-time Fourier Transform', 'Memory Size', 'Memory Mechanisms', 'Service Platform', 'Positive Instances', 'Parameters Of The Encoder', 'Original Pair', 'Input Encoding', 'Musical Excerpts', 'Image Retrieval']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"This paper describes a method based on deep metric learning for content-based cross-modal retrieval of a piece of music and its representative image (i.e., a music audio signal and its cover art image). We train music and image encoders so that the embeddings of a positive music-image pair lie close to each other, while those of a random pair lie far from each other, in a shared embedding space. Furthermore, we propose a mechanism called self- and cross-modal feature embedding memory, which stores both the music and image embeddings of any previous iterations in memory and enables the encoders to mine informative pairs for training. To perform such training, we constructed a dataset containing 78,325 music-image pairs. We demonstrate the effectiveness of the proposed mechanism on this dataset: specifically, our mechanism outperforms baseline methods by ×1.93 ∼ 3.38 for the mean reciprocal rank, ×2.19 ∼ 3.56 for recall@50, and 528 ∼ 891 ranks for the median rank."
Context-Empowered Visual Attention Prediction in Pedestrian Scenarios,"Igor Vozniak, Philipp Müller, Lorena Hell, Nils Lipp, Ahmed Abouelazm, Christian Müller",German Research Center for Artificial Intelligence (DFKI),0,,100,Germany,"Effective and flexible allocation of visual attention is key for pedestrians who have to navigate to a desired goal under different conditions of urgency and safety preferences. While automatic modelling of pedestrian attention holds great promise to improve simulations of pedestrian behavior, current saliency prediction approaches mostly focus on generic free-viewing scenarios and do not reflect the specific challenges present in pedestrian attention prediction. In this paper, we present Context-SalNET, a novel encoder-decoder architecture that explicitly addresses three key challenges of visual attention prediction in pedestrians: First, Context-SalNET explicitly models the context factors urgency and safety preference in the latent space of the encoder-decoder model. Second, we propose the exponentially weighted mean squared error loss (ew-MSE) that is able to better cope with the fact that only a small part of the ground truth saliency maps consist of non-zero entries. Third, we explicitly model epistemic uncertainty to account for the fact that training data for pedestrian attention prediction is limited. To evaluate Context-SalNET, we recorded the first dataset of pedestrian visual attention in VR that includes explicit variation of the context factors urgency and safety preference. Context-SalNET achieves clear improvements over state-of-the-art saliency prediction approaches as well as over ablations. Our novel dataset will be made fully available and can serve as a valuable resource for further research on pedestrian attention prediction.",https://openaccess.thecvf.com/content/WACV2023/html/Vozniak_Context-Empowered_Visual_Attention_Prediction_in_Pedestrian_Scenarios_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Vozniak_Context-Empowered_Visual_Attention_Prediction_in_Pedestrian_Scenarios_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030353/,"['Visualization', 'Uncertainty', 'Navigation', 'Training data', 'Predictive models', 'Safety', 'Behavioral sciences']","['Visual Attention', 'Pedestrian Scenario', 'Visual Attention Prediction', 'Ablation', 'Mean Square Error', 'Contextual Factors', 'Clear Improvement', 'Saliency Map', 'Mean Square Error Loss', 'Epistemic Uncertainty', 'Pedestrian Behavior', 'Quantitative Evaluation', 'Model Uncertainty', 'Importance Of Context', 'Static Images', 'Backbone Network', 'Traffic Light', 'Roll Angle', 'Navigation Task', 'Time Of Submission', 'Human Attention', 'Road Crossings', 'Traffic Scenarios', 'Center Bias', 'Bike Lanes', 'Road Layout', 'Virtual Reality Headset', 'Multiple Lanes', 'Critical Scenarios', 'Traffic Situation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",,"Effective and flexible allocation of visual attention is key for pedestrians who have to navigate to a desired goal under different conditions of urgency and safety preferences. While automatic modelling of pedestrian attention holds great promise to improve simulations of pedestrian behavior, current saliency prediction approaches mostly focus on generic free-viewing scenarios and do not reflect the specific challenges present in pedestrian attention prediction. In this paper, we present Context-SalNET, a novel encoder-decoder architecture that explicitly addresses three key challenges of visual attention prediction in pedestrians: First, Context-SalNET explicitly models the context factors urgency and safety preference in the latent space of the encoder-decoder model. Second, we propose the exponentially weighted mean squared error loss (ew-MSE) that is able to better cope with the fact that only a small part of the ground truth saliency maps consist of non-zero entries. Third, we explicitly model epistemic uncertainty to account for the fact that training data for pedestrian attention prediction is limited. To evaluate Context-SalNET, we recorded the first dataset of pedestrian visual attention in VR that includes explicit variation of the context factors urgency and safety preference. Context-SalNET achieves clear improvements over state-of-the-art saliency prediction approaches as well as over ablations. Our novel dataset will be made fully available and can serve as a valuable resource for further research on pedestrian attention prediction."
Continual Learning With Dependency Preserving Hypernetworks,"Dupati Srikar Chandra, Sakshi Varshney, P. K. Srijith, Sunil Gupta","Department of Artificial Intelligence, Indian Institute of Technology Hyderabad, India; Computer Science and Engineering, Indian Institute of Technology Hyderabad, India; Applied Artificial Intelligence Institute, Deakin University, Australia",100,"Australia, India",0,,"Humans learn continually throughout their lifespan by accumulating diverse knowledge and fine-tuning it for future tasks. When presented with a similar goal, neural networks suffer from catastrophic forgetting if data distributions across sequential tasks are not stationary over the course of learning. An effective approach to address such continual learning (CL) problems is to use hypernetworks which generate task dependent weights for a target network. However, the continual learning performance of existing hypernetwork based approaches are affected by the assumption of independence of the weights across the layers in order to maintain parameter efficiency. To address this limitation, we propose a novel approach that uses a dependency preserving hypernetwork to generate weights for the target network while also maintaining the parameter efficiency. We propose to use recurrent neural network (RNN) based hypernetwork that can generate layer weights efficiently while allowing for dependencies across them. In addition, we propose novel regularisation and network growth techniques for the RNN based hypernetwork to further improve the continual learning performance. To demonstrate the effectiveness of the proposed methods, we conducted experiments on several image classification continual learning tasks and settings. We found that the proposed methods based on the RNN hypernetworks outperformed the baselines in all these CL settings and tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Chandra_Continual_Learning_With_Dependency_Preserving_Hypernetworks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chandra_Continual_Learning_With_Dependency_Preserving_Hypernetworks_WACV_2023_paper.pdf,,,2209.07712,main,Poster,https://ieeexplore.ieee.org/document/10030950/,"['Computer vision', 'Recurrent neural networks', 'Computational modeling', 'Task analysis', 'Image classification']","['Incremental Learning', 'Neural Network', 'Image Classification', 'Recurrent Neural Network', 'Network Growth', 'Sequential Task', 'Continuous Performance', 'Regularization Techniques', 'Catastrophic Forgetting', 'Important Parameter', 'Training Time', 'Weight Matrix', 'Improvement In Accuracy', 'Long Short-term Memory', 'Generative Adversarial Networks', 'Regularization Term', 'Network Weights', 'Hidden State', 'Fisher Information', 'Parameter Update', 'Fisher Information Matrix', 'Variational Autoencoder', 'Task Parameters', 'Small Chunks', 'Previous Tasks', 'Fully-connected Network', 'Regularization Constant', 'Synthetic Examples', 'Regularization Loss', 'Regularization Approach']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Humans learn continually throughout their lifespan by accumulating diverse knowledge and fine-tuning it for future tasks. When presented with a similar goal, neural networks suffer from catastrophic forgetting if data distributions across sequential tasks are not stationary over the course of learning. An effective approach to address such continual learning (CL) problems is to use hypernetworks which generate task dependent weights for a target network. However, the continual learning performance of existing hypernetwork based approaches are affected by the assumption of independence of the weights across the layers in order to maintain parameter efficiency. To address this limitation, we propose a novel approach that uses a dependency preserving hypernetwork to generate weights for the target network while also maintaining the parameter efficiency. We propose to use recurrent neural network (RNN) based hypernetwork that can generate layer weights efficiently while allowing for dependencies across them. In addition, we propose novel regularisation and network growth techniques for the RNN based hypernetwork to further improve the continual learning performance. To demonstrate the effectiveness of the proposed methods, we conducted experiments on several image classification continual learning tasks and settings. We found that the proposed methods based on the RNN hypernetworks outperformed the baselines in all these CL settings and tasks."
Contrastive Knowledge-Augmented Meta-Learning for Few-Shot Classification,"Rakshith Subramanyam, Mark Heimann, T.S. Jayram, Rushil Anirudh, Jayaraman J. Thiagarajan","Lawrence Livermore National Laboratory, CA, USA; Arizona State University, AZ, USA",100,USA,0,,"Model agnostic meta-learning algorithms aim to infer priors from several observed tasks that can then be used to adapt to a new task with few examples. Given the inherent diversity of tasks arising in existing benchmarks, recent methods have resorted to task-specific adaptation of the prior. Our goal is to improve generalization of meta learners when the task distribution contains challenging distribution shifts and semantic disparities. To this end, we introduce CAML (Contrastive Knowledge-Augmented Meta Learning), a knowledge-enhanced few-shot learning approach that evolves a knowledge graph to encode historical experience, and employs a contrastive distillation strategy to leverage the encoded knowledge for task-aware modulation of the base learner. In addition to the standard few-shot task adaptation, we also consider the more challenging multi-domain task adaptation and few-shot dataset generalization settings in our evaluation with standard benchmarks. Our empirical study shows that CAML (i) enables simple task encoding schemes; (ii) eliminates the need for knowledge extraction at inference time; and most importantly, (iii) effectively aggregates historical experience thus leading to improved performance in both multi-domain adaptation and dataset generalization.",https://openaccess.thecvf.com/content/WACV2023/html/Subramanyam_Contrastive_Knowledge-Augmented_Meta-Learning_for_Few-Shot_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Subramanyam_Contrastive_Knowledge-Augmented_Meta-Learning_for_Few-Shot_Classification_WACV_2023_paper.pdf,,https://github.com/Rakshith-2905/CAML,2207.12346,main,Poster,https://ieeexplore.ieee.org/document/10030437/,"['Computer vision', 'Aggregates', 'Semantics', 'Prototypes', 'Modulation', 'Benchmark testing', 'Encoding']","['Few-shot Classification', 'Historical Experience', 'Inference Time', 'Base Learners', 'Self-supervised Learning', 'Standard Benchmark', 'Knowledge Extraction', 'Distribution Of Tasks', 'Few-shot Learning', 'Meta Learning', 'Training Set', 'Knowledge Of Structure', 'Latent Space', 'Learnable Parameters', 'Training Tasks', 'Graph Convolutional Network', 'Node Features', 'Graph Features', 'Support Set', 'Query Set', 'Task Representations', 'Training Episodes', 'Unseen Classes', 'Image Embedding', 'Gradient Step']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",,"Model agnostic meta-learning algorithms aim to infer priors from several observed tasks that can then be used to adapt to a new task with few examples. Given the inherent diversity of tasks arising in existing benchmarks, recent methods have resorted to task-specific adaptation of the prior. Our goal is to improve generalization of meta learners when the task distribution contains challenging distribution shifts and semantic disparities. To this end, we introduce CAML (Contrastive Knowledge-Augmented Meta Learning), a knowledge-enhanced few-shot learning approach that evolves a knowledge graph to encode historical experience, and employs a contrastive distillation strategy to leverage the encoded knowledge for task-aware modulation of the base learner. In addition to the standard few-shot task adaptation, we also consider the more challenging multi-domain task adaptation and few-shot dataset generalization settings in our evaluation with standard benchmarks. Our empirical study shows that CAML (i) enables simple task encoding schemes; (ii) eliminates the need for knowledge extraction at inference time; and most importantly, (iii) effectively aggregates historical experience thus leading to improved performance in both multi-domain adaptation and dataset generalization."
Contrastive Learning of Semantic Concepts for Open-Set Cross-Domain Retrieval,"Aishwarya Agarwal, Srikrishna Karanam, Balaji Vasan Srinivasan, Biplab Banerjee","Indian Institute of Technology Bombay, Mumbai India; Adobe Research, Bangalore India",50,India,50,USA,"We consider the problem of image retrieval where query images during testing belong to classes and domains both unseen during training. This requires learning a feature space that has the ability to generalize across both classes and domains. To this end, we propose semantic contrastive concept network (SCNNet), a new learning framework that helps take a step towards class and domain generalization in a principled fashion. Unlike existing methods that rely on global object representations, SCNNet proposes to learn a set of local concept vectors to facilitate unseen-class generalization. To this end, SCNNet's key innovations include (a) a novel trainable local concept extraction module that learns an orthonormal set of basis vectors, and (b) computes local features for any unseen-class data as a linear combination of the learned basis set. Next, to enable unseen-domain generalization, SCNNet proposes to generate supervisory signals from an adjacent data modality, i.e., natural language, by mining freely available textual label information associated with images. SCNNet derives these signals from our novel trainable semantic ordinal distance constraints that ensure semantic consistency between pairs of images sampled from different domains. Both the proposed modules above enable end-to-end training of the SCNNet, resulting in a model that helps establish state-of-the-art performance on the standard DomainNet, PACS, and Sketchy benchmark datasets with average Prec@200 improvements of 42.6%, 6.5%, and 13.6% respectively over the most recently reported results.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Contrastive_Learning_of_Semantic_Concepts_for_Open-Set_Cross-Domain_Retrieval_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Contrastive_Learning_of_Semantic_Concepts_for_Open-Set_Cross-Domain_Retrieval_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030515/,"['Training', 'Technological innovation', 'Computer vision', 'Semantics', 'Natural languages', 'Image retrieval', 'Feature extraction']","['Concept Of Learning', 'Semantic Knowledge', 'Self-supervised Learning', 'Cross-domain Retrieval', 'Local Features', 'Natural Language', 'Feature Space', 'Benchmark Datasets', 'Image Retrieval', 'Semantic Network', 'Local Module', 'Domain Generalization', 'Global Representation', 'Query Image', 'Local Concepts', 'Semantic Distance', 'Concept Extraction', 'Image Features', 'Feature Maps', 'Singular Value', 'Unseen Domains', 'Unseen Classes', 'Search Settings', 'Dolphins', 'Image X', 'Text Modality', 'Related Constraints', 'Conceptual Representations', 'Image Representation', 'Rainbow']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"We consider the problem of image retrieval where query images during testing belong to classes and domains both unseen during training. This requires learning a feature space that has the ability to generalize across both classes and domains together. To this end, we propose semantic contrastive concept network (SCNNet), a new learning framework that helps take a step towards class and domain generalization in a principled fashion. Unlike existing methods that rely on global object representations, SCNNet proposes to learn local feature vectors to facilitate unseen-class generalization. To this end, SCNNet’s key innovations include (a) a novel trainable local concept extraction module that learns an orthonormal set of basis vectors, and (b) computes local features for any unseen-class data as a linear combination of the learned basis set. Next, to enable unseen-domain generalization, SCNNet proposes to generate supervisory signals from an adjacent data modality, i.e., natural language, by mining freely available textual label information associated with images. SCNNet derives these signals from our novel trainable semantic ordinal distance constraints that ensure semantic consistency between pairs of images sampled from different domains. Both the proposed modules above enable end-to-end training of the SC-NNet, resulting in a model that helps establish state-of-the-art performance on the standard DomainNet, PACS, and Sketchy benchmark datasets with average Prec@200 improvements of 42.6%, 6.5%, and 13.6% respectively over the most recently reported results."
Contrastive Losses Are Natural Criteria for Unsupervised Video Summarization,"Zongshang Pang, Yuta Nakashima, Mayu Otani, Hajime Nagahara","Osaka University; CyberAgent, Inc.",50,Japan,50,Japan,"Video summarization aims to select a most informative subset of frames in a video to facilitate efficient video browsing. Unsupervised methods usually rely on heuristic training objectives such as diversity and representativeness. However, such methods need to bootstrap the online-generated summaries to compute the objectives for importance score regression. We consider such a pipeline inefficient and seek to directly quantify the frame-level importance with the help of contrastive losses in the representation learning literature. Leveraging the contrastive losses, we propose three metrics featuring a desirable key frame: local dissimilarity, global consistency, and uniqueness. With features pre-trained on an image classification task, the metrics can already yield high-quality importance scores, demonstrating better or competitive performance compared with past heavily-trained methods. We show that by refining the pre-trained features with contrastive learning, the frame-level importance scores can be further improved, and the model can learn from random videos and generalize to test videos with decent performance.",https://openaccess.thecvf.com/content/WACV2023/html/Pang_Contrastive_Losses_Are_Natural_Criteria_for_Unsupervised_Video_Summarization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pang_Contrastive_Losses_Are_Natural_Criteria_for_Unsupervised_Video_Summarization_WACV_2023_paper.pdf,,,2211.10056,main,Poster,https://ieeexplore.ieee.org/document/10030858/,"['Measurement', 'Representation learning', 'Training', 'Computer vision', 'Refining', 'Pipelines', 'Task analysis']","['Contrastive Loss', 'Video Summarization', 'Unsupervised Video Summarization', 'Unsupervised Methods', 'Representation Learning', 'Video Frames', 'Competitive Performance', 'Importance Scores', 'Self-supervised Learning', 'Image Classification Tasks', 'Key Frames', 'Global Consistency', 'Pre-trained Feature', 'Convolutional Network', 'Convolutional Neural Network', 'Rank Correlation', 'F1 Score', 'Unsupervised Learning', 'Traffic Accidents', 'Recurrent Neural Network', 'Local Neighborhood', 'Kinds Of Features', 'Set Of Frames', 'Diverse Information', 'Alignment Loss', 'Semantic Similarity', 'Deep Features', 'Hypersphere', 'Street Scenes', 'Frame Features']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",3,"Video summarization aims to select the most informative subset of frames in a video to facilitate efficient video browsing. Unsupervised methods usually rely on heuristic training objectives such as diversity and representativeness. However, such methods need to bootstrap the online-generated summaries to compute the objectives for importance score regression. We consider such a pipeline inefficient and seek to directly quantify the frame-level importance with the help of contrastive losses in the representation learning literature. Leveraging the contrastive losses, we propose three metrics featuring a desirable key frame: local dissimilarity, global consistency, and uniqueness. With features pre-trained on the image classification task, the metrics can already yield high-quality importance scores, demonstrating competitive or better performance than past heavily-trained methods. We show that by refining the pre-trained features with a lightweight contrastively learned projection module, the frame-level importance scores can be further improved, and the model can also leverage a large number of random videos and generalize to test videos with decent performance."
Control-NeRF: Editable Feature Volumes for Scene Rendering and Manipulation,"Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey Tulyakov, Gerard Pons-Moll","University of Tübingen, Max Planck Institute for Informatics, Saarland Informatics Campus; Snap Inc.",50,Germany,50,USA,"We present Control-NeRF, a method for performing flexible, 3D-aware image content manipulation while enabling high-quality novel view synthesis, from a set of posed input images. NeRF-based approaches are effective for novel view synthesis, however such models memorize the radiance for every point in a scene within a neural network. Since these models are scene-specific and lack a 3D scene representation, classical editing such as shape manipulation, or combining scenes is not possible. While there are some recent hybrid approaches that combine NeRF with external scene representations such as sparse voxels, planes, hash tables, etc., they focus mostly on efficiency and don't explore the scene editing and manipulation capabilities of hybrid approaches. With the aim of exploring controllable scene representations for novel view synthesis, our model couples learnt scene-specific 3D feature volumes with a general NeRF rendering network. We can generalize to novel scenes by optimizing only the scene-specific 3D feature volume, while keeping the parameters of the rendering network fixed. Since the feature volumes are independent of the rendering model, we can manipulate and combine scenes by editing their corresponding feature volumes. The edited volume can then be plugged into the rendering model to synthesise high-quality novel views. We demonstrate scene manipulations including: scene mixing; applying rigid and non-rigid transformations; inserting, moving and deleting objects in a scene; while producing photo-realistic novel-view synthesis results.",https://openaccess.thecvf.com/content/WACV2023/html/Lazova_Control-NeRF_Editable_Feature_Volumes_for_Scene_Rendering_and_Manipulation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lazova_Control-NeRF_Editable_Feature_Volumes_for_Scene_Rendering_and_Manipulation_WACV_2023_paper.pdf,https://virtualhumans.mpi-inf.mpg.de/control-nerf/,,,main,Poster,https://ieeexplore.ieee.org/document/10030510/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Image synthesis', 'Neural networks', 'Lighting']","['Feature Volume', 'Neural Network', 'Input Image', 'Corresponding Points', 'Hash Function', 'Rigid Transformation', 'Objects In The Scene', '3D Scene', 'Scene Representation', 'Scene Point', 'View Synthesis', 'Total Loss', 'Point Cloud', 'Deep Features', 'Neural Representations', 'Peak Signal-to-noise Ratio', 'Scene Images', 'Complex Background', 'Latent Representation', 'Image Synthesis', 'Real Scenes', 'View Of The Scene', 'Training Scenes', 'Feature Vector Of Length', 'Total Variation Regularization', 'Neural Field', 'Scene Content', 'Voxel Grid', 'Fine Network', 'Single Scene']","['Algorithms: 3D computer vision', 'Virtual/augmented reality', 'Visualization']",29,"We present Control-NeRF
<sup>1</sup>
, a method for performing flexible, 3D-aware image content manipulation while enabling high-quality novel view synthesis, from a set of posed input images. NeRF-based approaches [23] are effective for novel view synthesis, however such models memorize the radiance for every point in a scene within a neural network. Since these models are scene-specific and lack a 3D scene representation, classical editing such as shape manipulation, or combining scenes is not possible. While there are some recent hybrid approaches that combine NeRF with external scene representations such as sparse voxels, planes, hash tables, etc. [16], [5], [24], [9], they focus mostly on efficiency and don't explore the scene editing and manipulation capabilities of hybrid approaches. With the aim of exploring controllable scene representations for novel view synthesis, our model couples learnt scene-specific 3D feature volumes with a general NeRF rendering network. We can generalize to novel scenes by optimizing only the scene-specific 3D feature volume, while keeping the parameters of the rendering network fixed. Since the feature volumes are independent of the rendering model, we can manipulate and combine scenes by editing their corresponding feature volumes. The edited volume can then be plugged into the rendering model to synthesize high-quality novel views. We demonstrate scene manipulations including: scene mixing; applying rigid and non-rigid transformations; inserting, moving and deleting objects in a scene; while producing photo-realistic novel-view synthesis results."
Controllable 3D Generative Adversarial Face Model via Disentangling Shape and Appearance,"Fariborz Taherkhani, Aashish Rai, Quankai Gao, Shaunak Srivastava, Xuanbai Chen, Fernando de la Torre, Steven Song, Aayush Prakash, Daeil Kim",Facebook/Meta; Carnegie Mellon University,50,USA,50,USA,"3D face modeling has been an active area of research in computer vision and computer graphics, fueling applications ranging from facial expression transfer in virtual avatars to synthetic data generation. Existing 3D deep learning generative models (e.g., VAE, GANs) allow generating compact face representations (both shape and texture) that can model non-linearities in the shape and appearance space (e.g., scatter effects, specularities,..). However, they lack the capability to control the generation of subtle expressions. This paper proposes a new 3D face generative model that can decouple identity and expression and provides granular control over expressions. In particular, we propose using a pair of supervised auto-encoder and generative adversarial networks to produce high-quality 3D faces, both in terms of appearance and shape. Experimental results in the generation of 3D faces learned with holistic expression labels, or Action Unit (AU) labels, show how we can decouple identity and expression; gaining fine-control over expressions while preserving identity.",https://openaccess.thecvf.com/content/WACV2023/html/Taherkhani_Controllable_3D_Generative_Adversarial_Face_Model_via_Disentangling_Shape_and_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Taherkhani_Controllable_3D_Generative_Adversarial_Face_Model_via_Disentangling_Shape_and_WACV_2023_paper.pdf,,https://aashishrai3799.github.io/3DFaceCAM,2208.14263,main,Poster,https://ieeexplore.ieee.org/document/10030247/,"['Solid modeling', 'Computer vision', 'Gold', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Rendering (computer graphics)']","['Deep Learning', 'Computer Vision', 'Facial Expressions', 'Generative Adversarial Networks', 'Compact Representation', 'Computer Graphics', 'Generation Of Results', 'Virtual Avatar', 'Action Units', 'Subtle Expressions', 'Computer Vision Research', '3D Face', 'Identification Of Factors', 'Class Labels', 'Latent Space', 'Identification Code', 'Intensity Of Expression', 'Human Faces', '3D Coordinates', '3D Shape', 'Texture Map', 'Class Identity', 'Facial Action Coding System', 'Facial Shape', 'Fully-connected Network', 'Identity Space', 'Style Transfer', 'Embedding Vectors', 'Quantitative Metrics']","['Algorithms: 3D computer vision', 'Biometrics', 'face', 'gesture', 'body pose']",5,"3D face modeling has been an active area of research in computer vision and computer graphics, fueling applications ranging from facial expression transfer in virtual avatars to synthetic data generation. Existing 3D deep learning generative models (e.g., VAE, GANs) allow generating compact face representations (both shape and texture) that can model non-linearities in the shape and appearance space (e.g., scatter effects, specularities,..). However, they lack the capability to control the generation of subtle expressions. This paper proposes a new 3D face generative model that can decouple identity and expression and provides granular control over expressions. In particular, we propose using a pair of supervised auto-encoder and generative adversarial networks to produce high-quality 3D faces, both in terms of appearance and shape. Experimental results in the generation of 3D faces learned with holistic expression labels, or Action Unit (AU) labels, show how we can decouple identity and expression; gaining fine-control over expressions while preserving identity. 
<sup>1</sup>"
Cooperative Self-Training for Multi-Target Adaptive Semantic Segmentation,"Yangsong Zhang, Subhankar Roy, Hongtao Lu, Elisa Ricci, Stéphane Lathuilière","University of Trento, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Shanghai Jiao Tong University, Shanghai, China; LTCI, Télém-Paris, Intitute Polytechnique de Paris",100,"China, France, Italy",0,,"In this work we address multi-target domain adaptation (MTDA) in semantic segmentation, which consists in adapting a single model from an annotated source dataset to multiple unannotated target datasets that differ in their underlying data distributions. To address MTDA, we propose a self-training strategy that employs pseudo-labels to induce cooperation among multiple domain-specific classifiers. We employ feature stylization as an efficient way to generate image views that forms an integral part of self-training. Additionally, to prevent the network from overfitting to noisy pseudo-labels, we devise a rectification strategy that leverages the predictions from different classifiers to estimate the quality of pseudo-labels. Our extensive experiments on numerous settings, based on four different semantic segmentation datasets, validates the effectiveness of the proposed self-training strategy and shows that our method outperforms state-of-the-art MTDA approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Cooperative_Self-Training_for_Multi-Target_Adaptive_Semantic_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Cooperative_Self-Training_for_Multi-Target_Adaptive_Semantic_Segmentation_WACV_2023_paper.pdf,,https://github.com/Mael-zys/CoaST,2210.01578,main,Poster,https://ieeexplore.ieee.org/document/10030517/,"['Computer vision', 'Adaptation models', 'Uncertainty', 'Semantic segmentation', 'Measurement uncertainty', 'Benchmark testing', 'Data models']","['Semantic Segmentation', 'Domain Adaptation', 'Annotated Dataset', 'Underlying Data Distribution', 'Semantic Segmentation Datasets', 'Benchmark', 'Cross-entropy Loss', 'Intersection Over Union', 'Kullback-Leibler', 'Target Image', 'Small Objects', 'Target Domain', 'Unlabeled Data', 'Target Data', 'Prediction Uncertainty', 'Consistency Score', 'Source Domain', 'Encoder Layer', 'Encoder Network', 'Consistency Loss', 'Pseudo Labels', 'Unsupervised Domain Adaptation Methods', 'Domain Adaptation Methods']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",7,"In this work we address multi-target domain adaptation (MTDA) in semantic segmentation, which consists in adapting a single model from an annotated source dataset to multiple unannotated target datasets that differ in their underlying data distributions. To address MTDA, we propose a self-training strategy that employs pseudo-labels to induce cooperation among multiple domain-specific classifiers. We employ feature stylization as an efficient way to generate image views that forms an integral part of self-training. Additionally, to prevent the network from overfitting to noisy pseudo-labels, we devise a rectification strategy that leverages the predictions from different classifiers to estimate the quality of pseudo-labels. Our extensive experiments on numerous settings, based on four different semantic segmentation datasets, validates the effectiveness of the proposed self-training strategy and shows that our method outperforms state-of-the-art MTDA approaches. https://github.com/Mael-zys/CoaST."
CountNet3D: A 3D Computer Vision Approach To Infer Counts of Occluded Objects,"Porter Jenkins, Kyle Armstrong, Stephen Nelson, Siddhesh Gotad, J. Stockton Jenkins, Wade Wilkey, Tanner Watts",Delicious AI; University of Utah; Brigham Young University,66.66666667,USA,33.33333333,USA,"3D scene understanding is an important problem that has experienced great progress in recent years, in large part due to the development of state-of-the-art methods for 3D object detection. However, the performance of 3D object detectors can suffer in scenarios where extreme occlusion of objects is present, or the number of object classes is large. In this paper, we study the problem of inferring 3D counts from densely packed scenes with heterogeneous objects. This problem has applications to important tasks such as inventory management or automatic crop yield estimation. We propose a novel regression-based method, CountNet3D, that uses mature 2D object detectors for finegrained classification and localization, and a PointNet backbone for geometric embedding. The network processes fused data from images and point clouds for end-to-end learning of counts. We perform experiments on a novel synthetic dataset for inventory management in retail, which we construct and make publicly available to the community. Our results show that regression-based 3D counting methods systematically outperform detection-based methods, and reveal that directly learning from raw point clouds greatly assists count estimation under extreme occlusion. Finally, we study the effectiveness of CountNet3D on a large dataset of real-world scenes where extreme occlusion is present and achieve an error rate of 11.01%.",https://openaccess.thecvf.com/content/WACV2023/html/Jenkins_CountNet3D_A_3D_Computer_Vision_Approach_To_Infer_Counts_of_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jenkins_CountNet3D_A_3D_Computer_Vision_Approach_To_Infer_Counts_of_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030990/,"['Point cloud compression', 'Location awareness', 'Computer vision', 'Three-dimensional displays', 'Detectors', 'Object detection', 'Inventory management']","['Occluded Objects', '3D Computer Vision', 'Object Detection', 'Point Cloud', 'Counting Method', 'Inventory Management', '3D Scene', 'Raw Point', 'Regression-based Methods', 'Progress In Recent Years', 'Real-world Scenes', 'Count Estimates', '2D Object', 'Raw Point Cloud', '3D Object Detection', 'Crop Yield Estimation', 'Convolutional Neural Network', 'Mean Absolute Error', 'Real-world Data', '3D Space', 'Object Counting', 'Mean Absolute Percentage Error', '3D Detection', 'LiDAR Point Clouds', 'RGB Images', 'Real-world Datasets', 'Bounding Box', 'Regression-based Approach', 'Camera Pose', 'Ray Casting']","['Applications: Commercial/retail', '3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"3D scene understanding is an important problem that has experienced great progress in recent years, in large part due to the development of state-of-the-art methods for 3D object detection. However, the performance of 3D object detectors can suffer in scenarios where extreme occlusion of objects is present, or the number of object classes is large. In this paper, we study the problem of inferring 3D counts from densely packed scenes with heterogeneous objects. This problem has applications to important tasks such as inventory management or automatic crop yield estimation. We propose a novel regression-based method, CountNet3D, that uses mature 2D object detectors for finegrained classification and localization, and a PointNet backbone for geometric embedding. The network processes fused data from images and point clouds for end-to-end learning of counts. We perform experiments on a novel synthetic dataset for inventory management in retail, which we construct and make publicly available to the community. Our results show that regression-based 3D counting methods systematically outperform detection-based methods, and reveal that directly learning from raw point clouds greatly assists count estimation under extreme occlusion. Finally, we study the effectiveness of CountNet3D on a large dataset of real-world scenes where extreme occlusion is present and achieve an error rate of 11.01% ."
Couplformer: Rethinking Vision Transformer With Coupling Attention,"Hai Lan, Xihao Wang, Hao Shen, Peidong Liang, Xian Wei","Technical University of Munich; Fujian (Quanzhou) HIT; FJIRSM1; fortiss GmbH; FJIRSM1, FJOEL2",20,Germany,80,China,"With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory consumption hinders the deployment of the Transformer model on the embedded system where the computing resources are limited. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. Our method enables the Transformer model to improve time and memory efficiency while maintaining expressive power. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1K classification task, the Couplformer can significantly decrease 42% memory consumption compared with the regular Transformer. Meanwhile, it accesses sufficient accuracy requirements, which outperforms 0.56% on Top-1 accuracy and occupies the same memory footprint. Besides, the Couplformer achieves state-of-art performance in MS COCO 2017 object detection and instance segmentation tasks. As a result, the Couplformer can serve as an efficient backbone in visual tasks and provide a novel perspective on deploying attention mechanisms for researchers.",https://openaccess.thecvf.com/content/WACV2023/html/Lan_Couplformer_Rethinking_Vision_Transformer_With_Coupling_Attention_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lan_Couplformer_Rethinking_Vision_Transformer_With_Coupling_Attention_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030529/,"['Couplings', 'Visualization', 'Computer vision', 'Image segmentation', 'Computational modeling', 'Memory management', 'Object detection']","['Vision Transformer', 'Spatial Information', 'Image Classification', 'Object Detection', 'Attention Mechanism', 'Transformer Model', 'Alignment Score', 'Memory Consumption', 'Image Classification Tasks', 'Attention Map', 'Instance Segmentation', 'Self-attention Mechanism', 'Top-1 Accuracy', 'Computer Vision Domain', 'Time And Space', 'Convolutional Neural Network', 'Matrix Elements', 'Row Vector', 'Training Images', 'Standard Transformation', 'Attention Matrix', 'Position Embedding', 'Transformation Efficiency', 'Space Complexity', 'Number Of Heads', 'Dot Product', 'Embedding Dimension', 'CIFAR-100 Dataset', 'Attention Block']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Visualization']",2,"With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory consumption hinders the deployment of the Transformer model on the embedded system where the computing resources are limited. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. Our method enables the Transformer model to improve time and memory efficiency while maintaining expressive power. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1K classification task, the Couplformer can significantly decrease 42% memory consumption compared with the regular Transformer. Meanwhile, it accesses sufficient accuracy requirements, which outperforms 0.56% on Top-1 accuracy and occupies the same memory footprint. Besides, the Couplformer achieves state-of-art performance in MS COCO 2017 object detection and instance segmentation tasks. As a result, the Couplformer can serve as an efficient backbone in visual tasks and provide a novel perspective on deploying attention mechanisms for researchers."
Creating a Forensic Database of Shoeprints From Online Shoe-Tread Photos,"Samia Shafique, Bailey Kong, Shu Kong, Charless Fowlkes","University of California, Irvine; Ronin Institute; Texas A&M University",100,USA,0,,"Shoe tread impressions are one of the most common types of evidence left at crime scenes. However, the utility of such evidence is limited by the lack of databases of footwear prints that cover the large and growing number of distinct shoe models. Moreover, the database is preferred to contain the 3D shape, or depth, of shoe-tread photos so as to allow for extracting shoeprints to match a query (crime-scene) print. We propose to address this gap by leveraging shoe-tread photos collected by online retailers. The core challenge is to predict depth maps for these photos. As they do not have ground-truth 3D shapes allowing for training depth predictors, we exploit synthetic data that does. We develop a method termed ShoeRinsics that learns to predict depth by leveraging a mix of fully supervised synthetic data and unsupervised retail image data. In particular, we find domain adaptation and intrinsic image decomposition techniques effectively mitigate the synthetic-real domain gap and yield significantly better depth prediction. To validate our method, we introduce 2 validation sets consisting of shoe-tread image and print pairs and define a benchmarking protocol to quantify the quality of predicted depth. On this benchmark, ShoeRinsics outperforms existing methods of depth prediction and synthetic-to-real domain adaptation.",https://openaccess.thecvf.com/content/WACV2023/html/Shafique_Creating_a_Forensic_Database_of_Shoeprints_From_Online_Shoe-Tread_Photos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shafique_Creating_a_Forensic_Database_of_Shoeprints_From_Online_Shoe-Tread_Photos_WACV_2023_paper.pdf,,https://github.com/Samia067/ShoeRinsics,2205.02361,main,Poster,https://ieeexplore.ieee.org/document/10030814/,"['Training', 'Three-dimensional displays', 'Databases', 'Shape', 'Forensics', 'Footwear', 'Benchmark testing']","['Validation Set', 'Depth Map', 'Online Shopping', '3D Shape', 'Adaptive Technique', 'Domain Adaptation', 'Decomposition Technique', 'Crime Scene', 'Domain Gap', 'Depth Prediction', 'Domain Adaptation Techniques', 'Illumination', 'Training Set', 'Fine-tuned', 'Feature Space', 'Synthetic Images', 'Depth Estimation', 'Light Bulb', 'Sneakers', 'Reconstruction Loss', 'Mean Shift Algorithm', 'Intrinsic Component', 'Diffuse Light', 'Adversarial Domain Adaptation', 'Feature Alignment', 'Ground Truth Depth', 'Supplement For Details', 'Problem Setup']","['Algorithms: 3D computer vision', 'Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'low-shot', 'semi-', 'self-', 'and un-supervised learning']",3,"Shoe-tread impressions are one of the most common types of evidence left at crime scenes. However, the utility of such evidence is limited by the lack of databases of footwear prints that cover the large and growing number of distinct shoe models. Moreover, the database is preferred to contain the 3D shape, or depth, of shoe-tread photos so as to allow for extracting shoeprints to match a query (crime-scene) print. We propose to address this gap by leveraging shoe-tread photos collected by online retailers. The core challenge is to predict depth maps for these photos. As they do not have ground-truth 3D shapes allowing for training depth predictors, we exploit synthetic data that does. We develop a method, termed ShoeRinsics, that learns to predict depth from fully supervised synthetic data and unsupervised retail image data. In particular, we find domain adaptation and intrinsic image decomposition techniques effectively mitigate the synthetic-real domain gap and yield significantly better depth predictions. To validate our method, we introduce 2 validation sets consisting of shoe-tread image and print pairs and define a benchmarking protocol to quantify the quality of predicted depth. On this benchmark, ShoeRinsics outperforms existing methods of depth prediction and synthetic-to-real domain adaptation."
Cross-Domain Video Anomaly Detection Without Target Domain Adaptation,"Abhishek Aich, Kuan-Chuan Peng, Amit K. Roy-Chowdhury","Mitsubishi Electric Research Laboratories, USA; University of California, Riverside, USA",100,USA,0,,"Most cross-domain unsupervised Video Anomaly Detection (VAD) works assume that at least few task-relevant target domain training data are available for adaptation from the source to the target domain. However, this requires laborious model-tuning by the end-user who may prefer to have a system that works ""out-of-the-box"". To address such practical scenarios, we identify a novel target domain (inference-time) VAD task where no target domain training data are available. To this end, we propose a new 'Zero-shot Cross-domain Video Anomaly Detection (zxvad)' framework that includes a future-frame prediction generative model setup. Different from prior future-frame prediction models, our model uses a novel Normalcy Classifier module to learn the features of normal event videos by learning how such features are different ""relative"" to features in pseudo-abnormal examples. A novel Untrained Convolutional Neural Network based Anomaly Synthesis module crafts these pseudo-abnormal examples by adding foreign objects in normal video frames with no extra training cost. With our novel relative normalcy feature learning strategy, zxvad generalizes and learns to distinguish between normal and abnormal frames in a new target domain without adaptation during inference. Through evaluations on common datasets, we show that zxvad outperforms the state-of-the-art (SOTA), regardless of whether task-relevant (i.e., VAD) source training data are available or not. Lastly, zxvad also beats the SOTA methods in inference-time efficiency metrics including the model size, total parameters, GPU energy consumption, and GMACs.",https://openaccess.thecvf.com/content/WACV2023/html/Aich_Cross-Domain_Video_Anomaly_Detection_Without_Target_Domain_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Aich_Cross-Domain_Video_Anomaly_Detection_Without_Target_Domain_Adaptation_WACV_2023_paper.pdf,,,2212.0701,main,Poster,https://ieeexplore.ieee.org/document/10031024/,"['Measurement', 'Training', 'Representation learning', 'Adaptation models', 'Image color analysis', 'Training data', 'Predictive models']","['Target Domain', 'Anomaly Detection', 'Video Anomaly Detection', 'Target Domain Adaptation', 'Training Data', 'Convolutional Neural Network', 'Foreign Body', 'Video Frames', 'Extra Cost', 'Normal Events', 'Synthesis Module', 'Extra Training', 'Efficiency Metrics', 'Loss Function', 'Unsupervised Methods', 'Continuous Action', 'Action Recognition', 'Source Domain', 'Attention Map', 'Training Videos', 'Area Under Receiver Operating Characteristic Curve', 'Strong Prior', 'Memory Module', 'Kinds Of Objects', 'Input Frames', 'Problem Setup', 'Frame Features', 'Abnormal Events', 'Types Of Anomalies', 'Learning Loss']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",20,"Most cross-domain unsupervised Video Anomaly Detection (VAD) works assume that at least few task-relevant target domain training data are available for adaptation from the source to the target domain. However, this requires laborious model-tuning by the end-user who may prefer to have a system that works ""out-of-the-box."" To address such practical scenarios, we identify a novel target domain (inference-time) VAD task where no target domain training data are available. To this end, we propose a new ‘Zero-shot Cross-domain Video Anomaly Detection (zxVAD)’ framework that includes a future-frame prediction generative model setup. Different from prior future-frame prediction models, our model uses a novel Normalcy Classifier module to learn the features of normal event videos by learning how such features are different ""relatively"" to features in pseudo-abnormal examples. A novel Untrained Convolutional Neural Network based Anomaly Synthesis module crafts these pseudo-abnormal examples by adding foreign objects in normal video frames with no extra training cost. With our novel relative normalcy feature learning strategy, zxVAD generalizes and learns to distinguish between normal and abnormal frames in a new target domain without adaptation during inference. Through evaluations on common datasets, we show that zxVAD outperforms the state-of-the-art (SOTA), regardless of whether task-relevant (i.e., VAD) source training data are available or not. Lastly, zxVAD also beats the SOTA methods in inference-time efficiency metrics including the model size, total parameters, GPU energy consumption, and GMACs."
Cross-Identity Video Motion Retargeting With Joint Transformation and Synthesis,"Haomiao Ni, Yihao Liu, Sharon X. Huang, Yuan Xue","The Pennsylvania State University, University Park, PA, USA; Johns Hopkins University, Baltimore, MD, USA",100,USA,0,,"In this paper, we propose a novel dual-branch Transformation-Synthesis network (TS-Net), for video motion retargeting. Given one subject video and one driving video, TS-Net can produce a new plausible video with the subject appearance of the subject video and motion pattern of the driving video. TS-Net consists of a warp-based transformation branch and a warp-free synthesis branch. The novel design of dual branches combines the strengths of deformation-grid-based transformation and warp-free generation for better identity preservation and robustness to occlusion in the synthesized videos. A mask-aware similarity module is further introduced to the transformation branch to reduce computational overhead. Experimental results on face and dance datasets show that TS-Net achieves better performance in video motion retargeting than several state-of-the-art models as well as its single-branch variants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet.",https://openaccess.thecvf.com/content/WACV2023/html/Ni_Cross-Identity_Video_Motion_Retargeting_With_Joint_Transformation_and_Synthesis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ni_Cross-Identity_Video_Motion_Retargeting_With_Joint_Transformation_and_Synthesis_WACV_2023_paper.pdf,,https://github.com/nihaomiao/WACV23_TSNet,2210.01559,main,Poster,https://ieeexplore.ieee.org/document/10030992/,"['Humanities', 'Computer vision', 'Codes', 'Computational modeling', 'Robustness', 'Faces']","['Feature Space', 'Feature Maps', 'Facial Expressions', 'Similarity Matrix', 'Representation Learning', 'Image Generation', 'Video Frames', 'Direct Synthesis', 'Residual Block', 'Subjective Image', 'Perceptual Similarity', 'Human Motion', 'Affinity Values', 'Person Image', 'Fusion Network', 'Decoder Network', 'Video Dataset', 'Test Videos', 'Flow Motion', 'Warp Field']","['Algorithms: Computational photography', 'image and video synthesis']",6,"In this paper, we propose a novel dual-branch Transformation-Synthesis network (TS-Net), for video motion retargeting. Given one subject video and one driving video, TS-Net can produce a new plausible video with the subject appearance of the subject video and motion pattern of the driving video. TS-Net consists of a warp-based transformation branch and a warp-free synthesis branch. The novel design of dual branches combines the strengths of deformation-grid-based transformation and warp-free generation for better identity preservation and robustness to occlusion in the synthesized videos. A mask-aware similarity module is further introduced to the transformation branch to reduce computational overhead. Experimental results on face and dance datasets show that TS-Net achieves better performance in video motion retargeting than several state-of-the-art models as well as its single-branch variants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet."
Cross-Modal Semantic Enhanced Interaction for Image-Sentence Retrieval,"Xuri Ge, Fuhai Chen, Songpei Xu, Fuxiang Tao, Joemon M. Jose","Department of Computer Science, The University of Hong Kong, Hong Kong, China.; School of Computing Science, University of Glasgow, Glasgow, UK.",100,"Hong Kong, UK",0,,"Image-sentence retrieval has attracted extensive research attention in multimedia and computer vision due to its promising application. The key issue lies in jointly learning the visual and textual representation to accurately estimate their similarity. To this end, the mainstream schema adopts an object-word based attention to calculate their relevance scores and refine their interactive representations with the attention features, which, however, neglects the context of the object representation on the inter-object relationship that matches the predicates in sentences. In this paper, we propose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for image-sentence retrieval, which correlates the intra- and inter-modal semantics between objects and words. In particular, we first design the intra-modal spatial and semantic graphs based reasoning to enhance the semantic representations of objects guided by the explicit relationships of the objects' spatial positions and their scene graph. Then the visual and textual semantic representations are refined jointly via the inter-modal interactive attention and the cross-modal alignment. To correlate the context of objects with the textual context, we further refine the visual semantic representation via the cross-level object-sentence and word-image based interactive attention. Experimental results on seven standard evaluation metrics show that the proposed CMSEI outperforms the state-of-the-art and the alternative approaches on MS-COCO and Flickr30K benchmarks.",https://openaccess.thecvf.com/content/WACV2023/html/Ge_Cross-Modal_Semantic_Enhanced_Interaction_for_Image-Sentence_Retrieval_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ge_Cross-Modal_Semantic_Enhanced_Interaction_for_Image-Sentence_Retrieval_WACV_2023_paper.pdf,,,2210.08908,main,Poster,https://ieeexplore.ieee.org/document/10030377/,"['Measurement', 'Representation learning', 'Visualization', 'Computer vision', 'Correlation', 'Computational modeling', 'Semantics']","['Interactive', 'Visual Representation', 'Standard Evaluation', 'Semantic Representations', 'Text Representation', 'Explicit Relationship', 'Objects In Context', 'Textual Context', 'Scene Graph', 'Standard Evaluation Metrics', 'Semantic Graph', 'Convolutional Neural Network', 'Image Features', 'Visual Features', 'Recurrent Neural Network', 'Global Features', 'Representation Learning', 'Object Features', 'Latent Space', 'Semantic Similarity', 'Cross-modal Interactions', 'Graph Convolutional Network', 'Semantic Correlation', 'Textual Features', 'Image Retrieval', 'Object Regions', 'Visibility Graph', 'Validation Images', 'Relation Graph', 'Salient Object']",['Algorithms: Vision + language and/or other modalities'],19,"Image-sentence retrieval has attracted extensive research attention in multimedia and computer vision due to its promising application. The key issue lies in jointly learning the visual and textual representation to accurately estimate their similarity. To this end, the mainstream schema adopts an object-word based attention to calculate their relevance scores and refine their interactive representations with the attention features, which, however, neglects the context of the object representation on the inter-object relationship that matches the predicates in sentences. In this paper, we propose a Cross-modal Semantic Enhanced Interaction method, termed CMSEI for image-sentence retrieval, which correlates the intra- and inter-modal semantics be-tween objects and words. In particular, we first design the intra-modal spatial and semantic graphs based reasoning to enhance the semantic representations of objects guided by the explicit relationships of the objects’ spatial positions and their scene graph. Then the visual and textual semantic representations are refined jointly via the inter-modal interactive attention and the cross-modal alignment. To correlate the context of objects with the textual context, we further refine the visual semantic representation via the cross-level object-sentence and word-image based interactive attention. Experimental results on seven standard evaluation metrics show that the proposed CMSEI outperforms the state-of-the-art and the alternative approaches on MS-COCO and Flickr30K benchmarks."
Cross-Modality Feature Fusion Network for Few-Shot 3D Point Cloud Classification,"Minmin Yang, Jiajing Chen, Senem Velipasalar","Electrical Engineering and Computer Science Dept., Syracuse University, Syracuse, NY, USA",100,USA,0,,"Recent years have witnessed significant progress in the field of few-shot image classification while few-shot 3D point cloud classification still remains under-explored. Real-world 3D point cloud data often suffers from occlusions, noise and deformation, which make the few-shot 3D point cloud classification even more challenging. In this paper, we propose a cross-modality feature fusion network, for few-shot 3D point cloud classification, which aims to recognize an object given only a few labeled samples, and provides better performance even with point cloud data with missing points. More specifically, we train two models in parallel. One is a projection-based model with ResNet-18 as the backbone and the other one is a point-based model with a DGCNN backbone. Moreover, we design a Support-Query Mutual Attention (sqMA) module to fully exploit the correlation between support and query features. Extensive experiments on three datasets, namely ModelNet40, ModelNet40-C and ScanObjectNN, show the effectiveness of our method, and its robustness to missing points. Our proposed method outperforms different state-of-the-art baselines on all datasets. The margin of improvement is even larger on the ScanObjectNN dataset, which is collected from real-world scenes and is more challenging with objects having missing points.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Cross-Modality_Feature_Fusion_Network_for_Few-Shot_3D_Point_Cloud_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Cross-Modality_Feature_Fusion_Network_for_Few-Shot_3D_Point_Cloud_Classification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030367/,"['Point cloud compression', 'Representation learning', 'Computer vision', 'Three-dimensional displays', 'Correlation', 'Fuses', 'Robustness']","['Point Cloud', 'Feature Fusion', '3D Point', '3D Point Cloud', 'Point Cloud Classification', 'Attention Module', 'Point Cloud Data', 'Missing Points', 'Query Features', 'Few-shot Classification', '3D Point Cloud Data', 'Deep Learning', 'Convolutional Neural Network', 'Image Features', 'Feature Maps', 'Data Modalities', 'Feature Points', 'Depth Images', 'Pooling Operation', 'Average Operation', 'Few-shot Learning', 'Pyramid Pooling', 'Optimization-based Methods', 'Query Examples', 'Support Set', 'Global Max Pooling', 'Max-pooling Operation', 'Zero-shot', 'Triplet Loss', 'Depth Point']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",10,"Recent years have witnessed significant progress in the field of few-shot image classification while few-shot 3D point cloud classification still remains under-explored. Real-world 3D point cloud data often suffers from occlusions, noise and deformation, which make the few-shot 3D point cloud classification even more challenging. In this paper, we propose a cross-modality feature fusion network, for few-shot 3D point cloud classification, which aims to recognize an object given only a few labeled samples, and provides better performance even with point cloud data with missing points. More specifically, we train two models in parallel. One is a projection-based model with ResNet18 as the backbone and the other one is a point-based model with a DGCNN backbone. Moreover, we design a Support-Query Mutual Attention (sqMA) module to fully exploit the correlation between support and query features. Extensive experiments on three datasets, namely ModelNet40, ModelNet40-C and ScanObjectNN, show the effectiveness of our method, and its robustness to missing points. Our proposed method outperforms different state-of-the-art baselines on all datasets. The margin of improvement is even larger on the ScanObjectNN dataset, which is collected from real-world scenes and is more challenging with objects having missing points."
Cross-Resolution Flow Propagation for Foveated Video Super-Resolution,"Eugene Lee, Lien-Feng Hsu, Evan Chen, Chen-Yi Lee","National Yang Ming Chiao Tung University, Hsinchu, Taiwan",100,Taiwan,0,,"The demand of high-resolution video contents has grown over the years. However, the delivery of high-resolution video is constrained by either computational resources required for rendering or network bandwidth for remote transmission. To remedy this limitation, we leverage the eye trackers found alongside existing augmented and virtual reality headsets. We propose the application of video super-resolution (VSR) technique to fuse low-resolution context with regional high-resolution context for resource-constrained consumption of high-resolution content without perceivable drop in quality. Eye trackers provide us the gaze direction of a user, aiding us in the extraction of the regional high-resolution context. As only pixels that falls within the gaze region can be resolved by the human eye, a large amount of the delivered content is redundant as we can't perceive the difference in quality of the region beyond the observed region. To generate a visually pleasing frame from the fusion of high-resolution region and low-resolution region, we study the capability of a deep neural network of transferring the context of the observed region to other regions (low-resolution) of the current and future frames. We label this task a Foveated Video Super-Resolution (FVSR), as we need to super-resolve the low-resolution regions of current and future frames through the fusion of pixels from the gaze region. We propose Cross-Resolution Flow Propagation (CRFP) for FVSR. We train and evaluate CRFP on REDS dataset on the task of 8 times FVSR, i.e. a combination of 8 times VSR and the fusion of foveated region. Departing from the conventional evaluation of per frame quality using SSIM or PSNR, we propose the evaluation of past foveated region, measuring the capability of a model to leverage the noise present in eye trackers during FVSR.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Cross-Resolution_Flow_Propagation_for_Foveated_Video_Super-Resolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Cross-Resolution_Flow_Propagation_for_Foveated_Video_Super-Resolution_WACV_2023_paper.pdf,,https://github.com/eugenelet/CRFP,,main,Poster,https://ieeexplore.ieee.org/document/10031020/,"['Visualization', 'Solid modeling', 'Additives', 'Gaussian noise', 'Superresolution', 'Virtual reality', 'Streaming media']","['General Flow', 'Video Super-resolution', 'Eye-tracking', 'Human Eye', 'Capability Of Model', 'Video Content', 'Current Frame', 'Regional Quality', 'Virtual Reality Headset', 'Future Frames', 'Drop In Quality', 'Spatial Resolution', 'Convolutional Layers', 'Additive Noise', 'Flow Field', 'Additive Gaussian Noise', 'Visual Quality', 'Residual Block', 'Optical Flow', 'Feature Aggregation', 'Motion Compensation', 'Previous Frame', 'Output Of Block', 'Convolutional Block', 'Residual Connection', 'Early Layers', 'Motion Estimation', 'Flow Estimation', 'Optical Flow Estimation', 'Eye-tracking Device']","['Algorithms: Computational photography', 'image and video synthesis', 'Virtual/augmented reality']",3,"The demand of high-resolution video contents has grown over the years. However, the delivery of high-resolution video is constrained by either computational resources required for rendering or network bandwidth for remote transmission. To remedy this limitation, we leverage the eye trackers found alongside existing augmented and virtual reality headsets. We propose the application of video super-resolution (VSR) technique to fuse low-resolution context with regional high-resolution context for resource-constrained consumption of high-resolution content without perceivable drop in quality. Eye trackers provide us the gaze direction of a user, aiding us in the extraction of the regional high-resolution context. As only pixels that falls within the gaze region can be resolved by the human eye, a large amount of the delivered content is redundant as we can’t perceive the difference in quality of the region beyond the observed region. To generate a visually pleasing frame from the fusion of high-resolution region and low-resolution region, we study the capability of a deep neural network of transferring the context of the observed region to other regions (low-resolution) of the current and future frames. We label this task a Foveated Video Super-Resolution (FVSR), as we need to super-resolve the low-resolution regions of current and future frames through the fusion of pixels from the gaze region. We propose Cross-Resolution Flow Propagation (CRFP) for FVSR. We train and evaluate CRFP on REDS dataset on the task of 8× FVSR, i.e. a combination of 8× VSR and the fusion of foveated region. Departing from the conventional evaluation of per frame quality using SSIM or PSNR, we propose the evaluation of past foveated region, measuring the capability of a model to leverage the noise present in eye trackers during FVSR. Code is made available at https://github.com/eugenelet/CRFP."
Cross-Task Attention Mechanism for Dense Multi-Task Learning,"Ivan Lopes, Tuan-Hung Vu, Raoul de Charette","Inria; Valeo.ai, Inria",100,France,0,,"Multi-task learning has recently become a promising solution for a comprehensive understanding of complex scenes. With an appropriate design multi-task models can not only be memory-efficient but also favour the exchange of complementary signals across tasks. In this work, we jointly address 2D semantic segmentation, and two geometry-related tasks, namely dense depth, surface normal estimation as well as edge estimation showing their benefit on indoor and outdoor datasets. We propose a novel multi-task learning architecture that exploits pair-wise cross-task exchange through correlation-guided attention and self-attention to enhance the average representation learning for all tasks. We conduct extensive experiments considering three multi-task setups, showing the benefit of our proposal in comparison to competitive baselines in both synthetic and real benchmarks. We also extend our method to the novel multi-task unsupervised domain adaptation setting. Our code is open-source.",https://openaccess.thecvf.com/content/WACV2023/html/Lopes_Cross-Task_Attention_Mechanism_for_Dense_Multi-Task_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lopes_Cross-Task_Attention_Mechanism_for_Dense_Multi-Task_Learning_WACV_2023_paper.pdf,,https://github.com/cv-rits/DenseMTL,2206.08927,main,Poster,https://ieeexplore.ieee.org/document/10030834/,"['Representation learning', 'Geometry', 'Computer vision', 'Semantic segmentation', 'Semantics', 'Estimation', 'Computer architecture']","['Attention Mechanism', 'Multi-task Learning', 'Semantic Segmentation', 'Normal Approximation', 'Domain Adaptation', 'Scene Understanding', 'Multi-task Model', 'Complement Signaling', 'Surface Normals', 'Dense Depth', 'Experimental Setup', 'Model Performance', 'Depth Map', 'Spatial Attention', 'Primary Task', 'Target Domain', 'Pairwise Similarity', 'Semantic Task', 'Depth Estimation', 'Source Domain', 'Types Of Attention', 'Task-specific Features', 'Multi-task Learning Model', 'Multi-task Learning Framework', 'Supplement For Details', 'Auxiliary Task', 'Pair Of Tasks', 'Negative Transfer', 'Channel Attention', 'Decoding Stage']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Adversarial learning', 'adversarial attack and defense methods', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",13,"Multi-task learning has recently become a promising solution for comprehensive understanding of complex scenes. With an appropriate design, multi-task models can not only be memory-efficient but also favour the exchange of complementary signals across tasks. In this work, we jointly address 2D semantic segmentation, and two geometry-related tasks, namely dense depth, surface normal estimation as well as edge estimation showing their benefit on several datasets. We propose a novel multi-task learning architecture that exploits pair-wise cross-task exchange through correlation-guided attention and self-attention to enhance the average representation learning for all tasks. We conduct extensive experiments on three multi-task setups, showing the benefit of our proposal in comparison to competitive baselines in both synthetic and real benchmarks. We also extend our method to the novel multi-task unsupervised domain adaptation setting. Our code is available at https://github.com/cv-rits/DenseMTL"
Cross-View Image Sequence Geo-Localization,"Xiaohan Zhang, Waqas Sultani, Safwan Wshah","Intelligent Machine Lab, Information Technology University, Pakistan; Department of Computer Science, University of Vermont, USA",100,"Pakistan, USA",0,,"Cross-view geo-localization aims to estimate the GPS location of a query ground-view image by matching it to images from a reference database of geo-tagged aerial images. To address this challenging problem, recent approaches use panoramic ground-view images to increase the range of visibility. Although appealing, panoramic images are not readily available compared to the videos of limited Field-Of-View (FOV) images. In this paper, we present the first cross-view geo-localization method that works on a sequence of limited FOV images. Our model is trained end-to-end to capture the temporal structure that lies within the frames using the attention-based temporal feature aggregation module. To robustly tackle different sequences length and GPS noises during inference, we propose to use a sequential dropout scheme to simulate variant length sequences. To evaluate the proposed approach in realistic settings, we present a new large-scale dataset containing ground-view sequences along with the corresponding aerial-view images. Extensive experiments and comparisons demonstrate the superiority of the proposed approach compared to several competitive baselines.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Cross-View_Image_Sequence_Geo-Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Cross-View_Image_Sequence_Geo-Localization_WACV_2023_paper.pdf,,,2210.14295,main,Poster,https://ieeexplore.ieee.org/document/10031017/,"['Computer vision', 'Databases', 'Image sequences', 'Global Positioning System', 'Videos']","['Cross-view Image', 'Sequence Length', 'Large-scale Datasets', 'Aerial Images', 'Feature Aggregation', 'Image Database', 'Temporal Modulation', 'Limited Field Of View', 'Panoramic Images', 'Temporal Aggregation', 'Transformer', 'Single Image', 'Number Of Images', 'Generative Adversarial Networks', 'Unmanned Aerial Vehicles', 'Real-world Scenarios', 'Reference Image', 'Inertial Measurement Unit', 'Temporal Domain', 'Street View', 'Image Ground', 'Query Image', 'Street View Images', 'Multi-head Self-attention', 'Polar Transformation', 'Google Street View', 'Embedding Vectors', 'Number Of Heads', 'Average Pooling Layer']","['Applications: Remote Sensing', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",13,"Cross-view geo-localization aims to estimate the GPS location of a query ground-view image by matching it to images from a reference database of geo-tagged aerial images. To address this challenging problem, recent approaches use panoramic ground-view images to increase the range of visibility. Although appealing, panoramic images are not readily available compared to the videos of limited Field-Of-View (FOV) images. In this paper, we present the first cross-view geo-localization method that works on a sequence of limited FOV images. Our model is trained end-to-end to capture the temporal structure that lies within the frames using the attention-based temporal feature aggregation module. To robustly tackle different sequences length and GPS noises during inference, we propose to use a sequential dropout scheme to simulate variant length sequences. To evaluate the proposed approach in realistic settings, we present a new large-scale dataset containing ground-view sequences along with the corresponding aerial-view images. Extensive experiments and comparisons demonstrate the superiority of the proposed approach compared to several competitive baselines."
Cut-Paste Consistency Learning for Semi-Supervised Lesion Segmentation,"Boon Peng Yap, Beng Koon Ng","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",100,Singapore,0,,"Semi-supervised learning has the potential to improve the data-efficiency of training data-hungry deep neural networks, which is especially important for medical image analysis tasks where labeled data is scarce. In this work, we present a simple semi-supervised learning method for lesion segmentation tasks based on the ideas of cut-paste augmentation and consistency regularization. By exploiting the mask information available in the labeled data, we synthesize partially labeled samples from the unlabeled images so that the usual supervised learning objective (e.g., binary cross entropy) can be applied. Additionally, we introduce a background consistency term to regularize the training on the unlabeled background regions of the synthetic images. We empirically verify the effectiveness of the proposed method on two public lesion segmentation datasets, including an eye fundus photograph dataset and a brain CT scan dataset. The experiment results indicate that our method achieves consistent and superior performance over other self-training and consistency-based methods without introducing sophisticated network components.",https://openaccess.thecvf.com/content/WACV2023/html/Yap_Cut-Paste_Consistency_Learning_for_Semi-Supervised_Lesion_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yap_Cut-Paste_Consistency_Learning_for_Semi-Supervised_Lesion_Segmentation_WACV_2023_paper.pdf,,,2210.00191,main,Poster,https://ieeexplore.ieee.org/document/10030898/,"['Training', 'Image segmentation', 'Image analysis', 'Image synthesis', 'Computed tomography', 'Supervised learning', 'Neural networks']","['Lesion Segmentation', 'Neural Network', 'Computed Tomography', 'Medical Imaging', 'Deep Neural Network', 'Cross-entropy', 'Learning Objectives', 'Segmentation Task', 'Synthetic Images', 'Semi-supervised Learning', 'Semi-supervised Methods', 'Unlabeled Images', 'Consistency Regularization', 'F1 Score', 'Data Augmentation', 'Intersection Over Union', 'Bounding Box', 'Intracranial Hemorrhage', 'Types Of Lesions', 'Foreground Objects', 'Training Round', 'Unlabeled Data', 'Realistic Images', 'Target Network', 'Microaneurysms', 'Fundus Images', 'Hard Exudates', 'Image Synthesis', 'Color Matching']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Semi-supervised learning has the potential to improve the data-efficiency of training data-hungry deep neural networks, which is especially important for medical image analysis tasks where labeled data is scarce. In this work, we present a simple semi-supervised learning method for lesion segmentation tasks based on the ideas of cut-paste augmentation and consistency regularization. By exploiting the mask information available in the labeled data, we synthesize partially labeled samples from the unlabeled images so that the usual supervised learning objective (e.g., binary cross entropy) can be applied. Additionally, we introduce a background consistency term to regularize the training on the unlabeled background regions of the synthetic images. We empirically verify the effectiveness of the proposed method on two public lesion segmentation datasets, including an eye fundus photograph dataset and a brain CT scan dataset. The experiment results indicate that our method achieves consistent and superior performance over other self-training and consistency-based methods without introducing sophisticated network components."
D-Extract: Extracting Dimensional Attributes From Product Images,"Pushpendu Ghosh, Nancy Wang, Promod Yenigalla",Amazon,0,,100,USA,"Product dimension is a crucial piece of information enabling customers make better buying decisions. E-commerce websites extract dimension attributes to enable customers filter the search results according to their requirements. The existing methods extract dimension attributes from textual data like title and product description. However, this textual information often exists in an ambiguous, disorganised structure. In comparison, images can be used to extract reliable and consistent dimensional information. With this motivation, we hereby propose two novel architecture to extract dimensional information from product images. The first namely Single-Box Classification Network is designed to classify each text token in the image, one at a time, whereas the second architecture namely Multi-Box Classification Network uses a transformer network to classify all the detected text tokens simultaneously. To attain better performance, the proposed architectures are also fused with statistical inferences derived from the product category which further increased the F1-score of the Single-Box Classification Network by  3.78% and Multi-Box Classification Network by  0.9%. We use distance supervision technique to create a large scale automated dataset for pretraining purpose and notice considerable improvement when the models were pretrained on the large data before finetuning. The proposed model achieves a desirable precision of 91.54% at 89.75% recall and outperforms the other state of the art approaches by  4.76% in F1-score.",https://openaccess.thecvf.com/content/WACV2023/html/Ghosh_D-Extract_Extracting_Dimensional_Attributes_From_Product_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ghosh_D-Extract_Extracting_Dimensional_Attributes_From_Product_Images_WACV_2023_paper.pdf,,https://github.com/amazon-science/dimension-extraction-dataset,,main,Poster,https://ieeexplore.ieee.org/document/10030811/,"['Computer vision', 'Computational modeling', 'Computer network reliability', 'Computer architecture', 'Transformers', 'Information filters', 'Data models']","['Dimensional Attributes', 'Transformer', 'Reliable Information', 'Statistical Inference', 'Text Data', 'Classification Network', 'Product Categories', 'Purchase Decisions', 'Dimensional Information', 'Product Description', 'Image Features', 'Measurement Values', 'Input Image', 'Image Classification', 'Measurement Unit', 'Dense Layer', 'Precision And Recall', 'RGB Images', 'Frontal View', 'Optical Character Recognition', 'Image Lines', 'Saliency Map', 'Unstructured Text', 'Dropout Layer']","['Applications: Commercial/retail', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",,"Product dimension is a crucial piece of information enabling customers make better buying decisions. E-commerce websites extract dimension attributes to enable customers filter the search results according to their requirements. The existing methods extract dimension attributes from textual data like title and product description. However, this textual information often exists in an ambiguous, disorganised structure. In comparison, images can be used to extract reliable and consistent dimensional information. With this motivation, we hereby propose two novel architecture to extract dimensional information from product images. The first namely Single-Box Classification Net-work is designed to classify each text token in the image, one at a time, whereas the second architecture namely Multi-Box Classification Network uses a transformer network to classify all the detected text tokens simultaneously. To attain better performance, the proposed architectures are also fused with statistical inferences derived from the product category which further increased the F1-score of the Single-Box Classification Network by 3.78% and Multi-Box Classification Network by ≈ 0.9%≈. We use distance super-vision technique to create a large scale automated dataset for pretraining purpose and notice considerable improvement when the models were pretrained on the large data before finetuning. The proposed model achieves a desirable precision of 91.54% at 89.75% recall and outperforms the other state of the art approaches by ≈ 4.76% in F1-score
<sup>1</sup>
."
D2F2WOD: Learning Object Proposals for Weakly-Supervised Object Detection via Progressive Domain Adaptation,"Yuting Wang, Ricardo Guerrero, Vladimir Pavlovic","Rutgers University, Piscataway, NJ; Samsung AI Center, Cambridge, UK",100,"UK, USA",0,,"Weakly-supervised object detection (WSOD) models attempt to leverage image-level annotations in lieu of accurate but costly-to-obtain object localization labels. This oftentimes leads to substandard object detection and localization at inference time. To tackle this issue, we propose D2DF2WOD, a Dual-Domain Fully-to-Weakly Supervised Object Detection framework that leverages synthetic data, annotated with precise object localization, to supplement a natural image target domain, where only image-level labels are available. In its warm-up domain adaptation stage, the model learns a fully-supervised object detector (FSOD) to improve the precision of the object proposals in the target domain, and at the same time learns target-domain-specific and detection-aware proposal features. In its main WSOD stage, a WSOD model is specifically tuned to the target domain. The feature extractor and the object proposal generator of the WSOD model are built upon the fine-tuned FSOD model. We test D2DF2WOD on five dual-domain image benchmarks. The results show that our method results in consistently improved object detection and localization compared with state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_D2F2WOD_Learning_Object_Proposals_for_Weakly-Supervised_Object_Detection_via_Progressive_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_D2F2WOD_Learning_Object_Proposals_for_Weakly-Supervised_Object_Detection_via_Progressive_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
DBCE: A Saliency Method for Medical Deep Learning Through Anatomically-Consistent Free-Form Deformations,"Joshua Peters, Léo Lebrat, Rodrigo Santa Cruz, Aaron Nicolson, Gregg Belous, Salamata Konate, Parnesh Raniga, Vincent Dore, Pierrick Bourgeat, Jurgen Mejan-Fripp, Clinton Fookes, Olivier Salvado","University of Queensland; QUT; CSIRO; CSIRO, QUT; CSIRO and QUT",80,Australia,20,Australia,"Deep learning models are powerful tools for addressing challenging medical imaging problems. However, for an ever-growing range of applications, interpreting a model's prediction remains non-trivial. Understanding decisions made by black-box algorithms is critical, and assessing their fairness and susceptibility to bias is a key step towards healthcare deployment. In this paper, we propose DBCE (Deformation Based Counterfactual Explainability). We optimise a diffeomorphic transformation that deforms a given input image to change the prediction of the model. This provides anatomically meaningful saliency maps indicating tissue atrophy and expansion, which can be easily interpreted by clinicians. In our test case, DBCE replicates the transition of a patient from healthy control (HC) to Alzheimer's disease (AD). We benchmark DBCE against three commonly used saliency methods. We show that it provides more meaningful saliency maps when applied to one subject and disease-consistent atrophy patterns when used over a larger cohort. In addition, our method fulfils a recent sanity check and is repeatable for different model initialisations in contrast to classical sensitivity-based methods.",https://openaccess.thecvf.com/content/WACV2023/html/Peters_DBCE_A_Saliency_Method_for_Medical_Deep_Learning_Through_Anatomically-Consistent_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Peters_DBCE_A_Saliency_Method_for_Medical_Deep_Learning_Through_Anatomically-Consistent_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
DCVNet: Dilated Cost Volume Networks for Fast Optical Flow,"Huaizu Jiang, Erik Learned-Miller","Northeastern University, Boston, MA 02115; UMass Amherst, Amherst, MA 01003",100,USA,0,,"The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, coarse-to-fine or recurrent processing of the cost volume is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. In this paper, we propose an alternative by constructing cost volumes with different dilation factors to capture small and large displacements simultaneously. A U-Net with sikp connections is employed to convert the dilated cost volumes into interpolation weights between all possible captured displacements to get the optical flow. Our proposed model DCVNet only needs to process the cost volume once in a simple feedforward manner and does not rely on the sequential processing strategy. DCVNet obtains comparable accuracy to existing approaches and achieves real-time inference (30 fps on a mid-end 1080ti GPU).",https://openaccess.thecvf.com/content/WACV2023/html/Jiang_DCVNet_Dilated_Cost_Volume_Networks_for_Fast_Optical_Flow_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jiang_DCVNet_Dilated_Cost_Volume_Networks_for_Fast_Optical_Flow_WACV_2023_paper.pdf,,,2103.17271,main,Poster,https://ieeexplore.ieee.org/document/10030369/,"['Solid modeling', 'Interpolation', 'Computer vision', 'Costs', 'Computational modeling', 'Graphics processing units', 'Estimation']","['Optical Flow', 'Cost Volume', 'Input Image', 'Sequencing Strategy', 'Local Neighborhood', 'Large Displacement', 'Small Displacements', 'Neighborhood Radius', 'Dilation Factor', 'Neural Network', 'Spatial Resolution', 'Convolutional Neural Network', 'Deep Neural Network', 'Feature Maps', 'Recurrent Neural Network', 'Data Augmentation', 'Loss Term', 'Flow Estimation', 'Sequential Manner', 'Small Neighborhood', 'Optical Flow Estimation', 'Final Pass', 'Pyramid Level', 'Dilation Rate', 'Neural Network-based Approach', 'Pre-training Stage', 'Sequential Estimation', 'Standard Benchmark', 'Computer Vision Problems', 'Optical Model']",['Algorithms: Low-level and physics-based vision'],2,"The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, coarse-to-fine or recurrent processing of the cost volume is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. In this paper, we propose an alternative by constructing cost volumes with different dilation factors to capture small and large displacements simultaneously. A U-Net with sikp connections is employed to convert the dilated cost volumes into interpolation weights between all possible captured displacements to get the optical flow. Our proposed model DCVNet only needs to process the cost volume once in a simple feedforward manner and does not rely on the sequential processing strategy. DCVNet obtains comparable accuracy to existing approaches and achieves real-time inference (30 fps on a mid-end 1080ti GPU)."
DDNeRF: Depth Distribution Neural Radiance Fields,"David Dadon, Ohad Fried, Yacov Hel-Or","School of Computer Science, Reichman University, Herzliya, Israel",100,Israel,0,,"The field of implicit neural representation has made significant progress. Models such as neural radiance fields (NeRF), which uses relatively small neural networks, can represent high-quality scenes and achieve state-of-the-art results for novel view synthesis. Training these types of networks, however, is still computationally expensive and the model struggles with real life 360 degree scenes. In this work, we propose the depth distribution neural radiance field (DDNeRF), a new method that significantly increases sampling efficiency along rays during training, while achieving superior results for a given sampling budget. DDNeRF achieves this performance by learning a more accurate representation of the density distribution along rays. More specifically, the proposed framework trains a coarse model to predict the internal distribution of the transparency of an input volume along each ray. This estimated distribution then guides the sampling procedure of the fine model. Our method allows using fewer samples during training while achieving better output quality with the same computational resources.",https://openaccess.thecvf.com/content/WACV2023/html/Dadon_DDNeRF_Depth_Distribution_Neural_Radiance_Fields_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dadon_DDNeRF_Depth_Distribution_Neural_Radiance_Fields_WACV_2023_paper.pdf,,,2203.16626,main,Poster,https://ieeexplore.ieee.org/document/10030363/,"['Training', 'Solid modeling', 'Computer vision', 'Computational modeling', 'Neural networks', 'Predictive models']","['Radiance Field', 'Neural Radiance Fields', 'Final Model', 'Density Distribution', 'Field Representation', 'Implicit Representation', 'View Synthesis', 'Coarse Model', 'Mixture Model', 'Points In Space', 'Multilayer Perceptron', 'Information Section', '3D Mesh', 'Depth Estimation', 'Function Pdf', 'Single Interval', 'Fine Network', 'Hierarchical Sampling', 'Extra Memory']","['Algorithms: 3D computer vision', 'Computational photography', 'image and video synthesis']",2,"The field of implicit neural representation has made significant progress. Models such as neural radiance fields (NeRF) [12], which uses relatively small neural networks, can represent high-quality scenes and achieve state-of-the-art results for novel view synthesis. Training these types of networks, however, is still computationally expensive and the model struggles with real life 360° scenes. In this work, we propose the depth distribution neural radiance field (DDNeRF), a new method that significantly increases sampling efficiency along rays during training, while achieving superior results for a given sampling budget. DDNeRF achieves this performance by learning a more accurate representation of the density distribution along rays. More specifically, the proposed framework trains a coarse model to predict the internal distribution of the transparency of an input volume along each ray. This estimated distribution then guides the sampling procedure of the fine model. Our method allows using fewer samples during training while achieving better output quality with the same computational resources."
DE-CROP: Data-Efficient Certified Robustness for Pretrained Classifiers,"Gaurav Kumar Nayak, Ruchit Rawal, Anirban Chakraborty","Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India",100,India,0,,"Certified defense using randomized smoothing is a popular technique to provide robustness guarantees for deep neural networks against l2 adversarial attacks. Existing works use this technique to provably secure a pretrained non-robust model by training a custom denoiser network on entire training data. However, access to the training set may be restricted to a handful of data samples due to constraints such as high transmission cost and the proprietary nature of the data. Thus, we formulate a novel problem of ""how to certify the robustness of pretrained models using only a few training samples"". We observe that training the custom denoiser directly using the existing techniques on limited samples yields poor certification. To overcome this, our proposed approach (DE-CROP) generates class-boundary and interpolated samples corresponding to each training sample, ensuring high diversity in the feature space of the pretrained classifier. We train the denoiser by maximizing the similarity between the denoised output of the generated sample and the original training sample in the classifier's logit space. We also perform distribution level matching using domain discriminator and maximum mean discrepancy that yields further benefit. In white box setup, we obtain significant improvements over the baseline on multiple benchmark datasets and also report similar performance under the challenging black box setup.",https://openaccess.thecvf.com/content/WACV2023/html/Nayak_DE-CROP_Data-Efficient_Certified_Robustness_for_Pretrained_Classifiers_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nayak_DE-CROP_Data-Efficient_Certified_Robustness_for_Pretrained_Classifiers_WACV_2023_paper.pdf,https://sites.google.com/view/decrop,,,main,Poster,https://ieeexplore.ieee.org/document/10030172/,"['Deep learning', 'Computer vision', 'Smoothing methods', 'Costs', 'Neural networks', 'Training data', 'Robustness']","['Pre-trained Classifier', 'Smoothing', 'Interpolation', 'Training Set', 'Training Data', 'Deep Neural Network', 'Denoising', 'Black Box', 'Feature Space', 'White Box', 'Adversarial Attacks', 'Maximum Mean Discrepancy', 'Domain Discriminator', 'Gaussian Noise', 'Training Time', 'Weight Decay', 'Input Samples', 'Base Classifiers', 'Class Probabilities', 'Limited Training Data', 'Limited Training', 'Limited Training Samples', 'Adversarial Robustness', 'Amount Of Training Data', 'Adversarial Training', 'Adversarial Perturbations', 'Clean Samples', 'Label Prediction', 'Negative Gradient']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"Certified defense using randomized smoothing is a popular technique to provide robustness guarantees for deep neural networks against l
<inf>2</inf>
 adversarial attacks. Existing works use this technique to provably secure a pretrained non-robust model by training a custom denoiser network on entire training data. However, access to the training set may be restricted to a handful of data samples due to constraints such as high transmission cost and the proprietary nature of the data. Thus, we formulate a novel problem of ""how to certify the robustness of pretrained models using only a few training samples"". We observe that training the custom denoiser directly using the existing techniques on limited samples yields poor certification. To overcome this, our proposed approach (DE-CROP)
<sup>1</sup>
 generates class-boundary and interpolated samples corresponding to each training sample, ensuring high diversity in the feature space of the pretrained classifier. We train the denoiser by maximizing the similarity between the denoised output of the generated sample and the original training sample in the classifier’s logit space. We also perform distribution level matching using domain discriminator and maximum mean discrepancy that yields further benefit. In white box setup, we obtain significant improvements over the baseline on multiple benchmark datasets and also report similar performance under the challenging black box setup."
DELS-MVS: Deep Epipolar Line Search for Multi-View Stereo,"Christian Sormann, Emanuele Santellani, Mattia Rossi, Andreas Kuhn, Friedrich Fraundorfer","Graz University of Technology, Institute of Computer Graphics and Vision; Sony Europe B.V., R&D Center - Stuttgart Laboratory",100,"Austria, Germany",0,,"We propose a novel approach for deep learning-based Multi-View Stereo (MVS). For each pixel in the reference image, our method leverages a deep architecture to search for the corresponding point in the source image directly along the corresponding epipolar line. We denote our method DELS-MVS: Deep Epipolar Line Search Multi-View Stereo. Previous works in deep MVS select a range of interest within the depth space, discretize it, and sample the epipolar line according to the resulting depth values: this can result in an uneven scanning of the epipolar line, hence of the image space. Instead, our method works directly on the epipolar line: this guarantees an even scanning of the image space and avoids both the need to select a depth range of interest, which is often not known a priori and can vary dramatically from scene to scene, and the need for a suitable discretization of the depth space. In fact, our search is iterative, which avoids the building of a cost volume, costly both to store and to process. Finally, our method performs a robust geometry-aware fusion of the estimated depth maps, leveraging a confidence predicted alongside each depth. We test DELS-MVS on the ETH3D, Tanks and Temples and DTU benchmarks and achieve competitive results with respect to state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Sormann_DELS-MVS_Deep_Epipolar_Line_Search_for_Multi-View_Stereo_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sormann_DELS-MVS_Deep_Epipolar_Line_Search_for_Multi-View_Stereo_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030334/,"['Deep learning', 'Computer vision', 'Costs', 'Fuses', 'Neural networks', 'Buildings', 'Benchmark testing']","['Multi-view Stereo', 'Epipolar Line', 'Deep Search', 'Benchmark', 'Reference Image', 'Depth Map', 'Source Images', 'Depth Range', 'Depth Values', 'Cost Volume', 'Neural Network', 'Training Set', 'Convolutional Neural Network', 'Deep Neural Network', 'Cross-entropy', 'Cross-entropy Loss', 'Point Cloud', 'Level Of Resolution', 'Previous Iteration', 'Depth Estimation', 'Structure From Motion', 'Confidence Map', 'Advanced Setting', 'Outer Setting', 'Measure Of Confidence', 'Highest Confidence', 'Ground Truth Depth', 'Entropy Loss']",['Algorithms: 3D computer vision'],6,"We propose a novel approach for deep learning-based Multi-View Stereo (MVS). For each pixel in the reference image, our method leverages a deep architecture to search for the corresponding point in the source image directly along the corresponding epipolar line. We denote our method DELS-MVS: Deep Epipolar Line Search Multi-View Stereo. Previous works in deep MVS select a range of interest within the depth space, discretize it, and sample the epipolar line according to the resulting depth values: this can result in an uneven scanning of the epipolar line, hence of the image space. Instead, our method works directly on the epipolar line: this guarantees an even scanning of the image space and avoids both the need to select a depth range of interest, which is often not known a priori and can vary dramatically from scene to scene, and the need for a suitable discretization of the depth space. In fact, our search is iterative, which avoids the building of a cost volume, costly both to store and to process. Finally, our method performs a robust geometry-aware fusion of the estimated depth maps, leveraging a confidence predicted alongside each depth. We test DELS-MVS on the ETH3D, Tanks and Temples and DTU benchmarks and achieve competitive results with respect to state-of-the-art approaches."
DRAMA: Joint Risk Localization and Captioning in Driving,"Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, Jiachen Li",Stanford University; Samsung Semiconductor; Honda Research Institute,100,"Japan, South Korea, USA",0,,"Considering the functionality of situational awareness in safety-critical automation systems, the perception of risk in driving scenes and its explainability is of particular importance for autonomous and cooperative driving. Toward this goal, this paper proposes a new research direction of joint risk localization in driving scenes and its risk explanation as a natural language description. Due to the lack of standard benchmarks, we collected a large-scale dataset, DRAMA (Driving Risk Assessment Mechanism with A captioning module), which consists of 17,785 interactive driving scenarios collected in Tokyo, Japan. Our DRAMA dataset accommodates video- and object-level questions on driving risks with associated important objects to achieve the goal of visual captioning as a free-form language description utilizing closed and open-ended responses for multi-level questions, which can be used to evaluate a range of visual captioning capabilities in driving scenarios. We make this data available to the community for further research. Using DRAMA, we explore multiple facets of joint risk localization and captioning in interactive driving scenarios. In particular, we benchmark various multi-task prediction architectures and provide a detailed analysis of joint risk localization and risk captioning. The data set is available at https://usa.honda-ri.com/drama",https://openaccess.thecvf.com/content/WACV2023/html/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.pdf,https://usa.honda-ri.com/drama,,2209.10767,main,Poster,https://ieeexplore.ieee.org/document/10030940/,"['Location awareness', 'Visualization', 'Computer vision', 'Computational modeling', 'Computer architecture', 'Benchmark testing', 'Linguistics']","['Benchmark', 'Risk Perception', 'Natural Language', 'Important Objective', 'Situational Awareness', 'Open-ended Responses', 'Description Language', 'Range Of Capabilities', 'Visual Capabilities', 'Drivers Of Risk', 'Natural Language Descriptions', 'Loss Function', 'Visual Features', 'Pedestrian', 'Multilayer Perceptron', 'Bounding Box', 'Inertial Measurement Unit', 'Optical Flow', 'Traffic Light', 'Anomaly Detection', 'Controller Area Network', 'Visual Properties', 'Advanced Driver Assistance Systems', 'Visual Explanation', 'Elementary Operations', 'Visual Reasoning', 'Steering Control', 'Implicit Learning', 'Trajectory Prediction', 'Attention Map']","['Applications: Robotics', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",25,"Considering the functionality of situational awareness in safety-critical automation systems, the perception of risk in driving scenes and its explainability is of particular importance for autonomous and cooperative driving. Toward this goal, this paper proposes a new research direction of joint risk localization in driving scenes and its risk explanation as a natural language description. Due to the lack of standard benchmarks, we collected a large-scale dataset, DRAMA (Driving Risk Assessment Mechanism with A captioning module), which consists of 17,785 interactive driving scenarios collected in Tokyo, Japan. Our DRAMA dataset accommodates video- and object-level questions on driving risks with associated important objects to achieve the goal of visual captioning as a free-form language description utilizing closed and open-ended responses for multi-level questions, which can be used to evaluate a range of visual captioning capabilities in driving scenarios. We make this data available to the community for further re-search. Using DRAMA, we explore multiple facets of joint risk localization and captioning in interactive driving scenarios. In particular, we benchmark various multi-task pre-diction architectures and provide a detailed analysis of joint risk localization and risk captioning. The data set is available at https://usa.honda-ri.com/drama"
DSAG: A Scalable Deep Framework for Action-Conditioned Multi-Actor Full Body Motion Synthesis,"Debtanu Gupta, Shubh Maheshwari, Sai Shashank Kalakonda, Manasvi Vaidyula, Ravi Kiran Sarvadevabhatla","; Centre for Visual Information Technology, IIIT Hyderabad, Hyderabad, INDIA 500032",100,India,0,,"We introduce DSAG, a controllable deep neural framework for action-conditioned generation of full body multi-actor variable duration actions. To compensate for incompletely detailed finger joints in existing large-scale datasets, we introduce full body dataset variants with detailed finger joints. To overcome shortcomings in existing generative approaches, we introduce dedicated representations for encoding finger joints. We also introduce novel spatiotemporal transformation blocks with multi-head self-attention and specialized temporal processing. The design choices enable generations for a large range in body joint counts (24 - 52), frame rates (13 - 50), global body movement (in-place, locomotion) and action categories (12 - 120), across multiple datasets (NTU-120, HumanAct12, UESTC, Human3.6M). Our experimental results demonstrate DSAG's significant improvements over state-of-the-art, its suitability for action-conditioned generation at scale.",https://openaccess.thecvf.com/content/WACV2023/html/Gupta_DSAG_A_Scalable_Deep_Framework_for_Action-Conditioned_Multi-Actor_Full_Body_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_DSAG_A_Scalable_Deep_Framework_for_Action-Conditioned_Multi-Actor_Full_Body_WACV_2023_paper.pdf,http://skeleton.iiit.ac.in/dsag,,,main,Poster,https://ieeexplore.ieee.org/document/10030288/,"['Computer vision', 'Scalability', 'Fingers', 'Encoding', 'Spatiotemporal phenomena']","['Framework Synthesis', 'Attention Mechanism', 'Frame Rate', 'Multiple Datasets', 'Variable Duration', 'Action Classes', 'Finger Joints', 'Global Body', 'Sequence Of Actions', 'Combined Loss', 'Local Components', 'Temporal Modulation', 'Temporal Component', 'Body Components', 'Gaussian Components', 'Temporal Duration', 'Lower Frame', 'Latent Components', 'Encoder Module', 'Global Component', 'Global Trajectory', 'Small Number Of Categories', 'Fréchet Inception Distance', 'Body Pose', '3D Joint', 'RGB Video', 'Decoder Block', 'Subtle Movements', 'Spatial Components', 'Local Bodies']","['Algorithms: 3D computer vision', 'Biometrics', 'face', 'gesture', 'body pose']",3,"We introduce DSAG, a controllable deep neural framework for action-conditioned generation of full body multiactor variable duration actions. To compensate for incompletely detailed finger joints in existing large-scale datasets, we introduce full body dataset variants with detailed finger joints. To overcome shortcomings in existing generative approaches, we introduce dedicated representations for encoding finger joints. We also introduce novel spatiotemporal transformation blocks with multi-head self attention and specialized temporal processing. The design choices enable generations for a large range in body joint counts (24 - 52), frame rates (13 - 50), global body movement (inplace, locomotion) and action categories (12 - 120), across multiple datasets (NTU-120, HumanAct12, UESTC, Human3.6M). Our experimental results demonstrate DSAG’s significant improvements over state-of-the-art, its suitability for action-conditioned generation at scale."
DSFormer: A Dual-Domain Self-Supervised Transformer for Accelerated Multi-Contrast MRI Reconstruction,"Bo Zhou, Neel Dey, Jo Schlemper, Seyed Sadegh Mohseni Salehi, Chi Liu, James S. Duncan, Michal Sofka",New York University; Yale University; Hyperfine Research,66.66666667,"Canada, USA",33.33333333,USA,"Multi-contrast MRI (MC-MRI) captures multiple complementary imaging modalities to aid in radiological decision-making. Given the need for lowering the time cost of multiple acquisitions, current deep accelerated MRI reconstruction networks focus on exploiting the redundancy between multiple contrasts. However, existing works are largely supervised with paired data and/or prohibitively expensive fully-sampled MRI sequences. Further, reconstruction networks typically rely on convolutional architectures which are limited in their capacity to model long-range interactions and may lead to suboptimal recovery of fine anatomical detail. To these ends, we present a dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT) consisting of cascaded Swin transformer reconstruction networks (SwinRN) trained under two deep conditioning strategies to enable MC-MRI information sharing. We further use a dual-domain (image and k-space) self-supervised learning strategy for DCCT to alleviate the costs of acquiring fully sampled training data. DSFormer generates high-fidelity reconstructions which outperform current fully-supervised baselines. Moreover, we find that DSFormer achieves nearly the same performance when trained either with full supervision or with the proposed self-supervision.",https://openaccess.thecvf.com/content/WACV2023/html/Zhou_DSFormer_A_Dual-Domain_Self-Supervised_Transformer_for_Accelerated_Multi-Contrast_MRI_Reconstruction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhou_DSFormer_A_Dual-Domain_Self-Supervised_Transformer_for_Accelerated_Multi-Contrast_MRI_Reconstruction_WACV_2023_paper.pdf,,,2201.10776,main,Poster,https://ieeexplore.ieee.org/document/10030210/,"['Training', 'Costs', 'Magnetic resonance imaging', 'Redundancy', 'Training data', 'Information sharing', 'Computer architecture']","['Multi-contrast MRI', 'Training Data', 'Paired Data', 'Network Reconstruction', 'Self-supervised Learning', 'Anatomical Details', 'Deep Transformation', 'Convolutional Neural Network', 'Convolutional Layers', 'Image Reconstruction', 'Multilayer Perceptron', 'Reconstruction Method', 'Peak Signal-to-noise Ratio', 'Backbone Network', 'Image Domain', 'Inverse Fourier Transform', 'Reconstruction Quality', 'Residual Connection', 'Reconstruction Performance', 'Consistency Loss', 'Number Of Cascades', 'K-space Data', 'Deep Feature Extraction', 'Previous Reconstructions', 'Residual Learning', 'Local Fusion', 'Multi-head Self-attention', 'Data Partitioning', 'Upper Bound']","['Applications: Biomedical/healthcare/medicine', 'Computational photography', 'image and video synthesis']",23,"Multi-contrast MRI (MC-MRI) captures multiple complementary imaging modalities to aid in radiological decision-making. Given the need for lowering the time cost of multiple acquisitions, current deep accelerated MRI reconstruction networks focus on exploiting the redundancy between multiple contrasts. However, existing works are largely supervised with paired data and/or prohibitively expensive fully-sampled MRI sequences. Further, reconstruction networks typically rely on convolutional architectures which are limited in their capacity to model long-range interactions and may lead to suboptimal recovery of fine anatomical detail. To these ends, we present a dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT) consisting of cascaded Swin transformer reconstruction networks (SwinRN) trained under two deep conditioning strategies to enable MC-MRI information sharing. We further use a dual-domain (image and k-space) self-supervised learning strategy for DCCT to alleviate the costs of acquiring fully sampled training data. DSFormer generates high-fidelity reconstructions which outperform current fully-supervised baselines and approach the performance of full supervision."
DSTrans: Dual-Stream Transformer for Hyperspectral Image Restoration,"Dabing Yu, Qingwu Li, Xiaolin Wang, Zhiliang Zhang, Yixi Qian, Chang Xu",Hohai university,100,China,0,,"Most CNN models exhibit two major flaws in hyperspectral image (HSI) restoration tasks. First, limited high-dimensional HSI training examples exacerbate the difficulty of deep learning methods in learning effective spatial and spectral representations. Second, the existing CNN-based methods model local relations and present limitations in capturing long-range dependencies. In this paper, we customize a novel dual-stream Transformer (DSTrans) for HSI restoration, which mainly consists of the dual-stream attention and the dual-stream feed-forward network. Specifically, we develop the dual-stream attention consisting of Multi-Dconv-head spectral attention (MDSA) and Multi-head Spatial self-attention (MSSA). MDSA and MSSA respectively calculate self-attention along the spectral and spatial dimensions in local windows to capture long-range spectrum dependencies and model global spatial interactions. Meanwhile, the dual-stream feed-forward network is developed to extract global signals and local details in parallel branches. In addition, we exploit a multi-tasking network to train the auxiliary RGB image (RGBI) task and HSI task jointly so that both numerous RGBI samples and limited HSI samples are exploited to learn parameter distribution for DSTrans. Extensive experimental results demonstrate that our method achieves state-of-the-art results on HSI restoration tasks, including HSI super-resolution and denoising. The source code can be obtained at: https://github.com/yudadabing/Dual-Stream-Transformer-for-Hyperspectral-Image-Restoration.",https://openaccess.thecvf.com/content/WACV2023/html/Yu_DSTrans_Dual-Stream_Transformer_for_Hyperspectral_Image_Restoration_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yu_DSTrans_Dual-Stream_Transformer_for_Hyperspectral_Image_Restoration_WACV_2023_paper.pdf,,https://github.com/yudadabing/Dual-Stream-Transformer-for-Hyperspectral-Image-Restoration,,main,Poster,https://ieeexplore.ieee.org/document/10030920/,"['Training', 'Visualization', 'Source coding', 'Superresolution', 'Noise reduction', 'Transformer cores', 'Transformers']","['Hyperspectral Image Restoration', 'Spatial Dimensions', 'Long-range Interactions', 'Feed-forward Network', 'Spectral Properties', 'RGB Images', 'Training Examples', 'Local Details', 'Spatial Interaction', 'CNN-based Methods', 'Long-range Dependencies', 'Spectral Representation', 'Multi-head Self-attention', 'Restoration Tasks', 'Root Mean Square Error', 'Local Information', 'Spatial Information', 'Additive Noise', 'Global Information', 'Transformer Model', 'Auxiliary Task', 'Self-attention Mechanism', 'Spectral Angle Mapper', 'Global Correlation', 'Error Map', 'Hyperspectral Image Datasets', 'Attention Map', 'Shallow Features', 'Depthwise Convolution', 'Visual Error']","['Algorithms: Low-level and physics-based vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Agriculture']",16,"Most CNN models exhibit two major flaws in hyper-spectral image (HSI) restoration tasks. First, limited high-dimensional HSI training examples exacerbate the difficulty of deep learning methods in learning effective spatial and spectral representations. Second, the existing CNN-based methods model local relations and present limitations in capturing long-range dependencies. In this paper, we customize a novel dual-stream Transformer (DSTrans) for HSI restoration, which mainly consists of the dual-stream attention and the dual-stream feed-forward network. Specifically, we develop the dual-stream attention consisting of Multi-Dconv-head spectral attention (MDSA) and Multi-head Spatial self-attention (MSSA). MDSA and MSSA respectively calculate self-attention along the spectral and spatial dimensions in local windows to capture long-range spectrum dependencies and model global spatial interactions. Meanwhile, the dual-stream feed-forward network is developed to extract global signals and local details in parallel branches. In addition, we exploit a multi-tasking network to train the auxiliary RGB image (RGBI) task and HSI task jointly so that both numerous RGBI samples and limited HSI samples are exploited to learn parameter distribution for DSTrans. Extensive experimental results demonstrate that our method achieves state-of-the-art results on HSI restoration tasks, including HSI super-resolution and denoising. The source code can be obtained at: https://github.com/yudadabing/Dual-Stream-Transformer-for-Hyperspectral-Image-Restoration."
Dance Style Transfer With Cross-Modal Transformer,"Wenjie Yin, Hang Yin, Kim Baraka, Danica Kragic, Mårten Björkman","KTH Royal Institute of Technology, Stockholm, Sweden; Vrije Universiteit Amsterdam, Amsterdam, Netherlands",100,"Netherlands, Sweden",0,,"We present CycleDance, a dance style transfer system to transform an existing motion clip in one dance style to a motion clip in another dance style while attempting to preserve motion context of the dance. Our method extends an existing CycleGAN architecture for modeling audio sequences and integrates multimodal transformer encoders to account for music context. We adopt sequence length-based curriculum learning to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. We further introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. We perform an extensive ablation study as well as a human study including 30 participants with 5 or more years of dance experience. The results demonstrate that CycleDance generates realistic movements with the target style, significantly outperforming the baseline CycleGAN on naturalness, transfer strength, and content preservation.",https://openaccess.thecvf.com/content/WACV2023/html/Yin_Dance_Style_Transfer_With_Cross-Modal_Transformer_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yin_Dance_Style_Transfer_With_Cross-Modal_Transformer_WACV_2023_paper.pdf,https://youtu.be/kP4DBp8OUCk,,2208.09406,main,Poster,https://ieeexplore.ieee.org/document/10030800/,"['Training', 'Measurement', 'Humanities', 'Computer vision', 'Transforms', 'Computer architecture', 'Transformers']","['Style Transfer', 'Cross-modal Transformer', 'Ablation', 'Curriculum Learning', 'Dance Movement', 'Musical Context', 'Preservation Of Content', 'Convolutional Neural Network', 'Computer Vision', 'Complex Data', 'Video Games', 'User Study', 'Subjective Evaluation', 'Objective Evaluation', 'Video Clips', 'Human Motion', 'Variational Autoencoder', 'Motion Data', 'Rhythmic Patterns', 'Film Industry', 'Transfer Task', 'Video Game Industry', 'Movement Generation']","['Applications: Arts/games/social media', 'Biometrics', 'face', 'gesture', 'body pose']",5,"We present CycleDance, a dance style transfer system to transform an existing motion clip in one dance style to a motion clip in another dance style while attempting to preserve motion context of the dance. Our method extends an existing CycleGAN architecture for modeling audio sequences and integrates multimodal transformer encoders to account for music context. We adopt sequence length-based curriculum learning to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. We further introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. We perform an extensive ablation study as well as a human study including 30 participants with 5 or more years of dance experience. The results demonstrate that CycleDance generates realistic movements with the target style, significantly outperforming the baseline CycleGAN on naturalness, transfer strength, and content preservation.
<sup>1</sup>"
Dataset Condensation With Distribution Matching,"Bo Zhao, Hakan Bilen","School of Informatics, The University of Edinburgh",100,UK,0,,"Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and second-order derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost. We also show promising practical benefits of our method in continual learning and neural architecture search.",https://openaccess.thecvf.com/content/WACV2023/html/Zhao_Dataset_Condensation_With_Distribution_Matching_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhao_Dataset_Condensation_With_Distribution_Matching_WACV_2023_paper.pdf,,https://github.com/VICO-UoE/DatasetCondensation,2110.04181,main,Poster,https://ieeexplore.ieee.org/document/10030751/,"['Training', 'Computer vision', 'Costs', 'Computational modeling', 'Computer architecture', 'Computational efficiency', 'Task analysis']","['Condensation', 'Distribution Matching', 'Training Set', 'Large Datasets', 'Small Set', 'Deep Models', 'Second Derivative', 'Training Images', 'Latent Space', 'Incremental Learning', 'Synthetic Images', 'Training Costs', 'Bilevel Optimization', 'Neural Architecture Search', 'Deep Network', 'Deep Neural Network', 'Random Selection', 'Test Accuracy', 'Network Parameters', 'Updated Network', 'Real Data Distribution', 'Maximum Mean Discrepancy', 'Samples Of The Same Class', 'Data Augmentation', 'Random Initialization', 'MNIST Dataset', 'Real Training', 'Outer Loop', 'Real Training Data']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",60,"Computational cost of training state-of-the-art deep models in many learning problems is rapidly increasing due to more sophisticated models and larger datasets. A recent promising direction for reducing training cost is dataset condensation that aims to replace the original large training set with a significantly smaller learned synthetic set while preserving the original information. While training deep models on the small set of condensed images can be extremely fast, their synthesis remains computationally expensive due to the complex bi-level optimization and secondorder derivative computation. In this work, we propose a simple yet effective method that synthesizes condensed images by matching feature distributions of the synthetic and original training images in many sampled embedding spaces. Our method significantly reduces the synthesis cost while achieving comparable or better performance. Thanks to its efficiency, we apply our method to more realistic and larger datasets with sophisticated neural architectures and obtain a significant performance boost
<sup>1</sup>
. We also show promising practical benefits of our method in continual learning and neural architecture search."
Deep Learning Methodology for Early Detection and Outbreak Prediction of Invasive Species Growth,Nathan Elias,Liberal Arts and Science Academy,100,USA,0,,"Invasive species (IS) cause major environmental damages, costing approximately 1.4 Trillion globally. Early detection and rapid response (EDRR) is key to mitigating IS growth, but current EDRR methods are highly inadequate at addressing IS growth. In this paper, a machine-learning-based approach to combat IS spread is proposed, in which identification, detection, and prediction of IS growth are automated in a novel mobile application and scalable models. This paper details the techniques used for the novel development of deep, multi-dimensional Convolutional Neural Networks (CNNs) to detect the presence of IS in both 2D and 3D spaces, as well as the creation of geospatial Long Short-Term Memory (LSTMs) models to then accurately quantify, simulate, and project invasive species' future environmental spread. Results from conducting training and in-field validation studies show that this new methodology significantly improves current EDRR methods, by drastically decreasing the intensity of manual field labor while providing a toolkit that increases the efficiency and efficacy of ongoing efforts to combat IS. Furthermore, this research presents scalable expansion into dynamic LIDAR and aerial detection of IS growth, with the proposed toolkit already being deployed by state parks and national environmental/wildlife services.",https://openaccess.thecvf.com/content/WACV2023/html/Elias_Deep_Learning_Methodology_for_Early_Detection_and_Outbreak_Prediction_of_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Elias_Deep_Learning_Methodology_for_Early_Detection_and_Outbreak_Prediction_of_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030838/,"['Training', 'Deep learning', 'Solid modeling', 'Three-dimensional displays', 'Laser radar', 'Manuals', 'Predictive models']","['Early Detection', 'Invasive Species', 'Invasive Growth', 'Predictor Of Growth', 'Detection Of Growth', 'Convolutional Neural Network', 'Mobile App', 'Short-term Memory', 'Long Short-term Memory', '3D Space', 'Long Short-term Memory Model', 'Research In The Field', 'Training Dataset', 'Native Plants', 'Native Species', 'Macrophytes', 'Transfer Learning', 'Dense Layer', 'Point Cloud', 'Training Images', 'Invasive Plants', 'iNaturalist', '3D Detection', 'Image Augmentation', 'Generative Adversarial Networks', 'Amazon Web Services', 'Inception V3 Model', 'Test Split', '3D Scanning', 'Images Of Species']","['Applications: Environmental monitoring/climate change/ecology', 'Agriculture', 'Animals/Insects']",2,"Invasive species (IS) cause major environmental damages, costing approximately $1.4 Trillion globally. Early detection and rapid response (EDRR) is key to mitigating IS growth, but current EDRR methods are highly inadequate at addressing IS growth. In this paper, a machine-learning-based approach to combat IS spread is proposed, in which identification, detection, and prediction of IS growth are automated in a novel mobile application and scalable models. This paper details the techniques used for the novel development of deep, multi-dimensional Convolutional Neural Networks (CNNs) to detect the presence of IS in both 2D and 3D spaces, as well as the creation of geospatial Long Short-Term Memory (LSTMs) models to then accurately quantify, simulate, and project invasive species’ future environmental spread. Results from conducting training and in-field validation studies show that this new methodology significantly improves current EDRR methods, by drastically decreasing the intensity of manual field labor while providing a toolkit that increases the efficiency and efficacy of ongoing efforts to combat IS. Furthermore, this research presents scalable expansion into dynamic LIDAR and aerial detection of IS growth, with the proposed toolkit already being deployed by state parks and national environmental/wildlife services."
Deep Model-Based Super-Resolution With Non-Uniform Blur,"Charles Laroche, Andrés Almansa, Matias Tassano","CNRS & Université Paris Cité, andres.almansa@parisdescartes.fr; GoPro & MAP5, charles.laroche@u-paris.fr; Meta Inc.*, mtassano@meta.com",33.33333333,France,66.66666667,USA,"We propose a state-of-the-art method for super-resolution with non-uniform blur. Single-image super-resolution methods seek to restore a high-resolution image from blurred, subsampled, and noisy measurements. Despite their impressive performance, existing techniques usually assume a uniform blur kernel. Hence, these techniques do not generalize well to the more general case of non-uniform blur. Instead, in this paper, we address the more realistic and computationally challenging case of spatially-varying blur. To this end, we first propose a fast deep plug-and-play algorithm, based on linearized ADMM splitting techniques, which can solve the super-resolution problem with spatially-varying blur. Second, we unfold our iterative algorithm into a single network and train it end-to-end. In this way, we overcome the intricacy of manually tuning the parameters involved in the optimization scheme. Our algorithm presents remarkable performance and generalizes well after a single training to a large family of spatially-varying blur kernels, noise levels and scale factors.",https://openaccess.thecvf.com/content/WACV2023/html/Laroche_Deep_Model-Based_Super-Resolution_With_Non-Uniform_Blur_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Laroche_Deep_Model-Based_Super-Resolution_With_Non-Uniform_Blur_WACV_2023_paper.pdf,,,2204.10109,main,Poster,https://ieeexplore.ieee.org/document/10030233/,"['Training', 'Computer vision', 'Superresolution', 'Image restoration', 'Noise measurement', 'Iterative methods', 'Kernel']","['Non-uniform Blur', 'Scaling Factor', 'Iterative Algorithm', 'Single Image Super-resolution', 'Blur Kernel', 'Factorization', 'Fast Fourier Transform', 'Convolution Operation', 'Defocus', 'Gaussian Blur', 'Low-resolution Images', 'Deep Learning Architectures', 'Kernel Estimation', 'Reasonable Amount Of Time', 'Motion Blur', 'Image X', 'COCO Dataset', 'Edges Of Objects', 'Super-resolution Model']","['Algorithms: Computational photography', 'image and video synthesis']",9,"We propose a state-of-the-art method for super-resolution with non-uniform blur. Single-image super-resolution methods seek to restore a high-resolution image from blurred, subsampled, and noisy measurements. Despite their impressive performance, existing techniques usually assume a uniform blur kernel. Hence, these techniques do not generalize well to the more general case of non-uniform blur. Instead, in this paper, we address the more realistic and computationally challenging case of spatially-varying blur. To this end, we first propose a fast deep plug-and-play algorithm, based on linearized ADMM splitting techniques, which can solve the super-resolution problem with spatially-varying blur. Second, we unfold our iterative algorithm into a single network and train it end-to-end. In this way, we overcome the intricacy of manually tuning the parameters involved in the optimization scheme. Our algorithm presents remarkable performance and generalizes well after a single training to a large family of spatially-varying blur kernels, noise levels and scale factors."
DeepPrivacy2: Towards Realistic Full-Body Anonymization,"Håkon Hukkelås, Frank Lindseth",Norwegian University of Science and Technology,100,Norway,0,,"Generative Adversarial Networks (GANs) are widely adapted for anonymization of human figures. However, current state-of-the-art limit anonymization to the task of face anonymization. In this paper, we propose a novel anonymization framework (DeepPrivacy2) for realistic anonymization of human figures and faces. We introduce a new large and diverse dataset for human figure synthesis, which significantly improves image quality and diversity of generated images. Furthermore, we propose a style-based GAN that produces high quality, diverse and editable anonymizations. We demonstrate that our full-body anonymization framework provides stronger privacy guarantees than previously proposed methods.",https://openaccess.thecvf.com/content/WACV2023/html/Hukkelas_DeepPrivacy2_Towards_Realistic_Full-Body_Anonymization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hukkelas_DeepPrivacy2_Towards_Realistic_Full-Body_Anonymization_WACV_2023_paper.pdf,,github.com/hukkelas/deep privacy2,,main,Poster,https://ieeexplore.ieee.org/document/10030153/,"['Image quality', 'Data privacy', 'Computer vision', 'Privacy', 'Deepfakes', 'Computational modeling', 'Generative adversarial networks']","['Image Quality', 'Significantly Improved', 'Generative Adversarial Networks', 'Human Faces', 'Improve Image Quality', 'Human Figure', 'Privacy Guarantee', 'Computer Vision', 'Intersection Over Union', 'Bounding Box', 'Focus Of This Work', 'Latent Space', 'Constant Function', 'Directions In Space', 'Instance Segmentation', 'Face Detection', 'Mask R-CNN', 'Inpainting', 'COCO Dataset', 'Foreground Objects', 'Fréchet Inception Distance', 'Frames Per Second', 'Synthesis Quality', 'Instance Normalization', 'Downsampling Layer', 'Missing Regions', 'Amount Of Foci', 'Synthesis Method', 'Input State']","['Applications: Social good', 'Computational photography', 'image and video synthesis', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",22,"Generative Adversarial Networks (GANs) are widely adopted for anonymization of human figures. However, current state-of-the-art limits anonymization to the task of face anonymization. In this paper, we propose a novel anonymization framework (DeepPrivacy2) for realistic anonymization of human figures and faces. We introduce a new large and diverse dataset for full-body synthesis, which significantly improves image quality and diversity of generated images. Furthermore, we propose a style-based GAN that produces high-quality, diverse, and editable anonymizations. We demonstrate that our full-body anonymization framework provides stronger privacy guarantees than previously proposed methods. Source code and appendix is available at: github.com/hukkelas/deep_privacy2."
DeformIrisNet: An Identity-Preserving Model of Iris Texture Deformation,"Siamul Karim Khan, Patrick Tinsley, Adam Czajka","University of Notre Dame, IN, USA",100,USA,0,,"Nonlinear iris texture deformations due to pupil size variations are one of the main factors responsible for within-class variance of genuine comparison scores in iris recognition. In dominant approaches to iris recognition, the size of a ring-shaped iris region is linearly scaled to a canonical rectangle, used further in encoding and matching. However, the biological complexity of the iris sphincter and dilator muscles causes the movements of iris features to be nonlinear in a function of pupil size, and not solely organized along radial paths. Alternatively to the existing theoretical models based on the biomechanics of iris musculature, in this paper we propose a novel deep autoencoder-based model that can effectively learn complex movements of iris texture features directly from the data. The proposed model takes two inputs, (a) an ISO-compliant near-infrared iris image with initial pupil size, and (b) the binary mask defining the target shape of the iris. The model makes all the necessary nonlinear deformations to the iris texture to match the shape of the iris in an image (a) with the shape provided by the target mask (b). The identity-preservation component of the loss function helps the model in finding deformations that preserve identity and not only the visual realism of the generated samples. We also demonstrate two immediate applications of this model: better compensation for iris texture deformations in iris recognition algorithms, compared to linear models, and the creation of a generative algorithm that can aid human forensic examiners, who may need to compare iris images with a large difference in pupil dilation. We offer the source codes and model weights available along with this paper.",https://openaccess.thecvf.com/content/WACV2023/html/Khan_DeformIrisNet_An_Identity-Preserving_Model_of_Iris_Texture_Deformation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Khan_DeformIrisNet_An_Identity-Preserving_Model_of_Iris_Texture_Deformation_WACV_2023_paper.pdf,Not provided,Not provided,2207.0898,main,Poster,https://ieeexplore.ieee.org/document/10030175/,"['Deformable models', 'Shape', 'Biological system modeling', 'Source coding', 'Forensics', 'Computational modeling', 'Data models']","['Iris Texture', 'Application Of Model', 'Biomechanics', 'Model Weights', 'Pupil Size', 'Source Model', 'Forensic Investigations', 'Training Set', 'Training Dataset', 'Denoising', 'Validation Set', 'Constriction', 'Black Box', 'Image Segmentation', 'Generative Adversarial Networks', 'Recognition Accuracy', 'Output Image', 'Binary Code', 'Loss Of Components', 'U-Net Architecture', 'Equal Error Rate', 'Identity Preservation', 'L1 Loss', 'Pupil Constriction', 'Rubber Sheet', 'Autoencoder Architecture', 'Perceptual Loss', 'Max-pooling']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"Nonlinear iris texture deformations due to pupil size variations are one of the main factors responsible for within-class variance of genuine comparison scores in iris recognition. In dominant approaches to iris recognition, the size of a ring-shaped iris region is linearly scaled to a canonical rectangle, used further in encoding and matching. However, the biological complexity of the iris sphincter and dilator muscles causes the movements of iris features to be nonlinear in a function of pupil size, and not solely organized along radial paths. Alternatively to the existing theoretical models based on the biomechanics of iris musculature, in this paper we propose a novel deep autoencoder-based model that can effectively learn complex movements of iris texture features directly from the data. The proposed model takes two inputs, (a) an ISO-compliant near-infrared iris image with initial pupil size, and (b) the binary mask defining the target shape of the iris. The model makes all the necessary nonlinear deformations to the iris texture to match the shape of the iris in an image (a) with the shape provided by the target mask (b). The identity-preservation component of the loss function helps the model in finding deformations that preserve identity and not only the visual realism of the generated samples. We also demonstrate two immediate applications of this model: better compensation for iris texture deformations in iris recognition algorithms, compared to linear models, and the creation of a generative algorithm that can aid human forensic examiners, who may need to compare iris images with a large difference in pupil dilation. We offer the source codes and model weights available along with this paper."
Delving Into Masked Autoencoders for Multi-Label Thorax Disease Classification,"Junfei Xiao, Yutong Bai, Alan Yuille, Zongwei Zhou",Johns Hopkins University,100,USA,0,,"Vision Transformer (ViT) has become one of the most popular neural architectures due to its simplicity, scalability, and compelling performance in multiple vision tasks. However, since the scales of medical datasets are relatively small, ViT has shown inferior performance on medical datasets even after pre-trained on ImageNet. In this paper, we unleash the potential of ViT by pre-training on 266,340 unlabeled chest X-rays. Specifically, we explore Masked Autoencoders (MAE) whose task is to reconstruct missing pixels from a small proportion of each image and figure out a strong recipe for pre-training MAE and fine-tuning on chest X-ray datasets, revealing that medical reconstruction needs a much smaller proportion of an image than natural images (10% vs. 25%) and a more moderate RandomResizedCrop cropping range than natural images (0.5 1.0 vs. 0.2 1.0). With our recipe, ViT-S shows competitive results with the state-of-the-art CNN model (DenseNet-121) on three public chest X-ray datasets and 2.5x faster pre-training on the NIH ChestX-ray14 dataset and CheXpert. To the best of our knowledge, we are the first to make vanilla ViT achieve state-of-the-art performance on chest X-ray datasets. We hope that this study can direct future research on the application of Transformers to a larger variety of medical imaging tasks. Code will be made available.",https://openaccess.thecvf.com/content/WACV2023/html/Xiao_Delving_Into_Masked_Autoencoders_for_Multi-Label_Thorax_Disease_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xiao_Delving_Into_Masked_Autoencoders_for_Multi-Label_Thorax_Disease_Classification_WACV_2023_paper.pdf,,https://github.com/lambert-x/Medical MAE,2210.12843,main,Poster,https://ieeexplore.ieee.org/document/10030330/,"['Scalability', 'Transfer learning', 'X-rays', 'Transformers', 'Thorax', 'Convolutional neural networks', 'Task analysis']","['Multi-label', 'Thorax Disease', 'Medical Imaging', 'Convolutional Neural Network', 'Chest X-ray', 'Medical Data', 'Transfer Learning', 'Variety Of Tasks', 'Vision Tasks', 'Medical Tasks', 'Imaging Tasks', 'Vision Transformer', 'Computer Vision', 'Fair Comparison', 'Image Reconstruction', 'Data Augmentation', 'ImageNet', 'Bounding Box', 'Normal Pattern', 'Photographic Images', 'Radiographic Images', 'Transformer Architecture', 'Class Activation Maps', 'Pre-trained Convolutional Neural Network', 'Learning Rate Set', 'Self-supervised Learning', 'Transformer Block', 'X-ray Dataset']",['Applications: Biomedical/healthcare/medicine'],36,"Vision Transformer (ViT) has become one of the most popular neural architectures due to its great scalability, computational efficiency, and compelling performance in many vision tasks. However, ViT has shown inferior performance to Convolutional Neural Network (CNN) on medical tasks due to its data-hungry nature and the lack of an-notated medical data. In this paper, we pre-train ViTs on 266,340 chest X-rays using Masked Autoencoders (MAE) which reconstruct missing pixels from a small part of each image. For comparison, CNNs are also pre-trained on the same 266,340 X-rays using advanced self-supervised methods (e.g. MoCo v2). The results show that our pre-trained ViT performs comparably (sometimes better) to the state-of-the-art CNN (DenseNet-121) for multi-label thorax dis-ease classification. This performance is attributed to the strong recipes extracted from our empirical studies for pre-training and fine-tuning ViT. The pre-training recipe signifies that medical reconstruction requires a much smaller proportion of an image (10% vs. 25%) and a more moderate random resized crop range (0.5∼1.0 vs. 0.2∼1.0) compared with natural imaging. Furthermore, we remark that in-domain transfer learning is preferred whenever possible. The fine-tuning recipe discloses that layer-wise LR decay, RandAug magnitude, and DropPath rate are significant factors to consider. We hope that this study can direct future research on the application of Transformers to a larger variety of medical imaging tasks."
Dense Prediction With Attentive Feature Aggregation,"Yung-Hsu Yang, Thomas E. Huang, Min Sun, Samuel Rota Bulò, Peter Kontschieder, Fisher Yu",ETH Zürich; National Tsing Hua University; Facebook Reality Labs,66.66666667,"Switzerland, Taiwan",33.33333333,USA,"Aggregating information from features across different layers is essential for dense prediction models. Despite its limited expressiveness, vanilla feature concatenation dominates the choice of aggregation operations. In this paper, we introduce Attentive Feature Aggregation (AFA) to fuse different network layers with more expressive non-linear operations. AFA exploits both spatial and channel attention to compute weighted averages of the layer activations. Inspired by neural volume rendering, we further extend AFA with Scale-Space Rendering (SSR) to perform a late fusion of multi-scale predictions. AFA is applicable to a wide range of existing network designs. Our experiments show consistent and significant improvements on challenging semantic segmentation benchmarks, including Cityscapes and BDD100K at negligible computational and parameter overhead. In particular, AFA improves the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on Cityscapes. Our experimental analyses show that AFA learns to progressively refine segmentation maps and improve boundary details, leading to new state-of-the-art results on boundary detection benchmarks on NYUDv2 and BSDS500.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Dense_Prediction_With_Attentive_Feature_Aggregation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Dense_Prediction_With_Attentive_Feature_Aggregation_WACV_2023_paper.pdf,,,2111.0077,main,Poster,https://ieeexplore.ieee.org/document/10030736/,"['Computer vision', 'Costs', 'Fuses', 'Inference mechanisms', 'Computational modeling', 'Semantic segmentation', 'Predictive models']","['Feature Aggregation', 'Dense Prediction', 'Semantic Segmentation', 'Spatial Attention', 'Computational Overhead', 'Channel Attention', 'Feature Concatenation', 'Boundary Detection', 'Aggregation Operators', 'Volume Rendering', 'Validation Set', 'Spatial Information', 'Feature Maps', 'Input Features', 'Attention Mechanism', 'Segmentation Model', 'Linear Operator', 'Fine Details', 'Attention Module', 'Feature Fusion', 'Hierarchical Attention', 'Fewer Parameters', 'Multiple Fusion', 'Mean Intersection Over Union', 'Auxiliary Loss', 'Multi-scale Features', 'Shallow Features', 'Softplus', 'Multi-scale Context', 'Aggregator Node']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",3,"Aggregating information from features across different layers is essential for dense prediction models. Despite its limited expressiveness, vanilla feature concatenation dominates the choice of aggregation operations. In this paper, we introduce Attentive Feature Aggregation (AFA) to fuse different network layers with more expressive non-linear operations. AFA exploits both spatial and channel attention to compute weighted averages of the layer activations. Inspired by neural volume rendering, we further extend AFA with Scale-Space Rendering (SSR) to perform a late fusion of multi-scale predictions. AFA is applicable to a wide range of existing network designs. Our experiments show consistent and significant improvements on challenging semantic segmentation benchmarks, including Cityscapes and BDD100K at negligible computational and parameter overhead. In particular, AFA improves the performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on Cityscapes. Our experimental analyses show that AFA learns to progressively refine segmentation maps and improve boundary details, leading to new state-of-the-art results on boundary detection benchmarks on NYUDv2 and BSDS500."
Dense Voxel Fusion for 3D Object Detection,"Anas Mahmoud, Jordan S. K. Hu, Steven L. Waslander",University of Toronto Robotics Institute,100,Canada,0,,"Camera and LiDAR sensor modalities provide complementary appearance and geometric information useful for detecting 3D objects for autonomous vehicle applications. However, current end-to-end fusion methods are challenging to train and underperform state-of-the-art LiDAR-only detectors. Sequential fusion methods suffer from a limited number of pixel and point correspondences due to point cloud sparsity, or their performance is strictly capped by the detections of one of the modalities. Our proposed solution, Dense Voxel Fusion (DVF) is a sequential fusion method that generates multi-scale dense voxel feature representations, improving expressiveness in low point density regions. To enhance multi-modal learning, we train directly with projected ground truth 3D bounding box labels, avoiding noisy, detector-specific 2D predictions. Both DVF and the multi-modal training approach can be applied to any voxel-based LiDAR backbone. DVF ranks 3rd among published fusion methods on KITTI's 3D car detection benchmark without introducing additional trainable parameters, nor requiring stereo images or dense depth labels. In addition, DVF significantly improves 3D vehicle detection performance of voxel-based methods on the Waymo Open Dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Mahmoud_Dense_Voxel_Fusion_for_3D_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mahmoud_Dense_Voxel_Fusion_for_3D_Object_Detection_WACV_2023_paper.pdf,,,2203.00871,main,Poster,https://ieeexplore.ieee.org/document/10030866/,"['Training', 'Point cloud compression', 'Three-dimensional displays', 'Laser radar', 'Vehicle detection', 'Object detection', 'Detectors']","['Object Detection', '3D Object Detection', 'Dense Voxels', 'Ground-truth', 'Sequencing Methods', 'Point Clouds', 'Bounding Box', 'Fusion Method', 'Dense Representations', 'Dense Labeling', '3D Detector', 'Sequential Fusion', 'Sparse Point Cloud', 'Ground-truth 3D', '3D Bounding Boxes', 'Sensor Modalities', 'Multi-modal Training', 'Training Set', 'False Negatives', 'Imaging Features', 'Ground Truth Samples', 'Erroneous Prediction', 'KITTI Dataset', 'LIDAR Data', 'Foreground Object', 'LiDAR Point Cloud', 'Objects In The Scene', 'Central Voxel', 'Relative Gain', 'Lidar Point']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",34,"Camera and LiDAR sensor modalities provide complementary appearance and geometric information useful for detecting 3D objects for autonomous vehicle applications. However, current end-to-end fusion methods are challenging to train and underperform state-of-the-art LiDAR-only detectors. Sequential fusion methods suffer from a limited number of pixel and point correspondences due to point cloud sparsity, or their performance is strictly capped by the detections of one of the modalities. Our proposed solution, Dense Voxel Fusion (DVF) is a sequential fusion method that generates multi-scale dense voxel feature representations, improving expressiveness in low point density regions. To enhance multi-modal learning, we train directly with projected ground truth 3D bounding box labels, avoiding noisy, detector-specific 2D predictions. Both DVF and the multi-modal training approach can be applied to any voxel-based LiDAR backbone. DVF ranks 3
<sup>rd</sup>
 among published fusion methods on KITTI’s 3D car detection benchmark without introducing additional trainable parameters, nor requiring stereo images or dense depth labels. In addition, DVF significantly improves 3D vehicle detection performance of voxel-based methods on the Waymo Open Dataset."
Dense but Efficient VideoQA for Intricate Compositional Reasoning,"Jihyeon Lee, Wooyoung Kang, Eun-Sol Kim","Kakao Brain; Department of Computer Science, Hanyang University",50,South Korea,50,South Korea,"It is well known that most of the conventional video question answering (VideoQA) datasets consist of easy questions requiring simple reasoning processes. However, long videos inevitably contain complex and compositional semantic structures along with the spatio-temporal axis, which requires a model to understand the compositional structures inherent in the videos. In this paper, we suggest a new compositional VideoQA method based on transformer architecture with a deformable attention mechanism to address the complex VideoQA tasks. The deformable attentions are introduced to sample a subset of informative visual features from the dense visual feature map to cover a temporally long range of frames efficiently. Furthermore, the dependency structure within the complex question sentences is also combined with the language embeddings to readily understand the relations among question words. Extensive experiments and ablation studies show that the suggested dense but efficient model outperforms other baselines.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Dense_but_Efficient_VideoQA_for_Intricate_Compositional_Reasoning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Dense_but_Efficient_VideoQA_for_Intricate_Compositional_Reasoning_WACV_2023_paper.pdf,,,2210.103,main,Poster,https://ieeexplore.ieee.org/document/10030999/,"['Representation learning', 'Deformable models', 'Visualization', 'Computer vision', 'Computational modeling', 'Semantics', 'Transformers']","['Feature Maps', 'Feature Information', 'Extensive Experiments', 'Visual Features', 'Attention Mechanism', 'Density Map', 'Question Answering', 'Feature Subset', 'Dependence Structure', 'Density Model', 'Question Wording', 'Semantic Structure', 'Transformer Architecture', 'Extensive Ablation', 'Random Sampling', 'Local Features', 'Training Phase', 'Sampling Density', 'Video Clips', 'Visual Question Answering', 'Short Clips', 'Fine-grained Features', 'Attention Heads', 'Sparse Sampling', 'Transformer Layers', 'Gram Matrix', 'Attention Module', 'Substring', 'Motion Features']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",1,"It is well known that most of the conventional video question answering (VideoQA) datasets consist of easy questions requiring simple reasoning processes. However, long videos inevitably contain complex and compositional semantic structures along with the spatio-temporal axis, which requires a model to understand the compositional structures inherent in the videos. In this paper, we suggest a new compositional VideoQA method based on transformer architecture with a deformable attention mechanism to address the complex VideoQA tasks. The deformable attentions are introduced to sample a subset of informative visual features from the dense visual feature map to cover a temporally long range of frames efficiently. Furthermore, the dependency structure within the complex question sentences is also combined with the language embeddings to readily understand the relations among question words. Extensive experiments and ablation studies show that the suggested dense but efficient model outperforms other baselines."
Detection Recovery in Online Multi-Object Tracking With Sparse Graph Tracker,"Jeongseok Hyun, Myunggu Kang, Dongyoon Wee, Dit-Yan Yeung","Clova AI, NAVER Corp.; The Hong Kong University of Science and Technology",50,Hong Kong,50,South Korea,"In existing joint detection and tracking methods, pairwise relational features are used to match previous tracklets to current detections. However, the features may not be discriminative enough for a tracker to identify a target from a large number of detections. Selecting only high-scored detections for tracking may lead to missed detections whose confidence score is low. Consequently, in the online setting, this results in disconnections of tracklets which cannot be recovered. In this regard, we present Sparse Graph Tracker (SGT), a novel online graph tracker using higher-order relational features which are more discriminative by aggregating the features of neighboring detections and their relations. SGT converts video data into a graph where detections, their connections, and the relational features of two connected nodes are represented by nodes, edges, and edge features, respectively. The strong edge features allow SGT to track targets with tracking candidates selected by top-K scored detections with large K. As a result, even low-scored detections can be tracked, and the missed detections are also recovered. The robustness of K value is shown through the extensive experiments. In the MOT16/17/20 and HiEve Challenge, SGT outperforms the state-of-the-art trackers with real-time inference speed. Especially, a large improvement in MOTA is shown in the MOT20 and HiEve Challenge. Code is available at https://github.com/HYUNJS/SGT.",https://openaccess.thecvf.com/content/WACV2023/html/Hyun_Detection_Recovery_in_Online_Multi-Object_Tracking_With_Sparse_Graph_Tracker_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hyun_Detection_Recovery_in_Online_Multi-Object_Tracking_With_Sparse_Graph_Tracker_WACV_2023_paper.pdf,,https://github.com/HYUNJS/SGT,2205.00968,main,Poster,https://ieeexplore.ieee.org/document/10030164/,"['Computer vision', 'Target tracking', 'Codes', 'Image edge detection', 'Feature extraction', 'Robustness', 'Real-time systems']","['Sparse Graph', 'Multi-object Tracking', 'Related Features', 'Extensive Experiments', 'Edge Features', 'Online Setting', 'Inference Speed', 'Higher-order Features', 'Object Detection', 'Intersection Over Union', 'Kalman Filter', 'Technical Training', 'Coordinating Center', 'Tracking Performance', 'Appearance Features', 'Node Features', 'Inference Procedure', 'Graph Neural Networks', 'Current Frame', 'Fc Block', 'Edge Score', 'Intersection Over Union Score', 'Ground Truth Object', 'Pseudo Labels', 'Motion Prediction', 'Pedestrian Detection', 'Node Score', 'Edge Labels', 'Ablation Experiments', 'FC Layer']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",25,"In existing joint detection and tracking methods, pairwise relational features are used to match previous track-lets to current detections. However, the features may not be discriminative enough for a tracker to identify a target from a large number of detections. Selecting only high-scored detections for tracking may lead to missed detections whose confidence score is low. Consequently, in the on-line setting, this results in disconnections of tracklets which cannot be recovered. In this regard, we present Sparse Graph Tracker (SGT), a novel online graph tracker using higher-order relational features which are more discriminative by aggregating the features of neighboring detections and their relations. SGT converts video data into a graph where detections, their connections, and the relational features of two connected nodes are represented by nodes, edges, and edge features, respectively. The strong edge features allow SGT to track targets with tracking candidates selected by top-K scored detections with large K. As a result, even low-scored detections can be tracked, and the missed detections are also recovered. The robustness of K value is shown through the extensive experiments. In the MOT16/17/20 and HiEve Challenge, SGT outperforms the state-of-the-art trackers with real-time inference speed. Especially, a large improvement in MOTA is shown in the MOT20 and HiEve Challenge. Code is available at https://github.com/HYUNJS/SGT."
Diffeomorphic Image Registration With Neural Velocity Field,"Kun Han, Shanlin Sun, Xiangyi Yan, Chenyu You, Hao Tang, Junayed Naushad, Haoyu Ma, Deying Kong, Xiaohui Xie","University of California, Irvine, USA; Yale University, USA",100,USA,0,,"Diffeomorphic image registration, offering smooth transformation and topology preservation, is required in many medical image analysis tasks.Traditional methods impose certain modeling constraints on the space of admissible transformations and use optimization to find the optimal transformation between two images. Specifying the right space of admissible transformations is challenging: the registration quality can be poor if the space is too restrictive, while the optimization can be hard to solve if the space is too general. Recent learning-based methods, utilizing deep neural networks to learn the transformation directly, achieve fast inference, but face challenges in accuracy due to the difficulties in capturing the small local deformations and generalization ability. Here we propose a new optimization-based method named DNVF (Diffeomorphic Image Registration with Neural Velocity Field) which utilizes deep neural network to model the space of admissible transformations. A multilayer perceptron (MLP) with sinusoidal activation function is used to represent the continuous velocity field and assigns a velocity vector to every point in space, providing the flexibility of modeling complex deformations as well as the convenience of optimization. Moreover, we propose a cascaded image registration framework (Cas-DNVF) by combining the benefits of both optimization and learning based methods, where a fully convolutional neural network (FCN) is trained to predict the initial deformation, followed by DNVF for further refinement. Experiments on two large-scale 3D MR brain scan datasets demonstrate that our proposed methods significantly outperform the state-of-the-art registration methods.",https://openaccess.thecvf.com/content/WACV2023/html/Han_Diffeomorphic_Image_Registration_With_Neural_Velocity_Field_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Han_Diffeomorphic_Image_Registration_With_Neural_Velocity_Field_WACV_2023_paper.pdf,,,2202.12498,main,Poster,https://ieeexplore.ieee.org/document/10030312/,"['Deformable models', 'Learning systems', 'Deep learning', 'Image registration', 'Three-dimensional displays', 'Neural networks', 'Brain modeling']","['Flow Velocity', 'Image Registration', 'Neural Field', 'Diffeomorphic Image Registration', 'Neural Network', 'Convolutional Neural Network', 'Deep Network', 'Deep Neural Network', 'Multilayer Perceptron', 'Learning-based Methods', 'Velocity Vector', 'Local Deformation', 'Small Deformation', 'Registration Method', 'Medical Image Analysis', 'Space Transformation', 'Continuous Field', 'Optimization-based Methods', 'Complex Deformation', 'Input Image', 'Image Pairs', 'Deformable Registration', 'Neural Representations', 'Deformation Field', 'Dice Similarity Coefficient', '3D Coordinates', 'Precise Matching', 'Jacobian Determinant', 'Registration Problem', 'Normalized Cross-correlation']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",7,"Diffeomorphic image registration, offering smooth transformation and topology preservation, is required in many medical image analysis tasks. Traditional methods impose certain modeling constraints on the space of admissible transformations and use optimization to find the optimal transformation between two images. Specifying the right space of admissible transformations is challenging: the registration quality can be poor if the space is too restrictive, while the optimization can be hard to solve if the space is too general. Recent learning-based methods, utilizing deep neural networks to learn the transformation directly, achieve fast inference, but face challenges in accuracy due to the difficulties in capturing the small local deformations and generalization ability. Here we propose a new optimization-based method named DNVF (Diffeomorphic Image Registration with Neural Velocity Field) which utilizes deep neural network to model the space of admissible transformations. A multilayer perceptron (MLP) with sinusoidal activation function is used to represent the continuous velocity field and assigns a velocity vector to every point in space, providing the flexibility of modeling complex deformations as well as the convenience of optimization. Moreover, we propose a cascaded image registration framework (Cas-DNVF) by combining the benefits of both optimization and learning based methods, where a fully convolutional neural network (FCN) is trained to predict the initial deformation, followed by DNVF for further refinement. Experiments on two large-scale 3D MR brain scan datasets demonstrate that our proposed methods significantly outperform the state-of-the-art registration methods."
Difficulty-Net: Learning To Predict Difficulty for Long-Tailed Recognition,"Saptarshi Sinha, Hiroki Ohashi","Hitachi Ltd., Tokyo, Japan",0,,100,Japan,"Long-tailed datasets, where head classes comprise much more training samples than tail classes, cause recognition models to get biased towards the head classes. Weighted loss is one of the most popular ways of mitigating this issue, and a recent work has suggested that class-difficulty might be a better clue than conventionally used class-frequency to decide the distribution of weights. A heuristic formulation was used in the previous work for quantifying the difficulty, but we empirically find that the optimal formulation varies depending on the characteristics of datasets. Therefore, we propose Difficulty-Net, which learns to predict the difficulty of classes using the model's performance in a meta-learning framework. To make it learn reasonable difficulty of a class within the context of other classes, we newly introduce two key concepts, namely the relative difficulty and the driver loss. The former helps Difficulty-Net take other classes into account when calculating difficulty of a class, while the latter is indispensable for guiding the learning to a meaningful direction. Extensive experiments on popular long-tailed datasets demonstrated the effectiveness of the proposed method, and it achieved state-of-the-art performance on multiple long-tailed datasets. Code is available at https://github.com/hitachi-rd-cv/Difficulty_Net.",https://openaccess.thecvf.com/content/WACV2023/html/Sinha_Difficulty-Net_Learning_To_Predict_Difficulty_for_Long-Tailed_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sinha_Difficulty-Net_Learning_To_Predict_Difficulty_for_Long-Tailed_Recognition_WACV_2023_paper.pdf,,https://github.com/hitachi-rd-cv/Difficulty_Net,,main,Poster,https://ieeexplore.ieee.org/document/10031019/,"['Training', 'Computer vision', 'Codes', 'Tail', 'Predictive models']","['Long-tailed Recognition', 'Multiple Datasets', 'Relative Difficulty', 'Imbalance', 'Hidden Layer', 'Softmax', 'Knowledge Transfer', 'Simple Way', 'Representation Learning', 'Metric Learning', 'Difficulties Score', 'Cost-sensitive Learning', 'Equal Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",6,"Long-tailed datasets, where head classes comprise much more training samples than tail classes, cause recognition models to get biased towards the head classes. Weighted loss is one of the most popular ways of mitigating this issue, and a recent work has suggested that class-difficulty might be a better clue than conventionally used class-frequency to decide the distribution of weights. A heuristic formulation was used in the previous work for quantifying the difficulty, but we empirically find that the optimal formulation varies depending on the characteristics of datasets. Therefore, we propose Difficulty-Net, which learns to predict the difficulty of classes using the model’s performance in a meta-learning framework. To make it learn reasonable difficulty of a class within the context of other classes, we newly introduce two key concepts, namely the relative difficulty and the driver loss. The former helps Difficulty-Net take other classes into account when calculating difficulty of a class, while the latter is indispensable for guiding the learning to a meaningful direction. Extensive experiments on popular long-tailed datasets demonstrated the effectiveness of the proposed method, and it achieved state-of-the-art performance on multiple long-tailed datasets. Code is available at https://github.com/hitachi-rd-cv/Difficulty_Net."
DigiFace-1M: 1 Million Digital Face Images for Face Recognition,"Gwangbin Bae, Martin de La Gorce, Tadas Baltrušaitis, Charlie Hewitt, Dong Chen, Julien Valentin, Roberto Cipolla, Jingjing Shen",Microsoft; University of Cambridge,50,UK,50,USA,"State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a large-scale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline. We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to SynFace, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images.",https://openaccess.thecvf.com/content/WACV2023/html/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.pdf,,https://github.com/microsoft/DigiFace1M,,main,Poster,https://ieeexplore.ieee.org/document/10030286/,"['Ethics', 'Computer vision', 'Error analysis', 'Face recognition', 'Computational modeling', 'Pipelines', 'Lighting']","['Face Recognition', 'Face Images', 'Digital Face', 'Data Augmentation', 'Number Of Images', 'Large-scale Datasets', 'Label Noise', 'Real Faces', 'Millions Of Images', 'Face Recognition Model', 'Model Parameters', 'Image Quality', 'Deep Neural Network', 'Ethical Issues', 'Gaussian Blur', 'Synthetic Images', 'Hair Color', 'Large Number Of Images', 'Eye Color', 'Hairdressers', 'Pose Changes', 'Combination Of Geometry', 'Horizontal Angle', 'Deep Generative Models', 'Face Model', 'Vertical Angle', 'Noise Bias']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",58,"State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, makeup, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a large-scale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline
<sup>1</sup>
. We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to Syn-Face, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reason-ably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images."
Discrete Cosin TransFormer: Image Modeling From Frequency Domain,"Xinyu Li, Yanyi Zhang, Jianbo Yuan, Hanlin Lu, Yibo Zhu",ByteDance; Rutgers University,50,USA,50,China,"In this paper, we propose Discrete Cosin TransFormer (DCFormer) that directly learn semantics from DCT-based frequency domain representation. We first show that transformer-based networks are able to learn semantics directly from frequency domain representation based on discrete cosine transform (DCT) without compromising the performance. To achieve the desired efficiency-effectiveness trade-off, we then leverage an input information compression on its frequency domain representation, which highlights the visually significant signals inspired by JPEG compression. We explore different frequency domain down-sampling strategies and show that it is possible to preserve the semantic meaningful information by strategically dropping the high-frequency components. The proposed DCFormer is tested on various downstream tasks including image classification, object detection and instance segmentation, and achieves state-of-the-art comparable performance with less FLOPs, and outperforms the commonly used backbone (e.g. SWIN) at similar FLOPs. Our ablation results also show that the proposed method generalizes well on different transformer backbones.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Discrete_Cosin_TransFormer_Image_Modeling_From_Frequency_Domain_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Discrete_Cosin_TransFormer_Image_Modeling_From_Frequency_Domain_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030911/,"['Wavelet domain', 'Frequency-domain analysis', 'Computational modeling', 'Semantics', 'Refining', 'Transform coding', 'Object detection']","['Frequency Domain', 'Semantic', 'Ablation', 'Performance Comparison', 'Image Classification', 'Object Detection', 'Semantic Information', 'Instance Segmentation', 'Discrete Cosine Transform', 'JPEG Compression', 'Frequency Domain Representation', 'Convolutional Network', 'Frequency Band', 'Input Image', 'Feature Maps', 'Efficient Model', 'Detection Task', 'Frequency Components', 'RGB Images', 'Relative Location', 'Frequency-domain Model', 'Object Detection Task', 'Frequency Representation', 'Image Compression', 'Input Resolution', 'Patch Size', 'Semantic Representations', 'Compression Ratio', 'Frequency Information', 'Additional Calculations']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Low-level and physics-based vision']",7,"In this paper, we propose Discrete Cosin TransFormer (DCFormer) that directly learn semantics from DCT-based frequency domain representation. We first show that transformer-based networks are able to learn semantics directly from frequency domain representation based on discrete cosine transform (DCT) without compromising the performance. To achieve the desired efficiency-effectiveness trade-off, we then leverage an input information compression on its frequency domain representation, which highlights the visually significant signals inspired by JPEG compression. We explore different frequency domain downsampling strategies and show that it is possible to preserve the semantic meaningful information by strategically dropping the high-frequency components. The proposed DCFormer is tested on various downstream tasks including image classification, object detection and instance segmentation, and achieves state-of-the-art comparable performance with less FLOPs, and outperforms the commonly used backbone (e.g. SWIN) at similar FLOPs. Our ablation results also show that the proposed method generalizes well on different transformer backbones."
Dissecting Deep Metric Learning Losses for Image-Text Retrieval,"Hong Xuan, Xi (Stephen) Chen",Microsoft,0,,100,USA,"Visual-Semantic Embedding (VSE) is a prevalent approach in image-text retrieval by learning a joint embedding space between the image and language modalities where semantic similarities would be preserved. The triplet loss with hard-negative mining has become the de-facto objective for most VSE methods. Inspired by recent progress in deep metric learning (DML) in the image domain which gives rise to new loss functions that outperform triplet loss, in this paper we revisit the problem of finding better objectives for VSE in image-text matching. Despite some attempts in designing losses based on gradient movement, most DML losses are defined empirically in the embedding space. Instead of directly applying these loss functions which may lead to sub-optimal gradient updates in model parameters, in this paper we present a novel Gradient-based Objective AnaLysis framework, or GOAL, to systematically analyze the combinations and reweighting of the gradients in existing DML functions. With the help of this analysis framework, we further propose a new family of objectives in the gradient space exploring different gradient combinations. In the event that the gradients are not integrable to a valid loss function, we implement our proposed objectives such that they would directly operate in the gradient space instead of on the losses in the embedding space. Comprehensive experiments have demonstrated that our novel objectives have consistently improved performance over baselines across different visual/text features and model frameworks. We also showed the generalizability of the GOAL framework by extending it to other models using triplet family losses including vision-language model with heavy cross-modal interactions and have achieved state-of-the-art results on the image-text retrieval tasks on COCO and Flick30K.",https://openaccess.thecvf.com/content/WACV2023/html/Xuan_Dissecting_Deep_Metric_Learning_Losses_for_Image-Text_Retrieval_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xuan_Dissecting_Deep_Metric_Learning_Losses_for_Image-Text_Retrieval_WACV_2023_paper.pdf,,,2210.13188,main,Poster,https://ieeexplore.ieee.org/document/10030372/,"['Measurement', 'Training', 'Computer vision', 'Analytical models', 'Semantics', 'Space exploration', 'Task analysis']","['Deep Learning', 'Metric Learning', 'Learning Loss', 'Deep Metric Learning', 'Metric Learning Loss', 'Image-text Retrieval', 'Deep Metric Learning Loss', 'Loss Function', 'Integrable', 'Latent Space', 'Semantic Similarity', 'Triplet Loss', 'Gradient Update', 'Framework Of Goals', 'Image Features', 'Visual Features', 'Image Pairs', 'Gradient Method', 'Early Loss', 'Batch Of Images', 'Textual Features', 'Instance Pairs', 'Grid Features', 'Fine-tuning Step', 'Corner Cases', 'Gradient Components', 'Contrastive Loss', 'Joint Space', 'Exponential Term']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Visual-Semantic Embedding (VSE) is a prevalent approach in image-text retrieval by learning a joint embedding space between the image and language modalities where semantic similarities would be preserved. The triplet loss with hard-negative mining has become the de-facto objective for most VSE methods. Inspired by recent progress in deep metric learning (DML) in the image domain which gives rise to new loss functions that outperform triplet loss, in this paper we revisit the problem of finding better objectives for VSE in image-text matching. Despite some attempts in designing losses based on gradient movement, most DML losses are defined empirically in the embedding space. Instead of directly applying these loss functions which may lead to sub-optimal gradient updates in model parameters, in this paper we present a novel Gradient-based Objective AnaLysis framework, or GOAL, to systematically analyze the combinations and reweighting of the gradients in existing DML functions. With the help of this analysis framework, we further propose a new family of objectives in the gradient space exploring different gradient combinations. In the event that the gradients are not integrable to a valid loss function, we implement our proposed objectives such that they would directly operate in the gradient space instead of on the losses in the embedding space. Comprehensive experiments have demonstrated that our novel objectives have consistently improved performance over baselines across different visual/text features and model frameworks. We also showed the generalizability of the GOAL framework by extending it to other models using triplet family losses including vision-language model with heavy cross-modal interactions and have achieved state-of-the-art results on the image-text retrieval tasks on COCO and Flick30K."
Do Adaptive Active Attacks Pose Greater Risk Than Static Attacks?,"Nathan Drenkow, Max Lennon, I-Jeng Wang, Philippe Burlina",The Johns Hopkins University Applied Physics Laboratory,100,USA,0,,"In contrast to perturbation-based attacks, patch-based attacks are physically realizable, and are therefore increasingly studied. However, prior work neglects the possibility of adaptive attacks optimized for 3D pose. For the first time, to our knowledge, we consider the challenge of designing and evaluating attacks on image sequences using 3D optimization along entire 3D kinematic trajectories. In this context, we study a type of dynamic attack, referred to as ""adaptive active attacks"" (AAA), that takes into consideration the pose of the observer being targeted. To better address the threat and risk posed by AAA attacks, we develop several novel risk-based and trajectory-based metrics. These are designed to capture the risk of attack success for attacking earlier in the trajectory to derail autonomous driving systems as well as tradeoffs that may arise given the possibility of additional detection. We evaluate performance of white-box targeted attacks using a subset of ImageNet classes, and demonstrate, in aggregate, that AAA attacks can pose threats beyond static attacks in kinematic settings in situations of predominantly looming motion (i.,e., a prevalent use case in automated vehicular navigation). Results demonstrate that AAA attacks can exhibit targeted attack success exceeding 10% in aggregate, and for some specific classes, up to 15% over their static counterparts. However, taking into consideration the probability of detection by the defender shows a more nuanced risk pattern. These new insights are important for guiding future adversarial machine learning studies and suggest researchers should consider defense against novel threats posed by dynamic attacks for full trajectories and videos.",https://openaccess.thecvf.com/content/WACV2023/html/Drenkow_Do_Adaptive_Active_Attacks_Pose_Greater_Risk_Than_Static_Attacks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Drenkow_Do_Adaptive_Active_Attacks_Pose_Greater_Risk_Than_Static_Attacks_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030934/,"['Measurement', 'Three-dimensional displays', 'Sensitivity analysis', 'Aggregates', 'Kinematics', 'Observers', 'Trajectory']","['Specific Categories', 'Generative Adversarial Networks', 'Detection Probability', 'Types Of Attacks', 'Attack Success', 'Entire Trajectory', '3D Pose', 'Classification Accuracy', 'Autonomic System', 'Training Time', 'Pedestrian', 'Light Signal', 'Target Class', 'Test Points', 'Distance Range', 'Navigation System', 'Risk Of Onset', 'Background Image', 'Specific Intervals', 'Artificial Intelligence Applications', 'Range Of Transformations', 'Set Of Transformations', 'Camera Distance', 'Risk Metrics', 'Roll Angle', 'Adversarial Attacks', 'AI Systems']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",,"In contrast to perturbation-based attacks, patch-based attacks are physically realizable, and are therefore increasingly studied. However, prior work neglects the possibility of adaptive attacks optimized for 3D pose. For the first time, to our knowledge, we consider the challenge of designing and evaluating attacks on image sequences using 3D optimization along entire 3D kinematic trajectories. In this context, we study a type of dynamic attack, referred to as ""adaptive active attacks"" (AAA), that takes into consideration the pose of the observer being targeted. To better address the threat and risk posed by AAA attacks, we develop several novel risk-based and trajectory-based metrics. These are designed to capture the risk of attack success for attacking earlier in the trajectory to derail autonomous driving systems as well as tradeoffs that may arise given the possibility of additional detection. We evaluate performance of white-box targeted attacks using a subset of ImageNet classes, and demonstrate, in aggregate, that AAA attacks can pose threats beyond static attacks in kinematic settings in situations of predominantly looming motion (i. e., a prevalent use case in automated vehicular navigation). Results demonstrate that AAA attacks can exhibit targeted attack success exceeding 10% in aggregate, and for some specific classes, up to 15% over their static counterparts. However, taking into consideration the probability of detection by the defender shows a more nuanced risk pattern. These new insights are important for guiding future adversarial machine learning studies and suggest researchers should consider defense against novel threats posed by dynamic attacks for full trajectories and videos."
Do Pre-Trained Models Benefit Equally in Continual Learning?,"Kuan-Ying Lee, Yuanyi Zhong, Yu-Xiong Wang",University of Illinois at Urbana-Champaign,100,USA,0,,"A large part of the continual learning (CL) literature focuses on developing algorithms for models trained from scratch. While these algorithms work great with from-sc ratch trained models on widely used CL benchmarks, they show dramatic performance drops on more complex datasets (e.g., Split-CUB200). Pre-trained models, widely used to transfer knowledge to downstream tasks, could enhance these methods to be applicable in more realistic scenarios. However, surprisingly, improvements in CL algorithms from pre-training are inconsistent. For instance, while Incremental Classifier and Representation Learning (iCaRL) underperforms Supervised Contrastive Replay (SCR) when trained from scratch, it outperforms SCR when both are initialized with a pre-trained model. This indicates the paradigm current CL literature follows, where all methods are compared in from-scratch training, is not well reflective of the true CL objective and desired progress. Furthermore, we found 1) CL algorithms that exert less regularization benefit more from a pre-trained model; 2) a model pre-trained with a larger dataset (WebImageText in Contrastive Language-Image Pre-training (CLIP) vs. ImageNet) does not guarantee a better improvement. Based on these findings, we introduced a simple yet effective baseline that employs minimum regularization and leverages the more beneficial pre-trained model, which outperforms state-of-the-art methods when pre-training is applied. Our code is available at https://github.com/eric11220/pretrained-models-in-CL.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Do_Pre-Trained_Models_Benefit_Equally_in_Continual_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Do_Pre-Trained_Models_Benefit_Equally_in_Continual_Learning_WACV_2023_paper.pdf,,https://github.com/eric11220/pretrained-models-in-CL,2210.15701,main,Poster,https://ieeexplore.ieee.org/document/10030430/,"['Training', 'Computer vision', 'Systematics', 'Codes', 'Computational modeling', 'Pipelines', 'Benchmark testing']","['Incremental Learning', 'Evolutionary Algorithms', 'Real-world Scenarios', 'Strong Baseline', 'Encouraging Performance', 'Two-stage Training', 'Stochastic Gradient Descent', 'Data Streams', 'Classification Datasets', 'Complex Datasets', 'Regularization Method', 'Benefits Of Model', 'Random Cropping', 'Self-supervised Manner', 'Fine-tuning Strategy']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",5,"Existing work on continual learning (CL) is primarily devoted to developing algorithms for models trained from scratch. Despite their encouraging performance on contrived benchmarks, these algorithms show dramatic performance drop in real-world scenarios. Therefore, this paper advocates the systematic introduction of pre-training to CL, which is a general recipe for transferring knowledge to downstream tasks but is substantially missing in the CL community. Our investigation reveals the multifaceted complexity of exploiting pre-trained models for CL, along three different axes: pre-trained models, CL algorithms, and CL scenarios. Perhaps most intriguingly, improvements in CL algorithms from pre-training are very inconsistent – an underperforming algorithm could become competitive and even state of the art, when all algorithms start from a pretrained model. This indicates that the current paradigm, where all CL methods are compared in from-scratch training, is not well reflective of the true CL objective and desired progress. In addition, we make several other important observations, including that 1) CL algorithms that exert less regularization benefit more from a pre-trained model; and 2) a stronger pre-trained model such as CLIP does not guarantee a better improvement. Based on these findings, we introduce a simple yet effective baseline that employs minimum regularization and leverages the more beneficial pre-trained model, coupled with a two-stage training pipeline. We recommend including this strong baseline in the future development of CL algorithms, due to its demonstrated state-of-the-art performance. Our code is available at https://github.com/eric11220/pretrained-models-in-CL."
Domain Adaptation Using Self-Training With Mixup for One-Stage Object Detection,"Jitender Maurya, Keyur R. Ranipa, Osamu Yamaguchi, Tomoyuki Shibata, Daisuke Kobayashi",,,,,,"In this paper, we present an end-to-end domain adaptation technique that utilizes both feature distribution alignment and Self-Training effectively for object detection. One set of methods for domain adaptation relies on feature distribution alignment and adapts models on an unlabeled target domain by learning domain invariant representations through adversarial loss. Although this approach is effective, it may not be adequate or even have an adverse effect when domain shifts are large and inconsistent. Another set of methods utilizes Self-Training which relies on pseudo labels to approximate the target domain distribution directly. However, it can also have a negative impact on the model performance due to erroneous pseudo labels. To overcome these two issues, we propose to generate reliable pseudo labels through feature distribution alignment and data distillation. Further, to minimize the adverse effect of incorrect pseudo labels during Self-Training we employ interpolation-based consistency regularization called mixup. While distribution alignment helps in generating more accurate pseudo labels, mixup regularization of Self-Training reduces the adverse effect of less accurate pseudo labels. Both approaches supplement each other and achieve effective adaptation on the target domain which we demonstrate through extensive experiments on one-stage object detector. Experiment results show that our approach achieves a significant performance improvement on multiple benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Maurya_Domain_Adaptation_Using_Self-Training_With_Mixup_for_One-Stage_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Maurya_Domain_Adaptation_Using_Self-Training_With_Mixup_for_One-Stage_Object_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030551/,"['Adaptation models', 'Computer vision', 'Detectors', 'Object detection', 'Benchmark testing', 'Predictive models', 'Feature extraction']","['Object Detection', 'Domain Adaptation', 'One-stage Object Detection', 'Model Performance', 'Benchmark Datasets', 'Domain Shift', 'Target Domain', 'Effects Of Labels', 'Accurate Labels', 'Pseudo Labels', 'Domain Adaptation Methods', 'Incorrect Labels', 'Unlabeled Target Domain', 'Consistency Regularization', 'Domain Adaptation Techniques', 'Domain-invariant Representations', 'Classification Task', 'Real Samples', 'Bounding Box', 'Real-world Scenarios', 'Source Domain', 'Single Shot Multibox Detector', 'Target Domain Samples', 'Source Dataset', 'Hidden Representation', 'Domain Dataset', 'Object Location', 'Target Dataset', 'Domain Gap', 'mAP Improvement']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"In this paper, we present an end-to-end domain adaptation technique that utilizes both feature distribution alignment and Self-Training effectively for object detection. One set of methods for domain adaptation relies on feature distribution alignment and adapts models on an unlabeled target domain by learning domain invariant representations through adversarial loss. Although this approach is effective, it may not be adequate or even have an adverse effect when domain shifts are large and inconsistent. Another set of methods utilizes Self-Training which relies on pseudo labels to approximate the target domain distribution directly. However, it can also have a negative impact on the model performance due to erroneous pseudo labels. To overcome these two issues, we propose to generate reliable pseudo labels through feature distribution alignment and data distillation. Further, to minimize the adverse effect of incorrect pseudo labels during Self-Training we employ interpolation-based consistency regularization called mixup. While distribution alignment helps in generating more accurate pseudo labels, mixup regularization of Self-Training reduces the adverse effect of less accurate pseudo labels. Both approaches supplement each other and achieve effective adaptation on the target domain which we demonstrate through extensive experiments on one-stage object detector. Experiment results show that our approach achieves a significant performance improvement on multiple benchmark datasets."
Domain Adaptive Object Detection for Autonomous Driving Under Foggy Weather,"Jinlong Li, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, Hongkai Yu","Wuhan University; University of California, Los Angeles; Cleveland State University",100,"China, USA",0,,"Most object detection methods for autonomous driving usually assume a onsistent feature distribution between training and testing data, which is not always the case when weathers differ significantly. The object detection model trained under clear weather might be not effective enough on the foggy weather because of the domain gap. This paper proposes a novel domain adaptive object detection framework for autonomous driving under foggy weather. Our method leverages both image-level and object-level adaptation to diminish the domain discrepancy in image style and object appearance. To further enhance the model's capabilities under challenging samples, we also come up with a new adversarial gradient reversal layer to perform adversarial mining for the hard examples together with domain adaptation. Moreover, we propose to generate an auxiliary domain by data augmentation to enforce a new domain-level metric regularization. Experimental results on public benchmarks show the effectiveness and accuracy of the proposed method.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Domain_Adaptive_Object_Detection_for_Autonomous_Driving_Under_Foggy_Weather_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Domain_Adaptive_Object_Detection_for_Autonomous_Driving_Under_Foggy_Weather_WACV_2023_paper.pdf,,https://github.com/jinlong17/DA-Detect,2210.15176,main,Poster,https://ieeexplore.ieee.org/document/10030451/,"['Measurement', 'Training', 'Adaptation models', 'Computer vision', 'Transfer learning', 'Object detection', 'Feature extraction']","['Object Detection', 'Autonomous Vehicles', 'Domain Adaptation', 'Foggy Weather', 'Domain Adaptive Detection', 'Domain Adaptive Object Detection', 'Training Data', 'Distribution Characteristics', 'Data Augmentation', 'Adaptive Framework', 'Object Detection Methods', 'Object Appearance', 'Domain Gap', 'Public Benchmark', 'Style Image', 'Hard Examples', 'Clear Weather', 'Auxiliary Domains', 'Convolutional Neural Network', 'Source Domain', 'Target Domain', 'Faster R-CNN', 'Domain Classifier', 'Faster R-CNN Model', 'Domain Adaptation Methods', 'Transfer Learning', 'Region Proposal', 'Domain-invariant Features', 'Triplet Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",48,"Most object detection methods for autonomous driving usually assume a consistent feature distribution between training and testing data, which is not always the case when weathers differ significantly. The object detection model trained under clear weather might be not effective enough on the foggy weather because of the domain gap. This paper proposes a novel domain adaptive object detection framework for autonomous driving under foggy weather. Our method leverages both image-level and object-level adaptation to diminish the domain discrepancy in image style and object appearance. To further enhance the model’s capabilities under challenging samples, we also come up with a new adversarial gradient reversal layer to perform adversarial mining for the hard examples together with domain adaptation. Moreover, we propose to generate an auxiliary domain by data augmentation to enforce a new domain-level metric regularization. Experimental results on public benchmarks show the effectiveness and accuracy of the proposed method. The code is available at https://github.com/jinlong17/DA-Detect."
Domain Adaptive Video Semantic Segmentation via Cross-Domain Moving Object Mixing,"Kyusik Cho, Suhyeon Lee, Hongje Seong, Euntai Kim","School of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea",100,South Korea,0,,"The network trained for domain adaptation is prone to bias toward the easy-to-transfer classes. Since the ground truth label on the target domain is unavailable during training, the bias problem leads to skewed predictions, forgetting to predict hard-to-transfer classes. To address this problem, we propose Cross-domain Moving Object Mixing (CMOM) that cuts several objects, including hard-to-transfer classes, in the source domain video clip and pastes them into the target domain video clip. Unlike image-level domain adaptation, the temporal context should be maintained to mix moving objects in two different videos. Therefore, we design CMOM to mix with consecutive video frames, so that unrealistic movements are not occurring. We additionally propose Feature Alignment with Temporal Context (FATC) to enhance target domain feature discriminability. FATC exploits the robust source domain features, which are trained with ground truth labels, to learn discriminative target domain features in an unsupervised manner by filtering unreliable predictions with temporal consensus. We demonstrate the effectiveness of the proposed approaches through extensive experiments. In particular, our model reaches mIoU of 53.81% on VIPER -> Cityscapes-Seq benchmark and mIoU of 56.31% on SYNTHIA-Seq -> Cityscapes-Seq benchmark, surpassing the state-of-the-art methods by large margins.",https://openaccess.thecvf.com/content/WACV2023/html/Cho_Domain_Adaptive_Video_Semantic_Segmentation_via_Cross-Domain_Moving_Object_Mixing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cho_Domain_Adaptive_Video_Semantic_Segmentation_via_Cross-Domain_Moving_Object_Mixing_WACV_2023_paper.pdf,,,2211.02307,main,Poster,https://ieeexplore.ieee.org/document/10030889/,"['Training', 'Computer vision', 'Adaptation models', 'Filtering', 'Semantic segmentation', 'Benchmark testing']","['Semantic Segmentation', 'Domain Adaptation', 'Adaptive Segmentation', 'Domain Adaptive Segmentation', 'Video Semantic Segmentation', 'Video Clips', 'Discriminative Features', 'Video Frames', 'Robust Features', 'Domain Features', 'Ground Truth Labels', 'Target Domain', 'Consecutive Frames', 'Temporal Context', 'Source Domain', 'Problem Of Bias', 'Unsupervised Manner', 'Feature Alignment', 'Consecutive Video Frames', 'Feature Space', 'Previous Frame', 'Optical Flow', 'Data Augmentation', 'Adversarial Training', 'Mixing Ratio', 'Source Images', 'Generative Adversarial Networks', 'Data Augmentation Methods', 'Current Frame', 'Segmentation Branch']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"The network trained for domain adaptation is prone to bias toward the easy-to-transfer classes. Since the ground truth label on the target domain is unavailable during training, the bias problem leads to skewed predictions, forgetting to predict hard-to-transfer classes. To address this problem, we propose Cross-domain Moving Object Mixing (CMOM) that cuts several objects, including hard-to-transfer classes, in the source domain video clip and pastes them into the target domain video clip. Unlike image-level domain adaptation, the temporal context should be maintained to mix moving objects in two different videos. Therefore, we de-sign CMOM to mix with consecutive video frames, so that unrealistic movements are not occurring. We additionally propose Feature Alignment with Temporal Context (FATC) to enhance target domain feature discriminability. FATC exploits the robust source domain features, which are trained with ground truth labels, to learn discriminative target do-main features in an unsupervised manner by filtering unreliable predictions with temporal consensus. We demonstrate the effectiveness of the proposed approaches through extensive experiments. In particular, our model reaches mIoU of 53.81% on VIPER → Cityscapes-Seq benchmark and mIoU of 56.31% on SYNTHIA-Seq → Cityscapes-Seq benchmark, surpassing the state-of-the-art methods by large margins."
Domain Invariant Vision Transformer Learning for Face Anti-Spoofing,"Chen-Hao Liao, Wen-Cheng Chen, Hsuan-Tung Liu, Yi-Ren Yeh, Min-Chun Hu, Chu-Song Chen","National Kaohsiung Normal University; National Taiwan University; E.SUN Financial Holding Co., Ltd.; National Cheng Kung University; National Tsing Hua University",80,Taiwan,20,Taiwan,"Existing face anti-spoofing (FAS) models have achieved high performance on specific datasets. However, for the application of real-world systems, the FAS model should generalize to the data from unknown domains rather than only achieve good results on a single baseline. As vision transformer models have demonstrated astonishing performance and strong capability in learning discriminative information, we investigate applying transformers to distinguish the face presentation attacks over unknown domains. In this work, we propose the Domain-invariant Vision Transformer (DiVT) for FAS, which adopts two losses to improve the generalizability of the vision transformer. First, a concentration loss is employed to learn a domain-invariant representation that aggregates the features of real face data. Second, a separation loss is utilized to union each type of attack from different domains. The experimental results show that our proposed method achieves state-of-the-art performance on the protocols of domain-generalized FAS tasks. Compared to previous domain generalization FAS models, our proposed method is simpler but more effective.",https://openaccess.thecvf.com/content/WACV2023/html/Liao_Domain_Invariant_Vision_Transformer_Learning_for_Face_Anti-Spoofing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liao_Domain_Invariant_Vision_Transformer_Learning_for_Face_Anti-Spoofing_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030916/,"['Computer vision', 'Protocols', 'Computational modeling', 'Aggregates', 'Transformers', 'Feature extraction', 'Data models']","['Vision Transformer', 'Face Anti-spoofing', 'Transformer Model', 'Types Of Attacks', 'Domain Generalization', 'Real Faces', 'Unknown Domain', 'Training Set', 'Training Data', 'Learning Models', 'Convolutional Neural Network', 'Face Recognition', 'Object Classification', 'Latent Space', 'Video Frames', 'Face Images', 'Area Under Curve', 'Image Domain', 'Self-supervised Learning', 'Binary Cross-entropy Loss', 'Replay Attacks', 'Neural Architecture Search', 'Adversarial Training', 'Simple Loss', 'Training Domain', 'Different Types Of Groups', 'Mobile Camera', 'Feature Representation', 'Mobile Phone', 'Groups Face']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",27,"Existing face anti-spoofing (FAS) models have achieved high performance on specific datasets. However, for the application of real-world systems, the FAS model should generalize to the data from unknown domains rather than only achieve good results on a single baseline. As vision transformer models have demonstrated astonishing performance and strong capability in learning discriminative information, we investigate applying transformers to distinguish the face presentation attacks over unknown domains. In this work, we propose the Domain-invariant Vision Transformer (DiVT) for FAS, which adopts two losses to improve the generalizability of the vision transformer. First, a concentration loss is employed to learn a domain-invariant representation that aggregates the features of real face data. Second, a separation loss is utilized to union each type of attack from different domains. The experimental results show that our proposed method achieves state-of-the-art performance on the protocols of domain-generalized FAS tasks. Compared to previous domain generalization FAS models, our proposed method is simpler but more effective."
DyAnNet: A Scene Dynamicity Guided Self-Trained Video Anomaly Detection Network,"Kamalakar Vijay Thakare, Yash Raghuwanshi, Debi Prosad Dogra, Heeseung Choi, Ig-Jae Kim","Artificial Intelligence and Robotics Institute, Korea Institute of Science and Technology, Seoul 02792, Republic of Korea; Yonsei-KIST Convergence Research Institute, Yonsei University, Seoul 03722, Republic of Korea; Indian Institute of Technology, Bhubaneswar, Odisha, 752050, India",100,"India, South Korea",0,,"Unsupervised approaches for video anomaly detection may not perform as good as supervised approaches. However, learning unknown types of anomalies using an unsupervised approach is more practical than a supervised approach as annotation is an extra burden. In this paper, we use isolation tree-based unsupervised clustering to partition the deep feature space of the video segments. The RGB- stream generates a pseudo anomaly score and the flow stream generates a pseudo dynamicity score of a video segment. These scores are then fused using a majority voting scheme to generate preliminary bags of positive and negative segments. However, these bags may not be accurate as the scores are generated only using the current segment which does not represent the global behavior of a typical anomalous event. We then use a refinement strategy based on a cross-branch feed-forward network designed using a popular I3D network to refine both scores. The bags are then refined through a segment re-mapping strategy. The intuition of adding the dynamicity score of a segment with the anomaly score is to enhance the quality of the evidence. The method has been evaluated on three popular video anomaly datasets, i.e., UCF-Crime, CCTV-Fights, and UBI-Fights. Experimental results reveal that the proposed framework achieves competitive accuracy as compared to the state-of-the-art video anomaly detection methods.",https://openaccess.thecvf.com/content/WACV2023/html/Thakare_DyAnNet_A_Scene_Dynamicity_Guided_Self-Trained_Video_Anomaly_Detection_Network_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Thakare_DyAnNet_A_Scene_Dynamicity_Guided_Self-Trained_Video_Anomaly_Detection_Network_WACV_2023_paper.pdf,,,2211.00882,main,Poster,https://ieeexplore.ieee.org/document/10030236/,"['Computer vision', 'Annotations', 'Streaming media', 'Behavioral sciences', 'Anomaly detection']","['Anomaly Detection', 'Video Anomaly', 'Video Anomaly Detection', 'Deep Features', 'Video Segments', 'Anomalous Events', 'Segmentation Scores', 'Anomaly Score', 'Training Set', 'First Pass', 'Number Of Passes', 'High-level Features', 'Low-level Features', 'Handcrafted Features', 'False Alarm Rate', 'Incremental Learning', 'Optical Flow', 'Motion Features', 'Training Iterations', 'Data Instances', 'Pseudo Labels', 'Iterative Learning', 'Mean Square Error Loss', 'Thousands Of Samples', 'Weak Supervision', 'Backbone Architecture', 'Normal Segments', 'Hypersphere', 'Dynamic Datasets', 'Hidden Markov Model']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",14,"Unsupervised approaches for video anomaly detection may not perform as good as supervised approaches. However, learning unknown types of anomalies using an unsupervised approach is more practical than a supervised approach as annotation is an extra burden. In this paper, we use isolation tree-based unsupervised clustering to partition the deep feature space of the video segments. The RGB-stream generates a pseudo anomaly score and the flow stream generates a pseudo dynamicity score of a video segment. These scores are then fused using a majority voting scheme to generate preliminary bags of positive and negative segments. However, these bags may not be accurate as the scores are generated only using the current segment which does not represent the global behavior of a typical anomalous event. We then use a refinement strategy based on a cross-branch feed-forward network designed using a popular I3D network to refine both scores. The bags are then refined through a segment re-mapping strategy. The intuition of adding the dynamicity score of a segment with the anomaly score is to enhance the quality of the evidence. The method has been evaluated on three popular video anomaly datasets, i.e., UCF-Crime, CCTV-Fights, and UBI-Fights. Experimental results reveal that the proposed framework achieves competitive accuracy as compared to the state-of-the-art video anomaly detection methods."
DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editings,"Bingchuan Li, Shaofei Cai, Wei Liu, Peng Zhang, Qian He, Miao Hua, Zili Yi","ByteDance Ltd, Beijing, China",0,,100,China,"The semantic controllability of StyleGAN is enhanced by unremitting research. Although the existing weak supervision methods work well in manipulating the style codes along one attribute, the accuracy of manipulating multiple attributes is neglected. Multi-attribute representations are prone to entanglement in the StyleGAN latent space, while sequential editing leads to error accumulation. To address these limitations, we design a Dynamic Style Manipulation Network (DyStyle) whose structure and parameters vary by input samples, to perform nonlinear and adaptive manipulation of latent codes for flexible and precise attribute control. In order to efficient and stable optimization of the DyStyle network, we propose a Dynamic Multi-Attribute Contrastive Learning (DmaCL) method: including dynamic multi-attribute contrastor and dynamic multi-attribute contrastive loss, which simultaneously disentangle a variety of attributes from the generative image and latent space of model. As a result, our approach demonstrates fine-grained disentangled edits along multiple numeric and binary attributes. Qualitative and quantitative comparisons with existing style manipulation methods verify the superiority of our method in terms of the multi-attribute control accuracy and identity preservation without compromising photorealism.",https://openaccess.thecvf.com/content/WACV2023/html/Li_DyStyle_Dynamic_Neural_Network_for_Multi-Attribute-Conditioned_Style_Editings_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_DyStyle_Dynamic_Neural_Network_for_Multi-Attribute-Conditioned_Style_Editings_WACV_2023_paper.pdf,,,2109.10737,main,Poster,https://ieeexplore.ieee.org/document/10030444/,"['Learning systems', 'Photorealism', 'Codes', 'Adaptive systems', 'Neural networks', 'Semantics', 'Aerospace electronics']","['Dynamic Network', 'Semantic', 'Variety Of Properties', 'Multiple Dimensions', 'Latent Space', 'Accurate Control', 'Image Space', 'Numerous Properties', 'Self-supervised Learning', 'Contrastive Loss', 'Use Of Dynamics', 'Latent Code', 'Use Of Styles', 'Identity Preservation', 'Random Noise', 'Use Of Imaging', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Image Generation', 'Dynamic Architecture', 'Fréchet Inception Distance', 'Conditional Generative Adversarial Network', 'Real Faces', 'Black Hair', 'Loss Of Identity', 'Animal Faces', 'Generative Adversarial Network Architecture', 'Hair Color', 'Image Properties']","['Algorithms: Computational photography', 'image and video synthesis', 'Adversarial learning', 'adversarial attack and defense methods', 'Visualization']",4,"The semantic controllability of StyleGAN is enhanced by unremitting research. Although the existing weak supervision methods work well in manipulating the style codes along one attribute, the accuracy of manipulating multiple attributes is neglected. Multi-attribute representations are prone to entanglement in the StyleGAN latent space, while sequential editing leads to error accumulation. To address these limitations, we design a Dynamic Style Manipulation Network (DyStyle) whose structure and parameters vary by input samples, to perform nonlinear and adaptive manipulation of latent codes for flexible and precise attribute control. In order to efficient and stable optimization of the DyStyle network, we propose a Dynamic Multi-Attribute Contrastive Learning (DmaCL) method: including dynamic multi-attribute contrastor and dynamic multi-attribute contrastive loss, which simultaneously disentangle a variety of attributes from the generative image and latent space of model. As a result, our approach demonstrates fine-grained disentangled edits along multiple numeric and binary attributes. Qualitative and quantitative comparisons with existing style manipulation methods verify the superiority of our method in terms of the multi-attribute control accuracy and identity preservation without compromising photorealism."
Dynamic Mixture of Counter Network for Location-Agnostic Crowd Counting,"Mingjie Wang, Hao Cai, Yong Dai, Minglun Gong","School of Science, Zhejiang Sci-Tech University; School of Computer Science, University of Guelph; Tencent AI Lab; School of Computer Science, Memorial University of Newfoundland",75,"Canada, China",25,China,"Crowd counting has attracted increasing attentions in recent years due to its challenges and wide societal applications. Despite persevering efforts made by the research community, most of existing methods require a large amount of location-level annotations. Collecting such type of fine-granularity supervisory signals is extremely time-consuming and labour-intensive, thereby hindering the well generalization of these location-adherent models. To shun this drawback, several pioneering studies open a promising research direction of location-agonistic crowd counting. Albeit the noticeable efforts, they somewhat ignore the merits of diverse learning paradigms and the issue of intractable density shift. To ameliorate these issues, in this paper, a novel Dynamic Mixture of Counter Network (DMCNet) is proposed for location-agnostic crowd counting. Specifically, our DMCNet inherits the hybrid advantages of CNNs (e.g. locality-oriented and pyramidal property) and MLP-based structure (e.g. global receptive fields and light weight). Particularly, the dynamic counter predictor and the mixture of counter heads are delicately designed to hammer at combating huge density shift and overfitting. Extensive experiments demonstrate that our DMCNet attains state-of-the-art performance against existing location-agnostic approaches and performs on par with many conventional location-adherent ones.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_Dynamic_Mixture_of_Counter_Network_for_Location-Agnostic_Crowd_Counting_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Dynamic_Mixture_of_Counter_Network_for_Location-Agnostic_Crowd_Counting_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030423/,"['Computer vision', 'Protocols', 'Annotations', 'Focusing', 'Benchmark testing', 'Transformers', 'Mixers']","['Crowd Counting', 'Convolutional Neural Network', 'Research Community', 'Receptive Field', 'Global Field', 'Supervisory Signal', 'Ambiguity', 'Computer Vision', 'Batch Size', 'Training Phase', 'Backpropagation', 'Density Map', 'Large-scale Datasets', 'Max-pooling Layer', 'Loss Term', 'Single Counting', 'Feature Pyramid', 'Gradient Flow', 'Dynamic Weight', 'Crowded Scenes', 'Number Of Heads', 'Counting Problem', 'ReLU Function', 'Principled Way', 'ImageNet']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",14,"Crowd counting has attracted increasing attentions in recent years due to its challenges and wide societal applications. Despite persevering efforts made by the research community, most of existing methods require a large amount of location-level annotations. Collecting such type of fine-granularity supervisory signals is extremely time-consuming and labour-intensive, thereby hindering the well generalization of these location-adherent models. To shun this drawback, several pioneering studies open a promising research direction of location-agonistic crowd counting. Albeit the noticeable efforts, they somewhat ignore the merits of diverse learning paradigms and the issue of intractable density shift. To ameliorate these issues, in this paper, a novel Dynamic Mixture of Counter Network (DMCNet) is proposed for location-agnostic crowd counting. Specifically, our DMCNet inherits the hybrid advantages of CNNs (e.g. locality-oriented and pyramidal property) and MLP-based structure (e.g. global receptive fields and light weight). Particularly, the dynamic counter predictor and the mixture of counter heads are delicately designed to hammer at combating huge density shift and overfitting. Extensive experiments demonstrate that our DMCNet attains state-of-the-art performance against existing location-agnostic approaches and performs on par with many conventional location-adherent ones."
Dynamic Neural Portraits,"Michail Christos Doukas, Stylianos Ploumpis, Stefanos Zafeiriou","Huawei Technologies, London, UK; Imperial College London, UK; Huawei Technologies, London, UK",33.33333333,UK,66.66666667,China,"We present Dynamic Neural Portraits, a novel approach to the problem of full-head reenactment. Our method generates photo-realistic video portraits by explicitly controlling head pose, facial expressions and eye gaze. Our proposed architecture is different from existing methods that rely on GAN-based image-to-image translation networks for transforming renderings of 3D faces into photo-realistic images. Instead, we build our system upon a 2D coordinate-based MLP with controllable dynamics. Our intuition to adopt a 2D-based representation, as opposed to recent 3D NeRF-like systems, stems from the fact that video portraits are captured by monocular stationary cameras, therefore, only a single viewpoint of the scene is available. Primarily, we condition our generative model on expression blendshapes, nonetheless, we show that our system can be successfully driven by audio features as well. Our experiments demonstrate that the proposed method is 270 times faster than recent NeRF-based reenactment methods, with our networks achieving speeds of 24 fps for resolutions up to 1024x1024, while outperforming prior works in terms of visual quality.",https://openaccess.thecvf.com/content/WACV2023/html/Doukas_Dynamic_Neural_Portraits_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Doukas_Dynamic_Neural_Portraits_WACV_2023_paper.pdf,,,2211.13994,main,Poster,https://ieeexplore.ieee.org/document/10030555/,"['Visualization', 'Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Computer architecture', 'Rendering (computer graphics)', 'Cameras']","['Facial Expressions', 'Multilayer Perceptron', 'Eye Contact', 'Head Pose', '3D Face', 'Neural Network', 'Convolutional Network', 'Visual Features', 'Upper Body', 'Generative Adversarial Networks', 'Neural Representations', 'Acoustic Signals', 'Human Faces', 'Sequence Of Frames', 'Perceptual Similarity', 'Video Capture', 'Pixel Coordinates', 'Training Videos', 'Decoder Network', 'Reconstruction Task', 'Pose Parameters', 'Multilayer Perceptron Network', 'Fréchet Inception Distance', 'Explicit Control', 'Camera Viewpoint', 'Supplementary Video']","['Algorithms: Computational photography', 'image and video synthesis', 'Biometrics', 'face', 'gesture', 'body pose']",,"We present Dynamic Neural Portraits, a novel approach to the problem of full-head reenactment. Our method generates photo-realistic video portraits by explicitly controlling head pose, facial expressions and eye gaze. Our proposed architecture is different from existing methods that rely on GAN-based image-to-image translation networks for transforming renderings of 3D faces into photo-realistic images. Instead, we build our system upon a 2D coordinate-based MLP with controllable dynamics. Our intuition to adopt a 2D-based representation, as opposed to recent 3D NeRF-like systems, stems from the fact that video portraits are captured by monocular stationary cameras, therefore, only a single viewpoint of the scene is available. Primarily, we condition our generative model on expression blendshapes, nonetheless, we show that our system can be successfully driven by audio features as well. Our experiments demonstrate that the proposed method is 270 times faster than recent NeRF-based reenactment methods, with our networks achieving speeds of 24 fps for resolutions up to 1024×1024, while outperforming prior works in terms of visual quality."
Dynamic Re-Weighting for Long-Tailed Semi-Supervised Learning,"Hanyu Peng, Weiguo Pian, Mingming Sun, Ping Li","Cognitive Computing Lab, Baidu Research, Beijing, China; 10900 NE 8th St. Bellevue, Washington, USA; Cognitive Computing Lab, Baidu Research, Beijing, China; PhD student at the University of Luxembourg",50,"Luxembourg, USA",50,China,"The high demand for labeled data that characterizes deep learning is very labor-intensive. Semi-supervised Learning (SSL), acting as one of the breakthroughs, allows for the avoidance of this labeling loss thanks to its small amount of labeled data, alongside extracting information from a large amount of unlabeled data. And there is hope that the same performance for SSL can be achieved when compared to supervised learning methods. Regrettably, the research community has often developed SSL regarding the nature of a balanced data set; in contrast, real data is often imbalanced or even long-tailed. The need to study SSL under imbalance is therefore critical. In this paper, we shall essentially extend FixMatch (a SSL method) to the imbalanced case. We find that the unlabeled data is as well highly imbalanced during the training process; in this respect we propose a re-weighting solution based on the effective number. Furthermore, since prediction uncertainty leads to temporal variations in the number of pseudo-labels, we are innovative in proposing a dynamic re-weighting scheme on the unlabeled data. The simplicity and validity of our method are backed up by strong experimental evidence. Especially on CIFAR-10, CIFAR-100, ImageNet127 data sets, our approach provides the strongest results against previous methods across various scales of imbalance.",https://openaccess.thecvf.com/content/WACV2023/html/Peng_Dynamic_Re-Weighting_for_Long-Tailed_Semi-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Peng_Dynamic_Re-Weighting_for_Long-Tailed_Semi-Supervised_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030252/,"['Training', 'Computer vision', 'Uncertainty', 'Annotations', 'Semisupervised learning', 'Task analysis']","['Semi-supervised Learning', 'Dynamic Reweighting', 'Unlabeled Data', 'Loss Function', 'Training Set', 'Validation Set', 'Supervised Learning', 'Data Augmentation', 'Loss Value', 'Training Epochs', 'Class Imbalance', 'Labeled Data', 'Training Step', 'Domain Adaptation', 'Focal Loss', 'Minority Class', 'Balanced Ratio', 'Imbalance Ratio', 'Long-tailed Distribution', 'CIFAR-100 Dataset', 'Semi-supervised Learning Approach']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"Semi-supervised Learning (SSL) reduces significant human annotations by simply demanding a small number of labelled samples and a large number of unlabelled samples. The research community has often developed SSL regarding the nature of a balanced data set; in contrast, real data is often imbalanced or even long-tailed. The need to study SSL under imbalance is therefore critical. In this paper, we essentially extend FixMatch (a SSL method) to the imbalanced case. We find that the unlabeled data is as well highly imbalanced during the training process; in this respect we propose a re-weighting solution based on the effective number. Furthermore, since prediction uncertainty leads to temporal variations in the number of pseudo-labels, we are innovative in proposing a dynamic reweighting scheme on the unlabeled data. The simplicity and validity of our method are backed up by experimental evidence. Especially on CIFAR-10, CIFAR-100, ImageNet127 data sets, our approach provides the strongest results against previous methods across various scales of imbalance."
ETR: An Efficient Transformer for Re-Ranking in Visual Place Recognition,"Hao Zhang, Xin Chen, Heming Jing, Yingbin Zheng, Yuan Wu, Cheng Jin","Videt Technology, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",50,China,50,China,"Visual place recognition is to estimate the geographical location of a given image, which is usually addressed by recognizing its similar reference images from a database. The reference images are usually retrieved via similarity search using global descriptor, and the local descriptors are used to re-rank the initial retrieved candidates. The local descriptors re-ranking can significantly improve the accuracy of global retrieval but comes at a high computational cost. To achieve a good trade-off between accuracy and efficiency, we propose an Efficient Transformer for Re-ranking (ETR), utilizing both global and local descriptors to re-rank the top candidates in a single shot. In contrast to traditional re-ranking methods, we leverage self-attention to capture relationships between local descriptors in a single image and cross-attention to explore the similarity of the image pairs. We show that the proposed model can be regarded as a general re-ranking algorithm for significantly boosting the performance of other global-only retrieval methods. Extensive experimental results show that our method outperforms state-of-the-arts and is orders of magnitude faster in terms of computational efficiency.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_ETR_An_Efficient_Transformer_for_Re-Ranking_in_Visual_Place_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_ETR_An_Efficient_Transformer_for_Re-Ranking_in_Visual_Place_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030194/,"['Visualization', 'Image recognition', 'Navigation', 'Computational modeling', 'Memory management', 'Transformers', 'Real-time systems']","['Visual Recognition', 'Place Recognition', 'Visual Place Recognition', 'Image Pairs', 'Reference Image', 'Single Shot', 'Local Descriptors', 'Global Descriptors', 'Performance Of Other Methods', 'Global Features', 'Average Pooling', 'Image Representation', 'Image Scale', 'Feature Matching', 'Image Retrieval', 'Memory Consumption', 'CNN-based Methods', 'Illumination Changes', 'Linear Projection', 'Local Feature Extraction', 'Self-attention Layer', 'Query Image', 'Mutual Nearest Neighbors', 'Gallery Images', 'Multi-head Self-attention', 'Vision Transformer', 'Mobile Camera', 'Viewpoint Changes', 'Transformer Architecture', 'Local Features']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",13,"Visual place recognition is to estimate the geographical location of a given image, which is usually addressed by recognizing its similar reference images from a database. The reference images are usually retrieved via similarity search using global descriptor, and the local descriptors are used to re-rank the initial retrieved candidates. The local descriptors re-ranking can significantly improve the accuracy of global retrieval but comes at a high computational cost. To achieve a good trade-off between accuracy and efficiency, we propose an Efficient Transformer for Re-ranking (ETR), utilizing both global and local descriptors to re-rank the top candidates in a single shot. In contrast to traditional re-ranking methods, we leverage self-attention to capture relationships between local descriptors in a single image and cross-attention to explore the similarity of the image pairs. We show that the proposed model can be regarded as a general re-ranking algorithm for significantly boosting the performance of other global-only retrieval methods. Extensive experimental results show that our method outperforms state-of-the-arts and is orders of magnitude faster in terms of computational efficiency."
Effective Invertible Arbitrary Image Rescaling,"Zhihong Pan, Baopu Li, Dongliang He, Wenhao Wu, Errui Ding",Oracle Health and AI; Baidu VIS; Baidu Research (USA),0,,100,USA,"Great successes have been achieved using deep learning techniques for image super-resolution (SR) with fixed scales. To increase its real world applicability, numerous models have also been proposed to restore SR images with arbitrary scale factors, including asymmetric ones where images are resized to different scales along horizontal and vertical directions. Though most models are only optimized for the unidirectional upscaling task while assuming a predefined downscaling kernel for low-resolution (LR) inputs, recent models based on Invertible Neural Networks (INN) are able to increase upscaling accuracy significantly by optimizing the downscaling and upscaling cycle jointly. However, limited by the INN architecture, it is constrained to fixed integer scale factors and requires one model for each scale. Without increasing model complexity, a simple and effective invertible arbitrary rescaling network (IARN) is proposed to achieve arbitrary image rescaling by training only one model in this work. Using innovative components like position-aware scale encoding and preemptive channel splitting, the network is optimized to convert the non-invertible rescaling cycle to an effectively invertible process. It is shown to achieve a state-of-the-art (SOTA) performance in bidirectional arbitrary rescaling without compromising perceptual quality in LR outputs. It is also demonstrated to perform well on tests with asymmetric scales using the same network architecture.",https://openaccess.thecvf.com/content/WACV2023/html/Pan_Effective_Invertible_Arbitrary_Image_Rescaling_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pan_Effective_Invertible_Arbitrary_Image_Rescaling_WACV_2023_paper.pdf,,,2209.13055,main,Poster,https://ieeexplore.ieee.org/document/10030086/,"['Deep learning', 'Training', 'Image coding', 'Superresolution', 'Neural networks', 'Network architecture', 'Complexity theory']","['Image Resampling', 'Arbitrary Rescaling', 'Deep Learning', 'Scaling Factor', 'Vertical Direction', 'Real Applications', 'Real-world Applications', 'Super-resolution', 'Arbitrary Scale', 'Low-resolution Input', 'Convolution', 'Image Quality', 'Latent Variables', 'Input Image', 'Image Size', 'Receptive Field', 'Range Of Scales', 'Forward Direction', 'Transformation Function', 'Bidirectional Method', 'Lower Branch', 'Forward Process', 'Bicubic Interpolation', 'Inversion Process', 'Super-resolution Model', 'L2 Loss', 'Low-resolution Images', 'Inference Time', 'Blocking Layer']","['Algorithms: Low-level and physics-based vision', 'Computational photography', 'image and video synthesis']",6,"Great successes have been achieved using deep learning techniques for image super-resolution (SR) with fixed scales. To increase its real world applicability, numerous models have also been proposed to restore SR images with arbitrary scale factors, including asymmetric ones where images are resized to different scales along horizontal and vertical directions. Though most models are only optimized for the unidirectional upscaling task while assuming a predefined downscaling kernel for low-resolution (LR) inputs, recent models based on Invertible Neural Networks (INN) are able to increase upscaling accuracy significantly by optimizing the downscaling and upscaling cycle jointly. However, limited by the INN architecture, it is constrained to fixed integer scale factors and requires one model for each scale. Without increasing model complexity, a simple and effective invertible arbitrary rescaling network (IARN) is proposed to achieve arbitrary image rescaling by training only one model in this work. Using innovative components like position-aware scale encoding and preemptive channel splitting, the network is optimized to convert the non-invertible rescaling cycle to an effectively invertible process. It is shown to achieve a state-of-the-art (SOTA) performance in bidirectional arbitrary rescaling without compromising perceptual quality in LR outputs. It is also demonstrated to perform well on tests with asymmetric scales using the same network architecture."
Efficient Few-Shot Learning for Pixel-Precise Handwritten Document Layout Analysis,"Axel De Nardin, Silvia Zottin, Matteo Paier, Gian Luca Foresti, Emanuela Colombi, Claudio Piciarelli",University of Udine,100,Italy,0,,"Layout analysis is a task of uttermost importance in ancient handwritten document analysis and represents a fundamental step toward the simplification of subsequent tasks such as optical character recognition and automatic transcription. However, many of the approaches adopted to solve this problem rely on a fully supervised learning paradigm. While these systems achieve very good performance on this task, the drawback is that pixel-precise text labeling of the entire training set is a very time-consuming process, which makes this type of information rarely available in a real-world scenario. In the present paper, we address this problem by proposing an efficient few-shot learning framework that achieves performances comparable to current state-of-the-art fully supervised methods on the publicly available DIVA-HisDB dataset",https://openaccess.thecvf.com/content/WACV2023/html/De_Nardin_Efficient_Few-Shot_Learning_for_Pixel-Precise_Handwritten_Document_Layout_Analysis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/De_Nardin_Efficient_Few-Shot_Learning_for_Pixel-Precise_Handwritten_Document_Layout_Analysis_WACV_2023_paper.pdf,,,2210.1557,main,Poster,https://ieeexplore.ieee.org/document/10030489/,"['Training', 'Measurement', 'Text analysis', 'Semantic segmentation', 'Layout', 'Supervised learning', 'Optical character recognition']","['Few-shot Learning', 'Layout Analysis', 'Document Layout Analysis', 'Training Set', 'Time-consuming Process', 'Real-world Scenarios', 'Optical Character Recognition', 'Training Data', 'Support Vector Machine', 'Contextual Information', 'Main Text', 'Intersection Over Union', 'Feed-forward Network', 'Semantic Segmentation', 'Generalization Capability', 'Image Patches', 'Segmentation Task', 'Backbone Network', 'Segmentation Map', 'Frequent Class', 'Atrous Spatial Pyramid Pooling', 'Refinement Process', 'Structural Layout', 'Label Of Pixel', 'Convolutional Autoencoder', 'Ground Truth Segmentation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'scene modeling', 'visual reasoning', 'Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'low-shot', 'semi-', 'self-', 'and un-supervised learning']",4,"Layout analysis is a task of uttermost importance in ancient handwritten document analysis and represents a fundamental step toward the simplification of subsequent tasks such as optical character recognition and automatic transcription. However, many of the approaches adopted to solve this problem rely on a fully supervised learning paradigm. While these systems achieve very good performance on this task, the drawback is that pixel-precise text labeling of the entire training set is a very time-consuming process, which makes this type of information rarely available in a real-world scenario. In the present paper, we address this problem by proposing an efficient few-shot learning framework that achieves performances comparable to current state-of-the-art fully supervised methods on the publicly available DIVA-HisDB dataset."
Efficient Flow-Guided Multi-Frame De-Fencing,"Stavros Tsogkas, Fengjia Zhang, Allan Jepson, Alex Levinshtein",Samsung AI Center Toronto,0,,100,South Korea,"Taking photographs ""in-the-wild"" is often hindered by fence obstructions that stand between the camera user and the scene of interest, and which are hard or impossible to avoid. De-fencing is the algorithmic process of automatically removing such obstructions from images, revealing the invisible parts of the scene. While this problem can be formulated as a combination of fence segmentation and image inpainting, this often leads to implausible hallucinations of the occluded regions. Existing multi-frame approaches rely on propagating information to a selected keyframe from its temporal neighbors, but they are often inefficient and struggle with alignment of severely obstructed images. In this work we draw inspiration from the video completion literature and develop a simplified framework for multi-frame de-fencing that computes high quality flow maps directly from obstructed frames and uses them to accurately align frames. Our primary focus is efficiency and practicality in a real-world setting: the input to our algorithm is a short image burst (5 frames) -- a data modality commonly available in modern smartphones-- and the output is a single reconstructed keyframe, with the fence removed. Our approach leverages simple yet effective CNN modules, trained on carefully generated synthetic data, and outperforms more complicated alternatives real bursts, both quantitatively and qualitatively, while running real-time.",https://openaccess.thecvf.com/content/WACV2023/html/Tsogkas_Efficient_Flow-Guided_Multi-Frame_De-Fencing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tsogkas_Efficient_Flow-Guided_Multi-Frame_De-Fencing_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030531/,"['Training', 'Image segmentation', 'Computer vision', 'Runtime', 'Pipelines', 'Streaming media', 'Cameras']","['Hallucinations', 'Flow Map', 'Part Of The Scene', 'Image Inpainting', 'Supplemental Material', 'Single Image', 'Adam Optimizer', 'Data Augmentation', 'Single Frame', 'Optical Flow', 'Composite Layer', 'Video Sequences', 'Network Flow', 'Reconstruction Quality', 'Flow Estimation', 'L1 Loss', 'Fully Convolutional Network', 'Clear Background', 'Online Optimization', 'Optical Flow Estimation', 'Background Scene', 'Frame Alignment', 'Missing Areas', 'Pair Of Frames', 'Separate Layers', 'Segmentation Model']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Smartphones/end user devices']",,"Taking photographs ""in-the-wild"" is often hindered by fence obstructions that stand between the camera user and the scene of interest, and which are hard or impossible to avoid. De-fencing is the algorithmic process of automatically removing such obstructions from images, revealing the invisible parts of the scene. While this problem can be formulated as a combination of fence segmentation and image inpainting, this often leads to implausible hallucinations of the occluded regions. Existing multi-frame approaches rely on propagating information to a selected keyframe from its temporal neighbors, but they are often inefficient and struggle with alignment of severely obstructed images. In this work we draw inspiration from the video completion literature, and develop a simplified framework for multi-frame de-fencing that computes high quality flow maps directly from obstructed frames, and uses them to accurately align frames. Our primary focus is efficiency and practicality in a real world setting: the input to our algorithm is a short image burst (5 frames) – a data modality commonly available in modern smartphones– and the output is a single reconstructed keyframe, with the fence removed. Our approach leverages simple yet effective CNN modules, trained on carefully generated synthetic data, and outperforms more complicated alternatives real bursts, both quantitatively and qualitatively, while running real-time."
Efficient Reference-Based Video Super-Resolution (ERVSR): Single Reference Image Is All You Need,"Youngrae Kim, Jinsu Lim, Hoonhee Cho, Minji Lee, Dongman Lee, Kuk-Jin Yoon, Ho-Jin Choi","Mechanical Engineering, KAIST; School of Computing, KAIST",100,South Korea,0,,"Reference-based video super-resolution (RefVSR) is a promising domain of super-resolution that recovers high-frequency textures of a video using reference video. The multiple cameras with different focal lengths in mobile devices aid recent works in RefVSR, which aim to super-resolve a low-resolution ultra-wide video by utilizing wide-angle videos. Previous works in RefVSR used all reference frames of a Ref video at each time step for the super-resolution of low-resolution videos. However, computation on higher-resolution images increases the runtime and memory consumption, hence hinders the practical application of RefVSR. To solve this problem, we propose an Efficient Reference-based Video Super-Resolution (ERVSR) that exploits a single reference frame to super-resolve whole low-resolution video frames. We introduce an attention-based feature align module and an aggregation upsampling module that attends LR features using the correlation between the reference and LR frames. The proposed ERVSR achieves 12xfaster speed, 1/4 memory consumption than previous state-of-the-art RefVSR networks, and competitive performance on the RealMCVSR dataset while using a single reference image.",https://openaccess.thecvf.com/content/WACV2023/html/Kim_Efficient_Reference-Based_Video_Super-Resolution_ERVSR_Single_Reference_Image_Is_All_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_Efficient_Reference-Based_Video_Super-Resolution_ERVSR_Single_Reference_Image_Is_All_WACV_2023_paper.pdf,,https://github.com/haewonc/ERVSR,,main,Poster,https://ieeexplore.ieee.org/document/10030311/,"['Computer vision', 'Runtime', 'Correlation', 'Computational modeling', 'Superresolution', 'Memory management', 'Graphics processing units']","['Single Image', 'Reference Image', 'Single Reference', 'Video Super-resolution', 'Single Reference Image', 'Time Step', 'Reference Frame', 'Focal Length', 'Video Frames', 'Competitive Performance', 'Single Frame', 'Memory Consumption', 'Low-resolution Feature', 'Alignment Module', 'Training Set', 'Convolutional Layers', 'Attention Mechanism', 'Large Time', 'Performance Gap', 'High-resolution Video', 'Feature Alignment', 'Low-resolution Images', 'Inference Time', 'Center Of Frame', 'Large Differences In Time', 'Residual Block', 'Single Image Super-resolution', 'Optical Flow', 'Attention Map']","['Algorithms: Low-level and physics-based vision', 'Computational photography', 'image and video synthesis']",4,"Reference-based video super-resolution (RefVSR) is a promising domain of super-resolution that recovers high-frequency textures of a video using reference video. The multiple cameras with different focal lengths in mobile devices aid recent works in RefVSR, which aim to super-resolve a low-resolution ultra-wide video by utilizing wide-angle videos. Previous works in RefVSR used all reference frames of a Ref video at each time step for the super-resolution of low-resolution videos. However, computation on higher-resolution images increases the runtime and memory consumption, hence hinders the practical application of RefVSR. To solve this problem, we propose an Efficient Reference-based Video Super-Resolution (ERVSR) that exploits a single reference frame to super-resolve whole low-resolution video frames. We introduce an attention-based feature align module and an aggregation upsampling module that attends LR features using the correlation between the reference and LR frames. The proposed ERVSR achieves 12× faster speed, 1/4 memory consumption than previous state-of-the-art RefVSR networks, and competitive performance on the RealMCVSR dataset while using a single reference image."
Efficient Skeleton-Based Action Recognition via Joint-Mapping Strategies,"Min-Seok Kang, Dongoh Kang, HanSaem Kim","Kakao Enterprise, Pangyo, Gyeonggi-do, South Korea",0,,100,South Korea,"Graph convolutional networks (GCNs) have brought remarkable progress in skeleton-based action recognition. However, high computational cost and large model size make models difficult to be applied in real-world embedded system. Specifically, GCN that is applied in automated surveillance system pre-require models such as pedestrian detection and human pose estimation. Therefore, each model should be computationally lightweight and whole process should be operated in real-time. In this paper, we propose two different joint-mapping modules to reduce the number of joint representations, alleviating a total computational cost and model size. Our models achieve better accuracy-latency trade-off compared to previous state-of-the-arts on two datasets, namely NTU RGB+D and NTU RGB+D 120, demonstrating the suitability for practical applications. Furthermore, we measure the latency of the models by using TensorRT framework to compare the models from a practical perspective.",https://openaccess.thecvf.com/content/WACV2023/html/Kang_Efficient_Skeleton-Based_Action_Recognition_via_Joint-Mapping_Strategies_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kang_Efficient_Skeleton-Based_Action_Recognition_via_Joint-Mapping_Strategies_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030373/,"['Visualization', 'Computer vision', 'Embedded systems', 'Computational modeling', 'Surveillance', 'Pose estimation', 'Real-time systems']","['Action Recognition', 'Skeleton-based Action Recognition', 'Efficient Action Recognition', 'Computational Cost', 'Surveillance System', 'Graph Convolutional Network', 'Real-world Systems', 'Graph Convolution', 'Human Pose Estimation', 'Pedestrian Detection', 'Joint Representation', 'Training Data', 'Real-world Applications', 'Middle Layer', 'Competitive Performance', 'Semantic Features', 'Pie Chart', 'Action Classes', 'Graph Topology', 'Number Of Joints', 'Mapping Module', 'Global Graph', 'Manual Mapping', 'Temporal Convolution', 'Neural Architecture Search', 'Middle Node', 'Individual Graphs', 'Skeleton Data', 'Variety Of Scenes', 'Detailed Parameter Settings']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Robotics']",10,"Graph convolutional networks (GCNs) have brought remarkable progress in skeleton-based action recognition. However, high computational cost and large model size make models difficult to be applied in real-world embedded system. Specifically, GCN that is applied in automated surveillance system pre-require models such as pedestrian detection and human pose estimation. Therefore, each model should be computationally lightweight and whole process should be operated in real-time. In this paper, we propose two different joint-mapping modules to reduce the number of joint representations, alleviating a total computational cost and model size. Our models achieve better accuracy-latency trade-off compared to previous state-ofthe-arts on two datasets, namely NTU RGB+D and NTU RGB+D 120, demonstrating the suitability for practical applications. Furthermore, we measure the latency of the models by using TensorRT framework to compare the models from a practical perspective."
Efficient Visual Tracking With Exemplar Transformers,"Philippe Blatter, Menelaos Kanakis, Martin Danelljan, Luc Van Gool","ETH Zürich; ETH Zürich, KU Leuven",100,"Belgium, Switzerland",0,,"The design of more complex and powerful neural network models has significantly advanced the state-of-the-art in visual object tracking. These advances can be attributed to deeper networks, or the introduction of new building blocks, such as transformers. However, in the pursuit of increased tracking performance, runtime is often hindered. Furthermore, efficient tracking architectures have received surprisingly little attention. In this paper, we introduce the Exemplar Transformer, a transformer module utilizing a single instance level attention layer for realtime visual object tracking. E.T.Track, our visual tracker that incorporates Exemplar Transformer modules, runs at 47 FPS on a CPU. This is up to 8x faster than other transformer-based models. When compared to lightweight trackers that can operate in realtime on standard CPUs, E.T.Track consistently outperforms all other methods on the LaSOT, OTB-100, NFS, TrackingNet, and VOT-ST2020 datasets. Code and models are available at https://github.com/pblatter/ettrack.",https://openaccess.thecvf.com/content/WACV2023/html/Blatter_Efficient_Visual_Tracking_With_Exemplar_Transformers_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Blatter_Efficient_Visual_Tracking_With_Exemplar_Transformers_WACV_2023_paper.pdf,,https://github.com/pblatter/ettrack,2112.09686,main,Poster,https://ieeexplore.ieee.org/document/10030302/,"['Convolutional codes', 'Performance evaluation', 'Visualization', 'Runtime', 'Computational modeling', 'Computer architecture', 'Transformers']","['Tracking Efficiency', 'New Building', 'Object Tracking', 'Attention Layer', 'Frames Per Second', 'Transformation Module', 'Convolutional Layers', 'Feature Maps', 'Object Detection', 'Bounding Box', 'Feed-forward Network', 'Video Analysis', 'Single Object', 'Performance Gain', 'Attention Module', 'Multi-task Learning', 'Single Tracking', 'Machine Translation', 'Standard Transformation', 'Transformer Architecture', 'Transformer Layers', 'Neural Architecture Search', 'Drop-in Replacement', 'Ground-truth Bounding Box', 'Encoder Architecture', 'Depthwise Separable Convolution', 'Convolution Operation', 'Search Region', 'Convolution Module', 'Design Choices']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",59,"The design of more complex and powerful neural network models has significantly advanced the state-of-the-art in visual object tracking. These advances can be attributed to deeper networks, or the introduction of new building blocks, such as transformers. However, in the pursuit of increased tracking performance, runtime is often hindered. Furthermore, efficient tracking architectures have received surprisingly little attention. In this paper, we introduce the Exemplar Transformer, a transformer module utilizing a single instance level attention layer for realtime visual object tracking. E.T.Track, our visual tracker that incorporates Exemplar Transformer modules, runs at 47 FPS on a CPU. This is up to 8× faster than other transformer-based models. When compared to lightweight trackers that can operate in realtime on standard CPUs, E.T.Track consistently outperforms all other methods on the LaSOT [16], OTB-100 [52], NFS [27], TrackingNet [36], and VOTST2020 [29] datasets. Code and models are available at https://github.com/pblatter/ettrack."
"EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Cardiac Measurement","Xin Liu, Brian Hill, Ziheng Jiang, Shwetak Patel, Daniel McDuff","Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Microsoft Research, Redmond, WA, USA; Department of Computer Science, University of California, Los Angeles, CA, USA",66.66666667,USA,33.33333333,USA,"Camera-based physiological measurement is a growing field with neural models providing state-of-the-art performance. Prior research has explored various end-to-end architectures; however these methods still require several preprocessing steps and are not able to run directly on mobile and edge devices. The operations are often non-trivial to implement, making replication and deployment difficult and can even have a higher computational budget than the core network itself. In this paper, we propose two novel and efficient neural models for camera-based physiological measurement called EfficientPhys that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. Using an input of raw video frames, our models achieve strong accuracy on three public datasets. We show that this is the case whether using a transformer or convolutional backbone. We further evaluate the latency of the proposed networks and show that our most lightweight network also achieves a 33% improvement in efficiency.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_EfficientPhys_Enabling_Simple_Fast_and_Accurate_Camera-Based_Cardiac_Measurement_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_EfficientPhys_Enabling_Simple_Fast_and_Accurate_Camera-Based_Cardiac_Measurement_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030453/,"['Performance evaluation', 'Computational modeling', 'Image edge detection', 'Machine learning', 'Computer architecture', 'Transformers', 'Extraterrestrial measurements']","['Cardiac Measurements', 'Mobile Devices', 'Video Frames', 'Color Space', 'Edge Devices', 'Raw Video', 'Raw Frames', 'Convolutional Network', 'Deep Neural Network', 'Subtle Changes', 'Simple Design', 'Video Data', 'Consecutive Frames', 'Batch Normalization Layer', 'Pulse Signal', 'Signal Processing Methods', 'Vital Measurements', 'Transformer Architecture', 'Pixel Change', 'Motor Noise', 'Vision Transformer', 'Elegant Design', 'Pre-processing Module', 'Spatial-temporal Model', 'Temporal Axis', 'Average Pixel Value', 'Self-attention Module', 'Convolutional Neural Network', 'Video Understanding', 'Generative Adversarial Networks']","['Applications: Biomedical/healthcare/medicine', 'Biometrics', 'face', 'gesture', 'body pose']",54,"Camera-based physiological measurement is a growing field with neural models providing state-of-the-art performance. Prior research has explored various ""end-to-end"" architectures; however these methods still require several preprocessing steps and are not able to run directly on mobile and edge devices. The operations are often non-trivial to implement, making replication and deployment difficult and can even have a higher computational budget than the ""core"" network itself. In this paper, we propose two novel and efficient neural models for camera-based physiological measurement called EfficientPhys that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. Using an input of raw video frames, our models achieve strong accuracy on three public datasets. We show that this is the case whether using a transformer or convolutional backbone. We further evaluate the latency of the proposed networks and show that our most lightweight network also achieves a 33% improvement in efficiency."
Ego-Vehicle Action Recognition Based on Semi-Supervised Contrastive Learning,"Chihiro Noguchi, Toshihiro Tanizawa","Toyota Motor Corporation, Japan",0,,100,Japan,"In recent years, many automobiles have been equipped with cameras, which have accumulated an enormous amount of video footage of driving scenes. Autonomous driving demands the highest level of safety, for which even unimaginably rare driving scenes have to be collected in training data to improve the recognition accuracy for specific scenes. However, it is prohibitively costly to find very few specific scenes from an enormous amount of videos. In this article, we show that proper video-to-video distances can be defined by focusing on ego-vehicle actions. It is well known that existing methods based on supervised learning cannot handle videos that do not fall into predefined classes, though they work well in defining video-to-video distances in the embedding space between labeled videos. To tackle this problem, we propose a method based on semi-supervised contrastive learning. We consider two related but distinct contrastive learning: standard graph contrastive learning and our proposed SOIA-based contrastive learning. We observe that the latter approach can provide more sensible video-to-video distances between unlabeled videos. Next, the effectiveness of our method is quantified by evaluating the classification performance of the ego-vehicle action recognition using HDD dataset, which shows that our method including unlabeled data in training significantly outperforms the existing methods using only labeled data in training.",https://openaccess.thecvf.com/content/WACV2023/html/Noguchi_Ego-Vehicle_Action_Recognition_Based_on_Semi-Supervised_Contrastive_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Noguchi_Ego-Vehicle_Action_Recognition_Based_on_Semi-Supervised_Contrastive_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030895/,"['Training', 'Computer vision', 'Supervised learning', 'Training data', 'Focusing', 'Cameras', 'Safety']","['Action Recognition', 'Semi-supervised Learning', 'Self-supervised Learning', 'Latent Space', 'Unlabeled Data', 'Learning Methods', 'Convolutional Neural Network', 'Positive Samples', 'Computer Vision', 'Negative Samples', 'Hidden Markov Model', 'Data Augmentation', 'Bounding Box', 'Graph Structure', 'Batch Of Samples', 'Propagation Distance', 'Graph Convolutional Network', 'Semantic Labels', 'Adjacent Frames', 'Image Lines', 'Object Instances', 'Method In This Article', 'Lane Change', 'Left Turn', 'Original Video', 'Pedestrian', 'Object Tracking', 'Graph Construction', 'Rare Occasions', 'Loss Function']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Robotics']",5,"In recent years, many automobiles have been equipped with cameras, which have accumulated an enormous amount of video footage of driving scenes. Autonomous driving demands the highest level of safety, for which even unimaginably rare driving scenes have to be collected in training data to improve the recognition accuracy for specific scenes. However, it is prohibitively costly to find very few specific scenes from an enormous amount of videos. In this article, we show that proper video-to-video distances can be defined by focusing on ego-vehicle actions. It is well known that existing methods based on supervised learning cannot handle videos that do not fall into predefined classes, though they work well in defining video-to-video distances in the embedding space between labeled videos. To tackle this problem, we propose a method based on semi-supervised contrastive learning. We consider two related but distinct contrastive learning: standard graph contrastive learning and our proposed SOIA-based contrastive learning. We observe that the latter approach can provide more sensible video-to-video distances between unlabeled videos. Next, the effectiveness of our method is quantified by evaluating the classification performance of the ego-vehicle action recognition using HDD dataset, which shows that our method including unlabeled data in training significantly outperforms the existing methods using only labeled data in training."
Elimination of Non-Novel Segments at Multi-Scale for Few-Shot Segmentation,"Alper Kayabaşı, Gülin Tüfekci, İlkay Ulusoy","Research Center, Aselsan Inc; Middle East Technical University, Ankara, Turkey",50,Turkey,50,Turkey,"Few-shot segmentation aims to devise a generalizing model that segments query images from unseen classes during training with the guidance of a few support images whose class tally with the class of the query. There exist two domain-specific problems mentioned in the previous works, namely spatial inconsistency and bias towards seen classes. Taking the former problem into account, our method compares the support feature map with the query feature map at multi scales to become scale-agnostic. As a solution to the latter problem, a supervised model, called as base learner, is trained on available classes to accurately identify pixels belonging to seen classes. Hence, subsequent meta learner has a chance to discard areas belonging to seen classes with the help of an ensemble learning model that coordinates meta learner with the base learner. We simultaneously address these two vital problems for the first time and achieve state-of-the-art performances on both PASCAL-5i and COCO-20i datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Kayabasi_Elimination_of_Non-Novel_Segments_at_Multi-Scale_for_Few-Shot_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kayabasi_Elimination_of_Non-Novel_Segments_at_Multi-Scale_for_Few-Shot_Segmentation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031013/,"['Training', 'Image segmentation', 'Computer vision', 'Computational modeling', 'Predictive models', 'Benchmark testing', 'Ensemble learning']","['Few-shot Segmentation', 'Feature Maps', 'Ensemble Model', 'Base Learners', 'Query Image', 'Query Features', 'Unseen Classes', 'Meta Learning', 'Convolutional Layers', 'Semantic Segmentation', 'Average Pooling', 'Higher Levels Of Support', 'Base Classes', 'Training In Order', 'Support Set', 'Query Set', 'Pixel Features', 'Enriched Modules', 'Priority Map', 'Pyramid Pooling Module']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Biomedical/healthcare/medicine']",3,"Few-shot segmentation aims to devise a generalizing model that segments query images from unseen classes during training with the guidance of a few support images whose class tally with the class of the query. There exist two domain-specific problems mentioned in the previous works, namely spatial inconsistency and bias towards seen classes. Taking the former problem into account, our method compares the support feature map with the query feature map at multi scales to become scale-agnostic. As a solution to the latter problem, a supervised model, called as base learner, is trained on available classes to accurately identify pixels belonging to seen classes. Hence, subsequent meta learner has a chance to discard areas belonging to seen classes with the help of an ensemble learning model that coordinates meta learner with the base learner. We simultaneously address these two vital problems for the first time and achieve state-of-the-art performances on both PASCAL-5
<sup>i</sup>
 and COCO-20
<sup>i</sup>
 datasets."
ElliPose: Stereoscopic 3D Human Pose Estimation by Fitting Ellipsoids,"Christian Grund, Julian Tanke, Jürgen Gall",University of Bonn; AISC GmbH,50,Germany,50,Germany,"One of the most relevant tasks for augmented and virtual reality applications is the interaction of virtual objects with real humans which requires accurate 3D human pose predictions. Obtaining accurate 3D human poses requires careful camera calibration which is difficult for non-technical personal or in a pop-up scenario. Recent markerless motion capture approaches require accurate camera calibration at least for the final triangulation step. Instead, we solve this problem by presenting ElliPose, Stereoscopic 3D Human Pose Estimation by Fitting Ellipsoids, where we jointly estimate the 3D human as well as the camera pose. We exploit the fact that bones do not change in length over the course of a sequence and thus their relative trajectories have to lie on the surface of a sphere which we can utilize to iteratively correct the camera and 3D pose estimation. As another use-case we demonstrate that our approach can be used as replacement for ground-truth 3D poses to train monocular 3D pose estimators. We show that our method produces competitive results even when comparing with state-of-the-art methods that use more cameras or ground-truth camera extrinsics.",https://openaccess.thecvf.com/content/WACV2023/html/Grund_ElliPose_Stereoscopic_3D_Human_Pose_Estimation_by_Fitting_Ellipsoids_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Grund_ElliPose_Stereoscopic_3D_Human_Pose_Estimation_by_Fitting_Ellipsoids_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030449/,"['Three-dimensional displays', 'Stereo image processing', 'Pose estimation', 'Fitting', 'Virtual reality', 'Cameras', 'Bones']","['3D Images', 'Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Motion Capture', 'Spherical Surface', 'Camera Calibration', 'Camera Pose', 'Object Interaction', '3D Pose', 'Pose Prediction', 'Coordinate System', 'Stage 2', 'Weighting Factor', '2D Images', 'Generative Adversarial Networks', 'Iteration Step', 'Sequence Of Frames', 'Camera Position', '2D Pose', 'Pair Of Cameras', 'Bone Length', 'Left Camera', 'Person Image', 'Break Condition', 'World Coordinate', 'Front Camera', 'Principle Axis', 'Left Hip']",['Algorithms: 3D computer vision'],3,"One of the most relevant tasks for augmented and virtual reality applications is the interaction of virtual objects with real humans which requires accurate 3D human pose predictions. Obtaining accurate 3D human poses requires careful camera calibration which is difficult for nontechnical personal or in a pop-up scenario. Recent markerless motion capture approaches require accurate camera calibration at least for the final triangulation step. Instead, we solve this problem by presenting ElliPose, Stereoscopic 3D Human Pose Estimation by Fitting Ellipsoids, where we jointly estimate the 3D human as well as the camera pose. We exploit the fact that bones do not change in length over the course of a sequence and thus their relative trajectories have to lie on the surface of a sphere which we can utilize to iteratively correct the camera and 3D pose estimation. As another use-case we demonstrate that our approach can be used as replacement for ground-truth 3D poses to train monocular 3D pose estimators. We show that our method produces competitive results even when comparing with state-of-the-art methods that use more cameras or ground-truth camera extrinsics."
EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos Stage Development Classification,"Tien-Phat Nguyen, Trong-Thang Pham, Tri Nguyen, Hieu Le, Dung Nguyen, Hau Lam, Phong Nguyen, Jennifer Fowler, Minh-Triet Tran, Ngan Le","HOPE Research Center, My Duc Hospital, Ho Chi Minh City, Vietnam; University of Science, VNU-HCM; Vietnam National University, Ho Chi Minh City, Vietnam; Olea Fertility, Vinmec Central Park International Hospital, Ho Chi Minh City, Vietnam; Arkansas Economic Development Commission, Little Rock, AR USA 72202; John von Neumann Institute, Vietnam National University, Ho Chi Minh City, Vietnam; FPT Software AI Center, Ho Chi Minh City, Vietnam; Department of Computer Science and Computer Engineering, University of Arkansas, Fayetteville, AR, USA 72703; IVFMD, My Duc Phu Nhuan hospital, Ho Chi Minh City, Vietnam",77.77777778,"USA, Vietnam",22.22222222,USA,"The timing of cell divisions in early embryos during the In-Vitro Fertilization (IVF) process is a key predictor of embryo viability. However, observing cell divisions in Time-Lapse Monitoring (TLM) is a time-consuming process and highly depends on experts. In this paper, we propose EmbryosFormer, a computational model to automatically detect and classify cell divisions from original time-lapse images. Our proposed network is designed as an encoder-decoder deformable transformer with collaborative heads. The transformer contracting path predicts per-image labels and is optimized by a classification head. The transformer expanding path models the temporal coherency between embryo images to ensure monotonic non-decreasing constraint and is optimized by a segmentation head. Both contracting and expanding paths are synergetically learned by a collaboration head. We have benchmarked our proposed EmbryosFormer on two datasets: a public dataset with mouse embryos with 8-cell stage and an in-house dataset with human embryos with 4-cell stage. Source code: https://github.com/UARK-AICV/Embryos.",https://openaccess.thecvf.com/content/WACV2023/html/Nguyen_EmbryosFormer_Deformable_Transformer_and_Collaborative_Encoding-Decoding_for_Embryos_Stage_Development_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nguyen_EmbryosFormer_Deformable_Transformer_and_Collaborative_Encoding-Decoding_for_Embryos_Stage_Development_WACV_2023_paper.pdf,,https://github.com/UARK-AICV/Embryos,2210.04615,main,Poster,https://ieeexplore.ieee.org/document/10030102/,"['Image segmentation', 'Embryo', 'Head', 'Computational modeling', 'Source coding', 'Collaboration', 'Transformers']","['Deformation Transformation', 'Cell Division', 'Time-lapse Imaging', 'In Vitro Fertilization', 'Human Embryos', 'Temporal Coherence', 'Imaging Of Embryos', 'Classification Head', 'Cell Division Time', 'Convolutional Neural Network', 'Decoding', 'Deep Neural Network', 'Feature Maps', 'Number Of Images', 'Precision And Recall', 'Cell Stage', 'Ground Truth Labels', 'Multi-scale Features', 'Time-lapse Video', 'Feature Encoder', 'Conditional Random Field', 'Global Accuracy', 'Inference Time', 'Intracytoplasmic Sperm Injection', 'Image-level Labels', 'Decoder Network', 'Discrete Time Points', 'Position Embedding']","['Applications: Biomedical/healthcare/medicine', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",4,"The timing of cell divisions in early embryos during the In-Vitro Fertilization (IVF) process is a key predictor of embryo viability. However, observing cell divisions in Time-Lapse Monitoring (TLM) is a time-consuming process and highly depends on experts. In this paper, we propose EmbryosFormer, a computational model to automatically detect and classify cell divisions from original time-lapse images. Our proposed network is designed as an encoder-decoder deformable transformer with collaborative heads. The transformer contracting path predicts per-image labels and is optimized by a classification head. The transformer expanding path models the temporal coherency between embryo images to ensure monotonic non-decreasing constraint and is optimized by a segmentation head. Both contracting and expanding paths are synergetically learned by a collaboration head. We have benchmarked our proposed EmbryosFormer on two datasets: a public dataset with mouse embryos with 8-cell stage and an in-house dataset with human embryos with 4-cell stage. Source code: https://github.com/UARK-AICV/Embryos."
Empirical Generalization Study: Unsupervised Domain Adaptation vs. Domain Generalization Methods for Semantic Segmentation in the Wild,"Fabrizio J. Piva, Daan de Geus, Gijs Dubbelman",Eindhoven University of Technology,100,Netherlands,0,,"For autonomous vehicles and mobile robots to safely operate in the real world, i.e., the wild, scene understanding models should perform well in the many different scenarios that can be encountered. In reality, these scenarios are not all represented in the model's training data, leading to poor performance. To tackle this, current training strategies attempt to either exploit additional unlabeled data with unsupervised domain adaptation (UDA), or to reduce overfitting using the limited available labeled data with domain generalization (DG). However, it is not clear from current literature which of these methods allows for better generalization to unseen data from the wild. Therefore, in this work, we present an evaluation framework in which the generalization capabilities of state-of-the-art UDA and DG methods can be compared fairly. From this evaluation, we find that UDA methods, which leverage unlabeled data, outperform DG methods in terms of generalization, and can deliver similar performance on unseen data as fully-supervised training methods that require all data to be labeled. We show that semantic segmentation performance can be increased up to 30% for a priori unknown data without using any extra labeled data.",https://openaccess.thecvf.com/content/WACV2023/html/Piva_Empirical_Generalization_Study_Unsupervised_Domain_Adaptation_vs._Domain_Generalization_Methods_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Piva_Empirical_Generalization_Study_Unsupervised_Domain_Adaptation_vs._Domain_Generalization_Methods_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030340/,"['Training', 'Adaptation models', 'Computer vision', 'Protocols', 'Semantic segmentation', 'Computational modeling', 'Training data']","['Segmentation Method', 'Semantic Segmentation', 'Domain Adaptation', 'Domain Generalization', 'Domain Adaptation Methods', 'Semantic Segmentation Methods', 'Unsupervised Domain Adaptation Methods', 'Domain Generalization Methods', 'Training Data', 'Training Strategy', 'Evaluation Framework', 'Generalization Capability', 'Unlabeled Data', 'Unseen Data', 'Training Set', 'Uniform Distribution', 'Similar Conditions', 'Training Dataset', 'Deep Neural Network', 'Quantitative Comparison', 'Unseen Domains', 'Semantic Segmentation Models', 'Urban Scenes', 'Semantic Segmentation Network', 'Homogeneous Dataset', 'Training Domain', 'Mobile Agents', 'Heterogeneous Datasets', 'Standard Cross-entropy Loss', 'Evaluation Protocol']","['Applications: Robotics', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",9,"For autonomous vehicles and mobile robots to safely operate in the real world, i.e., the wild, scene understanding models should perform well in the many different scenarios that can be encountered. In reality, these scenarios are not all represented in the model’s training data, leading to poor performance. To tackle this, current training strategies attempt to either exploit additional unlabeled data with unsupervised domain adaptation (UDA), or to reduce overfitting using the limited available labeled data with domain generalization (DG). However, it is not clear from current literature which of these methods allows for better generalization to unseen data from the wild. Therefore, in this work, we present an evaluation framework in which the generalization capabilities of state-of-the-art UDA and DG methods can be compared fairly. From this evaluation, we find that UDA methods, which leverage unlabeled data, outperform DG methods in terms of generalization, and can deliver similar performance on unseen data as fully-supervised training methods that require all data to be labeled. We show that semantic segmentation performance can be increased up to 30% for a priori unknown data without using any extra labeled data."
Enabling ISPless Low-Power Computer Vision,"Gourav Datta, Zeyu Liu, Zihan Yin, Linyu Sun, Akhilesh R. Jaiswal, Peter A. Beerel","University of Southern California, Los Angeles, USA",100,USA,0,,"Current computer vision (CV) systems use an image signal processing (ISP) unit to convert the high resolution raw images captured by image sensors to visually pleasing RGB images. Typically, CV models are trained on these RGB images and have yielded state-of-the-art (SOTA) performance on a wide range of complex vision tasks, such as object detection. In addition, in order to deploy these models on resource-constrained low-power devices, recent works have proposed in-sensor and in-pixel computing approaches that try to partly/fully bypass the ISP and yield significant bandwidth reduction between the image sensor and the CV processing unit by downsampling the activation maps in the initial convolutional neural network (CNN) layers. However, direct inference on the raw images degrades the test accuracy due to the difference in covariance of the raw images captured by the image sensors compared to the ISP-processed images used for training. Moreover, it is difficult to train deep CV models on raw images, because most (if not all) large-scale open-source datasets consist of RGB images. To mitigate this concern, we propose to invert the ISP pipeline, which can convert the RGB images of any dataset to its raw counterparts, and enable model training on raw images. We release the raw version of the COCO dataset, a large-scale benchmark for generic high-level vision tasks. For ISP-less CV systems, training on these raw images result in a 7.1% increase in test accuracy on the visual wake works (VWW) dataset compared to relying on training with traditional ISP-processed RGB datasets. To further improve the accuracy of ISP-less CV models and to increase the energy and bandwidth benefits obtained by in-sensor/in-pixel computing, we propose an energy-efficient form of analog in-pixel demosaicing that may be coupled with in-pixel CNN computations. When evaluated on raw images captured by real sensors from the PASCALRAW dataset, our approach results in a 8.1% increase in mAP.",https://openaccess.thecvf.com/content/WACV2023/html/Datta_Enabling_ISPless_Low-Power_Computer_Vision_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Datta_Enabling_ISPless_Low-Power_Computer_Vision_WACV_2023_paper.pdf,,https://github.com/godatta/ISP-less-CV,,main,Poster,https://ieeexplore.ieee.org/document/10030746/,"['Training', 'Image sensors', 'Computer vision', 'Visualization', 'Computational modeling', 'Bandwidth', 'Sensors']","['Computer Vision', 'Neural Network', 'Convolutional Neural Network', 'Test Accuracy', 'Object Detection', 'Processing Unit', 'Large-scale Datasets', 'Raw Images', 'Learning Applications', 'RGB Images', 'Image Sensor', 'Increase In Accuracy', 'Vision Tasks', 'Convolutional Neural Network Layers', 'Wide Range Of Tasks', 'COCO Dataset', 'Few-shot Learning', 'Low-power Devices', 'Direct Inference', 'Energy Benefits', 'Pixel Array', 'Blue Pixels', 'Base Classes', 'Accuracy Drop', 'Gamma Correction', 'Faster R-CNN', 'Analog-to-digital Converter', 'Large-scale Image', 'Convolutional Layers', 'Input Channels']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Embedded sensing/real-time techniques']",3,"Current computer vision (CV) systems use an image signal processing (ISP) unit to convert the high resolution raw images captured by image sensors to visually pleasing RGB images. Typically, CV models are trained on these RGB images and have yielded state-of-the-art (SOTA) performance on a wide range of complex vision tasks, such as object detection. In addition, in order to deploy these models on resource-constrained low-power devices, recent works have proposed in-sensor and in-pixel computing approaches that try to partly/fully bypass the ISP and yield significant bandwidth reduction between the image sensor and the CV processing unit by downsampling the activation maps in the initial convolutional neural network (CNN) layers. However, direct inference on the raw images degrades the test accuracy due to the difference in covariance of the raw images captured by the image sensors compared to the ISP-processed images used for training. Moreover, it is difficult to train deep CV models on raw images, because most (if not all) large-scale open-source datasets consist of RGB images. To mitigate this concern, we propose to invert the ISP pipeline, which can convert the RGB images of any dataset to its raw counterparts, and enable model training on raw images. We release the raw version of the COCO dataset, a large-scale benchmark for generic high-level vision tasks. For ISP-less CV systems, training on these raw images result in a ∼7.1% increase in test accuracy on the visual wake works (VWW) dataset compared to relying on training with traditional ISP-processed RGB datasets. To further improve the accuracy of ISP-less CV models and to increase the energy and bandwidth benefits obtained by in-sensor/in-pixel computing, we propose an energy-efficient form of analog in-pixel demosaicing that may be coupled with in-pixel CNN computations. When evaluated on raw images captured by real sensors from the PASCALRAW dataset, our approach results in a 8.1% increase in mAP. Lastly, we demonstrate a further 20.5% increase in mAP by using a novel application of few-shot learning with thirty shots each for the novel PASCALRAW dataset, constituting 3 classes. Codes are available at https://github.com/godatta/ISP-less-CV."
Encouraging Disentangled and Convex Representation With Controllable Interpolation Regularization,"Yunhao Ge, Zhi Xu, Yao Xiao, Gan Xin, Yunkui Pang, Laurent Itti","University of Southern California, Los Angeles, CA, USA",100,USA,0,,"We focus on controllable disentangled representation learning (C-Dis-RL), where users can control the partition of the disentangled latent space to factorize dataset attributes (concepts) for downstream tasks. Two general problems remain under-explored in current methods: (1) They lack comprehensive disentanglement constraints, especially missing the minimization of mutual information between different attributes across latent and observation domains. (2) They lack convexity constraints, which is important for meaningfully manipulating specific attributes for downstream tasks. To encourage both comprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where disentanglement and convexity can help each other. Specifically, we conduct controlled interpolation in latent space during training, and we reuse the encoder to help form a 'perfect disentanglement' regularization. In that case, (a) disentanglement loss implicitly enlarges the potential understandable distribution to encourage convexity; (b) convexity can in turn improve robust and precise disentanglement. CIR is a general module and we merge CIR with three different algorithms: ELEGANT, I2I-Dis, and GZS-Net to show the compatibility and effectiveness. Qualitative and quantitative experiments show improvement in C-Dis-RL and latent convexity by CIR. This further improves downstream tasks: controllable image synthesis, cross-modality image translation and zero-shot synthesis.",https://openaccess.thecvf.com/content/WACV2023/html/Ge_Encouraging_Disentangled_and_Convex_Representation_With_Controllable_Interpolation_Regularization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ge_Encouraging_Disentangled_and_Convex_Representation_With_Controllable_Interpolation_Regularization_WACV_2023_paper.pdf,,,2112.03163,main,Poster,https://ieeexplore.ieee.org/document/10030283/,"['Representation learning', 'Training', 'Interpolation', 'Computer vision', 'Image synthesis', 'Aerospace electronics', 'Minimization']","['Convex Representation', 'Mutual Information', 'Representation Learning', 'Latent Space', 'Quantitative Experiments', 'Control Synthesis', 'Image Synthesis', 'Convex Constraints', 'Disentangled Representation', 'Image Quality', 'Linear Interpolation', 'Image Generation', 'Face Images', 'Image Space', 'PageRank', 'Loss Term', 'Latent Representation', 'Variational Autoencoder', 'Hair Color', 'Left Image', 'Latent Code', 'Reconstruction Loss', 'Attribute Values', 'Semantic Properties', 'Latent Dimensions', 'Font Color', 'Artifacts In Regions', 'Images Of Dogs', 'Translation Task', 'Important Terms']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Computational photography', 'image and video synthesis']",1,"We focus on controllable disentangled representation learning (C-Dis-RL), where users can control the partition of the disentangled latent space to factorize dataset attributes (concepts) for downstream tasks. Two general problems remain under-explored in current methods: (1) They lack comprehensive disentanglement constraints, especially missing the minimization of mutual information between different attributes across latent and observation domains. (2) They lack convexity constraints, which is important for meaningfully manipulating specific attributes for downstream tasks. To encourage both comprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where disentanglement and convexity can help each other. Specifically, we conduct controlled interpolation in latent space during training, and we reuse the encoder to help form a ’perfect disentanglement’ regularization. In that case, (a) disentanglement loss implicitly enlarges the potential understandable distribution to encourage convexity; (b) convexity can in turn improve robust and precise disentanglement. CIR is a general module and we merge CIR with three different algorithms: ELEGANT, I2I-Dis, and GZS-Net to show the compatibility and effectiveness. Qualitative and quantitative experiments show improvement in C-Dis-RL and latent convexity by CIR. This further improves downstream tasks: controllable image synthesis, cross-modality image translation and zero-shot synthesis."
End-to-End Single-Frame Image Signal Processing for High Dynamic Range Scenes,"Khanh Quoc Dinh, Kwang Pyo Choi","Samsung Research, Samsung Electronics",0,,100,South Korea,"This paper considers photography of high dynamic range scenes containing mixtures of shadows and highlights on mobile phones. Multi-frame merging constructs a high-quality image at the cost of capturing multiple frames of the same scene. Contrarily, end-to-end optimized image signal processing (E2EISP) produces an enhanced image from a single-frame Bayer array. This paper combines the merits of the two approaches by using labels of high-quality multi-frame merged images to train E2EISP with a novel neural network architecture composed of a multi-head mixture of brightness enhancement for accurately processing shadows/highlights and a multi-head mixture of image processing featured camera settings of white balance and color correction for a proper color generation. We also proposed a combination of supervised, unsupervised, and generative adversarial losses for brightness, edge, and detail enhancement. Experimental results show that the proposed single-frame ISP produces enhanced images and outperforms state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Dinh_End-to-End_Single-Frame_Image_Signal_Processing_for_High_Dynamic_Range_Scenes_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dinh_End-to-End_Single-Frame_Image_Signal_Processing_for_High_Dynamic_Range_Scenes_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030937/,"['Image color analysis', 'Array signal processing', 'Image edge detection', 'Neural networks', 'Brightness', 'Computer architecture', 'Transforms']","['Dynamic Range', 'High Dynamic Range', 'Dynamic Scenes', 'Image Signal Processing', 'High Dynamic Range Scenes', 'Neural Network', 'Image Processing', 'Mobile Phone', 'High-quality Images', 'Multiple Frames', 'White Balance', 'Color Correction', 'Edge Enhancement', 'Denoising', 'Loss Of Generality', 'Convolutional Layers', 'Mixture Model', 'Generative Adversarial Networks', 'High Dynamic Range Image', 'Color Processing', 'Clear Image', 'Low Dynamic Range', 'Gamma Correction', 'Crop Row', 'Training Loss', 'Perspective Transformation', 'Unwanted Artifacts', 'Diffuse Solar Radiation']","['Algorithms: Low-level and physics-based vision', 'Computational photography', 'image and video synthesis']",1,"This paper considers photography of high dynamic range scenes containing mixtures of shadows and highlights on mobile phones. Multi-frame merging constructs a high-quality image at the cost of capturing multiple frames of the same scene. Contrarily, end-to-end optimized image signal processing (E2EISP) produces an enhanced image from a single-frame Bayer array. This paper combines the merits of the two approaches by using labels of high-quality multi-frame merged images to train E2EISP with a novel neural network architecture composed of a multi-head mixture of brightness enhancement for accurately processing shadows/highlights and a multi-head mixture of image processing featured camera settings of white balance and color correction for a proper color generation. We also proposed a combination of supervised, unsupervised, and generative adversarial losses for brightness, edge, and detail enhancement. Experimental results show that the proposed single-frame ISP produces enhanced images and outperforms state-of-the-art methods."
Enhanced Bi-Directional Motion Estimation for Video Frame Interpolation,"Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen, Jayoon Koo, Cheul-hee Hahm","Samsung Electronics, South Korea; Samsung Electronics (China) R&D Center",0,,100,South Korea,"We propose a simple yet effective algorithm for motion-based video frame interpolation. Existing motion-based interpolation methods typically rely on an off-the-shelf optical flow model or a U-Net based pyramid network for motion estimation, which either suffer from large model size or limited capacity in handling various challenging motion cases. In this work, we present a novel compact model to simultaneously estimate the bi-directional motions between input frames. It is designed by carefully adapting the ingredients (e.g., warping, correlation) in optical flow research for simultaneous bi-directional motion estimation within a flexible pyramid recurrent framework. Our motion estimator is extremely lightweight (15x smaller than PWC-Net), yet enables reliable handling of large and complex motion cases. Based on estimated bi-directional motions, we employ a synthesis network to fuse forward-warped representations and predict the intermediate frame. Our method achieves excellent performance on a broad range of frame interpolation benchmarks. Code and trained models are available at: https://github.com/srcn-ivl/EBME.",https://openaccess.thecvf.com/content/WACV2023/html/Jin_Enhanced_Bi-Directional_Motion_Estimation_for_Video_Frame_Interpolation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jin_Enhanced_Bi-Directional_Motion_Estimation_for_Video_Frame_Interpolation_WACV_2023_paper.pdf,,https://github.com/srcn-ivl/EBME,2206.08572,main,Poster,https://ieeexplore.ieee.org/document/10030216/,"['Interpolation', 'Adaptation models', 'Fuses', 'Motion estimation', 'Ultraviolet sources', 'Bidirectional control', 'Benchmark testing']","['Motion Estimation', 'Frame Interpolation', 'Interpolation Method', 'Optical Flow', 'Large Motion', 'Simultaneous Estimation', 'Optical Model', 'Input Frames', 'Synthesis Network', 'Intermediate Frames', 'Low Resolution', 'Small Objects', 'Peak Signal-to-noise Ratio', 'Recurrent Unit', 'Feature Encoder', 'Original Frame', 'Motion Field', 'Frame Features', 'Pyramid Level', 'Image Pyramid', 'CNN Features', 'Downsampling Layer', 'Interpolation Results', 'Magnitude Of Motion', 'Downsampling Factor', '4K Resolution']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",12,"We propose a simple yet effective algorithm for motion-based video frame interpolation. Existing motion-based interpolation methods typically rely on an off-the-shelf optical flow model or a U-Net based pyramid network for motion estimation, which either suffer from large model size or limited capacity in handling various challenging motion cases. In this work, we present a novel compact model to simultaneously estimate the bi-directional motions between input frames. It is designed by carefully adapting the ingredients (e.g., warping, correlation) in optical flow research for simultaneous bi-directional motion estimation within a flexible pyramid recurrent framework. Our motion estimator is extremely lightweight (15x smaller than PWC-Net), yet enables reliable handling of large and complex motion cases. Based on estimated bi-directional motions, we employ a synthesis network to fuse forward-warped representations and predict the intermediate frame. Our method achieves excellent performance on a broad range of frame interpolation benchmarks. Code and trained models are available at https://github.com/srcn-ivl/EBME."
Enriched CNN-Transformer Feature Aggregation Networks for Super-Resolution,"Jinsu Yoo, Taehoon Kim, Sihaeng Lee, Seung Hwan Kim, Honglak Lee, Tae Hyun Kim",Hanyang University; LG AI Research,50,South Korea,50,South Korea,"Recent transformer-based super-resolution (SR) methods have achieved promising results against conventional CNN-based methods. However, these approaches suffer from essential shortsightedness created by only utilizing the standard self-attention-based reasoning. In this paper, we introduce an effective hybrid SR network to aggregate enriched features, including local features from CNNs and long-range multi-scale dependencies captured by transformers. Specifically, our network comprises transformer and convolutional branches, which synergetically complement each representation during the restoration procedure. Furthermore, we propose a cross-scale token attention module, allowing the transformer branch to exploit the informative relationships among tokens across different scales efficiently. Our proposed method achieves state-of-the-art SR results on numerous benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Yoo_Enriched_CNN-Transformer_Feature_Aggregation_Networks_for_Super-Resolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yoo_Enriched_CNN-Transformer_Feature_Aggregation_Networks_for_Super-Resolution_WACV_2023_paper.pdf,,https://github.com/jinsuyoo/act,2203.07682,main,Poster,https://ieeexplore.ieee.org/document/10030797/,"['Computer vision', 'Aggregates', 'Superresolution', 'Computer architecture', 'Benchmark testing', 'Transformers', 'Cognition']","['Super-resolution', 'Local Features', 'Benchmark Datasets', 'Attention Module', 'Long-range Dependencies', 'Super-resolution Network', 'Super-resolution Results', 'Convolutional Layers', 'Input Image', 'Feature Maps', 'Feed-forward Network', 'Deep Features', 'Peak Signal-to-noise Ratio', 'Multi-scale Features', 'Low-resolution Images', 'Convolutional Block', 'Tokenized', 'Intermediate Features', 'Shallow Features', 'Position Embedding', 'Multi-head Self-attention', 'Transformer Block', 'Fusion Block', 'Super-resolution Performance', 'Expansion Ratio', 'Attention Operation', 'Inductive Bias']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",42,"Recent transformer-based super-resolution (SR) methods have achieved promising results against conventional CNN-based methods. However, these approaches suffer from essential shortsightedness created by only utilizing the standard self-attention-based reasoning. In this paper, we introduce an effective hybrid SR network to aggregate enriched features, including local features from CNNs and long-range multi-scale dependencies captured by transformers. Specifically, our network comprises transformer and convolutional branches, which synergetically complement each representation during the restoration procedure. Furthermore, we propose a cross-scale token attention module, allowing the transformer branch to exploit the informative relationships among tokens across different scales efficiently. Our proposed method achieves state-of-the-art SR results on numerous benchmark datasets."
Ev-NeRF: Event Based Neural Radiance Field,"Inwoo Hwang, Junho Kim, Young Min Kim","Department of Electrical and Computer Engineering, Seoul National University; Interdisciplinary Program in Artificial Intelligence and INMC, Seoul National University; Department of Electrical and Computer Engineering, Seoul National University",100,South Korea,0,,"We present Ev-NeRF, a Neural Radiance Field derived from event data. While event cameras can measure subtle brightness changes in high frame rates, the measurements in low lighting or extreme motion suffer from significant domain discrepancy with complex noise. As a result, the performance of event-based vision tasks does not transfer to challenging environments, where the event cameras are expected to thrive over normal cameras. We find that the multi-view consistency of NeRF provides a powerful self-supervision signal for eliminating spurious measurements and extracting the consistent underlying structure despite highly noisy input. Instead of posed images of the original NeRF, the input to Ev-NeRF is the event measurements accompanied by the movements of the sensors. Using the loss function that reflects the measurement model of the sensor, Ev-NeRF creates an integrated neural volume that summarizes the unstructured and sparse data points captured for about 2-4 seconds. The generated neural volume can also produce intensity images from novel views with reasonable depth estimates, which can serve as a high-quality input to various vision-based tasks. Our results show that Ev-NeRF achieves competitive performance for intensity image reconstruction under extreme noise conditions and high-dynamic-range imaging.",https://openaccess.thecvf.com/content/WACV2023/html/Hwang_Ev-NeRF_Event_Based_Neural_Radiance_Field_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hwang_Ev-NeRF_Event_Based_Neural_Radiance_Field_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031005/,"['Image sensors', 'Solid modeling', 'Volume measurement', 'Lighting', 'Cameras', 'Loss measurement', 'Sensors']","['Neural Radiance Fields', 'Event Data', 'Image Reconstruction', 'Image Intensity', 'Depth Estimation', 'High Dynamic Range', 'Sensor Model', 'Brightness Changes', 'Complex Noise', 'Movement Sensor', 'Domain Discrepancy', 'High Dynamic Range Image', 'Dynamic Vision Sensor', 'Raw Data', 'Neural Network', 'Root Mean Square Error', 'Training Set', '3D Structure', 'Domain Shift', 'Neural Representations', 'Camera Pose', 'Real Noise', 'Structure From Motion', 'Severe Noise', 'Amount Of Noise', 'Low Light Conditions', 'Conventional Camera', 'Static Scenes', 'Heaviside Function', 'Event Stream']","['Algorithms: 3D computer vision', 'Computational photography', 'image and video synthesis']",17,"We present Ev-NeRF, a Neural Radiance Field derived from event data. While event cameras can measure subtle brightness changes in high frame rates, the measurements in low lighting or extreme motion suffer from significant domain discrepancy with complex noise. As a result, the performance of event-based vision tasks does not transfer to challenging environments, where the event cameras are expected to thrive over normal cameras. We find that the multi-view consistency of NeRF provides a powerful self-supervision signal for eliminating spurious measurements and extracting the consistent underlying structure despite highly noisy input. Instead of posed images of the original NeRF, the input to Ev-NeRF is the event measurements accompanied by the movements of the sensors. Using the loss function that reflects the measurement model of the sensor, Ev-NeRF creates an integrated neural volume that summarizes the unstructured and sparse data points captured for about 2-4 seconds. The generated neural volume can also produce intensity images from novel views with reasonable depth estimates, which can serve as a high-quality input to various vision-based tasks. Our results show that Ev-NeRF achieves competitive performance for intensity image reconstruction under extreme noise and high-dynamic-range imaging."
Evaluating Generative Networks Using Gaussian Mixtures of Image Features,"Lorenzo Luzi, Carlos Ortiz Marrero, Nile Wynar, Richard G. Baraniuk, Michael J. Henry","Rice University, Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Rice University",100,USA,0,,"We develop a measure for evaluating the performance of generative networks given two sets of images. A popular performance measure currently used to do this is the Frechet Inception Distance (FID). FID assumes that images featurized using the penultimate layer of Inception-v3 follow a Gaussian distribution, an assumption which cannot be violated if we wish to use FID as a metric. However, we show that Inception-v3 features of the ImageNet dataset are not Gaussian; in particular, every single marginal is not Gaussian. To remedy this problem, we model the featurized images using Gaussian mixture models (GMMs) and compute the 2-Wasserstein distance restricted to GMMs. We define a performance measure, which we call WaM, on two sets of images by using Inception-v3 (or another classifier) to featurize the images, estimate two GMMs, and use the restricted 2-Wasserstein distance to compare the GMMs. We experimentally show the advantages of WaM over FID, including how FID is more sensitive than WaM to imperceptible image perturbations. By modelling the non-Gaussian features obtained from Inception-v3 as GMMs and using a GMM metric, we can more accurately evaluate generative network performance.",https://openaccess.thecvf.com/content/WACV2023/html/Luzi_Evaluating_Generative_Networks_Using_Gaussian_Mixtures_of_Image_Features_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Luzi_Evaluating_Generative_Networks_Using_Gaussian_Mixtures_of_Image_Features_WACV_2023_paper.pdf,,,2110.0524,main,Poster,https://ieeexplore.ieee.org/document/10030239/,"['Computer vision', 'Image resolution', 'Inverse problems', 'Computational modeling', 'Perturbation methods', 'Gaussian noise', 'Gaussian distribution']","['Image Features', 'Gaussian Mixture Model', 'Imperceptible', 'Inception Distance', 'Fréchet Inception Distance', 'Training Set', 'High-dimensional', 'Akaike Information Criterion', 'Computational Efficiency', 'Distribution Characteristics', 'Expectation Maximization', 'Super-resolution', 'Generative Adversarial Networks', 'Joint Distribution', 'Second Moment', 'Additive Gaussian Noise', 'Discrete Space', 'Optimal Transport', 'Realistic Images', 'Moments Of Distribution', 'Moment Data', 'Higher-order Moments', 'Fast Gradient Sign Method', 'Translation Task', 'Infimum', 'Validation Set', 'Additive Noise', 'Average Pooling']","['Algorithms: Computational photography', 'image and video synthesis', 'Adversarial learning', 'adversarial attack and defense methods']",2,"We develop a measure for evaluating the performance of generative networks given two sets of images. A popular performance measure currently used to do this is the Fréchet Inception Distance (FID). FID assumes that images featurized using the penultimate layer of Inception-v3 follow a Gaussian distribution, an assumption which cannot be violated if we wish to use FID as a metric. However, we show that Inception-v3 features of the ImageNet dataset are not Gaussian; in particular, every single marginal is not Gaussian. To remedy this problem, we model the featurized images using Gaussian mixture models (GMMs) and compute the 2-Wasserstein distance restricted to GMMs. We define a performance measure, which we call WaM, on two sets of images by using Inception-v3 (or another classifier) to featurize the images, estimate two GMMs, and use the restricted 2-Wasserstein distance to compare the GMMs. We experimentally show the advantages of WaM over FID, including how FID is more sensitive than WaM to imperceptible image perturbations. By modelling the non-Gaussian features obtained from Inception-v3 as GMMs and using a GMM metric, we can more accurately evaluate generative network performance."
Event-Based RGB Sensing With Structured Light,"Seyed Ehsan Marjani Bajestani, Giovanni Beltrame",Polytechnique Montreal,100,Canada,0,,"Event-based cameras (ECs) are bio-inspired sensors that asynchronously report pixel brightness changes. Due to their high dynamic range, pixel bandwidth, temporal resolution, low power consumption, and computational simplicity, they are beneficial for vision-based projects in challenging lighting conditions and they can detect fast movements with their microsecond response time. The first generation of ECs are monochrome, but color data is very useful and sometimes essential for certain vision-based applications. The latest technology enables manufacturers to build color ECs, trading off the size of the sensor and substantially reducing the resolution compared to monochrome models, despite having the same bandwidth. In addition, ECs only detect changes in light and do not show static or slowly moving objects. We introduce a method to detect full RGB events using a monochrome EC aided by a structured light projector. The projector emits rapidly changing RGB patterns of light beams on the scene, the reflection of which is captured by the EC. We combine the benefits of ECs and projection-based techniques and allow depth and color detection of static or moving objects with a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, paving the way for frameless RGB-D sensing applications. Our code is available publicly: github.com/MISTLab/event_based_rgbd_ros",https://openaccess.thecvf.com/content/WACV2023/html/Bajestani_Event-Based_RGB_Sensing_With_Structured_Light_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bajestani_Event-Based_RGB_Sensing_With_Structured_Light_WACV_2023_paper.pdf,,github.com/MISTLab/event-based-rgbd-ros,,main,Poster,https://ieeexplore.ieee.org/document/10030829/,"['Visualization', 'Image color analysis', 'Wavelength measurement', 'Lighting', 'Bandwidth', 'Fluorescence', 'Cameras']","['Structured Illumination', 'Red-green-blue', 'Light Beam', 'High Dynamic Range', 'Sensor Size', 'Brightness Changes', 'Color Detection', 'Light Projection', 'Root Mean Square Error', 'Digital Camera', 'Point Cloud', 'Red Channel', 'Peak Signal-to-noise Ratio', 'Color Channels', 'Color Information', 'Objects In The Scene', 'Line Patterns', 'Number Of Dots', 'Loss Of Resolution', 'White Balance', 'Digital Light Processing', 'Simultaneous Localization And Mapping', 'Monochrome Camera', 'System Bandwidth', 'Event Stream', 'Color Correction', 'HSV Color', 'Color Wheel', 'Color Images', 'Convolutional Neural Network']","['Algorithms: Low-level and physics-based vision', '3D computer vision']",4,"Event-based cameras (ECs) are bio-inspired sensors that asynchronously report pixel brightness changes. Due to their high dynamic range, pixel bandwidth, temporal resolution, low power consumption, and computational simplicity, they are beneficial for vision-based projects in challenging lighting conditions and they can detect fast movements with their microsecond response time. The first generation of ECs are monochrome, but color data is very useful and sometimes essential for certain vision-based applications. The latest technology enables manufacturers to build color ECs, trading off the size of the sensor and substantially reducing the resolution compared to monochrome models, despite having the same bandwidth. In addition, ECs only detect changes in light and do not show static or slowly moving objects. We introduce a method to detect full RGB events using a monochrome EC aided by a structured light projector. The projector emits rapidly changing RGB patterns of light beams on the scene, the reflection of which is captured by the EC. We combine the benefits of ECs and projection-based techniques and allow depth and color detection of static or moving objects with a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, paving the way for frameless RGBD sensing applications. Our code is available publicly: github.com/MISTLab/event_based_rgbd_ros"
Event-Specific Audio-Visual Fusion Layers: A Simple and New Perspective on Video Understanding,"Arda Senocak, Junsik Kim, Tae-Hyun Oh, Dingzeyu Li, In So Kweon","Adobe Research; KAIST; Dept. of EE, POSTECH; Harvard University",75,"South Korea, USA",25,USA,"To understand our surrounding world, our brain is continuously inundated with multisensory information and their complex interactions coming from the outside world at any given moment. While processing this information might seem effortless for human brains, it is challenging to build a machine that can perform similar tasks since complex interactions cannot be dealt with a single type of integration but require more sophisticated approaches. In this paper, we propose a new simple method to address the multisensory integration in video understanding. Unlike previous works where a single fusion type is used, we design a multi-head model with individual event-specific layers to deal with different audio-visual relationships, enabling different ways of audio-visual fusion. Experimental results show that our event-specific layers can discover unique properties of the audio-visual relationships in the videos, e.g., semantically matched moments, and rhythmic events. Moreover, although our network is trained with single labels, our multi-head design can inherently output additional semantically meaningful multi-labels for a video. As an application, we demonstrate that our proposed method can expose the extent of event-characteristics of popular benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Senocak_Event-Specific_Audio-Visual_Fusion_Layers_A_Simple_and_New_Perspective_on_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Senocak_Event-Specific_Audio-Visual_Fusion_Layers_A_Simple_and_New_Perspective_on_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030378/,"['Computer vision', 'Benchmark testing', 'Multisensory integration', 'Floods', 'Task analysis']","['Fusion Layer', 'Video Understanding', 'Audio-visual Fusion', 'Complex Interactions', 'Individual Layers', 'Multisensory Integration', 'Single Label', 'Time Step', 'Scientific Knowledge', 'Visual Features', 'Visual Signals', 'Visual Effects', 'Video Analysis', 'Action Recognition', 'Backbone Network', 'Internal Combustion Engine', 'Characteristics Of Events', 'Real-world Effectiveness', 'Self-supervised Learning', 'Sound Localization', 'Video Events', 'Index Of Layer', 'Cross-modal Interactions', 'Frame Features', 'Multi-sensor Data', 'Video Features', 'Video Dataset', 'Part Of The Video', 'Target Dataset', 'Fusion Method']",['Algorithms: Vision + language and/or other modalities'],5,"To understand our surrounding world, our brain is continuously inundated with multisensory information and their complex interactions coming from the outside world at any given moment. While processing this information might seem effortless for human brains, it is challenging to build a machine that can perform similar tasks since complex interactions cannot be dealt with a single type of integration but require more sophisticated approaches. In this paper, we propose a new simple method to address the multisensory integration in video understanding. Unlike previous works where a single fusion type is used, we design a multi-head model with individual event-specific layers to deal with different audio-visual relationships, enabling different ways of audio-visual fusion. Experimental results show that our event-specific layers can discover unique properties of the audio-visual relationships in the videos, e.g., semantically matched moments, and rhythmic events. Moreover, although our network is trained with single labels, our multi-head design can inherently output additional semantically meaningful multi-labels for a video. As an application, we demonstrate that our proposed method can expose the extent of event-characteristics of popular benchmark datasets."
EventPoint: Self-Supervised Interest Point Detection and Description for Event-Based Camera,"Ze Huang, Li Sun, Cheng Zhao, Song Li, Songzhi Su",Bosch Research North America; University of Sheffield; Xiamen University,66.66666667,"China, UK",33.33333333,USA,"This paper proposes a self-supervised learned local detector and descriptor, called EventPoint, for event stream/camera tracking and registration. Event-based cameras have grown in popularity because of their biological inspiration and low power consumption. Despite this, applying local features directly to the event stream is difficult due to its peculiar data structure. We propose a new time-surface-like event stream representation method called Tencode. The event stream data processed by Tencode can obtain the pixel-level positioning of interest points while also simultaneously extracting descriptors through a neural network. Instead of using costly and unreliable manual annotation, our network leverages the prior knowledge of local feature extraction on color images and conducts self-supervised learning via homographic and spatio-temporal adaptation. To the best of our knowledge, our proposed method is the first research on event-based local features learning using a deep neural network. We provide comprehensive experiments of feature point detection and matching, and three public datasets are used for evaluation (i.e. DSEC, N-Caltech101, and HVGA ATIS Corner Dataset). The experimental findings demonstrate that our method outperforms SOTA in terms of feature point detection and description.",https://openaccess.thecvf.com/content/WACV2023/html/Huang_EventPoint_Self-Supervised_Interest_Point_Detection_and_Description_for_Event-Based_Camera_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Huang_EventPoint_Self-Supervised_Interest_Point_Detection_and_Description_for_Event-Based_Camera_WACV_2023_paper.pdf,,,2109.0021,main,Poster,https://ieeexplore.ieee.org/document/10030887/,"['Knowledge engineering', 'Representation learning', 'Power demand', 'Annotations', 'Neural networks', 'Detectors', 'Self-supervised learning']","['Feature Point Detection', 'Event-based Cameras', 'Neural Network', 'Deep Neural Network', 'Local Features', 'Descriptive Characteristics', 'Representative Methods', 'Feature Points', 'Low Power Consumption', 'Feature Matching', 'Self-supervised Learning', 'Local Descriptors', 'Local Feature Extraction', 'Event Stream', 'Imaging Data', 'Feature Maps', 'Surgical Margins', 'Distance Threshold', 'Temporal Window', 'Focal Loss', 'Dynamic Vision Sensor', 'Number Of Noise', 'Disparity Map', 'Reprojection Error', 'High Dynamic Range', 'Correct Matches', 'Input Of Neural Network', 'Event Frames', 'Detection Head', 'Detection Training']","['Algorithms: Low-level and physics-based vision', 'Robotics']",5,"This paper proposes a self-supervised learned local detector and descriptor, called EventPoint, for event stream/camera tracking and registration. Event-based cameras have grown in popularity because of their biological inspiration and low power consumption. Despite this, applying local features directly to the event stream is difficult due to its peculiar data structure. We propose a new time-surface-like event stream representation method called Ten-code. The event stream data processed by Tencode can obtain the pixel-level positioning of interest points while also simultaneously extracting descriptors through a neural network. Instead of using costly and unreliable manual annotation, our network leverages the prior knowledge of local feature extraction on color images and conducts self-supervised learning via homographic and spatio-temporal adaptation. To the best of our knowledge, our proposed method is the first research on event-based local features learning using a deep neural network. We provide comprehensive experiments of feature point detection and matching, and three public datasets are used for evaluation (i.e. DSEC, N-Caltech101, and HVGA ATIS Corner Dataset). The experimental findings demonstrate that our method outperforms SOTA in terms of feature point detection and description."
Exemplar Guided Deep Neural Network for Spatial Transcriptomics Analysis of Gene Expression Prediction,"Yan Yang, Md Zakir Hossain, Eric A. Stone, Shafin Rahman","BDSI, Australian National University, Australia; ECE, North South University, Bangladesh",100,"Australia, Bangladesh",0,,"Spatial transcriptomics (ST) is essential for understanding diseases and developing novel treatments. It measures gene expression of each fine-grained area (i.e., different windows) in the tissue slide with low throughput. This paper proposes an Exemplar Guided Network (EGN) to accurately and efficiently predict gene expression directly from each window of a tissue slide image. We apply exemplar learning to dynamically boost gene expression prediction from nearest/similar exemplars of a given tissue slide image window. Our EGN framework composes of three main components: 1) an extractor to structure a representation space for unsupervised exemplar retrievals; 2) a vision transformer (ViT) backbone to progressively extract representations of the input window; and 3) an Exemplar Bridging (EB) block to adaptively revise the intermediate ViT representations by using the nearest exemplars. Finally, we complete the gene expression prediction task with a simple attention-based prediction block. Experiments on standard benchmark datasets indicate the superiority of our approach when comparing with the past state-of-the-art (SOTA) methods.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Exemplar_Guided_Deep_Neural_Network_for_Spatial_Transcriptomics_Analysis_of_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Exemplar_Guided_Deep_Neural_Network_for_Spatial_Transcriptomics_Analysis_of_WACV_2023_paper.pdf,,,2210.16721,main,Poster,,,,,,
Expansion of Visual Hints for Improved Generalization in Stereo Matching,"Andrea Pilzer, Yuxin Hou, Niki Loppi, Arno Solin, Juho Kannala","Aalto University; Aalto University, Niantic; NVIDIA",66.66666667,"Finland, USA",33.33333333,USA,"We introduce visual hints expansion for guiding stereo matching to improve generalization. Our work is motivated by the robustness of Visual Inertial Odometry (VIO) in computer vision and robotics, where a sparse and unevenly distributed set of feature points characterizes a scene. To improve stereo matching, we propose to elevate 2D hints to 3D points. These sparse and unevenly distributed 3D visual hints are expanded using a 3D random geometric graph, which enhances the learning and inference process. We evaluate our proposal on multiple widely adopted benchmarks and show improved performance without access to additional sensors other than the image sequence. To highlight practical applicability and symbiosis with visual odometry, we demonstrate how our methods run on embedded hardware.",https://openaccess.thecvf.com/content/WACV2023/html/Pilzer_Expansion_of_Visual_Hints_for_Improved_Generalization_in_Stereo_Matching_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pilzer_Expansion_of_Visual_Hints_for_Improved_Generalization_in_Stereo_Matching_WACV_2023_paper.pdf,,,2211.00392,main,Poster,https://ieeexplore.ieee.org/document/10030205/,"['Symbiosis', 'Visualization', 'Solid modeling', 'Three-dimensional displays', 'Laser radar', 'Robustness', 'Sensors']","['Stereo Matching', 'Visual Hints', 'Computer Vision', 'Feature Points', '3D Graph', 'Visual Odometry', 'Visual-inertial Odometry', 'Prediction Accuracy', 'Error Rate', 'Convolutional Neural Network', 'Linear Approximation', 'Deep Convolutional Neural Network', 'Domain Shift', 'Sparse Matrix', 'Convex Hull', 'Simultaneous Localization And Mapping', 'RGB Values', 'Linear Expansion', 'Stereo Images', 'Sparse Point', 'Cost Volume', 'Visual Guidance', 'Patch Matching', '2D Image Plane', 'Disparity Values', '3D World', 'Close Points', 'Bottom Half', 'Stereo Image Pairs', 'Higher Density']",['Algorithms: 3D computer vision'],2,"We introduce visual hints expansion for guiding stereo matching to improve generalization. Our work is motivated by the robustness of Visual Inertial Odometry (VIO) in computer vision and robotics, where a sparse and unevenly distributed set of feature points characterizes a scene. To improve stereo matching, we propose to elevate 2D hints to 3D points. These sparse and unevenly distributed 3D visual hints are expanded using a 3D random geometric graph, which enhances the learning and inference process. We evaluate our proposal on multiple widely adopted benchmarks and show improved performance without access to additional sensors other than the image sequence. To highlight practical applicability and symbiosis with visual odometry, we demonstrate how our methods run on embedded hardware."
Expert-Defined Keywords Improve Interpretability of Retinal Image Captioning,"Ting-Wei Wu, Jia-Hong Huang, Joseph Lin, Marcel Worring","University of Amsterdam; Georgia Institute of Technology; University of California, Los Angeles / MediaTek Inc.",100,"Netherlands, USA",0,,"Automatic machine learning-based (ML-based) medical report generation systems for retinal images suffer from a relative lack of interpretability. Hence, such ML-based systems are still not widely accepted. The main reason is that trust is one of the important motivating aspects of interpretability and humans do not trust blindly. Precise technical definitions of interpretability still lack consensus. Hence, it is difficult to make a human-comprehensible ML-based medical report generation system. Heat maps/saliency maps, i.e., post-hoc explanation approaches, are widely used to improve the interpretability of ML-based medical systems. However, they are well known to be problematic. From an ML-based medical model's perspective, the highlighted areas of an image are considered important for making a prediction. However, from a doctor's perspective, even the hottest regions of a heat map contain both useful and non-useful information. Simply localizing the region, therefore, does not reveal exactly what it was in that area that the model considered useful. Hence, the post-hoc explanation-based method relies on humans who probably have a biased nature to decide what a given heat map might mean. Interpretability boosters, in particular expert-defined keywords, are effective carriers of expert domain knowledge and they are human-comprehensible. In this work, we propose to exploit such keywords and a specialized attention-based strategy to build a more human-comprehensible medical report generation system for retinal images. Both keywords and the proposed strategy effectively improve the interpretability. The proposed method achieves state-of-the-art performance under commonly used text evaluation metrics BLEU, ROUGE, CIDEr, and METEOR. Project website: https://github.com/Jhhuangkay/Expert-defined-Keywords-Improve-Interpretability-of-Retinal-Image-Captioning.",https://openaccess.thecvf.com/content/WACV2023/html/Wu_Expert-Defined_Keywords_Improve_Interpretability_of_Retinal_Image_Captioning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wu_Expert-Defined_Keywords_Improve_Interpretability_of_Retinal_Image_Captioning_WACV_2023_paper.pdf,,https://github.com/Jhhuangkay/Expert-defined-Keywords-Improve-Interpretability-of-Retinal-Image-Captioning,,main,Poster,https://ieeexplore.ieee.org/document/10030533/,"['Heating systems', 'Measurement', 'Computer vision', 'Medical services', 'Predictive models', 'Retina', 'Biomedical imaging']","['Retinal Images', 'Image Captioning', 'Heatmap', 'Medical Systems', 'Medical Model', 'Expert Domain Knowledge', 'Text Evaluation', 'Convolutional Neural Network', 'Image Features', 'Ophthalmology', 'Visual Features', 'Long Short-term Memory', 'Recurrent Neural Network', 'Image Regions', 'Attention Mechanism', 'Multilayer Perceptron', 'Natural Images', 'Retinal Diseases', 'Word Embedding', 'Semantic Knowledge', 'Transformer Decoder', 'Retina Specialists', 'Graph Convolutional Network', 'Class Activation Maps', 'Image Descriptors', 'Attention Weights', 'Visual Concepts', 'Attention Model', 'Image Patches', 'Fully-connected Layer']",['Applications: Biomedical/healthcare/medicine'],8,"Automatic machine learning-based (ML-based) medical report generation systems for retinal images suffer from a relative lack of interpretability. Hence, such ML-based systems are still not widely accepted. The main reason is that trust is one of the important motivating aspects of interpretability and humans do not trust blindly. Precise technical definitions of interpretability still lack consensus. Hence, it is difficult to make a human-comprehensible ML-based medical report generation system. Heat maps/saliency maps, i.e., post-hoc explanation approaches, are widely used to improve the interpretability of ML-based medical systems. However, they are well known to be problematic. From an ML-based medical model’s perspective, the highlighted areas of an image are considered important for making a prediction. However, from a doctor’s perspective, even the hottest regions of a heat map contain both useful and non-useful information. Simply localizing the region, therefore, does not reveal exactly what it was in that area that the model considered useful. Hence, the post-hoc explanation-based method relies on humans who probably have a biased nature to decide what a given heat map might mean. Interpretability boosters, in particular expert-defined keywords, are effective carriers of expert domain knowledge and they are human-comprehensible. In this work, we propose to exploit such keywords and a specialized attention-based strategy to build a more human-comprehensible medical report generation system for retinal images. Both keywords and the proposed strategy effectively improve the interpretability. The proposed method achieves state-of-the-art performance under commonly used text evaluation metrics BLEU, ROUGE, CIDEr, and METEOR. Project website: https://github.com/Jhhuangkay/Expert-defined-Keywords-Improve-Interpretability-of-Retinal-Image-Captioning."
Explainability-Aware One Point Attack for Point Cloud Neural Networks,"Hanxiao Tan, Helena Kotthaus","AI Group, TU Dortmund",100,Germany,0,,"Recent studies have shown an increased interest to investigate the reliability of point cloud networks by adversarial attacks. However, most of the existing studies aim to deceive humans, while few address the operation principles of the models themselves. In this work, we propose two adversarial methods: One Point Attack (OPA) and Critical Traversal Attack (CTA), which target the points crucial to predictions more precisely by incorporating explainability methods. Our results show that popular point cloud networks can be deceived with almost 100% success rate by shifting only one point from the input instance. We also show the interesting impact of different point attribution distributions on the adversarial robustness of point cloud networks. We discuss how our approaches facilitate the explainability study for point cloud networks. To the best of our knowledge, this is the first point-cloud-based adversarial approach concerning explainability. Our code is available at https://github.com/Explain3D/Exp-One-Point-Atk-PC.",https://openaccess.thecvf.com/content/WACV2023/html/Tan_Explainability-Aware_One_Point_Attack_for_Point_Cloud_Neural_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tan_Explainability-Aware_One_Point_Attack_for_Point_Cloud_Neural_Networks_WACV_2023_paper.pdf,,https://github.com/Explain3D/Exp-One-Point-Atk-PC,2110.04158,main,Poster,https://ieeexplore.ieee.org/document/10030275/,"['Point cloud compression', 'Computer vision', 'Codes', 'Filtering', 'Computer network reliability', 'Neural networks', 'Robustness']","['Neural Network', 'Point Cloud', 'High Success Rate', 'Robust Network', 'Adversarial Attacks', 'Input Instance', 'Deep Neural Network', 'Critical Point', 'Optimization Process', 'Attention In Recent Years', 'Counterfactual', 'Pooling Layer', 'Gini Coefficient', 'Lowest Activity', 'Decision Boundary', 'Point Cloud Data', 'Saliency Map', 'Attack Success', 'Hausdorff Distance', 'Adversarial Examples', 'Perturbation Magnitude', 'Attack Methods', 'Chamfer Distance', 'Model Explainability', 'Point Cloud Classification', 'Attack Performance']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', '3D computer vision', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",3,"Recent studies have shown an increased interest to investigate the reliability of point cloud networks by adversarial attacks. However, most of the existing studies aim to deceive humans, while few address the operation principles of the models themselves. In this work, we propose two adversarial methods: One Point Attack (OPA) and Critical Traversal Attack (CTA), which target the points crucial to predictions more precisely by incorporating explainability methods. Our results show that popular point cloud networks can be deceived with high success rate by shifting only one point from the input instance. We also show the interesting impact of different point attribution distributions on the adversarial robustness of point cloud networks. We discuss how our approaches facilitate the explainability study for point cloud networks. To the best of our knowledge, this is the first point-cloud-based adversarial approach concerning explainability. Our code is available at https://github.com/Explain3D/Exp-One-Point-Atk-PC."
Exploiting Instance-Based Mixed Sampling via Auxiliary Source Domain Supervision for Domain-Adaptive Action Detection,"Yifan Lu, Gurkirt Singh, Suman Saha, Luc Van Gool","ETH Zurich; ETH Zurich, KU Leuven",100,"Belgium, Switzerland",0,,"We propose a novel domain adaptive action detection approach and a new adaptation protocol that leverages the recent advancements in image-level unsupervised domain adaptation (UDA) techniques and handle vagaries of instance-level video data. Self-training combined with cross-domain mixed sampling has shown remarkable performance gain in semantic segmentation in UDA (unsupervised domain adaptation) context. Motivated by this fact, we propose an approach for human action detection in videos that transfers knowledge from the source domain (annotated dataset) to the target domain (unannotated dataset) using mixed sampling and pseudo-label-based selftraining. The existing UDA techniques follow a ClassMix algorithm for semantic segmentation. However, simply adopting ClassMix for action detection does not work, mainly because these are two entirely different problems, i.e., pixel-label classification vs. instance-label detection. To tackle this, we propose a novel action instance mixed sampling technique that combines information across domains based on action instances instead of action classes. Moreover, we propose a new UDA training protocol that addresses the long-tail sample distribution and domain shift problem by using supervision from an auxiliary source domain (ASD). For the ASD, we propose a new action detection dataset with dense frame-level annotations. We name our proposed framework as domain-adaptive action instance mixing (DA-AIM). We demonstrate that DA-AIM consistently outperforms prior works on challenging domain adaptation benchmarks. The source code is available at https://github.com/wwwfan628/DA-AIM.",https://openaccess.thecvf.com/content/WACV2023/html/Lu_Exploiting_Instance-Based_Mixed_Sampling_via_Auxiliary_Source_Domain_Supervision_for_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lu_Exploiting_Instance-Based_Mixed_Sampling_via_Auxiliary_Source_Domain_Supervision_for_WACV_2023_paper.pdf,,https://github.com/wwwfan628/DA-AIM,2209.15439,main,Poster,https://ieeexplore.ieee.org/document/10030256/,"['Training', 'Computer vision', 'Protocols', 'Three-dimensional displays', 'Semantic segmentation', 'Source coding', 'Self-supervised learning']","['Mixed Samples', 'Source Domain', 'Auxiliary Domains', 'Action Detection', 'Semantic Segmentation', 'Target Domain', 'Domain Adaptation', 'Action Classes', 'Long-tailed Distribution', 'Action Instances', 'Challenging Benchmark', 'Detection In Videos', 'Domain Adaptation Techniques', 'Domain Shift Problem', 'Primary Source', 'Knowledge Transfer', 'Object Detection', 'Stochastic Gradient Descent', 'Bounding Box', 'Generative Adversarial Networks', 'Underrepresented Classes', 'In-house Dataset', 'Video Clips', 'Primary Domains', 'Action Recognition', 'Ground Truth Annotations', 'Self-supervised Learning', 'Adversarial Training', 'Average Precision', 'Ground Truth Labels']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",1,"We propose a novel domain adaptive action detection approach and a new adaptation protocol that leverages the recent advancements in image-level unsupervised domain adaptation (UDA) techniques and handle vagaries of instance-level video data. Self-training combined with cross-domain mixed sampling has shown remarkable performance gain in semantic segmentation in UDA (unsupervised domain adaptation) context. Motivated by this fact, we propose an approach for human action detection in videos that transfers knowledge from the source domain (annotated dataset) to the target domain (unannotated dataset) using mixed sampling and pseudo-label-based self-training. The existing UDA techniques follow a Class-Mix algorithm for semantic segmentation. However, simply adopting ClassMix for action detection does not work, mainly because these are two entirely different problems, i.e., pixel-label classification vs. instance-label detection. To tackle this, we propose a novel action instance mixed sampling technique that combines information across domains based on action instances instead of action classes. Moreover, we propose a new UDA training protocol that addresses the long-tail sample distribution and domain shift problem by using supervision from an auxiliary source domain (ASD). For the ASD, we propose a new action detection dataset with dense frame-level annotations. We name our proposed framework as domain-adaptive action instance mixing (DA-AIM). We demonstrate that DA-AIM consistently outperforms prior works on challenging domain adaptation benchmarks. The source code is available at https://github.com/wwwfan628/DA-AIM."
Exploiting Long-Term Dependencies for Generating Dynamic Scene Graphs,"Shengyu Feng, Hesham Mostafa, Marcel Nassar, Somdeb Majumdar, Subarna Tripathi",Intel Labs; Carnegie Mellon University,50,USA,50,USA,"Dynamic scene graph generation from a video is challenging due to the temporal dynamics of the scene and the inherent temporal fluctuations of predictions. We hypothesize that capturing long-term temporal dependencies is the key to effective generation of dynamic scene graphs. We propose to learn the long-term dependencies in a video by capturing the object-level consistency and inter-object relationship dynamics over object-level long-term tracklets using transformers. Experimental results demonstrate that our ""Dynamic Scene Graph Detection Transformer"" (DSG-DETR) outperforms state-of-the-art methods by a significant margin on the benchmark dataset Action Genome. Our ablation studies validate the effectiveness of each component of the proposed approach. The source code is available at https://github.com/Shengyu-Feng/DSG-DETR.",https://openaccess.thecvf.com/content/WACV2023/html/Feng_Exploiting_Long-Term_Dependencies_for_Generating_Dynamic_Scene_Graphs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Feng_Exploiting_Long-Term_Dependencies_for_Generating_Dynamic_Scene_Graphs_WACV_2023_paper.pdf,,https://github.com/Shengyu-Feng/DSG-DETR,2112.09828,main,Poster,https://ieeexplore.ieee.org/document/10030513/,"['Computer vision', 'Fluctuations', 'Source coding', 'Genomics', 'Benchmark testing', 'Transformers', 'Bioinformatics']","['Dynamic Graph', 'Scene Graph', 'Temporal Dependencies', 'Object Detection', 'Visual Features', 'Bounding Box', 'Class Distribution', 'Object Classification', 'Video Frames', 'Correct Predictions', 'Majority Voting', 'Temporal Context', 'Subject And Object', 'Faster R-CNN', 'Hypothetical Case', 'Object Pairs', 'Temporal Consistency', 'Original Video', 'Positional Encoding', 'Spatial Encoding', 'Key Frames', 'Transformer Encoder', 'Semantic Embedding', 'L1 Loss', 'Object Tracking', 'Temporal Modulation', 'Intersection Over Union', 'Static Images']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,Dynamic scene graph generation from a video is challenging due to the temporal dynamics of the scene and the inherent temporal fluctuations of predictions. We hypothesize that capturing long-term temporal dependencies is the key to effective generation of dynamic scene graphs. We propose to learn the long-term dependencies in a video by capturing the object-level consistency and inter-object relationship dynamics over object-level long-term tracklets using transformers. Experimental results demonstrate that our Dynamic Scene Graph Detection Transformer (DSG- DETR) outperforms state-of-the-art methods by a significant margin on the benchmark dataset Action Genome. Our ablation studies validate the effectiveness of each component of the proposed approach. The source code is available at https://github.com/Shengyu-Feng/DS G-DETR.
Exploiting Visual Context Semantics for Sound Source Localization,"Xinchi Zhou, Dongzhan Zhou, Di Hu, Hang Zhou, Wanli Ouyang","Gaoling School of Artificial Intelligence, Renmin University of China; Baidu Inc.; The University of Sydney",66.66666667,"Australia, China",33.33333333,China,"Self-supervised sound source localization in unconstrained visual scenes is an important task of audio-visual learning. In this paper, we propose a visual reasoning module to explicitly exploit the rich visual context semantics, which alleviates the issue of insufficient utilization of visual information in previous works. The learning objectives are carefully designed to provide stronger supervision signals for the extracted visual semantics while enhancing the audio-visual interactions, which lead to more robust feature representations. Extensive experimental results demonstrate that our approach significantly boosts the localization performances on various datasets, even without initializations pretrained on ImageNet. Moreover, with the visual context exploitation, our framework can accomplish both the audio-visual and purely visual inference, which expands the application scope of the sound source localization task and further raises the competitiveness of our approach.",https://openaccess.thecvf.com/content/WACV2023/html/Zhou_Exploiting_Visual_Context_Semantics_for_Sound_Source_Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhou_Exploiting_Visual_Context_Semantics_for_Sound_Source_Localization_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030749/,"['Location awareness', 'Training', 'Visualization', 'Computer vision', 'Semantics', 'Feature extraction', 'Cognition']","['Sound Localization', 'Visual Context', 'Feature Representation', 'Localization Performance', 'Learning Objectives', 'Visual Modality', 'Visual Scene', 'Supervision Signal', 'Visual Reasoning', 'Robust Feature Representation', 'Visual Features', 'Positive Scores', 'Large-scale Datasets', 'Bounding Box', 'Video Clips', 'Object Location', 'Baseline Methods', 'Vanilla', 'Area Under Curve', 'Negative Region', 'Audio Cues', 'Visual Domain', 'Multimodal Learning', 'Global Max Pooling', 'Positive Subset', 'Visual Localization']",['Algorithms: Vision + language and/or other modalities'],6,"Self-supervised sound source localization in unconstrained visual scenes is an important task of audio-visual learning. In this paper, we propose a visual reasoning module to explicitly exploit the rich visual context semantics, which alleviates the issue of insufficient utilization of visual information in previous works. The learning objectives are carefully designed to provide stronger supervision signals for the extracted visual semantics while enhancing the audio-visual interactions, which lead to more robust feature representations. Extensive experimental results demonstrate that our approach significantly boosts the localization performances on various datasets, even without initializations pretrained on ImageNet. Moreover, with the visual context exploitation, our framework can accomplish both the audio-visual and purely visual inference, which expands the application scope of the sound source localization task and further raises the competitiveness of our approach."
FAN-Trans: Online Knowledge Distillation for Facial Action Unit Detection,"Jing Yang, Jie Shen, Yiming Lin, Yordan Hristov, Maja Pantic","; Imperial College London, UK; University of Nottingham, UK",100,UK,0,,"Due to its importance in facial behaviour analysis, facial action unit (AU) detection has attracted increasing attention from the research community. Leveraging the online knowledge distillation framework, we propose the ""FAN-Trans"" method for AU detection. Our model consists of a hybrid network of convolution layers and transformer blocks designed to learn per-AU features and to model AU co-occurrences. The model uses a pre-trained face alignment network as the feature extractor. After further transformation by a small learnable add-on convolutional subnet, the per-AU features are fed into transformer blocks to enhance their representation. As multiple AUs often appear together, we propose a learnable attention drop mechanism in the transformer block to learn the correlation between the features for different AUs. We also design a classifier that predicts AU presence by considering all AUs' features, to explicitly capture label dependencies. Finally, we make the first attempt of adapting online knowledge distillation in the training stage for this task, further improving the model's performance. Experiments on the BP4D and DISFA datasets show our method has achieved a new state-of-the-art performance on both, demonstrating its effectiveness.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_FAN-Trans_Online_Knowledge_Distillation_for_Facial_Action_Unit_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_FAN-Trans_Online_Knowledge_Distillation_for_Facial_Action_Unit_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030455/,"['Training', 'Gold', 'Computer vision', 'Correlation', 'Convolution', 'Computational modeling', 'Transformers']","['Action Units', 'Facial Action Units', 'Action Unit Detection', 'Facial Action Unit Detection', 'Online Knowledge Distillation', 'Transformer Block', 'Face Alignment', 'Classification Task', 'Feature Maps', 'Feature Learning', 'Multi-label', 'Receptive Field', 'Correlated Features', 'Human Faces', 'Intermediate Features', 'Attention Map', 'Convolution Module', 'Human Pose Estimation', 'Average F1-score', 'Transformation Module', 'Facial Action Coding System', 'SOTA Methods', 'Self-attention Module']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",5,"Due to its importance in facial behaviour analysis, facial action unit (AU) detection has attracted increasing attention from the research community. Leveraging the online knowledge distillation framework, we propose the ""FAN-Trans"" method for AU detection. Our model consists of a hybrid network of convolution and transformer blocks to learn per-AU features and to model AU co-occurrences. The model uses a pre-trained face alignment network as the feature extractor. After further transformation by a small learnable add-on convolutional subnet, the per-AU features are fed into transformer blocks to enhance their representation. As multiple AUs often appear together, we propose a learnable attention drop mechanism in the transformer block to learn the correlation between the features for different AUs. We also design a classifier that predicts AU presence by considering all AUs’ features, to explicitly capture label dependencies. Finally, we make the attempt of adapting online knowledge distillation in the training stage for this task, further improving the model’s performance. Experiments on the BP4D and DISFA datasets demonstrating the effectiveness of proposed method."
FFM: Injecting Out-of-Domain Knowledge via Factorized Frequency Modification,"Zijian Wang, Yadan Luo, Zi Huang, Mahsa Baktashmotlagh",The University of Queensland,100,Australia,0,,"This work addresses the Single Domain Generalization (SDG) problem, and aims to generalize a model from a single source (i.e., training) domain to multiple target (i.e., test) domains with different distributions. Most of the existing SDG approaches aim at generating out-of-domain samples by either transforming the source images into different styles or optimizing adversarial noise perturbations. In this paper, we show that generating images with diverse styles can be complementary to creating hard samples when tackling the SDG task. This inspires us to propose our approach of Factorized Frequency Modification (FFM) which can fulfill the requirement of generating diverse and hard samples to tackle the problem of out-of-domain generalization. Specifically, we design a unified framework consisting of a style transformation module, an adversarial perturbation module, and a dynamic frequency selection module. We seamlessly equip the framework with iterative adversarial training which facilitates the task model to learn discriminative features from hard and diverse augmented samples. We perform extensive experiments on four image recognition benchmark datasets of Digits-DG, CIFAR-10-C, CIFAR-100-C, and PACS, which demonstrates that our method outperforms existing state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_FFM_Injecting_Out-of-Domain_Knowledge_via_Factorized_Frequency_Modification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_FFM_Injecting_Out-of-Domain_Knowledge_via_Factorized_Frequency_Modification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030383/,"['Training', 'Computer vision', 'Image recognition', 'Perturbation methods', 'Frequency-domain analysis', 'Benchmark testing', 'Picture archiving and communication systems']","['Diverse Sample', 'Benchmark Datasets', 'Source Images', 'Target Domain', 'Frequency Selectivity', 'Source Domain', 'Domain Generalization', 'Adversarial Training', 'Dynamic Selection', 'Adversarial Perturbations', 'Noise Perturbation', 'Augmented Samples', 'Weather', 'Training Set', 'Uniform Distribution', 'Corruption', 'Hardness', 'Convolutional Layers', 'Frequency Spectrum', 'Frequency Components', 'Unseen Domains', 'Backbone Network', 'Domain Shift', 'Low-frequency Components', 'High Frequency Components', 'Image X', 'Fourier Domain', 'Image Augmentation', 'Augmentation Strategy', 'Latent Space']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",1,"This work investigates the Single Domain Generalization (SDG) problem and aims to generalize a model from a single source (i.e., training) domain to multiple target (i.e., test) domains coming from different distributions. Most of the existing SDG approaches focus on generating out-of-domain samples by either transforming the source images into different styles or optimizing adversarial noise perturbations applied on the source images. In this paper, we show that generating images with diverse styles can be complementary to creating hard samples when handling the SDG task, and propose our approach of Factorized Frequency Modification (FFM) to fulfill this requirement. Specifically, we design a unified framework consisting of a style transformation module, an adversarial perturbation module, and a dynamic frequency selection module. We seamlessly equip the framework with iterative adversarial training that facilitates learning discriminative features from hard and diverse augmented samples. Extensive experiments are performed on four image recognition benchmark datasets of Digits, CIFAR-10-C, CIFAR-100-C, and PACS, which demonstrates that our method outperforms existing state-of-the-art approaches."
FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation,"Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, Du Tran",CMU; Meta AI; UCSD,66.66666667,USA,33.33333333,USA,"Most modern frame interpolation approaches rely on explicit bidirectional optical flows between adjacent frames, thus are sensitive to the accuracy of underlying flow estimation in handling occlusions while additionally introducing computational bottlenecks unsuitable for efficient deployment. In this work, we propose a flow-free approach that is completely end-to-end trainable for multi-frame video interpolation. Our method, FLAVR, is designed to reason about non-linear motion trajectories and complex occlusions implicitly from unlabeled videos and greatly simplifies the process of training, testing and deploying frame interpolation models. Furthermore, FLAVR delivers up to 6x speed up compared to the current state-of-the-art methods for multi-frame interpolation while consistently demonstrating superior qualitative and quantitative results compared with prior methods on popular benchmarks including Vimeo-90K, Adobe-240FPS, and GoPro. Finally, we show that frame interpolation is a competitive self-supervised pre-training task for videos via demonstrating various novel applications of FLAVR including action recognition, optical flow estimation, motion magnification, and video object tracking. Code and trained models will be publicly released.",https://openaccess.thecvf.com/content/WACV2023/html/Kalluri_FLAVR_Flow-Agnostic_Video_Representations_for_Fast_Frame_Interpolation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kalluri_FLAVR_Flow-Agnostic_Video_Representations_for_Fast_Frame_Interpolation_WACV_2023_paper.pdf,https://tarun005.github.io/FLAVR/,https://github.com/tarun005/FLAVR,2012.08512,main,Poster,https://ieeexplore.ieee.org/document/10030268/,"['Training', 'Interpolation', 'Computer vision', 'Solid modeling', 'Three-dimensional displays', 'Computational modeling', 'Estimation']","['Frame Interpolation', 'Qualitative Results', 'Action Recognition', 'Optical Flow', 'Flow Estimation', 'Prior Methods', 'Motor Properties', 'Optical Flow Estimation', 'Temporal Dimension', 'Learning Objectives', 'Representation Learning', 'Video Frames', 'Inference Time', 'Simple Architecture', 'Self-supervised Learning', 'Motion Trajectory', 'Inference Speed', '3D Convolution', 'Prior Approaches', 'Flow Map', 'Flow-based Methods', 'Intermediate Frames', '3D Convolutional Layers', 'Input Frames', 'Raw Video', 'Label Propagation', 'Interpolation Accuracy', 'Group Convolution', 'Original Video', 'Endpoint Error']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Computational photography', 'image and video synthesis']",50,"Most modern frame interpolation approaches rely on explicit bidirectional optical flows between adjacent frames, thus are sensitive to the accuracy of underlying flow estimation in handling occlusions while additionally introducing computational bottlenecks unsuitable for efficient deployment. In this work, we propose a flow-free approach that is completely end-to-end trainable for multi-frame video interpolation. Our method, FLAVR, leverages 3D spatio-temporal kernels to directly learn motion properties from unlabeled videos and greatly simplifies the process of training, testing and deploying frame interpolation models. As a result, FLAVR delivers up to 6× speed up compared to the current state-of-the-art methods for multi-frame interpolation while consistently demonstrating superior qualitative and quantitative results compared with prior methods on popular benchmarks including Vimeo-90K, Adobe-240FPS, and GoPro. Finally, we show that frame interpolation is a competitive self-supervised pre-training task for videos via demonstrating various novel applications of FLAVR including action recognition, optical flow estimation, and video object tracking. Code and trained models are provided in the supplementary material."
FLOAT: Fast Learnable Once-for-All Adversarial Training for Tunable Trade-Off Between Accuracy and Robustness,"Souvik Kundu, Sairam Sundaresan, Massoud Pedram, Peter A. Beerel","Intel Labs, USA; Intel Labs, USA and University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA",66.66666667,USA,33.33333333,USA,"Existing models that achieve state-of-the-art (SOTA) performance on both clean and adversarially-perturbed images rely on convolution operations conditioned with feature-wise linear modulation (FiLM) layers. These layers require additional parameters and are hyperparameter sensitive. They significantly increase training time, memory cost, and potential latency which can be costly for resource-limited or real-time applications. In this paper, we present a fast learnable once-for-all adversarial training (FLOAT) algorithm, which instead of the existing FiLM-based conditioning, presents a unique weight conditioned learning that requires no additional layer, thereby incurring no significant increase in parameter count, training time, or network latency compared to standard adversarial training. In particular, we add configurable scaled noise to the weight tensors that enables a trade-off between clean and adversarial performance. Extensive experiments show that FLOAT can yield SOTA performance improving both clean and perturbed image classification by up to  6% and  10%, respectively. Moreover, real hardware measurement shows that FLOAT can reduce the training time by up to 1.43x with fewer model parameters of up to 1.47x on iso-hyperparameter settings compared to the FiLM-based alternatives. Additionally, to further improve memory efficiency we introduce FLOAT sparse (FLOATS), a form of non-iterative model pruning, and provide detailed empirical analysis in yielding a three-way accuracy-robustness-complexity trade-off for these new class of pruned conditionally trained models.",https://openaccess.thecvf.com/content/WACV2023/html/Kundu_FLOAT_Fast_Learnable_Once-for-All_Adversarial_Training_for_Tunable_Trade-Off_Between_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kundu_FLOAT_Fast_Learnable_Once-for-All_Adversarial_Training_for_Tunable_Trade-Off_Between_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
FUSSL: Fuzzy Uncertain Self Supervised Learning,"Salman Mohamadi, Gianfranco Doretto, Donald A. Adjeroh","West Virginia University, Morgantown, WV, USA",100,USA,0,,"Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. Similar to some other disciplines in deep representation learning, one main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regard- less of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. We performed extensive experiments under multiple settings to show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles.",https://openaccess.thecvf.com/content/WACV2023/html/Mohamadi_FUSSL_Fuzzy_Uncertain_Self_Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mohamadi_FUSSL_Fuzzy_Uncertain_Self_Supervised_Learning_WACV_2023_paper.pdf,,,2210.15818,main,Poster,https://ieeexplore.ieee.org/document/10030441/,"['Training', 'Representation learning', 'Protocols', 'Uncertainty', 'Annotations', 'Redundancy', 'Self-supervised learning']","['Supervised Learning', 'Self-supervised Learning', 'Deep Learning', 'Training Phase', 'Feature Learning', 'Data Augmentation', 'Representation Learning', 'Training Protocol', 'Multiple Scenarios', 'Pseudo Labels', 'Supervision Signal', 'Redundancy Reduction', 'Information Bottleneck', 'Loss Function', 'Objective Function', 'Transfer Learning', 'Projector', 'Latent Space', 'Clustering Approach', 'Cluster Level', 'Supervisory Signal', 'Soft Labels', 'Pretext Task', 'Ensemble Size', 'Triplet Loss', 'Invariant Representation', 'Learning Phase', 'Backbone Architecture', 'Evaluation Phase', 'Contrastive Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. Similar to some other disciplines in deep representation learning, one main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognise the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regardless of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. We performed extensive experiments under multiple settings to show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles."
FaceDancer: Pose- and Occlusion-Aware High Fidelity Face Swapping,"Felix Rosberg, Eren Erdal Aksoy, Fernando Alonso-Fernandez, Cristofer Englund","Berge Consulting, Gothenburg, Sweden; Halmstad University, Halmstad, Sweden",50,Sweden,50,Sweden,"In this work, we present a new single-stage method for subject agnostic face swapping and identity transfer, named FaceDancer. We have two major contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature Similarity Regularization (IFSR). The AFFA module is embedded in the decoder and adaptively learns to fuse attribute features and features conditioned on identity information without requiring any additional facial segmentation process. In IFSR, we leverage the intermediate features in an identity encoder to preserve important attributes such as head pose, facial expression, lighting, and occlusion in the target face, while still transferring the identity of the source face with high fidelity. We conduct extensive quantitative and qualitative experiments on various datasets and show that the proposed FaceDancer outperforms other state-of-the-art networks in terms of identity transfer, while having significantly better pose preservation than most of the previous methods.",https://openaccess.thecvf.com/content/WACV2023/html/Rosberg_FaceDancer_Pose-_and_Occlusion-Aware_High_Fidelity_Face_Swapping_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rosberg_FaceDancer_Pose-_and_Occlusion-Aware_High_Fidelity_Face_Swapping_WACV_2023_paper.pdf,,https://github.com/felixrosberg/FaceDance,,main,Poster,https://ieeexplore.ieee.org/document/10030244/,"['Adaptation models', 'Visualization', 'Computer vision', 'Image coding', 'Shape', 'Fuses', 'Computational modeling']","['Deepfake', 'Illumination', 'Facial Expressions', 'Identity Information', 'Intermediate Features', 'Target Face', 'Head Pose', 'Feature Maps', 'Generative Adversarial Networks', 'Target Image', 'Face Images', 'Residual Block', 'Skip Connections', 'Ideal Performance', 'Loss Of Identity', 'Gaze Direction', 'Reconstruction Loss', 'Perceptual Loss', 'Final Block', 'Facial Shape', 'Fréchet Inception Distance', 'Pose Error', 'Equal Error Rate', 'Hinge Loss', 'Fused Feature Map', 'Intermediate Feature Maps', 'ResNet-50 Backbone', 'Attention Map', 'Loss Function', 'Deep Features']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",14,"In this work, we present a new single-stage method for subject agnostic face swapping and identity transfer, named FaceDancer. We have two major contributions: Adaptive Feature Fusion Attention (AFFA) and Interpreted Feature Similarity Regularization (IFSR). The AFFA module is embedded in the decoder and adaptively learns to fuse attribute features and features conditioned on identity information without requiring any additional facial segmentation process. In IFSR, we leverage the intermediate features in an identity encoder to preserve important attributes such as head pose, facial expression, lighting, and occlusion in the target face, while still transferring the identity of the source face with high fidelity. We conduct extensive quantitative and qualitative experiments on various datasets and show that the proposed FaceDancer outperforms other state-of-the-art networks in terms of identity transfer, while having significantly better pose preservation than most of the previous methods. Code available at https://github.com/felixrosberg/FaceDance."
FaceOff: A Video-to-Video Face Swapping System,"Aditya Agarwal, Bipasha Sen, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C. V. Jawahar",IIIT Hyderabad; University of Bath,100,"India, UK",0,,"Doubles play an indispensable role in the movie industry. They take the place of the actors in dangerous stunt scenes or scenes where the same actor plays multiple characters. The double's face is later replaced with the actor's face and expressions manually using expensive CGI technology, costing millions of dollars and taking months to complete. An automated, inexpensive, and fast way can be to use face-swapping techniques that aim to swap an identity from a source face video (or an image) to a target face video. However, such methods cannot preserve the source expressions of the actor important for the scene's context. To tackle this challenge, we introduce video-to-video (V2V) face-swapping, a novel task of face-swapping that can preserve (1) the identity and expressions of the source (actor) face video and (2) the background and pose of the target (double) video. We propose FaceOff, a V2V face-swapping system that operates by learning a robust blending operation to merge two face videos following the constraints above. It reduces the videos to a quantized latent space and then blends them in the reduced space. FaceOff is trained in a self-supervised manner and robustly tackles the non-trivial challenges of V2V face-swapping. As shown in the experimental section, FaceOff significantly outperforms alternate approaches qualitatively and quantitatively.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_FaceOff_A_Video-to-Video_Face_Swapping_System_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_FaceOff_A_Video-to-Video_Face_Swapping_System_WACV_2023_paper.pdf,,,2208.09788,main,Poster,https://ieeexplore.ieee.org/document/10030407,"['Industries', 'Computer vision', 'Costing', 'Motion pictures', 'Computational efficiency', 'Task analysis', 'Faces']","['Latent Space', 'Identification Of Sources', 'Millions Of Dollars', 'Film Industry', 'Scene Context', 'Self-supervised Manner', 'Target Pose', 'Affine Transformation', 'Quantitative Metrics', 'None Of These Methods', 'Temporal Modulation', 'Latent Vector', 'Use Of Face', 'Temporal Coherence', 'Source Video', 'Self-supervised Training', 'Distance Loss', 'Video Output']","['Algorithms: Computational photography', 'image and video synthesis', 'Commercial/retail']",2,"Doubles play an indispensable role in the movie industry. They take the place of the actors in dangerous stunt scenes or scenes where the same actor plays multiple characters. The double's face is later replaced with the actor's face and expressions manually using expensive CGI technology, costing millions of dollars and taking months to complete. An automated, inexpensive, and fast way can be to use face-swapping techniques that aim to swap an identity from a source face video (or an image) to a target face video. However, such methods cannot preserve the source expressions of the actor important for the scene's context. To tackle this challenge, we introduce video-to-video (V2V) face-swapping, a novel task of face-swapping that can preserve (1) the identity and expressions of the source (actor) face video and (2) the background and pose of the target (double) video. We propose FaceOff, a V2V face-swapping system that operates by learning a robust blending operation to merge two face videos following the constraints above. It reduces the videos to a quantized latent space and then blends them in the reduced space. FaceOff is trained in a self-supervised manner and robustly tackles the non-trivial challenges of V2V face-swapping. As shown in the experimental section, FaceOff significantly outperforms alternate approaches qualitatively and quantitatively."
Fantastic Style Channels and Where To Find Them: A Submodular Framework for Discovering Diverse Directions in GANs,"Enis Simsar, Umut Kocasari, Ezgi Gülperi Er, Pinar Yanardag",Technical University of Munich; Bo˘gazic ¸i University,100,"Germany, Turkey",0,,"The discovery of interpretable directions in the latent spaces of pre-trained GAN models has recently become a popular topic. In particular, StyleGAN2 has enabled various image generation and manipulation tasks due to its rich and disentangled latent spaces. However, the discovery of such directions is typically made either in a supervised manner, which requires annotated data for each desired manipulation, or in an unsupervised manner, which requires a manual effort to identify the directions. As a result, existing work typically finds only a handful of directions in which controllable edits can be made. In this study, we design a novel submodular framework that finds the most representative and diverse subset of directions in the latent space of StyleGAN2. Our approach takes advantage of the latent space of channel-wise style parameters, so-called stylespace, in which we cluster channels that perform similar manipulations into groups. Our framework promotes diversity by using the notion of clusters and can be efficiently solved with a greedy optimization scheme. We evaluate our framework with qualitative and quantitative experiments and show that our method finds more diverse and disentangled directions.",https://openaccess.thecvf.com/content/WACV2023/html/Simsar_Fantastic_Style_Channels_and_Where_To_Find_Them_A_Submodular_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Simsar_Fantastic_Style_Channels_and_Where_To_Find_Them_A_Submodular_WACV_2023_paper.pdf,,,2203.08516,main,Poster,https://ieeexplore.ieee.org/document/10030134/,"['Computer vision', 'Image synthesis', 'Manuals', 'Aerospace electronics', 'Task analysis', 'Optimization']","['Latent Space', 'Directions In Space', 'Unsupervised Manner', 'Objective Function', 'Support Vector Machine', 'Unsupervised Methods', 'Agglomerative Clustering', 'Concave Function', 'Latent Vector', 'Eye Color', 'Latent Code', 'Type Of Editing', 'Subset Of Channels']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"The discovery of interpretable directions in the latent spaces of pre-trained GAN models has recently become a popular topic. In particular, StyleGAN2 has enabled various image generation and manipulation tasks due to its rich and disentangled latent spaces. However, the discovery of such directions is typically made either in a supervised manner, which requires annotated data for each desired manipulation, or in an unsupervised manner, which requires a manual effort to identify the directions. As a result, existing work typically finds only a handful of directions in which controllable edits can be made. In this study, we design a novel submodular framework that finds the most representative and diverse subset of directions in the latent space of StyleGAN2. Our approach takes advantage of the latent space of channel-wise style parameters, so-called stylespace, in which we cluster channels that perform similar manipulations into groups. Our framework promotes diversity by using the notion of clusters and can be efficiently solved with a greedy optimization scheme. We evaluate our framework with qualitative and quantitative experiments and show that our method finds more diverse and disentangled directions."
Far3Det: Towards Far-Field 3D Detection,"Shubham Gupta, Jeet Kanjani, Mengtian Li, Francesco Ferroni, James Hays, Deva Ramanan, Shu Kong",CMU; Texas A&M University; Gatech; Argo AI,50,USA,50,USA,"We focus on the task of far-field 3D detection (Far3Det) of objects beyond a certain distance from an observer, e.g., >50m. Far3Det is particularly important for autonomous vehicles (AVs) operating at highway speeds, which require detections of far-field obstacles to ensure sufficient braking distances. However, contemporary AV benchmarks such as nuScenes underemphasize this problem because they evaluate performance only up to a certain distance (50m). One reason is that obtaining far-field 3D annotations is difficult, particularly for lidar sensors that produce very few point returns for far-away objects. Indeed, we find that almost 50% of far-field objects (beyond 50m) contain zero lidar points. Secondly, current metrics for 3D detection employ a ""one-size-fits-all"" philosophy, using the same tolerance thresholds for near and far objects, inconsistent with tolerances for both human vision and stereo disparities. Both factors lead to an incomplete analysis of the Far3Det task. For example, while conventional wisdom tells us that high-resolution RGB sensors should be vital for the 3D detection of far-away objects, lidar-based methods still rank higher compared to RGB counterparts on the current benchmark leaderboards. As a first step towards a Far3Det benchmark, we develop a method to find well-annotated scenes from the nuScenes dataset and derive a well-annotated far-field validation set. We also propose a Far3Det evaluation protocol and explore various 3D detection methods for Far3Det. Our result convincingly justifies the long held conventional wisdom that high-resolution RGB improves 3D detection in the far-field. We further propose a simple yet effective method that fuses detections from RGB and lidar detectors based on non-maximum suppression, which remarkably outperforms state-of-the-art 3D detectors in the far-field.",https://openaccess.thecvf.com/content/WACV2023/html/Gupta_Far3Det_Towards_Far-Field_3D_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_Far3Det_Towards_Far-Field_3D_Detection_WACV_2023_paper.pdf,,,2211.13858,main,Poster,https://ieeexplore.ieee.org/document/10030220/,"['Measurement', 'Road transportation', 'Three-dimensional displays', 'Laser radar', 'Protocols', 'Philosophical considerations', 'Detectors']","['3D Detection', 'Benchmark', 'Validation Set', 'Object Detection', 'Autonomous Vehicles', 'Evaluation Protocol', 'Conventional Wisdom', 'Human Vision', 'Non-maximum Suppression', 'LiDAR Sensor', 'LiDAR Point', 'Precise Location', 'Point Cloud', 'Distance Range', 'Fusion Method', 'Distance Threshold', 'Adaptive Threshold', 'Object Distance', 'Secret Sharing', 'Image-based Methods', 'Multimodal Detection', 'High-quality Annotations', '3D Object Detection', 'Sparse Point', 'Multimodal Methods', '3D World', 'IoU Threshold', 'Image-based Detection', 'Virtual Point', 'Multimodal Processing']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', '3D computer vision', 'Robotics']",3,"We focus on the task of far-field 3D detection (Far3Det) of objects beyond a certain distance from an observer, e.g., >50m. Far3Det is particularly important for autonomous vehicles (AVs) operating at highway speeds, which require detections of far-field obstacles to ensure sufficient braking distances. However, contemporary AV benchmarks such as nuScenes underemphasize this problem because they evaluate performance only up to a certain distance (50m). One reason is that obtaining far-field 3D annotations is difficult, particularly for lidar sensors that produce very few point returns for far-away objects. Indeed, we find that almost 50% of far-field objects (beyond 50m) contain zero lidar points. Secondly, current metrics for 3D detection employ a ""one-size-fits-all"" philosophy, using the same tolerance thresholds for near and far objects, inconsistent with tolerances for both human vision and stereo disparities. Both factors lead to an incomplete analysis of the Far3Det task. For example, while conventional wisdom tells us that highresolution RGB sensors should be vital for 3D detection of far-away objects, lidar-based methods still rank higher compared to RGB counterparts on the current benchmark leaderboards. As a first step towards a Far3Det benchmark, we develop a method to find well-annotated scenes from the nuScenes dataset and derive a well-annotated far-field validation set. We also propose a Far3Det evaluation protocol and explore various 3D detection methods for Far3Det. Our result convincingly justifies the long-held conventional wisdom that high-resolution RGB improves 3D detection in the far-field. We further propose a simple yet effective method that fuses detections from RGB and lidar detectors based on non-maximum suppression, which remarkably outperforms state-of-the-art 3D detectors in the far-field."
Fashion Image Retrieval With Text Feedback by Additive Attention Compositional Learning,"Yuxin Tian, Shawn Newsam, Kofi Boakye","University of California, Merced; Pinterest",50,USA,50,USA,"Effective fashion image retrieval with text feedback stands to impact a range of real-world applications, such as e-commerce. Given a source image and text feedback that describes the desired modifications to that image, the goal is to retrieve the target images that resemble the source yet satisfy the given modifications by composing a multi-modal (image-text) query. We propose a novel solution to this problem, Additive Attention Compositional Learning (AACL), that uses a multi-modal transformer-based architecture and effectively models the image-text contexts. Specifically, we propose a novel image-text composition module based on additive attention that can be seamlessly plugged into deep neural networks. We also introduce a new challenging benchmark derived from the Shopping100k dataset. AACL is evaluated on three large-scale datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong baselines. Extensive experiments show that AACL achieves new state-of-the-art results on all three datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Tian_Fashion_Image_Retrieval_With_Text_Feedback_by_Additive_Attention_Compositional_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tian_Fashion_Image_Retrieval_With_Text_Feedback_by_Additive_Attention_Compositional_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030891/,"['Deep learning', 'Computer vision', 'Additives', 'Image retrieval', 'Neural networks', 'Computer architecture', 'Benchmark testing']","['Image Retrieval', 'Additional Attention', 'Text Feedback', 'Fashion Images', 'Deep Neural Network', 'Large-scale Datasets', 'Target Image', 'Source Images', 'Natural Language', 'Attention Mechanism', 'Training Images', 'Image Pairs', 'Latent Space', 'Dot Product', 'Final Layer', 'Visual Search', 'Image Representation', 'Linear Layer', 'Visual Content', 'Text Representation', 'Query Image', 'Image Encoder', 'Final Representation', 'Multiple Attention', 'Context Vector', 'Vector C', 'Text Encoder', 'Text Query', 'Visual Features', 'Semantic']","['Applications: Commercial/retail', 'Arts/games/social media']",11,"Effective fashion image retrieval with text feedback stands to impact a range of real-world applications, such as e-commerce. Given a source image and text feedback that describes the desired modifications to that image, the goal is to retrieve the target images that resemble the source yet satisfy the given modifications by composing a multi-modal (image-text) query. We propose a novel solution to this problem, Additive Attention Compositional Learning (AACL), that uses a multi-modal transformer-based architecture and effectively models the image-text contexts. Specifically, we propose a novel image-text composition module based on additive attention that can be seamlessly plugged into deep neural networks. We also introduce a new challenging benchmark derived from the Shopping100k dataset. AACL is evaluated on three large-scale datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong baselines. Extensive experiments show that AACL achieves new state-of-the-art results on all three datasets."
Fast Differentiable Transient Rendering for Non-Line-of-Sight Reconstruction,"Markus Plack, Clara Callenberg, Monika Schneider, Matthias B. Hullin","University of Bonn, Bonn, Germany",100,Germany,0,,"Research into non-line-of-sight imaging problems has gained momentum in recent years motivated by intriguing prospective applications in e.g. medicine and autonomous driving. While transient image formation is well understood and there exist various reconstruction approaches for non-line-of-sight scenes that combine efficient forward renderers with optimization schemes, those approaches suffer from runtimes in the order of hours even for moderately sized scenes. Furthermore, the ill-posedness of the inverse problem often leads to instabilities in the optimization. Inspired by the latest advances in direct-line-of-sight inverse rendering that have led to stunning results for reconstructing scene geometry and appearance, we present a fast differentiable transient renderer that accelerates the inverse rendering runtime to minutes on consumer hardware, making it possible to apply inverse transient imaging on a wider range of tasks and in more time-critical scenarios. We demonstrate its effectiveness on a series of applications using various datasets and show that it can be used for self-supervised learning.",https://openaccess.thecvf.com/content/WACV2023/html/Plack_Fast_Differentiable_Transient_Rendering_for_Non-Line-of-Sight_Reconstruction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Plack_Fast_Differentiable_Transient_Rendering_for_Non-Line-of-Sight_Reconstruction_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030170/,"['Training', 'Runtime', 'Imaging', 'Self-supervised learning', 'Rendering (computer graphics)', 'Software', 'Time factors']","['Inverse Problem', 'Image Formation', 'Self-supervised Learning', 'Reconstruction Approach', 'Wide Range Of Tasks', 'Imaging Problem', 'Series Of Applications', 'Momentum In Recent Years', 'Simulated Data', 'Intersection Over Union', 'Radial Basis Function', 'Forward Model', 'Depth Map', 'Stochastic Optimization', 'Reconstruction Quality', 'Gradient Calculation', 'Armadillo', 'Reconstruction Time', 'Point Scanning', 'L2 Loss', 'Single-photon Avalanche Diode', 'Background Network', 'Surface Normals', 'Scene Representation', 'Vertex Position', 'Gradient Backpropagation', 'Flat Field', 'Object Reconstruction', 'Real Measurements', 'Radial Function']","['Algorithms: 3D computer vision', 'Low-level and physics-based vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Research into non-line-of-sight imaging problems has gained momentum in recent years motivated by intriguing prospective applications in e.g. medicine and autonomous driving. While transient image formation is well understood and there exist various reconstruction approaches for non-line-of-sight scenes that combine efficient forward renderers with optimization schemes, those approaches suffer from runtimes in the order of hours even for moderately sized scenes. Furthermore, the ill-posedness of the inverse problem often leads to instabilities in the optimization.Inspired by the latest advances in direct-line-of-sight inverse rendering that have led to stunning results for reconstructing scene geometry and appearance, we present a fast differentiable transient renderer that accelerates the inverse rendering runtime to minutes on consumer hardware, making it possible to apply inverse transient imaging on a wider range of tasks and in more time-critical scenarios. We demonstrate its effectiveness on a series of applications using various datasets and show that it can be used for self-supervised learning."
Fast Online Video Super-Resolution With Deformable Attention Pyramid,"Dario Fuoli, Martin Danelljan, Radu Timofte, Luc Van Gool","KU Leuven, Belgium; CAIDAS, University of Würzburg, Germany; Computer Vision Lab, ETH Zürich, Switzerland",100,"Belgium, Germany, Switzerland",0,,"Video super-resolution (VSR) has many applications that pose strict causal, real-time, and latency constraints, including video streaming and TV. We address the VSR problem under these settings, which poses additional important challenges since information from future frames is unavailable. Importantly, designing efficient, yet effective frame alignment and fusion modules remain central problems. In this work, we propose a recurrent VSR architecture based on a deformable attention pyramid (DAP). Our DAP aligns and integrates information from the recurrent state into the current frame prediction. To circumvent the computational cost of traditional attention-based methods, we only attend to a limited number of spatial locations, which are dynamically predicted by the DAP. Comprehensive experiments and analysis of the proposed key innovations show the effectiveness of our approach. We significantly reduce processing time and computational complexity in comparison to state-of-the-art methods, while maintaining a high performance. We surpass state-of-the-art method EDVR-M on two standard benchmarks with a speed-up of over 3x.",https://openaccess.thecvf.com/content/WACV2023/html/Fuoli_Fast_Online_Video_Super-Resolution_With_Deformable_Attention_Pyramid_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Fuoli_Fast_Online_Video_Super-Resolution_With_Deformable_Attention_Pyramid_WACV_2023_paper.pdf,,,2202.01731,main,Poster,https://ieeexplore.ieee.org/document/10030257/,"['Technological innovation', 'TV', 'Runtime', 'Superresolution', 'Streaming media', 'Benchmark testing', 'Transformers']","['Video Super-resolution', 'Deformable Attention', 'High Performance', 'Computational Complexity', 'Current Frame', 'Dynamic Prediction', 'Standard Benchmark', 'Future Frames', 'Large Distances', 'Attention Mechanism', 'Receptive Field', 'Exhaustive Search', 'Hidden State', 'Residual Block', 'Consecutive Frames', 'Large Displacement', 'Convolutional Block', 'Iterative Refinement', 'Adjacent Frames', 'Aggregation Scheme', 'Temporal Aggregation', 'Quadratic Complexity', 'Bidirectional Method', 'Center Of Frame', 'Motion Compensation', 'Pyramid Level', 'Faster Runtime', 'Small Resolution', 'Convolutional Network', 'Reverse Order']","['Algorithms: Low-level and physics-based vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",12,"Video super-resolution (VSR) has many applications that pose strict causal, real-time, and latency constraints, including video streaming and TV. We address the VSR problem under these settings, which poses additional important challenges since information from future frames is unavailable. Importantly, designing efficient, yet effective frame alignment and fusion modules remain central problems. In this work, we propose a recurrent VSR architecture based on a deformable attention pyramid (DAP). Our DAP aligns and integrates information from the recurrent state into the current frame prediction. To circumvent the computational cost of traditional attention-based methods, we only attend to a limited number of spatial locations, which are dynamically predicted by the DAP. Comprehensive experiments and analysis of the proposed key innovations show the effectiveness of our approach. We significantly reduce processing time and computational complexity in comparison to state-of-the-art methods, while maintaining a high performance. We surpass state-of-the-art method EDVR-M on two standard benchmarks with a speed-up of over 3×."
Fast and Accurate: Video Enhancement Using Sparse Depth,"Yu Feng, Patrick Hansen, Paul N. Whatmough, Guoyu Lu, Yuhao Zhu",University of Rochester; Arm Research; Tenstorrent Inc.; University of Georgia,50,USA,50,UK,"This paper presents a general framework to build fast and accurate algorithms for video enhancement tasks such as super-resolution, deblurring, and denoising. Essential to our framework is the realization that the accuracy, rather than the density, of pixel flows is what is required for high-quality video enhancement. Most of prior works take the opposite approach: they estimate dense (per-pixel)--but generally less robust--flows, mostly using computationally costly algorithms. Instead, we propose a lightweight flow estimation algorithm; it fuses the sparse point cloud data and (even sparser and less reliable) IMU data available in modern autonomous agents to estimate the flow information. Building on top of the flow estimation, we demonstrate a general framework that integrates the flows in a plug-and-play fashion with different task-specific layers. Algorithms built in our framework achieve 1.78x -- 187.41x speedup while providing a 0.42dB - 6.70 dB quality improvement over competing methods.",https://openaccess.thecvf.com/content/WACV2023/html/Feng_Fast_and_Accurate_Video_Enhancement_Using_Sparse_Depth_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Feng_Fast_and_Accurate_Video_Enhancement_Using_Sparse_Depth_WACV_2023_paper.pdf,,,2103.08764,main,Poster,https://ieeexplore.ieee.org/document/10030100/,"['Point cloud compression', 'Computer vision', 'Laser radar', 'Fuses', 'Superresolution', 'Noise reduction', 'Buildings']","['Sparse Depth', 'Point Cloud', 'Fast Algorithm', 'Flow Estimation', 'Point Cloud Data', 'High-quality Video', 'Sparse Point Cloud', 'Deep Neural Network', 'Object Detection', 'Temporal Features', 'Block Size', 'Vision Tasks', 'Peak Signal-to-noise Ratio', 'Optical Flow', 'Current Frame', 'Execution Speed', 'Geometric Transformation', 'Temporal Frame', 'Accurate Flow', 'Temporal Alignment', 'Structural Similarity Index Measure', 'Point Cloud Registration', 'Time Of Different Methods', 'Camera Coordinate System', 'Alignment Module', 'Warped Image', 'Multiple Cloud', 'Single Cloud', 'Explicit Approach', 'Neighboring Pixels']","['Algorithms: 3D computer vision', 'Embedded sensing/real-time techniques']",3,"This paper presents a general framework to build fast and accurate algorithms for video enhancement tasks such as super-resolution, deblurring, and denoising. Essential to our framework is the realization that the accuracy, rather than the density, of pixel flows is what is required for high-quality video enhancement. Most of prior works take the opposite approach: they estimate dense (per-pixel)—but generally less robust—flows, mostly using computationally costly algorithms. Instead, we propose a lightweight flow estimation algorithm; it fuses the sparse point cloud data and (even sparser and less reliable) IMU data available in modern autonomous agents to estimate the flow information. Building on top of the flow estimation, we demonstrate a general framework that integrates the flows in a plug-andplay fashion with different task-specific layers. Algorithms built in our framework achieve 1.78× — 187.41× speedup while providing a 0.42 dB – 6.70 dB quality improvement over competing methods."
FastSwap: A Lightweight One-Stage Framework for Real-Time Face Swapping,"Sahng-Min Yoo, Tae-Min Choi, Jae-Woo Choi, Jong-Hwan Kim","2RIT Lab., KAIST; 1KLleon AI Research, 2RIT Lab., KAIST",100,South Korea,0,,"Recent face swapping frameworks have achieved high-fidelity results. However, the previous works suffer from high computation costs due to the deep structure and the use of off-the-shelf networks. To overcome such problems and achieve real-time face swapping, we propose a lightweight one-stage framework, FastSwap. We design a shallow network trained in a self-supervised manner without any manual annotations. The core of our framework is a novel decoder block, called Triple Adaptive Normalization (TAN) block, which effectively integrates the identity and pose information. Besides, we propose a novel data augmentation and switch-test strategy to extract the attributes from the target image, which further enables controllable attribute editing. Extensive experiments on VoxCeleb2 and wild faces demonstrate that our framework generates high-fidelity face swapping results in 123.22 FPS and better preserves the identity, pose, and attributes than other state-of-the-art methods. Furthermore, we conduct an in-depth study to demonstrate the effectiveness of our proposal.",https://openaccess.thecvf.com/content/WACV2023/html/Yoo_FastSwap_A_Lightweight_One-Stage_Framework_for_Real-Time_Face_Swapping_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yoo_FastSwap_A_Lightweight_One-Stage_Framework_for_Real-Time_Face_Swapping_WACV_2023_paper.pdf,,https://github.com/sahngmin/fastswap,,main,Poster,https://ieeexplore.ieee.org/document/10030485/,"['Training', 'Costs', 'Switches', 'Manuals', 'Control systems', 'Real-time systems', 'Decoding']","['Deepfake', 'Data Augmentation', 'Identity Information', 'Target Image', 'Manual Annotation', 'Augmentation Strategy', 'Shallow Network', 'Frames Per Second', 'Pose Information', 'Self-supervised Manner', 'Convolutional Layers', 'Skin Color', 'Identification Of Features', 'Training Stage', 'Face Images', 'Testing Stage', 'Source Images', 'Training Step', 'Self-supervised Learning', 'Modulation Parameters', 'Target Pose', 'Fréchet Inception Distance', 'Identity Integration', 'Color Distortion', 'Target Pairs', 'Identity Preservation', 'Deep Generative Network', 'Spatial Integration', 'Neural Framework']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Virtual/augmented reality']",1,"Recent face swapping frameworks have achieved high-fidelity results. However, the previous works suffer from high computation costs due to the deep structure and the use of off-the-shelf networks. To overcome such problems and achieve real-time face swapping, we propose a lightweight one-stage framework, FastSwap. We design a shallow network trained in a self-supervised manner without any manual annotations. The core of our framework is a novel decoder block, called Triple Adaptive Normalization (TAN) block, which effectively integrates the identity and pose information. Besides, we propose a novel data augmentation and switch-test strategy to extract the attributes from the target image, which further enables controllable attribute editing. Extensive experiments on VoxCeleb2 and wild faces demonstrate that our framework generates high-fidelity face swapping results in 123.22 FPS and better preserves the identity, pose, and attributes than other state-of-the-art methods. Furthermore, we conduct an in-depth study to demonstrate the effectiveness of our proposal."
FeTrIL: Feature Translation for Exemplar-Free Class-Incremental Learning,"Grégoire Petit, Adrian Popescu, Hugo Schindler, David Picard, Bertrand Delezoide","Amanda, 34 Avenue Des Champs Elys ´ees, F-75008, Paris, France; LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vall ´ee, France; Universit ´e Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France",66.66666667,France,33.33333333,France,"Exemplar-free class-incremental learning is very challenging due to the negative effect of catastrophic forgetting. A balance between stability and plasticity of the incremental process is needed in order to obtain good accuracy for past as well as new classes. Existing exemplar-free class-incremental methods focus either on successive fine tuning of the model, thus favoring plasticity, or on using a feature extractor fixed after the initial incremental state, thus favoring stability. We introduce a method which combines a fixed feature extractor and a pseudo-features generator to improve the stability-plasticity balance. The generator uses a simple yet effective geometric translation of new class features to create representations of past classes, made of pseudo-features. The translation of features only requires the storage of the centroid representations of past classes to produce their pseudo-features. Actual features of new classes and pseudo-features of past classes are fed into a linear classifier which is trained incrementally to discriminate between all classes. The incremental process is much faster with the proposed method compared to mainstream ones which update the entire deep model. Experiments are performed with three challenging datasets, and different incremental settings. A comparison with ten existing methods shows that our method outperforms the others in most cases. FeTrIL code is available at https://github.com/GregoirePetit/FeTrIL",https://openaccess.thecvf.com/content/WACV2023/html/Petit_FeTrIL_Feature_Translation_for_Exemplar-Free_Class-Incremental_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Petit_FeTrIL_Feature_Translation_for_Exemplar-Free_Class-Incremental_Learning_WACV_2023_paper.pdf,,https://github.com/GregoirePetit/FeTrIL,2211.13131,main,Poster,https://ieeexplore.ieee.org/document/10030748/,"['Performance evaluation', 'Location awareness', 'Codes', 'Filtering', 'Feature extraction', 'Generators', 'Stability analysis']","['Class-incremental Learning', 'Deep Models', 'Feature Classification', 'Linear Classifier', 'Representative Class', 'Incremental Process', 'Catastrophic Forgetting', 'Feature Dimension', 'Transfer Learning', 'Regions Of Space', 'Single Class', 'Representation Of Space', 'Fully-connected Layer', 'Final Layer', 'Incremental Learning', 'Classification Layer', 'Linear Layer', 'Incremental Steps', 'Single Thread', 'Incremental Update', 'Few-shot Learning', 'Memory Footprint', 'Class Prototypes', 'Memory Cost']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",49,"Exemplar-free class-incremental learning is very challenging due to the negative effect of catastrophic forgetting. A balance between stability and plasticity of the incremental process is needed in order to obtain good accuracy for past as well as new classes. Existing exemplar-free class-incremental methods focus either on successive fine tuning of the model, thus favoring plasticity, or on using a feature extractor fixed after the initial incremental state, thus favoring stability. We introduce a method which combines a fixed feature extractor and a pseudo-features generator to improve the stability-plasticity balance. The generator uses a simple yet effective geometric translation of new class features to create representations of past classes, made of pseudo-features. The translation of features only requires the storage of the centroid representations of past classes to produce their pseudo-features. Actual features of new classes and pseudo-features of past classes are fed into a linear classifier which is trained incrementally to discriminate between all classes. The incremental process is much faster with the proposed method compared to mainstream ones which update the entire deep model. Experiments are performed with three challenging datasets, and different incremental settings. A comparison with ten existing methods shows that our method outperforms the others in most cases. FeTrIL code is available at https://github.com/GregoirePetit/FeTrIL."
Feature Disentanglement Learning With Switching and Aggregation for Video-Based Person Re-Identification,"Minjung Kim, MyeongAh Cho, Sangyoun Lee","Yonsei University, Korea Institute of Science and Technology (KIST); Yonsei University",100,South Korea,0,,"In video person re-identification (Re-ID), the network must consistently extract features of the target person from successive frames. Existing methods tend to focus only on how to use temporal information, which often leads to networks being fooled by similar appearances and same backgrounds. In this paper, we propose a Disentanglement and Switching and Aggregation Network (DSANet), which segregates the features representing identity and features based on camera characteristics, and pays more attention to ID information. We also introduce an auxiliary task that utilizes a new pair of features created through switching and aggregation to increase the network's capability for various camera scenarios. Furthermore, we devise a Target Localization Module (TLM) that extracts robust features against a change in the position of the target according to the frame flow and a Frame Weight Generation (FWG) that reflects temporal information in the final representation. Various loss functions for disentanglement learning are designed so that each component of the network can cooperate while satisfactorily performing its own role. Quantitative and qualitative results from extensive experiments demonstrate the superiority of DSANet over state-of-the-art methods on three benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Kim_Feature_Disentanglement_Learning_With_Switching_and_Aggregation_for_Video-Based_Person_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_Feature_Disentanglement_Learning_With_Switching_and_Aggregation_for_Video-Based_Person_WACV_2023_paper.pdf,,,2212.09498,main,Poster,https://ieeexplore.ieee.org/document/10030459/,"['Location awareness', 'Visualization', 'Computer vision', 'Switches', 'Benchmark testing', 'Feature extraction', 'Cameras']","['Feature Disentanglement', 'Disentanglement Learning', 'Video-based Person Re-identification', 'Loss Function', 'Identity Information', 'Benchmark Datasets', 'Temporal Information', 'Similar Appearance', 'Final Representation', 'Successive Frames', 'Aggregation Network', 'Auxiliary Task', 'Camera Features', 'Semantic', 'Positive Samples', 'Spatial Information', 'Feature Maps', 'Training Phase', 'Test Phase', 'Final Embedding', 'Embedding Vectors', 'Scene Features', 'Pseudo Labels', 'Retrieval Results', 'Discriminative Features', 'Final Feature', 'Video Sequences', 'Separate Features', 'Consecutive Frames']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",11,"In video person re-identification (Re-ID), the network must consistently extract features of the target person from successive frames. Existing methods tend to focus only on how to use temporal information, which often leads to networks being fooled by similar appearances and same backgrounds. In this paper, we propose a Disentanglement and Switching and Aggregation Network (DSANet), which segregates the features representing identity and features based on camera characteristics, and pays more attention to ID information. We also introduce an auxiliary task that utilizes a new pair of features created through switching and aggregation to increase the network’s capability for various camera scenarios. Furthermore, we devise a Target Localization Module (TLM) that extracts robust features against a change in the position of the target according to the frame flow and a Frame Weight Generation (FWG) that reflects temporal information in the final representation. Various loss functions for disentanglement learning are designed so that each component of the network can cooperate while satisfactorily performing its own role. Quantitative and qualitative results from extensive experiments demonstrate the superiority of DSANet over state-of-the-art methods on three benchmark datasets."
Federated Domain Generalization for Image Recognition via Cross-Client Style Transfer,"Junming Chen, Meirui Jiang, Qi Dou, Qifeng Chen",HKUST; CUHK,100,Hong Kong,0,,"Domain generalization (DG) has been a hot topic in image recognition, with a goal to train a general model that can perform well on unseen domains. Recently, federated learning (FL), an emerging machine learning paradigm to train a global model from multiple decentralized clients without compromising data privacy, brings new challenges, also new possibilities, to DG. In the FL scenario, many existing state-of-the-art (SOTA) DG methods become ineffective, because they require the centralization of data from different domains during training. In this paper, we propose a novel domain generalization method for image recognition under federated learning through cross-client style transfer (CCST) without exchanging data samples. Our CCST method can lead to more uniform distributions of source clients, and thus make each local model learn to fit the image styles of all the clients to avoid the different model biases. Two types of style (single image style and overall domain style) with corresponding mechanisms are proposed to be chosen according to different scenarios. Our style representation is exceptionally lightweight and can hardly be used for the reconstruction of the dataset. The level of diversity is also flexible to be controlled with a hyper-parameter. Our method outperforms recent SOTA DG methods on two DG benchmarks (PACS, OfficeHome) and a large-scale medical image dataset (Camelyon17) in the FL setting. Last but not least, our method is orthogonal to many classic DG methods, achieving additive performance by combined utilization.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Federated_Domain_Generalization_for_Image_Recognition_via_Cross-Client_Style_Transfer_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Federated_Domain_Generalization_for_Image_Recognition_via_Cross-Client_Style_Transfer_WACV_2023_paper.pdf,,https://chenjunming.ml/proj/CCST,2210.00912,main,Poster,https://ieeexplore.ieee.org/document/10030422/,"['Training', 'Data privacy', 'Computer vision', 'Image recognition', 'Federated learning', 'Data models', 'Picture archiving and communication systems']","['Image Recognition', 'Domain Generalization', 'Style Transfer', 'Medical Imaging', 'Global Model', 'Federated Learning', 'Medical Datasets', 'Style Image', 'Medical Image Datasets', 'Large-scale Image Datasets', 'Unseen Domains', 'Training Set', 'Local Data', 'Average Accuracy', 'Privacy Issues', 'Domain Shift', 'Affine Transformation', 'Communication Cost', 'Source Domain', 'Local Computing', 'Types Of Styles', 'Local Clients', 'Augmented Levels', 'Multiple Styles', 'Generalization Ability Of The Model', 'Central Server', 'Domain-invariant Representations', 'Arbitrary Model', 'Domain-invariant Features', 'Real-world Images']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",19,"Domain generalization (DG) has been a hot topic in image recognition, with a goal to train a general model that can perform well on unseen domains. Recently, federated learning (FL), an emerging machine learning paradigm to train a global model from multiple decentralized clients without compromising data privacy, has brought new challenges and possibilities to DG. In the FL scenario, many existing state-of-the-art (SOTA) DG methods become ineffective because they require the centralization of data from different domains during training. In this paper, we propose a novel domain generalization method for image recognition under federated learning through cross-client style transfer (CCST) without exchanging data samples. Our CCST method can lead to more uniform distributions of source clients, and make each local model learn to fit the image styles of all the clients to avoid the different model biases. Two types of style (single image style and overall domain style) with corresponding mechanisms are proposed to be chosen according to different scenarios. Our style representation is exceptionally lightweight and can hardly be used to reconstruct the dataset. The level of diversity is also flexible to be controlled with a hyper-parameter. Our method outperforms recent SOTA DG methods on two DG benchmarks (PACS, OfficeHome) and a large-scale medical image dataset (Camelyon17) in the FL setting. Last but not least, our method is orthogonal to many classic DG methods, achieving additive performance by combined utilization. Our code is available at: https://chenjunming.ml/proj/CCST."
Federated Learning for Commercial Image Sources,"Shreyansh Jain, Koteswar Rao Jerripothula","IIIT Delhi, New Delhi, India",100,India,0,,"Federated Learning is a collaborative machine learning paradigm that enables multiple clients to learn a global model without exposing their data to each other. Consequently, it provides a secure learning platform with privacy-preserving capabilities. This paper introduces a new dataset containing 23,326 images collected from eight different commercial sources and classified into 31 categories, similar to the Office-31 dataset. To the best of our knowledge, this is the first image classification dataset specifically designed for Federated Learning. We also propose two new Federated Learning algorithms, namely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from its previous client, updates them through local training, and passes them to the next client, thus forming a cyclic topology. In Fed-Star, a client receives weights from all other clients, updates its local weights through pre-aggregation (to address statistical heterogeneity) and local training, and sends its updated local weights to all other clients, thus forming a star-like topology. Our experiments reveal that both algorithms perform better than existing baselines on our newly introduced dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Jain_Federated_Learning_for_Commercial_Image_Sources_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jain_Federated_Learning_for_Commercial_Image_Sources_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030788/,"['Training', 'Computer vision', 'Federated learning', 'Collaboration', 'Data models', 'Classification algorithms', 'Topology']","['Commercial Sources', 'Federated Learning', 'Image Classification', 'Global Model', 'Statistical Heterogeneity', 'Local Training', 'Local Weights', 'Federated Learning Algorithm', 'Data Distribution', 'Objective Function', 'Learning Rate', 'F1 Score', 'Stochastic Gradient Descent', 'Privacy Issues', 'Heterogeneous Data', 'Domain Shift', 'Simple Algorithm', 'Real-world Scenarios', 'Maximum Accuracy', 'Communication Cost', 'Local Clients', 'Central Server', 'Local Loss']","['Applications: Commercial/retail', 'Social good']",6,"Federated Learning is a collaborative machine learning paradigm that enables multiple clients to learn a global model without exposing their data to each other. Consequently, it provides a secure learning platform with privacy-preserving capabilities. This paper introduces a new dataset containing 23,326 images collected from eight different commercial sources and classified into 31 categories, similar to the Office-31 dataset. To the best of our knowledge, this is the first image classification dataset specifically designed for Federated Learning. We also propose two new Federated Learning algorithms, namely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from its previous client, updates them through local training, and passes them to the next client, thus forming a cyclic topology. In Fed-Star, a client receives weights from all other clients, updates its local weights through pre-aggregation (to address statistical heterogeneity) and local training, and sends its updated local weights to all other clients, thus forming a star-like topology. Our experiments reveal that both algorithms perform better than existing baselines on our newly introduced dataset."
Few-Shot Learning of Compact Models via Task-Specific Meta Distillation,"Yong Wu, Shekhor Chanda, Mehrdad Hosseinzadeh, Zhi Liu, Yang Wang",University of Manitoba; Huawei Technologies Canada; Shanghai University; Concordia University,75,"Canada, China",25,Canada,"We consider a new problem of few-shot learning of compact models. Meta-learning is a popular approach for few-shot learning. Previous work in meta-learning typically assumes that the model architecture during meta-training is the same as the model architecture used for final deployment. In this paper, we challenge this basic assumption. For final deployment, we often need the model to be small. But small models usually do not have enough capacity to effectively adapt to new tasks. In the mean time, we often have access to the large dataset and extensive computing power during meta-training since meta-training is typically performed on a server. In this paper, we propose task-specific meta distillation that simultaneously learns two models in meta-learning: a large teacher model and a small student model. These two models are jointly learned during meta-training. Given a new task during meta-testing, the teacher model is first adapted to this task, then the adapted teacher model is used to guide the adaptation of the student model. The adapted student model is used for final deployment. We demonstrate the effectiveness of our approach in few-shot image classification using model-agnostic meta-learning (MAML). Our proposed method outperforms other alternatives on several benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Wu_Few-Shot_Learning_of_Compact_Models_via_Task-Specific_Meta_Distillation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wu_Few-Shot_Learning_of_Compact_Models_via_Task-Specific_Meta_Distillation_WACV_2023_paper.pdf,,,2210.09922,main,Poster,https://ieeexplore.ieee.org/document/10030996/,"['Training', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Computer architecture', 'Benchmark testing', 'Servers']","['Learning Models', 'Few-shot Learning', 'Image Classification', 'Teacher Model', 'Model Architecture', 'Adaptive Model', 'Student Model', 'Few-shot Classification', 'Model Parameters', 'Learning Rate', 'Knowledge Transfer', 'Computational Resources', 'Kullback-Leibler', 'Baseline Methods', 'Updated Model', 'Training Examples', 'Problem Setting', 'Teacher Network', 'Central Server', 'Support Set', 'Gradient Update', 'Edge Devices', 'Query Set', 'Student Network', 'Distillation Loss', 'Server Side', 'Adaptation Stage', 'Global Model', 'Categorical Cross-entropy Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"We consider a new problem of few-shot learning of com-pact models. Meta-learning is a popular approach for few-shot learning. Previous work in meta-learning typically assumes that the model architecture during meta-training is the same as the model architecture used for final deployment. In this paper, we challenge this basic assumption. For final deployment, we often need the model to be small. But small models usually do not have enough capacity to effectively adapt to new tasks. In the mean time, we often have access to the large dataset and extensive computing power during meta-training since meta-training is typically per-formed on a server. In this paper, we propose task-specific meta distillation that simultaneously learns two models in meta-learning: a large teacher model and a small student model. These two models are jointly learned during meta-training. Given a new task during meta-testing, the teacher model is first adapted to this task, then the adapted teacher model is used to guide the adaptation of the student model. The adapted student model is used for final deployment. We demonstrate the effectiveness of our approach in few-shot image classification using model-agnostic meta-learning (MAML). Our proposed method outperforms other alternatives on several benchmark datasets."
Few-Shot Medical Image Segmentation With Cycle-Resemblance Attention,"Hao Ding, Changchang Sun, Hao Tang, Dawen Cai, Yan Yan",Affiliation of the First Author; Affiliation of the Second Author,0,,100,,"Recently, due to the increasing requirements of medical imaging applications and the professional requirements of annotating medical images, few-shot learning has gained increasing attention in the medical image semantic segmentation field. To perform segmentation with limited number of labeled medical images, most existing studies use Prototypical Networks (PN) and have obtained compelling success. However, these approaches overlook the query image features extracted from the proposed representation network, failing to preserving the spatial connection between query and support images. In this paper, we propose a novel self-supervised few-shot medical image segmentation network and introduce a novel Cycle-Resemblance Attention (CRA) module to fully leverage the pixel-wise relation between query and support medical images. Notably, we first line up multiple attention blocks to refine more abundant relation information. Then, we present CRAPNet by integrating the CRA module with a classic prototype network, where pixel-wise relations between query and support features are well recaptured for segmentation. Extensive experiments on two different medical image datasets, e.g., abdomen MRI and abdomen CT, demonstrate the superiority of our model over existing state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Ding_Few-Shot_Medical_Image_Segmentation_With_Cycle-Resemblance_Attention_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ding_Few-Shot_Medical_Image_Segmentation_With_Cycle-Resemblance_Attention_WACV_2023_paper.pdf,Project Link if Available,GitHub Link if Available,2212.03967,main,Poster,https://ieeexplore.ieee.org/document/10030099/,"['Computer vision', 'Semantic segmentation', 'Computed tomography', 'Magnetic resonance imaging', 'Prototypes', 'Task analysis', 'Biomedical imaging']","['Medical Imaging', 'Image Segmentation', 'Medical Image Segmentation', 'Magnetic Resonance Imaging', 'Image Features', 'Semantic Segmentation', 'Attention Module', 'Query Image', 'Few-shot Learning', 'Attention Block', 'Query Features', 'Medical Image Datasets', 'Prototypical Network', 'Learning Rate', 'Large Amount Of Data', 'Spatial Information', '3D Images', 'Feature Maps', 'Segmentation Task', 'Pixel Location', 'Left Kidney', 'Non-local Operation', 'Weight Map', 'Fully Convolutional Network', 'Dice Score', 'Support Set', 'Downsampling Factor', 'Abdominal Magnetic Resonance Imaging', 'Image Segmentation Tasks', 'Magnetic Resonance Imaging Datasets']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",27,"Recently, due to the increasing requirements of medical imaging applications and the professional requirements of annotating medical images, few-shot learning has gained increasing attention in the medical image semantic segmentation field. To perform segmentation with limited number of labeled medical images, most existing studies use Prototypical Networks (PN) and have obtained compelling success. However, these approaches overlook the query imagefeatures extracted from the proposed representation network, failing to preserving the spatial connection between query and support images. In this paper, we propose a novel self-supervised few-shot medical image segmentation network and introduce a novel Cycle-Resemblance Attention (CRA) module to fully leverage the pixel-wise relation between query and support medical images. Notably, we first line up multiple attention blocks to refine more abundant relation information. Then, we present CRAPNet by integrating the CRA module with a classic prototype network, where pixel-wise relations between query and support features are well recaptured for segmentation. Extensive experiments on two different medical image datasets, e.g., abdomen MRI and abdomen CT, demonstrate the superiority of our model over existing state-of-the-art methods."
Few-Shot Object Counting With Similarity-Aware Feature Enhancement,"Zhiyuan You, Kai Yang, Wenhan Luo, Xin Lu, Lei Cui, Xinyi Le",Sun Yat-sen University; SenseTime; Tsinghua University; Shanghai Jiao Tong University,75,China,25,China,"This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one. To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to 14.32. Code has been released in https://github.com/zhiyuanyou/SAFECount.",https://openaccess.thecvf.com/content/WACV2023/html/You_Few-Shot_Object_Counting_With_Similarity-Aware_Feature_Enhancement_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/You_Few-Shot_Object_Counting_With_Similarity-Aware_Feature_Enhancement_WACV_2023_paper.pdf,,https://github.com/zhiyuanyou/SAFECount,2201.08959,main,Poster,https://ieeexplore.ieee.org/document/10031021/,"['Training', 'Computer vision', 'Image recognition', 'Target recognition', 'Focusing', 'Benchmark testing', 'Finite element analysis']","['Feature Enhancement', 'Object Counting', 'Mean Absolute Error', 'Spatial Dimensions', 'Large Margin', 'Number Of Objects', 'Clear Boundaries', 'Target Object', 'Similarity Map', 'Score Map', 'Query Image', 'Query Features', 'Root Mean Square Error', 'Training Set', 'Similar Values', 'Validation Set', 'Convolutional Layers', 'Feature Maps', 'Object Detection', 'Density Map', 'Dataset Split', 'Inclusion Exclusion', 'Attention Module']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",26,"This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one. To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to 14.32 (35%↑). Code has been released in https://github.com/zhiyuanyou/SAFECount."
Few-Shot Object Detection via Improved Classification Features,"Xinyu Jiang, Zhengjia Li, Maoqing Tian, Jianbo Liu, Shuai Yi, Duoqian Miao",Chinese University of Hong Kong; Tongji University; SenseTime Research,66.66666667,"China, Hong Kong",33.33333333,China,"Few-shot object detection (FSOD) aims to transfer knowledge from base classes to novel classes, which receives widespread attention recently. The performance of current techniques is, however, limited by the poor classification ability and the improper features in the detection head. To circumvent this issue, we propose a Multi-level Feature Enhancement (MFE) model to improve the feature for classification from three different perspectives, including the spatial level, the task level and the regularization level. First, we revise the classifier's input feature at the spatial level by using information from the regression head. Secondly, we separate the RoI-Align feature into two different feature distributions in order to improve features at the task level. Finally, taking into account the overfitting problem in FSOD, we design a simple but efficient regularization enhancement module to sample features into various distributions and enhance the regularization ability of classification. Extensive experiments show that our method achieves competitive results on PASCAL VOC datasets, and exceeds current state-of-the-art methods in all shot settings on challenging MS-COCO datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Jiang_Few-Shot_Object_Detection_via_Improved_Classification_Features_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jiang_Few-Shot_Object_Detection_via_Improved_Classification_Features_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030456/,"['Computer vision', 'Adaptation models', 'Computational modeling', 'Object detection', 'Computer architecture', 'Benchmark testing', 'Feature extraction']","['Feature Classification', 'Object Detection', 'Few-shot Object Detection', 'Base Classes', 'Spatial Level', 'Feature Enhancement', 'Enhancement Module', 'PASCAL VOC Dataset', 'Training Data', 'Ability Of The Model', 'Attention Mechanism', 'Transfer Learning', 'Bounding Box', 'Original Features', 'Max-pooling', 'Training Stage', 'Quality Characteristics', 'Spatial Module', 'General Objective', 'Linear Layer', 'Fine-tuning Stage', 'Channel-wise Attention', 'Few-shot Learning', 'Attention Layer', 'Original Proposal', 'Faster R-CNN', 'Spatial Shift', 'Few-shot Classification', 'Similar Architecture']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",9,"Few-shot object detection (FSOD) aims to transfer knowledge from base classes to novel classes, which receives widespread attention recently. The performance of current techniques is, however, limited by the poor classification ability and the improper features in the detection head. To circumvent this issue, we propose a Multi-level Feature Enhancement (MFE) model to improve the feature for classification from three different perspectives, including the spatial level, the task level and the regularization level. First, we revise the classifier’s input feature at the spatial level by using information from the regression head. Secondly, we separate the RoI-Align feature into two different feature distributions in order to improve features at the task level. Finally, taking into account the overfitting problem in FSOD, we design a simple but efficient regularization enhancement module to sample features into various distributions and enhance the regularization ability of classification. Extensive experiments show that our method achieves competitive results on PASCAL VOC datasets, and exceeds current state-of-the-art methods in all shot settings on challenging MS-COCO datasets."
Fine Gaze Redirection Learning With Gaze Hardness-Aware Transformation,"Sangjin Park, Daeha Kim, Byung Cheol Song","Inha University, Incheon, Republic of Korea",100,South Korea,0,,"The gaze redirection is a task to adjust the gaze of a given face or eye image toward the desired direction and aims to learn the gaze direction of a face image through a neural network-based generator. Considering that the prior arts have learned coarse gaze directions, learning fine gaze directions is very challenging. In addition, explicit discriminative learning of high-dimensional gaze features has not been reported yet. This paper presents solutions to overcome the above limitations. First, we propose the featurelevel transformation which provides gaze features corresponding to various gaze directions in the latent feature space. Second, we propose a novel loss function for discriminative learning of gaze features. Specifically, features with insignificant or irrelevant effects on gaze (e.g., head pose and appearance) are set as negative pairs, and important gaze features are set as positive pairs, and then pair-wise similarity learning is performed. As a result, the proposed method showed a redirection error of only 2deg for the GazeCapture dataset. This is a 10% better performance than a state-of-the-art method, i.e., STED. Additionally, the rationale for why latent features of various attributes should be discriminated is presented through activation visualization. Code is available at https://github.com/san9569/Gaze-Redir-Learning.",https://openaccess.thecvf.com/content/WACV2023/html/Park_Fine_Gaze_Redirection_Learning_With_Gaze_Hardness-Aware_Transformation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Park_Fine_Gaze_Redirection_Learning_With_Gaze_Hardness-Aware_Transformation_WACV_2023_paper.pdf,,https://github.com/san9569/Gaze-Redir-Learning,,main,Poster,https://ieeexplore.ieee.org/document/10030864/,"['Representation learning', 'Computer vision', 'Visualization', 'Emotion recognition', 'Costs', 'Codes', 'Face recognition']","['Gaze Redirection', 'Loss Function', 'Feature Space', 'Feature Learning', 'Cognitive Map', 'Latent Space', 'Face Images', 'Direct Imaging', 'Latent Features', 'Gaze Direction', 'Similar Learning', 'Eye Images', 'Prior Art', 'Head Pose', 'Neural Network', 'Hardness', 'Input Image', 'Negative Samples', 'Linear Interpolation', 'Multilayer Perceptron', 'Source Images', 'Head Direction', 'Metric Learning', 'Representation Learning', 'Deep Metric Learning', 'Target Image', 'Generative Adversarial Networks', 'Trivial Solution', 'Softplus', 'Hypersphere']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose']",1,"The gaze redirection is a task to adjust the gaze of a given face or eye image toward the desired direction and aims to learn the gaze direction of a face image through a neural network-based generator. Considering that the prior arts have learned coarse gaze directions, learning fine gaze directions is very challenging. In addition, explicit discriminative learning of high-dimensional gaze features has not been reported yet. This paper presents solutions to overcome the above limitations. First, we propose the feature-level transformation which provides gaze features corresponding to various gaze directions in the latent feature space. Second, we propose a novel loss function for discriminative learning of gaze features. Specifically, features with insignificant or irrelevant effects on gaze (e.g., head pose and appearance) are set as negative pairs, and important gaze features are set as positive pairs, and then pair-wise similarity learning is performed. As a result, the proposed method showed a redirection error of only 2° for the Gaze-Capture dataset. This is a 10% better performance than a state-of-the-art method, i.e., STED. Additionally, the rationale for why latent features of various attributes should be discriminated is presented through activation visualization. Code is available at https://github.com/san9569/Gaze-Redir-Learning"
Fine-Context Shadow Detection Using Shadow Removal,"Jeya Maria Jose Valanarasu, Vishal M. Patel",Johns Hopkins University,100,USA,0,,"Current shadow detection methods perform poorly when detecting shadow regions that are small, unclear or have blurry edges. In this work, we attempt to address this problem on two fronts. First, we propose a Fine Context-aware Shadow Detection Network (FCSD-Net), where we constraint the receptive field size and focus on low-level features to learn fine context features better. Second, we propose a new learning strategy, called Restore to Detect (R2D), where we show that when a deep neural network is trained for restoration (shadow removal), it learns meaningful features to delineate the shadow masks as well. To make use of this complementary nature of shadow detection and removal tasks, we train an auxiliary network for shadow removal and propose a complementary feature learning block (CFL) to learn and fuse meaningful features from shadow removal network to the shadow detection network. We train the proposed network, FCSD-Net, using the R2D learning strategy across multiple datasets. Experimental results on three public shadow detection datasets (ISTD, SBU and UCF) show that our method improves the shadow detection performance while being able to detect fine context better compared to the other recent methods. Our proposed learning strategy can also be adopted easily as a useful pipeline in future advances in shadow detection and removal.",https://openaccess.thecvf.com/content/WACV2023/html/Valanarasu_Fine-Context_Shadow_Detection_Using_Shadow_Removal_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Valanarasu_Fine-Context_Shadow_Detection_Using_Shadow_Removal_WACV_2023_paper.pdf,,,2109.09609,main,Poster,https://ieeexplore.ieee.org/document/10030958/,"['Representation learning', 'Deep learning', 'Fuses', 'Image edge detection', 'Pipelines', 'Neural networks', 'Detectors']","['Shadow Detection', 'Shadow Removal', 'Deep Neural Network', 'Learning Strategies', 'Feature Learning', 'Receptive Field', 'Detection Task', 'Receptive Field Size', 'Shadow Regions', 'Training Set', 'False Negative', 'Convolutional Neural Network', 'Image Features', 'Computer Vision', 'Input Image', 'Feature Maps', 'Object Detection', 'Semantic Segmentation', 'Clear Image', 'Handcrafted Features', 'Conv Layer', 'Fusion Block', 'Upsampling Layer', 'Parallel Network', 'Deep Learning-based Methods', 'Unclear Boundaries', 'Fine-tuning Stage']","['Applications: Smartphones/end user devices', 'Environmental monitoring/climate change/ecology', 'Remote Sensing']",7,"Current shadow detection methods perform poorly when detecting shadow regions that are small, unclear or have blurry edges. In this work, we attempt to address this problem on two fronts. First, we propose a Fine Context-aware Shadow Detection Network (FCSD- Net), where we constraint the receptive field size and focus on low-level features to learn fine context features better. Second, we propose a new learning strategy, called Restore to Detect (R2D), where we show that when a deep neural network is trained for restoration (shadow removal), it learns meaningful features to delineate the shadow masks as well. To make use of this complementary nature of shadow detection and removal tasks, we train an auxiliary network for shadow removal and propose a complementary feature learning block (CFL) to learn and fuse meaningful features from shadow removal network to the shadow detection network. We train the proposed network, FCSD-Net, using the R2D learning strategy across multiple datasets. Experimental results on three public shadow detection datasets (ISTD, SBU and UCF) show that our method improves the shadow detection performance while being able to detect fine context better compared to the other recent methods. Our proposed learning strategy can also be adopted easily as a useful pipeline in future advances in shadow detection and removal."
Fine-Grained Activities of People Worldwide,"Jeffrey Byrne, Gregory Castañón, Zhongheng Li, Gil Ettinger","Systems & Technology Research, Woburn MA, USA; Visym Labs, Cambridge MA, USA",50,USA,50,USA,"Every day, humans perform many closely related activities that involve subtle discriminative motions, such as putting on a shirt vs. putting on a jacket, or shaking hands vs. giving a high five. Activity recognition by ethical visual AI could provide insights into our patterns of daily life, however existing activity recognition datasets do not capture the massive diversity of these human activities around the world. To address this limitation, we introduce Collector, a free mobile app to record video while simultaneously annotating objects and activities of consented subjects. This new data collection platform was used to curate the Consented Activities of People (CAP) dataset, the first large-scale, fine-grained activity dataset of people worldwide. The CAP dataset contains 1.45M video clips of 512 fine grained activity labels of daily life, grouped into 144 coarse activity classes, collected by 780 subjects in 33 countries. We provide activity classification and activity detection benchmarks for this dataset, and analyze baseline results to gain insight into how people around with world perform common activities. The dataset, benchmarks, evaluation tools, public leaderboards and mobile apps are available for use at visym.github.io/cap.",https://openaccess.thecvf.com/content/WACV2023/html/Byrne_Fine-Grained_Activities_of_People_Worldwide_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Byrne_Fine-Grained_Activities_of_People_Worldwide_WACV_2023_paper.pdf,https://visym.github.io/cap,https://github.com/visym/cap,,main,Poster,https://ieeexplore.ieee.org/document/10030954/,"['Visualization', 'Ethics', 'Computer vision', 'Benchmark testing', 'Activity recognition', 'Data collection', 'Mobile applications']","['Fine-grained Action', 'Benchmark', 'Mobile App', 'Diverse Activities', 'Action Recognition', 'Subtle Motion', 'Social Media', 'Training Set', 'Large Datasets', 'Apr 2020', 'Object Detection', 'Intersection Over Union', 'Bounding Box', 'Detection Task', 'Moment In Time', 'Implicit Bias', 'Personal Details', 'Collection Of Datasets', 'Activity Classification', 'Mean Average Precision', 'Baseline System', 'Object Interaction', 'Video Dataset', 'Actual Dataset', 'Hand-held Camera', 'Number Of Clips', 'Rating Task', 'Data Augmentation', 'YouTube', 'Training Data']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Every day, humans perform many closely related activities that involve subtle discriminative motions, such as putting on a shirt vs. putting on a jacket, or shaking hands vs. giving a high five. Activity recognition by ethical visual AI could provide insights into our patterns of daily life, however existing activity recognition datasets do not capture the massive diversity of these human activities around the world. To address this limitation, we introduce Collector, a free mobile app to record video while simultaneously annotating objects and activities of consented subjects. This new data collection platform was used to curate the Consented Activities of People (CAP) dataset, the first large-scale, fine-grained activity dataset of people worldwide. The CAP dataset contains 1.45M video clips of 512 fine grained activity labels of daily life, collected by 780 subjects in 33 countries. We provide activity classification and activity detection benchmarks for this dataset, and analyze baseline results to gain insight into how people around with world perform common activities. The dataset, benchmarks, evaluation tools, public leaderboards and mobile apps are available for use at https://visym.github.io/cap."
Fine-Grained Affordance Annotation for Egocentric Hand-Object Interaction Videos,"Zecheng Yu, Yifei Huang, Ryosuke Furuta, Takuma Yagi, Yusuke Goutsu, Yoichi Sato","Industrial Institute of Science, The University of Tokyo",100,Japan,0,,"Object affordance is an important concept in hand-object interaction, providing information on action possibilities based on human motor capacity and objects' physical property thus benefiting tasks such as action anticipation and robot imitation learning. However, the definition of affordance in existing datasets often: 1) mix up affordance with object functionality; 2) confuse affordance with goal-related action; and 3) ignore human motor capacity. This paper proposes an efficient annotation scheme to address these issues by combining goal-irrelevant motor actions and grasp types as affordance labels and introducing the concept of mechanical action to represent the action possibilities between two objects. We provide new annotations by applying this scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as affordance recognition, hand-object interaction hotspots prediction, and cross-domain evaluation of affordance. The results show that models trained with our annotation can distinguish affordance from other concepts, predict fine-grained interaction possibilities on objects, and generalize through different domains.",https://openaccess.thecvf.com/content/WACV2023/html/Yu_Fine-Grained_Affordance_Annotation_for_Egocentric_Hand-Object_Interaction_Videos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yu_Fine-Grained_Affordance_Annotation_for_Egocentric_Hand-Object_Interaction_Videos_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030409/,"['Computer vision', 'Annotations', 'Affordances', 'Manuals', 'Predictive models', 'Task analysis', 'Videos']","['Hand-object Interaction', 'Mechanism Of Action', 'Objective Function', 'Motor Activity', 'Activity Prediction', 'Human Capacity', 'Imitation Learning', 'Motor Capacity', 'Use Of Tools', 'Recognition Task', 'Video Clips', 'Interaction Region', 'Action Recognition', 'Manual Annotation', 'Target Domain', 'Recognition Model', 'Automatic Annotation', 'Source Domain', 'Video Dataset', 'Action Labels', 'Action Recognition Model', 'Object Interaction', 'Annotation Methods']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",3,"Object affordance is an important concept in hand-object interaction, providing information on action possibilities based on human motor capacity and objects’ physical property thus benefiting tasks such as action anticipation and robot imitation learning. However, the definition of affordance in existing datasets often: 1) mix up affordance with object functionality; 2) confuse affordance with goal-related action; and 3) ignore human motor capacity. This paper proposes an efficient annotation scheme to address these issues by combining goal-irrelevant motor actions and grasp types as affordance labels and introducing the concept of mechanical action to represent the action possibilities between two objects. We provide new annotations by applying this scheme to the EPIC-KITCHENS dataset and test our annotation with tasks such as affordance recognition, hand-object interaction hotspots prediction, and cross-domain evaluation of affordance. The results show that models trained with our annotation can distinguish affordance from other concepts, predict fine-grained interaction possibilities on objects, and generalize through different do-mains."
Foreground Guidance and Multi-Layer Feature Fusion for Unsupervised Object Discovery With Transformers,"Zhiwei Lin, Zengyu Yang, Yongtao Wang","Wangxuan Institute of Computer Technology, Peking University",100,China,0,,"Unsupervised object discovery (UOD) has recently shown encouraging progress with the adoption of pre-trained Transformer features. However, current methods based on Transformers mainly focus on designing the localization head (e.g., seed selection-expansion and normalized cut) and overlook the importance of improving Transformer features. In this work, we handle UOD task from the perspective of feature enhancement and propose FOReground guidance and MUlti-LAyer feature fusion for unsupervised object discovery, dubbed FORMULA. Firstly, we present a foreground guidance strategy with an off-the-shelf UOD detector to highlight the foreground regions on the feature maps and then refine object locations in an iterative fashion. Moreover, to solve the scale variation issues in object detection, we design a multi-layer feature fusion module that aggregates features responding to objects at different scales. The experiments on VOC07, VOC12, and COCO_20k show that the proposed FORMULA achieves new state-of-the-art results on unsupervised object discovery. The code will be released at https://github.com/VDIGPKU/FORMULA.",https://openaccess.thecvf.com/content/WACV2023/html/Lin_Foreground_Guidance_and_Multi-Layer_Feature_Fusion_for_Unsupervised_Object_Discovery_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lin_Foreground_Guidance_and_Multi-Layer_Feature_Fusion_for_Unsupervised_Object_Discovery_WACV_2023_paper.pdf,,https://github.com/VDIGPKU/FORMULA,2210.13053,main,Poster,https://ieeexplore.ieee.org/document/10030545/,"['Location awareness', 'Visualization', 'Computer vision', 'Aggregates', 'Detectors', 'Object detection', 'Transformers']","['Multilayer Feature', 'Unsupervised Discovery', 'Feature Maps', 'Object Detection', 'Feature Aggregation', 'Feature Enhancement', 'Foreground Regions', 'Local Information', 'Feature Representation', 'Probability Function', 'Bounding Box', 'Self-supervised Learning', 'Attention Map', 'Visual Learning', 'Object Scale', 'Foreground Objects', 'Transformer Layers', 'Vision Transformer', 'Backbone Architecture', 'Original Feature Map']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"Unsupervised object discovery (UOD) has recently shown encouraging progress with the adoption of pre-trained Transformer features. However, current methods based on Transformers mainly focus on designing the localization head (e.g., seed selection-expansion and normalized cut) and overlook the importance of improving Transformer features. In this work, we handle UOD task from the perspective of feature enhancement and propose FOReground guidance and MUlti-LAyer feature fusion for unsupervised object discovery, dubbed FORMULA. Firstly, we present a foreground guidance strategy with an off-the-shelf UOD detector to highlight the foreground regions on the feature maps and then refine object locations in an iterative fashion. Moreover, to solve the scale variation issues in object detection, we design a multi-layer feature fusion module that aggregates features responding to objects at different scales. The experiments on VOC07, VOC12, and COCO 20k show that the proposed FORMULA achieves new state-of-the-art results on unsupervised object discovery. The code will be released at https://github.com/VDIGPKU/FORMULA."
Fractual Projection Forest: Fast and Explainable Point Cloud Classifier,Hanxiao Tan,"AI Group, TU Dortmund",100,Germany,0,,"Point clouds are playing an increasingly important roll in autonomous driving and robotics. Although current point cloud classification models have achieved satisfactory accuracies, most of them trade slight performance gains by stacking complex modules on the grouping-local-global framework, which leads to prolonged processing time and deteriorating interpretability. In this work, we propose a new pipeline named Fractual Projection Forest (FPF) that exploits fractal features to enable traditional machine learning models to achieve competitive performance with DNNs on classification tasks. Though compromises by few percentages in accuracy compared to DNNs, FPF is faster, more interpretable, and easily extendable. We hope that FPF may provide the community with a novel view of point cloud classification. Our code is available on https://github.com/Explain3D/FracProjForest.",https://openaccess.thecvf.com/content/WACV2023/html/Tan_Fractual_Projection_Forest_Fast_and_Explainable_Point_Cloud_Classifier_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tan_Fractual_Projection_Forest_Fast_and_Explainable_Point_Cloud_Classifier_WACV_2023_paper.pdf,,https://github.com/Explain3D/FracProjForest,,main,Poster,https://ieeexplore.ieee.org/document/10030741/,['Portable document format'],,"['Algorithms: 3D computer vision', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"Point clouds are playing an increasingly important roll in autonomous driving and robotics. Although current point cloud classification models have achieved satisfactory accuracies, most of them trade slight performance gains by stacking complex modules on the grouping-local-global framework, which leads to prolonged processing time and deteriorating interpretability. In this work, we propose a new pipeline named Fractual Projection Forest (FPF) that exploits fractal features to enable traditional machine learning models to achieve competitive performance with DNNs on classification tasks. Though compromises by few percentages in accuracy compared to DNNs, FPF is faster, more interpretable, and easily extendable. We hope that FPF may provide the community with a novel view of point cloud classification. Our code is available on https://github.com/Explain3D/FracProjForest."
Frame Interpolation for Dynamic Scenes With Implicit Flow Encoding,"Pedro Figueirêdo, Avinash Paliwal, Nima Khademi Kalantari",Texas A&M University,100,USA,0,,"In this paper, we propose an algorithm to interpolate between a pair of images of a dynamic scene. While in the past years significant progress in frame interpolation has been made, current approaches are not able to handle images with brightness and illumination changes, which are common even when the images are captured shortly apart. We propose to address this problem by taking advantage of the existing optical flow methods that are highly robust to the variations in the illumination. Specifically, using the bidirectional flows estimated using an existing pre-trained flow network, we predict the flows from an intermediate frame to the two input images. To do this, we propose to encode the bidirectional flows into a coordinate-based network, powered by a hypernetwork, to obtain a continuous representation of the flow across time. Once we obtain the estimated flows, we use them within an existing blending network to obtain the final intermediate frame. Through extensive experiments, we demonstrate that our approach is able to produce significantly better results than state-of-the-art frame interpolation algorithms.",https://openaccess.thecvf.com/content/WACV2023/html/Figueiredo_Frame_Interpolation_for_Dynamic_Scenes_With_Implicit_Flow_Encoding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Figueiredo_Frame_Interpolation_for_Dynamic_Scenes_With_Implicit_Flow_Encoding_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030823/,"['Interpolation', 'Computer vision', 'Image coding', 'Heuristic algorithms', 'Brightness', 'Lighting', 'Optical flow']","['Dynamic Scenes', 'Frame Interpolation', 'Implicit Flows', 'Input Image', 'Image Pairs', 'Optical Flow', 'Scene Images', 'Pre-trained Network', 'Network Flow', 'Flow Estimation', 'Illumination Changes', 'Bidirectional Flow', 'Illumination Variations', 'Optical Flow Method', 'Intermediate Frames', 'Neural Network', 'Convolution', 'Time Of Day', 'Final Image', 'Supplementary Video', 'Intermediate Flow', 'Intermediate Image', 'Warped Image', 'Network Weights', 'Neural Network Weights', 'Large Motion', 'Variable Light', 'Arbitrary Time', 'Optimal Weight', 'Video Dataset']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"In this paper, we propose an algorithm to interpolate between a pair of images of a dynamic scene. While in the past years significant progress in frame interpolation has been made, current approaches are not able to handle images with brightness and illumination changes, which are common even when the images are captured shortly apart. We propose to address this problem by taking advantage of the existing optical flow methods that are highly robust to the variations in the illumination. Specifically, using the bidirectional flows estimated using an existing pre-trained flow network, we predict the flows from an intermediate frame to the two input images. To do this, we propose to encode the bidirectional flows into a coordinate-based network, powered by a hypernetwork, to obtain a continuous representation of the flow across time. Once we obtain the estimated flows, we use them within an existing blending network to obtain the final intermediate frame. Through extensive experiments, we demonstrate that our approach is able to produce significantly better results than state-of-the-art frame interpolation algorithms."
FreeREA: Training-Free Evolution-Based Architecture Search,"Niccolò Cavagnero, Luca Robbiano, Barbara Caputo, Giuseppe Averta","Politecnico di Torino, Italy",100,Italy,0,,"In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximises the model accuracy while preserving size and computational constraints typical of tiny devices. Our approach, named FreeREA, is a custom cell-based evolution NAS algorithm that exploits an optimised combination of training-free metrics to rank architectures during the search, thus without need of model training. Our experiments, carried out on the common benchmarks NAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is a fast, efficient and effective search method for models automatic design; ii) it outperforms State of the Art training-based and training-free techniques in all the datasets and benchmarks considered, and iii) it can easily generalise to constrained scenarios, representing a competitive solution for fast Neural Architecture Search in generic constrained applications. The code is available at https://github.com/NiccoloCavagnero/FreeREA.",https://openaccess.thecvf.com/content/WACV2023/html/Cavagnero_FreeREA_Training-Free_Evolution-Based_Architecture_Search_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cavagnero_FreeREA_Training-Free_Evolution-Based_Architecture_Search_WACV_2023_paper.pdf,,https://github.com/NiccoloCavagnero/FreeREA,2207.05135,main,Poster,https://ieeexplore.ieee.org/document/10030904/,"['Training', 'Measurement', 'Costs', 'Computational modeling', 'Search methods', 'Neural networks', 'Memory management']","['Benchmark', 'Neural Network', 'Machine Learning', 'Computation Time', 'Search Algorithm', 'Combination Of Metrics', 'Neural Architecture Search', 'Population Size', 'Research Community', 'Optimal Model', 'Test Accuracy', 'Average Accuracy', 'Search Space', 'Skip Connections', 'Input Space', 'Directed Acyclic Graph', 'Hamming Distance', 'Exploration Capabilities', 'Tournament Selection', 'Selection Of Metrics', 'Future Research Trends', 'Training Architecture', 'Evolutionist']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximises the model accuracy while preserving size and computational constraints typical of tiny devices. Our approach, named FreeREA, is a custom cell-based evolution NAS algorithm that exploits an optimised combination of training-free metrics to rank architectures during the search, thus without need of model training. Our experiments, carried out on the common benchmarks NAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is a fast, efficient, and effective search method for models automatic design; ii) it outperforms State of the Art training-based and training-free techniques in all the datasets and benchmarks considered, and iii) it can easily generalise to constrained scenarios, representing a competitive solution for fast Neural Architecture Search in generic constrained applications. The code is available at https://github.com/NiccoloCavagnero/FreeREA."
Frequency-Aware Self-Supervised Monocular Depth Estimation,"Xingyu Chen, Thomas H. Li, Ruonan Zhang, Ge Li","School of Electronic and Computer Engineering, Peking University; School of Electronic and Computer Engineering, Peking University; Advanced Institute of Information Technology, Peking University; Information Technology R&D Innovation Center of Peking University",100,China,0,,"We present two versatile methods to generally enhance self-supervised monocular depth estimation (MDE) models. The high generalizability of our methods is achieved by solving the fundamental and ubiquitous problems in photometric loss function. In particular, from the perspective of spatial frequency, we first propose Ambiguity-Masking to suppress the incorrect supervision under photometric loss at specific object boundaries, the cause of which could be traced to pixel-level ambiguity. Second, we present a novel frequency-adaptive Gaussian low-pass filter, designed to robustify the photometric loss in high-frequency regions. We are the first to propose blurring images to improve depth estimators with an interpretable analysis. Both modules are lightweight, adding no parameters and no need to manually change the network structures. Experiments show that our methods provide performance boosts to a large number of existing models, including those who claimed state-of-the-art, while introducing no extra inference computation at all.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Frequency-Aware_Self-Supervised_Monocular_Depth_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Frequency-Aware_Self-Supervised_Monocular_Depth_Estimation_WACV_2023_paper.pdf,,https://github.com/xingyuuchen/freq-aware-depth,2210.05479,main,Poster,https://ieeexplore.ieee.org/document/10030148/,"['Computer vision', 'Frequency-domain analysis', 'Computational modeling', 'Image edge detection', 'Estimation', 'Low-pass filters', 'Frequency estimation']","['Depth Estimation', 'Self-supervised Monocular Depth Estimation', 'Loss Function', 'Ambiguity', 'Object Boundaries', 'High Frequency Region', 'Extra Computation', 'Gaussian Low-pass Filter', 'Prediction Error', 'Large Errors', 'Input Image', 'Image Reconstruction', 'Local Optimum', 'Point Cloud', 'Receptive Field', 'Target Image', 'Color Features', 'Prediction Network', 'Gaussian Blur', 'Ground Truth Depth', 'Reprojection Error', 'Camera Pose', 'Depth Prediction', 'Image Pyramid', 'Manhattan Distance', 'Stereo Matching', 'High Spatial Frequency']","['Algorithms: 3D computer vision', 'Low-level and physics-based vision']",5,"We present two versatile methods to generally enhance self-supervised monocular depth estimation (MDE) models. The high generalizability of our methods is achieved by solving the fundamental and ubiquitous problems in photometric loss function. In particular, from the perspective of spatial frequency, we first propose Ambiguity-Masking to suppress the incorrect supervision under photometric loss at specific object boundaries, the cause of which could be traced to pixel-level ambiguity. Second, we present a novel frequency-adaptive Gaussian low-pass filter, designed to robustify the photometric loss in high-frequency regions. We are the first to propose blurring images to improve depth estimators with an interpretable analysis. Both modules are lightweight, adding no parameters and no need to manually change the network structures. Experiments show that our methods provide performance boosts to a large number of existing models, including those who claimed state-of-the-art, while introducing no extra inference computation at all."
From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments,"Britty Baby, Daksh Thapar, Mustafa Chasmai, Tamajit Banerjee, Kunal Dargan, Ashish Suri, Subhashis Banerjee, Chetan Arora","IIT Mandi, India; Ashoka University, India; AIIMS, New Delhi, India; IIT Delhi, India",100,India,0,,"Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head misclassifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training the proposed classifier module using metric learning with arc loss to handle low inter-class variance of surgical instruments. We conduct exhaustive experiments on the benchmark datasets EndoVis2017 and EndoVis2018. We demonstrate that our method outperforms all (more than 18) SOTA methods compared with and improves the \sota performance by at least 12 points (20%) on the EndoVis2017 benchmark challenge and generalizes effectively across the datasets. Project page with source code is available at nets-iitd.github.io/s3net.",https://openaccess.thecvf.com/content/WACV2023/html/Baby_From_Forks_to_Forceps_A_New_Framework_for_Instance_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Baby_From_Forks_to_Forceps_A_New_Framework_for_Instance_Segmentation_WACV_2023_paper.pdf,nets-iitd.github.io/s3net,https://github.com/nets-iitd/s3net,2211.162,main,Poster,https://ieeexplore.ieee.org/document/10030955/,"['Training', 'Image segmentation', 'Technological innovation', 'Instruments', 'Source coding', 'Neural networks', 'Imaging']","['Surgical Instruments', 'Instance Segmentation', 'Class Labels', 'Bounding Box', 'Minimally Invasive Surgery', 'Natural Images', 'Metric Learning', 'Surgical Tools', 'Classification Head', 'Medical Devices', 'Classification Accuracy', 'Deep Neural Network', 'Aspect Ratio', 'Stage 2', 'Feature Classification', 'Object Detection', 'Cross-entropy Loss', 'Weight Vector', 'Semantic Segmentation', 'Multi-scale Features', 'Instance Segmentation Methods', 'Segmentation Approach', 'Region Proposal', 'Region Proposal Network', 'Types Of Instruments', 'Instance Labels', 'Non-maximum Suppression', 'Transformer Architecture', 'U-Net Architecture', 'Representation Learning']","['Applications: Biomedical/healthcare/medicine', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",11,"Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head misclassifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training our classifier module using metric learning with arc loss to handle low inter-class variance of surgical instruments. We conduct exhaustive experiments on the benchmark datasets EndoVis2017 and EndoVis2018. We demonstrate that our method outperforms all (more than 18) SOTA methods compared with, and improves the SOTA performance by at least 12 points (20%) on the EndoVis2017 benchmark challenge and generalizes effectively across the datasets. Project page with source code is available at nets-iitd.github.io/s3net."
Full Contextual Attention for Multi-Resolution Transformers in Semantic Segmentation,"Loic Themyr, Clément Rambour, Nicolas Thome, Toby Collins, Alexandre Hostettler","Conservatoire National des Arts et M ´etiers, Paris, France; IRCAD, Strasbourg, France; Sorbonne Universit ´e, CNRS, ISIR, F-75005 Paris, France",66.66666667,France,33.33333333,France,"Transformers have proved to be very effective for visual recognition tasks. In particular, vision transformers construct compressed global representation through self-attention and learnable class tokens. Multi-resolution transformers have shown recent successes in semantic segmentation but can only capture local interactions in high-resolution feature maps. This paper extends the notion of global tokens to build GLobal Attention Multi-resolution (GLAM) transformers. GLAM is a generic module that can be integrated into most existing transformer backbones. GLAM includes learnable global tokens, which unlike previous methods can model interactions between all image regions, and extracts powerful representations during training. Extensive experiments show that GLAM-Swin or GLAM-Swin-Unet exhibit substantially better performances than their vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used to segment large 3D medical images, and GLAM-nnFormer achieves new state-of-the-art performance on the BCV dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Themyr_Full_Contextual_Attention_for_Multi-Resolution_Transformers_in_Semantic_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Themyr_Full_Contextual_Attention_for_Multi-Resolution_Transformers_in_Semantic_Segmentation_WACV_2023_paper.pdf,,,2212.0789,main,Poster,https://ieeexplore.ieee.org/document/10030837/,"['Training', 'Visualization', 'Solid modeling', 'Three-dimensional displays', 'Image resolution', 'Semantic segmentation', 'Transformers']","['Semantic Segmentation', 'Medical Imaging', 'Feature Maps', 'Image Regions', 'High-resolution Maps', 'Global Attention', 'Vision Transformer', 'High-resolution Feature Maps', 'Synapse', '3D Images', 'Image Classification', 'Receptive Field', 'Multilayer Perceptron', 'Long-range Interactions', 'Finer Resolution', 'Global Information', 'Semantic Features', 'Skip Connections', 'Attention Map', '3D Segmentation', 'Attention Matrix', 'Multi-head Self-attention', 'Transformer Block', 'Quadratic Complexity', 'Sequence Of Tokens', 'Global Interaction', 'Natural Language Processing Tasks', 'Medical Image Segmentation', 'High-resolution Images', 'Resolution Of The Feature Map']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"Transformers have proved to be very effective for visual recognition tasks. In particular, vision transformers construct compressed global representations through self-attention and learnable class tokens. Multi-resolution transformers have shown recent successes in semantic segmentation but can only capture local interactions in high-resolution feature maps. This paper extends the notion of global tokens to build GLobal Attention Multi-resolution (GLAM) transformers. GLAM is a generic module that can be integrated into most existing transformer backbones. GLAM includes learnable global tokens, which unlike previous methods can model interactions between all image regions, and extracts powerful representations during training. Extensive experiments show that GLAM-Swin or GLAM-Swin-Unet exhibit substantially better performances than their vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used to segment large 3D medical images, and GLAM-nnFormer achieves new state-of-the-art performance on the BCV dataset."
GAF-Net: Improving the Performance of Remote Sensing Image Fusion Using Novel Global Self and Cross Attention Learning,"Ankit Jha, Shirsha Bose, Biplab Banerjee","Technical University of Munich, Germany; Indian Institute of Technology Bombay, India",100,"Germany, India",0,,"The notion of self and cross-attention learning has been found to substantially boost the performance of remote sensing (RS) image fusion. However, while the self-attention models fail to incorporate the global context due to the limited size of the receptive fields, cross-attention learning may generate ambiguous features as the feature extractors for all the modalities are jointly trained. This results in the generation of redundant multi-modal features, thus limiting the fusion performance. To address these issues, we propose a novel fusion architecture called Global Attention based Fusion Network (GAF-Net), equipped with novel self and cross-attention learning techniques. We introduce the within-modality feature refinement module through global spectral-spatial attention learning using the query-key-value processing where both the global spatial and channel contexts are used to generate two channel attention masks. Since it is non-trivial to generate the cross-attention from within the fusion network, we propose to leverage two auxiliary tasks of modality-specific classification to produce highly discriminative cross-attention masks. Finally, to ensure non-redundancy, we propose to penalize the high correlation between attended modality-specific features. Our extensive experiments on five benchmark datasets, including optical, multispectral (MS), hyperspectral (HSI), light detection and ranging (LiDAR), synthetic aperture radar (SAR), and audio modalities establish the superiority of GAF-Net concerning the literature.",https://openaccess.thecvf.com/content/WACV2023/html/Jha_GAF-Net_Improving_the_Performance_of_Remote_Sensing_Image_Fusion_Using_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jha_GAF-Net_Improving_the_Performance_of_Remote_Sensing_Image_Fusion_Using_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030780/,"['Representation learning', 'Laser radar', 'Limiting', 'Benchmark testing', 'Feature extraction', 'Optical imaging', 'Optical sensors']","['Global Attention', 'Remote Sensing Images', 'Global Learning', 'Attention Learning', 'Global Self', 'Global Context', 'Light Detection And Ranging', 'Synthetic Aperture Radar', 'Channel Attention', 'Fusion Network', 'Notion Of Learning', 'Notion Of Self', 'Feature Refinement', 'Fusion Architecture', 'Feature Maps', 'Digital Elevation Model', 'Semantic Segmentation', 'High-level Features', 'Attention Module', 'Multispectral Images', 'Spatial Attention', 'Hyperspectral Image Data', 'Channel Attention Module', 'Synthetic Aperture Radar Data', 'Visual Modality', 'Multimodal Dataset', 'Multimodal Learning', 'Class Activation Maps', 'Audio Data', 'Global Average Pooling']","['Applications: Remote Sensing', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",13,"The notion of self and cross-attention learning has been found to substantially boost the performance of remote sensing (RS) image fusion. However, while the self-attention models fail to incorporate the global context due to the limited size of the receptive fields, cross-attention learning may generate ambiguous features as the feature extractors for all the modalities are jointly trained. This results in the generation of redundant multi-modal features, thus limiting the fusion performance. To address these issues, we propose a novel fusion architecture called Global Attention based Fusion Network (GAF-Net), equipped with novel self and cross-attention learning techniques. We introduce the within-modality feature refinement module through global spectral-spatial attention learning using the query-key-value processing where both the global spatial and channel contexts are used to generate two channel attention masks. Since it is non-trivial to generate the cross-attention from within the fusion network, we propose to leverage two auxiliary tasks of modality-specific classification to produce highly discriminative cross-attention masks. Finally, to ensure non-redundancy, we propose to penalize the high correlation between attended modality-specific features. Our extensive experiments on five benchmark datasets, including optical, multispectral (MS), hyperspectral (HSI), light detection and ranging (LiDAR), synthetic aperture radar (SAR), and audio modalities establish the superiority of GAF-Net concerning the literature."
GAFNet: A Global Fourier Self Attention Based Novel Network for Multi-Modal Downstream Tasks,"Onkar Susladkar, Gayatri Deshmukh, Dhruv Makwana, Sparsh Mittal, R. Sai Chandra Teja, Rekha Singhal","IIT Roorkee, India; TCS Research, India; Independent Researcher",33.33333333,India,66.66666667,India,"In ""vision and language"" problems, multimodal inputs are simultaneously processed for combined visual and textual understanding for image-text embedding. In this paper, we discuss the necessity of considering the difference between the feature space and the distribution when performing multimodal learning. We deal with this problem through deep learning and a generative model approach. We introduce a novel network, GAFNet (Global Attention Fourier Net) which learns through large-scale pre-training over three image-text datasets (COCO, SBU, and CC-3M), for achieving high performance on downstream vision and language tasks. We propose a GAF (Global Attention Fourier) module, which integrates multiple modalities into one latent space. GAF module is independent of the type of modality and it allows combining shared representations at each stage. There are various ways of thinking about the relationships between different modalities, which directly affect the model's design. Global attention is not considered as in conventional multimodal learning. A GAF-based model can work for any modality (language, image, audio, category) and is designed to be used for different tasks. In contrast to previous research, our work considers visual grounding as a pretrainable and transferable quality instead of something that must be trained from scratch. Experimental results demonstrate that our technique is competitive and achieves state-of-the-art performance on a variety of popular downstream vision-language tasks, including image generation and image-text retrieval.",https://openaccess.thecvf.com/content/WACV2023/html/Susladkar_GAFNet_A_Global_Fourier_Self_Attention_Based_Novel_Network_for_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Susladkar_GAFNet_A_Global_Fourier_Self_Attention_Based_Novel_Network_for_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030840/,"['Deep learning', 'Visualization', 'Computer vision', 'Image synthesis', 'Grounding', 'Semantics', 'Bidirectional control']","['Global Attention', 'Global Fourier', 'Feature Space', 'Multiple Modalities', 'Latent Space', 'Image Generation', 'Wide Range Of Tasks', 'Multimodal Learning', 'COCO Dataset', 'Shared Representation', 'Image Quality', 'Frequency Domain', 'F1 Score', 'Object Detection', 'Multilayer Perceptron', 'Search String', 'Textual Descriptions', 'Text Words', 'Text Representation', 'L1 Loss', 'Vision Transformer', 'Multi-head Self-attention', 'Masked Language Model', 'Feed-forward Layer', 'Visual Stream', 'Text Modality', 'Macro F1 Score', 'Hidden Representation', 'AdamW Optimizer', 'Middle Branch']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"In ""vision and language"" problems, multimodal inputs are simultaneously processed for combined visual and textual understanding for image-text embedding. In this paper, we discuss the necessity of considering the difference between the feature space and the distribution when performing multimodal learning. We deal with this problem through deep learning and a generative model approach. We introduce a novel network, GAFNet (Global Attention Fourier Net), which learns through large-scale pre-training over three image-text datasets (COCO, SBU, and CC-3M), for achieving high performance on downstream vision and language tasks. We propose a GAF (Global Attention Fourier) module, which integrates multiple modalities into one latent space. GAF module is independent of the type of modality, and it allows combining shared representations at each stage. Various ways of thinking about the relationships between different modalities directly affect the model’s design. In contrast to previous research, our work considers visual grounding as a pretrainable and transferable quality instead of something that must be trained from scratch. We show that GAFNet is a versatile network that can be used for a wide range of downstream tasks. Experimental results demonstrate that our technique achieves state-of-the-art performance on multimodal classification on the CrisisMD dataset and image generation on the COCO dataset. For image-text retrieval, our technique achieves competitive performance."
GEMS: Generating Efficient Meta-Subnets,"Varad Pimpalkhute, Shruti Kunde, Rekha Singhal","TCS Research, Mumbai, India",0,,100,India,"Gradient-based meta learners (GBML) such as MAML aim to learn a model initialization across similar tasks, such that the model generalizes well on unseen tasks sampled from the same distribution with few gradient updates. A limitation of GBML is its inability to adapt to real-world applications where input tasks are sampled from multiple distributions. An existing effort learns N initializations for tasks sampled from N distributions; roughly increasing training time by a factor of N. Instead, we use a single model initialization to learn distribution-specific parameters for every input task. This reduces negative knowledge transfer across distributions and overall computational cost. Specifically, we explore two ways of efficiently learning on multi-distribution tasks: 1) Binary Mask Perceptron (BMP), which learns distribution-specific layers, 2) Multi-modal Supermask (MMSUP), which learns distribution-specific parameters. We evaluate the performance of the proposed framework (GEMS) on few-shot vision classification tasks. The experimental results demonstrate a significant improvement in accuracy and reduction in training time over existing state of the art algorithms on quasi-benchmark tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Pimpalkhute_GEMS_Generating_Efficient_Meta-Subnets_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pimpalkhute_GEMS_Generating_Efficient_Meta-Subnets_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030088/,"['Training', 'Adaptation models', 'Image segmentation', 'Computer vision', 'Computational modeling', 'Reinforcement learning', 'Object detection']","['Training Time', 'Multiple Distributions', 'Field Dataset', 'Gradient Update', 'Negative Transfer', 'Meta Learning', 'Learning Rate', 'Gradient Descent', 'Number Of Steps', 'Sparsity', 'Agnostic', 'Trainable Parameters', 'Model Scenarios', 'Training Tasks', 'Outer Loop', 'Computational Expense', 'Single Distribution', 'Input Distribution', 'Distribution Of Tasks', 'Adaptive Step', 'Gradient Step', 'Train Multiple Models', 'Few-shot Learning', 'Training Distribution']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Vision + language and/or other modalities']",,"Gradient-based meta learners (GBML) such as MAML [6] aim to learn a model initialization across similar tasks, such that the model generalizes well on unseen tasks sampled from the same distribution with few gradient updates. A limitation of GBML is its inability to adapt to real-world applications where input tasks are sampled from multiple distributions. An existing effort [23] learns ${\mathcal{N}}$ initializations for tasks sampled from ${\mathcal{N}}$ distributions; roughly increasing training time by a factor of ${\mathcal{N}}$. Instead, we use a single model initialization to learn distribution-specific parameters for every input task. This reduces negative knowledge transfer across distributions and overall computational cost. Specifically, we explore two ways of efficiently learning on multi-distribution tasks: 1) Binary Mask Perceptron (BMP) which learns distribution-specific layers, 2) Multi-modal Supermask (MMSUP) which learns distribution-specific parameters. We evaluate the performance of the proposed framework (GEMS) on few-shot vision classification tasks. The experimental results demonstrate an improvement in accuracy and a speed-up of ~2× to 4× in the training time, over existing state of the art algorithms on quasi-benchmark datasets in the field of meta-learning."
GEMS: Scene Expansion Using Generative Models of Graphs,"Rishi Agarwal, Tirupati Saketh Chandra, Vaidehi Patil, Aniruddha Mahapatra, Kuldeep Kulkarni, Vishwa Vinay","Adobe Research, India; IIT Bombay, India; Carnegie Mellon University, USA; Stanford University, USA; UNC Chapel Hill, USA",80,"India, USA",20,USA,"Applications based on image retrieval require editing and associating in intermediate spaces that are representative of the high-level concepts like objects and their relationships rather than dense, pixel-level representations like RGB images or semantic-label maps. We focus on one such representation, scene graphs, and propose a novel scene expansion task where we enrich an input seed graph by adding new nodes (objects) and the corresponding relationships. To this end, we formulate scene graph expansion as a sequential prediction task involving multiple iterations of first predicting a new node and then predicting the set of relationships between the newly predicted node and previously chosen nodes in the graph. We propose and evaluate a sequencing strategy that retains the clustering patterns amongst nodes. In addition, we leverage external knowledge to train our graph generation model, enabling greater generalization of node predictions. Due to the inefficiency of existing maximum mean discrepancy (MMD) based metrics standard for graph generation problems, we design novel metrics that comprehensively evaluate different aspects of node and relation predictions. We conduct extensive experiments on Visual Genome and VRD datasets to evaluate the expanded scene graphs using the standard MMD based metrics and our proposed metrics. We observe that the graphs generated by our method, GEMS, better represent the real distribution of the scene graphs compared with baseline methods like GraphRNN.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_GEMS_Scene_Expansion_Using_Generative_Models_of_Graphs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_GEMS_Scene_Expansion_Using_Generative_Models_of_Graphs_WACV_2023_paper.pdf,,,2207.03729,main,Poster,https://ieeexplore.ieee.org/document/10030125/,"['Measurement', 'Visualization', 'Sequential analysis', 'Computer vision', 'Computational modeling', 'Image retrieval', 'Genomics']","['Prediction Task', 'Baseline Methods', 'Sequence Prediction', 'External Knowledge', 'Graph Generation', 'Maximum Mean Discrepancy', 'Scene Graph', 'Training Set', 'Autoregressive Model', 'Generative Adversarial Networks', 'Object Classification', 'Line Of Work', 'Word Embedding', 'Variational Autoencoder', 'Additional Objective', 'Multinomial Distribution', 'Set Of Graphs', 'Object Pairs', 'Depth-first', 'Node Labels', 'Sequence Of Nodes', 'Fréchet Inception Distance', 'Molecular Graph', 'Ith Node', 'Edge Labels', 'Graph Datasets', 'Previous Node', 'Qualitative Examples', 'Types Of Edges', 'Drawing Inspiration']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",2,"Applications based on image retrieval require editing and associating in intermediate spaces that are representative of the high-level concepts like objects and their relationships rather than dense, pixel-level representations like RGB images or semantic-label maps. We focus on one such representation, scene graphs, and propose a novel scene expansion task where we enrich an input seed graph by adding new nodes (objects) and the corresponding relationships. To this end, we formulate scene graph expansion as a sequential prediction task involving multiple iterations of first predicting a new node and then predicting the set of relationships between the newly predicted node and previously chosen nodes in the graph. We propose and evaluate a sequencing strategy that retains the clustering patterns amongst nodes. In addition, we leverage external knowledge to train our graph generation model, enabling greater generalization of node predictions. Due to the inefficiency of existing maximum mean discrepancy (MMD) based metrics standard for graph generation problems, we design novel metrics that comprehensively evaluate different aspects of node and relation predictions. We conduct extensive experiments on Visual Genome and VRD datasets to evaluate the expanded scene graphs using the standard MMD based metrics, as well as our proposed metrics. We observe that the graphs generated by our method, GEMS, better represent the real distribution of the scene graphs compared with baseline methods like GraphRNN."
GLAD: A Global-to-Local Anomaly Detector,"Aitor Artola, Yannis Kolodziej, Jean-Michel Morel, Thibaud Ehret","Visionairy; Universit´e Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, France",50,France,50,USA,"Learning to detect automatic anomalies in production plants remains a machine learning challenge. Since anomalies by definition cannot be learned, their detection must rely on a very accurate ""normality model"". To this aim, we introduce here a global-to-local Gaussian model for neural network features, learned from a set of normal images. This probabilistic model enables unsupervised anomaly detection. A global Gaussian mixture model of the features is first learned using all available features from normal data. This global Gaussian mixture model is then localized by an adaptation of the K-MLE algorithm, which learns a spatial weight map for each Gaussian. These weights are then used instead of the mixture weights to detect anomalies. This method enables precise modeling of complex data, even with limited data. Applied on WideResnet50-2 features, our approach outperforms the previous state of the art on the MVTec dataset, particularly on the object category. It is robust to perturbations that are frequent in production lines, such as imperfect alignment, and is on par in terms of memory and computation time with the previous state of the art.",https://openaccess.thecvf.com/content/WACV2023/html/Artola_GLAD_A_Global-to-Local_Anomaly_Detector_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Artola_GLAD_A_Global-to-Local_Anomaly_Detector_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030772/,"['Adaptation models', 'Computer vision', 'Perturbation methods', 'Neural networks', 'Production', 'Machine learning', 'Detectors']","['Anomaly Detection', 'Neural Network', 'Data Normalization', 'Computation Time', 'Feature Model', 'State Of The Art', 'Global Model', 'Mixture Model', 'Gaussian Model', 'Production Line', 'Gaussian Mixture Model', 'Weight Map', 'Neural Network Features', 'Deep Learning', 'Covariance Matrix', 'Image Size', 'Network Layer', 'Local Map', 'Number Of Gaussians', 'Pre-trained Network', 'Single Gaussian', 'Area Under Receiver Operating Characteristic Curve', 'Pre-trained Neural Network', 'Variational Autoencoder', 'Nonlocal Method', 'Background Model', 'Image Patches', 'Boundary Effects']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Learning to detect automatic anomalies in production plants remains a machine learning challenge. Since anomalies by definition cannot be learned, their detection must rely on a very accurate ""normality model"". To this aim, we introduce here a global-to-local Gaussian model for neural network features, learned from a set of normal images. This probabilistic model enables unsupervised anomaly detection. A global Gaussian mixture model of the features is first learned using all available features from normal data. This global Gaussian mixture model is then localized by an adaptation of the K-MLE algorithm, which learns a spatial weight map for each Gaussian. These weights are then used instead of the mixture weights to detect anomalies. This method enables precise modeling of complex data, even with limited data. Applied on WideResnet50-2 features, our approach outperforms the previous state of the art on the MVTec dataset, particularly on the object category. It is robust to perturbations that are frequent in production lines, such as imperfect alignment, and is on par in terms of memory and computation time with the previous state of the art."
GaIA: Graphical Information Gain Based Attention Network for Weakly Supervised Point Cloud Semantic Segmentation,"Min Seok Lee, Seok Woo Yang, Sung Won Han","School of Industrial and Management Engineering, Korea University",100,South Korea,0,,"While point cloud semantic segmentation is a significant task in 3D scene understanding, this task demands a time-consuming process of fully annotating labels. To address this problem, recent studies adopt a weakly supervised learning approach under the sparse annotation. Different from the existing studies, this study aims to reduce the epistemic uncertainty measured by the entropy for a precise semantic segmentation. We propose the graphical information gain based attention network called GaIA, which alleviates the entropy of each point based on the reliable information. The graphical information gain discriminates the reliable point by employing relative entropy between target point and its neighborhoods. We further introduce anchor-based additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled points containing high entropy towards semantically similar classes of the labeled points on hypersphere space. Experimental results on S3DIS and ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly supervised methods.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_GaIA_Graphical_Information_Gain_Based_Attention_Network_for_Weakly_Supervised_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_GaIA_Graphical_Information_Gain_Based_Attention_Network_for_Weakly_Supervised_WACV_2023_paper.pdf,,,2210.01558,main,Poster,https://ieeexplore.ieee.org/document/10030243/,"['Point cloud compression', 'Uncertainty', 'Additives', 'Three-dimensional displays', 'Semantic segmentation', 'Computer network reliability', 'Supervised learning']","['Point Cloud', 'Semantic Segmentation', 'Point Cloud Semantic Segmentation', 'Kullback-Leibler', 'Additional Loss', 'High Entropy', 'Epistemic Uncertainty', 'Precise Segmentation', 'Additional Margin', 'K-nearest Neighbor', 'Cross-entropy Loss', 'Softmax Function', 'Segmentation Results', 'Shannon Entropy', 'Affine Transformation', 'Segmentation Performance', 'Uncertainty Quantification', 'False Predictions', 'Low Entropy', 'Reliable Representation', 'Siamese Network', 'Encoder Block', 'Point Xi', 'Annotated Sets', 'Uncertainty Information', 'Improve Segmentation Performance']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",12,"While point cloud semantic segmentation is a significant task in 3D scene understanding, this task demands a time-consuming process of fully annotating labels. To address this problem, recent studies adopt a weakly supervised learning approach under the sparse annotation. Different from the existing studies, this study aims to reduce the epistemic uncertainty measured by the entropy for a precise semantic segmentation. We propose the graphical information gain based attention network called GaIA, which alleviates the entropy of each point based on the reliable information. The graphical information gain discriminates the reliable point by employing relative entropy between target point and its neighborhoods. We further introduce anchor-based additive angular margin loss, ArcPoint. The ArcPoint optimizes the unlabeled points containing high entropy towards semantically similar classes of the labeled points on hypersphere space. Experimental results on S3DIS and ScanNet-v2 datasets demonstrate our framework outperforms the existing weakly supervised methods."
Gait Recognition Using 3-D Human Body Shape Inference,"Haidong Zhu, Zhaoheng Zheng, Ram Nevatia",University of Southern California,100,USA,0,,"Gait recognition, which identifies individuals based on their walking patterns, is an important biometric technique since it can be observed from a distance and does not require the subject's cooperation. Recognizing a person's gait is difficult because of the appearance variants in human silhouette sequences produced by varying viewing angles, carrying objects, and clothing. Recent research has produced a number of ways for coping with these variants. In this paper, we present the usage of inferring 3-D body shapes distilled from limited images, which are, in principle, invariant to the specified variants. Inference of 3-D shape is a difficult task, especially when only silhouettes are provided in a dataset. We provide a method for learning 3-D body inference from silhouettes by transferring knowledge from 3-D shape prior from RGB photos. We use our method on multiple existing state-of-the-art gait baselines and obtain consistent improvements for gait identification on two public datasets, CASIA-B and OUMVLP, on several variants and settings, including a new setting of novel views not seen during training.",https://openaccess.thecvf.com/content/WACV2023/html/Zhu_Gait_Recognition_Using_3-D_Human_Body_Shape_Inference_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhu_Gait_Recognition_Using_3-D_Human_Body_Shape_Inference_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030762/,"['Training', 'Legged locomotion', 'Computer vision', 'Shape', 'Clothing', 'Cameras', 'Task analysis']","['Body Shape', 'Human Body Shape', 'Gait Recognition', 'Public Datasets', 'Biometric', 'Gait Analysis', 'Walking Pattern', 'Learning Rate', 'Local Features', 'Knowledge Transfer', 'Convolution Operation', 'RGB Images', 'Baseline Methods', 'Fully-connected Layer', 'Video Sequences', 'Camera Position', 'Temporal Shift', 'Feature Encoder', 'Gait Characteristics', 'Camera Viewpoint', 'Single-frame Images', 'Triplet Loss', 'Gallery Set', 'Normal Walking', 'Subject Sequence', 'Training Instances', 'Mutually Exclusive', 'Training Set']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",17,"Gait recognition, which identifies individuals based on their walking patterns, is an important biometric technique since it can be observed from a distance and does not require the subject’s cooperation. Recognizing a person’s gait is difficult because of the appearance variants in human silhouette sequences produced by varying viewing angles, carrying objects, and clothing. Recent research has produced a number of ways for coping with these variants. In this paper, we present the usage of inferring 3-D body shapes distilled from limited images, which are, in principle, invariant to the specified variants. Inference of 3-D shape is a difficult task, especially when only silhouettes are provided in a dataset. We provide a method for learning 3-D body inference from silhouettes by transferring knowledge from 3-D shape prior from RGB photos. We use our method on multiple existing state-of-the-art gait baselines and obtain consistent improvements for gait identification on two public datasets, CASIA-B and OUMVLP, on several variants and settings, including a new setting of novel views not seen during training."
Gallery Filter Network for Person Search,"Lucas Jaffe, Avideh Zakhor",UC Berkeley,100,USA,0,,"In person search, we aim to localize a query person from one scene in other gallery scenes. The cost of this search operation is dependent on the number of gallery scenes, making it beneficial to reduce the pool of likely scenes. We describe and demonstrate the Gallery Filter Network (GFN), a novel module which can efficiently discard gallery scenes from the search process, and benefit scoring for persons detected in remaining scenes. We show that the GFN is robust under a range of different conditions by testing on different retrieval sets, including cross-camera, occluded, and low-resolution scenarios. In addition, we develop the base SeqNeXt person search model, which improves and simplifies the original SeqNet model. We show that the SeqNeXt+GFN combination yields significant performance gains over other state-of-the-art methods on the standard PRW and CUHK-SYSU person search datasets. To aid experimentation for this and other models, we provide standardized tooling for the data processing and evaluation pipeline typically used for person search research.",https://openaccess.thecvf.com/content/WACV2023/html/Jaffe_Gallery_Filter_Network_for_Person_Search_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jaffe_Gallery_Filter_Network_for_Person_Search_WACV_2023_paper.pdf,,,2210.12903,main,Poster,https://ieeexplore.ieee.org/document/10030443/,"['Training', 'Computer vision', 'Costs', 'Surveillance', 'Pipelines', 'Performance gain', 'Data processing']","['Range Of Different Conditions', 'Training Set', 'Cross-entropy Loss', 'Bounding Box', 'Lookup Table', 'Model Configuration', 'Scene Images', 'Inference Time', 'Embedding Dimension', 'Faster R-CNN', 'Final Configuration', 'Cropped Images', 'Feature Pyramid Network', 'Region Proposal Network', 'Multipartite', 'Top-1 Accuracy', 'Image Resampling', 'Vision Transformer', 'Unknown Person', 'Backbone Feature', 'Reduced Search Space', 'Average Precision']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Biometrics', 'face', 'gesture', 'body pose', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",9,"In person search, we aim to localize a query person from one scene in other gallery scenes. The cost of this search operation is dependent on the number of gallery scenes, making it beneficial to reduce the pool of likely scenes. We describe and demonstrate the Gallery Filter Network (GFN), a novel module which can efficiently discard gallery scenes from the search process, and benefit scoring for persons detected in remaining scenes. We show that the GFN is robust under a range of different conditions by testing on different retrieval sets, including cross-camera, occluded, and low-resolution scenarios. In addition, we develop the base SeqNeXt person search model, which improves and simplifies the original SeqNet model. We show that the SeqNeXt+GFN combination yields significant performance gains over other state-of-the-art methods on the standard PRW and CUHK-SYSU person search datasets. To aid experimentation for this and other models, we provide standardized tooling for the data processing and evaluation pipeline typically used for person search research."
GarSim: Particle Based Neural Garment Simulator,"Lokender Tiwari, Brojeshwar Bhowmick","TCS Research, India",0,,100,India,"We present a particle-based neural garment simulator (dubbed as GarSim) that can simulate template garments on the target arbitrary body poses. Existing learning-based methods majorly work for specific garment type (e.g. t-shirt, skirt, etc) or garment topology, and needs retraining for a new type of garment. Similarly, some methods focus on a particular fabric, body shape, and pose. To circumvent these limitations, our method fundamentally learns the physical dynamics of the garment vertices conditioned on underlying body shape, motion, and fabric properties to generalize across garment types, topology, and fabric along with different body shape and pose. In particular, we represent the garment as a graph, where the nodes represent the physical state of the garment vertices, and the edges represent the relation between the two nodes. The nodes and edges of the garment graph encode various properties of garments and the human body to compute the dynamics of the vertices through a learned message-passing. Learning of such dynamics of the garment vertices conditioned on underlying body motion and fabric properties enables our method to be trained simultaneously for multiple types of garments (e.g., tops, skirts, etc) with arbitrary mesh resolutions, varying topologies, and fabric properties. Our experimental results show that GarSim with less amount of training data not only outperforms the SOTA methods on challenging CLOTH3D dataset both qualitatively and quantitatively, but also works reliably well on the unseen poses obtained from YouTube videos, and give satisfactory results on unseen cloth types which were not present during the training.",https://openaccess.thecvf.com/content/WACV2023/html/Tiwari_GarSim_Particle_Based_Neural_Garment_Simulator_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tiwari_GarSim_Particle_Based_Neural_Garment_Simulator_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030826/,"['Training', 'Video on demand', 'Shape', 'Clothing', 'Dynamics', 'Training data', 'Fabrics']","['Body Shape', 'Body Motion', 'Properties Of Fabrics', 'Mesh Resolution', 'SOTA Methods', 'Body Pose', 'Collision', 'Latent Space', 'Leather', 'Alternative Choice', 'Node Features', 'Latent Features', 'Edge Features', 'Displacement In Direction', 'Types Of Fabrics', 'L2 Loss', 'Node Update', 'Decoding Step', 'Physics-based Simulation']","['Algorithms: 3D computer vision', 'Virtual/augmented reality', 'Visualization']",9,"We present a particle-based neural garment simulator (dubbed as GarSim) that can simulate template garments on the target arbitrary body poses. Existing learning-based methods majorly work for specific garment type (e.g. top, skirt, etc) or garment topology, and needs retraining for a new type of garment. Similarly, some methods focus on a particular fabric, body shape, and pose. To circumvent these limitations, our method fundamentally learns the physical dynamics of the garment vertices conditioned on underlying body shape, motion, and fabric properties to generalize across garment types, topology, and fabric along with different body shape and pose. In particular, we represent the garment as a graph, where the nodes represent the physical state of the garment vertices, and the edges represent the relation between the two nodes. The nodes and edges of the garment graph encode various properties of garments and the human body to compute the dynamics of the vertices through a learned message-passing. Learning of such dynamics of the garment vertices conditioned on underlying body motion and fabric properties enables our method to be trained simultaneously for multiple types of garments (e.g., tops, skirts, etc) with arbitrary mesh resolutions, varying topologies, and fabric properties. Our experimental results show that GarSim with less amount of training data not only outperforms the SOTA methods on challenging CLOTH3D dataset both qualitatively and quantitatively, but also works reliably well on the unseen poses obtained from YouTube videos, and give satisfactory results on unseen cloth types which were not present during the training."
Generative Alignment of Posterior Probabilities for Source-Free Domain Adaptation,"Sachin Chhabra, Hemanth Venkateswara, Baoxin Li","Georgia State University, Atlanta, GA, USA; Arizona State University, Tempe, AZ, USA",100,USA,0,,"Existing domain adaptation literature comprises multiple techniques that align the labeled source and unlabeled target domains at different stages, and predict the target labels. In a source-free domain adaptation setting, the source data is not available for alignment. We present a source-free generative paradigm that captures the relations between the source categories and enforces them onto the unlabeled target data, thereby circumventing the need for source data without introducing any new hyper-parameters. The adaptation is performed through the adversarial alignment of the posterior probabilities of the source and target categories. The proposed approach demonstrates competitive performance against other source-free domain adaptation techniques and can also be used for source-present settings.",https://openaccess.thecvf.com/content/WACV2023/html/Chhabra_Generative_Alignment_of_Posterior_Probabilities_for_Source-Free_Domain_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chhabra_Generative_Alignment_of_Posterior_Probabilities_for_Source-Free_Domain_Adaptation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030839/,"['Computer vision', 'Adaptation models', 'MIMICs', 'Generators']","['Posterior Probability', 'Domain Adaptation', 'Source-free Domain Adaptation', 'Data Sources', 'Target Domain', 'Target Data', 'Source Domain', 'Target Label', 'Source Categories', 'Unlabeled Target Domain', 'Unlabeled Target Data', 'Loss Function', 'Batch Size', 'Feature Space', 'Generative Adversarial Networks', 'Target Features', 'Uniform Prior', 'Probability Vector', 'Classification Parameters', 'Low Entropy', 'Minimum Entropy', 'Label Space', 'Target Dataset', 'Pseudo Labels', 'Domain Adaptation Methods', 'Feature Alignment', 'Unsupervised Domain Adaptation Methods', 'Source Class', 'Bottom Section', 'Domain Alignment']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",2,"Existing domain adaptation literature comprises multiple techniques that align the labeled source and unlabeled target domains at different stages, and predict the target labels. In a source-free domain adaptation setting, the source data is not available for alignment. We present a source-free generative paradigm that captures the relations between the source categories and enforces them onto the unlabeled target data, thereby circumventing the need for source data without introducing any new hyper-parameters. The adaptation is performed through the adversarial alignment of the posterior probabilities of the source and target categories. The proposed approach demonstrates competitive performance against other source-free domain adaptation techniques and can also be used for source-present settings."
Generative Colorization of Structured Mobile Web Pages,"Kotaro Kikuchi, Naoto Inoue, Mayu Otani, Edgar Simo-Serra, Kota Yamaguchi",Waseda University; CyberAgent,50,Japan,50,Japan,"Color is a critical design factor for web pages, affecting important factors such as viewer emotions and the overall trust and satisfaction of a website. Effective coloring requires design knowledge and expertise, but if this process could be automated through data-driven modeling, efficient exploration and alternative workflows would be possible. However, this direction remains underexplored due to the lack of a formalization of the web page colorization problem, datasets, and evaluation protocols. In this work, we propose a new dataset consisting of e-commerce mobile web pages in a tractable format, which are created by simplifying the pages and extracting canonical color styles with a common web browser. The web page colorization problem is then formalized as a task of estimating plausible color styles for a given web page content with a given hierarchical structure of the elements. We present several Transformer-based methods that are adapted to this task by prepending structural message passing to capture hierarchical relationships between elements. Experimental results, including a quantitative evaluation designed for this task, demonstrate the advantages of our methods over statistical and image colorization methods. The code is available at https://github.com/CyberAgentAILab/webcolor.",https://openaccess.thecvf.com/content/WACV2023/html/Kikuchi_Generative_Colorization_of_Structured_Mobile_Web_Pages_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kikuchi_Generative_Colorization_of_Structured_Mobile_Web_Pages_WACV_2023_paper.pdf,,https://github.com/CyberAgentAILab/webcolor,2212.11541,main,Poster,https://ieeexplore.ieee.org/document/10030322/,"['Data-driven modeling', 'Computer vision', 'Protocols', 'Codes', 'Image color analysis', 'Message passing', 'Web pages']","['Web Page', 'Color Images', 'Hierarchical Relationships', 'Efficient Exploration', 'Transformer-based Methods', 'Maximum Likelihood Estimation', 'Quantitative Results', 'Latent Variables', 'Information Content', 'Alpha Value', 'Autoregressive Model', 'Reference Image', 'Residual Connection', 'Core Model', 'Latent Vector', 'Color Model', 'Dataset Construction', 'RGB Values', 'Structural Color', 'Target Elements', 'Text Color', 'Conditional Variational Autoencoder', 'Transformer Decoder', 'Transformer Encoder']","['Applications: Arts/games/social media', 'Vision + language and/or other modalities']",3,"Color is a critical design factor for web pages, affecting important factors such as viewer emotions and the overall trust and satisfaction of a website. Effective coloring requires design knowledge and expertise, but if this process could be automated through data-driven modeling, efficient exploration and alternative workflows would be possible. However, this direction remains underexplored due to the lack of a formalization of the web page colorization problem, datasets, and evaluation protocols. In this work, we propose a new dataset consisting of e-commerce mobile web pages in a tractable format, which are created by simplifying the pages and extracting canonical color styles with a common web browser. The web page colorization problem is then formalized as a task of estimating plausible color styles for a given web page content with a given hierarchical structure of the elements. We present several Transformer-based methods that are adapted to this task by prepending structural message passing to capture hierarchical relation-ships between elements. Experimental results, including a quantitative evaluation designed for this task, demonstrate the advantages of our methods over statistical and image colorization methods. The code is available at https://github.com/CyberAgentAILab/webcolor."
Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data,"Kazuto Nakashima, Yumi Iwashita, Ryo Kurazume","Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Kyushu University, Fukuoka, Japan",100,"Japan, USA",0,,"3D LiDAR sensors are indispensable for the robust vision of autonomous mobile robots. However, deploying LiDAR-based perception algorithms often fails due to a domain gap from the training environment, such as inconsistent angular resolution and missing properties. Existing studies have tackled the issue by learning inter-domain mapping, while the transferability is constrained by the training configuration and the training is susceptible to peculiar lossy noises called ray-drop. To address the issue, this paper proposes a generative model of LiDAR range images applicable to the data-level domain transfer. Motivated by the fact that LiDAR measurement is based on point-by-point range imaging, we train an implicit image representation-based generative adversarial networks along with a differentiable ray-drop effect. We demonstrate the fidelity and diversity of our model in comparison with the point-based and image-based state-of-the-art generative models. We also showcase upsampling and restoration applications. Furthermore, we introduce a Sim2Real application for LiDAR semantic segmentation. We demonstrate that our method is effective as a realistic ray-drop simulator and outperforms state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Nakashima_Generative_Range_Imaging_for_Learning_Scene_Priors_of_3D_LiDAR_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nakashima_Generative_Range_Imaging_for_Learning_Scene_Priors_of_3D_LiDAR_WACV_2023_paper.pdf,,,2210.1175,main,Poster,https://ieeexplore.ieee.org/document/10030845/,"['Training', 'Adaptation models', 'Laser radar', 'Three-dimensional displays', 'Semantic segmentation', 'Imaging', 'Rendering (computer graphics)']","['Lidar Data', 'Scene Prior', 'Generative Adversarial Networks', 'Semantic Segmentation', 'Training Environment', 'Angular Resolution', 'LiDAR Sensor', 'Automated Guided Vehicles', 'Training Configurations', 'Lidar Measurements', 'Neural Network', 'Continuous-time', 'Point Cloud', 'Natural Images', 'Discrete Distribution', 'Image Domain', 'Domain Adaptation', 'Variational Autoencoder', 'Jensen-Shannon Divergence', 'Domain Adaptation Methods', 'Earth Mover’s Distance', 'Implicit Representation', 'LiDAR Point Clouds', 'Real Domain', 'Deep Generative Models', 'Spherical Projection', 'Point Cloud Generation', 'Latent Code', 'Positional Encoding', 'Invertible Function']","['Applications: Robotics', '3D computer vision']",6,"3D LiDAR sensors are indispensable for the robust vision of autonomous mobile robots. However, deploying LiDAR-based perception algorithms often fails due to a domain gap from the training environment, such as inconsistent angular resolution and missing properties. Existing studies have tackled the issue by learning inter-domain mapping, while the transferability is constrained by the training configuration and the training is susceptible to peculiar lossy noises called ray-drop. To address the issue, this paper proposes a generative model of LiDAR range images applicable to the data-level domain transfer. Motivated by the fact that LiDAR measurement is based on point-by-point range imaging, we train an implicit image representation-based generative adversarial networks along with a differentiable ray-drop effect. We demonstrate the fidelity and diversity of our model in comparison with the point-based and image-based state-of-the-art generative models. We also showcase upsampling and restoration applications. Furthermore, we introduce a Sim2Real application for LiDAR semantic segmentation. We demonstrate that our method is effective as a realistic ray-drop simulator and outperforms state-of-the-art methods."
GeoFill: Reference-Based Image Inpainting With Better Geometric Understanding,"Yunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shechtman, Sohrab Amirghodsi, Charless Fowlkes","Adobe Research; Adobe Research, IFP, UIUC; UC Irvine",66.66666667,USA,33.33333333,USA,"Reference-guided image inpainting restores image pixels by leveraging the content from another single reference image. The primary challenge is how to precisely place the pixels from the reference image into the hole region. Therefore, understanding the 3D geometry that relates pixels between two views is a crucial step towards building a better model. Given the complexity of handling various types of reference images, we focus on the scenario where the images are captured by freely moving the same camera around. Compared to the previous work, we propose a principled approach that does not make heuristic assumptions about the planarity of the scene. We leverage a monocular depth estimate and predict relative pose between cameras, then align the reference image to the target by a differentiable 3D reprojection and a joint optimization of relative pose and depth map scale and offset. Our approach achieves state-of-the-art performance on both RealEstate10K and MannequinChallenge dataset with large baselines, complex geometry and extreme camera motions. We experimentally verify our approach is also better at handling large holes.",https://openaccess.thecvf.com/content/WACV2023/html/Zhao_GeoFill_Reference-Based_Image_Inpainting_With_Better_Geometric_Understanding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhao_GeoFill_Reference-Based_Image_Inpainting_With_Better_Geometric_Understanding_WACV_2023_paper.pdf,,,2201.08131,main,Poster,https://ieeexplore.ieee.org/document/10030438/,"['Geometry', 'Semiconductor device modeling', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Buildings', 'Cameras']","['Image Inpainting', 'Single Image', 'Reference Image', 'Depth Map', 'Joint Optimization', 'Depth Estimation', 'Large Holes', 'Reprojection', 'Camera Motion', 'Relative Pose', 'Large Baseline', 'Hole Region', 'Loss Function', '3D Structure', 'Deep Models', 'Image Pairs', 'Color Difference', 'Perception Of Quality', 'Target Image', 'Linear Problem', 'Source Images', 'Camera Pose', 'Monocular Depth Estimation', 'Pre-trained Deep Models', 'Complex 3D Structures', 'Weight Map', '3D Scene', 'Depth Prediction', 'Source Depth', 'Triangular Mesh']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision']",2,"Reference-guided image inpainting restores image pixels by leveraging the content from another single reference image. The primary challenge is how to precisely place the pixels from the reference image into the hole region. Therefore, understanding the 3D geometry that relates pixels between two views is a crucial step towards building a better model. Given the complexity of handling various types of reference images, we focus on the scenario where the images are captured by freely moving the same camera around. Compared to the previous work, we propose a principled approach that does not make heuristic assumptions about the planarity of the scene. We lever-age a monocular depth estimate and predict relative pose between cameras, then align the reference image to the target by a differentiable 3D reprojection and a joint optimization of relative pose and depth map scale and offset. Our approach achieves state-of-the-art performance on both RealEstate10K and MannequinChallenge dataset with large baselines, complex geometry and extreme camera motions. We experimentally verify our approach is also better at handling large holes."
GliTr: Glimpse Transformers With Spatiotemporal Consistency for Online Action Prediction,"Samrudhdhi B. Rangrej, Kevin J. Liang, Tal Hassner, James J. Clark",Meta AI; McGill University,50,Canada,50,USA,"Many online action prediction models observe complete frames to locate and attend to informative subregions in the frames called glimpses and recognize an ongoing action based on global and local information. However, in applications with constrained resources, an agent may not be able to observe the complete frame, yet must still locate useful glimpses to predict an incomplete action based on local information only. In this paper, we develop Glimpse Transformers (GliTr), which observe only narrow glimpses at all times, thus predicting an ongoing action and the following most informative glimpse location based on the partial spatiotemporal information collected so far. In the absence of a ground truth for the optimal glimpse locations for action recognition, we train GliTr using a novel spatiotemporal consistency objective: We require GliTr to attend to the glimpses with features similar to the corresponding complete frames (i.e. spatial consistency) and the resultant class logits at time t equivalent to the ones predicted using whole frames up to t (i.e. temporal consistency). Inclusion of our proposed consistency objective yields  10% higher accuracy on the Something-Something-v2 (SSv2) dataset than the baseline cross-entropy objective. Overall, despite observing only  33% of the total area per frame, GliTr achieves 53.02% and 93.91% accuracy on the SSv2 and Jester datasets, respectively.",https://openaccess.thecvf.com/content/WACV2023/html/Rangrej_GliTr_Glimpse_Transformers_With_Spatiotemporal_Consistency_for_Online_Action_Prediction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rangrej_GliTr_Glimpse_Transformers_With_Spatiotemporal_Consistency_for_Online_Action_Prediction_WACV_2023_paper.pdf,,,2210.13605,main,Poster,https://ieeexplore.ieee.org/document/10030310/,"['Computer vision', 'Predictive models', 'Transformers', 'Cameras', 'Spatiotemporal phenomena', 'Sensors', 'Observability']","['Online Activities', 'Activity Prediction', 'Spatio-temporal Consistency', 'Local Information', 'Global Information', 'Partial Information', 'Action Recognition', 'Temporal Consistency', 'Spatial Consistency', 'Upper Bound', 'Spatial Information', 'Teacher Model', 'GB Memory', 'Early Prediction', 'Linear Classifier', 'Classification Loss', 'Training Objective', 'Current Frame', 'Student Model', 'Partial Observation', 'Online Fashion', 'Consistency Loss', 'Video Action Recognition', 'Action Recognition Model', 'Complete Video', 'Base Learning Rate', 'Position Embedding', 'Entire Video']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Many online action prediction models observe complete frames to locate and attend to informative subregions in the frames called glimpses and recognize an ongoing action based on global and local information. However, in applications with constrained resources, an agent may not be able to observe the complete frame, yet must still locate useful glimpses to predict an incomplete action based on local information only. In this paper, we develop Glimpse Transformers (GliTr), which observe only narrow glimpses at all times, thus predicting an ongoing action and the following most informative glimpse location based on the partial spatiotemporal information collected so far. In the absence of a ground truth for the optimal glimpse locations for action recognition, we train GliTr using a novel spatiotemporal consistency objective: We require GliTr to attend to the glimpses with features similar to the corresponding complete frames (i.e. spatial consistency) and the resultant class logits at time t equivalent to the ones predicted using whole frames up to t (i.e. temporal consistency). Inclusion of our proposed consistency objective yields ∼ 10% higher accuracy on the Something-Something-v2 (SSv2) dataset than the baseline cross-entropy objective. Overall, despite observing only ∼ 33% of the total area per frame, GliTr achieves 53.02% and 93.91% accuracy on the SSv2 and Jester datasets, respectively."
Global-Local Self-Distillation for Visual Representation Learning,"Tim Lebailly, Tinne Tuytelaars",KU Leuven,100,Belgium,0,,"The downstream accuracy of self-supervised methods is tightly linked to the proxy task solved during training and the quality of the gradients extracted from it. Richer and more meaningful gradients updates are key to allow self-supervised methods to learn better and in a more efficient manner. In a typical self-distillation framework, the representation of two augmented images are enforced to be coherent at the global level. Nonetheless, incorporating local cues in the proxy task can be beneficial and improve the model accuracy on downstream tasks. This leads to a dual objective in which, on the one hand, coherence between global-representations is enforced and on the other, coherence between local-representations is enforced. Unfortunately, an exact correspondence mapping between two sets of local-representations does not exist making the task of matching local-representations from one augmentation to another non-trivial. We propose to leverage the spatial information in the input images to obtain geometric matchings and compare this geometric approach against previous methods based on similarity matchings. Our study shows that not only 1) geometric matchings perform better than similarity based matchings in low-data regimes but also 2) that similarity based matchings are highly hurtful in low-data regimes compared to the vanilla baseline without local self-distillation. The code is available at https://github.com/tileb1/global-local-self-distillation.",https://openaccess.thecvf.com/content/WACV2023/html/Lebailly_Global-Local_Self-Distillation_for_Visual_Representation_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lebailly_Global-Local_Self-Distillation_for_Visual_Representation_Learning_WACV_2023_paper.pdf,,https://github.com/tileb1/global-local-self-distillation,2207.14676,main,Poster,https://ieeexplore.ieee.org/document/10030381/,"['Training', 'Representation learning', 'Visualization', 'Computer vision', 'Codes', 'Coherence', 'Task analysis']","['Visual Representation', 'Representation Learning', 'Input Image', 'Vanilla', 'Local Cues', 'Image Augmentation', 'Training Set', 'Parametrized', 'Similar Set', 'Matching Model', 'Overlap Region', 'Local Loss', 'Self-supervised Learning', 'Forward Pass', 'Global Representation', 'Image X', 'Dense Representation', 'Multiple Resolutions', 'Positional Encoding', 'Vision Transformer', 'Matching Distance', 'SOTA Methods', 'Small-scale Datasets', 'Self-supervised Training', 'Geometric Loss', 'Notable Work', 'Teacher Network', 'Benchmark', 'Pair Representation', 'Row Block']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",2,"The downstream accuracy of self-supervised methods is tightly linked to the proxy task solved during training and the quality of the gradients extracted from it. Richer and more meaningful gradients updates are key to allow self-supervised methods to learn better and in a more efficient manner. In a typical self-distillation framework, the representation of two augmented images are enforced to be coherent at the global level. Nonetheless, incorporating local cues in the proxy task can be beneficial and improve the model accuracy on downstream tasks. This leads to a dual objective in which, on the one hand, coherence between global-representations is enforced and on the other, coherence between local-representations is enforced. Unfortunately, an exact correspondence mapping between two sets of local-representations does not exist making the task of matching local-representations from one augmentation to another non-trivial. We propose to leverage the spatial information in the input images to obtain geometric matchings and compare this geometric approach against previous methods based on similarity matchings. Our study shows that not only 1) geometric matchings perform better than similarity based matchings in low-data regimes but also 2) that similarity based matchings are highly hurtful in low-data regimes compared to the vanilla baseline without local self-distillation. The code is available at https://github.com/tileb1/global-local-self-distillation."
GlobalFlowNet: Video Stabilization Using Deep Distilled Global Motion Estimates,"Jerin Geo James, Devansh Jain, Ajit Rajwade",Indian Institute of Technology Bombay,100,India,0,,"Videos shot by laymen using hand-held cameras contain undesirable shaky motion. Estimating the global motion between successive frames, in a manner not influenced by moving objects, is central to many video stabilization techniques, but poses significant challenges. A large body of work uses 2D affine transformations or homography for the global motion. However, in this work, we introduce a more general representation scheme, which adapts any existing optical flow network to ignore the moving objects and obtain a spatially smooth approximation of the global motion between video frames. We achieve this by a knowledge distillation approach, where we first introduce a low pass filter module into the optical flow network to constrain the predicted optical flow to be spatially smooth. This becomes our student network, named as GLOBALFLOWNET. Then, using the original optical flow network as the teacher network, we train the student network using a robust loss function. Given a trained GLOBALFLOWNET, we stabilize videos using a two stage process. In the first stage, we correct the instability in affine parameters using a quadratic programming approach constrained by a user-specified cropping limit to control loss of field of view. In the second stage, we stabilize the video further by smoothing global motion parameters, expressed using small number of discrete cosine transform coefficients. In extensive experiments on a variety of different videos, our technique outperforms state of the art techniques in terms of subjective quality and different quantitative measures of video stability. Additionally, we present a new measure for evaluation of video stabilization based on the flow generated by GLOBALFLOWNET and argue that it is based on a more general motion model in contrast to the affine motion model on which most existing measures are based. The source code is publicly available at https://github.com/GlobalFlowNet/GlobalFlowNet",https://openaccess.thecvf.com/content/WACV2023/html/James_GlobalFlowNet_Video_Stabilization_Using_Deep_Distilled_Global_Motion_Estimates_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/James_GlobalFlowNet_Video_Stabilization_Using_Deep_Distilled_Global_Motion_Estimates_WACV_2023_paper.pdf,,https://github.com/GlobalFlowNet/GlobalFlowNet,2210.13769,main,Poster,https://ieeexplore.ieee.org/document/10030085/,"['Optical losses', 'Smoothing methods', 'Source coding', 'Transforms', 'Stability analysis', 'Motion measurement', 'Discrete cosine transforms']","['Global Estimates', 'Motion Estimation', 'Global Motion', 'Video Stabilization', 'Quality Measures', 'Video Frames', 'Motion Parameters', 'Optical Flow', 'Motion Model', 'Affine Transformation', 'Quadratic Programming', 'Teacher Network', 'Discrete Cosine Transform', 'Optical Networks', 'Student Network', 'Affine Parameter', 'Robust Loss', 'Hand-held Camera', 'Deep Neural Network', 'Consecutive Frames', 'Warp Field', 'Pair Of Frames', 'Regional Flow', 'Salient Points', 'Consecutive Video Frames', 'Temporal Smoothing', 'Smoothing Parameter', 'Residual Motion', 'Regional Motion']","['Algorithms: Computational photography', 'image and video synthesis']",7,"Videos shot by laymen using hand-held cameras contain undesirable shaky motion. Estimating the global motion between successive frames, in a manner not influenced by moving objects, is central to many video stabilization techniques, but poses significant challenges. A large body of work uses 2D affine transformations or homography for the global motion. However, in this work, we introduce a more general representation scheme, which adapts any existing optical flow network to ignore the moving objects and obtain a spatially smooth approximation of the global motion between video frames. We achieve this by a knowledge distillation approach, where we first introduce a low pass filter module into the optical flow network to constrain the predicted optical flow to be spatially smooth. This becomes our student network, named as GlobalFlowNet. Then, using the original optical flow network as the teacher network, we train the student network using a robust loss function. Given a trained GlobalFlowNet, we stabilize videos using a two stage process. In the first stage, we correct the instability in affine parameters using a quadratic programming approach constrained by a user-specified cropping limit to control loss of field of view. In the second stage, we stabilize the video further by smoothing global motion parameters, expressed using a small number of discrete cosine transform coefficients. In extensive experiments on a variety of different videos, our technique outperforms state of the art techniques in terms of subjective quality and different quantitative measures of video stability. Additionally, we present a new measure for evaluation of video stabilization based on the flow generated by GlobalFlowNet and argue that it is based on a more general motion model in contrast to the affine motion model on which most existing measures are based. The source code is publicly available at https://github.com/GlobalFlowNet/GlobalFlowNet"
Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors,"Tobias Riedlinger, Matthias Rottmann, Marius Schubert, Hanno Gottschalk","School of Mathematics and Natural Sciences, IZMD, University of Wuppertal, Germany; School of Computer and Communication Sciences, CVLab, EPFL, Switzerland; School of Mathematics and Natural Sciences, IZMD, University of Wuppertal, Germany",100,"Germany, Switzerland",0,,"The majority of uncertainty quantification methods for deep object detectors are based on the network output, such as sampling strategies like Monte-Carlo dropout or deep ensembles with straight-forward transfers to object detection. Here, we study gradient-based uncertainty features for object detection. We show that they contain information orthogonal to that of common, output-based uncertainty approximation methods. Meta classification and meta regression are used to produce confidence estimates using gradient features and other methods which are applicable to numerous object detection architectures. Our results show that gradient uncertainty itself performs on par with stateof-the-art methods across different detectors and datasets. We find that combined meta classifiers outperform standalone models. This suggests that sampling strategies may be supplemented by gradient-based uncertainty to obtain improved confidences, contributing to the probabilistic reliability of object detectors in down-stream applications.",https://openaccess.thecvf.com/content/WACV2023/html/Riedlinger_Gradient-Based_Quantification_of_Epistemic_Uncertainty_for_Deep_Object_Detectors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Riedlinger_Gradient-Based_Quantification_of_Epistemic_Uncertainty_for_Deep_Object_Detectors_WACV_2023_paper.pdf,,,2107.04517,main,Poster,https://ieeexplore.ieee.org/document/10030773/,"['Uncertainty', 'Three-dimensional displays', 'Monte Carlo methods', 'Object detection', 'Detectors', 'Computer architecture', 'Probabilistic logic']","['Object Detection', 'Uncertainty Quantification', 'Epistemic Uncertainty', 'Deep Object Detection', 'Meta-regression', 'Network Output', 'Confidence Estimation', 'Gradient Features', 'Hidden Layer', 'Bounding Box', 'Score Threshold', 'Deep Features', 'Classification Output', 'Prediction Confidence', 'Instance Segmentation', 'Uncertainty Information', 'Forward Pass', 'Floating-point Operations', 'Objective Scores', 'Small Threshold', 'Deep Artificial Neural Networks', 'Non-maximum Suppression', 'Pedestrian Detection', 'Meta-regression Model', 'Aleatoric Uncertainty', 'KITTI Dataset', 'Sampling-based Methods', 'Variational Inference', 'Semantic Segmentation', 'Full Baseline']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"The majority of uncertainty quantification methods for deep object detectors are based on the network output, such as sampling strategies like Monte-Carlo dropout or deep ensembles with straight-forward transfers to object detection. Here, we study gradient-based uncertainty features for object detection. We show that they contain information orthogonal to that of common, output-based uncertainty approximation methods. Meta classification and meta regression are used to produce confidence estimates using gradient features and other methods which are applicable to numerous object detection architectures. Our results show that gradient uncertainty itself performs on par with state-of-the-art methods across different detectors and datasets. We find that combined meta classifiers outperform standalone models. This suggests that sampling strategies may be supplemented by gradient-based uncertainty to obtain improved confidences, contributing to the probabilistic reliability of object detectors in down-stream applications."
Graph-Based Self-Learning for Robust Person Re-Identification,"Yuqiao Xian, Jinrui Yang, Fufu Yu, Jun Zhang, Xing Sun","School of Computer Science and Engineering, Sun Yat-sen University, China; Youtu Lab, Tencent",100,China,0,,"Existing deep learning approaches for person re-identification (Re-ID) mostly rely on large-scale and well-annotated training data. However, human-annotated labels are prone to label noise in real-world applications. Previous person Re-ID works mainly focus on random label noise, which doesn't properly reflect the characteristic of label noise in practical human-annotated process. In this work, we find the visual ambiguity noise is more common and reasonable noise assumption in annotation of person Re-ID. To handle the kind of noise, we propose a simple and effective robust person Re-ID framework, namely Graph-Based Self-Learning (GBSL), to iteratively learn discriminative representation and rectify noisy labels with limited annotated samples for each identity. Meanwhile, considering the practical annotation process in person Re-ID, we further extend the visual ambiguity noise assumption and propose a type of more practical label noise in person Re-ID, namely the tracklet-level label noise (TLN). Without modifying network architecture or loss function, our approach significantly improves the robustness against label noise of the Re-ID system. Our model obtains competitive performance with training data corrupted by various types of label noise and outperforms the existing methods for robust Re-ID on public benchmarks.",https://openaccess.thecvf.com/content/WACV2023/html/Xian_Graph-Based_Self-Learning_for_Robust_Person_Re-Identification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xian_Graph-Based_Self-Learning_for_Robust_Person_Re-Identification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030111/,"['Training', 'Deep learning', 'Visualization', 'Computer vision', 'Annotations', 'Training data', 'Network architecture']","['Deep Learning', 'Random Noise', 'Real-world Applications', 'Types Of Noise', 'Annotation Process', 'Identical Samples', 'Public Benchmark', 'Kinds Of Noise', 'Noisy Labels', 'Label Noise', 'Deep Network', 'Deep Neural Network', 'Image Classification', 'Precision And Recall', 'Bounding Box', 'Discriminative Features', 'Representation Learning', 'Video Frames', 'Propagation Model', 'Low Precision', 'Incorrect Labels', 'Correct Label', 'Robust Architecture', 'Similar Appearance', 'Different Types Of Noise', 'Noise Samples', 'Noise Rate', 'Person Image', 'Clean Dataset', 'Re-identification Task']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Biometrics', 'face', 'gesture', 'body pose']",5,"Existing deep learning approaches for person re-identification (Re-ID) mostly rely on large-scale and well-annotated training data. However, human-annotated labels are prone to label noise in real-world applications. Previous person Re-ID works mainly focus on random label noise, which doesn’t properly reflect the characteristic of label noise in practical human-annotated process. In this work, we find the visual ambiguity noise is more common and reasonable noise assumption in annotation of person Re-ID. To handle the kind of noise, we propose a simple and effective robust person Re-ID framework, namely Graph-Based Self-Learning (GBSL), to iteratively learn discriminative representation and rectify noisy labels with limited annotated samples for each identity. Meanwhile, considering the practical annotation process in person Re-ID, we further extend the visual ambiguity noise assumption and propose a type of more practical label noise in person Re-ID, namely the tracklet-level label noise (TLN). Without modifying network architecture or loss function, our approach significantly improves the robustness against label noise of the Re-ID system. Our model obtains competitive performance with training data corrupted by various types of label noise and outperforms the existing methods for robust Re-ID on public benchmarks."
Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing,"Aditay Tripathi, Anand Mishra, Anirban Chakraborty",Indian Institute of Science; Indian Institute of Technology Jodhpur,100,India,0,,"This paper presents a framework for jointly grounding objects that follow certain semantic relationship constraints given in a scene graph. A typical natural scene contains several objects, often exhibiting visual relationships of varied complexities between them. These inter-object relationships provide strong contextual cues towards improving grounding performance compared to a traditional object query-only-based localization task. A scene graph is an efficient and structured way to represent all the objects and their semantic relationships in the image. In an attempt towards bridging these two modalities representing scenes and utilizing contextual information for improving object localization, we rigorously study the problem of grounding scene graphs on natural images. To this end, we propose a novel graph neural network-based approach referred to as Visio-Lingual Message PAssing Graph Neural Network (VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object proposals as nodes and an edge between a pair of nodes representing a plausible relation between them. Then a three-step inter-graph and intra-graph message passing is performed to learn the context-dependent representation of the proposals and query objects. These object representations are used to score the proposals to generate object localization. The proposed method significantly outperforms the baselines on four public datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Tripathi_Grounding_Scene_Graphs_on_Natural_Images_via_Visio-Lingual_Message_Passing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tripathi_Grounding_Scene_Graphs_on_Natural_Images_via_Visio-Lingual_Message_Passing_WACV_2023_paper.pdf,https://iiscaditaytripathi.github.io/sgl/,https://github.com/iiscaditaytripathi/sgl,2211.01969,main,Poster,https://ieeexplore.ieee.org/document/10030854/,"['Location awareness', 'Visualization', 'Grounding', 'Message passing', 'Image edge detection', 'Semantics', 'Directed graphs']","['Natural Images', 'Message Passing', 'Scene Graph', 'Neural Network', 'Public Datasets', 'Semantic Similarity', 'Object Location', 'Directed Graph', 'Natural Scenes', 'Graph Neural Networks', 'Related Images', 'Object Proposals', 'Visual Relationship', 'Computer Vision', 'Intersection Over Union', 'Bounding Box', 'Representation Learning', 'Multiple Objects', 'Nodes In The Graph', 'Graph Convolutional Network', 'Node Representations', 'Region Proposal', 'Unseen Objects', 'Region Proposal Network', 'Objects In The Scene', 'Positive Edges', 'Ground-truth Bounding Box', 'Graph Generation', 'Set Of Proposals', 'Conditional Random Field']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",1,"This paper presents a framework for jointly grounding objects that follow certain semantic relationship constraints given in a scene graph. A typical natural scene contains several objects, often exhibiting visual relationships of varied complexities between them. These inter-object relationships provide strong contextual cues towards improving grounding performance compared to a traditional object query-only-based localization task. A scene graph is an efficient and structured way to represent all the objects and their semantic relationships in the image. In an attempt towards bridging these two modalities representing scenes and utilizing contextual information for improving object localization, we rigorously study the problem of grounding scene graphs on natural images. To this end, we propose a novel graph neural network-based approach referred to as Visio-Lingual Message Passing Graph Neural Network (VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object proposals as nodes and an edge between a pair of nodes representing a plausible relation between them. Then a three-step inter-graph and intra-graph message passing is performed to learn the context- dependent representation of the proposals and query objects. These object representations are used to score the proposals to generate object localization. The proposed method significantly outperforms the baselines on four public datasets."
Guiding Users to Where To Give Color Hints for Efficient Interactive Sketch Colorization via Unsupervised Region Prioritization,"Youngin Cho, Junsoo Lee, Soyoung Yang, Juntae Kim, Yeojeong Park, Haneol Lee, Mohammad Azam Khan, Daesik Kim, Jaegul Choo",UNIST; Korea University; NAVER WEBTOON AI; KAIST AI,75,South Korea,25,South Korea,"Existing deep interactive colorization models have focused on ways to utilize various types of interactions, such as point-wise color hints, scribbles, or natural-language texts, as methods to reflect a user's intent at runtime. However, another approach, which actively informs the user of the most effective regions to give hints for sketch image colorization, has been under-explored. This paper proposes a novel model-guided deep interactive colorization framework that reduces the required amount of user interactions, by prioritizing the regions in a colorization model. Our method, called GuidingPainter, prioritizes these regions where the model most needs a color hint, rather than just relying on the user's manual decision on where to give a color hint. In our extensive experiments, we show that our approach outperforms existing interactive colorization methods in terms of the conventional metrics, such as PSNR and FID, and reduces required amount of interactions.",https://openaccess.thecvf.com/content/WACV2023/html/Cho_Guiding_Users_to_Where_To_Give_Color_Hints_for_Efficient_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cho_Guiding_Users_to_Where_To_Give_Color_Hints_for_Efficient_WACV_2023_paper.pdf,,,2210.1427,main,Poster,https://ieeexplore.ieee.org/document/10030307/,"['Training', 'Measurement', 'Image quality', 'Computer vision', 'Runtime', 'Image color analysis', 'Computational modeling']","['Color Hints', 'Color Images', 'Human-computer Interaction', 'Amount Of Interaction', 'Color Model', 'Scribble', 'Various Types Of Interactions', 'Fréchet Inception Distance', 'Objective Function', 'Convolutional Neural Network', 'Training Time', 'Total Loss', 'Image Segmentation', 'Interactive Process', 'Grayscale Images', 'Image Generation', 'Position Vector', 'Inference Time', 'Output Image', 'Semantic Regions', 'Image X', 'Realistic Images', 'Smoothness Loss', 'Reconstruction Loss', 'Geometric Distribution', 'Content Creation', 'Single Color']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"Existing deep interactive colorization models have focused on ways to utilize various types of interactions, such as point-wise color hints, scribbles, or natural-language texts, as methods to reflect a user’s intent at runtime. However, another approach, which actively informs the user of the most effective regions to give hints for sketch image colorization, has been under-explored. This paper proposes a novel model-guided deep interactive colorization framework that reduces the required amount of user interactions, by prioritizing the regions in a colorization model. Our method, called GuidingPainter, prioritizes these regions where the model most needs a color hint, rather than just relying on the user’s manual decision on where to give a color hint. In our extensive experiments, we show that our approach outperforms existing interactive colorization methods in terms of the conventional metrics, such as PSNR and FID, and reduces required amount of interactions."
Guiding Visual Question Answering With Attention Priors,"Thao Minh Le, Vuong Le, Sunil Gupta, Svetha Venkatesh, Truyen Tran","Applied Artificial Intelligence Institute, Deakin University, Australia",100,Australia,0,,"The current success of modern visual reasoning systems is arguably attributed to cross-modality attention mechanisms. However, in deliberative reasoning such as in VQA, attention is unconstrained at each step, and thus may serve as a statistical pooling mechanism rather than a semantic operation intended to select information relevant to inference. This is because at training time, attention is only guided by a very sparse signal (i.e. the answer label) at the end of the inference chain. This causes the cross-modality attention weights to deviate from the desired visual-language bindings. To rectify this deviation, we propose to guide the attention mechanism using explicit linguistic-visual grounding. This grounding is derived by connecting structured linguistic concepts in the query to their referents among the visual objects. Here we learn the grounding from the pairing of questions and images alone, without the need for answer annotation or external grounding supervision. This grounding guides the attention mechanism inside VQA models through a duality of mechanisms: pre-training attention weight calculation and directly guiding the weights at inference time on a case-by-case basis. The resultant algorithm is capable of probing attention-based reasoning models, injecting relevant associative knowledge, and regulating the core reasoning process. This scalable enhancement improves the performance of VQA models, fortifies their robustness to limited access to supervised data, and increases interpretability.",https://openaccess.thecvf.com/content/WACV2023/html/Le_Guiding_Visual_Question_Answering_With_Attention_Priors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Le_Guiding_Visual_Question_Answering_With_Attention_Priors_WACV_2023_paper.pdf,,,2205.12616,main,Poster,https://ieeexplore.ieee.org/document/10030971/,"['Training', 'Visualization', 'Systematics', 'Grounding', 'Semantics', 'Linguistics', 'Cognition']","['Visual Question Answering', 'Semantic', 'Pairing', 'Attention Mechanism', 'Modern Systems', 'Visual Object', 'Inference Time', 'Reasoning Process', 'Attention Weights', 'Sparse Signal', 'Visual Reasoning', 'Need For Annotation', 'Image Regions', 'Kullback-Leibler', 'Image Pairs', 'Visual Attention', 'Types Of Questions', 'Attention Model', 'Self-supervised Learning', 'Gating Mechanism', 'Joint Attention', 'Attention Scores', 'Prior Learning', 'Unsupervised Manner', 'Grammatical Structure', 'Parse Tree', 'Attention Matrix', 'Point Of Attention', 'Inductive Bias', 'Noun Phrase']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"The current success of modern visual reasoning systems is arguably attributed to cross-modality attention mechanisms. However, in deliberative reasoning such as in VQA, attention is unconstrained at each step, and thus may serve as a statistical pooling mechanism rather than a semantic operation intended to select information relevant to inference. This is because at training time, attention is only guided by a very sparse signal (i.e. the answer label) at the end of the inference chain. This causes the cross-modality attention weights to deviate from the desired visual-language bindings. To rectify this deviation, we propose to guide the attention mechanism using explicit linguistic-visual grounding. This grounding is derived by connecting structured linguistic concepts in the query to their referents among the visual objects. Here we learn the grounding from the pairing of questions and images alone, without the need for answer annotation or external grounding supervision. This grounding guides the attention mechanism inside VQA models through a duality of mechanisms: pre-training attention weight calculation and directly guiding the weights at inference time on a case- by-case basis. The resultant algorithm is capable of probing attention-based reasoning models, injecting relevant associative knowledge, and regulating the core reasoning process. This scalable enhancement improves the performance of VQA models, fortifies their robustness to limited access to supervised data, and increases interpretability."
HIME: Efficient Headshot Image Super-Resolution With Multiple Exemplars,"Xiaoyu Xiang, Jon Morton, Fitsum A. Reda, Lucas D. Young, Federico Perazzi, Rakesh Ranjan, Amit Kumar, Andrea Colaco, Jan P. Allebach","Purdue University; Meta Reality Labs; Purdue University, Meta Reality Labs",66.66666667,USA,33.33333333,USA,"A promising direction for recovering the lost information in low-resolution headshot images is utilizing a set of high-resolution exemplars from the same identity. Complementary images in the reference set can improve the generated headshot quality across many different views and poses. However, it is challenging to make the best use of multiple exemplars: the quality and alignment of each exemplar cannot be guaranteed. Using low-quality and mismatched images as references will impair the output results. To overcome these issues, we propose the efficient Headshot Image Super-Resolution with Multiple Exemplars network (HIME) method. Compared with previous methods, our network can effectively handle the misalignment between the input and the reference without requiring facial priors and learn the aggregated reference set representation in an end-to-end manner. Furthermore, to reconstruct more detailed facial features, we propose a correlation loss that provides a rich representation of the local texture in a controllable spatial range. Experimental results demonstrate that the proposed framework not only has significantly fewer computation cost than recent exemplar-guided methods but also achieves better qualitative and quantitative performance.",https://openaccess.thecvf.com/content/WACV2023/html/Xiang_HIME_Efficient_Headshot_Image_Super-Resolution_With_Multiple_Exemplars_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xiang_HIME_Efficient_Headshot_Image_Super-Resolution_With_Multiple_Exemplars_WACV_2023_paper.pdf,,,2203.14863,main,Poster,https://ieeexplore.ieee.org/document/10030439/,"['Computer vision', 'Costs', 'Correlation', 'Superresolution', 'Image reconstruction', 'Facial features']","['Multiple Exemplars', 'Computational Cost', 'Low-resolution Images', 'Local Texture', 'High-resolution Images', 'Convolutional Layers', 'Window Size', 'Input Image', 'Feature Maps', 'Reference Image', 'Transfer Characteristics', 'Corresponding Points', 'Residual Block', 'Peak Signal-to-noise Ratio', 'Optical Flow', 'Feature Matching', 'Feature Aggregation', 'Reconstruction Loss', 'Feature Alignment', 'Low-resolution Feature', 'Low-resolution Input', 'Larger Window Size', 'High-resolution Reference', 'Single Image Super-resolution', 'Similarity Score', 'Perceptual Loss', 'Videoconferencing', 'Image Guidance', 'High-resolution Features', 'Network Reconstruction']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",,"A promising direction for recovering the lost information in low-resolution headshot images is utilizing a set of high-resolution exemplars from the same identity. Complementary images in the reference set can improve the generated headshot quality across many different views and poses. However, it is challenging to make the best use of multiple exemplars: the quality and alignment of each exemplar cannot be guaranteed. Using low-quality and mismatched images as references will impair the output results. To overcome these issues, we propose the Headshot Image Super-Resolution with Multiple Exemplars network (HIME) method. Compared with previous methods, our network can effectively handle the misalignment between the input and the reference without requiring facial priors and learn the aggregated reference set representation in an end-to-end manner. Furthermore, to reconstruct more detailed facial features, we propose a correlation loss that provides a rich representation of the local texture in a controllable spatial range. Experimental results demonstrate that the proposed framework not only has significantly fewer computation cost than recent exemplar-guided methods but also achieves better qualitative and quantitative performance."
HOOT: Heavy Occlusions in Object Tracking Benchmark,"Gozde Sahin, Laurent Itti","University of Southern California, University Park, Los Angeles",100,USA,0,,"In this paper, we present HOOT, the Heavy Occlusions in Object Tracking Benchmark, a new visual object tracking dataset aimed towards handling high occlusion scenarios for single-object tracking tasks. The benchmark consists of 581 high-quality videos, which have 436K frames densely annotated with rotated bounding boxes for the targets spanning 74 object classes. The dataset is geared for development, evaluation and analysis of visual tracking algorithms that are robust to occlusions. It is comprised of videos with high occlusion levels, where the median percentage of occluded frames per-video is 68%. It also provides critical attributes on occlusions, which include defining a taxonomy for occluders, providing occlusion masks for every bounding box, per-frame partial/full occlusion labels and more. HOOT has been compiled to encourage development of new methods targeting occlusion handling in visual tracking, by providing training and test splits with high occlusion levels. This makes HOOT the first densely-annotated, large dataset designed for single-object tracking under severe occlusion. We evaluate 15 state-of-the-art trackers on this new dataset to act as a baseline for future work focusing on occlusions.",https://openaccess.thecvf.com/content/WACV2023/html/Sahin_HOOT_Heavy_Occlusions_in_Object_Tracking_Benchmark_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sahin_HOOT_Heavy_Occlusions_in_Object_Tracking_Benchmark_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030507/,"['Training', 'Visualization', 'Target tracking', 'Protocols', 'Taxonomy', 'Focusing', 'Benchmark testing']","['Object Tracking', 'Tracking Benchmark', 'Object Tracking Benchmark', 'Bounding Box', 'Tracking Algorithm', 'Evaluation Of Algorithms', 'Test Split', 'Severe Occlusion', 'Tracking Dataset', 'Occlusion Level', 'Computer Vision', 'Visual Information', 'Pedestrian', 'Intersection Over Union', 'Autonomous Vehicles', 'Tracking Performance', 'Evaluation Protocol', 'Similar Objects', 'Evaluation Dataset', 'Partial Occlusion', 'Target Frame', 'Field Dataset', 'Ground-truth Box', 'Long-term Tracking', 'Benchmark Evaluation', 'Multi-object Tracking', 'Everyday Objects', 'Parallax', 'Single Tracking']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",,"In this paper, we present HOOT, the Heavy Occlusions in Object Tracking Benchmark, a new visual object tracking dataset aimed towards handling high occlusion scenarios for single-object tracking tasks. The dataset consists of 581 high-quality videos, which have 436K frames densely annotated with rotated bounding boxes for targets spanning 74 object classes. The dataset is geared for development, evaluation and analysis of visual tracking algorithms that are robust to occlusions. It is comprised of videos with high occlusion levels, where the median percentage of occluded frames per-video is 68%. It also provides critical attributes on occlusions, which include defining a taxonomy for occluders, providing occlusion masks for every bounding box, per-frame partial/full occlusion labels and more. HOOT has been compiled to encourage development of new methods targeting occlusion handling in visual tracking, by providing training and test splits with high occlusion levels. This makes HOOT the first densely-annotated, large dataset designed for single-object tracking under severe occlusion. We evaluate 15 state-of-the-art trackers on this new dataset to act as a baseline for future work focusing on occlusions."
HandGCNFormer: A Novel Topology-Aware Transformer Network for 3D Hand Pose Estimation,"Yintong Wang, LiLi Chen, Jiamao Li, Xiaolin Zhang","Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Xiongan Institute of Innovation, Xiongan, 071700, China; University of Science and Technology of China, Hefei, Anhui, 230027, China; ShanghaiTech University, Shanghai 201210, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Xiongan Institute of Innovation, Xiongan, 071700, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China",100,China,0,,"Despite the substantial progress in 3D hand pose estimation, inferring plausible and accurate poses in the presence of severe self-occlusion and high self-similarity remains an inherent challenge. To mitigate the ambiguity arising from invisible and similar joints, we propose a novel Topology-aware Transformer network named HandGCNFormer, incorporating the prior knowledge of hand kinematic topology into the network while modeling long-range context information. Specifically, we present a novel Graphformer decoder with an additional node-offset graph convolutional layer (NoffGConv) that optimizes the synergy of Transformer and GCN, capturing long-range dependencies as well as local topology connection between joints. Furthermore, we replace the standard MLP prediction head with a novel Topology-aware head to better utilize local topology constraints for more plausible and accurate poses. Our method achieves state-of-the-art performance on four challenging datasets including Hands2017, NYU, ICVL, and MSRA.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_HandGCNFormer_A_Novel_Topology-Aware_Transformer_Network_for_3D_Hand_Pose_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_HandGCNFormer_A_Novel_Topology-Aware_Transformer_Network_for_3D_Hand_Pose_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030397/,"['Three-dimensional displays', 'Head', 'Network topology', 'Pose estimation', 'Kinematics', 'Transformers', 'Topology']","['Pose Estimation', 'Pose Estimation Network', 'Hand Pose', 'Hand Pose Estimation', '3D Hand Pose Estimation', 'Invisible', 'Local Connectivity', 'Graph Convolutional Network', 'Graph Convolution', 'Long-range Dependencies', 'Human Pose Estimation', '3D Pose', 'Local Topology', 'Accurate Pose', 'Feature Maps', '3D Space', 'Feed-forward Network', 'Encoder-decoder', 'Attention Module', 'Depth Images', 'Hand Joints', 'Cropped Images', 'Neighboring Nodes', 'Transformer Encoder', 'Self-attention Layer', 'Image Encoder', 'Decoder Layer', 'Transformer Decoder', 'Node Features', 'Decoder Block']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision']",4,"Despite the substantial progress in 3D hand pose estimation, inferring plausible and accurate poses in the presence of severe self-occlusion and high self-similarity remains an inherent challenge. To mitigate the ambiguity arising from invisible and similar joints, we propose a novel Topology-aware Transformer network named HandGCNFormer, incorporating the prior knowledge of hand kinematic topology into the network while modeling long-range context information. Specifically, we present a novel Graphformer decoder with an additional node-offset graph convolutional layer (NoffGConv) that optimizes the synergy of Transformer and GCN, capturing long-range dependencies as well as local topology connection between joints. Furthermore, we replace the standard MLP prediction head with a novel Topology-aware head to better utilize local topology constraints for more plausible and accurate poses. Our method achieves state-of-the-art performance on four challenging datasets including Hands2017, NYU, ICVL, and MSRA."
Handling Image and Label Resolution Mismatch in Remote Sensing,"Scott Workman, Armin Hadzic, M. Usman Rafique",Kitware Inc.; DZYNE Technologies,50,USA,50,USA,"Though semantic segmentation has been heavily explored in vision literature, unique challenges remain in the remote sensing domain. One such challenge is how to handle resolution mismatch between overhead imagery and ground-truth label sources, due to differences in ground sample distance. To illustrate this problem, we introduce a new dataset and use it to showcase weaknesses inherent in existing strategies that naively upsample the target label to match the image resolution. Instead, we present a method that is supervised using low-resolution labels (without upsampling), but takes advantage of an exemplar set of high-resolution labels to guide the learning process. Our method incorporates region aggregation, adversarial learning, and self-supervised pretraining to generate fine-grained predictions, without requiring high-resolution annotations. Extensive experiments demonstrate the real-world applicability of our approach.",https://openaccess.thecvf.com/content/WACV2023/html/Workman_Handling_Image_and_Label_Resolution_Mismatch_in_Remote_Sensing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Workman_Handling_Image_and_Label_Resolution_Mismatch_in_Remote_Sensing_WACV_2023_paper.pdf,,,2211.1579,main,Poster,https://ieeexplore.ieee.org/document/10030144/,"['Computer vision', 'Image resolution', 'Annotations', 'Semantic segmentation', 'Adversarial machine learning', 'Task analysis', 'Remote sensing']","['Image Resolution', 'Remote Sensing', 'Generative Adversarial Networks', 'Semantic Segmentation', 'Ground Truth Labels', 'Target Label', 'Ground Sampling Distance', 'Spatial Resolution', 'F1 Score', 'Types Of Strategies', 'Intersection Over Union', 'Supplementary Materials For Details', 'Land Cover Classes', 'Channel Dimension', 'Domain Adaptation', 'Self-supervised Learning', 'High-resolution Imagery', 'Direct Supervision', 'Classification Index', 'Architecture For Segmentation', 'Native Resolution', 'Chesapeake Bay', 'Pretext Task', 'Held-out Test Set', 'Annotated Training Data']","['Applications: Remote Sensing', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"Though semantic segmentation has been heavily explored in vision literature, unique challenges remain in the remote sensing domain. One such challenge is how to handle resolution mismatch between overhead imagery and ground-truth label sources, due to differences in ground sample distance. To illustrate this problem, we introduce a new dataset and use it to showcase weaknesses inherent in existing strategies that naively upsample the target label to match the image resolution. Instead, we present a method that is supervised using low-resolution labels (without upsampling), but takes advantage of an exemplar set of highresolution labels to guide the learning process. Our method incorporates region aggregation, adversarial learning, and self-supervised pretraining to generate fine-grained predictions, without requiring high-resolution annotations. Extensive experiments demonstrate the real-world applicability of our approach."
Hard To Track Objects With Irregular Motions and Similar Appearances? Make It Easier by Buffering the Matching Space,"Fan Yang, Shigeyuki Odashima, Shoichi Masui, Shan Jiang","Fujitsu Research, Japan",0,,100,Japan,"We propose a Cascaded Buffered IoU (C-BIoU) tracker to track multiple objects that have irregular motions and indistinguishable appearances. When appearance features are unreliable and geometric features are confused by irregular motions, applying conventional Multiple Object Tracking (MOT) methods may generate unsatisfactory results. To address this issue, our C-BIoU tracker adds buffers to expand the matching space of detections and tracks, which mitigates the effect of irregular motions in two aspects: one is to directly match identical but non-overlapping detections and tracks in adjacent frames, and the other is to compensate for the motion estimation bias in the matching space. In addition, to reduce the risk of overexpansion of the matching space, cascaded matching is employed: first matching alive tracks and detections with a small buffer, and then matching unmatched tracks and detections with a large buffer. Despite its simplicity, our C-BIoU tracker works surprisingly well and achieves state-of-the-art results on MOT datasets that focus on irregular motions and indistinguishable appearances. Moreover, the C-BIoU tracker is the dominant component for our 2nd place solution in the CVPR'22 SoccerNet MOT and the ECCV'22 MOTComplex DanceTrack challenges. Finally, we analyze the limitation of our C-BIoU tracker in ablation studies and discuss its application scope.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.pdf,,,2211.14317,main,Poster,https://ieeexplore.ieee.org/document/10030951/,"['Computer vision', 'Tracking', 'Motion estimation', 'Object tracking']","['Similar Appearance', 'Object Tracking', 'Irregular Motion', 'Geometric Features', 'Effect Of Motion', 'Appearance Features', 'Motion Estimation', 'Adjacent Frames', 'Large Buffer', 'Multiple Object Tracking', 'Overexpansion', 'Object Detection', 'Bounding Box', 'Kalman Filter', 'Tracking Performance', 'Ratio Scale', 'Target Tracking', 'Tracking Results', 'Geometric Measures', 'Geometric Consistency', 'Track Manager', 'Original Track']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",48,"We propose a Cascaded Buffered IoU (C-BIoU) tracker to track multiple objects that have irregular motions and indistinguishable appearances. When appearance features are unreliable and geometric features are confused by irregular motions, applying conventional Multiple Object Tracking (MOT) methods may generate unsatisfactory results. To address this issue, our C-BIoU tracker adds buffers to expand the matching space of detections and tracks, which mitigates the effect of irregular motions in two aspects: one is to directly match identical but non-overlapping detections and tracks in adjacent frames, and the other is to compensate for the motion estimation bias in the matching space. In addition, to reduce the risk of overexpansion of the matching space, cascaded matching is employed: first matching alive tracks and detections with a small buffer, and then matching unmatched tracks and detections with a large buffer. Despite its simplicity, our C-BIoU tracker works surprisingly well and achieves state-of-the-art results on MOT datasets that focus on irregular motions and indistinguishable appearances. Moreover, the C-BIoU tracker is the dominant component for our 2
<sup>nd</sup>
 place solution in the CVPR’22 SoccerNet MOT and the ECCV’22 MOTComplex DanceTrack challenges. Finally, we analyze the limitation of our C-BIoU tracker in ablation studies and discuss its application scope."
Harnessing Unrecognizable Faces for Improving Face Recognition,"Siqi Deng, Yuanjun Xiong, Meng Wang, Wei Xia, Stefano Soatto",AWS AI Labs,0,,100,USA,"The common implementation of face recognition systems as a cascade of a detection stage and a recognition or verification stage can cause problems beyond failures of the detector. When the detector succeeds, it can detect faces that cannot be recognized, no matter how capable the recognition system is. Recognizability, a latent variable, should therefore be factored into the design and implementation of face recognition systems. We propose a measure of recognizability of a face image that leverages a key empirical observation: An embedding of face images, implemented by a deep neural network trained using mostly recognizable identities, induces a partition of the hypersphere whereby unrecognizable identities cluster together. This occurs regardless of the phenomenon that causes a face to be unrecognizable, be it optical or motion blur, partial occlusion, spatial quantization, or poor illumination. Therefore, we use the distance from such an ""unrecognizable identity"" as a measure of recognizability, and incorporate it into the design of the overall system. We show that accounting for recognizability reduces the error rate of single-image face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification benchmark, and reduces the verification error rate by 24% at FAR=1e-5 in set-based recognition on the IJB-C benchmark.",https://openaccess.thecvf.com/content/WACV2023/html/Deng_Harnessing_Unrecognizable_Faces_for_Improving_Face_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Deng_Harnessing_Unrecognizable_Faces_for_Improving_Face_Recognition_WACV_2023_paper.pdf,,,2106.04112,main,Poster,https://ieeexplore.ieee.org/document/10030586/,"['Image recognition', 'Quantization (signal)', 'Error analysis', 'Face recognition', 'Neural networks', 'Lighting', 'Detectors']","['Face Recognition', 'Improve Face Recognition', 'Unrecognizable Faces', 'Deep Neural Network', 'Latent Variables', 'Face Images', 'Motion Blur', 'Hypersphere', 'Partial Occlusion', 'Image Embedding', 'False Acceptance Rate', 'Training Dataset', 'Image Quality', 'Bias Analysis', 'Bounding Box', 'Direct Approach', 'Representation Of Space', 'Error Reduction', 'Gaussian Blur', 'Decision Boundary', 'Multiple Benchmarks', 'Algorithmic Bias', 'Gallery Images', 'Backbone Architecture', 'Positive Matches', 'Single Face']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose']",4,"The common implementation of face recognition systems as a cascade of a detection stage and a recognition or verification stage can cause problems beyond failures of the detector. When the detector succeeds, it can detect faces that cannot be recognized, no matter how capable the recognition system is. Recognizability, a latent variable, should therefore be factored into the design and implementation of face recognition systems. We propose a measure of recognizability of a face image that leverages a key empirical observation: An embedding of face images, implemented by a deep neural network trained using mostly recognizable identities, induces a partition of the hypersphere whereby unrecognizable identities cluster together. This occurs regardless of the phenomenon that causes a face to be unrecognizable, be it optical or motion blur, partial occlusion, spatial quantization, or poor illumination. Therefore, we use the distance from such an ""unrecognizable identity"" as a measure of recognizability, and incorporate it into the design of the overall system. We show that accounting for recognizability reduces the error rate of single-image face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification benchmark, and reduces the verification error rate by 24% at FAR=1e-5 in set-based recognition on the IJB-C benchmark."
Hear the Flow: Optical Flow-Based Self-Supervised Visual Sound Source Localization,"Dennis Fedorishin, Deen Dayal Mohan, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju","University at Buffalo, Buffalo, New York, USA",100,USA,0,,"Learning to localize the sound source in videos without explicit annotations is a novel area of audio-visual research. Existing work in this area focuses on creating attention maps to capture the correlation between the two modalities to localize the source of the sound. In a video, oftentimes, the objects exhibiting movement are the ones generating the sound. In this work, we capture this characteristic by modeling the optical flow in a video as a prior to better aid in localizing the sound source. We further demonstrate that the addition of flow-based attention substantially improves visual sound source localization. Finally, we benchmark our method on standard sound source localization datasets and achieve state-of-the-art performance on the Soundnet Flickr and VGG Sound Source datasets. Code: https://github.com/denfed/heartheflow.",https://openaccess.thecvf.com/content/WACV2023/html/Fedorishin_Hear_the_Flow_Optical_Flow-Based_Self-Supervised_Visual_Sound_Source_Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Fedorishin_Hear_the_Flow_Optical_Flow-Based_Self-Supervised_Visual_Sound_Source_Localization_WACV_2023_paper.pdf,,https://github.com/denfed/heartheflow,2211.03019,main,Poster,https://ieeexplore.ieee.org/document/10030890/,"['Location awareness', 'Computer vision', 'Image motion analysis', 'Visualization', 'Image databases', 'Multimedia Web sites', 'Self-supervised learning']","['Sound Source', 'Sound Localization', 'Work In This Area', 'Optical Flow', 'Standard Datasets', 'Feature Maps', 'Local Network', 'Visual Features', 'Intersection Over Union', 'ImageNet', 'Localization Performance', 'Video Frames', 'Visual Modality', 'Max-pooling Layer', 'Visual Space', 'Self-supervised Learning', 'Sound Detection', 'Saliency Map', 'Region Proposal Network', 'Salient Object', 'Object In Frame', 'Self-supervised Training', 'Middle Frame', 'Extract Visual Features', 'Attention Matrix', 'Visual Representation', 'Feature Representation', 'Image Object', 'Active Area Of Research', 'Prior Information']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",9,"Learning to localize the sound source in videos without explicit annotations is a novel area of audio-visual research. Existing work in this area focuses on creating attention maps to capture the correlation between the two modalities to localize the source of the sound. In a video, oftentimes, the objects exhibiting movement are the ones generating the sound. In this work, we capture this characteristic by modeling the optical flow in a video as a prior to better aid in localizing the sound source. We further demonstrate that the addition of flow-based attention substantially improves visual sound source localization. Finally, we benchmark our method on standard sound source localization datasets and achieve state-of-the-art performance on the Soundnet Flickr and VGG Sound Source datasets. Code: https://github.com/denfed/heartheflow."
Heatmap-Based Out-of-Distribution Detection,"Julia Hornauer, Vasileios Belagiannis","Friedrich-Alexander-University Erlangen-Nürnberg, Germany; Ulm University, Germany",100,Germany,0,,"Our work investigates out-of-distribution (OOD) detection as a neural network output explanation problem. We learn a heatmap representation for detecting OOD images while visualizing in- and out-of-distribution image regions at the same time. Given a trained and fixed classifier, we train a decoder neural network to produce heatmaps with zero response for in-distribution samples and high response heatmaps for OOD samples, based on the classifier features and the class prediction. Our main innovation lies in the heatmap definition for an OOD sample, as the normalized difference from the closest in-distribution sample. The heatmap serves as a margin to distinguish between in- and out-of-distribution samples. Our approach generates the heatmaps not only for OOD detection, but also to indicates in- and out-of-distribution regions of the input image. In our evaluations, our approach mostly outperforms the prior work on fixed classifiers, trained on CIFAR-10, CIFAR-100 and Tiny ImageNet. The code is publicly available at: https://github.com/jhornauer/heatmap_ood.",https://openaccess.thecvf.com/content/WACV2023/html/Hornauer_Heatmap-Based_Out-of-Distribution_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hornauer_Heatmap-Based_Out-of-Distribution_Detection_WACV_2023_paper.pdf,,https://github.com/jhornauer/heatmap_ood,2211.08115,main,Poster,https://ieeexplore.ieee.org/document/10030868,"['Heating systems', 'Visualization', 'Technological innovation', 'Computer vision', 'Codes', 'Neural networks', 'Decoding']","['Neural Network', 'Heatmap', 'Input Image', 'Image Regions', 'Class Prediction', 'Training Set', 'Deep Neural Network', 'Scaling Factor', 'False Positive Rate', 'Binary Classification', 'Feature Space', 'Softmax', 'Scoring Function', 'Feature Representation', 'Training Images', 'Detection In Samples', 'Attention Map', 'Energy Score', 'High Brightness', 'Performance Of Neural Networks', 'Saliency Map', 'Image X', 'Predicted Probability Distribution', 'Training Distribution', 'Neural Classifier']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",5,"Our work investigates out-of-distribution (OOD) detection as a neural network output explanation problem. We learn a heatmap representation for detecting OOD images while visualizing in- and out-of-distribution image regions at the same time. Given a trained and fixed classifier, we train a decoder neural network to produce heatmaps with zero response for in-distribution samples and high response heatmaps for OOD samples, based on the classifier features and the class prediction. Our main innovation lies in the heatmap definition for an OOD sample, as the normalized difference from the closest in-distribution sample. The heatmap serves as a margin to distinguish between in- and out-of-distribution samples. Our approach generates the heatmaps not only for OOD detection, but also to indicates in- and out-of-distribution regions of the input image. In our evaluations, our approach mostly outperforms the prior work on fixed classifiers, trained on CIFAR-10, CIFAR-100 and Tiny ImageNet. The code is publicly available at: https://github.com/jhornauer/heatmap_ood."
Heightfields for Efficient Scene Reconstruction for AR,"Jamie Watson, Sara Vicente, Oisin Mac Aodha, Clément Godard, Gabriel Brostow, Michael Firman","University of Edinburgh; Niantic, UCL; Niantic; Google",50,"UK, USA",50,USA,"3D scene reconstruction from a sequence of posed RGB images is a cornerstone task for computer vision and augmented reality (AR). While depth-based fusion is the foundation of most real-time approaches for 3D reconstruction, recent learning based methods that operate directly on RGB images can achieve higher quality reconstructions, but at the cost of increased runtime and memory requirements, making them unsuitable for AR applications. We propose an efficient learning-based method that refines the 3D reconstruction obtained by a traditional fusion approach. By leveraging a top-down heightfield representation, our method remains real-time while approaching the quality of other learning-based methods. Despite being a simplification, our heightfield is perfectly appropriate for robotic path planning or augmented reality character placement. We outline several innovations that push the performance beyond existing top-down prediction baselines, and we present an evaluation framework on the challenging ScanNetV2 dataset, targeting AR tasks. Ultimately, we show that our method improves over the baselines for AR applications. Full code and pretrained models will be released on acceptance.",https://openaccess.thecvf.com/content/WACV2023/html/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Watson_Heightfields_for_Efficient_Scene_Reconstruction_for_AR_WACV_2023_paper.pdf,,https://github.com/nianticlabs/heightfields,,main,Poster,https://ieeexplore.ieee.org/document/10031000/,"['Learning systems', 'Computer vision', 'Technological innovation', 'Three-dimensional displays', 'Runtime', 'Training data', 'Real-time systems']","['Scene Reconstruction', '3D Reconstruction', 'Learning-based Methods', 'RGB Images', '3D Scene', 'Augmented Reality Applications', 'Training Time', 'Feature Maps', 'Grid Cells', 'Deep Features', 'Depth Map', '3D Volume', 'Inference Time', 'Full Method', 'Depth Estimation', 'Network Depth', 'Objects In The Scene', 'Reprojection', 'Structure From Motion', '3D Convolution', 'Camera Pose', '2D Grid', 'Cost Volume', 'Navigation Map', 'Ray Casting', 'Raw Volume', 'Semantic Segmentation', 'Height Estimation', 'Free Space']",['Algorithms: 3D computer vision'],4,"3D scene reconstruction from a sequence of posed RGB images is a cornerstone task for computer vision and augmented reality (AR). While depth-based fusion is the foundation of most real-time approaches for 3D reconstruction, recent learning based methods that operate directly on RGB images can achieve higher quality reconstructions, but at the cost of increased runtime and memory requirements, making them unsuitable for AR applications. We propose an efficient learning-based method that refines the 3D reconstruction obtained by a traditional fusion approach. By leveraging a top-down heightfield representation, our method remains real-time while approaching the quality of other learning-based methods. Despite being a simplification, our heightfield is perfectly appropriate for robotic path planning or augmented reality character placement. We outline several innovations that push the performance beyond existing top-down prediction baselines, and we present an evaluation framework on the challenging ScanNetV2 dataset, targeting AR tasks."
HiFormer: Hierarchical Multi-Scale Representations Using Transformers for Medical Image Segmentation,"Moein Heidari, Amirhossein Kazerouni, Milad Soltany, Reza Azad, Ehsan Khodapanah Aghdam, Julien Cohen-Adad, Dorit Merhof","School of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran; Faculty of Informatics and Data Science, University of Regensburg, Regensburg, Germany; Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany; Department of Electrical Engineering, Shahid Beheshti University, Tehran, Iran; Institute of Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany; MILA, Quebec AI Institute, Montreal, Canada",100,"Canada, Germany, Iran",0,,"Convolutional neural networks (CNNs) have been the consensus for medical image segmentation tasks. However, they inevitably suffer from the limitation in modeling long-range dependencies and spatial correlations due to the nature of convolution operation. Although Transformers were first developed to address this issue, they fail to capture low-level features. In contrast, it is demonstrated that both local and global features are crucial for dense prediction, such as segmenting in challenging contexts. In this paper, we propose HiFormer, a novel method that efficiently bridges a Convolutional neural network and a Transformer for medical image segmentation. Specifically, we design two multi-scale feature representations using the seminal Swin-Transformer module and a CNN-based encoder. To secure a fine fusion of global and local features obtained from the two aforementioned representations, we propose a Double-Level Fusion (DLF) module in the skip connection of the encoder-decoder outline. Extensive experiments on various medical image segmentation datasets demonstrate the effectiveness of HiFormer over other CNN-based, Transformer-based, and hybrid methods in terms of computational complexity, quantitative and qualitative results",https://openaccess.thecvf.com/content/WACV2023/html/Heidari_HiFormer_Hierarchical_Multi-Scale_Representations_Using_Transformers_for_Medical_Image_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Heidari_HiFormer_Hierarchical_Multi-Scale_Representations_Using_Transformers_for_Medical_Image_Segmentation_WACV_2023_paper.pdf,,https://github.com/...,2207.08518,main,Poster,https://ieeexplore.ieee.org/document/10030768/,"['Image segmentation', 'Computer vision', 'Correlation', 'Convolution', 'Computational modeling', 'Transformers', 'Convolutional neural networks']","['Medical Imaging', 'Image Segmentation', 'Medical Image Segmentation', 'Multi-scale Representation', 'Convolutional Network', 'Convolutional Neural Network', 'Local Features', 'Feature Representation', 'Global Features', 'Low-level Features', 'Segmentation Task', 'Skip Connections', 'CNN-based Methods', 'Long-range Dependencies', 'Segmentation Dataset', 'Medical Tasks', 'Image Segmentation Tasks', 'Transformation Module', 'Transformer-based Methods', 'Local Information', 'Convolutional Neural Network Module', 'Vision Transformer', 'Feature Maps', 'Smallest Level', 'Multi-head Self-attention', 'U-shaped Structure', 'Fully Convolutional Network', 'High-level Features', 'Transformer Encoder', 'Convolutional Neural Networks Backbone']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Biomedical/healthcare/medicine']",123,"Convolutional neural networks (CNNs) have been the consensus for medical image segmentation tasks. However, they suffer from the limitation in modeling long-range dependencies and spatial correlations due to the nature of convolution operation. Although transformers were first developed to address this issue, they fail to capture low-level features. In contrast, it is demonstrated that both local and global features are crucial for dense prediction, such as segmenting in challenging contexts. In this paper, we propose HiFormer, a novel method that efficiently bridges a CNN and a transformer for medical image segmentation. Specifically, we design two multi-scale feature representations using the seminal Swin Transformer module and a CNN-based encoder. To secure a fine fusion of global and local features obtained from the two aforementioned representations, we propose a Double-Level Fusion (DLF) module in the skip connection of the encoder-decoder structure. Extensive experiments on various medical image segmentation datasets demonstrate the effectiveness of HiFormer over other CNN-based, transformer-based, and hybrid methods in terms of computational complexity, quantitative and qualitative results. Our code is publicly available at GitHub."
High-Quality RGB-D Reconstruction via Multi-View Uncalibrated Photometric Stereo and Gradient-SDF,"Lu Sang, Björn Häfner, Xingxing Zuo, Daniel Cremers","Technical University of Munich, Munich Center for Machine Learning",100,Germany,0,,"Fine-detailed reconstructions are in high demand in many applications. However, most of the existing RGB-D reconstruction methods rely on pre-calculated accurate camera poses to recover the detailed surface geometry, where the representation of a surface needs to be adapted when optimizing different quantities. In this paper, we present a novel multi-view RGB-D based reconstruction method that tackles camera pose, lighting, albedo, and surface normal estimation via the utilization of a gradient signed distance field (Gradient-SDF). The proposed method formulates the image rendering process using specific physically-based model(s) and optimizes the surface's quantities on the actual surface using its volumetric representation, as opposed to other works which estimate surface quantities only near the actual surface. To validate our method, we investigate two physically-based image formation models for natural light and point light source applications. The experimental results on synthetic and real-world datasets demonstrate that the proposed method can recover high-quality geometry of the surface more faithfully than the state-of-art and further improves the accuracy of estimated camera poses.",https://openaccess.thecvf.com/content/WACV2023/html/Sang_High-Quality_RGB-D_Reconstruction_via_Multi-View_Uncalibrated_Photometric_Stereo_and_Gradient-SDF_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sang_High-Quality_RGB-D_Reconstruction_via_Multi-View_Uncalibrated_Photometric_Stereo_and_Gradient-SDF_WACV_2023_paper.pdf,,https://github.com/Sangluisme/PSgradientSDF,,main,Poster,https://ieeexplore.ieee.org/document/10030841/,"['Geometry', 'Surface reconstruction', 'Lighting', 'Estimation', 'Reconstruction algorithms', 'Cameras', 'Rendering (computer graphics)']","['High-quality Reconstruction', 'Multi-view Stereo', 'Photometric Stereo', 'RGB-D Reconstruction', 'Light Source', 'Daylight', 'Active Surface', 'Real-world Datasets', 'Pose Estimation', 'Distance Map', 'Surface Representation', 'Surface Geometry', 'Camera Pose', 'Signed Distance Function', 'Camera Pose Estimation', 'Point Light Source', 'Light-emitting Diodes', 'Material Surface', 'Point Cloud', 'RGB Images', 'Surface Points', 'Complete Pipeline', 'Depth Images', 'Material For More Details', 'Implicit Representation', 'Central Voxel', 'Direct Light', 'Source Model', 'Pose Tracking', 'Bundle Adjustment']","['Algorithms: Low-level and physics-based vision', '3D computer vision', 'Computational photography', 'image and video synthesis']",7,"Fine-detailed reconstructions are in high demand in many applications. However, most of the existing RGB-D reconstruction methods rely on pre-calculated accurate camera poses to recover the detailed surface geometry, where the representation of a surface needs to be adapted when optimizing different quantities. In this paper, we present a novel multi-view RGB-D based reconstruction method that tackles camera pose, lighting, albedo, and surface normal estimation via the utilization of a gradient signed distance field (gradient-SDF). The proposed method formulates the image rendering process using specific physically-based model(s) and optimizes the surface’s quantities on the actual surface using its volumetric representation, as opposed to other works which estimate surface quantities only near the actual surface. To validate our method, we investigate two physically-based image formation models for natural light and point light source applications. The experimental results on synthetic and real-world datasets demonstrate that the proposed method can recover high-quality geometry of the surface more faithfully than the state-of-the-art and further improves the accuracy of estimated camera poses
<sup>1</sup>
."
High-Resolution Depth Estimation for 360deg Panoramas Through Perspective and Panoramic Depth Images Registration,"Chi-Han Peng, Jiayao Zhang",ByteDance; National Yang Ming Chiao Tung University,50,Taiwan,50,China,"We propose a novel approach to compute high-resolution (2048x1024 and higher) depths for panoramas that is significantly faster and qualitatively and qualitatively more accurate than the current state-of-the-art method (360MonoDepth). As traditional neural network-based methods have limitations in the output image sizes (up to 1024x512) due to GPU memory constraints, both 360MonoDepth and our method rely on stitching multiple perspective disparity or depth images to come out a unified panoramic depth map. However, to achieve globally consistent stitching, 360MonoDepth relied on solving extensive disparity map alignment and Poisson-based blending problems, leading to high computation time. Instead, we propose to use an existing panoramic depth map (computed in real-time by any panorama-based method) as the common target for the individual perspective depth maps to register to. This key idea made producing globally consistent stitching results from a straightforward task. Our experiments show that our method generates qualitatively better results than existing panorama-based methods, and further outperforms them quantitatively on datasets unseen by these methods.",https://openaccess.thecvf.com/content/WACV2023/html/Peng_High-Resolution_Depth_Estimation_for_360deg_Panoramas_Through_Perspective_and_Panoramic_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Peng_High-Resolution_Depth_Estimation_for_360deg_Panoramas_Through_Perspective_and_Panoramic_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial Networks,"Georg Wölflein, In Hwa Um, David J. Harrison, Ognjen Arandjelović","School of Medicine, University of St Andrews; School of Computer Science, University of St Andrews",100,UK,0,,"The presence and density of specific types of immune cells are important to understand a patient's immune response to cancer. However, immunofluorescence staining required to identify T cell subtypes is expensive, timeconsuming, and rarely performed in clinical settings. We present a framework to virtually stain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to identify T cell subtypes in clear cell renal cell carcinoma using generative adversarial networks. Our proposed method jointly learns both staining tasks, incentivising the network to incorporate mutually beneficial information from each task. We devise a novel metric to quantify the virtual staining quality, and use it to evaluate our method.",https://openaccess.thecvf.com/content/WACV2023/html/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.pdf,,https://georg.woelflein.eu/hoechstgan,,main,Poster,https://ieeexplore.ieee.org/document/10030547/,"['Measurement', 'Computer vision', 'Computer architecture', 'Generative adversarial networks', 'Task analysis', 'Signal to noise ratio', 'Cancer']","['Generative Adversarial Networks', 'T Cells', 'Colorectal Cancer', 'Training Set', 'Tumor Microenvironment', 'CD8 T Cells', 'Risk Stratification', 'High Heterogeneity', 'Tyrosine Kinase Inhibitors', 'Focus Of This Work', 'Skip Connections', 'Number Of Filters', 'Latent Representation', 'Slide Images', 'Digital Pathology', 'L1 Loss', 'Conditional Generative Adversarial Network', 'Latent Code', 'CD3 Staining', 'Generative Adversarial Networks Loss', 'Hoechst 33342', 'Patch Level', 'Risk Stratification System', 'Hoechst Staining']","['Applications: Biomedical/healthcare/medicine', 'Adversarial learning', 'adversarial attack and defense methods', 'Computational photography', 'image and video synthesis']",4,"The presence and density of specific types of immune cells are important to understand a patient’s immune response to cancer. However, immunofluorescence staining required to identify T cell subtypes is expensive, time-consuming, and rarely performed in clinical settings. We present a framework to virtually stain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to identify T cell subtypes in clear cell renal cell carcinoma using generative adversarial networks. Our proposed method jointly learns both staining tasks, incentivising the network to incorporate mutually beneficial information from each task. We devise a novel metric to quantify the virtual staining quality, and use it to evaluate our method."
Holistic Interaction Transformer Network for Action Detection,"Gueter Josmy Faure, Min-Hung Chen, Shang-Hong Lai","National Tsing Hua University, Taiwan; Microsoft AI R&D Center, Taiwan",50,Taiwan,50,USA,"Actions are about how we interact with the environment, including other people, objects, and ourselves. In this paper, we propose a novel multi-modal Holistic Interaction Transformer Network (HIT) that leverages the largely ignored, but critical hand and pose information essential to most human actions. The proposed HIT network is a comprehensive bi-modal framework that comprises an RGB stream and a pose stream. Each of them separately models person, object, and hand interactions. Within each sub-network, an Intra-Modality Aggregation module (IMA) is introduced that selectively merges individual interaction units. The resulting features from each modality are then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues from the temporal context to better classify the occurring actions using cached memory. Our method significantly outperforms previous approaches on the J-HMDB, UCF101-24, and MultiSports datasets. We also achieve competitive results on AVA. The code will be available at https://github.com/joslefaure/HIT.",https://openaccess.thecvf.com/content/WACV2023/html/Faure_Holistic_Interaction_Transformer_Network_for_Action_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Faure_Holistic_Interaction_Transformer_Network_for_Action_Detection_WACV_2023_paper.pdf,,https://github.com/joslefaure/HIT,2210.12686,main,Poster,https://ieeexplore.ieee.org/document/10030203/,"['Computer vision', 'Codes', 'Computational modeling', 'Aggregates', 'Benchmark testing', 'Transformers', 'Feature extraction']","['Action Detection', 'Human Activities', 'Attention Mechanism', 'Personal Interactions', 'Object Interaction', 'Remote Memory', 'Personal Characteristics', 'Interaction Model', 'Object Detection', 'Bounding Box', 'Video Clips', 'Object Features', 'Active Space', 'Video Analysis', 'Attention Module', 'Pose Estimation', 'Interaction Module', 'Current Frame', 'Different Types Of Interactions', 'Video Dataset', 'RGB Features', 'Temporal Interactions', 'Target Person', 'Video Features', 'Classification Head', 'Early Fusion', 'Temporal Unit', 'Action Classes', 'Information Aggregation', 'Fusion Method']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",17,"Actions are about how we interact with the environment, including other people, objects, and ourselves. In this paper, we propose a novel multi-modal Holistic Interaction Transformer Network (HIT) that leverages the largely ignored, but critical hand and pose information essential to most human actions. The proposed HIT network is a comprehensive bi-modal framework that comprises an RGB stream and a pose stream. Each of them separately models person, object, and hand interactions. Within each sub-network, an Intra-Modality Aggregation module (IMA) is introduced that selectively merges individual interaction units. The resulting features from each modality are then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues from the temporal context to better classify the occurring actions using cached memory. Our method significantly outperforms previous approaches on the J-HMDB, UCF101-24, and MultiSports datasets. We also achieve competitive results on AVA. The code will be available at https://github.com/joslefaure/HIT."
How To Practice VQA on a Resource-Limited Target Domain,"Mingda Zhang, Rebecca Hwa, Adriana Kovashka","Department of Computer Science, University of Pittsburgh; Currently at Google Research; Department of Computer Science, University of Pittsburgh",66.66666667,USA,33.33333333,USA,"Visual question answering (VQA) is an active research area at the intersection of computer vision and natural language understanding. One major obstacle that keeps VQA models that perform well on benchmarks from being as successful on real-world applications, is the lack of annotated Image-Question-Answer triplets in the task of interest. In this work, we focus on a previously overlooked perspective, which is the disparate effectiveness of transfer learning and domain adaptation methods depending on the amount of labeled/unlabeled data available. We systematically investigated the visual domain gaps and question-defined textual gaps, and compared different knowledge transfer strategies under unsupervised, self-supervised, semi-supervised and fully-supervised adaptation scenarios. We show that different methods have varied sensitivity and requirements for data amount in the target domain. We conclude by sharing the best practice from our exploration regarding transferring VQA models to resource-limited target domains.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_How_To_Practice_VQA_on_a_Resource-Limited_Target_Domain_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_How_To_Practice_VQA_on_a_Resource-Limited_Target_Domain_WACV_2023_paper.pdf,https://cs.pitt.edu/~mzhang/practice-vqa/,,,main,Poster,https://ieeexplore.ieee.org/document/10030465/,"['Visualization', 'Computer vision', 'Adaptation models', 'Sensitivity', 'Computational modeling', 'Transfer learning', 'Question answering (information retrieval)']","['Target Domain', 'Visual Question Answering', 'Knowledge Transfer', 'Transfer Learning', 'Question Answering', 'Domain Adaptation', 'Visual Domain', 'Transfer Learning Method', 'Domain Gap', 'Unsupervised Adaptation', 'Natural Language Understanding', 'Amount Of Sample', 'Domain Shift', 'Types Of Questions', 'Language Model', 'Labeled Samples', 'Unlabeled Data', 'Relaxed State', 'Source Domain', 'Self-supervised Learning', 'Target Dataset', 'Moment Matching', 'Domain Discrepancy', 'Masked Language Model', 'Auxiliary Loss', 'Future Practitioners', 'Source Dataset']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Visual question answering (VQA) is an active research area at the intersection of computer vision and natural language understanding. One major obstacle that keeps VQA models that perform well on benchmarks from being as successful on real-world applications, is the lack of annotated Image–Question–Answer triplets in the task of interest. In this work, we focus on a previously overlooked perspective, which is the disparate effectiveness of transfer learning and domain adaptation methods depending on the amount of labeled/unlabeled data available. We systematically investigated the visual domain gaps and question-defined textual gaps, and compared different knowledge transfer strategies under unsupervised, self-supervised, semi-supervised and fully-supervised adaptation scenarios. We show that different methods have varied sensitivity and requirements for data amount in the target domain. We conclude by sharing the best practice from our exploration regarding transferring VQA models to resource-limited target domains."
HuPR: A Benchmark for Human Pose Estimation Using Millimeter Wave Radar,"Shih-Po Lee, Niraj Prakash Kini, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang","University of Washington, USA; National Yang Ming Chiao Tung University, Taiwan",100,"Taiwan, USA",0,,"This paper introduces a novel human pose estimation benchmark, Human Pose with Millimeter Wave Radar (HuPR), that includes synchronized vision and radio signal components. This dataset is created using cross-calibrated mmWave radar sensors and a monocular RGB camera for cross-modality training of radar-based human pose estimation. There are two advantages of using mmWave radar to perform human pose estimation. First, it is robust to dark and low-light conditions. Second, it is not visually perceivable by humans and therefore, can be widely applied to applications with privacy concerns, e.g., surveillance systems in patient rooms. In addition to the benchmark, we propose a cross-modality training framework that leverages the ground-truth 2D keypoints representing human body joints for training, which are systematically generated from the pre-trained 2D pose estimation network based on a monocular camera input image, avoiding laborious manual label annotation efforts. The framework consists of a new radar pre-processing method that better extracts the velocity information from radar data, Cross- and Self-Attention Module (CSAM), to fuse multi-scale radar features, and Pose Refinement Graph Convolutional Networks (PRGCN), to refine the predicted keypoint confidence heatmaps. Our intensive experiments on the HuPR benchmark show that the proposed scheme achieves better human pose estimation performance with only radar data, as compared to traditional pre-processing solutions and previous radio-frequency-based methods. Our proposed scheme further outperforms state-of-the-art pointcloud-based methods.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_HuPR_A_Benchmark_for_Human_Pose_Estimation_Using_Millimeter_Wave_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_HuPR_A_Benchmark_for_Human_Pose_Estimation_Using_Millimeter_Wave_WACV_2023_paper.pdf,,https://github.com/robert80203/HuPR-A-Benchmark-for-Human-Pose-Estimation-Using-Millimeter-Wave-Radar,2210.12564,main,Poster,https://ieeexplore.ieee.org/document/10030786/,"['Training', 'Three-dimensional displays', 'Pose estimation', 'Millimeter wave radar', 'Benchmark testing', 'Radar imaging', 'Radar antennas']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', 'Wave Radar', 'Millimeter-wave Radar', 'Multi-scale Features', 'Pre-trained Network', 'Graph Convolutional Network', 'Radar Data', 'Low Light Conditions', 'Preprocessing Methods', 'RGB Camera', 'Radar Sensor', 'Velocity Information', 'Self-attention Module', '2D Pose', '2D Keypoints', 'Raw Data', 'Comparative Method', 'Convolutional Layers', 'Multi-scale Feature Fusion', 'Azimuth Resolution', 'Radar Signal', 'Fast Fourier Transform', 'Elevation Information', 'Frequency Modulated Continuous Wave', 'Azimuth Angle', 'Node Features', 'Human Bone', 'Feature Fusion']","['Algorithms: Low-level and physics-based vision', 'Biometrics', 'face', 'gesture', 'body pose', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",24,"This paper introduces a novel human pose estimation benchmark, Human Pose with Millimeter Wave Radar (HuPR), that includes synchronized vision and radio signal components. This dataset is created using cross-calibrated mmWave radar sensors and a monocular RGB camera for cross-modality training of radar-based human pose estimation. There are two advantages of using mmWave radar to perform human pose estimation. First, it is robust to dark and low-light conditions. Second, it is not visually perceivable by humans and thus, can be widely applied to applications with privacy concerns, e.g., surveillance systems in patient rooms. In addition to the benchmark, we propose a cross-modality training framework that leverages the ground-truth 2D keypoints representing human body joints for training, which are systematically generated from the pre-trained 2D pose estimation network based on a monocular camera input image, avoiding laborious manual label annotation efforts. The framework consists of a new radar pre-processing method that better extracts the velocity information from radar data, Cross- and Self-Attention Module (CSAM), to fuse multi-scale radar features, and Pose Refinement Graph Convolutional Networks (PRGCN), to refine the predicted keypoint confidence heatmaps. Our intensive experiments on the HuPR benchmark show that the proposed scheme achieves better human pose estimation performance with only radar data, as compared to traditional pre-processing solutions and previous radiofrequency-based methods. Our code is available at here<sup>1</sup>"
Human-in-the-Loop Video Semantic Segmentation Auto-Annotation,"Nan Qiao, Yuyin Sun, Chong Liu, Lu Xia, Jiajia Luo, Ke Zhang, Cheng-Hao Kuo","UC Santa Barbara; Device CoRo, Amazon",50,USA,50,USA,"Accurate per-pixel semantic class annotations of the entire video are crucial for designing and evaluating video semantic segmentation algorithms. However, the annotations are usually limited to a small subset of the video frames due to the high annotation cost and limited budget in practice. In this paper, we propose a novel human-in-the-loop framework called HVSA to generate semantic segmentation annotations for the entire video using only a small annotation budget. Our method alternates between active sample selection and test-time fine-tuning algorithms until annotation quality is satisfied. In particular, the active sample selection algorithm picks the most important samples to get manual annotations, where the sample can be a video frame, a rectangle, or even a super-pixel. Further, the test-time fine-tuning algorithm propagates the manual annotations of selected samples to the entire video. Real-world experiments show that our method generates highly accurate and consistent semantic segmentation annotations while simultaneously enjoys significantly small annotation cost.",https://openaccess.thecvf.com/content/WACV2023/html/Qiao_Human-in-the-Loop_Video_Semantic_Segmentation_Auto-Annotation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qiao_Human-in-the-Loop_Video_Semantic_Segmentation_Auto-Annotation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030462/,"['Computer vision', 'Costs', 'Annotations', 'Semantic segmentation', 'Semantics', 'Manuals', 'Multitasking']","['Semantic Segmentation', 'Video Semantic Segmentation', 'Selection Algorithm', 'Video Frames', 'Limited Budget', 'Manual Annotation', 'Annotation Quality', 'Entire Video', 'Semantic Annotation', 'Consistent Annotation', 'Segmentation Annotations', 'Low Cost', 'Semantic Information', 'Segmentation Model', 'Optical Flow', 'Segmentation Task', 'Advantage Of Information', 'Semi-supervised Learning', 'Video Content', 'Human Effort', 'Label Propagation', 'Annotated Samples', 'Quality Labels', 'Temporal Loss', 'Temporal Consistency', 'Spatial-temporal Information', 'Pixel Accuracy', 'Segmentation Labels', 'Test Videos', 'Semantic Labels']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"Accurate per-pixel semantic class annotations of the entire video are crucial for designing and evaluating video semantic segmentation algorithms. However, the annotations are usually limited to a small subset of the video frames due to the high annotation cost and limited budget in practice. In this paper, we propose a novel human-in-the-loop framework called HVSA to generate semantic segmentation annotations for the entire video using only a small annotation budget. Our method alternates between active sample selection and test-time fine-tuning algorithms until annotation quality is satisfied. In particular, the active sample selection algorithm picks the most important samples to get manual annotations, where the sample can be a video frame, a rectangle, or even a super-pixel. Further, the test-time fine-tuning algorithm propagates the manual annotations of selected samples to the entire video. Real-world experiments show that our method generates highly accurate and consistent semantic segmentation annotations while simultaneously enjoys significantly small annotation cost."
HyperPosePDF - Hypernetworks Predicting the Probability Distribution on SO(3),"Timon Höfer, Benjamin Kiefer, Martin Messmer, Andreas Zell","University of T ¨ubingen, Wilhelm-Schickard-Institute for Computer Science",100,Germany,0,,"Pose estimation of objects in images is an essential problem in virtual and augmented reality and robotics. Traditional solutions use depth cameras, which are expensive, and working solutions require long processing times. This work focuses on the more difficult task when only RGB information is available. To this end, we predict not only the pose of an object but the complete probability density function (pdf) on the rotation manifold. This is the most general way to approach the pose estimation problem and is particularly useful in analysing object symmetries. In this work, we leverage implicit neural representations for the task of pose estimation and show that hypernetworks can be used to predict the rotational pdf. Furthermore, we analyse the Fourier embedding on SO(3) and evaluate the effectiveness of an initial Fourier embedding that proved successful. Our HyperPosePDF outperforms the current SOTA approach on the SYMSOL dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Hofer_HyperPosePDF_-_Hypernetworks_Predicting_the_Probability_Distribution_on_SO3_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hofer_HyperPosePDF_-_Hypernetworks_Predicting_the_Probability_Distribution_on_SO3_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030767/,"['Manifolds', 'Uncertainty', 'Three-dimensional displays', 'Shape', 'Pose estimation', 'Robot vision systems', 'Probability density function']","['Probability Density Function', 'Depth Camera', 'Pose Estimation', 'Human Pose Estimation', 'Object Pose', 'Implicit Representation', 'Neural Network', 'Fourier Transform', 'Convolutional Neural Network', 'Log-likelihood', 'Tetrahedral', 'Mixture Model', 'Adam Optimizer', 'Multilayer Perceptron', 'RGB Images', 'Network Weights', 'Icosahedral', 'Mixture Distribution', 'Euler Angles', 'Marked Points', 'Learning Rate Of 1e', 'Positional Encoding', 'Rotation Group', 'Continuous Symmetry', '3D Pose', 'Field Representation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"Pose estimation of objects in images is an essential problem in virtual and augmented reality and robotics. Traditional solutions use depth cameras, which can be expensive, and working solutions require long processing times. This work focuses on the more difficult task when only RGB information is available. To this end, we predict not only the pose of an object but the complete probability density function (pdf) on the rotation manifold. This is the most general way to approach the pose estimation problem and is particularly useful in analysing object symmetries. In this work, we leverage implicit neural representations for the task of pose estimation and show that hypernetworks can be used to predict the rotational pdf. Furthermore, we analyse the Fourier embedding on SO(3) and evaluate the effectiveness of an initial Fourier embedding that proved successful. Our HyperPosePDF outperforms the current SOTA approaches on the SYMSOL dataset."
HyperShot: Few-Shot Learning by Kernel HyperNetworks,"Marcin Sendera, Marcin Przewięźlikowski, Konrad Karanowski, Maciej Zięba, Jacek Tabor, Przemysław Spurek","Department of Artiﬁcial Intelligence, University of Science and Technology; Faculty of Mathematics and Computer Science, Jagiellonian University",100,"Poland, South Korea",0,,"Few-shot models aim at making predictions using a minimal number of labeled examples from a given task. The main challenge in this area is the one-shot setting where only one element represents each class. We propose HyperShot - the fusion of kernels and hypernetwork paradigm. Compared to reference approaches that apply a gradient-based adjustment of the parameters, our model aims to switch the classification module parameters depending on the task's embedding. In practice, we utilize a hypernetwork, which takes the aggregated information from support data and returns the classifier's parameters handcrafted for the considered problem. Moreover, we introduce the kernel-based representation of the support examples delivered to hypernetwork to create the parameters of the classification module. Consequently, we rely on relations between embeddings of the support examples instead of direct feature values provided by the backbone models. Thanks to this approach, our model can adapt to highly different tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Sendera_HyperShot_Few-Shot_Learning_by_Kernel_HyperNetworks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sendera_HyperShot_Few-Shot_Learning_by_Kernel_HyperNetworks_WACV_2023_paper.pdf,,https://github.com/gmum/few-shot-hypernets-public,,main,Poster,https://ieeexplore.ieee.org/document/10030925/,"['Training', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Switches', 'Predictive models', 'Planning']","['Few-shot Learning', 'Classification Task', 'Parametrized', 'Kernel Function', 'Latent Space', 'Gaussian Process', 'Task Accuracy', 'Null Space', 'Target Network', 'Kernel Methods', 'Multiple Examples', 'Support Set', 'Query Set', 'Choice Architecture', 'Image X', 'Query Image', 'Backbone Architecture', 'Kernel Values', 'Few-shot Classification', 'Prototypical Network', 'Query Examples', 'Training Procedure', 'Kernel-based Methods', 'Hidden Size', 'Neural Network', 'Inference Stage', 'Feature Space', 'Target Model', 'Input In Order', 'Benchmark']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",8,"Few-shot models aim at making predictions using a minimal number of labeled examples from a given task. The main challenge in this area is the one-shot setting where only one element represents each class. We propose HyperShot - the fusion of kernels and hypernetwork paradigm. Compared to reference approaches that apply a gradientbased adjustment of the parameters, our model aims to switch the classification module parameters depending on the task’s embedding. In practice, we utilize a hypernetwork, which takes the aggregated information from support data and returns the classifier’s parameters handcrafted for the considered problem. Moreover, we introduce the kernel-based representation of the support examples delivered to hypernetwork to create the parameters of the classification module. Consequently, we rely on relations between embeddings of the support examples instead of direct feature values provided by the backbone models. Thanks to this approach, our model can adapt to highly different tasks.
<sup>*</sup>"
Hyperblock Floating Point: Generalised Quantization Scheme for Gradient and Inference Computation,"Marcelo Gennari do Nascimento, Victor Adrian Prisacariu, Roger Fawcett, Martin Langhammer",Intel Corporation; University of Oxford - Active Vision Lab,50,UK,50,USA,"Prior quantization methods focus on producing networks for fast and lightweight inference. However, the cost of unquantised training is overlooked, despite requiring significantly more time and energy than inference. We present a method for quantizing convolutional neural networks for efficient training. Quantizing gradients is challenging because it requires higher granularity and their values span a wider range than the weight and feature maps. We propose an extension of the Channel-wise Block Floating Point format that allows for quick gradient computation, using a minimal amount of quantization time. This is achieved through sharing an exponent across both depth and batch dimensions in order to quantize tensors once and reuse them during backpropagation. We test our method using standard models such as AlexNet, VGG, and ResNet, on the CIFAR10, SVHN and ImageNet datasets. We show no loss of accuracy when quantizing AlexNet weights, activations and gradients to only 4 bits training ImageNet.",https://openaccess.thecvf.com/content/WACV2023/html/do_Nascimento_Hyperblock_Floating_Point_Generalised_Quantization_Scheme_for_Gradient_and_Inference_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/do_Nascimento_Hyperblock_Floating_Point_Generalised_Quantization_Scheme_for_Gradient_and_Inference_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030813/,"['Training', 'Backpropagation', 'Computer vision', 'Quantization (signal)', 'Tensors', 'Costs', 'Computational modeling']","['Gradient Calculation', 'Floating Point', 'Convolutional Neural Network', 'Feature Maps', 'Accuracy Loss', 'AlexNet', 'Quantum', 'Transposable', 'Convolutional Layers', 'Previous Paper', 'Fixed Point', 'Convolution Operation', 'Block Size', 'Low Precision', 'Dot Product', 'Output Channels', 'Channel Dimension', 'Color Channels', 'Exponent Values', 'Forward Pass', 'Backward Pass', 'Neural Architecture Search', 'Entire Block', 'Bit Length', 'Least Significant Bit']","['Applications: Smartphones/end user devices', 'Vision + language and/or other modalities']",1,"Prior quantization methods focus on producing networks for fast and lightweight inference. However, the cost of unquantised training is overlooked, despite requiring significantly more time and energy than inference. We present a method for quantizing convolutional neural networks for efficient training. Quantizing gradients is challenging because it requires higher granularity and their values span a wider range than the weight and feature maps. We propose an extension of the Channel-wise Block Floating Point format that allows for quick gradient computation, using a minimal amount of quantization time. This is achieved through sharing an exponent across both depth and batch dimensions in order to quantize tensors once and reuse them during backpropagation. We test our method using standard models such as AlexNet, VGG, and ResNet, on the CIFAR10, SVHN and ImageNet datasets. We show no loss of accuracy when quantizing AlexNet weights, activations and gradients to only 4 bits training ImageNet."
Hyperdimensional Feature Fusion for Out-of-Distribution Detection,"Samuel Wilson, Tobias Fischer, Niko Sünderhauf, Feras Dayoub","Queensland University of Technology, 2 George St, Brisbane, QLD 4000, Australia; University of Adelaide, North Terrace, Adelaide, SA 5005, Australia",100,Australia,0,,"We introduce powerful ideas from Hyperdimensional Computing into the challenging field of Out-of-Distribution (OOD) detection. In contrast to many existing works that perform OOD detection based on only a single layer of a neural network, we use similarity-preserving semi-orthogonal projection matrices to project the feature maps from multiple layers into a common vector space. By repeatedly applying the bundling operation , we create expressive class-specific descriptor vectors for all in-distribution classes. At test time, a simple and efficient cosine similarity calculation between descriptor vectors consistently identifies OOD samples with competitive performance to the current state-of-the-art whilst being significantly faster. We show that our method is orthogonal to recent state-of-the-art OOD detectors and can be combined with them to further improve upon the performance.",https://openaccess.thecvf.com/content/WACV2023/html/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wilson_Hyperdimensional_Feature_Fusion_for_Out-of-Distribution_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030789/,"['Visualization', 'Computer vision', 'Sensitivity', 'Neural networks', 'Detectors', 'Feature extraction', 'Computational efficiency']","['Single Layer', 'Feature Maps', 'Multiple Layers', 'Network Layer', 'Vector Space', 'Projection Matrix', 'Common Space', 'Descriptor Vector', 'Training Set', 'Deep Network', 'Deep Neural Network', 'Generalization Performance', 'Random Vector', 'Critical Threshold', 'Error Detection', 'Vector Representation', 'Multi-scale Features', 'Curse Of Dimensionality', 'Target Distribution', 'Image Descriptors', 'M-dimensional Space', 'Image X', 'Softmax Probability', 'Angular Distance', 'Interest Of Brevity', 'Standard Cross-entropy Loss', 'Gram Matrix', 'Visual Similarity', 'Spectral Detection', 'Gray Regions']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",5,"We introduce powerful ideas from Hyperdimensional Computing into the challenging field of Out-of-Distribution (OOD) detection. In contrast to most existing works that perform OOD detection based on only a single layer of a neural network, we use similarity-preserving semi-orthogonal projection matrices to project the feature maps from multiple layers into a common vector space. By repeatedly applying the bundling operation ⊕, we create expressive class-specific descriptor vectors for all in-distribution classes. At test time, a simple and efficient cosine similarity calculation between descriptor vectors consistently identifies OOD samples with competitive performance to the current state-of-the-art whilst being significantly faster. We show that our method is orthogonal to recent state-of-the-art OOD detectors and can be combined with them to further improve upon the performance."
Hyperspherical Quantization: Toward Smaller and More Accurate Models,"Dan Liu, Xi Chen, Chen Ma, Xue Liu",,,,,,"Model quantization enables the deployment of deep neural networks under resource-constrained devices. Vector quantization aims at reducing the model size by indexing model weights with full-precision embeddings, i.e., codewords, while the index needs to be restored to 32-bit during computation. Binary and other low-precision quantization methods can reduce the model size up to 32x, however, at the cost of a considerable accuracy drop. In this paper, we propose an efficient framework for ternary quantization to produce smaller and more accurate compressed models. By integrating hyperspherical learning, pruning and reinitialization, our proposed Hyperspherical Quantization (HQ) method reduces the cosine distance between the full-precision and ternary weights, thus reducing the bias of the straight-through gradient estimator during ternary quantization. Compared with existing work at similar compression levels ( 30x,  40x), our method significantly improves the test accuracy and reduces the model size.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_Hyperspherical_Quantization_Toward_Smaller_and_More_Accurate_Models_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_Hyperspherical_Quantization_Toward_Smaller_and_More_Accurate_Models_WACV_2023_paper.pdf,,,2212.12653,main,Poster,https://ieeexplore.ieee.org/document/10030249/,"['Deep learning', 'Computer vision', 'Quantization (signal)', 'Costs', 'Computational modeling', 'Vector quantization', 'Neural networks']","['Accuracy Of Model', 'Hypersphere', 'Deep Neural Network', 'Quantification Method', 'Model Size', 'Model Weights', 'Codeword', 'Accuracy Drop', 'Vector Quantization', 'Quantum', 'Learning Rate', 'Step Size', 'Image Classification', 'Sparsity', 'Feature Maps', 'Object Detection', 'PyTorch', 'Weight Decay', 'Stochastic Gradient Descent', 'Fully-connected Layer', 'Deep Neural Network Model', 'Bit-width', 'Cosine Annealing', 'Memory Footprint', 'Training Error', 'Compression Ratio', 'Pre-trained Weights', 'Mask R-CNN']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",,"Model quantization enables the deployment of deep neural networks under resource-constrained devices. Vector quantization aims at reducing the model size by indexing model weights with full-precision embeddings, i.e., codewords, while the index needs to be restored to 32-bit during computation. Binary and other low-precision quantization methods can reduce the model size up to 32×, however, at the cost of a considerable accuracy drop. In this paper, we propose an efficient framework for ternary quantization to produce smaller and more accurate compressed models. By integrating hyperspherical learning, pruning and reinitialization, our proposed Hyperspherical Quantization (HQ) method reduces the cosine distance between the full-precision and ternary weights, thus reducing the bias of the straight-through gradient estimator during ternary quantization. Compared with existing work at similar compression levels (~30×, ~40×), our method significantly improves the test accuracy and reduces the model size."
I See-Through You: A Framework for Removing Foreground Occlusion in Both Sparse and Dense Light Field Images,"Jiwan Hur, Jae Young Lee, Jaehyun Choi, Junmo Kim","School of Electrical Engineering, KAIST, South Korea",100,South Korea,0,,"Light field (LF) camera captures rich information from a scene. Using the information, the LF de-occlusion (LF-DeOcc) task aims to reconstruct the occlusion-free center view image. Existing LF-DeOcc studies mainly focus on the sparsely sampled (sparse) LF images where most of the occluded regions are visible in other views due to the large disparity. In this paper, we expand LF-DeOcc in more challenging datasets, densely sampled (dense) LF images, which are taken by a micro-lens-based portable LF camera. Due to the small disparity ranges of dense LF images, most of the background regions are invisible in any view. To apply LF-DeOcc in both LF datasets, we propose a framework, ISTY, which is defined and divided into three roles: (1) extract LF features, (2) define the occlusion, and (3) inpaint occluded regions. By dividing the framework into three specialized components according to the roles, the development and analysis can be easier. Furthermore, an explainable intermediate representation, an occlusion mask, can be obtained in the proposed framework. The occlusion mask is useful for comprehensive analysis of the model and other applications by manipulating the mask. In experiments, qualitative and quantitative results show that the proposed framework outperforms state-of-the-art LF-DeOcc methods in both sparse and dense LF datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Hur_I_See-Through_You_A_Framework_for_Removing_Foreground_Occlusion_in_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hur_I_See-Through_You_A_Framework_for_Removing_Foreground_Occlusion_in_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030370/,"['Computer vision', 'Analytical models', 'Feature extraction', 'Cameras', 'Light fields', 'Task analysis', 'Image reconstruction']","['Light Field', 'Density Imaging', 'Sparse Imaging', 'Light Field Images', 'Dense Light Field', 'Qualitative Results', 'Large Disparities', 'Sparse Datasets', 'Camera Array', 'Occluded Regions', 'Disparity Range', 'Training Dataset', 'Visual Information', 'Single Image', 'Background Information', 'Learning-based Methods', 'Fusion Method', 'Peak Signal-to-noise Ratio', 'Channel Dimension', 'Deep Learning-based Methods', 'Masked Images', 'Foreground Objects', 'Attention Map', 'Self-attention Module', 'Structural Similarity Index Measure', 'Decoder Features', 'Foreground-background', 'Background Objects', 'Score Map', 'Feature Fusion Method']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision']",3,"Light field (LF) camera captures rich information from a scene. Using the information, the LF de-occlusion (LF-DeOcc) task aims to reconstruct the occlusion-free center view image. Existing LF-DeOcc studies mainly focus on the sparsely sampled (sparse) LF images where most of the occluded regions are visible in other views due to the large disparity. In this paper, we expand LF-DeOcc in more challenging datasets, densely sampled (dense) LF images, which are taken by a micro-lens-based portable LF camera. Due to the small disparity ranges of dense LF images, most of the background regions are invisible in any view. To apply LF-DeOcc in both LF datasets, we propose a framework, ISTY, which is defined and divided into three roles: (1) extract LF features, (2) define the occlusion, and (3) inpaint occluded regions. By dividing the framework into three specialized components according to the roles, the development and analysis can be easier. Furthermore, an explainable intermediate representation, an occlusion mask, can be obtained in the proposed framework. The occlusion mask is useful for comprehensive analysis of the model and other applications by manipulating the mask. In experiments, qualitative and quantitative results show that the proposed framework outperforms state-of-the-art LF-DeOcc methods in both sparse and dense LF datasets."
IDD-3D: Indian Driving Dataset for 3D Unstructured Road Scenes,"Shubham Dokania, A. H. Abdul Hafez, Anbumani Subramanian, Manmohan Chandraker, C. V. Jawahar",IIIT Hyderabad; Hasan Kalyoncu University; UC San Diego,100,"India, Turkey, USA",0,,"Autonomous driving and assistance systems rely on annotated data from traffic and road scenarios to model and learn the various object relations in complex real-world scenarios. Preparation and training of deploy-able deep learning architectures require the models to be suited to different traffic scenarios and adapt to different situations. Currently, existing datasets, while large-scale, lack such diversities and are geographically biased towards mainly developed cities. An unstructured and complex driving layout found in several developing countries such as India poses a challenge to these models due to the sheer degree of variations in the object types, densities, and locations. To facilitate better research toward accommodating such scenarios, we build a new dataset,  IDD-3D , which consists of multi-modal data from multiple cameras and LiDAR sensors with 12k annotated driving LiDAR frames across various traffic scenarios. We discuss the need for this dataset through statistical comparisons with existing datasets and highlight benchmarks on standard 3D object detection and tracking tasks in complex layouts.",https://openaccess.thecvf.com/content/WACV2023/html/Dokania_IDD-3D_Indian_Driving_Dataset_for_3D_Unstructured_Road_Scenes_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dokania_IDD-3D_Indian_Driving_Dataset_for_3D_Unstructured_Road_Scenes_WACV_2023_paper.pdf,,https://github.com/shubham1810/idd3d_kit.git,,main,Poster,https://ieeexplore.ieee.org/document/10030865/,"['Training', 'Adaptation models', 'Three-dimensional displays', 'Laser radar', 'Roads', 'Urban areas', 'Layout']","['Street Scenes', 'DRIVE Dataset', 'Object Detection', 'Detection Task', 'Multiple Sensors', 'Object Relations', 'Object Tracking', 'Object Detection Task', 'Traffic Scenarios', 'LiDAR Sensor', '3D Tracking', '3D Object Detection', 'Complex Layout', 'Point Cloud', 'Bounding Box', 'Traffic Congestion', 'Distribution Of Categories', 'Safety-critical', 'Score Map', 'Unstructured Environments', '3D Bounding Box', 'Multi-object Tracking', 'Scene Perception', 'Object Bounding Boxes']","['Applications: Robotics', '3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",7,"Autonomous driving and assistance systems rely on annotated data from traffic and road scenarios to model and learn the various object relations in complex real-world scenarios. Preparation and training of deploy-able deep learning architectures require the models to be suited to different traffic scenarios and adapt to different situations. Currently, existing datasets, while large-scale, lack such diversities and are geographically biased towards mainly developed cities. An unstructured and complex driving layout found in several developing countries such as India poses a challenge to these models due to the sheer degree of variations in the object types, densities, and locations. To facilitate better research toward accommodating such scenarios, we build a new dataset, IDD-3D, which consists of multimodal data from multiple cameras and LiDAR sensors with 12k annotated driving LiDAR frames across various traffic scenarios. We discuss the need for this dataset through statistical comparisons with existing datasets and highlight benchmarks on standard 3D object detection and tracking tasks in complex layouts. Code and data available
<sup>1</sup>
."
IFQA: Interpretable Face Quality Assessment,"Byungho Jo, Donghyeon Cho, In Kyu Park, Sungeun Hong",Inha University; Chungnam National University,100,South Korea,0,,"Existing face restoration models have relied on general assessment metrics that do not consider the characteristics of facial regions. Recent works have therefore assessed their methods using human studies, which is not scalable and involves significant effort. This paper proposes a novel face-centric metric based on an adversarial framework where a generator simulates face restoration and a discriminator assesses image quality. Specifically, our per-pixel discriminator enables interpretable evaluation that cannot be provided by traditional metrics. Moreover, our metric emphasizes facial primary regions considering that even minor changes to the eyes, nose, and mouth significantly affect human cognition. Our face-oriented metric consistently surpasses existing general or facial image quality assessment metrics by impressive margins. We demonstrate the generalizability of the proposed strategy in various architectural designs and challenging scenarios. Interestingly, we find that our IFQA can lead to performance improvement as an objective function. The code and models are available at https://github.com/VCLLab/IFQA.",https://openaccess.thecvf.com/content/WACV2023/html/Jo_IFQA_Interpretable_Face_Quality_Assessment_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jo_IFQA_Interpretable_Face_Quality_Assessment_WACV_2023_paper.pdf,,https://github.com/VCLLab/IFQA,2211.07077,main,Poster,https://ieeexplore.ieee.org/document/10030259/,"['Measurement', 'Image quality', 'Psychology', 'Nose', 'Mouth', 'Linear programming', 'Image restoration']","['Quality Assessment', 'Human Studies', 'Objective Function', 'Image Quality', 'Human Cognition', 'Face Images', 'General Assessment', 'Challenging Scenarios', 'Assessment Metrics', 'Traditional Metrics', 'General Metrics', 'Quality Assessment Metrics', 'Low Scores', 'Face Recognition', 'High-quality Images', 'Facial Features', 'Real-world Scenarios', 'Reference Image', 'Real-world Datasets', 'Regional Scores', 'Real-world Images', 'Human Vision', 'Face Recognition Task', 'Low-quality Images', 'Human Faces', 'Rank-order Correlation']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Computational photography', 'image and video synthesis']",7,"Existing face restoration models have relied on general assessment metrics that do not consider the characteristics of facial regions. Recent works have therefore assessed their methods using human studies, which is not scalable and involves significant effort. This paper proposes a novel face-centric metric based on an adversarial framework where a generator simulates face restoration and a discriminator assesses image quality. Specifically, our per-pixel discriminator enables interpretable evaluation that cannot be provided by traditional metrics. Moreover, our metric emphasizes facial primary regions considering that even minor changes to the eyes, nose, and mouth significantly affect human cognition. Our face-oriented metric consistently surpasses existing general or facial image quality assessment metrics by impressive margins. We demonstrate the generalizability of the proposed strategy in various architectural designs and challenging scenarios. Interestingly, we find that our IFQA can lead to performance improvement as an objective function. The code and models are available at https://github.com/VCLLab/IFQA."
ImPosing: Implicit Pose Encoding for Efficient Visual Localization,"Arthur Moreau, Thomas Gilles, Nathan Piasco, Dzmitry Tsishkou, Bogdan Stanciulescu, Arnaud de La Fortelle","Huawei Technologies; MINES ParisTech, Huawei Technologies; MINES ParisTech",66.66666667,France,33.33333333,China,"We propose a novel learning-based formulation for visual localization of vehicles that can operate in real-time in city-scale environments. Visual localization algorithms determine the position and orientation from which an image has been captured, using a set of geo-referenced images or a 3D scene representation. Our new localization paradigm, named Implicit Pose Encoding (ImPosing), embeds images and camera poses into a common latent representation with 2 separate neural networks, such that we can compute a similarity score for each image-pose pair. By evaluating candidates through the latent space in a hierarchical manner, the camera position and orientation are not directly regressed but incrementally refined. Very large environments force competitors to store gigabytes of map data, whereas our method is very compact independently of the reference database size. In this paper, we describe how to effectively optimize our learned modules, how to combine them to achieve real-time localization, and demonstrate results on diverse large scale scenarios that significantly outperform prior work in accuracy and computational efficiency.",https://openaccess.thecvf.com/content/WACV2023/html/Moreau_ImPosing_Implicit_Pose_Encoding_for_Efficient_Visual_Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Moreau_ImPosing_Implicit_Pose_Encoding_for_Efficient_Visual_Localization_WACV_2023_paper.pdf,,,2205.02638,main,Poster,https://ieeexplore.ieee.org/document/10030856/,"['Location awareness', 'Manifolds', 'Learning systems', 'Visualization', 'Three-dimensional displays', 'Neural networks', 'Pose estimation']","['Visual Localization', 'Pose Encoder', 'Neural Network', 'Paradigm Shift', 'Similarity Score', 'Latent Space', 'Local Algorithm', 'Latent Representation', 'Camera Pose', 'Classical Methods', 'Number Of Images', 'Point Cloud', 'Learning-based Methods', 'Image Representation', 'Pose Estimation', 'Image Retrieval', 'Euler Angles', 'Global Image', 'Memory Footprint', 'Decoder Layer', 'Image Encoder', 'Implicit Representation', 'Global Descriptors', 'Coarse Localization', 'Query Image', 'Camera Pose Estimation', 'Geometric Reasoning', 'View Synthesis', '3D Rotation', 'Deep Neural Network']","['Algorithms: 3D computer vision', 'Robotics']",10,"We propose a novel learning-based formulation for visual localization of vehicles that can operate in real-time in city-scale environments. Visual localization algorithms determine the position and orientation from which an image has been captured, using a set of geo-referenced images or a 3D scene representation. Our new localization paradigm, named Implicit Pose Encoding (ImPosing), embeds images and camera poses into a common latent representation with 2 separate neural networks, such that we can compute a similarity score for each image-pose pair. By evaluating candidates through the latent space in a hierarchical manner, the camera position and orientation are not directly regressed but incrementally refined. Very large environments force competitors to store gigabytes of map data, whereas our method is very compact independently of the reference database size. In this paper, we describe how to effectively optimize our learned modules, how to combine them to achieve real-time localization, and demonstrate results on diverse large scale scenarios that significantly outperform prior work in accuracy and computational efficiency."
Image Completion With Heterogeneously Filtered Spectral Hints,"Xingqian Xu, Shant Navasardyan, Vahram Tadevosyan, Andranik Sargsyan, Yadong Mu, Humphrey Shi",Peking University; Picsart AI Research (PAIR); SHI Lab @ UIUC & UO,66.66666667,"China, USA",33.33333333,Ukraine,"Image completion with large-scale free-form missing regions is one of the most challenging tasks for the computer vision community. While researchers pursue better solutions, drawbacks such as pattern unawareness, blurry textures, and structure distortion remain noticeable, and thus leave space for improvement. To overcome these challenges, we propose a new StyleGAN-based image completion network, Spectral Hint GAN (SH-GAN), inside which a carefully designed spectral processing module, Spectral Hint Unit, is introduced. We also propose two novel 2D spectral processing strategies, Heterogeneous Filtering, and Gaussian Split that well-fit modern deep learning models and may further be extended to other tasks. From our inclusive experiments, we demonstrate that our model can reach FID scores of 3.4134 and 7.0277 on the benchmark datasets FFHQ and Places2, and therefore outperforms prior works and reaches a new state-of-the-art. We also prove the effectiveness of our design via ablation studies, from which one may notice that the aforementioned challenges, i.e. pattern unawareness, blurry textures, and structure distortion, can be noticeably resolved. Our code will be open-sourced at: https://github.com/SHI-Labs/SH-GAN.",https://openaccess.thecvf.com/content/WACV2023/html/Xu_Image_Completion_With_Heterogeneously_Filtered_Spectral_Hints_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xu_Image_Completion_With_Heterogeneously_Filtered_Spectral_Hints_WACV_2023_paper.pdf,,https://github.com/SHI-Labs/SH-GAN,2211.037,main,Poster,https://ieeexplore.ieee.org/document/10030408/,"['Deep learning', 'Computer vision', 'Image resolution', 'Codes', 'Filtering', 'Transforms', 'Benchmark testing']","['Deep Learning', 'Computer Vision', 'Fréchet Inception Distance', 'Fourier Transform', 'Semantic', 'Training Set', 'Validation Set', 'Frequency Domain', 'Feature Maps', 'Spectral Analysis', 'Fast Fourier Transform', 'Wavelet Transform', 'Spatial Domain', 'Peak Signal-to-noise Ratio', 'Discrete Fourier Transform', 'Image Synthesis', 'Inpainting', 'Difference Of Gaussian', 'Spectral Transformation', 'Synthesis Network', 'Aliasing Effect', 'Computer Vision Research', 'Training Details']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Computational photography', 'image and video synthesis', 'Arts/games/social media']",13,"Image completion with large-scale free-form missing regions is one of the most challenging tasks for the computer vision community. While researchers pursue better solutions, drawbacks such as pattern unawareness, blurry textures, and structure distortion remain noticeable, and thus leave space for improvement. To overcome these challenges, we propose a new StyleGAN-based image completion network, Spectral Hint GAN (SH-GAN), inside which a carefully designed spectral processing module, Spectral Hint Unit, is introduced. We also propose two novel 2D spectral processing strategies, Heterogeneous Filtering and Gaussian Split that well-fit modern deep learning models and may further be extended to other tasks. From our inclusive experiments, we demonstrate that our model can reach FID scores of 3.4134 and 7.0277 on the benchmark datasets FFHQ and Places2, and therefore outperforms prior works and reaches a new state-of-the-art. We also prove the effectiveness of our design via ablation studies, from which one may notice that the aforementioned challenges, i.e. pattern unawareness, blurry textures, and structure distortion, can be noticeably resolved. Our code will be open-sourced at: https://github.com/SHI-Labs/SH-GAN."
Image Segmentation-Based Unsupervised Multiple Objects Discovery,"Sandra Kara, Hejer Ammar, Florian Chabot, Quoc-Cuong Pham","Universit ´e Paris-Saclay, CEA, List, F-91120, Palaiseau, France",100,France,0,,"Unsupervised object discovery aims to localize objects in images, while removing the dependence on annotations required by most deep learning-based methods. To address this problem, we propose a fully unsupervised, bottom-up approach, for multiple objects discovery. The proposed approach is a two-stage framework. First, instances of object parts are segmented by using the intra-image similarity between self-supervised local features. The second step merges and filters the object parts to form complete object instances. The latter is performed by two CNN models that capture semantic information on objects from the entire dataset. We demonstrate that the pseudo-labels generated by our method provide a better precision-recall trade-off than existing single and multiple objects discovery methods. In particular, we provide state-of-the-art results for both unsupervised class-agnostic object detection and unsupervised image segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Kara_Image_Segmentation-Based_Unsupervised_Multiple_Objects_Discovery_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kara_Image_Segmentation-Based_Unsupervised_Multiple_Objects_Discovery_WACV_2023_paper.pdf,,,2212.10124,main,Poster,https://ieeexplore.ieee.org/document/10030964/,"['Learning systems', 'Image segmentation', 'Computer vision', 'Annotations', 'Semantics', 'Object detection', 'Information filters']","['Multiple Objects', 'Unsupervised Discovery', 'Image Segmentation', 'Object Detection', 'Image Object', 'Semantic Information', 'Object Location', 'Object Parts', 'CNN Model', 'Object Instances', 'Image Dataset', 'Unsupervised Learning', 'Multi-label', 'Intersection Over Union', 'Bounding Box', 'Local Clustering', 'Segmentation Task', 'Semantic Knowledge', 'Vision Transformer', 'Object Proposals', 'Object Regions', 'Eigenvectors', 'High Dissimilarity', 'Number Of Proposals', 'Unsupervised Way', 'Saliency Detection', 'PASCAL VOC Dataset', 'Objective Scores']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Unsupervised object discovery aims to localize objects in images, while removing the dependence on annotations required by most deep learning-based methods. To address this problem, we propose a fully unsupervised, bottom-up approach, for multiple objects discovery. The proposed approach is a two-stage framework. First, instances of object parts are segmented by using the intra-image similarity between self-supervised local features. The second step merges and filters the object parts to form complete object instances. The latter is performed by two CNN models that capture semantic information on objects from the entire dataset. We demonstrate that the pseudo-labels generated by our method provide a better precision-recall trade-off than existing single and multiple objects discovery methods. In particular, we provide state-of-the-art results for both unsupervised class-agnostic object detection and unsupervised image segmentation."
Image-Consistent Detection of Road Anomalies As Unpredictable Patches,"Tomáš Vojíř, Jiří Matas","Czech Technical University in Prague, FEE",100,Czech Republic,0,,"We propose a novel method for anomaly detection primarily aiming at autonomous driving. The design of the method, called DaCUP (Detection of anomalies as Consistent Unpredictable Patches), is based on two general properties of anomalous objects: an anomaly is (i) not from a class that could be modelled and (ii) it is not similar (in appearance) to non-anomalous objects in the image. To this end, we propose a novel embedding bottleneck in an auto-encoder like architecture that enables modelling of a diverse, multi-modal known class appearance (e.g. road). Secondly, we introduce novel image-conditioned distance features that allow known class identification in a nearest-neighbour manner on-the-fly, greatly increasing its ability to distinguish true and false positives. Lastly, an inpainting module is utilized to model the uniqueness of detected anomalies and significantly reduce false positives by filtering regions that are similar, thus reconstructable from their neighbourhood. We demonstrate that filtering of regions based on their similarity to neighbour regions, using e.g. an inpainting module, is general and can be used with other methods for reduction of false positives. The proposed method is evaluated on several publicly available datasets for road anomaly detection and on a maritime benchmark for obstacle avoidance. The method achieves state-of-the-art performance in both tasks with the same hyper-parameters with no domain specific design.",https://openaccess.thecvf.com/content/WACV2023/html/Vojir_Image-Consistent_Detection_of_Road_Anomalies_As_Unpredictable_Patches_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Vojir_Image-Consistent_Detection_of_Road_Anomalies_As_Unpredictable_Patches_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030141/,"['Computer vision', 'Filtering', 'Roads', 'Design methodology', 'Benchmark testing', 'Feature extraction', 'Task analysis']","['Anomaly Detection', 'Road Anomalies', 'False Positive', 'True Positive', 'Image Object', 'Object Properties', 'Obstacle Avoidance', 'Training Data', 'Positive Samples', 'Input Image', 'Negative Samples', 'Latent Space', 'Semantic Segmentation', 'Small Patches', 'Image Patches', 'Semantic Network', 'Perceptual Loss', 'Triplet Loss', 'Semantic Segmentation Network', 'Atrous Spatial Pyramid Pooling', 'Reconstruction Module', 'Anomaly Score', 'Road Class', 'Road Region', 'Lane Markings', 'Backbone Feature', 'VGG Network', 'Softmax', 'Feature Channels', 'Surface Appearance']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",5,"We propose a novel method for anomaly detection primarily aiming at autonomous driving. The design of the method, called DaCUP (Detection of anomalies as Consistent Unpredictable Patches), is based on two general properties of anomalous objects: an anomaly is (i) not from a class that could be modelled and (ii) it is not similar (in appearance) to non-anomalous objects in the image. To this end, we propose a novel embedding bottleneck in an auto-encoder like architecture that enables modelling of a diverse, multi-modal known class appearance (e.g. road). Secondly, we introduce novel image-conditioned distance features that allow known class identification in a nearest-neighbour manner on-the-fly, greatly increasing its ability to distinguish true and false positives. Lastly, an inpainting module is utilized to model the uniqueness of detected anomalies and significantly reduce false positives by filtering regions that are similar, thus reconstructable from their neighbourhood. We demonstrate that filtering of regions based on their similarity to neighbour regions, using e.g. an inpainting module, is general and can be used with other methods for reduction of false positives. The proposed method is evaluated on several publicly available datasets for road anomaly detection and on a maritime benchmark for obstacle avoidance. The method achieves state-of-the-art performance in both tasks with the same hyper-parameters with no domain specific design."
Image-Free Domain Generalization via CLIP for 3D Hand Pose Estimation,"Seongyeong Lee, Hansoo Park, Dong Uk Kim, Jihyeon Kim, Muhammadjon Boboev, Seungryul Baek","UNIST, South Korea; UNIST, South Korea and NC Soft, South Korea",100,South Korea,0,,"RGB-based 3D hand pose estimation has been successful for decades thanks to large-scale databases and deep learning. However, the hand pose estimation network does not operate well for hand pose images whose characteristics are far different from the training data. This is caused by various factors such as illuminations, camera angles, diverse backgrounds in the input images, etc. Many existing methods tried to solve it by supplying additional large-scale unconstrained/target domain images to augment data space; however collecting such large-scale images takes a lot of labors. In this paper, we present a simple image-free domain generalization approach for the hand pose estimation framework that uses only source domain data. We try to manipulate the image features of the hand pose estimation network by adding the features from text descriptions using the CLIP (Contrastive Language-Image Pre-training) model. The manipulated image features are then exploited to train the hand pose estimation network via the contrastive learning framework. In experiments with STB and RHD datasets, our algorithm shows improved performance over the state-of-the-art domain generalization approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Image-Free_Domain_Generalization_via_CLIP_for_3D_Hand_Pose_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Image-Free_Domain_Generalization_via_CLIP_for_3D_Hand_Pose_Estimation_WACV_2023_paper.pdf,,,2210.16788,main,Poster,https://ieeexplore.ieee.org/document/10030743/,"['Training', 'Learning systems', 'Deep learning', 'Three-dimensional displays', 'Error analysis', 'Databases', 'Pose estimation']","['Pose Estimation', 'Domain Generalization', 'Hand Pose', 'Hand Pose Estimation', '3D Hand Pose Estimation', 'Contrastive Language-Image Pre-training', 'Image Features', 'Input Image', 'Data Augmentation', 'Source Domain', 'Self-supervised Learning', 'Source Domain Data', 'Indoor Environments', 'RGB Images', 'Depth Map', 'Target Domain', 'Unlabeled Data', 'Remainder Of This Section', 'Domain Adaptation', 'Graph Convolutional Network', '3D Pose', 'Image Encoder', 'Domain Dataset', 'Image X', 'Text Encoder', 'Target Domain Data', 'Human Pose Estimation', 'Encoding Vector', 'Contrastive Loss', 'Unseen Domains']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision', 'Vision + language and/or other modalities']",6,"RGB-based 3D hand pose estimation has been successful for decades thanks to large-scale databases and deep learning. However, the hand pose estimation network does not operate well for hand pose images whose characteristics are far different from the training data. This is caused by various factors such as illuminations, camera angles, diverse backgrounds in the input images, etc. Many existing methods tried to solve it by supplying additional large-scale unconstrained/target domain images to augment data space; however collecting such large-scale images takes a lot of labors. In this paper, we present a simple image-free domain generalization approach for the hand pose estimation framework that uses only source domain data. We try to manipulate the image features of the hand pose estimation network by adding the features from text descriptions using the CLIP (Contrastive Language-Image Pretraining) model. The manipulated image features are then exploited to train the hand pose estimation network via the contrastive learning framework. In experiments with STB and RHD datasets, our algorithm shows improved performance over the state-of-the-art domain generalization approaches."
Image-Text Pre-Training for Logo Recognition,"Mark Hubenthal, Suren Kumar",Amazon Inc.,0,,100,USA,"Open-set logo recognition is commonly solved by first detecting possible logo regions and then matching the detected parts against an ever-evolving dataset of cropped logo images. The matching model, a metric learning problem, is especially challenging for logo recognition due to the mixture of text and symbols in logos. We propose two novel contributions to improve the matching model's performance: (a) using image-text paired samples for pre-training, and (b) an improved metric learning loss function. A standard paradigm of fine-tuning ImageNet pre-trained models fails to discover the text sensitivity necessary to solve the matching problem effectively. This work demonstrates the importance of pre-training on image-text pairs, which significantly improves the performance of a visual embedder trained for the logo retrieval task, especially for more text-dominant classes. We construct a composite public logo dataset combining LogoDet3K, OpenLogo, and FlickrLogos-47 deemed OpenLogoDet3K47. We show that the same vision backbone pre-trained on image-text data, when fine-tuned on OpenLogoDet3K47, achieves 98.6% recall@1, significantly improving performance over pre-training on Imagenet1K (97.6%). We generalize the ProxyNCA++ loss function to propose ProxyNCAHN++ which incorporates class-specific hard negative images. The proposed method sets new state-of-the-art on five public logo datasets considered, with a 3.5% zero-shot recall@1 improvement on LogoDet3K test, 4% on OpenLogo, 6.5% on FlickrLogos-47, 6.2% on Logos In The Wild, and 0.6% on BelgaLogo.",https://openaccess.thecvf.com/content/WACV2023/html/Hubenthal_Image-Text_Pre-Training_for_Logo_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hubenthal_Image-Text_Pre-Training_for_Logo_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030524/,"['Visualization', 'Sensitivity', 'Image recognition', 'Text recognition', 'Pipelines', 'Optical character recognition', 'Symbols']","['Logo Recognition', 'Loss Function', 'Public Datasets', 'Matching Model', 'Metric Learning', 'Training Set', 'Deep Learning', 'Learning Rate', 'Hyperparameters', 'Input Image', 'Bounding Box', 'Image Recognition', 'Set Of Classes', 'Positive Class', 'Closest Neighbors', 'Optical Character Recognition', 'Contrastive Loss', 'Query Set', 'Final Pool', 'Scale-invariant Feature Transform', 'Vision Transformer', 'Triplet Loss', 'Visual Question Answering', 'Gallery Set', 'E-commerce Websites', 'Batch Of Images', 'Deep Neural Network', 'Backbone Network', 'Object Detection', 'Large Datasets']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",,"Open-set logo recognition is commonly solved by first detecting possible logo regions and then matching the detected parts against an ever-evolving dataset of cropped logo images. The matching model, a metric learning problem, is especially challenging for logo recognition due to the mixture of text and symbols in logos. We propose two novel contributions to improve the matching model’s performance: (a) using image-text paired samples for pre-training, and (b) an improved metric learning loss function. A standard paradigm of fine-tuning ImageNet pre-trained models fails to discover the text sensitivity necessary to solve the matching problem effectively. This work demonstrates the importance of pre-training on image-text pairs, which significantly improves the performance of a visual embedder trained for the logo retrieval task, especially for more text-dominant classes. We construct a composite public logo dataset combining LogoDet3K, OpenLogo, and FlickrLogos-47 deemed OpenLogoDet3K47. We show that the same vision backbone pre-trained on image-text data, when fine-tuned on OpenLogoDet3K47, achieves 98.6% recall@1, significantly improving performance over pre-training on Imagenet1K (97.6%). We generalize the ProxyNCA++ loss function to propose ProxyNCAHN++ which incorporates class-specific hard negative images. The proposed method sets new state-of-the-art on five public logo datasets considered, with a 3.5% zero-shot recall@1 improvement on LogoDet3K test, 4% on OpenLogo, 6.5% on FlickrLogos-47, 6.2% on Logos In The Wild, and 0.6% on BelgaLogo."
ImpDet: Exploring Implicit Fields for 3D Object Detection,"Xuelin Qian, Li Wang, Yi Zhu, Li Zhang, Yanwei Fu, Xiangyang Xue",Agora; Amazon Inc.; Fudan University,33.33333333,China,66.66666667,Greece,"Conventional 3D object detection approaches concentrate on bounding boxes representation learning with several parameters, i.e., localization, dimension, and orientation. Despite its popularity and universality, such a straightforward paradigm is sensitive to slight numerical deviations, especially in localization. By exploiting the property that point clouds are naturally captured on the surface of objects along with accurate location and intensity information, we introduce a new perspective that views bounding box regression as an implicit function. This leads to our proposed framework, termed Implicit Detection or ImpDet, which leverages implicit field learning for 3D object detection. Our ImpDet assigns specific values to points in different local 3D spaces, thereby high-quality boundaries can be generated by classifying points inside or outside the boundary. To solve the problem of sparsity on the object surface, we further present a simple yet efficient virtual sampling strategy to not only fill the empty region, but also learn rich semantic features to help refine the boundaries. Extensive experimental results on KITTI and Waymo benchmarks demonstrate the effectiveness and robustness of unifying implicit fields into object detection.",https://openaccess.thecvf.com/content/WACV2023/html/Qian_ImpDet_Exploring_Implicit_Fields_for_3D_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qian_ImpDet_Exploring_Implicit_Fields_for_3D_Object_Detection_WACV_2023_paper.pdf,,,2203.1724,main,Poster,https://ieeexplore.ieee.org/document/10030846/,"['Location awareness', 'Representation learning', 'Point cloud compression', 'Computer vision', 'Three-dimensional displays', 'Semantics', 'Object detection']","['Object Detection', '3D Detection', '3D Object Detection', 'Implicit Field', 'Sparsity', 'Extensive Experiments', 'Point Cloud', '3D Space', 'Bounding Box', 'Local Space', 'Implicit Function', 'Object Surface', 'Implicit Learning', 'Accurate Location Information', 'Convolutional Neural Network', 'Convolutional Layers', 'Multilayer Perceptron', 'Average Precision', 'Night Shift', 'Feature Points', 'Virtual Point', 'KITTI Dataset', 'Implicit Values', 'Multilayer Perceptron Layer', '3D Bounding Box', 'Minimum Bounding Box', 'Raw Point', 'Segmentation Branch', 'Centrosymmetry', 'Raw Point Cloud']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",3,"Conventional 3D object detection approaches concentrate on bounding boxes representation learning with several parameters, i.e., localization, dimension, and orientation. Despite its popularity and universality, such a straightforward paradigm is sensitive to slight numerical deviations, especially in localization. By exploiting the property that point clouds are naturally captured on the surface of objects along with accurate location and intensity information, we introduce a new perspective that views bounding box regression as an implicit function. This leads to our proposed framework, termed Implicit Detection or ImpDet, which leverages implicit field learning for 3D object detection. Our ImpDet assigns specific values to points in different local 3D spaces, thereby high-quality boundaries can be generated by classifying points inside or outside the boundary. To solve the problem of sparsity on the object surface, we further present a simple yet efficient virtual sampling strategy to not only fill the empty region, but also learn rich semantic features to help refine the boundaries. Extensive experimental results on KITTI and Waymo benchmarks demonstrate the effectiveness and robustness of unifying implicit fields into object detection."
Improving Deep Facial Phenotyping for Ultra-Rare Disorder Verification Using Model Ensembles,"Alexander Hustinx, Fabio Hellmann, Ömer Sümer, Behnam Javanmardi, Elisabeth André, Peter Krawitz, Tzung-Chien Hsieh","Institute for Genomic Statistics and Bioinformatics, University Hospital Bonn, University of Bonn; Chair for Human-Centered Artificial Intelligence, University of Augsburg",100,Germany,0,,"Rare genetic disorders affect more than 6% of the global population. Reaching a diagnosis is challenging because rare disorders are very diverse. Many disorders have recognizable facial features that are hints for clinicians to diagnose patients. Previous work, such as GestaltMatcher, utilized representation vectors produced by a DCNN similar to AlexNet to match patients in high-dimensional feature space to support ""unseen"" ultra-rare disorders. However, the architecture and dataset used for transfer learning in GestaltMatcher have become outdated. Moreover, a way to train the model for generating better representation vectors for unseen ultra-rare disorders has not yet been studied. Because of the overall scarcity of patients with ultra-rare disorders, it is infeasible to directly train a model on them. Therefore, we first analyzed the influence of replacing GestaltMatcher DCNN with a state-of-the-art face recognition approach, iResNet with ArcFace. Additionally, we experimented with different face recognition datasets for transfer learning. Furthermore, we proposed test-time augmentation, and model ensembles that mix general face verification models and models specific for verifying disorders to improve the disorder verification accuracy of unseen ultra-rare disorders. Our proposed ensemble model achieves state-of-the-art performance on both seen and unseen disorders. Code is available at https://www.github.com/igsb/GestaltMatcher-Arc",https://openaccess.thecvf.com/content/WACV2023/html/Hustinx_Improving_Deep_Facial_Phenotyping_for_Ultra-Rare_Disorder_Verification_Using_Model_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hustinx_Improving_Deep_Facial_Phenotyping_for_Ultra-Rare_Disorder_Verification_Using_Model_WACV_2023_paper.pdf,,github.com/igsb/GestaltMatcher-Arc,,main,Poster,https://ieeexplore.ieee.org/document/10030218/,"['Computer vision', 'Codes', 'Face recognition', 'Computational modeling', 'Transfer learning', 'Sociology', 'Computer architecture']","['Ensemble Model', 'Ultra-rare Disorders', 'Genetic Disorders', 'Rare Disease', 'Feature Space', 'Transfer Learning', 'Deep Convolutional Neural Network', 'Face Recognition', 'Rare Disorder', 'Facial Features', 'Vector Representation', 'Model Verification', 'Model Performance', 'Training Set', 'Large Datasets', 'Cross-entropy', 'Small Datasets', 'Batch Normalization', 'Mean Accuracy', 'Individual Datasets', 'Top-1 Accuracy', 'Dysmorphic Features', 'Overview Of The Dataset', 'Top-5 Accuracy', 'Long-tailed Distribution', 'Fine-tuned Model', 'Gallery Set', 'Frontal Images', 'Inference Scheme', 'Classification Layer']","['Applications: Biomedical/healthcare/medicine', 'Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",16,"Rare genetic disorders affect more than 6% of the global population. Reaching a diagnosis is challenging because rare disorders are very diverse. Many disorders have recognizable facial features that are hints for clinicians to diagnose patients. Previous work, such as GestaltMatcher, utilized representation vectors produced by a DCNN similar to AlexNet to match patients in high-dimensional feature space to support ""unseen"" ultra-rare disorders. However, the architecture and dataset used for transfer learning in GestaltMatcher have become outdated. Moreover, a way to train the model for generating better representation vectors for unseen ultra-rare disorders has not yet been studied. Because of the overall scarcity of patients with ultra-rare disorders, it is infeasible to directly train a model on them. Therefore, we first analyzed the influence of replacing GestaltMatcher DCNN with a state-of-the-art face recognition approach, iResNet with ArcFace. Additionally, we experimented with different face recognition datasets for transfer learning. Furthermore, we proposed test-time augmentation, and model ensembles that mix general face verification models and models specific for verifying disorders to improve the disorder verification accuracy of unseen ultra-rare disorders. Our proposed ensemble model achieves state-of-the-art performance on both seen and unseen disorders. Code is available at github.com/igsb/GestaltMatcher-Arc."
Improving Diversity With Adversarially Learned Transformations for Domain Generalization,"Tejas Gokhale, Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Chitta Baral, Yezhou Yang",Arizona State University; Lawrence Livermore National Laboratory,100,USA,0,,"To be successful in single source domain generalization (SSDG), maximizing diversity of synthesized domains has emerged as one of the most effective strategies. Recent success in SSDG comes from methods that pre-specify diversity inducing image augmentations during training, so that it may lead to better generalization on new domains. However, naive pre-specified augmentations are not always effective, either because they cannot model large domain shift, or because the specific choice of transforms may not cover the types of shifts commonly occurring in domain generalization. To address this issue, we present a novel framework called ALT: adversarially learned transformations, that uses an adversary neural network to model plausible, yet hard image transformations that fool the classifier. ALT learns image transformations by randomly initializing the adversary network for each batch and optimizing it for a fixed number of steps to maximize classification error. The classifier is trained by enforcing a consistency between its predictions on the clean and transformed images. With extensive empirical analysis, we find that this new form of adversarial transformations achieves both objectives of diversity and hardness simultaneously, outperforming all existing techniques on competitive benchmarks for SSDG. We also show that ALT can seamlessly work with existing diversity modules to produce highly distinct, and large transformations of the source domain leading to state-of-the-art performance. Code: https://github.com/tejas-gokhale/ALT",https://openaccess.thecvf.com/content/WACV2023/html/Gokhale_Improving_Diversity_With_Adversarially_Learned_Transformations_for_Domain_Generalization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gokhale_Improving_Diversity_With_Adversarially_Learned_Transformations_for_Domain_Generalization_WACV_2023_paper.pdf,,https://github.com/tejas-gokhale/ALT,2206.07736,main,Poster,https://ieeexplore.ieee.org/document/10030875/,"['Training', 'Computer vision', 'Perturbation methods', 'Neural networks', 'Transforms', 'Benchmark testing', 'Performance gain']","['Generative Adversarial Networks', 'Domain Generalization', 'Hardness', 'Number Of Steps', 'Single Domain', 'Large Shift', 'Domain Shift', 'Adversarial Network', 'Source Domain', 'Image Transformation', 'Learning Rate', 'Convolutional Layers', 'Input Image', 'Total Loss', 'Data Augmentation', 'Generalization Performance', 'Kullback-Leibler', 'Target Domain', 'Adversarial Training', 'Source Dataset', 'Unseen Domains', 'Adversarial Examples', 'Adversarial Attacks', 'Multiple Benchmarks', 'Semantic Change', 'Adversarial Perturbations', 'Training Domain', 'Data Augmentation Methods', 'Neural Network Weights', 'Types Of Corruption']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",12,"To be successful in single source domain generalization (SSDG), maximizing diversity of synthesized domains has emerged as one of the most effective strategies. Recent success in SSDG comes from methods that pre-specify diversity inducing image augmentations during training, so that it may lead to better generalization on new domains. However, naïve pre-specified augmentations are not always effective, either because they cannot model large domain shift, or be-cause the specific choice of transforms may not cover the types of shift commonly occurring in domain generalization. To address this issue, we present a novel framework called ALT: adversarially learned transformations, that uses an adversary neural network to model plausible, yet hard image transformations that fool the classifier. ALT learns image transformations by randomly initializing the adversary net-work for each batch and optimizing it for a fixed number of steps to maximize classification error. The classifier is trained by enforcing a consistency between its predictions on the clean and transformed images. With extensive empirical analysis, we find that this new form of adversarial transformations achieves both objectives of diversity and hardness simultaneously, outperforming all existing techniques on competitive benchmarks for SSDG. We also show that ALT can seamlessly work with existing diversity modules to produce highly distinct, and large transformations of the source domain leading to state-of-the-art performance. Code: https://github.com/tejas-gokhale/ALT"
Improving Multi-Fidelity Optimization With a Recurring Learning Rate for Hyperparameter Tuning,"HyunJae Lee, Gihyeon Lee, Junhwan Kim, Sungjun Cho, Dohyun Kim, Donggeun Yoo",Lunit Inc.,0,,100,South Korea,"Despite the evolution of Convolutional Neural Networks (CNNs), their performance is surprisingly dependent on the choice of hyperparameters. However, it remains challenging to efficiently explore large hyperparameter search space due to the long training times of modern CNNs. Multi-fidelity optimization enables the exploration of more hyperparameter configurations given budget by early termination of unpromising configurations. However, it often results in selecting a sub-optimal configuration as training with the high-performing configuration typically converges slowly in an early phase. In this paper, we propose Multi-fidelity Optimization with a Recurring Learning rate (MORL) which incorporates CNNs' optimization process into multi-fidelity optimization. MORL alleviates the problem of slow-starter and achieves a more precise low-fidelity approximation. Our comprehensive experiments on general image classification, transfer learning, and semi-supervised learning demonstrate the effectiveness of MORL over other multi-fidelity optimization methods such as Successive Halving Algorithm (SHA) and Hyperband. Furthermore, it achieves significant performance improvements over hand-tuned hyperparameter configuration within a practical budget.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Improving_Multi-Fidelity_Optimization_With_a_Recurring_Learning_Rate_for_Hyperparameter_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Improving_Multi-Fidelity_Optimization_With_a_Recurring_Learning_Rate_for_Hyperparameter_WACV_2023_paper.pdf,,,2209.12499,main,Poster,https://ieeexplore.ieee.org/document/10030888/,"['Training', 'Schedules', 'Computer vision', 'Transfer learning', 'Optimization methods', 'Semisupervised learning', 'Convolutional neural networks']","['Learning Rate', 'Hyperparameter Tuning', 'Multi-fidelity Optimization', 'Early Phase', 'Convolutional Neural Network', 'Optimization Process', 'Search Space', 'Transfer Learning', 'Semi-supervised Learning', 'Hyperparameter Configuration', 'Probabilistic Model', 'Stochastic Gradient Descent', 'Object Classification', 'End Of Cycle', 'Final Performance', 'Early Stopping', 'Convolutional Neural Network Architecture', 'Range Of Tasks', 'Random Search', 'Number Of Configurations', 'Learning Rate Schedule', 'Wide Range Of Tasks', 'Bayesian Optimization', 'Tuning Process', 'Small Learning Rate', 'CIFAR-100 Dataset', 'Original Implementation', 'Minimal Resources', 'Maximum Resource', 'Performance In Phase']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Despite the evolution of Convolutional Neural Networks (CNNs), their performance is surprisingly dependent on the choice of hyperparameters. However, it remains challenging to efficiently explore large hyperparameter search space due to the long training times of modern CNNs. Multi-fidelity optimization enables the exploration of more hyperparameter configurations given budget by early termination of unpromising configurations. However, it often results in selecting a sub-optimal configuration as training with the high-performing configuration typically converges slowly in an early phase. In this paper, we propose Multi-fidelity Optimization with a Recurring Learning rate (MORL) which incorporates CNNs’ optimization process into multi-fidelity optimization. MORL alleviates the problem of slow-starter and achieves a more precise low-fidelity approximation. Our comprehensive experiments on general image classification, transfer learning, and semi-supervised learning demonstrate the effectiveness of MORL over other multi-fidelity optimization methods such as Successive Halving Algorithm (SHA) and Hyperband. Furthermore, it achieves significant performance improvements over hand-tuned hyperparameter configuration within a practical budget."
Improving Pixel-Level Contrastive Learning by Leveraging Exogenous Depth Information,"Ahmed Ben Saad, Kristina Prokopetc, Josselin Kherroubi, Axel Davy, Adrien Courtois, Gabriele Facciolo","ENS Paris-Saclay, Centre Borelli; Schlumberger AI Lab",50,France,50,USA,"Self-supervised representation learning based on Contrastive Learning (CL) has been the subject of much attention in recent years. This is due to the excellent results obtained on a variety of subsequent tasks (in particular classification), without requiring a large amount of labeled samples. However, most reference CL algorithms (such as SimCLR and MoCo, but also BYOL and Barlow Twins) are not adapted to pixel-level downstream tasks. One existing solution known as PixPro proposes a pixel-level approach that is based on filtering of pairs of positive/negative image crops of the same image using the distance between the crops in the whole image. We argue that this idea can be further enhanced by incorporating semantic information provided by exogenous data as an additional selection filter, which can be used (at training time) to improve the selection of the pixel-level positive/negative samples. In this paper we will focus on the depth information, which can be obtained by using a depth estimation network or measured from available data (stereovision, parallax motion, lidar, ...). Scene depth can provide meaningful cues to distinguish pixels belonging to different objects based on their depth. We show that using this exogenous information in the contrastive loss leads to improved results and that the learned representations better follow the shapes of objects. In addition, we introduce a multi-scale loss that alleviates the issue of finding the training parameters adapted to different object sizes. We demonstrate the effectiveness of our ideas on the Breakout Segmentation on Borehole Images where we achieve an improvement of 1.9% over PixPro and nearly 5% over the supervised baseline. We further validate our technique on the indoor scene segmentation tasks with ScanNet and outdoor scenes with CityScapes ( 1.6% and 1.1% improvement over PixPro respectively).",https://openaccess.thecvf.com/content/WACV2023/html/Saad_Improving_Pixel-Level_Contrastive_Learning_by_Leveraging_Exogenous_Depth_Information_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Saad_Improving_Pixel-Level_Contrastive_Learning_by_Leveraging_Exogenous_Depth_Information_WACV_2023_paper.pdf,,,2211.10177,main,Poster,https://ieeexplore.ieee.org/document/10030883/,"['Training', 'Representation learning', 'Image segmentation', 'Shape', 'Semantics', 'Estimation', 'Crops']","['Depth Information', 'Self-supervised Learning', 'Exogenous Information', 'Semantic Information', 'Trainable Parameters', 'Representation Learning', 'Segmentation Task', 'Depth Estimation', 'Stereopsis', 'Contrastive Loss', 'Transformer', 'Hyperparameters', 'Computer Vision', 'Batch Size', 'Feature Maps', 'Prior Information', 'Binocular', 'ImageNet', 'Semantic Segmentation', 'Depth Map', 'Pair Of Vectors', 'Quality Of Representations', 'Monocular Depth Estimation', 'Disparity Map', 'Selection Of Pairs', 'Geological Features', 'Group Of Pixels', 'Pre-training Dataset', 'Segmentation Scores', 'Feature Map Size']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Remote Sensing']",1,"Self-supervised representation learning based on Contrastive Learning (CL) has been the subject of much attention in recent years. This is due to the excellent results obtained on a variety of subsequent tasks (in particular classification), without requiring a large amount of labeled samples. However, most reference CL algorithms (such as SimCLR and MoCo, but also BYOL and Barlow Twins) are not adapted to pixel-level downstream tasks. One existing solution known as PixPro proposes a pixel-level approach that is based on filtering of pairs of positive/negative image crops of the same image using the distance between the crops in the whole image. We argue that this idea can be further enhanced by incorporating semantic information provided by exogenous data as an additional selection filter, which can be used (at training time) to improve the selection of the pixel-level positive/negative samples. In this paper we will focus on the depth information, which can be obtained by using a depth estimation network or measured from available data (stereovision, parallax motion, LiDAR, etc.). Scene depth can provide meaningful cues to distinguish pixels belonging to different objects based on their depth. We show that using this exogenous information in the contrastive loss leads to improved results and that the learned representations better follow the shapes of objects. In addition, we introduce a multi-scale loss that alleviates the issue of finding the training parameters adapted to different object sizes. We demonstrate the effectiveness of our ideas on the Breakout Segmentation on Borehole Images where we achieve an improvement of 1.9% over PixPro and nearly 5% over the supervised baseline. We further validate our technique on the indoor scene segmentation tasks with ScanNet and outdoor scenes with CityScapes (1.6% and 1.1% improvement over PixPro respectively)."
Improving Predicate Representation in Scene Graph Generation by Self-Supervised Learning,"So Hasegawa, Masayuki Hiromoto, Akira Nakagawa, Yuhei Umeda","Fujitsu Limited, Japan",100,USA,0,,"Scene graph generation (SGG) aims to understand sophisticated visual information by detecting triplets of subject, object, and their relationship (predicate). Since the predicate labels are heavily imbalanced, existing supervised methods struggle to improve accuracy for the rare predicates due to insufficient labeled data. In this paper, we propose SePiR, a novel self-supervised learning method for SGG to improve the representation of rare predicates. We first train a relational encoder by contrastive learning without using predicate labels, and then fine-tune a predicate classifier with labeled data. To apply contrastive learning to SGG, we newly propose data augmentation in which subject-object pairs are augmented by replacing their visual features with those from other images having the same object labels. By such augmentation, we can increase the variation of the visual features while keeping the relationship between the objects. Comprehensive experimental results on the Visual Genome dataset show that the SGG performance of SePiR is comparable to the state-of-the-art, and especially with the limited labeled dataset, our method significantly outperforms the existing supervised methods. Moreover, SePiR's improved representation enables the model architecture simpler, resulting in 3.6x and 6.3x reduction of the parameters and inference time from the existing method, independently.",https://openaccess.thecvf.com/content/WACV2023/html/Hasegawa_Improving_Predicate_Representation_in_Scene_Graph_Generation_by_Self-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hasegawa_Improving_Predicate_Representation_in_Scene_Graph_Generation_by_Self-Supervised_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030744/,"['Visualization', 'Computer vision', 'Genomics', 'Self-supervised learning', 'Computer architecture', 'Task analysis', 'Bioinformatics']","['Self-supervised Learning', 'Scene Graph', 'Predicate Representations', 'Visual Features', 'Data Augmentation', 'Inference Time', 'Object Labels', 'Self-supervised Learning Methods', 'Comprehensive Experimental Results', 'Local Features', 'Batch Size', 'Object Detection', 'Image Object', 'Bounding Box', 'Object Classification', 'Image Recognition', 'Number Of Objects', 'Word Embedding', 'Linguistic Features', 'Mean Of Recall', 'Imbalanced Class Distribution', 'Long-tailed Distribution', 'Self-supervised Manner', 'Imbalanced Distribution', 'Upper Area', 'Image Captioning', 'Visual Question Answering', 'Contrastive Loss', 'Semi-supervised Methods', 'Class Distribution']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"Scene graph generation (SGG) aims to understand sophisticated visual information by detecting triplets of subject, object, and their relationship (predicate). Since the predicate labels are heavily imbalanced, existing supervised methods struggle to improve accuracy for the rare predicates due to insufficient labeled data. In this paper, we propose SePiR, a novel self-supervised learning method for SGG to improve the representation of rare predicates. We first train a relational encoder by contrastive learning without using predicate labels, and then fine-tune a predicate classifier with labeled data. To apply contrastive learning to SGG, we newly propose data augmentation in which subject-object pairs are augmented by replacing their visual features with those from other images having the same object labels. By such augmentation, we can increase the variation of the visual features while keeping the relationship between the objects. Comprehensive experimental results on the Visual Genome dataset show that the SGG performance of SePiR is comparable to the state-of-theart, and especially with the limited labeled dataset, our method significantly outperforms the existing supervised methods. Moreover, SePiR’s improved representation enables the model architecture simpler, resulting in 3.6x and 6.3x reduction of the parameters and inference time from the existing method, independently."
Improving Saliency Models' Predictions of the Next Fixation With Humans' Intrinsic Cost of Gaze Shifts,"Florian Kadner, Tobias Thomas, David Hoppe, Constantin A. Rothkopf","Centre for Cognitive Science & Institute of Psychology, TU Darmstadt",100,Germany,0,,"The human prioritization of image regions can be modeled in a time invariant fashion with saliency maps or sequentially with scanpath models. However, while both types of models have steadily improved on several benchmarks and datasets, there is still a considerable gap in predicting human gaze. Here, we leverage two recent developments to reduce this gap: theoretical analyses establishing a principled framework for predicting the next gaze target and the empirical measurement of the human cost for gaze switches independently of image content. We introduce an algorithm in the framework of sequential decision making, which converts any static saliency map into a sequence of dynamic history-dependent value maps, which are recomputed after each gaze shift. These maps are based on 1) a saliency map provided by an arbitrary saliency model, 2) the recently measured human cost function quantifying preferences in magnitude and direction of eye movements, and 3) a sequential exploration bonus, which changes with each subsequent gaze shift. The parameters of the spatial extent and temporal decay of this exploration bonus are estimated from human gaze data. The relative contributions of these three components were optimized on the MIT1003 dataset for the NSS score and are sufficient to significantly outperform predictions of the next gaze target on NSS and AUC scores for five state of the art saliency models on three image data sets.",https://openaccess.thecvf.com/content/WACV2023/html/Kadner_Improving_Saliency_Models_Predictions_of_the_Next_Fixation_With_Humans_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kadner_Improving_Saliency_Models_Predictions_of_the_Next_Fixation_With_Humans_WACV_2023_paper.pdf,,,2207.0425,main,Poster,https://ieeexplore.ieee.org/document/10030906/,"['Computer vision', 'Costs', 'Heuristic algorithms', 'Computational modeling', 'Decision making', 'Predictive models', 'Benchmark testing']","['Gaze Shifts', 'Saliency Models', 'Benchmark', 'Eye Movements', 'Image Dataset', 'Area Under Curve', 'Human Costs', 'Saliency Map', 'Algorithmic Framework', 'Sequential Decision', 'Arbitrary Model', 'Human Gaze', 'Covariance Matrix', 'Image Features', 'Visual Task', 'Entire Sequence', 'Saccade', 'Semantic Features', 'Human Observers', 'Visual Model', 'Free-viewing', 'Fixation Location', 'Internal Costs', 'Outdoor Scenes', 'Reward Function', 'Markov Decision Process', 'Large Image Datasets', 'Gaze Location', 'Ongoing Task', 'Psychophysical Experiments']",['Applications: Psychology and cognitive science'],4,"The human prioritization of image regions can be modeled in a time invariant fashion with saliency maps or sequentially with scanpath models. However, while both types of models have steadily improved on several benchmarks and datasets, there is still a considerable gap in predicting human gaze. Here, we leverage two recent developments to reduce this gap: theoretical analyses establishing a principled framework for predicting the next gaze target and the empirical measurement of the human cost for gaze switches independently of image content. We introduce an algorithm in the framework of sequential decision making, which converts any static saliency map into a sequence of dynamic history-dependent value maps, which are recomputed after each gaze shift. These maps are based on 1) a saliency map provided by an arbitrary saliency model, 2) the recently measured human cost function quantifying preferences in magnitude and direction of eye movements, and 3) a sequential exploration bonus, which changes with each subsequent gaze shift. The parameters of the spatial extent and temporal decay of this exploration bonus are estimated from human gaze data. The relative contributions of these three components were optimized on the MIT1003 dataset for the NSS score and are sufficient to significantly outperform predictions of the next gaze target on NSS and AUC scores for five state of the art saliency models on three image data sets."
Improving the Pair Selection and the Model Fusion Steps of Satellite Multi-View Stereo Pipelines,"Alvaro Gómez, Gregory Randall, Gabriele Facciolo, Rafael Grompone von Gioi","Facultad de Ingeniería, Universidad de la República, Uruguay; Centre Borelli, ENS Paris-Saclay, France",100,"France, Uruguay",0,,"Multi-view stereo reconstruction of scenes from satellite images is traditionally performed with a pair-wise stereo-vision approach: (1) multiple views are grouped into pairs, (2) each pair is processed by two-view stereo methods producing an elevation model or point cloud, lastly (3) the pair-wise reconstructions are integrated and filtered to obtain a final result. These steps are organized in a pipeline and the end-to-end performance of reconstructions depends on the behavior of these steps. This work introduces two changes that increase the performance of the reconstructions: a new pair selection approach and a new integration method are presented. The new pair selection replaces commonly used heuristics with a principled criterion that predicts the completeness of a pair based on offline simulations. The presented integration method is based on an iterated bilateral filter. Experiments show that these changes yield a systematic improvement on the performance of the pipeline.",https://openaccess.thecvf.com/content/WACV2023/html/Gomez_Improving_the_Pair_Selection_and_the_Model_Fusion_Steps_of_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gomez_Improving_the_Pair_Selection_and_the_Model_Fusion_Steps_of_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030230/,"['Point cloud compression', 'Computer vision', 'Satellites', 'Systematics', 'Computational modeling', 'Pipelines', 'Predictive models']","['Selection Of Pairs', 'Multi-view Stereo', 'Heuristic', 'Satellite Images', '3D Reconstruction', 'Point Cloud', 'Integration Method', 'Stereopsis', 'Reconstruction Performance', 'Bilateral Filter', 'Scene Reconstruction', 'Performance Of Pipelines', 'Altitude', 'Less Than Or Equal', 'Digital Elevation Model', 'Types Of Errors', 'Image Pairs', 'Relative Orientation', 'Simulation Tool', 'Reconstruction Quality', 'Stereo Matching', 'Stereo Pairs', 'Camera Model', '3D Scene', 'Experiments In This Work', 'Zenith Angle', 'Height Range', 'Image Metadata', 'Integration Step', 'Spatial Regularization']",['Applications: Remote Sensing'],2,"Multi-view stereo reconstruction of scenes from satellite images is traditionally performed with a pair-wise stereovision approach: (1) multiple views are grouped into pairs, (2) each pair is processed by two-view stereo methods producing an elevation model or point cloud, lastly (3) the pairwise reconstructions are integrated and filtered to obtain a final result. These steps are organized in a pipeline and the end-to-end performance of reconstructions depends on the behavior of these steps. This work introduces two changes that increase the performance of the reconstructions: a new pair selection approach and a new integration method are presented. The new pair selection replaces commonly used heuristics with a principled criterion that predicts the completeness of a pair based on offline simulations. The presented integration method is based on an iterated bilateral filter. Experiments show that these changes yield a systematic improvement on the performance of the pipeline."
Improving the Robustness of Point Convolution on K-Nearest Neighbor Neighborhoods With a Viewpoint-Invariant Coordinate Transform,"Xingyi Li, Wenxuan Wu, Xiaoli Z. Fern, Li Fuxin","School of Electrical Engineering and Computer Science, Oregon State University",100,USA,0,,"Recently, there is significant interest in performing convolution over irregularly sampled point clouds. Point clouds are very different from raster images, in that one cannot have a regular sampling grid on point clouds, which makes robustness under irregular neighborhoods an important issue. Especially, the k-nearest neighbor (kNN) neighborhood presents challenges for generalization because the location of the neighbors can be very different between training and testing times. In order to improve the robustness to different neighborhood samplings, this paper proposes a novel viewpoint-invariant coordinate transform as the input to the weight-generating function for point convolution, in addition to the regular 3D coordinates. This allows us to feed the network with non-invariant, scale-invariant and scale+rotation-invariant coordinates simultaneously, so that the network can learn which to include in the convolution function automatically. Empirically, we demonstrate that this effectively improves the performance of point cloud convolutions on the SemanticKITTI and ScanNet datasets, as well as the robustness to significant test-time downsampling, which can substantially change the distance of neighbors in a kNN neighborhood. Experimentally, among pure point-based approaches, we achieve comparable semantic segmentation performance with a comparable point-based convolution framework KPConv on SemanticKITTI and ScanNet, yet is significantly more efficient by virtue of using a kNN neighborhood instead of an -ball.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Improving_the_Robustness_of_Point_Convolution_on_K-Nearest_Neighbor_Neighborhoods_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Improving_the_Robustness_of_Point_Convolution_on_K-Nearest_Neighbor_Neighborhoods_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030159/,"['Point cloud compression', 'Training', 'Three-dimensional displays', 'Simultaneous localization and mapping', 'Convolution', 'Semantic segmentation', 'Transforms']","['K-nearest Neighbor', 'Coordinate Transformation', 'Training Time', 'Point Cloud', 'Semantic Segmentation', '3D Coordinates', 'Neighbor Distance', 'Convolution Function', 'Convolutional Network', 'Convolutional Neural Network', 'Validation Set', 'Central Point', '2D Images', '3D Space', 'Multilayer Perceptron', 'Deep Convolutional Neural Network', 'Convolution Operation', '3D Point', 'Rotation Invariance', 'Surface Normals', '3D Point Cloud', 'Simultaneous Localization And Mapping', 'Convolutional Weights', 'Volumetric Approach', 'Neighboring Points', 'Orthonormal Basis', 'Convolutional Approach', 'Neighborhood Size']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"Recently, there is significant interest in performing convolution over irregularly sampled point clouds. Point clouds are very different from raster images, in that one cannot have a regular sampling grid on point clouds, which makes robustness under irregular neighborhoods an important issue. Especially, the k-nearest neighbor (kNN) neighborhood presents challenges for generalization because the location of the neighbors can be very different between training and testing times. In order to improve the robustness to different neighborhood samplings, this paper proposes a novel viewpoint-invariant coordinate transform as the input to the weight-generating function for point convolution, in addition to the regular 3D coordinates. This allows us to feed the network with non-invariant, scale-invariant and scale+rotation-invariant coordinates simultaneously, so that the network can learn which to include in the convolution function automatically. Empirically, we demonstrate that this effectively improves the performance of point cloud convolutions on the SemanticKITTI and ScanNet datasets, as well as the robustness to significant test-time downsampling, which can substantially change the distance of neighbors in a kNN neighborhood. Experimentally, among pure point-based approaches, we achieve comparable semantic segmentation performance with a comparable point-based convolution framework KPConv on SemanticKITTI and ScanNet, yet is significantly more efficient by virtue of using a kNN neighborhood instead of an ϵ-ball."
InDiReCT: Language-Guided Zero-Shot Deep Metric Learning for Images,"Konstantin Kobs, Michael Steininger, Andreas Hotho",University of Würzburg,100,Germany,0,,"Common Deep Metric Learning (DML) datasets specify only one notion of similarity, e.g., two images in the Cars196 dataset are deemed similar if they show the same car model. We argue that depending on the application, users of image retrieval systems have different and changing similarity notions that should be incorporated as easily as possible. Therefore, we present Language-Guided Zero-Shot Deep Metric Learning (LanZ-DML) as a new DML setting in which users control the aspects that should be important for image representations without training data by only using natural language. To this end, we propose InDiReCT (Image representations using Dimensionality Reduction on CLIP embedded Texts), a model for LanZ-DML on images that exclusively uses a few text prompts for training. InDiReCT utilizes CLIP as a fixed feature extractor for images and texts and transfers the variation in text prompt embeddings to the image embedding space. Extensive experiments on five datasets and overall thirteen similarity notions show that, despite not seeing any images during training, InDiReCT performs better than strong baselines and approaches the performance of fully-supervised models. An analysis reveals that InDiReCT learns to focus on regions of the image that correlate with the desired similarity notion, which makes it a fast to train and easy to use method to create custom embedding spaces only using natural language.",https://openaccess.thecvf.com/content/WACV2023/html/Kobs_InDiReCT_Language-Guided_Zero-Shot_Deep_Metric_Learning_for_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kobs_InDiReCT_Language-Guided_Zero-Shot_Deep_Metric_Learning_for_Images_WACV_2023_paper.pdf,,,2211.1276,main,Poster,https://ieeexplore.ieee.org/document/10030368/,"['Measurement', 'Training', 'Dimensionality reduction', 'Natural languages', 'Image retrieval', 'Training data', 'Image representation']","['Metric Learning', 'Deep Metric Learning', 'Training Data', 'Dimensionality Reduction', 'Natural Language', 'Image Regions', 'Latent Space', 'Image Representation', 'Image Retrieval', 'Similar Note', 'Strong Baseline', 'Car Model', 'Image Embedding', 'Loss Function', 'Percentage Points', 'Dimensional Vector', 'Large Model', 'Training Images', 'Bird Species', 'Face Recognition', 'Saliency Map', 'Embedding Dimension', 'Similar Direction', 'Hypersphere', 'Types Of Cars', 'Embedding Vectors', 'Text Encoder', 'Early Stopping', 'Training Labels', 'Transformation Matrix']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Common Deep Metric Learning (DML) datasets specify only one notion of similarity, e.g., two images in the Cars196 dataset are deemed similar if they show the same car model. We argue that depending on the application, users of image retrieval systems have different and changing similarity notions that should be incorporated as easily as possible. Therefore, we present Language-Guided Zero-Shot Deep Metric Learning (LanZ-DML) as a new DML setting in which users control the properties that should be important for image representations without training data by only using natural language. To this end, we propose InDiReCT (Image representations using Dimensionality Reduction on CLIP embedded Texts), a model for LanZ-DML on images that exclusively uses a few text prompts for training. InDiReCT utilizes CLIP as a fixed feature extractor for images and texts and transfers the variation in text prompt embeddings to the image embedding space. Extensive experiments on five datasets and overall thirteen similarity notions show that, despite not seeing any images during training, InDiReCT performs better than strong baselines and approaches the performance of fully-supervised models. An analysis reveals that InDiReCT learns to focus on regions of the image that correlate with the desired similarity notion, which makes it a fast to train and easy to use method to create custom embedding spaces only using natural language."
Indirect Adversarial Losses via an Intermediate Distribution for Training GANs,"Rui Yang, Duc Minh Vo, Hideki Nakayama","The University of Tokyo, Japan",100,Japan,0,,"In this study, we consider the weak convergence characteristics of the Integral Probability Metrics (IPM) methods in training Generative Adversarial Networks (GANs). We first concentrate on a successful IPM-based GAN method that employs a repulsive version of the Maximum Mean Discrepancy (MMD) as the discriminator loss (called repulsive MMD-GAN). We reinterpret its repulsive metrics as an indirect discriminator loss function toward an intermediate distribution. This allows us to propose a novel generator loss via such an intermediate distribution based on our reinterpretation. Our indirect adversarial losses use a simple known distribution (i.e., the Normal or Uniform distribution in our experiments) to simulate indirect adversarial learning between three parts -- real, fake, and intermediate distributions. Furthermore, we found the Kernelized Stein Discrepancy (KSD) from the IPM family as the adversarial loss function to avoid randomness from intermediate distribution samples because the target side (intermediate one) is sample-free in KSD. Experiments on several real-world datasets show that our methods can successfully train GANs with the intermediate-distribution-based KSD and MMD and can outperform previous loss metrics.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Indirect_Adversarial_Losses_via_an_Intermediate_Distribution_for_Training_GANs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Indirect_Adversarial_Losses_via_an_Intermediate_Distribution_for_Training_GANs_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030540/,"['Measurement', 'Training', 'Computer vision', 'Generative adversarial networks', 'Generators', 'Adversarial machine learning', 'Convergence']","['Generative Adversarial Networks', 'Indirect Losses', 'Generative Adversarial Networks Training', 'Intermediate Distribution', 'Loss Function', 'Uniform Distribution', 'Loss Of Generality', 'Real-world Datasets', 'Real Distribution', 'Simple Distribution', 'Discriminator Loss', 'Maximum Mean Discrepancy', 'Target Side', 'Normal Distribution', 'Real Samples', 'Dynamic Balance', 'Past Work', 'Baseline Methods', 'Human Faces', 'Radial Basis Function Kernel', 'Real Output', 'CIFAR-100 Dataset', 'Target Distribution', 'Output Of The Discriminator', 'Source Distribution', 'Machine Learning Tasks', 'Hinge Loss', 'Layer Of The Discriminator', 'Choice Of Distribution', 'Version Of Function']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"In this study, we consider the weak convergence characteristics of the Integral Probability Metrics (IPM) methods in training Generative Adversarial Networks (GANs). We first concentrate on a successful IPM-based GAN method that employs a repulsive version of the Maximum Mean Discrepancy (MMD) as the discriminator loss (called repulsive MMD-GAN). We reinterpret its repulsive metrics as an indirect discriminator loss function toward an intermediate distribution. This allows us to propose a novel generator loss via such an intermediate distribution based on our reinterpretation. Our indirect adversarial losses use a simple known distribution (i.e., the Normal or Uniform distribution in our experiments) to simulate indirect adversarial learning between three parts – real, fake, and intermediate distributions. Furthermore, we found the Kernelized Stein Discrepancy (KSD) from the IPM family as the adversarial loss function to avoid randomness from intermediate distribution samples because the target side (intermediate one) is sample-free in KSD. Experiments on several real-world datasets show that our methods can successfully train GANs with the intermediate-distribution-based KSD and MMD and can outperform previous loss metrics."
Inducing Data Amplification Using Auxiliary Datasets in Adversarial Training,"Saehyung Lee, Hyungyu Lee",,,,,,"Several recent studies have shown that the use of extra in-distribution data can lead to a high level of adversarial robustness. However, there is no guarantee that it will always be possible to obtain sufficient extra data for a selected dataset. In this paper, we propose a biased multi-domain adversarial training (BiaMAT) method that induces training data amplification on a primary dataset using publicly available auxiliary datasets, without requiring the class distribution match between the primary and auxiliary datasets. The proposed method can achieve increased adversarial robustness on a primary dataset by leveraging auxiliary datasets via multi-domain learning. Specifically, data amplification on both robust and non-robust features can be accomplished through the application of BiaMAT as demonstrated through a theoretical and empirical analysis. Moreover, we demonstrate that while existing methods are vulnerable to negative transfer due to the distributional discrepancy between auxiliary and primary data, the proposed method enables neural networks to flexibly leverage diverse image datasets for adversarial training by successfully handling the domain discrepancy through the application of a confidence-based selection strategy. The code and pre-trained models of our study are available at: https://github.com/BiaMAT/BiaMAT_under_review.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Inducing_Data_Amplification_Using_Auxiliary_Datasets_in_Adversarial_Training_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Inducing_Data_Amplification_Using_Auxiliary_Datasets_in_Adversarial_Training_WACV_2023_paper.pdf,,,2209.14053,main,Poster,https://ieeexplore.ieee.org/document/10030504/,"['Training', 'Computer vision', 'Codes', 'Computational modeling', 'Neural networks', 'Training data', 'Robustness']","['Adversarial Training', 'Auxiliary Dataset', 'Theoretical Analysis', 'Selection Strategy', 'Training Methods', 'Application Of Strategies', 'Robust Features', 'Auxiliary Data', 'Primary Dataset', 'Extra Data', 'Negative Transfer', 'Bias Training', 'Images In The Training Dataset', 'Domain Discrepancy', 'Adversarial Robustness', 'Deep Neural Network', 'Gradient Descent', 'Effects Of Training', 'Transfer Learning', 'Confidence Score', 'Primary Task', 'Auxiliary Task', 'Adversarial Examples', 'Unlabeled Data', 'Training Signal', 'Inductive Bias', 'Fast Gradient Sign Method', 'Semi-supervised Learning', 'Random Labeling', 'Pre-training Method']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods']",1,"Several recent studies have shown that the use of extra in-distribution data can lead to a high level of adversarial robustness. However, there is no guarantee that it will always be possible to obtain sufficient extra data for a selected dataset. In this paper, we propose a biased multi-domain adversarial training (BiaMAT) method that induces training data amplification on a primary dataset using publicly available auxiliary datasets, without requiring the class distribution match between the primary and auxiliary datasets. The proposed method can achieve increased adversarial robustness on a primary dataset by leveraging auxiliary datasets via multi-domain learning. Specifically, data amplification on both robust and non-robust features can be accomplished through the application of BiaMAT as demonstrated through a theoretical and empirical analysis. Moreover, we demonstrate that while existing methods are vulnerable to negative transfer due to the distributional discrepancy between auxiliary and primary data, the proposed method enables neural networks to flexibly leverage diverse image datasets for adversarial training by successfully handling the domain discrepancy through the application of a confidence-based selection strategy. The pre-trained models and code are available at: https://github.com/Saehyung-Lee/BiaMAT."
Instance-Dependent Noisy Label Learning via Graphical Modelling,"Arpit Garg, Cuong Nguyen, Rafael Felix, Thanh-Toan Do, Gustavo Carneiro","Australian Institute for Machine Learning, University of Adelaide, Australia and Centre for Vision, Speech and Signal Processing, University of Surrey, United Kingdom; Department of Data Science and AI, Monash University, Australia; Australian Institute for Machine Learning, University of Adelaide, Australia",100,"Australia, UK",0,,"Noisy labels are unavoidable yet troublesome in the ecosystem of deep learning because models can easily overfit them. There are many types of label noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with IDN being the only type that depends on image information. Such dependence on image information makes IDN a critical type of label noise to study, given that labelling mistakes are caused in large part by insufficient or ambiguous information about the visual classes present in images. Aiming to provide an effective technique to address IDN, we present a new graphical modelling approach called InstanceGM, that combines discriminative and generative models. The main contributions of InstanceGM are: i) the use of the continuous Bernoulli distribution to train the generative model, offering significant training advantages, and ii) the exploration of a state-of-the-art noisy-label discriminative classifier to generate clean labels from instance-dependent noisy-label samples. InstanceGM is competitive with current noisy-label learning approaches, particularly in instance-dependent noise benchmarks using synthetic and real-world datasets, where our method shows better accuracy than the competitors in most experiments.",https://openaccess.thecvf.com/content/WACV2023/html/Garg_Instance-Dependent_Noisy_Label_Learning_via_Graphical_Modelling_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Garg_Instance-Dependent_Noisy_Label_Learning_via_Graphical_Modelling_WACV_2023_paper.pdf,,https://github.com/arpit2412/InstanceGM,2209.00906,main,Poster,https://ieeexplore.ieee.org/document/10030953/,"['Training', 'Deep learning', 'Visualization', 'Computer vision', 'Biological system modeling', 'Ecosystems', 'Benchmark testing']","['Graphical Model', 'Noisy Labels', 'Benchmark', 'Continuous Distribution', 'Real-world Datasets', 'Model Discrimination', 'Types Of Noise', 'Ambiguous Information', 'Label Noise', 'Training Set', 'Training Data', 'Random Variables', 'Deep Neural Network', 'Image Features', 'Latent Variables', 'Feature Maps', 'Clean Data', 'Stochastic Gradient Descent', 'Error Propagation', 'Truth Labels', 'Clean Samples', 'Reconstruction Loss', 'Noise Rate', 'Variational Autoencoder', 'Semi-supervised Learning', 'Gaussian Mixture Model', 'Improve Classification Accuracy', 'Distribution Of Categories', 'Part Of Table', 'Validation Set']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",13,"Noisy labels are unavoidable yet troublesome in the ecosystem of deep learning because models can easily overfit them. There are many types of label noise, such as symmetric, asymmetric and instance-dependent noise (IDN), with IDN being the only type that depends on image information. Such dependence on image information makes IDN a critical type of label noise to study, given that labelling mistakes are caused in large part by insufficient or ambiguous information about the visual classes present in images. Aiming to provide an effective technique to address IDN, we present a new graphical modelling approach called InstanceGM, that combines discriminative and generative models. The main contributions of InstanceGM are: i) the use of the continuous Bernoulli distribution to train the generative model, offering significant training advantages, and ii) the exploration of a state-of-the-art noisy-label discriminative classifier to generate clean labels from instance-dependent noisy-label samples. InstanceGM is competitive with current noisy-label learning approaches, particularly in IDN benchmarks using synthetic and real-world datasets, where our method shows better accuracy than the competitors in most experiments
<sup>1</sup>
."
Intention-Conditioned Long-Term Human Egocentric Action Anticipation,"Esteve Valls Mascaró, Hyemin Ahn, Dongheui Lee","Autonomous Systems, Technische Universität Wien (TU Wien), Vienna, Austria; Artificial Intelligence Graduate School (AIGS), Ulsan National Institute of Science and Technology (UNIST), Ulsan, Korea; Institute of Robotics and Mechatronics, German Aerospace Center, Wessling, Germany",100,"Austria, Germany, South Korea",0,,"To anticipate how a person would act in the future, it is essential to understand the human intention since it guides the subject towards a certain action. In this paper, we propose a hierarchical architecture which assumes a sequence of human action (low-level) can be driven from the human intention (high-level). Based on this, we deal with long-term action anticipation task in egocentric videos. Our framework first extracts this low- and high-level human information over the observed human actions in a video through a Hierarchical Multi-task Multi-Layer Perceptrons Mixer (H3M). Then, we constrain the uncertainty of the future through an Intention-Conditioned Variational Auto-Encoder (I-CVAE) that generates multiple stable predictions of the next actions that the observed human might perform. By leveraging human intention as high-level information, we claim that our model is able to anticipate more time-consistent actions in the long-term, thus improving the results over the baseline in Ego4D dataset. This work results in the state-of-the-art for Long-Term Anticipation (LTA) task in Ego4D by providing more plausible anticipated sequences, improving the anticipation scores of nouns and actions. Our work ranked first in both CVPR@2022 and ECCV@2022 Ego4D LTA Challenge.",https://openaccess.thecvf.com/content/WACV2023/html/Mascaro_Intention-Conditioned_Long-Term_Human_Egocentric_Action_Anticipation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mascaro_Intention-Conditioned_Long-Term_Human_Egocentric_Action_Anticipation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030492/,"['Computer vision', 'Uncertainty', 'Multitasking', 'Behavioral sciences', 'Data mining', 'Task analysis', 'Forecasting']","['Human Activities', 'Activity Prediction', 'Long-term Activity', 'Sequence Of Actions', 'Variational Autoencoder', 'Human Intention', 'Inner Layer', 'Recurrent Neural Network', 'Future Actions', 'Generative Adversarial Networks', 'Video Clips', 'Latent Space', 'Temporal Dependencies', 'Action Representation', 'Latent Representation', 'Human Motion', 'Past Observations', 'Action Labels', 'Future Uncertainty', 'Human Activity Recognition', 'Latent Distribution', 'Deep Generative Models', 'Anticipatory Action', 'Transformer Encoder', 'Conditional Variational Autoencoder', 'Computer Vision Research', 'Computer Vision', 'Validation Set', 'Hierarchical Structure', 'Sequence Labeling']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Robotics']",15,"To anticipate how a person would act in the future, it is essential to understand the human intention since it guides the subject towards a certain action. In this paper, we propose a hierarchical architecture which assumes a sequence of human action (low-level) can be driven from the human intention (high-level). Based on this, we deal with long-term action anticipation task in egocentric videos. Our framework first extracts this low- and high-level human information over the observed human actions in a video through a Hierarchical Multi-task Multi-Layer Perceptrons Mixer (H3M). Then, we constrain the uncertainty of the future through an Intention-Conditioned Variational Auto-Encoder (I-CVAE) that generates multiple stable predictions of the next actions that the observed human might perform. By leveraging human intention as high-level information, we claim that our model is able to anticipate more time-consistent actions in the long-term, thus improving the results over the baseline in Ego4D dataset. This work results in the state-of-the-art for Long-Term Anticipation (LTA) task in Ego4D by providing more plausible anticipated sequences, improving the anticipation scores of nouns and actions. Our work ranked first in both CVPR@2022 and ECCV@2022 Ego4D LTA Challenge."
Interacting Hand-Object Pose Estimation via Dense Mutual Attention,"Rong Wang, Wei Mao, Hongdong Li",The Australian National University,100,Australia,0,,"3D hand-object pose estimation is the key to the success of many computer vision applications. The main focus of this task is to effectively model the interaction between the hand and an object. To this end, existing works either rely on interaction constraints in a computationally-expensive iterative optimization, or consider only a sparse correlation between sampled hand and object keypoints. In contrast, we propose a novel dense mutual attention mechanism that is able to model fine-grained dependencies between the hand and the object. Specifically, we first construct the hand and object graphs according to their mesh structures. For each hand node, we aggregate features from every object node by the learned attention and vice versa for each object node. Thanks to such dense mutual attention, our method is able to produce physically plausible poses with high quality and real-time inference speed. Extensive quantitative and qualitative experiments on large benchmark datasets show that our method outperforms state-of-the-art methods. The code is available at https://github.com/rongakowang/DenseMutualAttention.git.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.pdf,,https://github.com/rongakowang/DenseMutualAttention.git,2211.08805,main,Poster,https://ieeexplore.ieee.org/document/10030922/,"['Computer vision', 'Correlation', 'Codes', 'Computational modeling', 'Pose estimation', 'Benchmark testing', 'Real-time systems']","['Pose Estimation', 'Mutual Attention', 'Hand-object Pose Estimation', 'Attention Mechanism', 'Iterative Optimization', 'Feature Aggregation', 'Mesh Structure', 'Iterative Process', 'Image Features', 'Local Features', 'Global Features', 'Bounding Box', 'Graph Convolutional Network', 'Node Features', 'Image Feature Extraction', 'Attention Map', 'Pixel Coordinates', 'L1 Loss', 'Graph Convolution', 'Joint Estimation', 'Object Pose', 'Human Pose Estimation', 'Hand Joints', 'View Synthesis', 'Sparse Graph', 'Softmax Operation', 'Optimization-based Methods', 'Iterative Refinement', 'Feature Maps', 'Loss Of Contact']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Virtual/augmented reality']",6,"2D hand-object pose estimation is the key to the success of many computer vision applications. The main focus of this task is to effectively model the interaction between the hand and an object. To this end, existing works either rely on interaction constraints in a computationally-expensive iterative optimization, or consider only a sparse correlation between sampled hand and object keypoints. In contrast, we propose a novel dense mutual attention mechanism that is able to model fine-grained dependencies between the hand and the object. Specifically, we first construct the hand and object graphs according to their mesh structures. For each hand node, we aggregate features from every object node by the learned attention and vice versa for each object node. Thanks to such dense mutual attention, our method is able to produce physically plausible poses with high quality and real-time inference speed. Extensive quantitative and qualitative experiments on large benchmark datasets show that our method outperforms state-of-the-art methods. The code is available at https://github.com/rongakowang/DenseMutualAttention.git."
Interactive Image Manipulation With Complex Text Instructions,"Ryugo Morita, Zhiqiang Zhang, Man M. Ho, Jinjia Zhou","Hosei University, Tokyo, Japan",100,Japan,0,,"Recently, text-guided image manipulation has received increasing attention in the research field of multimedia processing and computer vision due to its high flexibility and controllability. Its goal is to semantically manipulate parts of an input reference image according to the text descriptions. However, most of the existing works have the following problems: (1) text-irrelevant content cannot always be maintained but randomly changed, (2) the performance of image manipulation still needs to be further improved, (3) only can manipulate descriptive attributes. To solve these problems, we propose a novel image manipulation method that interactively edits an image using complex text instructions. It allows users to not only improve the accuracy of image manipulation but also achieve complex tasks such as enlarging, dwindling, or removing objects and replacing the background with the input image. To make these tasks possible, we apply three strategies. First, the given image is divided into text-relevant content and text-irrelevant content. Only the text-relevant content is manipulated and the text-irrelevant content can be maintained. Second, a super-resolution method is used to enlarge the manipulation region to further improve the operability and to help manipulate the object itself. Third, a user interface is introduced for editing the segmentation map interactively to re-modify the generated image according to the user's desires. Extensive experiments on the Caltech-UCSD Birds-200-2011 (CUB) dataset and Microsoft Common Objects in Context (MS COCO) datasets demonstrate our proposed method can enable interactive, flexible, and accurate image manipulation in real-time. Through qualitative and quantitative evaluations, we show that the proposed model outperforms other state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Morita_Interactive_Image_Manipulation_With_Complex_Text_Instructions_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Morita_Interactive_Image_Manipulation_With_Complex_Text_Instructions_WACV_2023_paper.pdf,,,2211.15352,main,Poster,https://ieeexplore.ieee.org/document/10030880/,"['Computer vision', 'Semantic segmentation', 'Computational modeling', 'Superresolution', 'Process control', 'User interfaces', 'Controllability']","['Use Of Imaging', 'Instructional Text', 'Input Image', 'Textual Descriptions', 'Segmentation Map', 'Accurate Use', 'Objects In Context', 'Image Features', 'Image Information', 'Generative Adversarial Networks', 'Color Difference', 'Object Size', 'Large Objects', 'Use Of Phase', 'COCO Dataset', 'Style Transfer', 'Preprocessing Phase', 'Modification Of Content', 'Image Inpainting', 'Super-resolution Network']","['Applications: Arts/games/social media', 'Adversarial learning', 'adversarial attack and defense methods', 'Computational photography', 'image and video synthesis']",2,"Recently, text-guided image manipulation has received increasing attention in the research field of multimedia processing and computer vision due to its high flexibility and controllability. Its goal is to semantically manipulate parts of an input reference image according to the text descriptions. However, most of the existing works have the following problems: (1) text-irrelevant content cannot always be maintained but randomly changed, (2) the performance of image manipulation still needs to be further improved, (3) only can manipulate descriptive attributes. To solve these problems, we propose a novel image manipulation method that interactively edits an image using complex text instructions. It allows users to not only improve the accuracy of image manipulation but also achieve complex tasks such as enlarging, dwindling, or removing objects and replacing the background with the input image. To make these tasks possible, we apply three strategies. First, the given image is divided into text-relevant content and text-irrelevant content. Only the text-relevant content is manipulated and the text-irrelevant content can be maintained. Second, a super-resolution method is used to enlarge the manipulation region to further improve the operability and to help manipulate the object itself. Third, a user interface is introduced for editing the segmentation map interactively to re-modify the generated image according to the user’s desires. Extensive experiments on the Caltech-UCSD Birds-200-2011 (CUB) dataset and Microsoft Common Objects in Context (MS COCO) datasets demonstrate our proposed method can enable interactive, flexible, and accurate image manipulation in real-time. Through qualitative and quantitative evaluations, we show that the proposed model outperforms other state-of-the-art methods."
Interpolated SelectionConv for Spherical Images and Surfaces,"David Hart, Michael Whitney, Bryan Morse",Brigham Young University,100,USA,0,,"We present a new and general framework for convolutional neural network operations on spherical (or omnidirectional) images. Our approach represents the surface as a graph of connected points that doesn't rely on a particular sampling strategy. Additionally, by using an interpolated version of SelectionConv, we can operate on the sphere while using existing 2D CNNs and their weights. Since our method leverages existing graph implementations, it is also fast and can be fine-tuned efficiently. Our method is also general enough to be applied to any surface type, even those that are topologically non-simple. We demonstrate the effectiveness of our technique on the tasks of style transfer and segmentation for spheres as well as stylization for 3D meshes. We provide a thorough ablation study of the performance of various spherical sampling strategies.",https://openaccess.thecvf.com/content/WACV2023/html/Hart_Interpolated_SelectionConv_for_Spherical_Images_and_Surfaces_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hart_Interpolated_SelectionConv_for_Spherical_Images_and_Surfaces_WACV_2023_paper.pdf,,,2210.10123,main,Poster,https://ieeexplore.ieee.org/document/10030138/,"['Image segmentation', 'Computer vision', 'Three-dimensional displays', 'Convolutional neural networks', 'Task analysis']","['Spherical Surface', 'Spherical Image', 'Convolutional Neural Network', 'Convolution Operation', 'Segmentation Task', '3D Mesh', 'Style Transfer', 'Simple Way', 'Point Cloud', '3D Space', 'Edge Weights', 'Semantic Segmentation', 'Layering', 'Graph Structure', 'Network Graph', 'Clustering Techniques', 'Single Pass', 'Source Node', 'Multiple Edges', 'Target Node', 'Texture Map', 'Interpolation Technique', 'Graph Convolution', 'Spherical Domain', 'Interpolation Of Values', 'Traditional Convolutional Neural Network', '2D Convolutional Network', 'Graphical Representation']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"We present a new and general framework for convolutional neural network operations on spherical (or omnidirectional) images. Our approach represents the surface as a graph of connected points that doesn’t rely on a particular sampling strategy. Additionally, by using an interpolated version of SelectionConv, we can operate on the sphere while using existing 2D CNNs and their weights. Since our method leverages existing graph implementations, it is also fast and can be fine-tuned efficiently. Our method is also general enough to be applied to any surface type, even those that are topologically non-simple. We demonstrate the effectiveness of our technique on the tasks of style transfer and segmentation for spheres as well as stylization for 3D meshes. We provide a thorough ablation study of the performance of various spherical sampling strategies."
Interpreting Disparate Privacy-Utility Tradeoff in Adversarial Learning via Attribute Correlation,"Likun Zhang, Yahong Chen, Ang Li, Binghui Wang, Yiran Chen, Fenghua Li, Jin Cao, Ben Niu","Institute of Information Engineering, CAS, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; School of Cyber Engineering, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, Duke University, Durham, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, USA",100,"China, USA",0,,"Adversarial learning is commonly used to extract latent data representations which are expressive to predict the target attribute but indistinguishable in the privacy attribute. However, whether they can achieve an expected privacy-utility tradeoff is of great uncertainty. In this paper, we posit it is the complex interaction between different attributes in the training set that causes disparate tradeoff results. We first formulate the measurement of utility, privacy and their tradeoff in adversarial learning. Then we propose the metrics of Statistical Reliability (SR) and Feature Reliability (FR) to quantify the relationship between attributes. Specifically, SR reflects the co-occurrence sampling bias of the joint distribution between two attributes. Beyond the explicit dependence, FR exploits the intrinsic interaction one attribute exerts on the other via exploring the representation disentanglement. We validate the metrics in an adversarial learning scheme on CelebA and LFW dataset with a suite of target-privacy attribute pairs. Experiments demonstrate the strong correlations between the metrics and utility, privacy and their tradeoff. We further conclude how to use SR and FR as a guide to the selection of the privacy-utility tradeoff parameter.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Interpreting_Disparate_Privacy-Utility_Tradeoff_in_Adversarial_Learning_via_Attribute_Correlation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Interpreting_Disparate_Privacy-Utility_Tradeoff_in_Adversarial_Learning_via_Attribute_Correlation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030960/,"['Measurement', 'Training', 'Privacy', 'Data privacy', 'Computer vision', 'Correlation', 'Uncertainty']","['Generative Adversarial Networks', 'Privacy-utility Trade-off', 'Training Set', 'Greater Uncertainty', 'Statistical Reliability', 'Latent Representation', 'Trade-off Parameter', 'Training Dataset', 'Feature Representation', 'Skin Color', 'Mutual Information', 'Representation Learning', 'Representation Of Space', 'Trend Line', 'Hair Color', 'Cramer’s V', 'Random Guessing', 'Calculated Pearson Correlation', 'Coordinate Axis', 'Object Task', 'Intrinsic Correlation', 'Privacy Leakage', 'Privacy Preservation', 'Sensitive Attributes', 'Utility Loss', 'Loss Of Privacy', 'Impact Of Correlation']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Adversarial learning', 'adversarial attack and defense methods']",3,"Adversarial learning is commonly used to extract latent data representations which are expressive to predict the target attribute but indistinguishable in the privacy attribute. However, whether they can achieve an expected privacy-utility tradeoff is of great uncertainty. In this paper, we posit it is the complex interaction between different attributes in the training set that causes disparate tradeoff results. We first formulate the measurement of utility, privacy and their tradeoff in adversarial learning. Then we propose the metrics of Statistical Reliability (SR) and Feature Reliability (FR) to quantify the relationship between attributes. Specifically, SR reflects the co-occurrence sampling bias of the joint distribution between two attributes. Beyond the explicit dependence, FR exploits the intrinsic interaction one attribute exerts on the other via exploring the representation disentanglement. We validate the metrics on CelebA and LFW dataset with a suite of target-privacy attribute pairs. Experimental results demonstrate the strong correlations between the metrics and utility, privacy and their tradeoff. We further conclude how to use SR and FR as a guide to the setting of the privacy-utility tradeoff parameter."
Intra-Batch Supervision for Panoptic Segmentation on High-Resolution Images,"Daan de Geus, Gijs Dubbelman",Eindhoven University of Technology,100,Netherlands,0,,"Unified panoptic segmentation methods are achieving state-of-the-art results on several datasets. To achieve these results on high-resolution datasets, these methods apply crop-based training. In this work, we find that, although crop-based training is advantageous in general, it also has a harmful side-effect. Specifically, it limits the ability of unified networks to discriminate between large object instances, causing them to make predictions that are confused between multiple instances. To solve this, we propose Intra-Batch Supervision (IBS), which improves a network's ability to discriminate between instances by introducing additional supervision using multiple images from the same batch. We show that, with our IBS, we successfully address the confusion problem and consistently improve the performance of unified networks. For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem.",https://openaccess.thecvf.com/content/WACV2023/html/de_Geus_Intra-Batch_Supervision_for_Panoptic_Segmentation_on_High-Resolution_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/de_Geus_Intra-Batch_Supervision_for_Panoptic_Segmentation_on_High-Resolution_Images_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030315/,"['Training', 'Measurement', 'Image segmentation', 'Computer vision', 'Crops', 'Task analysis']","['High-resolution Images', 'Panoptic Segmentation', 'Univariate Analysis', 'Large Objects', 'Consistent Improvement', 'Unique Network', 'High-resolution Dataset', 'Harmful Side Effects', 'Object Instances', 'Unified Segmentation', 'Pixel Accuracy', 'Training Set', 'Field Of View', 'Part Of Network', 'Unique Approach', 'Matrix Multiplication', 'Semantic Segmentation', 'Small Segments', 'Large Segments', 'Standard Metrics', 'Segment Classification', 'Ground Truth Segmentation', 'Cropped Images', 'Instance Segmentation', 'Accuracy Metrics', 'Crop Samples', 'Multiple Segments', 'Stable Training', 'Training Batch', 'Inaccurate Predictions']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",4,"Unified panoptic segmentation methods are achieving state-of-the-art results on several datasets. To achieve these results on high-resolution datasets, these methods apply crop-based training. In this work, we find that, although crop-based training is advantageous in general, it also has a harmful side-effect. Specifically, it limits the ability of unified networks to discriminate between large object instances, causing them to make predictions that are confused between multiple instances. To solve this, we propose Intra-Batch Supervision (IBS), which improves a network’s ability to discriminate between instances by introducing additional supervision using multiple images from the same batch. We show that, with our IBS, we successfully address the confusion problem and consistently improve the performance of unified networks. For the high-resolution Cityscapes and Mapillary Vistas datasets, we achieve improvements of up to +2.5 on the Panoptic Quality for thing classes, and even more considerable gains of up to +5.8 on both the pixel accuracy and pixel precision, which we identify as better metrics to capture the confusion problem."
Intra-Source Style Augmentation for Improved Domain Generalization,"Yumeng Li, Dan Zhang, Margret Keuper, Anna Khoreva","Bosch Center for AI, University of Tübingen; University of Siegen, MPI for Informatics; Bosch Center for AI, University of Siegen",100,Germany,0,,"The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to 12.4% mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by 3% mIoU in Cityscapes to Dark Zurich.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Intra-Source_Style_Augmentation_for_Improved_Domain_Generalization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Intra-Source_Style_Augmentation_for_Improved_Domain_Generalization_WACV_2023_paper.pdf,,,2210.10175,main,Poster,https://ieeexplore.ieee.org/document/10030166/,"['Training', 'Semantic segmentation', 'Semantics', 'Layout', 'Training data', 'Predictive models', 'Network architecture']","['Domain Generalization', 'Style Augmentation', 'Training Set', 'Training Data', 'Transformer', 'Joint Effect', 'Domain Shift', 'Semantic Segmentation', 'Augmentation Methods', 'Adverse Weather Conditions', 'Diversity Of Training Data', 'Information Content', 'Input Image', 'Image Reconstruction', 'Additive Noise', 'Data Augmentation', 'Image Information', 'Generative Adversarial Networks', 'Latent Space', 'Target Domain', 'Source Domain', 'Latent Code', 'Noise Map', 'Unseen Domains', 'Semantic Content', 'Reconstruction Quality', 'High-quality Reconstruction', 'Image X', 'Geometric Transformation', 'Data Augmentation Methods']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Computational photography', 'image and video synthesis']",7,"The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an intra-source style augmentation (ISSA) method to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, ISSA effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to 12.4% mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by 3% mIoU in Cityscapes to Dark Zürich."
Is Bigger Always Better? An Empirical Study on Efficient Architectures for Style Transfer and Beyond,"Jie An, Tao Li, Haozhi Huang, Jinwen Ma, Jiebo Luo","Peking University, Beijing, China; University of Rochester, Rochester, NY, USA; Xverse Inc., Shenzhen, Guangdong, China",66.66666667,"China, USA",33.33333333,China,"Network architecture plays a pivotal role in the performance of style transfer algorithms. Most existing algorithms use VGG19 as the feature extractor, which incurs a high computational cost. In this work, we conduct an empirical study on the popular network architectures and find that some more efficient networks can replace VGG19 while having comparable style transfer performance. Beyond that, we show that an efficient network can be further accelerated by removing its empty channels via a simple channel pruning method tweaked for style transfer. To prevent the potential performance drop due to using a more lightweight network and obtain better style transfer results, we introduce a more accurate deep feature alignment strategy to improve existing style transfer modules. Taking GoogLeNet as an exemplary efficient network, the pruned GoogLeNet with the improved style transfer module is 2.3   107.4x faster than the state-of-the-art approaches and can achieve 68.03 FPS on 512 x 512 images. Extensive experiments demonstrate that VGG19 can be replaced by a more lightweight network with significantly improved efficiency and comparable style transfer quality.",https://openaccess.thecvf.com/content/WACV2023/html/An_Is_Bigger_Always_Better_An_Empirical_Study_on_Efficient_Architectures_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/An_Is_Bigger_Always_Better_An_Empirical_Study_on_Efficient_Architectures_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030460/,"['Computer vision', 'Computer architecture', 'Transforms', 'Network architecture', 'Feature extraction', 'Real-time systems', 'Computational efficiency']","['Deep Features', 'Network Efficiency', 'Alignment Accuracy', 'Feature Alignment', 'Style Transfer', 'Lightweight Network', 'Pruning Method', 'Empty Channel', 'Contralateral', 'Convolutional Layers', 'Upper Layer', 'Feature Maps', 'Computational Resources', 'User Study', 'Network Layer', 'Visual Comparison', 'Feed-forward Network', 'Residual Block', 'ImageNet Dataset', 'COCO Dataset', 'BN Layer', 'ReLU Layer', 'Preservation Of Content', 'Style Image', 'Filters In Each Convolutional Layer', 'Fine Texture', 'VGG Network', 'Local Texture', 'Residual Connection', 'Loss Term']","['Algorithms: Computational photography', 'image and video synthesis']",2,"Network architecture plays a pivotal role in style transfer. Most existing algorithms use VGG19 as the feature extractor, which incurs a high computational cost. In this work, we conduct an empirical study on the popular network architectures and find that some more efficient networks can replace VGG19 while having comparable style transfer performance. Beyond that, we show that an efficient network can be further accelerated by removing its empty channels via a simple channel pruning method tweaked for style transfer. To prevent the potential performance drop due to using a more lightweight network and obtain better style transfer results, we introduce a more accurate deep feature alignment strategy to improve existing style transfer modules. Taking GoogLeNet as an exemplary efficient network, the pruned GoogLeNet with the improved style transfer module is 2.3 ~ 107.4× faster than the state-of-the-art approaches and can achieve 68.03 FPS on 512×512 images. Extensive experiments demonstrate that VGG19 can be replaced by a more lightweight network with significantly improved efficiency and comparable style transfer quality."
Is Your Noise Correction Noisy? PLS: Robustness To Label Noise With Two Stage Detection,"Paul Albert, Eric Arazo, Tarun Krishna, Noel E. O’Connor, Kevin McGuinness","School of Electronic Engineering, Insight SFI Centre for Data Analytics, Dublin City University (DCU)",100,Ireland,0,,"Designing robust algorithms capable of training accurate neural networks on uncurated datasets from the web has been the subject of much research as it reduces the need for time consuming human labor. The focus of many previous research contributions has been on the detection of different types of label noise; however, this paper proposes to improve the correction accuracy of noisy samples once they have been detected. In many state-of-the-art contributions, a two phase approach is adopted where the noisy samples are detected before guessing a corrected pseudo-label in a semi-supervised fashion. The guessed pseudo-labels are then used in the supervised objective without ensuring that the label guess is likely to be correct. This can lead to confirmation bias, which reduces the noise robustness. Here we propose the pseudo-loss, a simple metric that we find to be strongly correlated with pseudo-label correctness on noisy samples. Using the pseudo-loss, we dynamically down weight under-confident pseudo-labels throughout training to avoid confirmation bias and improve the network accuracy. We additionally propose to use a confidence guided contrastive objective that learns robust representation on an interpolated objective between class bound (supervised) for confidently corrected samples and unsupervised representation for under-confident label corrections. Experiments demonstrate the state-of-the-art performance of our Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets including curated data synthetically corrupted with in-distribution and out-of-distribution noise, and two real world web noise datasets. Our experiments are fully reproducible github.com/PaulAlbert31/PLS.",https://openaccess.thecvf.com/content/WACV2023/html/Albert_Is_Your_Noise_Correction_Noisy_PLS_Robustness_To_Label_Noise_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Albert_Is_Your_Noise_Correction_Noisy_PLS_Robustness_To_Label_Noise_WACV_2023_paper.pdf,,github.com/PaulAlbert31/PLS,2210.04578,main,Poster,https://ieeexplore.ieee.org/document/10030843/,"['Training', 'Computer vision', 'Semantics', 'Neural networks', 'Benchmark testing', 'Robustness', 'Noise robustness']","['Noise Correction', 'Neural Network', 'Confirmation Bias', 'Real Noise', 'Contrast Objective', 'Noise In The Dataset', 'Label Noise', 'Subject Of Much Research', 'Detection Of Different Types', 'Feature Space', 'Real-world Data', 'Cross-entropy Loss', 'Stochastic Gradient Descent', 'Kullback-Leibler', 'Small Loss', 'Truth Labels', 'Gaussian Mixture Model', 'Training Loss', 'Classification Loss', 'Semi-supervised Learning', 'Contrastive Loss', 'Detector Noise', 'Noisy Labels', 'Self-supervised Learning', 'Jensen-Shannon Divergence', 'Rényi Entropy', 'Random Horizontal Flipping', 'Contrast Features', 'One-hot Encoding', 'Label Distribution']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",6,"Designing robust algorithms capable of training accurate neural networks on uncurated datasets from the web has been the subject of much research as it reduces the need for time consuming human labor. The focus of many previous research contributions has been on the detection of different types of label noise; however, this paper proposes to improve the correction accuracy of noisy samples once they have been detected. In many state-of-the-art contributions, a two phase approach is adopted where the noisy samples are detected before guessing a corrected pseudo-label in a semi-supervised fashion. The guessed pseudo-labels are then used in the supervised objective without ensuring that the label guess is likely to be correct. This can lead to confirmation bias, which reduces the noise robustness. Here we propose the pseudo-loss, a simple metric that we find to be strongly correlated with pseudo-label correctness on noisy samples. Using the pseudo-loss, we dynamically down weight under-confident pseudo-labels throughout training to avoid confirmation bias and improve the network accuracy. We additionally propose to use a confidence guided contrastive objective that learns robust representation on an interpolated objective between class bound (supervised) for confidently corrected samples and unsupervised representation for under-confident label corrections. Experiments demonstrate the state-of-the-art performance of our Pseudo-Loss Selection (PLS) algorithm on a variety of benchmark datasets including curated data synthetically corrupted with in-distribution and out-of-distribution noise, and two real world web noise datasets. Our experiments are fully reproducible github.com/PaulAlbert31/PLS."
Joint Video Rolling Shutter Correction and Super-Resolution,"Akash Gupta, Sudhir Kumar Singh, Amit K. Roy-Chowdhury",,,,,,"With the prevalence of CMOS cameras in many computer vision applications, there is an increase in the appearance of rolling shutter (RS) artifacts in captured videos. However, existing video super-resolution algorithms assume that the motion is globally consistent in each video frame and no rolling shutter effect is present. The problem of video super-resolution for video captured using RS cameras is challenging as the model needs to learn the row-wise local pixel displacements and the global structure of the frame for RS correction and super-resolution, simultaneously. Different from existing works, we address a more realistic problem of joint rolling shutter correction and super-resolution (RS-SR). We introduce a novel architecture, deformable Patch Attention Network (PatchNet), that utilizes patch-recurrence property along with deformable receptive fields to learn the global and local structure of the video. Specifically, PatchNet leverages bi-directional motion field in the feature space to extract relevant information from neighboring patches using attention mechanism, and deformable fields using deformable convolutions to extract local pixel-level information for joint rolling shutter correction and super-resolution. Our work is the first to tackle the task of RS correction and super-resolution on the recently released BS-RSCD dataset. Experiments on the BS-RSCD dataset and FastecRS datasets demonstrate that our model performs favorably against various state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Gupta_Joint_Video_Rolling_Shutter_Correction_and_Super-Resolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_Joint_Video_Rolling_Shutter_Correction_and_Super-Resolution_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030180/,"['Training', 'Computer vision', 'Computational modeling', 'Superresolution', 'Cameras', 'Feature extraction', 'Data mining']","['Rolling Shutter', 'Local Information', 'Feature Space', 'Attention Mechanism', 'Receptive Field', 'Video Capture', 'Computer Vision Applications', 'Motion Field', 'Complementary Metal-oxide Semiconductor Camera', 'Deformable Convolution', 'Super-resolution Task', 'Convolutional Layers', 'Global Information', 'Flow Characteristics', 'Patch Size', 'Attention Module', 'Peak Signal-to-noise Ratio', 'Optical Flow', 'Bilinear Interpolation', 'Forward Flow', 'Backward Flow', 'High-resolution Features', 'Current Frame', 'Feature Encoder', 'Super-resolution Approaches', 'Accurate Flow', 'Bidirectional Flow', 'Camera Motion', 'High-resolution Video']","['Algorithms: Computational photography', 'image and video synthesis', 'Commercial/retail']",,"With the prevalence of CMOS cameras in many computer vision applications, there is an increase in the appearance of rolling shutter (RS) artifacts in captured videos. However, existing video super-resolution algorithms assume that the motion is globally consistent in each video frame and no rolling shutter effect is present. The problem of video super-resolution for video captured using RS cameras is challenging as the model needs to learn the row-wise local pixel displacements and the global structure of the frame for RS correction and super-resolution, simultaneously. Different from existing works, we address a more realistic problem of joint rolling shutter correction and super-resolution (RS-SR). We introduce a novel architecture, deformable Patch Attention Network (PatchNet), that utilizes patch-recurrence property along with deformable receptive fields to learn the global and local structure of the video. Specifically, PatchNet leverages bi-directional motion field in the feature space to extract relevant information from neighboring patches using attention mechanism, and deformable fields using deformable convolutions to extract local pixel-level information for joint rolling shutter correction and super-resolution. Our work is the first to tackle the task of RS correction and super-resolution on the recently released BS-RSCD dataset. Experiments on the BS-RSCD and FastecRS datasets demonstrate that our model performs favorably against various state-of-the-art approaches. Project details are available at https://akashagupta.com/publication/wacv23_patchnet/project.html"
Jointly Learning Band Selection and Filter Array Design for Hyperspectral Imaging,"Ke Li, Dengxin Dai, Luc Van Gool","MPI for Informatics; CVL, ETH Zurich; PSI, KU Leuven",100,"Belgium, Germany, Switzerland",0,,"A single-shot multispectral camera equipped with an optimized color filter array (CFA) has the potential to deliver a fast and low-cost hyperspectral (HS) imaging system. Previous solutions are largely restricted to designing demosaicing algorithms for fixed CFAs - be it the Bayer color pattern or evenly-spaced spectral multiplexing patterns. Since sampling and reconstruction are tightly-coupled, the ability to search for an optimal solution is severely constrained by using predefined CFAs. In this work, we simultaneously address the problem of spectral band selection, CFA design, image demosaicing, and spectral image recovery in a joint learning framework for single-shot HS imaging. We propose a reinforcement learning (RL) based method for spectral band selection and a novel neural network for CFA generation, image demosaicing, and HS image recovery. The final spectral reconstruction accuracy is used to supervise the training of the main network to maximize the synergies between those tightly-related tasks. The RL method regards the main network as an agent to collect reward. Our final method delivers a simple setup - as simple as an RGB camera - for HS imaging. Experimental results show that our method outperforms competing methods by a large margin.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Jointly_Learning_Band_Selection_and_Filter_Array_Design_for_Hyperspectral_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Jointly_Learning_Band_Selection_and_Filter_Array_Design_for_Hyperspectral_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030979/,"['Training', 'Image color analysis', 'Neural networks', 'Prototypes', 'Reinforcement learning', 'Cameras', 'Task analysis']","['Joint Learning', 'Band Selection', 'Band Filter', 'Filter Array', 'Neural Network', 'Spectral Bands', 'Image Recovery', 'Reinforcement Learning Methods', 'RGB Camera', 'Color Filter', 'Spectral Selectivity', 'Hyperspectral Imaging System', 'Spectral Reconstruction', 'High-resolution', 'Root Mean Square Error', 'Spatial Resolution', 'Convolutional Neural Network', 'Value Function', 'High Spatial Resolution', 'Super-resolution', 'Multispectral Images', 'Large Number Of Bands', 'Implicit Function', 'RGB Images', 'Spectral Resolution', 'Spectral Color', 'Peak Signal-to-noise Ratio', 'Color Bands', 'High Frame Rate', 'Color Channels']","['Algorithms: Computational photography', 'image and video synthesis']",6,"A single-shot multispectral camera equipped with an optimized color filter array (CFA) has the potential to deliver a fast and low-cost hyperspectral (HS) imaging system. Previous solutions are largely restricted to designing demosaicing algorithms for fixed CFAs – be it the Bayer color pattern or evenly-spaced spectral multiplexing patterns. Since sampling and reconstruction are tightly-coupled, the ability to search for an optimal solution is severely constrained by using predefined CFAs. In this work, we simultaneously address the problem of spectral band selection, CFA design, image demosaicing, and spectral image recovery in a joint learning framework for single-shot HS imaging. We propose a reinforcement learning (RL) based method for spectral band selection and a novel neural network for CFA generation, image demosaicing, and HS image recovery. The final spectral reconstruction accuracy is used to supervise the training of the main network to maximize the synergies between those tightly-related tasks. The RL method regards the main network as an agent to collect reward. Our final method delivers a simple setup – as simple as an RGB camera – for HS imaging. Experimental results show that our method outperforms competing methods by a large margin."
K-VQG: Knowledge-Aware Visual Question Generation for Common-Sense Acquisition,"Kohei Uehara, Tatsuya Harada","The University of Tokyo, Tokyo, Japan; The University of Tokyo / RIKEN, Tokyo, Japan",100,Japan,0,,"Visual Question Generation (VQG) is a task to generate questions from images. When humans ask questions about an image, their goal is often to acquire some new knowledge. However, existing studies on VQG have mainly addressed question generation from answers or question categories, overlooking the objectives of knowledge acquisition. To introduce a knowledge acquisition perspective into VQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is the first large, humanly annotated dataset in which questions regarding images are tied to structured knowledge. We also developed a new VQG model that can encode and use knowledge as the target for a question. The experiment results show that our model outperforms existing models on the K-VQG dataset. Our dataset is publicly available at https://uehara-mech.github.io/kvqg.",https://openaccess.thecvf.com/content/WACV2023/html/Uehara_K-VQG_Knowledge-Aware_Visual_Question_Generation_for_Common-Sense_Acquisition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Uehara_K-VQG_Knowledge-Aware_Visual_Question_Generation_for_Common-Sense_Acquisition_WACV_2023_paper.pdf,https://uehara-mech.github.io/kvqg,,,main,Poster,https://ieeexplore.ieee.org/document/10030855/,"['Learning systems', 'Visualization', 'Computer vision', 'Knowledge acquisition', 'Benchmark testing', 'Task analysis']","['Knowledge Acquisition', 'Image Object', 'Bounding Box', 'Latent Space', 'Target Object', 'Transformer Model', 'Target Information', 'Question Wording', 'Problem Setting', 'Variational Autoencoder', 'Dataset Construction', 'Answer Categories', 'Visual Question Answering', 'Special Token']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Visual Question Generation (VQG) is a task to generate questions from images. When humans ask questions about an image, their goal is often to acquire some new knowledge. However, existing studies on VQG have mainly addressed question generation from answers or question categories, overlooking the objectives of knowledge acquisition. To introduce a knowledge acquisition perspective into VQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is the first large, humanly annotated dataset in which questions regarding images are tied to structured knowledge. We also developed a new VQG model that can encode and use knowledge as the target for a question. The experiment results show that our model outperforms existing models on the K-VQG dataset. Our dataset is publicly available at https://uehara-mech.github.io/kvqg."
Kernel-Aware Burst Blind Super-Resolution,"Wenyi Lian, Shanglian Peng","School of Computer Science, Chengdu University of Information Technology, China; Department of Information Technology, Uppsala University, Sweden",100,"China, Sweden",0,,"Burst super-resolution technique provides a possibility of restoring rich details from low-quality images. However, since real world low-resolution (LR) images in practical applications have multiple complicated and unknown degradations, existing non-blind (e.g., bicubic) designed networks usually suffer severe performance drop in recovering high-resolution (HR) images. In this paper, we address the problem of reconstructing HR images from raw burst sequences acquired from modern handheld devices. The central idea is a kernel-guided strategy which can solve the burst SR problem with two steps: kernel estimation and HR image restoration. The former estimates burst kernels from raw inputs, while the latter predicts the super-resolved image based on the estimated kernels. Furthermore, we introduce a pyramid kernel-aware deformable alignment module which can effectively align the raw images with consideration of the blurry priors. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method can perform favorable state-of-the-art performance in the burst SR problem.",https://openaccess.thecvf.com/content/WACV2023/html/Lian_Kernel-Aware_Burst_Blind_Super-Resolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lian_Kernel-Aware_Burst_Blind_Super-Resolution_WACV_2023_paper.pdf,,,2112.07315,main,Poster,https://ieeexplore.ieee.org/document/10030173/,['Portable document format'],,"['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",3,"Burst super-resolution technique provides a possibility of restoring rich details from low-quality images. However, since real world low-resolution (LR) images in practical applications have multiple complicated and unknown degradations, existing non-blind (e.g., bicubic) designed networks usually suffer severe performance drop in recovering high-resolution (HR) images. In this paper, we address the problem of reconstructing HR images from raw burst sequences acquired from modern handheld devices. The central idea is a kernel-guided strategy which can solve the burst SR problem with two steps: kernel estimation and HR image restoration. The former estimates burst kernels from raw inputs, while the latter predicts the super-resolved image based on the estimated kernels. Furthermore, we introduce a pyramid kernel-aware deformable alignment module which can effectively align the raw images with consideration of the blurry priors. Extensive experiments on synthetic and real-world datasets demonstrate that the proposed method can perform favorable state-of-the-art performance in the burst SR problem."
Keys To Better Image Inpainting: Structure and Texture Go Hand in Hand,"Jitesh Jain, Yuqian Zhou, Ning Yu, Humphrey Shi","1SHI Lab @ University of Oregon, 3Picsart AI Research (PAIR); 1SHI Lab @ University of Oregon, 2IIT Roorkee, 3Picsart AI Research (PAIR); 4Adobe Inc.; 5Salesforce Research",50,"India, USA",50,Ukraine,"Deep image inpainting has made impressive progress with recent advances in image generation and processing algorithms. We claim that the performance of inpainting algorithms can be better judged by the generated structures and textures. Structures refer to the generated object boundary or novel geometric structures within the hole, while texture refers to high-frequency details, especially man-made repeating patterns filled inside the structural regions. We believe that better structures are usually obtained from a coarse-to-fine GAN-based generator network while repeating patterns nowadays can be better modeled using state-of-the-art high-frequency fast fourier convolutional layers. In this paper, we propose a novel inpainting network combining the advantages of the two designs. Therefore, our model achieves a remarkable visual quality to match state-of-the-art performance in both structure generation and repeating texture synthesis using a single network. Extensive experiments demonstrate the effectiveness of the method, and our conclusions further highlight the two critical factors of image inpainting quality, structures, and textures, as the future design directions of inpainting networks.",https://openaccess.thecvf.com/content/WACV2023/html/Jain_Keys_To_Better_Image_Inpainting_Structure_and_Texture_Go_Hand_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jain_Keys_To_Better_Image_Inpainting_Structure_and_Texture_Go_Hand_WACV_2023_paper.pdf,https://praeclarumjj3.github.io/fcf-inpainting/,https://github.com/praeclarumjj3/fcf-inpainting,2208.03382,main,Poster,https://ieeexplore.ieee.org/document/10030973/,"['Visualization', 'Computer vision', 'Statistical analysis', 'Image synthesis', 'Computational modeling', 'Generators', 'Periodic structures']","['Image Inpainting', 'Convolutional Layers', 'Fast Fourier', 'Image Texture', 'Object Boundaries', 'Feature Maps', 'Fast Fourier Transform', 'Receptive Field', 'Residual Block', 'Skip Connections', 'Reconstruction Loss', 'Latent Vector', 'Large Holes', 'Encoder Network', 'Synthesis Module', 'Deep Generative Models', 'Fast Synthesis', 'Hole Region']","['Algorithms: Computational photography', 'image and video synthesis']",32,"Deep image inpainting has made impressive progress with recent advances in image generation and processing algorithms. We claim that the performance of inpainting algorithms can be better judged by the generated structures and textures. Structures refer to the generated object boundary or novel geometric structures within the hole, while texture refers to high-frequency details, especially man-made repeating patterns filled inside the structural regions. We believe that better structures are usually obtained from a coarse-to-fine GAN-based generator network while repeating patterns nowadays can be better modeled using state-of-the-art high-frequency fast fourier convolutional layers. In this paper, we propose a novel inpainting network combining the advantages of the two designs. Therefore, our model achieves a remarkable visual quality to match state-of-the-art performance in both structure generation and repeating texture synthesis using a single network. Extensive experiments demonstrate the effectiveness of the method, and our conclusions further highlight the two critical factors of image inpainting quality, structures, and textures, as the future design directions of inpainting networks."
Kinematic-Aware Hierarchical Attention Network for Human Pose Estimation in Videos,"Kyung-Min Jin, Byoung-Sung Lim, Gun-Hee Lee, Tae-Kyung Kang, Seong-Whan Lee","Department of Computer Science and Engineering, Korea University; Department of Artificial Intelligence, Korea University",100,South Korea,0,,"Previous video-based human pose estimation methods have shown promising results by leveraging aggregated features of consecutive frames. However, most approaches compromise accuracy to mitigate jitter or do not sufficiently comprehend the temporal aspects of human motion. Furthermore, occlusion increases uncertainty between consecutive frames, which results in unsmooth results. To address these issues, we design an architecture that exploits the keypoint kinematic features with the following components. First, we effectively capture the temporal features by leveraging individual keypoint's velocity and acceleration. Second, the proposed hierarchical transformer encoder aggregates spatio-temporal dependencies and refines the 2D or 3D input pose estimated from existing estimators. Finally, we provide an online cross-supervision between the refined input pose generated from the encoder and the final pose from our decoder to enable joint optimization. We demonstrate comprehensive results and validate the effectiveness of our model in various tasks: 2D pose estimation, 3D pose estimation, body mesh recovery, and sparsely annotated multi-human pose estimation. Our code is available at https://github.com/KyungMinJin/HANet.",https://openaccess.thecvf.com/content/WACV2023/html/Jin_Kinematic-Aware_Hierarchical_Attention_Network_for_Human_Pose_Estimation_in_Videos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jin_Kinematic-Aware_Hierarchical_Attention_Network_for_Human_Pose_Estimation_in_Videos_WACV_2023_paper.pdf,,https://github.com/KyungMinJin/HANet,2211.15868,main,Poster,https://ieeexplore.ieee.org/document/10030274/,"['Solid modeling', 'Three-dimensional displays', 'Uncertainty', 'Computational modeling', 'Pose estimation', 'Kinematics', 'Jitter']","['Pose Estimation', 'Human Pose Estimation', 'Hierarchical Attention Network', 'Estimation In Videos', 'Video Pose Estimation', 'Temporal Features', 'Consecutive Frames', 'Motor Aspects', '3D Pose', 'Final Pose', 'Pose Estimation Methods', 'Spatio-temporal Dependencies', '2D Pose', 'Long Short-term Memory', 'Recurrent Neural Network', 'Fundamental Problem', 'Training Loss', 'Multi-scale Features', 'Secondary Loss', 'Joint Position', 'Multi-scale Feature Maps', 'Attention Map', 'Motion Blur', '1D Convolutional Layers', 'Crowded Scenes', '3D Body', 'Offset Position', 'L1-norm', 'Severe Occlusion', 'Movement Tracking']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",5,"Previous video-based human pose estimation methods have shown promising results by leveraging aggregated features of consecutive frames. However, most approaches compromise accuracy to mitigate jitter or do not sufficiently comprehend the temporal aspects of human motion. Furthermore, occlusion increases uncertainty between consecutive frames, which results in unsmooth results. To address these issues, we design an architecture that exploits the keypoint kinematic features with the following components. First, we effectively capture the temporal features by leveraging individual keypoint’s velocity and acceleration. Second, the proposed hierarchical transformer encoder aggregates spatio-temporal dependencies and refines the 2D or 3D input pose estimated from existing estimators. Finally, we provide an online cross-supervision between the refined input pose generated from the encoder and the final pose from our decoder to enable joint optimization. We demonstrate comprehensive results and validate the effectiveness of our model in various tasks: 2D pose estimation, 3D pose estimation, body mesh recovery, and sparsely annotated multi-human pose estimation. Our code is available at https://github.com/KyungMinJin/HANet."
Knowing What To Label for Few Shot Microscopy Image Cell Segmentation,"Youssef Dawoud, Arij Bouazizi, Katharina Ernst, Gustavo Carneiro, Vasileios Belagiannis","Friedrich-Alexander-Universit ¨at Erlangen-N ¨urnberg, Erlangen, Germany; Ulm University Medical Center, Ulm, Germany; Universit ¨at Ulm, Ulm, Germany; Mercedes-Benz AG, Stuttgart, Germany; Universit ¨at Ulm, Ulm, Germany; Centre for Vision, Speech and Signal Processing, University of Surrey, United Kingdom",100,"Germany, UK",0,,"In microscopy image cell segmentation, it is common to train a deep neural network on source data, containing different types of microscopy images, and then fine-tune it using a support set comprising a few randomly selected and annotated training target images. In this paper, we argue that the random selection of unlabelled training target images to be annotated and included in the support set may not enable an effective fine-tuning process, so we propose a new approach to optimise this image selection process. Our approach involves a new scoring function to find informative unlabelled target images. In particular, we propose to measure the consistency in the model predictions on target images against specific data augmentations. However, we observe that the model trained with source datasets does not reliably evaluate consistency on target images. To alleviate this problem, we propose novel self-supervised pretext tasks to compute the scores of unlabelled target images. Finally, the top few images with the least consistency scores are added to the support set for oracle (i.e., expert) annotation and later used to fine-tune the model to the target images. In our evaluations that involve the segmentation of five different types of cell images, we demonstrate promising results on several target test sets compared to the random selection approach as well as other selection approaches, such as Shannon's entropy and Monte-Carlo dropout. Our code will be made publicly available.",https://openaccess.thecvf.com/content/WACV2023/html/Dawoud_Knowing_What_To_Label_for_Few_Shot_Microscopy_Image_Cell_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dawoud_Knowing_What_To_Label_for_Few_Shot_Microscopy_Image_Cell_WACV_2023_paper.pdf,,,2211.10244,main,Poster,https://ieeexplore.ieee.org/document/10030398/,"['Training', 'Image segmentation', 'Microscopy', 'Computational modeling', 'Semisupervised learning', 'Predictive models', 'Particle measurements']","['Microscopy Images', 'Cell Segmentation', 'Data Sources', 'Deep Neural Network', 'Scoring Function', 'Random Selection', 'Data Augmentation', 'Types Of Images', 'Target Image', 'Shannon Entropy', 'Consistency Score', 'Source Dataset', 'Support Set', 'Annotated Training', 'Fine-tuning Process', 'Different Types Of Images', 'Expert Annotations', 'Pretext Task', 'Self-supervised Task', 'Electron Microscopy', 'Target Dataset', 'Binary Segmentation', 'Artificial Cells', 'Segmentation Model', 'Image Segmentation', 'Image Patches', 'Entropy Approach', 'Cells Of Interest', 'Fine-tuned Model', 'Image X']",['Applications: Biomedical/healthcare/medicine'],3,"In microscopy image cell segmentation, it is common to train a deep neural network on source data, containing different types of microscopy images, and then fine-tune it using a support set comprising a few randomly selected and annotated training target images. In this paper, we argue that the random selection of unlabelled training target images to be annotated and included in the support set may not enable an effective fine-tuning process, so we propose a new approach to optimise this image selection process. Our approach involves a new scoring function to find informative unlabelled target images. In particular, we propose to measure the consistency in the model predictions on target images against specific data augmentations. However, we observe that the model trained with source datasets does not reliably evaluate consistency on target images. To alleviate this problem, we propose novel self-supervised pretext tasks to compute the scores of unlabelled target images. Finally, the top few images with the least consistency scores are added to the support set for oracle (i.e., expert) annotation and later used to fine-tune the model to the target images. In our evaluations that involve the segmentation of five different types of cell images, we demonstrate promising results on several target test sets compared to the random selection approach as well as other selection approaches, such as Shannon's entropy and Monte-Carlo dropout."
LAB: Learnable Activation Binarizer for Binary Neural Networks,"Sieger Falkena, Hadi Jamali-Rad, Jan van Gemert","TU Delft, Delft, The Netherlands; TU Delft, Delft, The Netherlands; Shell Global Solutions International B.V., Amsterdam, The Netherlands",100,Netherlands,0,,"Binary Neural Networks (BNNs) are receiving an upsurge of attention for bringing power-hungry deep learning towards edge devices. The traditional wisdom in this space is to employ sign() for binarizing featuremaps. We argue and illustrate that sign() is a uniqueness bottleneck, limiting information propagation throughout the network. To alleviate this, we propose to dispense sign(), replacing it with a learnable activation binarizer (LAB), allowing the network to learn a fine-grained binarization kernel per layer - as opposed to global thresholding. LAB is a novel universal module that can seamlessly be integrated into existing architectures. To confirm this, we plug it into four seminal BNNs and show a considerable performance boost at the cost of tolerable increase in delay and complexity. Finally, we build an end-to-end BNN (coined as LAB-BNN) around LAB, and demonstrate that it achieves competitive performance on par with the state-of-the-art on ImageNet. Codebase in the supplementary will be made publicly available upon acceptance.",https://openaccess.thecvf.com/content/WACV2023/html/Falkena_LAB_Learnable_Activation_Binarizer_for_Binary_Neural_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Falkena_LAB_Learnable_Activation_Binarizer_for_Binary_Neural_Networks_WACV_2023_paper.pdf,,https://github.com/sfalkena/LAB,2210.13858,main,Poster,https://ieeexplore.ieee.org/document/10030154/,"['Deep learning', 'Computer vision', 'Limiting', 'Costs', 'Neural networks', 'Computer architecture', 'Delays']","['Binary Network', 'Binary Neural Networks', 'Deep Learning', 'Feature Maps', 'ImageNet', 'Global Threshold', 'Delay Increases', 'Convolutional Neural Network', 'Real-valued', 'Convolutional Layers', 'Model Size', 'Binary Data', 'Convolution Kernel', 'Quantization Error', 'Validation Accuracy', 'Output Feature Map', 'Binary Function', 'Floating-point Operations', 'Theoretical Number', 'Depthwise Convolution']","['Applications: Embedded sensing/real-time techniques', 'Smartphones/end user devices']",3,"Binary Neural Networks (BNNs) are receiving an up-surge of attention for bringing power-hungry deep learning towards edge devices. The traditional wisdom in this space is to employ sign(.) for binarizing feature maps. We argue and illustrate that sign(.) is a uniqueness bottleneck, limiting information propagation throughout the network. To alleviate this, we propose to dispense sign(.), replacing it with a learnable activation binarizer (LAB), allowing the network to learn a fine-grained binarization kernel per layer - as opposed to global thresholding. LAB is a novel universal module that can seamlessly be integrated into existing architectures. To confirm this, we plug it into four seminal BNNs and show a considerable accuracy boost at the cost of tolerable increase in delay and complexity. Finally, we build an end-to-end BNN (coined as LAB-BNN) around LAB, and demonstrate that it achieves competitive performance on par with the state-of-the-art on ImageNet. Our code can be found in our repository: https://github.com/sfalkena/LAB."
LAVA: Label-Efficient Visual Learning and Adaptation,"Islam Nassar, Munawar Hayat, Ehsan Abbasnejad, Hamid Rezatofighi, Mehrtash Harandi, Gholamreza Haffari","Monash University, Australia; University of Adelaide, Australia",100,Australia,0,,"We present LAVA, a simple yet effective method for multi-domain visual transfer learning with limited data. LAVA builds on a few recent innovations to enable adapting to partially labelled datasets with class and domain shifts. First, LAVA learns self-supervised visual representations on the source dataset and ground them using class label semantics to overcome transfer collapse problems associated with supervised pretraining. Secondly, LAVA maximises the gains from unlabelled target data via a novel method which uses multi-crop augmentations to obtain highly robust pseudo-labels. By combining these ingredients, LAVA achieves a new state-of-the-art on ImageNet semi-supervised protocol, as well as on 7 out of 10 datasets in multi-domain few-shot learning on the Meta-dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Nassar_LAVA_Label-Efficient_Visual_Learning_and_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nassar_LAVA_Label-Efficient_Visual_Learning_and_Adaptation_WACV_2023_paper.pdf,,github.com/islam-nassar/lava.git,2210.10317,main,Poster,https://ieeexplore.ieee.org/document/10030474/,"['Visualization', 'Technological innovation', 'Computer vision', 'Protocols', 'Semantics', 'Transfer learning', 'Object detection']","['Visual Learning', 'Visual Representation', 'Transfer Learning', 'Class Labels', 'Domain Shift', 'Unlabeled Data', 'Source Dataset', 'Few-shot Learning', 'Unlabeled Target Data', 'Language Model', 'Target Domain', 'Transfer Time', 'Semi-supervised Learning', 'Appendix For Details', 'Self-supervised Learning', 'Visual Domain', 'Target Dataset', 'Visual Similarity', 'Glass Of Wine', 'Semantic Space', 'Linguistic Similarity', 'School Bus', 'Large Crop', 'Multiple Benchmarks', 'Bottle Of Wine', 'Semantic Language', 'Source Class', 'Query Image', 'Real Domain', 'Teacher Network']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",,"We present LAVA, a simple yet effective method for multi-domain visual transfer learning with limited data. LAVA builds on a few recent innovations to enable adapting to partially labelled datasets with class and domain shifts. First, LAVA learns self-supervised visual representations on the source dataset and ground them using class label semantics to overcome transfer collapse problems associated with supervised pretraining. Secondly, LAVA maximises the gains from unlabelled target data via a novel method which uses multi-crop augmentations to obtain highly robust pseudo-labels. By combining these ingredients, LAVA achieves a new state-of-the-art on ImageNet semi-supervised protocol, as well as on 7 out of 10 datasets in multi-domain few-shot learning on the Meta-dataset.
<sup>1</sup>"
"LCS: Learning Compressible Subspaces for Efficient, Adaptive, Real-Time Network Compression at Inference Time","Elvis Nunez, Maxwell Horton, Anish Prabhu, Anurag Ranjan, Ali Farhadi, Mohammad Rastegari","Apple; University of California, Los Angeles",50,USA,50,USA,"When deploying deep neural networks (DNNs) to a device, it is traditionally assumed that available computational resources (compute, memory, and power) remain static. However, real-world computing systems do not always provide stable resource guarantees. Computational resources need to be conserved when load from other processes is high, or available memory is low. In this work, we present a training procedure to produce DNNs that can be compressed in real-time to arbitrary compression levels entirely on-device. This enables the deployment of a single model that can efficiently adapt to its host device's available resources. We formulate this problem as learning an adaptively compressible network subspace, where one end is optimized for accuracy, and the other for efficiency. Our subspace model requires no recalibration nor retraining when changing compression levels. Moreover, our generic training framework is amenable to multiple forms of compression, and we present results for unstructured sparsity, structured sparsity, and quantization on a variety of architectures. We present models that require a single extra copy of network parameters, as well as models that require no extra parameters. Both models allow for operation at any compression level within a wide range (for example, 0% to 90% for structured sparsity with ResNet18 on ImageNet). At each compression level, our models achieve an accuracy comparable to a baseline model optimized for that particular compression level. To our knowledge, our method is the first to enable adaptive on-device network compression with little to no computational overhead.",https://openaccess.thecvf.com/content/WACV2023/html/Nunez_LCS_Learning_Compressible_Subspaces_for_Efficient_Adaptive_Real-Time_Network_Compression_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nunez_LCS_Learning_Compressible_Subspaces_for_Efficient_Adaptive_Real-Time_Network_Compression_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030852/,['Portable document format'],,"['Applications: Smartphones/end user devices', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"When deploying deep neural networks (DNNs) to a device, it is traditionally assumed that available computational resources (compute, memory, and power) remain static. However, real-world computing systems do not always provide stable resource guarantees. Computational resources need to be conserved when load from other processes is high, or available memory is low. In this work, we present a training procedure to produce DNNs that can be compressed in real-time to arbitrary compression levels entirely on-device. This enables the deployment of a single model that can efficiently adapt to its host device’s available resources. We formulate this problem as learning an adaptively compressible network subspace, where one end is optimized for accuracy, and the other for efficiency. Our subspace model requires no recalibration nor retraining when changing compression levels. Moreover, our generic training framework is amenable to multiple forms of compression, and we present results for unstructured sparsity, structured sparsity, and quantization on a variety of architectures. We present models that require a single extra copy of network parameters, as well as models that require no extra parameters. Both models allow for operation at any compression level within a wide range (for example, 0% to 90% for structured sparsity with ResNet18 on ImageNet). At each compression level, our models achieve an accuracy comparable to a baseline model optimized for that particular compression level. To our knowledge, our method is the first to enable adaptive on-device network compression with little to no computational overhead."
LRA&LDRA: Rethinking Residual Predictions for Efficient Shadow Detection and Removal,"Mehmet Kerim Yücel, Valia Dimaridou, Bruno Manganelli, Mete Ozay, Anastasios Drosou, Albert Saà-Garriga","Samsung Research UK; CERTH ITI, Greece",50,Greece,50,South Korea,"The majority of the state-of-the-art shadow removal models (SRMs) reconstruct whole input images, where their capacity is needlessly spent on reconstructing non-shadow regions. SRMs that predict residuals remedy this up to a degree, but fall short of providing an accurate and flexible solution. In this paper, we rethink residual predictions and propose Learnable Residual Attention (LRA) and Learnable Dense Reconstruction Attention (LDRA) modules, which operate over the input and the output of SRMs. These modules guide an SRM to concentrate on shadow region reconstruction, and limit reconstruction of non-shadow regions. The modules improve shadow removal (up to 20%) and detection accuracy across various backbones, and even improve the accuracy of other removal methods (up to 10%). In addition, the modules have minimal overhead (+<1MB memory) and are implemented in a few lines of code. Furthermore, to combat the challenge of training SRMs with small datasets, we present a synthetic dataset generation pipeline. Using our pipeline, we create a dataset called PITSA, which has 10 times more unique shadow-free images than the largest benchmark dataset. Pre-training models on the PITSA significantly improves shadow removal (+2 MAE on shadow regions) and detection accuracy of multiple methods. Our results show that LRA&LDRA, when plugged into a lightweight architecture pre-trained on the PITSA, outperform state-of-the-art shadow removal (+0.7 all-region MAE) and detection (+0.1 BER) methods on the benchmark ISTD and SRD datasets, despite running faster (+5%) and consuming less memory (x150).",https://openaccess.thecvf.com/content/WACV2023/html/Yucel_LRALDRA_Rethinking_Residual_Predictions_for_Efficient_Shadow_Detection_and_Removal_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yucel_LRALDRA_Rethinking_Residual_Predictions_for_Efficient_Shadow_Detection_and_Removal_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030463/,"['Training', 'Computer vision', 'Computational modeling', 'Pipelines', 'Memory management', 'Benchmark testing', 'Predictive models']","['Shadow Detection', 'Accurate Method', 'Input Image', 'Removal Method', 'Largest Dataset', 'Unique Images', 'Minimal Overhead', 'Shadow Regions', 'Deep Neural Network', 'Image Reconstruction', 'Functional Identification', 'Supplementary Materials For Details', 'Handcrafted Features', 'User Input', 'Deep Neural Network Model', 'L1 Loss', 'Color Correction', 'Supplementary Material For Results', 'Shadow Images']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Arts/games/social media']",1,"The majority of the state-of-the-art shadow removal models (SRMs) reconstruct whole input images, where their capacity is needlessly spent on reconstructing non-shadow regions. SRMs that predict residuals remedy this up to a degree, but fall short of providing an accurate and flexible solution. In this paper, we rethink residual predictions and propose Learnable Residual Attention (LRA) and Learnable Dense Reconstruction Attention (LDRA) modules, which operate over the input and the output of SRMs. These modules guide an SRM to concentrate on shadow region reconstruction, and limit reconstruction of non-shadow regions. The modules improve shadow removal (up to 20%) and detection accuracy across various backbones, and even improve the accuracy of other removal methods (up to 10%). In addition, the modules have minimal overhead (+<1MB memory) and are implemented in a few lines of code. Furthermore, to combat the challenge of training SRMs with small datasets, we present a synthetic dataset generation pipeline. Using our pipeline, we create a dataset called PITSA, which has 10 times more unique shadow-free images than the largest benchmark dataset. Pre-training models on the PITSA significantly improves shadow removal (+2 MAE on shadow regions) and detection accuracy of multiple methods. Our results show that LRA&LDRA, when plugged into a lightweight architecture pre-trained on the PITSA, outperform state-of-the-art shadow removal (+0.7 all-region MAE) and detection (+0.1 BER) methods on the benchmark ISTD and SRD datasets, despite running faster (+5%) and consuming less memory (×150)."
Language-Free Training for Zero-Shot Video Grounding,"Dahye Kim, Jungin Park, Jiyoung Lee, Seongheon Park, Kwanghoon Sohn",Yonsei University/Korea Institute of Science and Technology (KIST); NAVER AI Lab; Yonsei University,66.66666667,South Korea,33.33333333,South Korea,"Given an untrimmed video and a language query depicting a specific temporal moment in the video, video grounding aims to localize the time interval by understanding the text and video simultaneously. One of the most challenging issues is an extremely time- and cost-consuming annotation collection, including video captions in a natural language form and their corresponding temporal regions. In this paper, we present a simple yet novel training framework for video grounding in the zero-shot setting, which learns a network with only video data without any annotation. Inspired by the recent language-free paradigm, i.e. training without language data, we train the network without compelling the generation of fake (pseudo) text queries into a natural language form. Specifically, we propose a method for learning a video grounding model by selecting a temporal interval as a hypothetical correct answer and considering the visual feature selected by our method in the interval as a language feature, with the help of the well-aligned visual-language space of CLIP. Extensive experiments demonstrate the prominence of our language-free training framework, outperforming the existing zero-shot video grounding method and even several weakly-supervised approaches with large margins on two standard datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Kim_Language-Free_Training_for_Zero-Shot_Video_Grounding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_Language-Free_Training_for_Zero-Shot_Video_Grounding_WACV_2023_paper.pdf,,,2210.12977,main,Poster,https://ieeexplore.ieee.org/document/10030412/,"['Training', 'Visualization', 'Computer vision', 'Grounding', 'Annotations', 'Natural languages', 'Standards']","['Video Grounding', 'Temporal Lobe', 'Natural Language', 'Visual Features', 'Large Margin', 'Query Language', 'Interval Method', 'Text Query', 'Video Captioning', 'Transformer', 'Qualitative Results', 'Object Detection', 'Attention Mechanism', 'Line Of Work', 'Dominant Feature', 'Loss Term', 'Temporal Model', 'Textual Features', 'Real Words', 'Training Videos', 'Video Features', 'Pseudo Labels', 'Temporal Boundaries', 'Video Encoding', 'Semantic Space', 'Temporal Loss', 'Visual Encoding', 'Contrastive Loss', 'Video Surveillance', 'Single Dominant']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",9,"Given an untrimmed video and a language query depicting a specific temporal moment in the video, video grounding aims to localize the time interval by understanding the text and video simultaneously. One of the most challenging issues is an extremely time- and cost-consuming annotation collection, including video captions in a natural language form and their corresponding temporal regions. In this paper, we present a simple yet novel training framework for video grounding in the zero-shot setting, which learns a network with only video data without any annotation. Inspired by the recent language-free paradigm, i.e. training without language data, we train the network without compelling the generation of fake (pseudo) text queries into a natural language form. Specifically, we propose a method for learning a video grounding model by selecting a temporal interval as a hypothetical correct answer and considering the visual feature selected by our method in the interval as a language feature, with the help of the well-aligned visual-language space of CLIP. Extensive experiments demonstrate the prominence of our language-free training framework, outperforming the existing zero-shot video grounding method and even several weakly-supervised approaches with large margins on two standard datasets."
Large-Scale Open-Set Classification Protocols for ImageNet,"Andres Palechor, Annesha Bhoumik, Manuel Günther","Department of Informatics, University of Zurich",100,Switzerland,0,,"Open-Set Classification (OSC) intends to adapt closed-set classification models to real-world scenarios, where the classifier must correctly label samples of known classes while rejecting previously unseen unknown samples. Only recently, research started to investigate on algorithms that are able to handle these unknown samples correctly. Some of these approaches address OSC by including into the training set negative samples that a classifier learns to reject, expecting that these data increase the robustness of the classifier on unknown classes. Most of these approaches are evaluated on small-scale and low-resolution image datasets like MNIST, SVHN or CIFAR, which makes it difficult to assess their applicability to the real world, and to compare them among each other. We propose three open-set protocols that provide rich datasets of natural images with different levels of similarity between known and unknown classes. The protocols consist of subsets of ImageNet classes selected to provide training and testing data closer to real-world scenarios. Additionally, we propose a new validation metric that can be employed to assess whether the training of deep learning models addresses both the classification of known samples and the rejection of unknown samples. We use the protocols to compare the performance of two baseline open-set algorithms to the standard SoftMax baseline and find that the algorithms work well on negative samples that have been seen during training, and partially on out-of-distribution detection tasks, but drop performance in the presence of samples from previously unseen unknown classes.",https://openaccess.thecvf.com/content/WACV2023/html/Palechor_Large-Scale_Open-Set_Classification_Protocols_for_ImageNet_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Palechor_Large-Scale_Open-Set_Classification_Protocols_for_ImageNet_WACV_2023_paper.pdf,https://www.ifi.uzh.ch/en/aiml.html,,,main,Poster,https://ieeexplore.ieee.org/document/10030943/,"['Training', 'Measurement', 'Protocols', 'Earth Observing System', 'Robustness', 'Classification algorithms', 'Partitioning algorithms']","['ImageNet', 'Open Set Classification', 'Deep Learning', 'Negative Samples', 'Present Sample', 'Classification Of Samples', 'Unknown Samples', 'Low-resolution Images', 'Training Deep Learning Models', 'Validation Metrics', 'Support Vector Machine', 'Supplemental Material', 'False Positive Rate', 'Image Classification', 'Additional Categories', 'Semantic Similarity', 'Class Probabilities', 'Evaluation Protocol', 'Labeled Samples', 'Base Classes', 'Correct Classification Rate', 'Softmax Loss', 'Softmax Activation', 'Binary Classification Task', 'Small-scale Datasets', 'Unseen Classes', 'Reproducible Research', 'Unknown Data', 'Lot Of Samples', 'Respective Classes']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"Open-Set Classification (OSC) intends to adapt closed-set classification models to real-world scenarios, where the classifier must correctly label samples of known classes while rejecting previously unseen unknown samples. Only recently, research started to investigate on algorithms that are able to handle these unknown samples correctly. Some of these approaches address OSC by including into the training set negative samples that a classifier learns to reject, expecting that these data increase the robustness of the classifier on unknown classes. Most of these approaches are evaluated on small-scale and low-resolution image datasets like MNIST, SVHN or CIFAR, which makes it difficult to assess their applicability to the real world, and to compare them among each other. We propose three open-set protocols that provide rich datasets of natural images with different levels of similarity between known and unknown classes. The protocols consist of subsets of ImageNet classes selected to provide training and testing data closer to real-world scenarios. Additionally, we propose a new validation metric that can be employed to assess whether the training of deep learning models addresses both the classification of known samples and the rejection of unknown samples. We use the protocols to compare the performance of two baseline open-set algorithms to the standard SoftMax baseline and find that the algorithms work well on negative samples that have been seen during training, and partially on out-of-distribution detection tasks, but drop performance in the presence of samples from previously unseen unknown classes."
LayerDoc: Layer-Wise Extraction of Spatial Hierarchical Structure in Visually-Rich Documents,"Puneet Mathur, Rajiv Jain, Ashutosh Mehra, Jiuxiang Gu, Franck Dernoncourt, Anandhavelu N., Quan Tran, Verena Kaynig-Fittkau, Ani Nenkova, Dinesh Manocha, Vlad I. Morariu","Adobe Research; University of Maryland, College Park",50,USA,50,USA,"Digital documents often contain images and scanned text. Parsing such visually-rich documents is a core task for workflow automation, but it remains challenging since most documents do not encode explicit layout information, e.g., how characters and words are grouped into boxes and ordered into larger semantic entities. Current state-of-the-art layout extraction methods are challenged on such documents as they rely on word sequences to have correct reading order and do not exploit their hierarchical structure. We propose LayerDoc, an approach that uses visual features, textual semantics, and spatial coordinates along with constraint inference to extract the hierarchical layout structure of documents in a bottom-up layer-wise fashion. LayerDoc recursively groups smaller regions into larger semantic elements in 2D to infer complex nested hierarchies. Experiments show that our approach outperforms competitive baselines by 10-15% on three diverse datasets of forms and mobile app screen layouts for the tasks of spatial region classification, higher-order group identification, layout hierarchy extraction, reading order detection, and word grouping.",https://openaccess.thecvf.com/content/WACV2023/html/Mathur_LayerDoc_Layer-Wise_Extraction_of_Spatial_Hierarchical_Structure_in_Visually-Rich_Documents_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mathur_LayerDoc_Layer-Wise_Extraction_of_Spatial_Hierarchical_Structure_in_Visually-Rich_Documents_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030764/,"['Visualization', 'Computer vision', 'Automation', 'Layout', 'Semantics', 'Feature extraction', 'Mobile applications']","['Hierarchical Structure', 'Spatial Structure', 'Mobile App', 'Classification Task', 'Visual Features', 'Spatial Regions', 'Structural Layout', 'Parents Of Children', 'Negative Samples', 'Visual Cues', 'Object Detection', 'Bounding Box', 'Language Model', 'Mean Average Precision', 'Spatial Cues', 'Part Of The Input', 'Text Field', 'Spatial Signal', 'Multimodal Learning', 'Link Prediction', 'Structural Cues', 'Semantic Cues', 'Multimodal Input', 'Probability Of Link', 'Blocks Of Text', 'Box Coordinates', 'Sentence Embedding', 'Key-value Pairs', 'IoU Threshold', 'Box Type']",['Algorithms: Vision + language and/or other modalities'],2,"Digital documents often contain images and scanned text. Parsing such visually-rich documents is a core task for work-flow automation, but it remains challenging since most documents do not encode explicit layout information, e.g., how characters and words are grouped into boxes and ordered into larger semantic entities. Current state-of-the-art layout extraction methods are challenged by such documents as they rely on word sequences to have correct reading order and do not exploit their hierarchical structure. We propose LayerDoc, an approach that uses visual features, textual semantics, and spatial coordinates along with constraint inference to extract the hierarchical layout structure of documents in a bottom-up layer-wise fashion. LayerDoc recursively groups smaller regions into larger semantic elements in 2D to infer complex nested hierarchies. Experiments show that our approach outperforms competitive baselines by 10-15% on three diverse datasets of forms and mobile app screen layouts for the tasks of spatial region classification, higher-order group identification, layout hierarchy extraction, reading order detection, and word grouping."
Learnable Human Mesh Triangulation for 3D Human Pose and Shape Estimation,"Sungho Chun, Sungbum Park, Ju Yong Chang","NCSOFT, Korea; Dept of ECE, Kwangwoon University, Korea",50,South Korea,50,South Korea,"Compared to joint position, the accuracy of joint rotation and shape estimation has received relatively little attention in the skinned multi-person linear model (SMPL)-based human mesh reconstruction from multi-view images. The work in this field is broadly classified into two categories. The first approach performs joint estimation and then produces SMPL parameters by fitting SMPL to resultant joints. The second approach regresses SMPL parameters directly from the input images through a convolutional neural network (CNN)-based model. However, these approaches suffer from the lack of information for resolving the ambiguity of joint rotation and shape reconstruction and the difficulty of network learning. To solve the aforementioned problems, we propose a two-stage method. The proposed method first estimates the coordinates of mesh vertices through a CNN-based model from input images, and acquires SMPL parameters by fitting the SMPL model to the estimated vertices. Estimated mesh vertices provide sufficient information for determining joint rotation and shape, and are easier to learn than SMPL parameters. According to experiments using Human3.6M and MPI-INF-3DHP datasets, the proposed method significantly outperforms the previous works in terms of joint rotation and shape estimation, and achieves competitive performance in terms of joint location estimation.",https://openaccess.thecvf.com/content/WACV2023/html/Chun_Learnable_Human_Mesh_Triangulation_for_3D_Human_Pose_and_Shape_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chun_Learnable_Human_Mesh_Triangulation_for_3D_Human_Pose_and_Shape_WACV_2023_paper.pdf,,,2208.11251,main,Poster,https://ieeexplore.ieee.org/document/10030828/,"['Surface reconstruction', 'Three-dimensional displays', 'Image resolution', 'Shape', 'Fitting', 'Estimation', 'Graphics processing units']","['Pose Estimation', 'Shape Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Human Mesh', 'Convolutional Neural Network', 'Input Image', 'Two-stage Method', 'Joint Estimation', 'Root Mean Square Error Of Cross-validation', 'Joint Rotation', 'Multi-view Images', 'Shape Reconstruction', 'Vertex Coordinates', 'Mesh Vertices', 'Single Image', 'Adam Optimizer', '3D Space', 'Position Error', '3D Joint', 'Human Shape', '3D Pose', '3D Mesh', 'Angular Distance', 'Multi-view Feature', 'Human Joint', 'Convolutional Neural Networks Backbone', 'Volumetric Features', '3D Convolution']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision']",15,"Compared to joint position, the accuracy of joint rotation and shape estimation has received relatively little attention in the skinned multi-person linear model (SMPL)-based human mesh reconstruction from multi-view images. The work in this field is broadly classified into two categories. The first approach performs joint estimation and then produces SMPL parameters by fitting SMPL to resultant joints. The second approach regresses SMPL parameters directly from the input images through a convolutional neural network (CNN)-based model. However, these approaches suffer from the lack of information for resolving the ambiguity of joint rotation and shape reconstruction and the difficulty of network learning. To solve the aforementioned problems, we propose a two-stage method. The proposed method first estimates the coordinates of mesh vertices through a CNN-based model from input images, and acquires SMPL parameters by fitting the SMPL model to the estimated vertices. Estimated mesh vertices provide sufficient information for determining joint rotation and shape, and are easier to learn than SMPL parameters. According to experiments using Human3.6M and MPI-INF-3DHP datasets, the proposed method significantly outperforms the previous works in terms of joint rotation and shape estimation, and achieves competitive performance in terms of joint location estimation."
Learning 3D Human Pose Estimation From Dozens of Datasets Using a Geometry-Aware Autoencoder To Bridge Between Skeleton Formats,"István Sárándi, Alexander Hermans, Bastian Leibe","RWTH Aachen University, Germany",100,Germany,0,,"Deep learning-based 3D human pose estimation performs best when trained on large amounts of labeled data, making combined learning from many datasets an important research direction. One obstacle to this endeavor are the different skeleton formats provided by different datasets, i.e., they do not label the same set of anatomical landmarks. There is little prior research on how to best supervise one model with such discrepant labels. We show that simply using separate output heads for different skeletons results in inconsistent depth estimates and insufficient information sharing across skeletons. As a remedy, we propose a novel affine-combining autoencoder (ACAE) method to perform dimensionality reduction on the number of landmarks. The discovered latent 3D points capture the redundancy among skeletons, enabling enhanced information sharing when used for consistency regularization. Our approach scales to an extreme multi-dataset regime, where we use 28 3D human pose datasets to supervise one model, which outperforms prior work on a range of benchmarks, including the challenging 3D Poses in the Wild (3DPW) dataset. Our code and models are available for research purposes.",https://openaccess.thecvf.com/content/WACV2023/html/Sarandi_Learning_3D_Human_Pose_Estimation_From_Dozens_of_Datasets_Using_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sarandi_Learning_3D_Human_Pose_Estimation_From_Dozens_of_Datasets_Using_WACV_2023_paper.pdf,,https://vision.rwth-aachen.de/wacv23sarandi,,main,Poster,https://ieeexplore.ieee.org/document/10030167/,"['Training', 'Dimensionality reduction', 'Solid modeling', 'Three-dimensional displays', 'Codes', 'Pose estimation', 'Redundancy']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Dozens Of Datasets', 'Dimensionality Reduction', '3D Datasets', '3D Pose', 'Number Of Landmarks', 'Separate Heading', 'Latent Space', 'Individual Datasets', 'Training Examples', 'Geometric Relationship', 'Direct Prediction', 'Convex Combination', 'Reconstruction Loss', '2D Projection', 'L1 Loss', 'Left Shoulder', 'Linear Dimensionality Reduction', 'Mean Euclidean Distance', 'Camera Intrinsics', 'Large-scale Training']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision']",13,"Deep learning-based 3D human pose estimation performs best when trained on large amounts of labeled data, making combined learning from many datasets an important research direction. One obstacle to this endeavor are the different skeleton formats provided by different datasets, i.e., they do not label the same set of anatomical landmarks. There is little prior research on how to best supervise one model with such discrepant labels. We show that simply using separate output heads for different skeletons results in inconsistent depth estimates and insufficient information sharing across skeletons. As a remedy, we propose a novel affine-combining autoencoder (ACAE) method to perform dimensionality reduction on the number of landmarks. The discovered latent 3D points capture the redundancy among skeletons, enabling enhanced information sharing when used for consistency regularization. Our approach scales to an extreme multi-dataset regime, where we use 28 3D human pose datasets to supervise one model, which outperforms prior work on a range of benchmarks, including the challenging 3D Poses in the Wild (3DPW) dataset. Our code and models are available for research purposes.
<sup>1</sup>"
Learning Across Domains and Devices: Style-Driven Source-Free Domain Adaptation in Clustered Federated Learning,"Donald Shenaj, Eros Fanì, Marco Toldo, Debora Caldarola, Antonio Tavera, Umberto Michieli, Marco Ciccone, Pietro Zanuttigh, Barbara Caputo","University of Padova, Italy; Politecnico di Torino, Italy",100,Italy,0,,"Federated Learning (FL) has recently emerged as a possible way to tackle the domain shift in real-world Semantic Segmentation (SS) without compromising the private nature of the collected data. However, most of the existing works on FL unrealistically assume labeled data in the remote clients. Here we propose a novel task (FFREEDA) in which the clients' data is unlabeled and the server accesses a source labeled dataset for pre-training only. To solve FFREEDA, we propose LADD, which leverages the knowledge of the pre-trained model by employing self-supervision with ad-hoc regularization techniques for local training and introducing a novel federated clustered aggregation scheme based on the clients' style. Our experiments show that our algorithm is able to efficiently tackle the new task outperforming existing approaches. The code is available at https://github.com/Erosinho13/LADD.",https://openaccess.thecvf.com/content/WACV2023/html/Shenaj_Learning_Across_Domains_and_Devices_Style-Driven_Source-Free_Domain_Adaptation_in_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shenaj_Learning_Across_Domains_and_Devices_Style-Driven_Source-Free_Domain_Adaptation_in_WACV_2023_paper.pdf,,https://github.com/Erosinho13/LADD,,main,Poster,https://ieeexplore.ieee.org/document/10030428/,"['Training', 'Computer vision', 'Adaptation models', 'Codes', 'Federated learning', 'Semantic segmentation', 'Clustering algorithms']","['Domain Adaptation', 'Federated Learning', 'Source-free Domain Adaptation', 'Clustered Federated Learning', 'Domain Shift', 'Semantic Segmentation', 'Regularization Techniques', 'Local Training', 'Aggregation Scheme', 'Data Sources', 'Local Data', 'Heterogeneous Distribution', 'Statistical Heterogeneity', 'Vision Tasks', 'Source Images', 'Target Data', 'Adaptive Technique', 'Self-driving', '2D Space', 'Local Dataset', 'Source Dataset', 'Client-side', 'Pre-training Phase', 'Target Dataset', 'Pre-training Stage', 'Central Server', 'Style Transfer', 'Cluster Aggregation', 'Federated Learning Framework', 'Target Distribution']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",17,"Federated Learning (FL) has recently emerged as a possible way to tackle the domain shift in real-world Semantic Segmentation (SS) without compromising the private nature of the collected data. However, most of the existing works on FL unrealistically assume labeled data in the re-mote clients. Here we propose a novel task (FFreeDA) in which the clients’ data is unlabeled and the server accesses a source labeled dataset for pre-training only. To solve FFreeDA, we propose LADD, which leverages the knowledge of the pre-trained model by employing self-supervision with ad-hoc regularization techniques for local training and introducing a novel federated clustered aggregation scheme based on the clients’ style. Our experiments show that our algorithm is able to efficiently tackle the new task out-performing existing approaches. The code is available at https://github.com/Erosinho13/LADD."
Learning Attention Propagation for Compositional Zero-Shot Learning,"Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Alain Pagani, Didier Stricker, Muhammad Zeshan Afzal","ETH Zürich; DFKI, ETH Zürich, TU Kaiserslautern, MindGarage; DFKI, TU Kaiserslautern, MindGarage",100,"Germany, Switzerland",0,,"Compositional zero-shot learning aims to recognize unseen compositions of seen visual primitives of object classes and their states. While all primitives (states and objects) are observable during training in some combination, their complex interaction makes this task especially hard. For example, wet changes the visual appearance of a dog very differently from a bicycle. Furthermore, we argue that relationships between compositions go beyond shared states or objects. A cluttered office can contain a busy table; even though these compositions don't share a state or object, the presence of a busy table can guide the presence of a cluttered office. We propose a novel method called Compositional Attention Propagated Embedding (CAPE) as a solution. The key intuition to our method is that a rich dependency structure exists between compositions arising from complex interactions of primitives in addition to other dependencies between compositions. CAPE learns to identify this structure and propagates knowledge between them to learn class embedding for all seen and unseen compositions. In the challenging generalized compositional zero-shot setting, we show that our method outperforms previous baselines to set a new state-of-the-art on three publicly available benchmarks.",https://openaccess.thecvf.com/content/WACV2023/html/Khan_Learning_Attention_Propagation_for_Compositional_Zero-Shot_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Khan_Learning_Attention_Propagation_for_Compositional_Zero-Shot_Learning_WACV_2023_paper.pdf,,,2210.11557,main,Poster,https://ieeexplore.ieee.org/document/10030495/,"['Training', 'Visualization', 'Computer vision', 'Buildings', 'Dogs', 'Bicycles', 'Benchmark testing']","['Zero-shot', 'Dependence Structure', 'Transformer', 'Validation Set', 'State Of The Art', 'Qualitative Results', 'Attention Mechanism', 'Multilayer Perceptron', 'Harmonic Mean', 'Dot Product', 'Area Under Curve', 'Transformation Function', 'Word Embedding', 'Graph Convolutional Network', 'Propagation Constant', 'Image Classification Tasks', 'Number Of Heads', 'Label Noise', 'Image Embedding', 'Unseen Classes', 'Composition Of Objects']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",7,"Compositional zero-shot learning aims to recognize unseen compositions of seen visual primitives of object classes and their states. While all primitives (states and objects) are observable during training in some combination, their complex interaction makes this task especially hard. For example, wet changes the visual appearance of a dog very differently from a bicycle. Furthermore, we argue that relationships between compositions go beyond shared states or objects. A cluttered office can contain a busy table; even though these compositions don’t share a state or object, the presence of a busy table can guide the presence of a cluttered office. We propose a novel method called Compositional Attention Propagated Embedding (CAPE) as a solution. The key intuition to our method is that a rich dependency structure exists between compositions arising from complex interactions of primitives in addition to other dependencies between compositions. CAPE learns to identify this structure and propagates knowledge between them to learn class embedding for all seen and unseen compositions. In the challenging generalized compositional zero-shot setting, we show that our method outperforms previous baselines to set a new state-of-the-art on three publicly available benchmarks."
Learning Classifiers of Prototypes and Reciprocal Points for Universal Domain Adaptation,"Sungsu Hur, Inkyu Shin, Kwanyong Park, Sanghyun Woo, In So Kweon",KAIST,100,South Korea,0,,"Universal Domain Adaptation aims to transfer the knowledge between the datasets by handling two shifts: domain-shift and category-shift. The main challenge is correctly distinguishing the unknown target samples while adapting the distribution of known class knowledge from source to target. Most existing methods approach this problem by first training the target adapted known classifier and then relying on the single threshold to distinguish unknown target samples. However, this simple threshold-based approach prevents the model from considering the underlying complexities existing between the known and unknown samples in the high-dimensional feature space. In this paper, we propose a new approach in which we use two sets of feature points, namely dual Classifiers for Prototypes and Reciprocals (CPR). Our key idea is to associate each prototype with corresponding known class features while pushing the reciprocals apart from these prototypes to locate them in the potential unknown feature space. The target samples are then classified as unknown if they fall near any reciprocals at test time. To successfully train our framework, we collect the partial, confident target samples that are classified as known or unknown through on our proposed multi-criteria selection. We then additionally apply the entropy loss regularization to them. For further adaptation, we also apply standard consistency regularization that matches the predictions of two different views of the input to make more compact target feature space. We evaluate our proposal, CPR, on three standard benchmarks and achieve comparable or new state-of-the-art results. We also provide extensive ablation experiments to verify our main design choices in our framework.",https://openaccess.thecvf.com/content/WACV2023/html/Hur_Learning_Classifiers_of_Prototypes_and_Reciprocal_Points_for_Universal_Domain_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hur_Learning_Classifiers_of_Prototypes_and_Reciprocal_Points_for_Universal_Domain_WACV_2023_paper.pdf,,,2212.08355,main,Poster,https://ieeexplore.ieee.org/document/10030124/,"['Training', 'Adaptation models', 'Computer vision', 'Prototypes', 'Computer architecture', 'Benchmark testing', 'Minimization']","['Domain Adaptation', 'Universal Domain Adaptation', 'Feature Space', 'Target Sample', 'Domain Shift', 'Unknown Samples', 'Consistency Regularization', 'Unknown Space', 'Deep Neural Network', 'Open Space', 'Large-scale Datasets', 'Generative Adversarial Networks', 'Open Set', 'Target Domain', 'Nearest Point', 'Source Domain', 'Benchmark Set', 'Adaptation Phase', 'Strong Views', 'Pseudo Labels', 'Warm-up Phase', 'Label Space', 'Anomaly Score', 'Adaptation Stage']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",6,"Universal Domain Adaptation aims to transfer the knowledge between the datasets by handling two shifts: domain-shift and category-shift. The main challenge is correctly distinguishing the unknown target samples while adapting the distribution of known class knowledge from source to target. Most existing methods approach this problem by first training the target adapted known classifier and then relying on the single threshold to distinguish unknown target samples. However, this simple threshold-based approach prevents the model from considering the underlying complexities existing between the known and unknown samples in the high-dimensional feature space. In this paper, we propose a new approach in which we use two sets of feature points, namely dual Classifiers for Prototypes and Reciprocals (CPR). Our key idea is to associate each prototype with corresponding known class features while pushing the reciprocals apart from these prototypes to locate them in the potential unknown feature space. The target samples are then classified as unknown if they fall near any reciprocals at test time. To successfully train our framework, we collect the partial, confident target samples that are classified as known or unknown through on our proposed multi-criteria selection. We then additionally apply the entropy loss regularization to them. For further adaptation, we also apply standard consistency regularization that matches the predictions of two different views of the input to make more compact target feature space. We evaluate our proposal, CPR, on three standard benchmarks and achieve comparable or new state-of-the-art results. We also provide extensive ablation experiments to verify our main design choices in our framework."
Learning Few-Shot Segmentation From Bounding Box Annotations,"Byeolyi Han, Tae-Hyun Oh","Georgia Tech, Atlanta, Georgia, USA; Dept. of EE, POSTECH, Pohang, Korea",100,"South Korea, USA",0,,"We present a new weakly-supervised few-shot semantic segmentation setting and a meta-learning method for tackling the new challenge. Different from existing settings, we leverage bounding box annotations as weak supervision signals during the meta-training phase, i.e., more label-efficient. Bounding box provides a cheaper label representation than segmentation mask but contains both an object of interest and a disturbing background. We first show that meta-training with bounding boxes degrades recent few-shot semantic segmentation methods, which are typically meta-trained with full semantic segmentation supervision. We postulate that this challenge is originated from the impure information of bounding box representation. We propose a pseudo trimap estimator and trimap-attention based prototype learning to extract clearer supervision signals from bounding boxes. These developments robustify and generalize our method well to noisy support masks at test time. We empirically show that our method consistently improves performance. Our method gains 1.4% and 3.6% mean-IoU over the competing one in full and weak test supervision cases, respectively, in the 1-way 5-shot setting on Pascal-5i.",https://openaccess.thecvf.com/content/WACV2023/html/Han_Learning_Few-Shot_Segmentation_From_Bounding_Box_Annotations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Han_Learning_Few-Shot_Segmentation_From_Bounding_Box_Annotations_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030847/,"['Computer vision', 'Annotations', 'Semantic segmentation', 'Computational modeling', 'Semantics', 'Prototypes', 'Performance gain']","['Bounding Box', 'Bounding Box Annotations', 'Few-shot Segmentation', 'Semantic Segmentation', 'Object Of Interest', 'Weak Supervision', 'Supervision Signal', 'Neural Network', 'Feature Maps', 'Classifier Training', 'Segmentation Performance', 'Base Classes', 'Support Set', 'Query Set', 'Semantic Segmentation Task', 'Query Image', 'Few-shot Learning', 'Segmentation Labels', 'Query Sample', 'Weak Labels', 'Foreground Regions', 'Unseen Domains', 'Unseen Classes', 'Segmentation Annotations', 'Segmentation Prediction', 'Image-level Labels', 'Prototypical Network', 'Class Prototypes', 'Gray Regions']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"We present a new weakly-supervised few-shot semantic segmentation setting and a meta-learning method for tackling the new challenge. Different from existing settings, we leverage bounding box annotations as weak supervision signals during the meta-training phase, i.e., more label-efficient. Bounding box provides a cheaper label representation than segmentation mask but contains both an object of interest and a disturbing background. We first show that meta-training with bounding boxes degrades recent few-shot semantic segmentation methods, which are typically meta-trained with full semantic segmentation supervisions. We postulate that this challenge is originated from the impure information of bounding box representation. We propose a pseudo trimap estimator and trimap-attention based prototype learning to extract clearer supervision signals from bounding boxes. These developments robustify and generalize our method well to noisy support masks at test time. We empirically show that our method consistently improves performance. Our method gains 1.4% and 3.6% mean-IoU over the competing one in full and weak test supervision cases, respectively, in the 1-way 5-shot setting on Pascal-5
<sup>i</sup>
."
Learning Graph Variational Autoencoders With Constraints and Structured Priors for Conditional Indoor 3D Scene Generation,"Aditya Chattopadhyay, Xi Zhang, David Paul Wipf, Himanshu Arora, René Vidal","Amazon.com, Inc.; Johns Hopkins University, MD, USA",50,USA,50,USA,"We present a graph variational autoencoder with a structured prior for generating the layout of indoor 3D scenes. Given the room type (e.g., living room or library) and the room layout (e.g., room elements such as floor and walls), our architecture generates a collection of objects (e.g., furniture items such as sofa, table and chairs) that is consistent with the room type and layout. This is a challenging problem because the generated scene needs to satisfy multiple constrains, e.g., each object should lie inside the room and two objects should not occupy the same volume. To address these challenges, we propose a deep generative model that encodes these relationships as soft constraints on an attributed graph (e.g., the nodes capture attributes of room and furniture elements, such as shape, class, pose and size, and the edges capture geometric relationships such as relative orientation). The architecture consists of a graph encoder that maps the input graph to a structured latent space, and a graph decoder that generates a furniture graph, given a latent code and the room graph. The latent space is modeled with autoregressive priors, which facilitates the generation of highly structured scenes. We also propose an efficient training procedure that combines matching and constrained learning. Experiments on the 3D-FRONT dataset show that our method produces scenes that are diverse and are adapted to the room layout.",https://openaccess.thecvf.com/content/WACV2023/html/Chattopadhyay_Learning_Graph_Variational_Autoencoders_With_Constraints_and_Structured_Priors_for_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chattopadhyay_Learning_Graph_Variational_Autoencoders_With_Constraints_and_Structured_Priors_for_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030809/,"['Training', 'Adaptation models', 'Solid modeling', 'Three-dimensional displays', 'Shape', 'Databases', 'Layout']","['Variational Autoencoder', '3D Scene', 'Relative Orientation', 'Latent Space', 'Geometric Relationship', 'Graph Properties', 'Soft Constraints', 'Deep Generative Models', 'Latent Code', 'Room Type', 'Room Layout', 'Latent Variables', 'Probabilistic Model', 'Diagonal Matrix', 'Autoregressive Model', 'Shape Features', 'Kullback-Leibler', 'Nodes In The Graph', 'Ground Plane', 'Node Features', 'Graph Neural Networks', 'Scene Graph', 'Shape Descriptors', 'Evidence Lower Bound', 'Variational Autoencoder Model', 'Form Of Supervision', 'Types Of Nodes', 'Preferred Characteristics', 'Types Of Edges', 'Edge Features']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Computational photography', 'image and video synthesis']",6,"We present a graph variational autoencoder with a structured prior for generating the layout of indoor 3D scenes. Given the room type (e.g., living room or library) and the room layout (e.g., room elements such as floor and walls), our architecture generates a collection of objects (e.g., furniture items such as sofa, table and chairs) that is consistent with the room type and layout. This is a challenging problem because the generated scene needs to satisfy multiple constrains, e.g., each object should lie inside the room and two objects should not occupy the same volume. To address these challenges, we propose a deep generative model that encodes these relationships as soft constraints on an attributed graph (e.g., the nodes capture attributes of room and furniture elements, such as shape, class, pose and size, and the edges capture geometric relationships such as relative orientation). The architecture consists of a graph encoder that maps the input graph to a structured latent space, and a graph decoder that generates a furniture graph, given a latent code and the room graph. The latent space is modeled with autoregressive priors, which facilitates the generation of highly structured scenes. We also propose an efficient training procedure that combines matching and constrained learning. Experiments on the 3D-FRONT dataset show that our method produces scenes that are diverse and are adapted to the room layout."
Learning How to MIMIC: Using Model Explanations To Guide Deep Learning Training,"Matthew Watson, Bashar Awwad Shiekh Hasan, Noura Al Moubayed","Durham University, Durham, UK",100,UK,0,,"Healthcare is seen as one of the most influential applications of Deep Learning (DL). Increasingly, DL models are applied in healthcare settings with seemingly high levels of performance on-par with medical experts. Yet, very few are deployed into real-life scenarios with variable success rate. One of the main reasons for this is the lack of trust in those models by medical professionals driven by the black-box nature of the deployed models. Numerous explainable techniques have been developed to alleviate this issue by providing a view on how the model reached a given decision. Recent studies have shown that those explanations can expose the models' reliance on areas of the feature space that has no justifiable medical interpretation, widening the gap with the medical experts. In this paper we evaluate the deviation of saliency maps produced by DL classification models from radiologist's eye-gaze while they study the MIMIC-CXR-EGD images, and we propose a novel model architecture that utilises model explanations during training only (i.e. not during inference) to improve the overall plausibility of the model explanations. We substantially improve the similarity between the model's explanations and radiologists' eye-gaze data, reducing Kullback-Leibler Divergence by 90% and increasing Normalised Scanpath Saliency by 216%. We argue that this significant improvement is an important step towards building more robust and interpretable DL solutions in healthcare.",https://openaccess.thecvf.com/content/WACV2023/html/Watson_Learning_How_to_MIMIC_Using_Model_Explanations_To_Guide_Deep_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Watson_Learning_How_to_MIMIC_Using_Model_Explanations_To_Guide_Deep_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030326/,"['Training', 'Deep learning', 'Measurement', 'Heating systems', 'Analytical models', 'Visualization', 'MIMICs']","['Deep Learning', 'Explanatory Model', 'Deep Learning Models', 'Kullback-Leibler', 'Eye Contact', 'Model Architecture', 'Medical Experts', 'Application Of Deep Learning', 'Saliency Map', 'Healthcare Solutions', 'Deep Learning Model For Classification', 'Loss Function', 'Learning Rate', 'Convolutional Layers', 'Chest X-ray', 'Congestive Heart Failure', 'Adam Optimizer', 'Feature Learning', 'Joint Effect', 'Set Of Explanations', 'Chest X-ray Images', 'Different Sets Of Features', 'Ensemble Model', 'U-Net Architecture', 'Intersection Over Union', 'Discriminator Loss', 'Final Convolutional Layer']","['Applications: Biomedical/healthcare/medicine', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"Healthcare is seen as one of the most influential applications of Deep Learning (DL). Increasingly, DL models have been shown to achieve high-levels of performance on medical diagnosis tasks, in some cases achieving levels of performance on-par with medical experts. Yet, very few are deployed into real-life scenarios. One of the main reasons for this is the lack of trust in those models by medical professionals driven by the black-box nature of the deployed models. Numerous explainability techniques have been developed to alleviate this issue by providing a view on how the model reached a given decision. Recent studies have shown that those explanations can expose the models’ reliance on areas of the feature space that has no justifiable medical interpretation, widening the gap with the medical experts. In this paper we evaluate the deviation of saliency maps produced by DL classification models from radiologist’s eye-gaze while they study the MIMIC-CXR-EGD images, and we propose a novel model architecture that utilises model explanations during training only (i.e. not during inference) to improve the overall plausibility of the model explanations. We substantially improve the similarity between the model’s explanations and radiologists’ eye-gaze data, reducing Kullback-Leibler Divergence by 90% and increasing Normalised Scanpath Saliency by 216%. We argue that this significant improvement is an important step towards building more robust and interpretable DL solutions in health-care."
Learning Incoherent Light Emission Steering From Metasurfaces Using Generative Models,"Prasad P. Iyer, Saaketh Desai, Sadhvikas Addamane, Remi Dingreville, Igal Brener","Center for Intergrated Nanotechnologies, Sandia National Laboratories",100,USA,0,,"Spatiotemporal control over incoherent light sources is critically important for applications such as displays, remote sensing, clean energy, and illumination. Incoherent light emission made up of randomized wavefronts is incompatible with known beam steering techniques that rely on coherent electromagnetic wave interference. The emerging field of tunable dielectric metasurfaces consisting of sub- wavelength arrays of optical nanoresonators has recently enabled active re-direction of incoherent light (photoluminescence, PL) emission. This was achieved by illuminating (pumping) the metasurface with a pump laser reflecting off a programmable spatial light modulator (SLM) with sawtooth grating patterns as input. Achieving efficient beam steering requires the generation of optimal pump patterns programmed into the SLM to maximize the PL emitted towards a given direction. Given the innumerable possibilities and the lack of a theoretical physical framework to guide the exploration of pump patterns, we use an active learning algorithm running a closed loop optical experiment with a generative model to explore and optimize novel pump patterns. We achieve up to an order of magnitude enhancement in the steering efficiency by using pump patterns that are generated by a variational auto-encoder, with minimal number of experiments. The results presented in this paper highlight the unique ability of generative models and active learning to dramatically improve steering efficiency by finding novel optical pump patterns that are beyond human intuition. Our combination of advanced machine learning techniques driving closed loop nanophotonic experiments might pave the way to derive the underlying physics of emergent light-matter phenomena.",https://openaccess.thecvf.com/content/WACV2023/html/Iyer_Learning_Incoherent_Light_Emission_Steering_From_Metasurfaces_Using_Generative_Models_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Iyer_Learning_Incoherent_Light_Emission_Steering_From_Metasurfaces_Using_Generative_Models_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030475/,"['Beam steering', 'Stimulated emission', 'Pumps', 'Metasurfaces', 'Optical pumping', 'Spatiotemporal phenomena', 'Optical sensors']","['Light-emitting Diodes', 'Incoherent Light', 'Incoherent Emission', 'Theoretical Framework', 'Machine Learning', 'Active Learning', 'Photoluminescence', 'Grating', 'Variational Autoencoder', 'Spatial Light Modulator', 'Optical Pumping', 'Beam Steering', 'Human Intuition', 'Optical Patterns', 'Incoherent Source', 'Training Set', 'Field Of View', 'Dimensional Space', 'Latent Space', 'Exhaustive Search', 'Latent Dimensions', 'Active Learning Strategies', 'Output Intensity', 'Active Learning Process', 'Subject Matter Experts', 'Figure Of Merit', 'Acquisition Function', 'Image X', 'Input Profile', 'Machine Learning Framework']","['Applications: Embedded sensing/real-time techniques', 'Remote Sensing']",3,"Spatiotemporal control over incoherent light sources is critically important for applications such as displays, remote sensing, clean energy, and illumination. Incoherent light emission made up of randomized wavefronts is incompatible with known beam steering techniques that rely on coherent electromagnetic wave interference. The emerging field of tunable dielectric metasurfaces consisting of sub-wavelength arrays of optical nanoresonators has recently enabled active re-direction of incoherent light (photoluminescence, PL) emission. This was achieved by illuminating (pumping) the metasurface with a pump laser reflecting off a programmable spatial light modulator (SLM) with sawtooth grating patterns as input. Achieving efficient beam steering requires the generation of optimal pump patterns programmed into the SLM to maximize the PL emitted towards a given direction. Given the innumerable possibilities and the lack of a theoretical physical framework to guide the exploration of pump patterns, we use an active learning algorithm running a closed loop optical experiment with a generative model to explore and optimize novel pump patterns. We achieve up to an order of magnitude enhancement in the steering efficiency by using pump patterns that are generated by a variational auto-encoder, with minimal number of experiments. The results presented in this paper highlight the unique ability of generative models and active learning to dramatically improve steering efficiency by finding novel optical pump patterns that are beyond human intuition. Our combination of advanced machine learning techniques driving closed loop nanophotonic experiments might pave the way to derive the underlying physics of emergent light-matter phenomena."
Learning Latent Structural Relations With Message Passing Prior,"Shaogang Ren, Hongliang Fei, Dingcheng Li, Ping Li","Cognitive Computing Lab, Baidu Research, 10900 NE 8th St. Bellevue, WA 98004, USA",0,,100,China,"Learning disentangled representations is an important topic in machine learning with a wide range of applications. Disentangled latent variables represent interpretable semantic information and reflect separate factors of variation in data. Although generative models can learn latent representations as well, most existing models ignore the structural information among latent variables. In this paper, we propose a novel approach to learn the disentangled latent structural representations from data using decomposable variational auto-encoders. We design a novel message passing prior to the latent representations to capture the interactions among different data components. Different from many previous methods that ignore data component or object interaction, our approach simultaneously learns component representation and encodes component relationships. We have applied our model to tasks of data segmentation and latent representation learning among different data components. Experiments on several benchmarks demonstrate the utility of the proposed method.",https://openaccess.thecvf.com/content/WACV2023/html/Ren_Learning_Latent_Structural_Relations_With_Message_Passing_Prior_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ren_Learning_Latent_Structural_Relations_With_Message_Passing_Prior_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030305/,"['Representation learning', 'Computer vision', 'Message passing', 'Semantics', 'Benchmark testing', 'Data models', 'Task analysis']","['Latent Structure', 'Message Passing', 'Latent Variables', 'Representation Learning', 'Latent Representation', 'Variational Autoencoder', 'Object Interaction', 'Disentangled Representation', 'Diamond', 'Invertible', 'Unsupervised Learning', 'Latent Factors', 'Generative Adversarial Networks', 'Independent Component Analysis', 'Latent Space', 'Multiple Objects', 'Attention Network', 'Total Correlation', 'Objects In The Scene', 'Evidence Lower Bound', 'Bi-level Model', 'Scene Segmentation', 'Latent Vector', 'Adjusted Rand Index', 'Global Section', 'Representative Components', 'Sufficient Statistics', 'Object Relations']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Learning disentangled representations is an important topic in machine learning with a wide range of applications. Disentangled latent variables represent interpretable semantic information and reflect separate factors of variation in data. Although generative models can learn latent representations as well, most existing models ignore the structural information among latent variables. In this paper, we propose a novel approach to learn the disentangled latent structural representations from data using decomposable variational auto-encoders. We design a novel message passing prior for the latent representations to capture the interactions among different data components. Different from many previous methods that ignore data component or object interaction, our approach simultaneously learns component representation and encodes component relationships. We have applied our model to tasks of data segmentation and latent representation learning among different data components. Experiments on several benchmarks demonstrate the utility of the proposed method."
Learning Lightweight Neural Networks via Channel-Split Recurrent Convolution,"Guojun Wu, Xin Zhang, Ziming Zhang, Yanhua Li, Xun Zhou, Christopher Brinton, Zhenming Liu",Purdue University; University of Iowa; College of William & Mary; Worcester Polytechnic Institute,100,USA,0,,"Lightweight neural networks refer to deep networks with small numbers of parameters, which are allowed to be implemented in resource-limited hardware such as embedded systems. To learn such lightweight networks effectively and efficiently, in this paper we propose a novel convolutional layer, namely Channel-Split Recurrent Convolution (CSR-Conv), where we split the output channels to generate data sequences with length T as the input to the recurrent layers with shared weights. As a consequence, we can construct lightweight convolutional networks by simply replacing (some) linear convolutional layers with CSR-Conv layers. We prove that under mild conditions the model size decreases with the rate of O(1 / T^2). Empirically we demonstrate the state-of-the-art performance using VGG-16, ResNet-50, ResNet-56, ResNet-110, DenseNet-40, MobileNet, and EfficientNet as backbone networks on CIFAR-10 and ImageNet. Codes can be found on https://github.com/tuaxon/CSR_Conv.",https://openaccess.thecvf.com/content/WACV2023/html/Wu_Learning_Lightweight_Neural_Networks_via_Channel-Split_Recurrent_Convolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wu_Learning_Lightweight_Neural_Networks_via_Channel-Split_Recurrent_Convolution_WACV_2023_paper.pdf,,https://github.com/tuaxon/CSR-Conv,,main,Poster,https://ieeexplore.ieee.org/document/10030351/,"['Convolutional codes', 'Training', 'Computer vision', 'Image coding', 'Embedded systems', 'Convolution', 'Neural networks']","['Neural Network', 'Lightweight Neural Network', 'Recurrent Convolution', 'Convolutional Layers', 'Model Size', 'ImageNet', 'Backbone Network', 'Output Channels', 'Lightweight Network', 'Recurrent Layers', 'Linear Convolution', 'Convolutional Neural Network', 'Transition State', 'Sequence Length', 'Fast Fourier Transform', 'Network Layer', 'Long Short-term Memory', 'Recurrent Neural Network', 'Grid Search', 'Compression Rate', 'Gated Recurrent Unit', 'Network Compression', 'Input Channels', 'Vision Transformer', 'Convolutional Recurrent Neural Network', 'Deeper Network', 'Hidden State', 'Group Convolution', 'Compression Algorithm']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",,"Lightweight neural networks refer to deep networks with small numbers of parameters, which can be deployed in resource-limited hardware such as embedded systems. To learn such lightweight networks effectively and efficiently, in this paper we propose a novel convolutional layer, namely Channel-Split Recurrent Convolution (CSR-Conv), where we split the output channels to generate data sequences with length T as the input to the recurrent layers with shared weights. As a consequence, we can construct lightweight convolutional networks by simply replacing (some) linear convolutional layers with CSR-Conv layers. We prove that under mild conditions the model size decreases with the rate of $O\left( {\frac{1}{{{T^2}}}} \right)$. Empirically we demonstrate the state-of-the-art performance using VGG-16, ResNet-50, ResNet-56, ResNet-110, DenseNet-40, MobileNet, and EfficientNet as backbone networks on CIFAR-10 and ImageNet. Codes can be found on https://github.com/tuaxon/CSR_Conv."
Learning Style Subspaces for Controllable Unpaired Domain Translation,"Gaurav Bhatt, Vineeth N. Balasubramanian","Indian Institute of Technology Hyderabad, Hyderabad, India; University of British Columbia, Vancouver, Canada",100,"Canada, India",0,,"The unpaired domain-to-domain translation aims to learn inter-domain relationships between diverse modalities without relying on paired data, which can help complex structure prediction tasks such as age transformation, where it is challenging to attain paired samples. A common approach used by most current methods is to factorize the data into a domain-invariant content space and a domain-specific style space. In this work, we argue that the style space can be further decomposed into smaller subspaces. Learning these style subspaces has two-fold advantages: (i) it allows more robustness and reliability in the generation of images in unpaired domain translation; and (ii) it allows better control and thereby interpolating the latent space, which can be helpful in complex translation tasks involving multiple domains. To achieve this decomposition, we propose a novel scalable approach to partition the latent space into style subspaces. We also propose a new evaluation metric that quantifies the controllable generation capability of domain translation methods. We compare our proposed method with several strong baselines on standard domain translation tasks such as gender translation (male-to-female and female-to-male), age transformation, reference-guided image synthesis, multi-domain image translation, and multi-attribute domain translation on celebA-HQ and AFHQ datasets. The proposed technique achieves state-of-the-art performance on various domain translation tasks while outperforming all the baselines on controllable generation tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Bhatt_Learning_Style_Subspaces_for_Controllable_Unpaired_Domain_Translation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bhatt_Learning_Style_Subspaces_for_Controllable_Unpaired_Domain_Translation_WACV_2023_paper.pdf,,https://github.com/GauravBh1010tt/Controllable-Domain-Translation,,main,Poster,https://ieeexplore.ieee.org/document/10030384/,"['Measurement', 'Interpolation', 'Computer vision', 'Image synthesis', 'Computational modeling', 'Computer architecture', 'Aerospace electronics']","['Latent Space', 'Image Synthesis', 'Translation Task', 'Control Method', 'Multiple Dimensions', 'Generative Adversarial Networks', 'General Sample', 'Reference Image', 'Source Images', 'Target Domain', 'Inference Time', 'Loss Term', 'Target Distribution', 'Hair Color', 'Source Domain', 'Variety Of Styles', 'Consistency Loss', 'Black Hair', 'Network Partitioning', 'Blond Hair', 'Cycle Consistency']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",1,"The unpaired domain-to-domain translation aims to learn inter-domain relationships between diverse modalities without relying on paired data, which can help complex structure prediction tasks such as age transformation where it is challenging to attain paired samples. A common approach used by most current methods is to factorize the data into a domain-invariant content space and a domain-specific style space. In this work, we argue that the style space can be further decomposed into smaller subspaces. Learning these style subspaces has two-fold advantages: (i) it allows more robustness and reliability in the generation of images in unpaired domain translation; and (ii) it allows better control and thereby interpolation of the latent space, which can be helpful in complex translation tasks involving multiple domains. To achieve this decomposition, we propose a novel scalable approach to partition the latent space into style subspaces. We also propose a new evaluation metric that quantifies the controllable generation capability of domain translation methods. We compare our proposed method with several strong baselines on standard domain translation tasks such as gender translation (male-to-female and female-to-male), age transformation, reference-guided image synthesis, multi-domain image translation and multi-attribute domain translation on celebA-HQ and AFHQ datasets. The proposed technique achieves state-of-the-art performance on various domain translation tasks while outperforming all the baselines on controllable generation tasks. Code - https://github.com/GauravBh1010tt/Controllable-Domain-Translation"
Learning To Detect 3D Lanes by Shape Matching and Embedding,"Ruixin Liu, Zhihao Guan, Zejian Yuan, Ao Liu, Tong Zhou, Tang Kun, Erlong Li, Chao Zheng, Shuqi Mei","Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, China; T Lab, Tencent Map, Tencent, China",50,China,50,China,"3D lane detection based on LiDAR point clouds is a challenging task that requires precise locations, accurate topologies, and distinguishable instances. In this paper, we propose a dual-level shape attention network (DSANet) with two branches for high-precision 3D lane predictions. Specifically, one branch predicts the refined lane segment shapes and the shape embeddings that encode the approximate lane instance shapes, the other branch detects the coarse-grained structures of the lane instances. In the training stage, two-level shape matching loss functions are introduced to jointly optimize the shape parameters of the two-branch outputs, which are simple yet effective for precision enhancement. Furthermore, a shape-guided segments aggregator is proposed to help local lane segments aggregate into complete lane instances, according to the differences of instance shapes predicted at different levels. Experiments conducted on our BEV-3DLanes dataset demonstrate that our method outperforms previous methods.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_Learning_To_Detect_3D_Lanes_by_Shape_Matching_and_Embedding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_Learning_To_Detect_3D_Lanes_by_Shape_Matching_and_Embedding_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030447/,"['Training', 'Point cloud compression', 'Three-dimensional displays', 'Laser radar', 'Shape', 'Lane detection', 'Network topology']","['Point Cloud', 'Joint Optimization', 'Local Segments', 'LiDAR Point Clouds', 'Shape Of Segment', 'Matching Loss', 'Positive Samples', 'F1 Score', 'Feature Maps', '3D Space', 'Semantic Segmentation', 'Frontal View', 'Bird’s Eye', 'Shape Representation', 'Global Shape', 'Sparse Point', 'Straight Segments', 'Polyline', 'Self-attention Module', 'Grid-based Method', 'Segmentation-based Methods', 'Higher F1 Score', 'Bipartite Matching', 'Slender Shape', 'Local Shape', 'Kullback-Leibler', 'Feed-forward Network', 'Cluster Centers']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Robotics']",1,"3D lane detection based on LiDAR point clouds is a challenging task that requires precise locations, accurate topologies, and distinguishable instances. In this paper, we propose a dual-level shape attention network (DSANet) with two branches for high-precision 3D lane predictions. Specifically, one branch predicts the refined lane segment shapes and the shape embeddings that encode the approximate lane instance shapes, the other branch detects the coarse-grained structures of the lane instances. In the training stage, two-level shape matching loss functions are introduced to jointly optimize the shape parameters of the twobranch outputs, which are simple yet effective for precision enhancement. Furthermore, a shape-guided segments aggregator is proposed to help local lane segments aggregate into complete lane instances, according to the differences of instance shapes predicted at different levels. Experiments conducted on our BEV-3DLanes dataset demonstrate that our method outperforms previous methods."
Learning by Hallucinating: Vision-Language Pre-Training With Weak Supervision,"Tzu-Jui Julius Wang, Jorma Laaksonen, Tomas Langer, Heikki Arponen, Tom E. Bishop","Aalto University, Finland; Intuition Machines Inc.; Systematic Alpha*; Glass Imaging*",50,"Finland, USA",50,USA,"Weakly-supervised vision-language (V-L) pre-training (W-VLP) aims at learning cross-modal alignment with little or no paired data, such as aligned images and captions. Recent W-VLP methods, which pair visual features with object tags, help achieve performances comparable with some VLP models trained with aligned pairs in various V-L downstream tasks. This, however, is not the case in cross- modal retrieval (XMR). We argue that the learning of such a W-VLP model is curbed and biased by the object tags of limited semantics. We address the lack of paired V-L data for model supervision with a novel Visual Vocabulary based Feature Hallucinator (WFH), which is trained via weak supervision as a W-VLP model, not requiring images paired with captions. WFH generates visual hallucinations from texts, which are then paired with the originally unpaired texts, allowing more diverse interactions across modalities. Empirically, WFH consistently boosts the prior W-VLP works, e.g. U-VisualBERT (U-VB), over a variety of V-L tasks, i.e. XMR, Visual Question Answering, etc. Notably, benchmarked with recall@ 1,5,10 , it consistently improves U-VB on image-to-text and text-to-image retrieval on two popular datasets Flickr30K and MSCOCO. Meanwhile, it gains by at least 14.5% in cross-dataset generalization tests on these XMR tasks. Moreover, in other V-L downstream tasks considered, our WFH models are on par with models trained with paired V-L data, revealing the utility of unpaired data. These results demonstrate greater generalization of the proposed W-VLP model with WFH.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_Learning_by_Hallucinating_Vision-Language_Pre-Training_With_Weak_Supervision_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Learning_by_Hallucinating_Vision-Language_Pre-Training_With_Weak_Supervision_WACV_2023_paper.pdf,,,2210.13591,main,Poster,https://ieeexplore.ieee.org/document/10030360/,"['Visualization', 'Vocabulary', 'Computer vision', 'Computational modeling', 'Detectors', 'Benchmark testing', 'Transformers']","['Hallucinations', 'Weak Supervision', 'Vision-language Pre-training', 'Paired Data', 'Visual Hallucinations', 'Visual Question Answering', 'Visual Representation', 'Object Detection', 'Weight Decay', 'Generative Adversarial Networks', 'Representative Regions', 'Training Objective', 'Recall Values', 'Linear Layer', 'Tokenized', 'Massive Amounts Of Data', 'Masked Images', 'Text Representation', 'Linear Projection', 'Early Layers', 'Masked Language Model', 'Transformer Layers', 'Token Embedding', 'Word Tokens', 'Attention Heads', 'Sequence Of Tokens', 'Medical Imaging', 'Image Regions', 'Attention Mechanism']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"Weakly-supervised vision-language (V-L) pre-training (W-VLP) aims at learning cross-modal alignment with little or no paired data, such as aligned images and captions. Recent W-VLP methods, which pair visual features with object tags, help achieve performances comparable with some VLP models trained with aligned pairs in various V-L downstream tasks. This, however, is not the case in cross-modal retrieval (XMR). We argue that the learning of such a W-VLP model is curbed and biased by the object tags of limited semantics.We address the lack of paired V-L data for model supervision with a novel Visual Vocabulary based Feature Hallucinator (WFH), which is trained via weak supervision as a W-VLP model, not requiring images paired with captions. WFH generates visual hallucinations from texts, which are then paired with the originally unpaired texts, allowing more diverse interactions across modalities.Empirically, WFH consistently boosts the prior W-VLP works, e.g. U-VisualBERT (U-VB), over a variety of V-L tasks, i.e. XMR, Visual Question Answering, etc. Notably, benchmarked with recall@{1,5,10}, it consistently U-VB on image-to-text and improves text-to-image retrieval on two popular datasets Flickr30K and MSCOCO. Meanwhile, it gains by at least 14.5% in cross-dataset generalization tests on these XMR tasks. Moreover, in other V-L downstream tasks considered, our WFH models are on par with models trained with paired V-L data, revealing the utility of unpaired data. These results demonstrate greater generalization of the proposed W-VLP model with WFH."
Leveraging Local Patch Differences in Multi-Object Scenes for Generative Adversarial Attacks,"Abhishek Aich, Shasha Li, Chengyu Song, M. Salman Asif, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury","University of California, Riverside, USA",100,USA,0,,"State-of-the-art generative model-based attacks against image classifiers overwhelmingly focus on single-object (ie., single dominant object) images. Different from such settings, we tackle a more practical problem of generating adversarial perturbations using multi-object (ie., multiple dominant objects) images as they are representative of most real-world scenes. Our goal is to design an attack strategy that can learn from such natural scenes by leveraging the local patch differences that occur inherently in such images (eg. difference between the local patch on the object 'person' and the object 'bike' in a traffic scene). Our key idea is to misclassify an adversarial multi-object image by confusing the victim classifier for each local patch in the image. Based on this, we propose a novel generative attack (called Local Patch Difference or LPD-Attack) where a novel contrastive loss function uses the aforesaid local differences in feature space of multi-object scenes to optimize the perturbation generator. Through various experiments across diverse victim convolutional neural networks, we show that our approach outperforms baseline generative attacks with highly transferable perturbations when evaluated under different white-box and black-box settings.",https://openaccess.thecvf.com/content/WACV2023/html/Aich_Leveraging_Local_Patch_Differences_in_Multi-Object_Scenes_for_Generative_Adversarial_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Aich_Leveraging_Local_Patch_Differences_in_Multi-Object_Scenes_for_Generative_Adversarial_WACV_2023_paper.pdf,,,2209.09883,main,Poster,https://ieeexplore.ieee.org/document/10030314/,"['Computer vision', 'Perturbation methods', 'Computational modeling', 'Closed box', 'Generators', 'Convolutional neural networks', 'Glass box']","['Adversarial Attacks', 'Multi-object Scenes', 'Loss Function', 'Multiple Objects', 'Single Object', 'Image Patches', 'Contrastive Loss', 'Adversarial Perturbations', 'Local Level', 'Alternative Models', 'Deep Neural Network', 'Image Features', 'Feature Maps', 'Clear Image', 'Final Objective', 'Individual Objects', 'Self-supervised Learning', 'Mean Square Error Loss', 'Image X', 'Adversarial Examples', 'Patch Features', 'PASCAL VOC Dataset', 'Set Of Attacks', 'Real-world Use Cases', 'Attack Scenarios', 'Feature Maps Of Images', 'White-box Attack', 'Positive Patch', 'Data Distribution', 'Training Distribution']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods']",5,"State-of-the-art generative model-based attacks against image classifiers overwhelmingly focus on single-object(i.e., single dominant object) images. Different from such settings, we tackle a more practical problem of generating adversarial perturbations using multi-object (i.e., multiple dominant objects) images as they are representative of most real-world scenes. Our goal is to design an attack strategy that can learn from such natural scenes by leveraging the local patch differences that occur inherently in such images (e.g. difference between the local patch on the object ‘person’ and the object ‘bike’ in a traffic scene). Our key idea is to misclassify an adversarial multi-object image by confusing the victim classifier for each local patch in the image. Based on this, we propose a novel generative attack (called Local Patch Difference or LPD-Attack) where a novel contrastive loss function uses the aforesaid local differences in feature space of multi-object scenes to optimize the perturbation generator. Through various experiments across diverse victim convolutional neural networks, we show that our approach outperforms baseline generative attacks with highly transferable perturbations when evaluated under different white-box and black-box settings."
Leveraging Off-the-Shelf Diffusion Model for Multi-Attribute Fashion Image Manipulation,"Chaerin Kong, DongHyeon Jeon, Ohjoon Kwon, Nojun Kwak",Seoul National University; NAVER,50,South Korea,50,South Korea,"Fashion attribute editing is a task that aims to convert the semantic attributes of a given fashion image while preserving the irrelevant regions. Previous works typically employ conditional GANs where the generator explicitly learns the target attributes and directly execute the conversion. These approaches, however, are neither scalable nor generic as they operate only with few limited attributes and a separate generator is required for each dataset or attribute set. Inspired by the recent advancement of diffusion models, we explore the classifier-guided diffusion that leverages the off-the-shelf diffusion model pretrained on general visual semantics such as Imagenet. In order to achieve a generic editing pipeline, we pose this as multi-attribute image manipulation task, where the attribute ranges from item category, fabric, pattern to collar and neckline. We empirically show that conventional methods fail in our challenging setting, and study efficient adaptation scheme that involves recently introduced attention-pooling technique to obtain a multi-attribute classifier guidance. Based on this, we present a mask-free fashion attribute editing framework that leverages the classifier logits and the cross-attention map for manipulation. We empirically demonstrate that our framework achieves convincing sample quality and attribute alignments.",https://openaccess.thecvf.com/content/WACV2023/html/Kong_Leveraging_Off-the-Shelf_Diffusion_Model_for_Multi-Attribute_Fashion_Image_Manipulation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kong_Leveraging_Off-the-Shelf_Diffusion_Model_for_Multi-Attribute_Fashion_Image_Manipulation_WACV_2023_paper.pdf,,,2210.05872,main,Poster,https://ieeexplore.ieee.org/document/10030169/,"['Visualization', 'Computer vision', 'Scalability', 'Semantics', 'Pipelines', 'Diffusion processes', 'Generators']","['Use Of Imaging', 'Diffusion Model', 'Fashion Images', 'ImageNet', 'Generative Adversarial Networks', 'Item Category', 'Neural Network', 'Fine-tuned', 'Gaussian Noise', 'Diffusion Process', 'Data Augmentation', 'Latent Space', 'Relevant Regions', 'Variational Autoencoder', 'Spatial Regions', 'Image Synthesis', 'Attention Map', 'Masked Images', 'Diverse Tasks', 'Class Activation Maps', 'Edit Operations', 'Image Editing', 'Guidance Signaling', 'Rich Space', 'Categorical Attributes']","['Algorithms: Computational photography', 'image and video synthesis']",5,"Fashion attribute editing is a task that aims to convert the semantic attributes of a given fashion image while preserving the irrelevant regions. Previous works typically employ conditional GANs where the generator explicitly learns the target attributes and directly execute the conversion. These approaches, however, are neither scalable nor generic as they operate only with few limited attributes and a separate generator is required for each dataset or attribute set. Inspired by the recent advancement of diffusion models, we explore the classifier-guided diffusion that leverages the off-the-shelf diffusion model pretrained on general visual semantics such as Imagenet. In order to achieve a generic editing pipeline, we pose this as multi-attribute image manipulation task, where the attribute ranges from item category, fabric, pattern to collar and neckline. We empirically show that conventional methods fail in our challenging setting, and study efficient adaptation scheme that involves recently introduced attention-pooling technique to obtain a multi-attribute classifier guidance. Based on this, we present a mask-free fashion attribute editing framework that leverages the classifier logits and the cross-attention map for manipulation. We empirically demonstrate that our framework achieves convincing sample quality and attribute alignments."
Li3DeTr: A LiDAR Based 3D Detection Transformer,"Gopi Krishna Erabati, Helder Araujo","Institute of Systems and Robotics, University of Coimbra, Portugal",100,Portugal,0,,"Inspired by recent advances in vision transformers for object detection, we propose Li3DeTr, an end-to-end LiDAR based 3D Detection Transformer for autonomous driving, that inputs LiDAR point clouds and regresses 3D bounding boxes. The LiDAR local and global features are encoded using sparse convolution and multi-scale deformable attention respectively. In the decoder head, firstly, in the novel Li3DeTr cross-attention block, we link the LiDAR global features to 3D predictions leveraging the sparse set of object queries learnt from the data. Secondly, the object query interactions are formulated using multi-head self-attention. Finally, the decoder layer is repeated Ldec number of times to refine the object queries. Inspired by DETR, we employ set-to-set loss to train the Li3DeTr network. Without bells and whistles, the Li3DeTr network achieves 61.3% mAP and 67.6% NDS surpassing the state-of-the-art methods with non-maximum suppression (NMS) on the nuScenes dataset and it also achieves competitive performance on the KITTI dataset. We also employ knowledge distillation (KD) using a teacher and student model that slightly improves the performance of our network.",https://openaccess.thecvf.com/content/WACV2023/html/Erabati_Li3DeTr_A_LiDAR_Based_3D_Detection_Transformer_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Erabati_Li3DeTr_A_LiDAR_Based_3D_Detection_Transformer_WACV_2023_paper.pdf,,,2210.15365,main,Poster,https://ieeexplore.ieee.org/document/10030563/,"['Point cloud compression', 'Knowledge engineering', 'Computer vision', 'Laser radar', 'Three-dimensional displays', 'Convolution', 'Object detection']","['3D Detection', 'Local Features', 'Object Detection', 'Global Features', 'Teacher Model', 'Point Cloud', 'Bounding Box', 'Whistle', 'Student Model', 'Decoder Layer', 'Non-maximum Suppression', 'Sparse Set', 'KITTI Dataset', 'LiDAR Point Clouds', '3D Prediction', 'Multi-head Self-attention', 'Vision Transformer', '3D Bounding Box', 'Feature Maps', 'Pedestrian', '3D Object Detection', 'Decoder Block', 'Encoder Module', 'Transformer Architecture', 'Decoder Module', 'Point-based Methods', 'Attention Mechanism', 'Post-processing Methods', 'Grid-based Method', 'Multi-scale Feature Maps']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', '3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",14,"Inspired by recent advances in vision transformers for object detection, we propose Li3DeTr, an end-to-end LiDAR based 3D Detection Transformer for autonomous driving, that inputs LiDAR point clouds and regresses 3D bounding boxes. The LiDAR local and global features are encoded using sparse convolution and multi-scale deformable attention respectively. In the decoder head, firstly, in the novel Li3DeTr cross-attention block, we link the LiDAR global features to 3D predictions leveraging the sparse set of object queries learnt from the data. Secondly, the object query interactions are formulated using multi-head self-attention. Finally, the decoder layer is repeated L
<inf>dec</inf>
 number of times to refine the object queries. Inspired by DETR, we employ set-to-set loss to train the Li3DeTr network. Without bells and whistles, the Li3DeTr network achieves 61.3% mAP and 67.6% NDS surpassing the state-of-the-art methods with non-maximum suppression (NMS) on the nuScenes dataset and it also achieves competitive performance on the KITTI dataset. We also employ knowledge distillation (KD) using a teacher and student model that slightly improves the performance of our network."
Lightweight Network for Video Motion Magnification,"Jasdeep Singh, Subrahmanyam Murala, G. Sankara Raju Kosuru","Computer Vision and Pattern Recognition Lab, Indian Institute of Technology Ropar, India",100,India,0,,"Video motion magnification provides information to understand the subtle changes present in objects for applications like industrial, healthcare, sports, etc. Most state-ofthe-art (SOTA) methods use hand-crafted bandpass filters, which require prior information for the motion magnification, produces ringing artifacts, and small magnification in dynamic scenarios etc. While others use deep-learning based techniques, but their output suffers from artificially induced motion, distortions, blurriness, etc. Further, SOTA methods are computationally complex, which makes them less suitable for real-time applications. To address these problems, we proposed deep learning based simple yet effective solution for motion magnification. The proposed method uses a feature sharing and appearance encoder for better motion magnification with less distortions, artifacts etc. Additionally, for reducing magnification of noise and other unwanted changes, proxy-model based training is proposed. A computationally lightweight model (  0.12 M parameters) is proposed along with the base model. The performance of the proposed models is tested qualitatively and quantitatively, with the SOTA methods. Results demonstrate the effectiveness of the proposed lightweight and base model over the existing SOTA methods.",https://openaccess.thecvf.com/content/WACV2023/html/Singh_Lightweight_Network_for_Video_Motion_Magnification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Singh_Lightweight_Network_for_Video_Motion_Magnification_WACV_2023_paper.pdf,,https://github.com/jasdeep-singh-007/LightweightNetworkForVideoMotionMagnification,,main,Poster,https://ieeexplore.ieee.org/document/10030470/,"['Deep learning', 'Training', 'Computational modeling', 'Lighting', 'Medical services', 'Distortion', 'Manipulators']","['Motion Video', 'Motion Magnification', 'Video Motion Magnification', 'Deep Learning', 'State Of The Art', 'Subtle Changes', 'State Of The Art Methods', 'Lightweight Model', 'Unwanted Changes', 'Color Change', 'Feature Space', 'Balloon', 'Red Box', 'Rotational Motion', 'Residual Block', 'Optical Flow', 'Motor Changes', 'Healthcare Applications', 'Motion Information', 'Large Motion', 'Subtle Motion', 'Static Scenario', 'Input Frames', 'L1 Loss', 'Dynamic Scenarios', 'Magnification Factor', 'Perceptual Loss', 'Final Loss Function', 'Spatial Decomposition', 'Red Stripes']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Video motion magnification provides information to understand the subtle changes present in objects for applications like industrial, healthcare, sports, etc. Most state-of- the-art (SOTA) methods use hand-crafted bandpass filters, which require prior information for the motion magnification, produces ringing artifacts, and small magnification etc. While others use deep-learning based techniques for higher magnification, but their output suffers from artificially induced motion, distortions, blurriness, etc. Further, SOTA methods are computationally complex, which makes them less suitable for real-time applications. To address these problems, we proposed deep learning based simple yet effective solution for motion magnification. The proposed method uses a feature sharing and appearance encoder for better motion magnification with fewer distortions, artifacts etc. Additionally, for reducing magnification of noise and other unwanted changes, proxy-model based training is proposed. A computationally lightweight model (~0.12 M parameters) is proposed along with the base model. The performance of the proposed models is tested qualitatively and quantitatively, with the SOTA methods. Results demonstrate the effectiveness of the proposed lightweight and base model over the existing SOTA methods."
Lightweight Video Denoising Using Aggregated Shifted Window Attention,"Lydia Lindner, Alexander Effland, Filip Ilic, Thomas Pock, Erich Kobler",University of Bonn; Graz University of Technology,100,"Austria, Germany",0,,"Video denoising is a fundamental problem in numerous computer vision applications. State-of-the-art attention-based denoising methods typically yield good results, but require vast amounts of GPU memory and usually suffer from very long computation times. Especially in the field of restoring digitized high-resolution historic films, these techniques are not applicable in practice. To overcome these issues, we introduce a lightweight video denoising network that combines efficient axial-coronal-sagittal (ACS) convolutions with a novel shifted window attention formulation (ASwin), which is based on the memory-efficient aggregation of self- and cross-attention across video frames. We numerically validate the performance and efficiency of our approach on synthetic Gaussian noise. Moreover, we train our network as a general-purpose blind denoising model for real-world videos, using a realistic noise synthesis pipeline to generate clean-noisy video pairs. A user study and non- reference quality assessment prove that our method outperforms the state-of-the-art on real-world historic videos in terms of denoising performance and temporal consistency.",https://openaccess.thecvf.com/content/WACV2023/html/Lindner_Lightweight_Video_Denoising_Using_Aggregated_Shifted_Window_Attention_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lindner_Lightweight_Video_Denoising_Using_Aggregated_Shifted_Window_Attention_WACV_2023_paper.pdf,,https://github.com/LLindn/ASwin-Video-Denoising,,main,Poster,https://ieeexplore.ieee.org/document/10030343/,"['Computer vision', 'Runtime', 'Films', 'Noise reduction', 'Memory management', 'Pipelines', 'Quality assessment']","['Window Shift', 'Video Denoising', 'High-resolution', 'Quality Assessment', 'Computation Time', 'Computer Vision', 'Gaussian Noise', 'User Study', 'Video Frames', 'Temporal Consistency', 'Video Modeling', 'Denoising Methods', 'Image Processing', 'Convolutional Neural Network', 'Image Quality', 'Additive Noise', 'Attention Mechanism', 'Visual Assessment', 'Additive Gaussian', 'Optical Flow', 'Motion Compensation', 'Memory Consumption', 'Dark Energy', 'Commercial Methods', 'Collaborative Filtering', 'Temporal Shift', '3D Convolution', 'Kinds Of Noise', 'JPEG Compression', 'Denoising Algorithm']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Video denoising is a fundamental problem in numerous computer vision applications. State-of-the-art attention-based denoising methods typically yield good results, but require vast amounts of GPU memory and usually suffer from very long computation times. Especially in the field of restoring digitized high-resolution historic films, these techniques are not applicable in practice. To overcome these issues, we introduce a lightweight video denoising network that combines efficient axial-coronal-sagittal (ACS) convolutions with a novel shifted window attention formulation (ASwin), which is based on the memory-efficient aggregation of self- and cross-attention across video frames. We numerically validate the performance and efficiency of our approach on synthetic Gaussian noise. Moreover, we train our network as a general-purpose blind denoising model for real-world videos, using a realistic noise synthesis pipeline to generate clean-noisy video pairs. A user study and non-reference quality assessment prove that our method outperforms the state-of-the-art on real-world historic videos in terms of denoising performance and temporal consistency."
"Line Search-Based Feature Transformation for Fast, Stable, and Tunable Content-Style Control in Photorealistic Style Transfer","Tai-Yin Chiu, Danna Gurari",University of Colorado Boulder; University of Texas at Austin,100,USA,0,,"Photorealistic style transfer is the task of synthesizing a realistic-looking image when adapting the content from one image to appear in the style of another image. Modern models commonly embed a transformation that fuses features describing the content image and style image and then decodes the resulting feature into a stylized image. We introduce a general-purpose transformation that enables controlling the balance between how much content is preserved and the strength of the infused style. We offer the first experiments that demonstrate the performance of existing transformations across different style transfer models, and demonstrate how transformation performs better in its ability to simultaneously run fast, produce consistently reasonable results, and control the balance between content and style in different models. To support reproducing our method and models, we share the code at https://github.com/chiutaiyin/LS-FT.",https://openaccess.thecvf.com/content/WACV2023/html/Chiu_Line_Search-Based_Feature_Transformation_for_Fast_Stable_and_Tunable_Content-Style_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chiu_Line_Search-Based_Feature_Transformation_for_Fast_Stable_and_Tunable_Content-Style_WACV_2023_paper.pdf,,https://github.com/chiutaiyin/LS-FT,2210.05996,main,Poster,https://ieeexplore.ieee.org/document/10030759/,"['Photorealism', 'Computer vision', 'Adaptation models', 'Codes', 'Fuses', 'Computer architecture', 'Boosting']","['Feature Transformation', 'Style Transfer', 'Reasonable Results', 'Style Image', 'Learning Rate', 'Gradient Descent', 'Autoencoder', 'Loss Value', 'Mean Vector', 'Loss Of Content', 'Content Features', 'Line Search', 'Severe Artifacts', 'Style Features', 'Multiple Transformations', 'Second-order Statistics', 'Descent Direction', 'Row Of Panels', 'Analytical Gradient', 'Updated Feature', 'Preservation Of Content']","['Algorithms: Computational photography', 'image and video synthesis']",1,"Photorealistic style transfer is the task of synthesizing a realistic-looking image when adapting the content from one image to appear in the style of another image. Modern models commonly embed a transformation that fuses features describing the content image and style image and then decodes the resulting feature into a stylized image. We introduce a general-purpose transformation that enables controlling the balance between how much content is preserved and the strength of the infused style. We offer the first experiments that demonstrate the performance of existing transformations across different style transfer models, and demonstrate how our transformation performs better in its ability to simultaneously run fast, produce consistently reasonable results, and control the balance between content and style in different models. To support reproducing our method and models, we share the code at https://github.com/chiutaiyin/LS-FT."
LineEX: Data Extraction From Scientific Line Charts,"Shivasankaran V. P., Muhammad Yusuf Hassan, Mayank Singh","IIT Gandhinagar, Gujarat, India",100,India,0,,"In this paper, we introduce LineEX that extracts data from scientific line charts. We adapt existing vision transformers and pose detection methods and showcase significant performance gains over existing SOTA baselines. We also propose a new loss function and present its effectiveness against existing loss functions. In addition, we synthetically created the largest line chart dataset comprising 430K images. The code and the dataset will be placed in the public domain soon after the acceptance.",https://openaccess.thecvf.com/content/WACV2023/html/P._LineEX_Data_Extraction_From_Scientific_Line_Charts_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/P._LineEX_Data_Extraction_From_Scientific_Line_Charts_WACV_2023_paper.pdf,,https://github.com/Shiva-sankaran/LineEX,,main,Poster,https://ieeexplore.ieee.org/document/10030557/,"['Computer vision', 'Codes', 'Computational modeling', 'Performance gain', 'Transformers', 'Data mining', 'Proposals']","['Line Chart', 'Loss Function', 'Vision Transformer', 'F1 Score', 'Image Dataset', 'Bounding Box', 'Feed-forward Network', 'Average Precision', 'Matrix M', 'Ablation Experiments', 'Pose Estimation', 'Precision Score', 'L1 Loss', 'Score Map', 'Chart Data', 'Rule-based Methods', 'Object Detection Model', 'Text Extraction', 'Axis Labels', 'Data Extraction Methods', 'Chart Types']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",1,"In this paper, we introduce LineEX that extracts data from scientific line charts. We adapt existing vision transformers and pose detection methods and showcase significant performance gains over existing SOTA baselines. We also propose a new loss function and present its effectiveness against existing loss functions. In addition, we synthetically created the largest line chart dataset comprising 430K images. The code is available at https: //github.com/Shiva-sankaran/LineEX."
LiveSeg: Unsupervised Multimodal Temporal Segmentation of Long Livestream Videos,"Jielin Qiu, Franck Dernoncourt, Trung Bui, Zhaowen Wang, Ding Zhao, Hailin Jin","Adobe Research, Carnegie Mellon University; Carnegie Mellon University; Adobe Research",66.66666667,USA,33.33333333,USA,"Livestream videos have become a significant part of online learning, where design, digital marketing, creative painting, and other skills are taught by experienced experts in the sessions, making them valuable materials. However, Livestream tutorial videos are usually hours long, recorded, and uploaded to the Internet directly after the live sessions, making it hard for other people to catch up quickly. An outline will be a beneficial solution, which requires the video to be temporally segmented according to topics. In this work, we introduced a large Livestream video dataset named MultiLive, and formulated the temporal segmentation of the long Livestream videos (TSLLV) task. We propose LiveSeg, an unsupervised Livestream video temporal Segmentation solution, which takes advantage of multimodal features from different domains. Our method achieved a 16.8% F1-score performance improvement compared with the state-of-the-art method.",https://openaccess.thecvf.com/content/WACV2023/html/Qiu_LiveSeg_Unsupervised_Multimodal_Temporal_Segmentation_of_Long_Livestream_Videos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qiu_LiveSeg_Unsupervised_Multimodal_Temporal_Segmentation_of_Long_Livestream_Videos_WACV_2023_paper.pdf,,,2210.0584,main,Poster,https://ieeexplore.ieee.org/document/10030771/,"['Computer vision', 'Statistical analysis', 'Annotations', 'Computational modeling', 'Focusing', 'Tutorials', 'Internet']","['Video Segments', 'Multimodal Segmentation', 'Digital Marketing', 'Visual Information', 'Visual Features', 'Video Frames', 'Small Segments', 'Short Segments', 'Hidden State', 'Segmentation Results', 'Short Video', 'Canonical Correlation Analysis', 'Language Domains', 'Visual Domain', 'Segment Boundaries', 'Video Length', 'Scene Changes', 'Bayesian Nonparametric Model', 'Wasserstein Distance', 'Word Error Rate', 'Video Summarization', 'Dirichlet Process', 'Unsupervised Methods', 'Visible Changes', 'Raw Video', 'Baseline Methods', 'Video Scenes']","['Algorithms: Vision + language and/or other modalities', 'Arts/games/social media']",1,"Livestream videos have become a significant part of online learning, where design, digital marketing, creative painting, and other skills are taught by experienced experts in the sessions, making them valuable materials. However, Livestream tutorial videos are usually hours long, recorded, and uploaded to the Internet directly after the live sessions, making it hard for other people to catch up quickly. An outline will be a beneficial solution, which requires the video to be temporally segmented according to topics. In this work, we introduced a large Livestream video dataset named MultiLive, and formulated the temporal segmentation of the long Livestream videos (TSLLV) task. We propose LiveSeg, an unsupervised Livestream video temporal Segmentation solution, which takes advantage of multimodal features from different domains. Our method achieved a 16.8% F1-score performance improvement compared with the state-of-the-art method."
LoopDA: Constructing Self-Loops To Adapt Nighttime Semantic Segmentation,"Fengyi Shen, Zador Pataki, Akhil Gurram, Ziyuan Liu, He Wang, Alois Knoll","Technical University of Munich; Peking University; Huawei Munich Research Center; Technical University of Munich, Huawei Munich Research Center; Huawei Munich Research Center, ETH Zurich",80,"China, Germany, Switzerland",20,China,"Due to the lack of training labels and the difficulty of annotating, dealing with adverse driving conditions such as nighttime has posed a huge challenge to the perception system of autonomous vehicles. Therefore, adapting knowledge from a labelled daytime domain to an unlabelled nighttime domain has been widely researched. In addition to labelled daytime datasets, existing nighttime datasets usually provide nighttime images with corresponding daytime reference images captured at nearby locations for reference. The key challenge is to minimize the performance gap between the two domains. In this paper, we propose LoopDA for domain adaptive nighttime semantic segmentation. It consists of self-loops that result in reconstructing the input data using predicted semantic maps, by rendering them into the encoded features. In a warm-up training stage, the self-loops comprise of an inner-loop and an outer-loop, which are responsible for intra-domain refinement and inter-domain alignment, respectively. To reduce the impact of day-night pose shifts, in the later self-training stage, we propose a co-teaching pipeline that involves an offline pseudo-supervision signal and an online reference-guided signal 'DNA' (Day-Night Agreement), bringing substantial benefits to enhance nighttime segmentation. Our model outperforms prior methods on Dark Zurich and Nighttime Driving datasets for semantic segmentation. Code and pretrained models are available at https://github.com/fy-vision/LoopDA.",https://openaccess.thecvf.com/content/WACV2023/html/Shen_LoopDA_Constructing_Self-Loops_To_Adapt_Nighttime_Semantic_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shen_LoopDA_Constructing_Self-Loops_To_Adapt_Nighttime_Semantic_Segmentation_WACV_2023_paper.pdf,,https://github.com/fy-vision/LoopDA,2211.1187,main,Poster,https://ieeexplore.ieee.org/document/10030192/,"['Training', 'Adaptation models', 'Computer vision', 'Codes', 'Semantic segmentation', 'Semantics', 'Pipelines']","['Semantic Segmentation', 'Domain Adaptation', 'Semantic Map', 'Adaptive Segmentation', 'Night-time Images', 'Deep Neural Network', 'Image Reconstruction', 'Autoencoder', 'Image Pairs', 'Unlabeled Data', 'Outer Loop', 'Image Retrieval', 'Dark Regions', 'Latent Features', 'Pixels In Region', 'Static Function', 'Gradient Calculation', 'Feature Encoder', 'Perceptual Loss', 'Camera Pose', 'Semantic Prediction', 'Domain Gap', 'Segmentation Output', 'Pose Changes', 'Domain Transfer', 'Feature Maps', 'Pixel Location', 'Source Domain', 'Training Iterations', 'Domain Discriminator']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Due to the lack of training labels and the difficulty of annotating, dealing with adverse driving conditions such as nighttime has posed a huge challenge to the perception system of autonomous vehicles. Therefore, adapting knowledge from a labelled daytime domain to an unlabelled nighttime domain has been widely researched. In addition to labelled daytime datasets, existing nighttime datasets usually provide nighttime images with corresponding daytime reference images captured at nearby locations for reference. The key challenge is to minimize the performance gap between the two domains. In this paper, we propose LoopDA for domain adaptive nighttime semantic segmentation. It consists of self-loops that result in reconstructing the input data using predicted semantic maps, by rendering them into the encoded features. In a warm-up training stage, the self-loops comprise of an inner-loop and an outer-loop, which are responsible for intra-domain refinement and inter-domain alignment, respectively. To reduce the impact of day-night pose shifts, in the later self-training stage, we propose a co-teaching pipeline that involves an offline pseudo-supervision signal and an online reference-guided signal ‘DNA’ (Day-Night Agreement), bringing substantial benefits to enhance nighttime segmentation. Our model outperforms prior methods on Dark Zurich and Nighttime Driving datasets for semantic segmentation. Code and pretrained models are available at https://github.com/fyvision/LoopDA."
Lossy Image Compression With Quantized Hierarchical VAEs,"Zhihao Duan, Ming Lu, Zhan Ma, Fengqing Zhu","Purdue University, West Lafayette, Indiana, U.S.; Nanjing University, Nanjing, Jiangsu, China",100,"China, USA",0,,"Recent work has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting from ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding for image compression. Along with improved neural network blocks, we present a powerful and efficient class of lossy image coders, outperforming previous methods on natural image (lossy) compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs.",https://openaccess.thecvf.com/content/WACV2023/html/Duan_Lossy_Image_Compression_With_Quantized_Hierarchical_VAEs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Duan_Lossy_Image_Compression_With_Quantized_Hierarchical_VAEs_WACV_2023_paper.pdf,,Available online,2208.13056,main,Poster,https://ieeexplore.ieee.org/document/10030851/,"['Adaptation models', 'Computer vision', 'Image coding', 'Quantization (signal)', 'Computational modeling', 'Neural networks', 'Rate distortion theory']","['Variational Autoencoder', 'Image Compression', 'Lossy Image', 'Data Distribution', 'Latent Variables', 'Natural Images', 'Latent Variable Model', 'Rate-distortion', 'Lossy Compression', 'Entropy Coding', 'Source Code', 'Probabilistic Model', 'Additive Noise', 'Autoregressive Model', 'Latent Space', 'Peak Signal-to-noise Ratio', 'Bitrate', 'Bitstream', 'Hierarchical Architecture', 'Least Significant Bit', 'Average Bit', 'Image X', 'Entropy Model', 'Image Inpainting', 'Discrete Cosine Transform', 'Learning Image', 'Image Steganography', 'Lossless Compression', 'Pixel Information', 'Order Markov']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",24,"Recent work has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting from ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding for image compression. Along with improved neural network blocks, we present a powerful and efficient class of lossy image coders, outperforming previous methods on natural image (lossy) compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs. Code is made available online."
M-FUSE: Multi-Frame Fusion for Scene Flow Estimation,"Lukas Mehl, Azin Jahedi, Jenny Schmalfuss, Andrés Bruhn","Institute for Visualization and Interactive Systems, University of Stuttgart",100,Germany,0,,"Recently, neural network for scene flow estimation show impressive results on automotive data such as the KITTI benchmark. However, despite of using sophisticated rigidity assumptions and parametrizations, such networks are typically limited to only two frame pairs which does not allow them to exploit temporal information. In our paper we address this shortcoming by proposing a novel multi-frame approach that considers an additional preceding stereo pair. To this end, we proceed in two steps: Firstly, building upon the recent RAFT-3D approach, we develop an improved two-frame baseline by incorporating an advanced stereo method. Secondly, and even more importantly, exploiting the specific modeling concepts of RAFT-3D, we propose a U-Net architecture that performs a fusion of forward and backward flow estimates and hence allows to integrate temporal information on demand. Experiments on the KITTI benchmark do not only show that the advantages of the improved baseline and the temporal fusion approach complement each other, they also demonstrate that the computed scene flow is highly accurate. More precisely, our approach ranks second overall and first for the even more challenging foreground objects, in total outperforming the original RAFT-3D method by more than 16%. Code is available at https://github.com/cv-stuttgart/M-FUSE.",https://openaccess.thecvf.com/content/WACV2023/html/Mehl_M-FUSE_Multi-Frame_Fusion_for_Scene_Flow_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mehl_M-FUSE_Multi-Frame_Fusion_for_Scene_Flow_Estimation_WACV_2023_paper.pdf,,https://github.com/cv-stuttgart/M-FUSE,,main,Poster,https://ieeexplore.ieee.org/document/10030262/,"['Extrapolation', 'Computer vision', 'Codes', 'Neural networks', 'Buildings', 'Estimation', 'Computer architecture']","['Flow Estimation', 'Scene Flow', 'Scene Flow Estimation', 'Multi-frame Fusion', 'Neural Network', 'Temporal Information', 'U-Net Architecture', 'Foreground Objects', 'Forward Flow', 'Stereo Pairs', 'Backward Flow', 'Convolutional Layers', 'Inverter', 'Forward Direction', 'Additional Input', 'Optical Flow', 'Motion Model', 'Rigid Transformation', 'Embedding Vectors', 'Motion Estimation', 'Original Resolution', 'Optical Flow Estimation', 'Changes In Disparity', 'Disparity Estimation', 'Cost Volume', '3D Motion', 'Previous Frame', 'Fusion Step', 'Occluded Regions', 'Parameter Count']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', '3D computer vision', 'Low-level and physics-based vision']",6,"Recently, neural network for scene flow estimation show impressive results on automotive data such as the KITTI benchmark. However, despite of using sophisticated rigidity assumptions and parametrizations, such networks are typically limited to only two frame pairs which does not allow them to exploit temporal information. In our paper we address this shortcoming by proposing a novel multi-frame approach that considers an additional preceding stereo pair. To this end, we proceed in two steps: Firstly, building upon the recent RAFT-3D approach, we develop an improved two-frame baseline by incorporating an advanced stereo method. Secondly, and even more importantly, exploiting the specific modeling concepts of RAFT-3D, we propose a U-Net architecture that performs a fusion of forward and backward flow estimates and hence allows to integrate temporal information on demand. Experiments on the KITTI benchmark do not only show that the advantages of the improved baseline and the temporal fusion approach complement each other, they also demonstrate that the computed scene flow is highly accurate. More precisely, our approach ranks second overall and first for the even more challenging foreground objects, in total outperforming the original RAFT-3D method by more than 16%. Code is available at https://github.com/cv-stuttgart/M-FUSE."
MASTAF: A Model-Agnostic Spatio-Temporal Attention Fusion Network for Few-Shot Video Classification,"Xin Liu, Huanle Zhang, Hamed Pirsiavash, Xin Liu","University of California, Davis",100,USA,0,,"We propose MASTAF, a Model-Agnostic Spatio-Temporal Attention Fusion network for few-shot video classification. MASTAF takes input from a general video spatial and temporal representation,e.g., using 2D CNN, 3D CNN, and Video Transformer. Then, to make the most of such representations, we use self- and cross-attention models to highlight the critical spatio-temporal region to increase the inter-class variations and decrease the intra-class variations. Last, MASTAF applies a lightweight fusion network and a nearest neighbor classifier to classify each query video. We demonstrate that MASTAF improves the state-of-the-art performance on three few-shot video classification benchmarks(UCF101, HMDB51, and Something-Something-V2), e.g., by up to 91.6%, 69.5%, and 60.7% for five-way one-shot video classification, respectively.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_MASTAF_A_Model-Agnostic_Spatio-Temporal_Attention_Fusion_Network_for_Few-Shot_Video_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_MASTAF_A_Model-Agnostic_Spatio-Temporal_Attention_Fusion_Network_for_Few-Shot_Video_WACV_2023_paper.pdf,,,2112.04585,main,Poster,https://ieeexplore.ieee.org/document/10030894/,"['Computer vision', 'Three-dimensional displays', 'Computational modeling', 'Benchmark testing', 'Transformers']","['Classification Network', 'Video Analysis', 'Fusion Network', 'Spatiotemporal Network', 'Few-shot Classification', 'Attention Fusion', 'Few-shot Video Classification', 'K-nearest Neighbor', 'Critical Region', 'Convolutional Neural Network', 'Attention Mechanism', 'Video Clips', 'Representation Learning', 'Latent Space', 'Video Frames', '3D Network', 'Representative Class', 'Support Set', 'Metric Learning', 'Few-shot Learning', 'Spatiotemporal Representation', 'Network Embedding', 'Unseen Classes', 'Self-attention Module', 'Embedding Module', 'Temporal Alignment', 'Video Samples', 'CNN Model', 'Training Dataset']","['Algorithms: Machine learning architectures,formulations,and algorithms (including transfer)', 'Video recognition and understanding (tracking,action recognition,etc.)']",9,"We propose MASTAF, a Model-Agnostic Spatio-Temporal Attention Fusion network for few-shot video classification. MASTAF takes input from a general video spatial and temporal representation,e.g., using 2D CNN, 3D CNN, and Video Transformer. Then, to make the most of such representations, we use self- and cross-attention models to highlight the critical spatio-temporal region to increase the inter-class variations and decrease the intra-class variations. Last, MASTAF applies a lightweight fusion network and a nearest neighbor classifier to classify each query video. We demonstrate that MASTAF improves the state-of-the-art performance on three few-shot video classification benchmarks(UCF101, HMDB51, and Something-Something-V2), e.g., by up to 91.6%, 69.5%, and 60.7% for five-way one-shot video classification, respectively."
MEVID: Multi-View Extended Videos With Identities for Video Person Re-Identification,"Daniel Davila, Dawei Du, Bryon Lewis, Christopher Funk, Joseph Van Pelt, Roderic Collins, Kellie Corona, Matt Brown, Scott McCloskey, Anthony Hoogs, Brian Clipp","Kitware, NY & NC, USA",100,USA,0,,"In this paper, we present the Multi-view Extended Videos with Identities (MEVID) dataset for large-scale, video person re-identification (ReID) in the wild. To our knowledge, MEVID represents the most-varied video person ReID dataset, spanning an extensive indoor and outdoor environment across nine unique dates in a 73-day window, various camera viewpoints, and entity clothing changes. Specifically, we label the identities of 158 unique people wearing 598 outfits taken from 8,092 tracklets, average length of about 590 frames, seen in 33 camera views from the very-large-scale MEVA person activities dataset. While other datasets have more unique identities, MEVID emphasizes a richer set of information about each individual, such as: 4 outfits/identity vs. 2 outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5 simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID. Being based on the MEVA video dataset, we also inherit data that is intentionally demographically balanced to the continental United States. To accelerate the annotation process, we developed a semi-automatic annotation framework and GUI that combines state-of-the-art real-time models for object detection, pose estimation, person ReID, and multi-object tracking. We evaluate several state-of-the-art methods on MEVID challenge problems and comprehensively quantify their robustness in terms of changes of outfit, scale, and background location. Our quantitative analysis on the realistic, unique aspects of MEVID shows that there are significant remaining challenges in video person ReID and indicates important directions for future research.",https://openaccess.thecvf.com/content/WACV2023/html/Davila_MEVID_Multi-View_Extended_Videos_With_Identities_for_Video_Person_Re-Identification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Davila_MEVID_Multi-View_Extended_Videos_With_Identities_for_Video_Person_Re-Identification_WACV_2023_paper.pdf,,https://github.com/Kitware/MEVID,2211.04656,main,Poster,https://ieeexplore.ieee.org/document/10030983/,"['Computer vision', 'Annotations', 'Statistical analysis', 'Clothing', 'Pose estimation', 'Cameras', 'Robustness']","['Video Person Re-identification', 'Pose Estimation', 'Camera View', 'Annotation Process', 'Human Pose Estimation', 'Changing Clothes', 'Multi-object Tracking', 'Training Set', 'Scale Variation', 'Bounding Box', 'Human Subjects Research', 'Action Recognition', 'Video Data', 'Previous Datasets', 'Score Map', 'Global Identity', 'Outdoor Scenes', 'Bus Stop', 'Hours Of Video', 'Independent Institutional Review Board', 'Current Research Directions', 'Re-identification Methods']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",11,"In this paper, we present the Multi-view Extended Videos with Identities (MEVID) dataset for large-scale, video person re-identification (ReID) in the wild. To our knowledge, MEVID represents the most-varied video person ReID dataset, spanning an extensive indoor and outdoor environment across nine unique dates in a 73-day window, various camera viewpoints, and entity clothing changes. Specifically, we label the identities of 158 unique people wearing 598 outfits taken from 8, 092 tracklets, average length of about 590 frames, seen in 33 camera views from the very-large-scale MEVA person activities dataset. While other datasets have more unique identities, MEVID emphasizes a richer set of information about each individual, such as: 4 outfits/identity vs. 2 outfits/identity in CCVID, 33 viewpoints across 17 locations vs. 6 in 5 simulated locations for MTA, and 10 million frames vs. 3 million for LS-VID. Being based on the MEVA video dataset, we also inherit data that is intentionally demographically balanced to the continental United States. To accelerate the annotation process, we developed a semi-automatic annotation framework and GUI that combines state-of-the-art real-time models for object detection, pose estimation, person ReID, and multi-object tracking. We evaluate several state-of-the-art methods on MEVID challenge problems and comprehensively quantify their robustness in terms of changes of outfit, scale, and background location. Our quantitative analysis on the realistic, unique aspects of MEVID shows that there are significant remaining challenges in video person ReID and indicates important directions for future research."
MFCFlow: A Motion Feature Compensated Multi-Frame Recurrent Network for Optical Flow Estimation,"Yonghu Chen, Dongchen Zhu, Wenjun Shi, Guanghui Zhang, Tianyu Zhang, Xiaolin Zhang, Jiamao Li","Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Xiongan Institute of Innovation, Xiongan, 071700, China; University of Science and Technology of China, Hefei, Anhui, 230027, China; ShanghaiTech University, Shanghai 201210, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Xiongan Institute of Innovation, Xiongan, 071700, China",100,China,0,,"Occlusions have long been a hard nut to crack in optical flow estimation due to ambiguous pixels matching between abutting images. Current methods only take two consecutive images as input, which is challenging to capture temporal coherence and reason about occluded regions. In this paper, we propose a novel optical flow estimation framework, namely MFCFlow, which attempts to compensate for the information of occlusions by mining and transferring motion features between multiple frames. Specifically, we construct a Motion-guided Feature Compensation cell (MFC cell) to enhance the ambiguous motion features according to the correlation of previous features obtained by attention-based structure. Furthermore, a TopK attention strategy is developed and embedded into the MFC cell to improve the subsequent matching quality. Extensive experiments demonstrate that our MFCFlow achieves significant improvements in occluded regions and attains state-of-the-art performances on both Sintel and KITTI benchmarks among other multi-frame optical flow methods.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_MFCFlow_A_Motion_Feature_Compensated_Multi-Frame_Recurrent_Network_for_Optical_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_MFCFlow_A_Motion_Feature_Compensated_Multi-Frame_Recurrent_Network_for_Optical_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030805/,"['Optical filters', 'Geometry', 'Matched filters', 'Correlation', 'Computational modeling', 'Estimation', 'Coherence']","['Optical Flow', 'Motion Features', 'Flow Estimation', 'Optical Flow Estimation', 'Multiple Frames', 'Temporal Coherence', 'Occluded Regions', 'Subsequent Matching', 'Ambiguity', 'Deep Learning', 'Convolutional Neural Network', 'Significantly Improved', 'Attention Mechanism', 'Unsupervised Methods', 'Softmax Function', 'Variable Approach', 'Feature Fusion', 'Relevant Points', 'Large Displacement', 'Feature Aggregation', 'Point In Frame', 'Cost Volume', 'Final Pass', 'Input Frames', 'Attention Matrix', 'Attention Map', 'Effect Of Illumination', 'Endpoint Error']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Low-level and physics-based vision']",3,"Occlusions have long been a hard nut to crack in optical flow estimation due to ambiguous pixels matching between abutting images. Current methods only take two consecutive images as input, which is challenging to capture temporal coherence and reason about occluded regions. In this paper, we propose a novel optical flow estimation framework, namely MFCFlow, which attempts to compensate for the information of occlusions by mining and transferring motion features between multiple frames. Specifically, we construct a Motion-guided Feature Compensation cell (MFC cell) to enhance the ambiguous motion features according to the correlation of previous features obtained by attention-based structure. Furthermore, a TopK attention strategy is developed and embedded into the MFC cell to improve the subsequent matching quality. Extensive experiments demonstrate that our MFCFlow achieves significant improvements in occluded regions and attains state-of-the-art performances on both Sintel and KITTI benchmarks among other multi-frame optical flow methods."
MFFN: Multi-View Feature Fusion Network for Camouflaged Object Detection,"Dehua Zheng, Xiaochen Zheng, Laurence T. Yang, Yuan Gao, Chenlu Zhu, Yiheng Ruan","ETH Zurich, Switzerland; Huazhong University of Science and Technology, China; Hubei Chutian Expressway Digital Technology, China; Hainan University, China",75,"China, Switzerland",25,China,"Recent research about camouflaged object detection (COD) aims to segment highly concealed objects hidden in complex surroundings. The tiny, fuzzy camouflaged objects result in visually indistinguishable properties. However, current single-view COD detectors are sensitive to background distractors. Therefore, blurred boundaries and variable shapes of the camouflaged objects are challenging to be fully captured with a single-view detector. To overcome these obstacles, we propose a behavior-inspired framework, called Multi-view Feature Fusion Network (MFFN), which mimics the human behaviors of finding indistinct objects in images, i.e., observing from multiple angles, distances, perspectives. Specifically, the key idea behind it is to generate multiple ways of observation (multi-view) by data augmentation and apply them as inputs. MFFN captures critical boundary and semantic information by comparing and fusing extracted multi-view features. In addition, our MFFN exploits the dependence and interaction between views and channels. Specifically, our methods leverage the complementary information between different views through a two-stage attention module called Co-attention of Multi-view (CAMV). And we design a local-overall module called Channel Fusion Unit (CFU) to explore the channel-wise contextual clues of diverse feature maps in an iterative manner. The experiment results show that our method performs favorably against existing state-of-the-art methods via training with the same data. The code will be available at https://github.com/dwardzheng/MFFN_COD.",https://openaccess.thecvf.com/content/WACV2023/html/Zheng_MFFN_Multi-View_Feature_Fusion_Network_for_Camouflaged_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_MFFN_Multi-View_Feature_Fusion_Network_for_Camouflaged_Object_Detection_WACV_2023_paper.pdf,,https://github.com/dwardzheng/MFFN_COD,2210.06361,main,Poster,https://ieeexplore.ieee.org/document/10030968/,"['Training', 'Tensors', 'Shape', 'Semantics', 'MIMICs', 'Object detection', 'Detectors']","['Object Detection', 'Feature Fusion', 'Multi-view Feature', 'Camouflaged Object', 'Camouflaged Object Detection', 'Feature Maps', 'Data Augmentation', 'Image Object', 'Semantic Information', 'Iterative Manner', 'Boundary Information', 'Training Set', 'Validation Set', 'Convolutional Layers', 'Attention Mechanism', 'Viewing Angle', 'Visual Perspective', 'Channel Dimension', 'Multi-task Learning', 'Salient Object Detection', 'Feature Tensor', 'Feature Pyramid Network', 'Combined View', 'Closer View', 'Feature Fusion Module', 'Auxiliary Task', 'Hyperparameter Settings', 'Accurate Detection Results']","['Applications: Animals/Insects', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Environmental monitoring/climate change/ecology']",18,"Recent research about camouflaged object detection (COD) aims to segment highly concealed objects hidden in complex surroundings. The tiny, fuzzy camouflaged objects result in visually indistinguishable properties. However, current single-view COD detectors are sensitive to background distractors. Therefore, blurred boundaries and variable shapes of the camouflaged objects are challenging to be fully captured with a singleview detector. To overcome these obstacles, we propose a behavior-inspired framework, called Multi-view Feature Fusion Network (MFFN), which mimics the human behaviors of finding indistinct objects in images, i.e., observing from multiple angles, distances, perspectives. Specifically, the key idea behind it is to generate multiple ways of observation (multi-view) by data augmentation and apply them as inputs. MFFN captures critical boundary and semantic information by comparing and fusing extracted multi-view features. In addition, our MFFN exploits the dependence and interaction between views and channels. Specifically, our methods leverage the complementary information between different views through a two-stage attention module called Co-attention of Multi-view (CAMV). And we design a local-overall module called Channel Fusion Unit (CFU) to explore the channel-wise contextual clues of diverse feature maps in an iterative manner. The experiment results show that our method performs favorably against existing state-of-the-art methods via training with the same data. The code will be available at https://github.com/dwardzheng/MFFN_COD."
ML-Decoder: Scalable and Versatile Classification Head,"Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben-Baruch, Asaf Noy","DAMO Academy, Alibaba Group",100,China,0,,"In this paper, we introduce ML-Decoder, a new attention-based classification head. ML-Decoder predicts the existence of class labels via queries, and enables better utilization of spatial data compared to global average pooling. By redesigning the decoder architecture, and using a novel group-decoding scheme, ML-Decoder is highly efficient, and can scale well to thousands of classes. Compared to using a larger backbone, ML-Decoder consistently provides a better speed-accuracy trade-off. ML-Decoder is also versatile - it can be used as a drop-in replacement for various classification heads, and generalize to unseen classes when operated with word queries. Novel query augmentations further improve its generalization ability. Using ML-Decoder, we achieve state-of-the-art results on several classification tasks: on MS-COCO multi-label, we reach 91.1% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP; and on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top score of 80.7%, without extra data or distillation. Public code will be available.",https://openaccess.thecvf.com/content/WACV2023/html/Ridnik_ML-Decoder_Scalable_and_Versatile_Classification_Head_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ridnik_ML-Decoder_Scalable_and_Versatile_Classification_Head_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030822/,"['Computer vision', 'Head', 'Codes', 'Computer architecture', 'Spatial databases', 'Decoding', 'Task analysis']","['Classification Head', 'Multi-label', 'ImageNet', 'Average Pooling', 'Speed-accuracy Trade-off', 'Extra Data', 'Zero-shot', 'Unseen Classes', 'Computational Cost', 'Attention Mechanism', 'Fully-connected Layer', 'Projection Matrix', 'Word Embedding', 'Training Speed', 'Statistical Noise', 'Embedding Vectors', 'Linear Projection', 'Score Map', 'Open Image', 'Types Of Queries', 'Output Logits', 'Feed-forward Layer']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",50,"In this paper, we introduce ML-Decoder, a new attention-based classification head. ML-Decoder predicts the existence of class labels via queries, and enables better utilization of spatial data compared to global average pooling. By redesigning the decoder architecture, and using a novel group-decoding scheme, ML-Decoder is highly efficient, and can scale well to thousands of classes. Compared to using a larger backbone, ML-Decoder consistently provides a better speed-accuracy trade-off. ML-Decoder is also versatile - it can be used as a drop-in replacement for various classification heads, and generalize to unseen classes when operated with word queries. Novel query augmentations further improve its generalization ability. Using ML-Decoder, we achieve state-of-the-art results on several classification tasks: on MS-COCO multi-label, we reach 91.1% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP; and on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top score of 80.7%, without extra data or distillation. Public code will be available."
MMPTRACK: Large-Scale Densely Annotated Multi-Camera Multiple People Tracking Benchmark,"Xiaotian Han, Quanzeng You, Chunyu Wang, Zhizheng Zhang, Peng Chu, Houdong Hu, Jiang Wang, Zicheng Liu",Microsoft,0,,100,USA,"Multi-camera tracking systems are gaining popularity in applications that demand high-quality tracking results, such as frictionless checkout. In cluttered and crowded environments, monocular multi-object tracking (MOT) systems often fail due to occlusions. Multiple highly overlapped cameras are capable of recovering partial 3D information. When used properly, 3D data can significantly alleviate the occlusion issue. However, training a multi-camera tracker demands a large-scale multi-camera tracking dataset with diverse camera settings and backgrounds. These requirements make the collection of multi-camera tracking dataset challenging and expensive. The cost of creating such a dataset has limited the availability and scale of datasets in this domain. Instead, we appeal to an auto-annotation system to reduce the cost. The system uses overlapped and calibrated depth and RGB cameras to build a 3D tracker and automatically generates the 3D tracking results. We then manually check and correct the 3D tracking results to ensure the label quality, which is much cheaper than solely manual annotation. Next, the 3D tracking results are projected to each calibrated RGB camera view to create 2D tracking results. In this way, we collect and annotate a large-scale densely labeled multi-camera tracking dataset from five different environments. We have conducted extensive experiments using two real-time multi-camera trackers and a person re-identification (ReID) model under different settings. This dataset provides a reliable benchmark for multi-camera, multi-object tracking systems in cluttered and crowded environments. We expect this benchmark to encourage more research attempts in this domain. Also, our results demonstrate that adapting the trackers and ReID models on this dataset significantly improves their performance. Our dataset will be publicly released upon the acceptance of this work.",https://openaccess.thecvf.com/content/WACV2023/html/Han_MMPTRACK_Large-Scale_Densely_Annotated_Multi-Camera_Multiple_People_Tracking_Benchmark_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Han_MMPTRACK_Large-Scale_Densely_Annotated_Multi-Camera_Multiple_People_Tracking_Benchmark_WACV_2023_paper.pdf,,,2111.15157,main,Poster,https://ieeexplore.ieee.org/document/10030358/,"['Training', 'Adaptation models', 'Three-dimensional displays', 'Benchmark testing', 'Cameras', 'Data models', 'Sensor systems']","['Multiple Tracking', 'Dense Annotations', 'Tracking System', 'Large-scale Datasets', 'Monocular', 'Depth Camera', 'Camera View', 'Tracking Results', 'RGB Camera', 'Crowded Environment', '3D Tracking', 'Domain Dataset', 'Tracking Dataset', 'Multi-object Tracking', 'Environmental Challenges', 'Point Cloud', 'Bounding Box', 'Spatial Distance', 'Tracking Error', 'External Data', '3D Pose', 'Multi-camera System', 'Top-down View', 'Ground Points', 'Multiple Object Tracking', 'Human Pose Estimation', 'Manual Labeling', 'Multiple Cameras', 'Pose Estimation', 'World Coordinate System']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",8,"Multi-camera tracking systems are gaining popularity in applications that demand high-quality tracking results, such as frictionless checkout. In cluttered and crowded environments, monocular multi-object tracking (MOT) systems often fail due to occlusions. Multiple highly overlapped cameras are capable of recovering partial 3D information. When used properly, 3D data can significantly alleviate the occlusion issue. However, training a multi-camera tracker demands a large-scale multi-camera tracking dataset with diverse camera settings and backgrounds. These requirements make the collection of multi-camera tracking dataset challenging and expensive. The cost of creating such a dataset has limited the availability and scale of datasets in this domain. Instead, we appeal to an auto-annotation system to reduce the cost, which uses overlapped and calibrated depth and RGB cameras to build a 3D tracker and automatically generates the 3D tracking results. The results are manually checked and corrected to ensure the label quality, which is much cheaper than solely manual annotation. Next, the 3D tracking results are projected to each calibrated RGB camera view to create 2D tracking results. In this way, we collect and annotate a large-scale densely labeled multi-camera tracking dataset from five different environments. We have conducted extensive experiments using two real-time multi-camera trackers and a person re-identification (ReID) model under different settings. This dataset provides a reliable benchmark for multi-camera, multi-object tracking systems in cluttered and crowded environments. We expect this benchmark to encourage more research attempts in this domain. Our dataset will be publicly released upon the acceptance of this work."
MORGAN: Meta-Learning-Based Few-Shot Open-Set Recognition via Generative Adversarial Network,"Debabrata Pal, Shirsha Bose, Biplab Banerjee, Yogananda Jeppu","Honeywell Technology Solutions, India; Technical University of Munich, Germany; Indian Institute of Technology, Bombay",66.66666667,"Germany, India",33.33333333,India,"In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment known-unknowns. A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented known-unknowns or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.",https://openaccess.thecvf.com/content/WACV2023/html/Pal_MORGAN_Meta-Learning-Based_Few-Shot_Open-Set_Recognition_via_Generative_Adversarial_Network_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pal_MORGAN_Meta-Learning-Based_Few-Shot_Open-Set_Recognition_via_Generative_Adversarial_Network_WACV_2023_paper.pdf,,https://github.com/DebabrataPal7/MORGAN,,main,Poster,https://ieeexplore.ieee.org/document/10030577/,"['Training', 'Measurement', 'Computer vision', 'Image recognition', 'Benchmark testing', 'Generative adversarial networks', 'Feature extraction']","['Generative Adversarial Networks', 'Open Set Recognition', 'Open Space', 'Class Distribution', 'Noise Variance', 'Closed And Open', 'Hyperspectral Image Datasets', 'Receiver Operating Characteristic Curve', 'Spectral Bands', 'Singular Value', 'Reptiles', 'Hallucinations', 'Classification Of Samples', 'Land Cover Classes', 'Noise Vector', 'Base Classes', 'Support Set', 'Hyperspectral Image Classification', 'Few-shot Learning', 'Indian Pines', 'Indian Pines Dataset', 'Salinas Dataset', 'Class Prototypes']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Agriculture', 'Remote Sensing']",8,"In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment ""known-unknowns"". A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented ""known-unknowns"" or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.
<sup>1</sup>"
MRI Imputation Based on Fused Index- and Intensity-Registration,"Jiyoon Shin, Jungwoo Lee","Seoul National University, HodooAI Lab; Seoul National University",100,South Korea,0,,"3D MRI imaging is based on a number of imaging sequences such as T1, T2, T1ce, and Flair, and each of them is performed by a group of two-dimensional scans. In practical MRI, some scans are often missing while many medical applications require a full set of scans. An MRI imputation method is presented, which synthesizes such missing scans. Key components in this method are the index registration and the intensity registration. The index registration models anatomical differences between two different scans in the same imaging sequence, and the intensity registration reflects the image contrast differences between two different scans of the same index. Two registration fields are learned to be invariant, and accordingly, allow two estimates of a missing scan, one within corresponding imaging sequence and another along scan index; the two estimates are combined to yield the final synthesized scan. Experimental results highlight that the proposed method improves prevalent limitations existing in previous synthesis methods, blending both structural and contrast aspects and capturing subtle parts of the brain. Quantitative results also show the superiority in various data sets, transitions, and measures.",https://openaccess.thecvf.com/content/WACV2023/html/Shin_MRI_Imputation_Based_on_Fused_Index-_and_Intensity-Registration_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shin_MRI_Imputation_Based_on_Fused_Index-_and_Intensity-Registration_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030119/,"['Visualization', 'Computer vision', 'Biomedical equipment', 'Three-dimensional displays', 'Magnetic resonance imaging', 'Medical services', 'Indexes']","['Magnetic Resonance Imaging', 'Anatomical Differences', 'Set Of Scans', '3D Magnetic Resonance Imaging', 'Two-dimensional Scanning', 'Medical Imaging', 'Multiple Imputation', 'Image Registration', 'Transverse Plane', 'Image Intensity', 'Fluid-attenuated Inversion Recovery', 'Direction Parallel', '3D Volume', 'Central Brain', 'Encoder-decoder Structure', 'Orange Arrows', 'Vision Transformer', 'Structural Similarity Index Measure', 'Auxiliary Loss', 'T1-weighted Inversion Recovery']","['Applications: Biomedical/healthcare/medicine', 'Computational photography', 'image and video synthesis']",1,"3D MRI imaging is based on a number of imaging sequences such as T1, T2, T1ce, and Flair, and each of them is performed by a group of two-dimensional scans. In practical MRI, some scans are often missing while many medical applications require a full set of scans. An MRI imputation method is presented, which synthesizes such missing scans. Key components in this method are the index registration and the intensity registration. The index registration models anatomical differences between two different scans in the same imaging sequence, and the intensity registration reflects the image contrast differences between two different scans of the same index. Two registration fields are learned to be invariant, and accordingly, allow two estimates of a missing scan, one within corresponding imaging sequence and another along scan index; the two estimates are combined to yield the final synthesized scan. Experimental results highlight that the proposed method improves prevalent limitations existing in previous synthesis methods, blending both structural and contrast aspects and capturing subtle parts of the brain. Quantitative results also show the superiority in various data sets, transitions, and measures."
MT-DETR: Robust End-to-End Multimodal Detection With Confidence Fusion,"Shih-Yun Chu, Ming-Sui Lee",National Taiwan University,100,Taiwan,0,,"Due to the trending need for autonomous driving, camera-based object detection has recently attracted lots of attention and successful development. However, there are times when unexpected and severe weather occurs in outdoor environments, making the detection tasks less effective and unexpected. In this case, additional sensors like lidar and radar are adopted to help the camera work in bad weather. However, existing multimodal detection methods do not consider the characteristics of different vehicle sensors to complement each other. Therefore, a novel end-to-end multimodal multistage object detection network called MT-DETR is proposed. Unlike the unimodal object detection networks, MT-DETR adds fusion modules and enhancement modules and adopts a hierarchical fusion mechanism. The Residual Fusion Module (RFM) and Confidence Fusion Module (CFM) are designed to fuse camera, lidar, radar, and time features. The Residual Enhancement Module (REM) reinforces each unimodal branch while a multistage loss is introduced to strengthen each branch's effectiveness. The synthesis algorithm for generating camera-lidar data pairs in foggy conditions further boosts the performance in unseen adverse weather. Extensive experiments on various weather conditions of the STF dataset demonstrate that MT-DETR outperforms state-of-the-art methods. The generality of MT-DETR has also been confirmed by replacing the feature extractor in the experiments. The code and pre-trained models are available on https://github.com/Chushihyun/MT-DETR.",https://openaccess.thecvf.com/content/WACV2023/html/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.pdf,,https://github.com/Chushihyun/MT-DETR,,main,Poster,https://ieeexplore.ieee.org/document/10030494/,"['Meteorological radar', 'Laser radar', 'Scalability', 'Training data', 'Object detection', 'Sensor phenomena and characterization', 'Feature extraction']","['Multimodal Detection', 'Confidence Fusion', 'Extreme Weather', 'Object Detection', 'Sensor Characteristics', 'Bad Weather', 'Multimodal Methods', 'Residual Module', 'Object Detection Network', 'Hierarchical Mechanism', 'Camerawork', 'Enhancement Module', 'Loss Function', 'Training Data', 'Point Cloud', 'Bounding Box', 'Autonomous Vehicles', 'Depth Map', 'Feature Fusion', 'Multiple Sensors', 'Clear Weather', 'Early Fusion', 'Atmospheric Light', 'Detection Head', 'Depth Features', 'Convolutional Block', 'Multimodal Tasks', 'Lidar Data', 'Sensor Inputs', 'Radar Signal']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",6,"Due to the trending need for autonomous driving, camera-based object detection has recently attracted lots of attention and successful development. However, there are times when unexpected and severe weather occurs in outdoor environments, making the detection tasks less effective and unexpected. In this case, additional sensors like lidar and radar are adopted to help the camera work in bad weather. However, existing multimodal detection methods do not consider the characteristics of different vehicle sensors to complement each other. Therefore, a novel end-to-end multimodal multistage object detection network called MT-DETR is proposed. Unlike the unimodal object detection networks, MT-DETR adds fusion modules and enhancement modules and adopts a hierarchical fusion mechanism. The Residual Fusion Module (RFM) and Confidence Fusion Module (CFM) are designed to fuse camera, lidar, radar, and time features. The Residual Enhancement Module (REM) reinforces each unimodal branch while a multistage loss is introduced to strengthen each branch’s effectiveness. The synthesis algorithm for generating camera-lidar data pairs in foggy conditions further boosts the performance in unseen adverse weather. Extensive experiments on various weather conditions of the STF dataset demonstrate that MT-DETR outperforms state-of-the-art methods. The generality of MT-DETR has also been confirmed by replacing the feature extractor in the experiments. The code and pre-trained models are available on https://github.com/Chushihyun/MT-DETR."
Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images,"Prakash Chandra Chhipa, Richa Upadhyay, Gustav Grund Pihlgren, Rajkumar Saini, Seiichi Uchida, Marcus Liwicki","Machine Learning Group, EISLAB, Lulea Tekniska Universitet, Lulea, Sweden; Human Interface Laboratory, Kyushu University, Fukuoka, Japan",100,"Japan, Sweden",0,,"This work presents a novel self-supervised pre-training method to learn efficient representations without labels on histopathology medical images utilizing magnification factors. Other state-of-the-art works mainly focus on fully supervised learning approaches that rely heavily on human annotations. However, the scarcity of labeled and unlabeled data is a long-standing challenge in histopathology. Currently, representation learning without labels remains unexplored in the histopathology domain. The proposed method, Magnification Prior Contrastive Similarity (MPCS), enables self-supervised learning of representations without labels on small-scale breast cancer dataset BreakHis by exploiting magnification factor, inductive transfer, and reducing human prior. The proposed method matches fully supervised learning state-of-the-art performance in malignancy classification when only 20% of labels are used in fine-tuning and outperform previous works in fully supervised learning settings for three public breast cancer datasets, including BreakHis. Further, It provides initial support for a hypothesis that reducing human-prior leads to efficient representation learning in self-supervision, which will need further investigation. The implementation of this work is available online on GitHub.",https://openaccess.thecvf.com/content/WACV2023/html/Chhipa_Magnification_Prior_A_Self-Supervised_Method_for_Learning_Representations_on_Breast_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chhipa_Magnification_Prior_A_Self-Supervised_Method_for_Learning_Representations_on_Breast_WACV_2023_paper.pdf,,https://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method/,2203.07707,main,Poster,https://ieeexplore.ieee.org/document/10030107/,"['Representation learning', 'Computer vision', 'Histopathology', 'Microscopy', 'Supervised learning', 'Redundancy', 'Focusing']","['Breast Cancer', 'Histopathological', 'Representation Learning', 'Histopathological Images', 'Breast Cancer Histopathological Images', 'Supervised Learning', 'Public Datasets', 'Transfer Learning', 'Efficient Learning', 'Self-supervised Learning', 'Efficient Representation', 'Breast Cancer Dataset', 'Magnification Factor', 'Implementation Work', 'Degrees Of Freedom', 'Convolutional Neural Network', 'Paired Samples', 'Majority Voting', 'Image Patches', 'Digital Pathology', 'Self-supervised Learning Methods', 'Encoder Architecture', 'Class Activation Maps', 'Small-scale Datasets', 'ImageNet Pre-trained Model', 'Random Pairs', 'Small-scale Data', 'Supervision Signal', 'Self-supervised Representation', 'Joint Embedding']","['self-supervised learning', 'contrastive learning', 'representation learning', 'breast cancer', 'histopathological images', 'transfer learning', 'medical images']",19,"This work presents a novel self-supervised pre-training method to learn efficient representations without labels on histopathology medical images utilizing magnification factors. Other state-of-the-art works mainly focus on fully supervised learning approaches that rely heavily on human annotations. However, the scarcity of labeled and unlabeled data is a long-standing challenge in histopathology. Currently, representation learning without labels remains unexplored in the histopathology domain. The proposed method, Magnification Prior Contrastive Similarity (MPCS), enables self-supervised learning of representations without labels on small-scale breast cancer dataset BreakHis by exploiting magnification factor, inductive transfer, and reducing human prior. The proposed method matches fully supervised learning state-of-the-art performance in malignancy classification when only 20% of labels are used in fine-tuning and outperform previous works in fully supervised learning settings for three public breast cancer datasets, including BreakHis. Further, It provides initial support for a hypothesis that reducing human-prior leads to efficient representation learning in self-supervision, which will need further investigation. The implementation of this work is available online on GitHub
<sup>1</sup>
."
Mapping DNN Embedding Manifolds for Network Generalization Prediction,"Molly O’Brien, Brett Wolfinger, Julia Bukowski, Mathias Unberath, Aria Pezeshk, Gregory D. Hager",,,,,,"Deep Neural Networks(DNN) often fail in surprising ways, and predicting how well a trained DNN will generalize in a new, external operating domain is essential for deploying DNNs in safety critical applications, e.g., perception for self-driving vehicles or medical image analysis. Recently, the task of Network Generalization Prediction (NGP) has been proposed to predict how a DNN will generalize in an external operating domain. Previous NGP approaches have leveraged multiple labeled test sets or labeled metadata. In this study, we propose an embedding map, the first NGP approach that predicts DNN performance based on how unlabeled images from an external operating domain map in the DNN embedding space. We evaluate our proposed Embedding Map and other recently proposed NGP approaches for pedestrian, melanoma, and animal classification tasks. We find that our embedding map has the best average NGP performance, and that our embedding map is effective at modeling complex, non-linear embedding space structures.",https://openaccess.thecvf.com/content/WACV2023/html/OBrien_Mapping_DNN_Embedding_Manifolds_for_Network_Generalization_Prediction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/OBrien_Mapping_DNN_Embedding_Manifolds_for_Network_Generalization_Prediction_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030896/,"['Manifolds', 'Visualization', 'Computer vision', 'Image analysis', 'Melanoma', 'Computer architecture', 'Metadata']","['Deep Neural Network', 'Classification Task', 'Average Performance', 'Latent Space', 'Autonomous Vehicles', 'Structure Of Space', 'Safety-critical', 'Medical Image Analysis', 'Open Domain', 'Unlabeled Images', 'Performance Of Deep Neural Networks', 'External Domain', 'Safety-critical Applications', 'External Operations', 'Training Data', 'Decision Tree', 'Binary Classification', 'Multiple Datasets', 'Domain Shift', 'Probability Of Outcome', 'Internal Dataset', 'External Dataset', 'Deep Neural Network Architecture', 'Change In Accuracy', 'Training Domain', 'Unconstrained Environment', 'Nonlinear Manifold', 'Empirical Risk Minimization', 'Distance Metrics', 'Decision Boundary']","['Applications: Robotics', 'Biomedical/healthcare/medicine']",1,"Deep Neural Networks(DNN) often fail in surprising ways, and predicting how well a trained DNN will generalize in a new, external operating domain is essential for deploying DNNs in safety critical applications, e.g., perception for self-driving vehicles or medical image analysis. Recently, the task of Network Generalization Prediction (NGP) has been proposed to predict how a DNN will generalize in an external operating domain. Previous NGP approaches have leveraged multiple labeled test sets or labeled metadata. In this study, we propose an embedding map, the first NGP approach that predicts DNN performance based on how unlabeled images from an external operating domain map in the DNN embedding space. We evaluate our proposed Embedding Map and other recently proposed NGP approaches for pedestrian, melanoma, and animal classification tasks. We find that our embedding map has the best average NGP performance, and that our embedding map is effective at modeling complex, non-linear embedding space structures."
Marker-Removal Networks To Collect Precise 3D Hand Data for RGB-Based Estimation and Its Application in Piano,"Erwin Wu, Hayato Nishioka, Shinichi Furuya, Hideki Koike","Tokyo Institute of Technology, Tokyo, Japan; Sony Computer Science Laboratory, Tokyo, Japan",50,Japan,50,Japan,"Hand pose analysis is a key step to understanding dexterous hand performances of many high-level skills, such as playing the piano. Currently, most accurate hand tracking systems are using fabric-/marker-based sensing that potentially disturbs users' performance. On the other hand, markerless computer vision-based methods rely on a precise bare-hand dataset for training, which is difficult to obtain. In this paper, we collect a large-scale high precision 3D hand pose dataset with a small workload using a novel marker-removal network (MR-Net). The proposed MR-Net translates the marked-hand images to realistic bare-hand images, and the corresponding 3D postures are captured by a motion capture system thus few manual annotations are required. A baseline estimation network PiaNet is introduced and we report the accuracy of various metrics together with a blind qualitative test to show the practical effect.",https://openaccess.thecvf.com/content/WACV2023/html/Wu_Marker-Removal_Networks_To_Collect_Precise_3D_Hand_Data_for_RGB-Based_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wu_Marker-Removal_Networks_To_Collect_Precise_3D_Hand_Data_for_RGB-Based_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030863/,"['Training', 'Measurement', 'Computer vision', 'Three-dimensional displays', 'Annotations', 'Pose estimation', 'Manuals']","['Blinded Study', 'Motion Capture', 'Manual Annotation', '3D Position', 'Hand Tracking', 'Raw Data', 'Right-hand', 'Training Set', 'Validation Set', 'Left Hand', 'Bounding Box', 'Generative Adversarial Networks', 'RGB Images', 'Depth Camera', 'Joint Position', 'Music Education', 'Hand Position', 'L1 Loss', 'Hand Motion', 'Domain Gap', 'Single Hand', 'GAN-based Methods', 'Bare Hands', 'Data Generation Method', 'L2 Loss', 'Convolutional Neural Network', 'Single Image', 'Quantitative Experiments', 'Data Augmentation', 'Angle Error']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Computational photography', 'image and video synthesis', 'Arts/games/social media']",1,"Hand pose analysis is a key step to understanding dexterous hand performances of many high-level skills, such as playing the piano. Currently, most accurate hand tracking systems are using fabric-/marker-based sensing that potentially disturbs users’ performance. On the other hand, markerless computer vision-based methods rely on a precise bare-hand dataset for training, which is difficult to obtain. In this paper, we collect a large-scale high precision 3D hand pose dataset with a small workload using a marker-removal network (MR-Net). The proposed MR-Net translates the marked-hand images to realistic bare-hand images, and the corresponding 3D postures are captured by a motion capture thus few manual annotations are required. A baseline estimation network PiaNet is introduced and we report the accuracy of various metrics together with a blind qualitative test to show the practical effect."
Masked Image Modeling Advances 3D Medical Image Analysis,"Zekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem Safta, Mariann Micsinai Balan, Kevin Brown",Bristol Myers Squibb,0,,100,USA,"Recently, masked image modeling (MIM) has gained considerable attention due to its capacity to learn from vast amounts of unlabeled data and has been demonstrated to be effective on a wide variety of vision tasks involving natural images. Meanwhile, the potential of self-supervised learning in modeling 3D medical images is anticipated to be immense due to the high quantities of unlabeled images, and the expense and difficulty of quality labels. However, MIM's applicability to medical images remains uncertain. In this paper, we demonstrate that masked image modeling approaches can also advance 3D medical images analysis in addition to natural images. We study how masked image modeling strategies leverage performance from the viewpoints of 3D medical image segmentation as a representative downstream task: i) when compared to naive contrastive learning, masked image modeling approaches accelerate the convergence of supervised training even faster (1.40x) and ultimately produce a higher dice score; ii) predicting raw voxel values with a high masking ratio and a relatively smaller patch size is non-trivial self-supervised pretext-task for medical images modeling; iii) a lightweight decoder or projection head design for reconstruction is powerful for masked image modeling on 3D medical images which speeds up training and reduce cost; iv) finally, we also investigate the effectiveness of MIM methods under different practical scenarios where different image resolutions and labeled data ratios are applied. Anonymized codes are available at https://anonymous.4open.science/r/MIM-Med3D.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Masked_Image_Modeling_Advances_3D_Medical_Image_Analysis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Masked_Image_Modeling_Advances_3D_Medical_Image_Analysis_WACV_2023_paper.pdf,,https://github.com/ZEKAICHEN/MIM-Med3D,2204.11716,main,Poster,https://ieeexplore.ieee.org/document/10030105/,"['Training', 'Solid modeling', 'Analytical models', 'Image segmentation', 'Three-dimensional displays', 'Self-supervised learning', 'Predictive models']","['Medical Imaging', '3D Images', '3D Analysis', 'Masked Images', 'Medical Image Analysis', '3D Image Analysis', '3D Medical Image', 'Image Resolution', 'Natural Images', 'Patch Size', 'Labeled Data', 'Unlabeled Data', 'Self-supervised Learning', 'Medical Image Segmentation', 'Dice Score', 'Deep Learning', 'Low Resolution', 'Medical Data', 'Transfer Learning', 'Representation Learning', 'Vision Transformer', 'Brain Tumor Segmentation', 'Masking Strategy', 'Effective Representation', 'General Representation', 'Masked Area', 'Pre-training Data', 'Abdominal Computed Tomography', 'Original Signal', 'Inpainting']","['Applications: Biomedical/healthcare/medicine', '3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",33,"Recently, masked image modeling (MIM) has gained considerable attention due to its ability to learn from vast amounts of unlabeled data and has been demonstrated to be effective on various vision tasks involving natural images. Meanwhile, the potential of self-supervised learning in modeling 3D medical images is anticipated to be immense due to the high quantities of unlabeled images and the expense and difficulty of quality labels. However, MIM’s applicability to medical images remains uncertain. In this paper, we demonstrate that masked image modeling approaches can also advance 3D medical image analysis in addition to natural images. We study how masked image modeling strategies leverage performance from the viewpoints of 3D medical image segmentation as a representative downstream task: i) when compared to naive contrastive learning, masked image modeling approaches accelerate the convergence of supervised training even faster (1.40×) and ultimately produce a higher dice score; ii) predicting raw voxel values with a high masking ratio and a relatively smaller patch size is nontrivial self-supervised pretext-task for medical images modeling; iii) a lightweight decoder or projection head design for reconstruction is robust for masked image modeling on 3D medical images which speeds up training and reduce cost; iv) finally, we also investigate the effectiveness of MIM methods under different practical scenarios where different image resolutions and labeled data ratios are applied. Anonymized codes are available at https://github.com/ZEKAICHEN/MIM-Med3D."
Match Cutting: Finding Cuts With Smooth Visual Transitions,"Boris Chen, Amir Ziai, Rebecca S. Tucker, Yuchen Xie",Netflix Inc.,100,USA,0,,"A match cut is a transition between a pair of shots that uses similar framing, composition, or action to fluidly bring the viewer from one scene to the next. Match cuts are frequently used in film, television, and advertising. However, finding shots that work together is a highly manual and time-consuming process that can take days. We propose a modular and flexible system to efficiently find high-quality match cut candidates starting from millions of shot pairs. We annotate and release a dataset of approximately 20,000 labeled pairs that we use to evaluate our system, using both classification and metric learning approaches that leverage a variety of image, video, audio, and audio-visual feature extractors. In addition, we release code and embeddings for reproducing our experiments at github.com/netflix/matchcut.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Match_Cutting_Finding_Cuts_With_Smooth_Visual_Transitions_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Match_Cutting_Finding_Cuts_With_Smooth_Visual_Transitions_WACV_2023_paper.pdf,,github.com/netflix/matchcut,2210.05766,main,Poster,https://ieeexplore.ieee.org/document/10030941/,"['Measurement', 'Visualization', 'Computer vision', 'Codes', 'TV', 'Manuals', 'Feature extraction']","['Flexible System', 'Metric Learning', 'Scalable', 'Binary Classification', 'Adam Optimizer', 'Intersection Over Union', 'Optical Flow', 'Learning-based Approaches', 'XGBoost', 'Matched Pairs', 'Units In Layer', 'Instance Segmentation', 'Aggregation Function', 'Random Prediction', 'Center Of Frame', 'Video Editing', 'Camera Movement', 'Candidate Pairs', 'Image Encoder', 'Video Encoding']","['Applications: Arts/games/social media', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",3,"A match cut is a transition between a pair of shots that uses similar framing, composition, or action to fluidly bring the viewer from one scene to the next. Match cuts are frequently used in film, television, and advertising. However, finding shots that work together is a highly manual and time-consuming process that can take days. We propose a modular and flexible system to efficiently find high-quality match cut candidates starting from millions of shot pairs. We annotate and release a dataset of approximately 20k labeled pairs that we use to evaluate our system, using both classification and metric learning approaches that leverage a variety of image, video, audio, and audio-visual feature extractors. In addition, we release code and embeddings for reproducing our experiments at github.com/netflix/matchcut."
Medical Image Segmentation via Cascaded Attention Decoding,"Md Mostafijur Rahman, Radu Marculescu",The University of Texas at Austin,100,USA,0,,"Transformers have shown great promise in medical image segmentation due to their ability to capture long-range dependencies through self-attention. However, they lack the ability to learn the local (contextual) relations among pixels. Previous works try to overcome this problem by embedding convolutional layers either in the encoder or decoder modules of transformers thus ending up sometimes with inconsistent features. To address this issue, we propose a novel attention-based decoder, namely CASCaded Attention DEcoder (CASCADE), which leverages the multiscale features of hierarchical vision transformers. CASCADE consists of i) an attention gate which fuses features with skip connections and ii) a convolutional attention module that enhances the long-range and local context by suppressing background information. We use a multi-stage feature and loss aggregation framework due to their faster convergence and better performance. Our experiments demonstrate that transformers with CASCADE significantly outperform state-of-the-art CNN- and transformer-based approaches, obtaining up to 5.07% and 6.16% improvements in DICE and mIoU scores, respectively. CASCADE opens new ways of designing better attention-based decoders.",https://openaccess.thecvf.com/content/WACV2023/html/Rahman_Medical_Image_Segmentation_via_Cascaded_Attention_Decoding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rahman_Medical_Image_Segmentation_via_Cascaded_Attention_Decoding_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030763/,"['Image segmentation', 'Computer vision', 'Medical services', 'Logic gates', 'Transformers', 'Decoding', 'Lesions']","['Medical Image Segmentation', 'Convolutional Neural Network', 'Convolutional Layers', 'Attention Module', 'Multi-scale Features', 'Skip Connections', 'Long-range Dependencies', 'Convolution Module', 'Dice Score', 'Vision Transformer', 'Convolutional Attention Module', 'Attention Gate', 'Spatial Information', 'Feature Maps', 'Attention Mechanism', 'Batch Normalization', 'Spatial Attention', 'Backbone Network', 'Sigmoid Activation', 'Segmentation Map', 'Channel Attention', 'Transformer-based Methods', 'ReLU Activation Layer', 'Prediction Head', 'Accurate Output', 'Convolutional Block', 'Medical Tasks', 'Average Ice', 'Attention Block', 'Decoder Block']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",86,"Transformers have shown great promise in medical image segmentation due to their ability to capture long-range dependencies through self-attention. However, they lack the ability to learn the local (contextual) relations among pixels. Previous works try to overcome this problem by embedding convolutional layers either in the encoder or decoder modules of transformers thus ending up sometimes with inconsistent features. To address this issue, we propose a novel attention-based decoder, namely CASCaded Attention DEcoder (CASCADE), which leverages the multi-scale features of hierarchical vision transformers. CASCADE consists of i) an attention gate which fuses features with skip connections and ii) a convolutional attention module that enhances the long-range and local context by suppressing background information. We use a multi-stage feature and loss aggregation framework due to their faster convergence and better performance. Our experiments demonstrate that transformers with CASCADE significantly outperform state-of-the-art CNN- and transformer-based approaches, obtaining up to 5.07% and 6.16% improvements in DICE and mIoU scores, respectively. CASCADE opens new ways of designing better attention-based decoders."
Mesh-Tension Driven Expression-Based Wrinkles for Synthetic Faces,"Chirag Raman, Charlie Hewitt, Erroll Wood, Tadas Baltrušaitis",Delft University of Technology; Microsoft,50,Netherlands,50,USA,"Recent advances in synthesizing realistic faces have shown that synthetic training data can replace real data for various face-related computer vision tasks. A question arises: how important is realism? Is the pursuit of photorealism excessive? In this work, we show otherwise. We boost the realism of our synthetic faces by introducing dynamic skin wrinkles in response to facial expressions, and observe significant performance improvements in downstream computer vision tasks. Previous approaches for producing such wrinkles either required prohibitive artist effort to scale across identities and expressions, or were not capable of reconstructing high-frequency skin details with sufficient fidelity. Our key contribution is an approach that produces realistic wrinkles across a large and diverse population of digital humans. Concretely, we formalize the concept of mesh-tension and use it to aggregate possible wrinkles from high-quality expression scans into albedo and displacement texture maps. At synthesis, we use these maps to produce wrinkles even for expressions not represented in the source scans. Additionally, to provide a more nuanced indicator of model performance under deformations resulting from compressed expressions, we introduce the 300W-winks evaluation subset and the Pexels dataset of closed eyes and winks.",https://openaccess.thecvf.com/content/WACV2023/html/Raman_Mesh-Tension_Driven_Expression-Based_Wrinkles_for_Synthetic_Faces_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Raman_Mesh-Tension_Driven_Expression-Based_Wrinkles_for_Synthetic_Faces_WACV_2023_paper.pdf,,,2210.03529,main,Poster,https://ieeexplore.ieee.org/document/10030945/,"['Computer vision', 'Photorealism', 'Sociology', 'Training data', 'Skin', 'Task analysis', 'Statistics']","['Synthetic Faces', 'Skin Wrinkles', 'Synthetic Training Data', 'Machine Learning', 'Single Image', 'Weight Range', 'Evaluation Dataset', 'Changes In Geometry', 'Neutral Expressions', 'Eye Region', 'Landmark Localization', 'Noise Artifacts', 'Physical Simulation', 'Domain Gap', 'Face Model', 'Landmark Detection', 'Surface Normals', 'Bezier Curve', 'Normal Map', '3D Face', 'Learning Rate Of 1e', 'Displacement Maps']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', '3D computer vision']",3,"Recent advances in synthesizing realistic faces have shown that synthetic training data can replace real data for various face-related computer vision tasks. A question arises: how important is realism? Is the pursuit of photorealism excessive? In this work, we show otherwise. We boost the realism of our synthetic faces by introducing dynamic skin wrinkles in response to facial expressions, and observe significant performance improvements in downstream computer vision tasks. Previous approaches for producing such wrinkles either required prohibitive artist effort to scale across identities and expressions, or were not capable of reconstructing high-frequency skin details with sufficient fidelity. Our key contribution is an approach that produces realistic wrinkles across a large and diverse population of digital humans. Concretely, we formalize the concept of mesh-tension and use it to aggregate possible wrinkles from high-quality expression scans into albedo and displacement texture maps. At synthesis, we use these maps to produce wrinkles even for expressions not represented in the source scans. Additionally, to provide a more nuanced indicator of model performance under deformations resulting from com-pressed expressions, we introduce the 300W-winks evaluation subset and the Pexels dataset of closed eyes and winks."
Meta-Auxiliary Learning for Future Depth Prediction in Videos,"Huan Liu, Zhixiang Chi, Yuanhao Yu, Yang Wang, Jun Chen, Jin Tang",Concordia University; McMaster University; Huawei Noah’s Ark Lab,66.66666667,Canada,33.33333333,China,"We consider a new problem of future depth prediction in video. Given a sequence of observed frames, the goal is to predict the depth map of a future frame that has not been observed yet. Depth estimation plays a vital role for scene understanding and decision-making in intelligent systems. Predicting future depth maps can be valuable for autonomous vehicles to anticipate the behaviors of their surrounding objects. Our proposed model for this problem has a two-branch architecture. One branch is for the primary task of future depth estimation. The other branch is for an auxiliary task of image reconstruction. The auxiliary branch can act as a regularization. Inspired by some recent work on test-time adaption, we use the auxiliary task during testing to adapt the model to a specific test video. We also propose a novel meta-auxiliary learning that learn the model specifically for the purpose of effective test-time adaptation. Experimental results demonstrate that our proposed approach significantly outperforms other alternative methods.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_Meta-Auxiliary_Learning_for_Future_Depth_Prediction_in_Videos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_Meta-Auxiliary_Learning_for_Future_Depth_Prediction_in_Videos_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030818/,"['Adaptation models', 'Decision making', 'Estimation', 'Predictive models', 'Task analysis', 'Intelligent systems', 'Image reconstruction']","['Depth Prediction', 'Video Prediction', 'Image Reconstruction', 'Autonomous Vehicles', 'Video Frames', 'Prediction Task', 'Depth Map', 'Primary Task', 'Prediction Problem', 'Depth Estimation', 'Test Videos', 'Auxiliary Task', 'Future Frames', 'Model Parameters', 'Root Mean Square Error', 'Time Step', 'Task Performance', 'Validation Set', 'Quantitative Results', 'Convolutional Layers', 'Primary Branches', 'Input Frames', 'Self-supervised Task', 'Catastrophic Forgetting', 'Sequence Of Frames', 'L1 Loss', 'Backbone Network', 'Training Pairs', 'Batch Size']","['Algorithms: Low-level and physics-based vision', '3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"We consider a new problem of future depth prediction in videos. Given a sequence of observed frames in a video, the goal is to predict the depth map of a future frame that has not been observed yet. Depth estimation plays a vital role for scene understanding and decision-making in intelligent systems. Predicting future depth maps can be valuable for autonomous vehicles to anticipate the behaviours of their surrounding objects. Our proposed model for this problem has a two-branch architecture. One branch is for the primary task of future depth prediction. The other branch is for an auxiliary task of image reconstruction. The auxiliary branch can act as a regularization. Inspired by some recent work on test-time adaption, we use the auxiliary task during testing to adapt the model to a specific test video. We also propose a novel meta-auxiliary learning that learns the model specifically for the purpose of effective test-time adaptation. Experimental results demonstrate that our proposed approach outperforms other alternative methods."
Meta-Learning for Adaptation of Deep Optical Flow Networks,"Chaerin Min, Taehyun Kim, Jongwoo Lim","Department of Computer Science, Hanyang University, Seoul, Korea; Department of Computer Science, Hanyang University, Seoul, Korea; Department of Artificial Intelligence, Hanyang University, Seoul, Korea",100,South Korea,0,,"In this paper, we propose an instance-wise meta-learning algorithm for optical flow domain adaptation. Typical optical flow algorithms with deep learning suffer from weak cross-domain performance since their trainings largely rely on synthetic datasets in specific domains. This prevents optical flow performance on different scenes from carrying similar performance in practice. Meanwhile, test-time domain adaptation approaches for optical flow estimation are yet to be studied. Our proposed method, with some training data, learns to adapt more sensitively to incoming inputs in the target domain. During the inference process, our method readily exploits the information only accessible in the test-time. Since our algorithm adapts to each input image, we incorporate traditional unsupervised losses for optical flow estimation. Moreover, with the observation that optical flows in a single domain typically contain many similar motions, we show that our method demonstrates high performance with only a small number of training data. This allows to save labeling efforts. Through the experiments on KITTI and MPI-Sintel datasets, our algorithm significantly outperforms the results without adaptation and shows consistently better performance in comparison to typical fine-tuning with the same amount of data. Also qualitatively our proposed method demonstrates more accurate results for the images with high errors in the original networks.",https://openaccess.thecvf.com/content/WACV2023/html/Min_Meta-Learning_for_Adaptation_of_Deep_Optical_Flow_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Min_Meta-Learning_for_Adaptation_of_Deep_Optical_Flow_Networks_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031014/,"['Training', 'Optical losses', 'Neural networks', 'Estimation', 'Training data', 'Reliability', 'Labeling']","['Optical Flow', 'Network Flow', 'Optical Flow Network', 'Deep Learning', 'Specific Domains', 'Target Domain', 'Domain Adaptation', 'Flow Estimation', 'Similar Motion', 'Number Of Training Data', 'KITTI Dataset', 'Optical Flow Estimation', 'Final Results', 'Learning Rate', 'Unsupervised Learning', 'Regularization Term', 'Domain Data', 'Pre-trained Network', 'Specific Input', 'Conventional Estimation', 'Accurate Flow', 'Endpoint Error', 'Fast Adaptation', 'Consecutive Video Frames', 'Gradient Descent Step', 'Update Step', 'Final Pass', 'Ground Truth Dataset', 'Input Frames', 'Training Domain']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",5,"In this paper, we propose an instance-wise meta-learning algorithm for optical flow domain adaptation. Typical optical flow algorithms with deep learning suffer from weak cross-domain performance since their trainings largely rely on synthetic datasets in specific domains. This prevents optical flow performance on different scenes from carrying similar performance in practice. Meanwhile, test-time do-main adaptation approaches for optical flow estimation are yet to be studied. Our proposed method, with some training data, learns to adapt more sensitively to incoming in-puts in the target domain. During the inference process, our method readily exploits the information only accessible in the test-time. Since our algorithm adapts to each input image, we incorporate traditional unsupervised losses for optical flow estimation. Moreover, with the observation that optical flows in a single domain typically contain many similar motions, we show that our method demonstrates high performance with only a small number of training data. This allows to save labeling efforts. Through the experiments on KITTI and MPI-Sintel datasets, our algorithm significantly outperforms the results without adaptation and shows consistently better performance in comparison to typical fine-tuning with the same amount of data. Also qualitatively our proposed method demonstrates more accurate results for the images with high errors in the original networks."
Meta-OLE: Meta-Learned Orthogonal Low-Rank Embedding,"Ze Wang, Yue Lu, Qiang Qiu","Electrical and Computer Engineering, Purdue University; School of Engineering and Applied Sciences, Harvard University",100,USA,0,,"We introduce Meta-OLE, a new geometry-regularized method for fast adaptation to novel tasks in few-shot image classification. The proposed method learns to adapt for each few-shot classification task a feature space with simultaneous inter-class orthogonality and intra-class low-rankness. Specifically, a deep feature extractor is trained by explicitly imposing orthogonal low-rank subspace structures among features corresponding to different classes within a given task. To adapt to novel tasks with unseen categories, we further meta-learn a light-weight transformation to enhance the inter-class margins. As an additional benefit, this light-weight transformation lets us exploit the query data for label propagation from labeled to unlabeled data without any auxiliary network components. The explicitly geometry-regularized feature subspaces allow the classifiers on novel tasks to be inferred in a closed form, with an adaptive subspace truncation that selectively discards non-discriminative dimensions. We perform experiments on standard few-shot image classification tasks, and observe performance superior to state-of-the-art meta-learning methods.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_Meta-OLE_Meta-Learned_Orthogonal_Low-Rank_Embedding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Meta-OLE_Meta-Learned_Orthogonal_Low-Rank_Embedding_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030416/,"['Geometry', 'Computer vision', 'Feature extraction', 'Robustness', 'Regulation', 'Task analysis', 'Standards']","['Classification Task', 'Feature Space', 'Orthogonal Subspace', 'Low-rank Structure', 'Auxiliary Components', 'Few-shot Classification', 'Common Practice', 'Deep Network', 'Hyperparameters', 'Singular Value', 'Parametrized', 'Singular Value Decomposition', 'Projection Matrix', 'Orthogonal Matrix', 'Linear Classifier', 'Training Deep Networks', 'Support Set', 'Metric Learning', 'Query Set', 'Geometry Features', 'Few-shot Learning', 'Meta Learning', 'Query Sample', 'Pseudo Labels', 'Subspace Projection', 'Unseen Classes', 'ReLU Activation', 'Adaptive Step']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"We introduce Meta-OLE, a new geometry-regularized method for fast adaptation to novel tasks in few-shot image classification. The proposed method learns to adapt for each few-shot classification task a feature space with simultaneous inter-class orthogonality and intra-class low-rankness. Specifically, a deep feature extractor is trained by explicitly imposing orthogonal low-rank subspace structures among features corresponding to different classes within a given task. To adapt to novel tasks with unseen categories, we further meta-learn a light-weight transformation to enhance the inter-class margins. As an additional benefit, this light-weight transformation lets us exploit the query data for label propagation from labeled to unlabeled data without any auxiliary network components. The explicitly geometry-regularized feature subspaces allow the classifiers on novel tasks to be inferred in a closed form, with an adaptive subspace truncation that selectively discards non-discriminative dimensions. We perform experiments on standard few-shot image classification tasks, and observe performance superior to state-of-the-art meta-learning methods."
Misclassifications of Contact Lens Iris PAD Algorithms: Is It Gender Bias or Environmental Conditions?,"Akshay Agarwal, Nalini Ratha, Afzel Noore, Richa Singh, Mayank Vatsa","Texas A&M University Kingsville, USA; University at Buffalo, USA; IISER Bhopal, India; IIT Jodhpur, India",100,"India, USA",0,,"One of the critical steps in biometrics pipeline is detection of presentation attacks, a physical adversary. Several presentation (adversary) attack detection (PAD) algorithms, including iris PAD, are proposed and have shown superlative performance. However, a recent study, on a small-scale database, has highlighted that iris PAD may have gender biases. In this research, we present a rigorous study on gender bias in iris presentation attack detection algorithms using a large-scale and gender-balanced database. The paper provides several interesting observations which can help in building future presentation attack detection algorithms with aim of fair treatment of each demography. In addition, we also present a robust iris presentation attack detection algorithm by combining gender-covariate biased classifiers. The proposed robust classifier not only reduces the difference in accuracy between different genders but also improves the overall performance of the PAD system.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Misclassifications_of_Contact_Lens_Iris_PAD_Algorithms_Is_It_Gender_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Misclassifications_of_Contact_Lens_Iris_PAD_Algorithms_Is_It_Gender_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030240/,"['Computer vision', 'Databases', 'Demography', 'Pipelines', 'Buildings', 'Classification algorithms', 'Detection algorithms']","['Contact Lens', 'Difference In Accuracy', 'Biometric', 'Gender Balance', 'Attack Detection', 'Algorithmic Bias', 'Neural Network', 'Machine Learning', 'Training Set', 'Learning Algorithms', 'Convolutional Neural Network', 'Support Vector Machine', 'Deep Neural Network', 'Number Of Images', 'Transfer Learning', 'Multiple Classes', 'Training Images', 'Support Vector Machine Classifier', 'Indoor Environments', 'Unseen Environments', 'Issue Of Bias', 'Equal Error Rate', 'Issues Of Fairness', 'Softmax Classifier', 'Multiple Convolutional Neural Networks', 'Linear Support Vector Machine', 'Convolutional Neural Network Architecture']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose']",4,"One of the critical steps in biometrics pipeline is detection of presentation attacks, a physical adversary. Several presentation (adversary) attack detection (PAD) algorithms, including iris PAD, have been proposed and have shown superlative performance. However, a recent study, on a small-scale database, has highlighted that iris PAD may have gender biases. In this research, we present a rigorous study on gender bias in iris presentation attack detection algorithms using a large-scale and gender-balanced database. The paper provides several interesting observations which can help in building future presentation attack detection algorithms with aim of fair treatment of each demography. In addition, we also present a robust iris presentation attack detection algorithm by combining gender-covariate based classifiers. The proposed robust classifier not only reduces the difference in accuracy between different genders but also improves the overall performance of the PAD system."
MixVPR: Feature Mixing for Visual Place Recognition,"Amar Ali-bey, Brahim Chaib-draa, Philippe Giguère","Universit ´e Laval, Qu ´ebec, Canada",100,Canada,0,,"Visual Place Recognition (VPR) is a crucial part of mobile robotics and autonomous driving as well as other computer vision tasks. It refers to the process of identifying a place depicted in a query image using only computer vision. At large scale, repetitive structures, weather, and illumination changes pose a real challenge, as appearances can drastically change over time. Along with tackling these challenges, an efficient VPR technique must also be practical in real-world scenarios where latency matters. To address this, we introduce MixVPR, a new holistic feature aggregation technique that takes feature maps from pre-trained backbones as a set of global features. Then, it incorporates a global relationship between elements in each feature map in a cascade of feature mixing, eliminating the need for local or pyramidal aggregation as done in NetVLAD or TransVPR. We demonstrate the effectiveness of our technique through extensive experiments on multiple large-scale benchmarks. Our method outperforms all existing techniques by a large margin while having less than half the number of parameters compared to CosPlace and NetVLAD. We achieve a new all-time high recall@1 score of 94.6% on Pitts250k-test, 88.0% on MapillarySLS, and more importantly, 58.4% on Nordland. Finally, our method outperforms two-stage retrieval techniques such as Patch-NetVLAD, TransVPR and SuperGLUE, all while being orders of magnitude faster.",https://openaccess.thecvf.com/content/WACV2023/html/Ali-bey_MixVPR_Feature_Mixing_for_Visual_Place_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ali-bey_MixVPR_Feature_Mixing_for_Visual_Place_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030191/,"['Computer vision', 'Visualization', 'Neurons', 'Lighting', 'Color', 'Benchmark testing', 'Task analysis']","['Place Recognition', 'Visual Place Recognition', 'Feature Maps', 'Visual Recognition', 'Cyanoacrylate', 'Repeat Structure', 'Illumination Changes', 'Query Image', 'Multiple Benchmarks', 'Aggregation Techniques', 'Part Of The Robot', 'Convolutional Neural Network', 'Local Features', 'Spatial Features', 'Qualitative Results', 'Multilayer Perceptron', 'Stochastic Gradient Descent', 'Handcrafted Features', 'Changes In Appearance', 'Global Descriptors', 'Challenging Benchmark', 'Convolutional Neural Networks Backbone', 'General Mean', 'Viewpoint Changes', 'Global Representation', 'Image Retrieval', 'Robust Representation', 'Vision Transformer', 'Backbone Architecture']","['Applications: Robotics', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",74,"Visual Place Recognition (VPR) is a crucial part of mobile robotics and autonomous driving as well as other computer vision tasks. It refers to the process of identifying a place depicted in a query image using only computer vision. At large scale, repetitive structures, weather and illumination changes pose a real challenge, as appearances can drastically change over time. Along with tackling these challenges, an efficient VPR technique must also be practical in real-world scenarios where latency matters. To address this, we introduce MixVPR, a new holistic feature aggregation technique that takes feature maps from pre-trained backbones as a set of global features. Then, it incorporates a global relationship between elements in each feature map in a cascade of feature mixing, eliminating the need for local or pyramidal aggregation as done in NetVLAD or TransVPR. We demonstrate the effectiveness of our technique through extensive experiments on multiple large-scale benchmarks. Our method outperforms all existing techniques by a large margin while having less than half the number of parameters compared to CosPlace and NetVLAD. We achieve a new all-time high recall@1 score of 94.6% on Pitts250k-test, 88.0% on MapillarySLS, and more importantly, 58.4% on Nordland. Finally, our method outperforms two-stage retrieval techniques such as Patch-NetVLAD, TransVPR and SuperGLUE all while being orders of magnitude faster."
Mixture Outlier Exposure: Towards Out-of-Distribution Detection in Fine-Grained Environments,"Jingyang Zhang, Nathan Inkawhich, Randolph Linderman, Yiran Chen, Hai Li",Air Force Research Laboratory; Duke University,50,USA,50,USA,"Many real-world scenarios in which DNN-based recognition systems are deployed have inherently fine-grained attributes (e.g., bird-species recognition, medical image classification). In addition to achieving reliable accuracy, a critical subtask for these models is to detect Out-of-distribution (OOD) inputs. Given the nature of the deployment environment, one may expect such OOD inputs to also be fine-grained w.r.t. the known classes (e.g., a novel bird species), which are thus extremely difficult to identify. Unfortunately, OOD detection in fine-grained scenarios remains largely underexplored. In this work, we aim to fill this gap by first carefully constructing four large-scale fine-grained test environments, in which existing methods are shown to have difficulties. Particularly, we find that even explicitly incorporating a diverse set of auxiliary outlier data during training does not provide sufficient coverage over the broad region where fine-grained OOD samples locate. We then propose Mixture Outlier Exposure (MixOE), which mixes ID data and training outliers to expand the coverage of different OOD granularities, and trains the model such that the prediction confidence linearly decays as the input transitions from ID to OOD. Extensive experiments and analyses demonstrate the effectiveness of MixOE for building up OOD detector in fine-grained environments. The code is available at https://github.com/zjysteven/MixOE.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Mixture_Outlier_Exposure_Towards_Out-of-Distribution_Detection_in_Fine-Grained_Environments_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Mixture_Outlier_Exposure_Towards_Out-of-Distribution_Detection_in_Fine-Grained_Environments_WACV_2023_paper.pdf,,https://github.com/zjysteven/MixOE,2106.03917,main,Poster,https://ieeexplore.ieee.org/document/10030810/,"['Training', 'Computer vision', 'Image recognition', 'Codes', 'Detectors', 'Predictive models', 'Prediction algorithms']","['Environmental Detection', 'Bird Species', 'Real-world Scenarios', 'Test Environment', 'Broad Region', 'Outlier Data', 'Prediction Confidence', 'Auxiliary Data', 'Large-scale Environments', 'Medical Image Classification', 'Standard Model', 'Batch Size', 'Feature Space', 'Detection Performance', 'Butterfly', 'Mixed Samples', 'Hyperparameter Tuning', 'Reliable Detection', 'Baseline Methods', 'Detector Set', 'Fine-grained Data', 'Mixed Operator', 'Distribution Of Outliers', 'Portion Of The Sample', 'Mixed Training', 'Test Bench', 'Outlier Samples', 'Soft Targets', 'Linear Mixing']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",9,"Many real-world scenarios in which DNN-based recognition systems are deployed have inherently fine-grained attributes (e.g., bird-species recognition, medical image classification). In addition to achieving reliable accuracy, a critical subtask for these models is to detect Out-of-distribution (OOD) inputs. Given the nature of the deployment environment, one may expect such OOD inputs to also be fine-grained w.r.t. the known classes (e.g., a novel bird species), which are thus extremely difficult to identify. Unfortunately, OOD detection in fine-grained scenarios remains largely underexplored. In this work, we aim to fill this gap by first carefully constructing four large-scale fine-grained test environments, in which existing methods are shown to have difficulties. Particularly, we find that even explicitly incorporating a diverse set of auxiliary outlier data during training does not provide sufficient coverage over the broad region where fine-grained OOD samples locate. We then propose Mixture Outlier Exposure (MixOE), which mixes ID data and training outliers to expand the coverage of different OOD granularities, and trains the model such that the prediction confidence linearly decays as the input transitions from ID to OOD. Extensive experiments and analyses demonstrate the effectiveness of MixOE for building up OOD detector in finegrained environments. The code is available at https://github.com/zjysteven/MixOE."
Mobile Robot Manipulation Using Pure Object Detection,Brent Griffin,Agility Robotics,0,,100,USA,"This paper addresses the problem of mobile robot manipulation using object detection. Our approach uses detection and control as complimentary functions that learn from real-world interactions. We develop an end-to-end manipulation method based solely on detection and introduce Task-focused Few-shot Object Detection (TFOD) to learn new objects and settings. Our robot collects its own training data and automatically determines when to retrain detection to improve performance across various subtasks (e.g., grasping). Notably, detection training is low-cost, and our robot learns to manipulate new objects using as few as four clicks of annotation. In physical experiments, our robot learns visual control from a single click of annotation and a novel update formulation, manipulates new objects in clutter and other mobile settings, and achieves state-of-the-art results on an existing visual servo control and depth estimation benchmark. Finally, we develop a TFOD Benchmark to support future object detection research for robotics: https://github.com/griffbr/TFOD.",https://openaccess.thecvf.com/content/WACV2023/html/Griffin_Mobile_Robot_Manipulation_Using_Pure_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Griffin_Mobile_Robot_Manipulation_Using_Pure_Object_Detection_WACV_2023_paper.pdf,,https://github.com/griffbr/TFOD,2201.12437,main,Poster,https://ieeexplore.ieee.org/document/10030974/,"['Visualization', 'Annotations', 'Robot vision systems', 'Estimation', 'Object detection', 'Benchmark testing', 'Mobile robots']","['Object Detection', 'Robot Manipulator', 'Training Data', 'Depth Estimation', 'Visual Control', 'Physical Experiments', 'Visual Servoing', 'Environmental Changes', 'Image Features', 'Specific Tasks', 'Detection Model', 'Bounding Box', 'Target Object', 'Consecutive Trials', 'Robot Control', 'End-effector', 'Minimum Height', 'Change In Error', 'Standard Configuration', 'Camera Pose', 'Annotated Examples', 'Mobile Manipulator', 'Camera Motion', 'Bounding Box Annotations', 'Revolute Joints', 'Task Space']","['Applications: Robotics', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",1,"This paper addresses the problem of mobile robot manipulation using object detection. Our approach uses detection and control as complimentary functions that learn from real-world interactions. We develop an end-to-end manipulation method based solely on detection and introduce Task-focused Few-shot Object Detection (TFOD) to learn new objects and settings. Our robot collects its own training data and automatically determines when to retrain detection to improve performance across various subtasks (e.g., grasping). Notably, detection training is low-cost, and our robot learns to manipulate new objects using as few as four clicks of annotation. In physical experiments, our robot learns visual control from a single click of annotation and a novel update formulation, manipulates new objects in clutter and other mobile settings, and achieves state-of-the-art results on an existing visual servo control and depth estimation benchmark. Finally, we develop a TFOD Benchmark to support future object detection research for robotics: https://github.com/griffbr/TFOD."
Modality Mixer for Multi-Modal Action Recognition,"Sumin Lee, Sangmin Woo, Yeonju Park, Muhammad Adi Nugroho, Changick Kim",KAIST,100,South Korea,0,,"In multi-modal action recognition, it is important to consider not only the complementary nature of different modalities but also global action content. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, to leverage complementary information across modalities and temporal context of an action for multi-modal action recognition. We also introduce a simple yet effective recurrent unit, called Multi-modal Contextualization Unit (MCU), which is a core component of M-Mixer. Our MCU temporally encodes a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth, IR). This process encourages M-Mixer to exploit global action content and also to supplement complementary information of other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NWUCLA datasets. Moreover, we demonstrate the effectiveness of M-Mixer by conducting comprehensive ablation studies.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Modality_Mixer_for_Multi-Modal_Action_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Modality_Mixer_for_Multi-Modal_Action_Recognition_WACV_2023_paper.pdf,,,2208.11314,main,Poster,https://ieeexplore.ieee.org/document/10030488/,"['Computer vision', 'Mixers']","['Action Recognition', 'Multimodal Action Recognition', 'Content Features', 'Recurrent Unit', 'Extensive Experiments', 'Sequence Features', 'Max-pooling', 'Video Clips', 'Confidence Score', 'Normalization Layer', 'Average Pooling', 'Hyperbolic Tangent', 'Hidden State', 'Optical Flow', 'Depth Data', 'Action Classes', 'Temporal Coding', 'RGB Data', 'Standard Cross-entropy Loss', 'RGB Video', 'RGB Features', 'Video Action Recognition', 'Matrix Learning', 'Previous Hidden State']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",4,"In multi-modal action recognition, it is important to consider not only the complementary nature of different modalities but also global action content. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, to leverage complementary information across modalities and temporal context of an action for multi-modal action recognition. We also introduce a simple yet effective recurrent unit, called Multi-modal Contextualization Unit (MCU), which is a core component of M-Mixer. Our MCU temporally encodes a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth, IR). This process encourages M-Mixer to exploit global action content and also to supplement complementary information of other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets. Moreover, we demonstrate the effectiveness of M-Mixer by conducting comprehensive ablation studies."
Modeling Stroke Mask for End-to-End Text Erasing,"Xiangcheng Du, Zhao Zhou, Yingbin Zheng, Tianlong Ma, Xingjiao Wu, Cheng Jin","Videt Technology, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",50,China,50,China,"Scene text erasing aims to wipe text regions in scene images with reasonable background. Most previous approaches employ scene text detectors to assist localization of the text regions. However, detected text boxes contain both text strokes and background clutters, and directly inpainting on the whole boxes may remain text artifacts and make regions unnatural. In this paper, we present an end-to-end network that focuses on modeling text stroke masks that provide more accurate locations to compute erased images. The network consists of two stages, i.e., a basic network with stroke generation and a refinement network with stroke awareness. The basic network predicts the text stroke masks and initial erasing results simultaneously. The refinement network receives the masks as supervision to generate natural erased results. Experiments on both synthetic and real-world scene images demonstrate the effectiveness of our framework in producing high quality erasing results.",https://openaccess.thecvf.com/content/WACV2023/html/Du_Modeling_Stroke_Mask_for_End-to-End_Text_Erasing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Du_Modeling_Stroke_Mask_for_End-to-End_Text_Erasing_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030956/,"['Location awareness', 'Computer vision', 'Computational modeling', 'Detectors', 'Maintenance engineering', 'Decoding', 'Feeds']","['Scene Images', 'Basic Network', 'Text Box', 'Optical Character Recognition', 'Real-world Images', 'Refinement Network', 'Loss Function', 'Structural Similarity', 'Feature Maps', 'Training Images', 'Residual Block', 'Peak Signal-to-noise Ratio', 'Two-stage Method', 'Reconstruction Loss', 'Percentage Of Pixels', 'Encoder-decoder Structure', 'Image Inpainting', 'Weight Of Pixel']","['Applications: Arts/games/social media', 'Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",1,"Scene text erasing aims to wipe text regions in scene images with reasonable background. Most previous approaches employ scene text detectors to assist localization of the text regions. However, detected text boxes contain both text strokes and background clutters, and directly in-painting on the whole boxes may remain text artifacts and make regions unnatural. In this paper, we present an end-to-end network that focuses on modeling text stroke masks that provide more accurate locations to compute erased images. The network consists of two stages, i.e., a basic network with stroke generation and a refinement network with stroke awareness. The basic network predicts the text stroke masks and initial erasing results simultaneously. The refinement network receives the masks as supervision to generate natural erased results. Experiments on both synthetic and real-world scene images demonstrate the effectiveness of our framework in producing high quality erasing results."
Modeling the Lighting in Scenes As Style for Auto White-Balance Correction,"Furkan Kınlı, Doğa Yılmaz, Barış Özcan, Furkan Kıraç","Vision and Graphics Lab, Ozyegin University, Turkiye; Ozyegin University, Turkiye",100,Turkey,0,,"Style may refer to different concepts (e.g. painting style, hairstyle, texture, color, filter, etc.) depending on how the feature space is formed. In this work, we propose a novel idea of interpreting the lighting in the single- and multi-illuminant scenes as the concept of style. To verify this idea, we introduce an enhanced auto white-balance (AWB) method that models the lighting in single- and mixed-illuminant scenes as the style factor. Our AWB method does not require any illumination estimation step, yet contains a network learning to generate the weighting maps of the images with different WB settings. Proposed network utilizes the style information, extracted from the scene by a multi-head style extraction module. AWB correction is completed after blending these weighting maps and the scene. Experiments on single- and mixed-illuminant datasets demonstrate that our proposed method achieves promising correction results when compared to the recent works. This shows that the lighting in the scenes with multiple illuminations can be modeled by the concept of style. Source code and trained models are available on https://github.com/birdortyedi/lighting-as-style-awb-correction.",https://openaccess.thecvf.com/content/WACV2023/html/Kinli_Modeling_the_Lighting_in_Scenes_As_Style_for_Auto_White-Balance_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kinli_Modeling_the_Lighting_in_Scenes_As_Style_for_Auto_White-Balance_WACV_2023_paper.pdf,,https://github.com/birdortyedi/lighting-as-style-awb-correction,,main,Poster,https://ieeexplore.ieee.org/document/10030573/,"['Photography', 'Computer vision', 'Image color analysis', 'Source coding', 'Lighting', 'Estimation', 'Rendering (computer graphics)']","['Auto White Balance', 'Feature Space', 'Weight Map', 'Statistical Methods', 'Convolutional Neural Network', 'High-resolution Images', 'Final Output', 'Raw Images', 'Natural Images', 'Learning Mechanisms', 'Patch Size', 'Pre-trained Network', 'Small Imaging', 'Version Of Image', 'Gamma Correction', 'Post-processing Methods', 'Pre-defined Set', 'Affine Parameter', 'Style Image', 'Style Factors', 'Final Objective Function', 'Post-processing Operations', 'Latent Space', 'Daylight', 'Encoder Layer', 'Feature Maps', 'Output Image', 'Effects Of Different Conditions']","['Algorithms: Computational photography', 'image and video synthesis']",4,"Style may refer to different concepts (e.g. painting style, hairstyle, texture, color, filter, etc.) depending on how the feature space is formed. In this work, we propose a novel idea of interpreting the lighting in the single- and multi-illuminant scenes as the concept of style. To verify this idea, we introduce an enhanced auto white-balance (AWB) method that models the lighting in single- and mixed-illuminant scenes as the style factor. Our AWB method does not require any illumination estimation step, yet contains a network learning to generate the weighting maps of the images with different WB settings. Proposed network utilizes the style information, extracted from the scene by a multi-head style extraction module. AWB correction is completed after blending these weighting maps and the scene. Experiments on single- and mixed-illuminant datasets demonstrate that our proposed method achieves promising correction results when compared to the recent works. This shows that the lighting in the scenes with multiple illuminations can be modeled by the concept of style. Source code and trained models are available on https://github.com/birdortyedi/lighting-as-style-awb-correction."
MonoDVPS: A Self-Supervised Monocular Depth Estimation Approach to Depth-Aware Video Panoptic Segmentation,"Andra Petrovai, Sergiu Nedevschi","Technical University of Cluj-Napoca, Cluj-Napoca, Romania",100,Romania,0,,"Depth-aware video panoptic segmentation tackles the inverse projection problem of restoring panoptic 3D point clouds from video sequences, where the 3D points are augmented with semantic classes and temporally consistent instance identifiers. We propose a novel solution with a multi-task network that performs monocular depth estimation and video panoptic segmentation. Since acquiring ground truth labels for both depth and image segmentation has a relatively large cost, we leverage the power of unlabeled video sequences with self-supervised monocular depth estimation and semi-supervised learning from pseudo-labels for video panoptic segmentation. To further improve the depth prediction, we introduce panoptic-guided depth losses and a novel panoptic masking scheme for moving objects to avoid corrupting the training signal. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that our model with the proposed improvements achieves competitive results and fast inference speed.",https://openaccess.thecvf.com/content/WACV2023/html/Petrovai_MonoDVPS_A_Self-Supervised_Monocular_Depth_Estimation_Approach_to_Depth-Aware_Video_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Petrovai_MonoDVPS_A_Self-Supervised_Monocular_Depth_Estimation_Approach_to_Depth-Aware_Video_WACV_2023_paper.pdf,,,2210.07577,main,Poster,https://ieeexplore.ieee.org/document/10030189/,"['Training', 'Image segmentation', 'Three-dimensional displays', 'Motion segmentation', 'Video sequences', 'Semantics', 'Estimation']","['Depth Estimation', 'Monocular Depth Estimation', 'Panoptic Segmentation', 'Self-supervised Monocular Depth Estimation', 'Video Panoptic Segmentation', '3D Point', 'Video Sequences', 'Semi-supervised Learning', 'Temporal Consistency', 'Depth Prediction', 'Multi-task Network', 'Masking Strategy', 'Training Set', 'Network Training', 'Intersection Over Union', 'Semantic Segmentation', 'Depth Map', 'Optical Flow', 'Object Motion', 'Sequence Of Frames', 'Triplet Loss', 'Ego-motion', 'Adjacent Frames', 'Current Frame', 'Target Frame', 'Smoothness Loss', 'Original Resolution', 'Adjacent Pixels', 'Reprojection Error', 'Camera Pose']",['Algorithms: 3D computer vision'],1,"Depth-aware video panoptic segmentation tackles the inverse projection problem of restoring panoptic 3D point clouds from video sequences, where the 3D points are augmented with semantic classes and temporally consistent instance identifiers. We propose a novel solution with a multi-task network that performs monocular depth estimation and video panoptic segmentation. Since acquiring ground truth labels for both depth and image segmentation has a relatively large cost, we leverage the power of unlabeled video sequences with self-supervised monocular depth estimation and semi-supervised learning from pseudo-labels for video panoptic segmentation. To further improve the depth prediction, we introduce panoptic-guided depth losses and a novel panoptic masking scheme for moving objects to avoid corrupting the training signal. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that our model with the proposed improvements achieves competitive results and fast inference speed."
MonoEdge: Monocular 3D Object Detection Using Local Perspectives,"Minghan Zhu, Lingting Ge, Panqu Wang, Huei Peng",University of Michigan; TuSimple Inc,100,USA,0,,"We propose a novel approach for monocular 3D object detection by leveraging local perspective effects of each object. While the global perspective effect shown as size and position variations has been exploited for monocular 3D detection extensively, the local perspectives has long been overlooked. We propose a new regression target named keyedge-ratios as the parameterization of the local shape distortion to account for the local perspective, and derive the object depth and yaw angle from it. Theoretically, this approach does not rely on the absolute size or position of the objects in the image, therefore independent of the camera intrinsic parameters. This approach provides a new perspective for monocular 3D reasoning and can be plugged in flexibly to existing monocular 3D object detection frameworks. We demonstrate effectiveness and superior performance over strong baseline methods in multiple datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Zhu_MonoEdge_Monocular_3D_Object_Detection_Using_Local_Perspectives_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhu_MonoEdge_Monocular_3D_Object_Detection_Using_Local_Perspectives_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030978/,"['Visualization', 'Three-dimensional displays', 'Shape', 'Estimation', 'Object detection', 'Network architecture', 'Distortion']","['Local Perspectives', '3D Object Detection', 'Monocular 3D Object Detection', 'Image Object', 'Global Perspective', 'Image Position', 'Local Module', 'Intrinsic Parameters', 'Yaw Angle', 'Local Distortion', '3D Detection', 'Camera Intrinsic Parameters', 'Object Depth', 'Camera Intrinsics', 'Training Set', 'Validation Set', 'Quantitative Results', 'Bounding Box', 'Global Information', 'Viewing Angle', 'Object Appearance', 'Depth Estimation', 'KITTI Dataset', 'Object Height', 'Detection Confidence', '3D Bounding Box', 'Depth Prediction', 'Physical Size', 'Two-stage Network', 'Pixel Position']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Robotics']",6,"We propose a novel approach for monocular 3D object detection by leveraging local perspective effects of each object. While the global perspective effect shown as size and position variations has been exploited for monocular 3D detection extensively, the local perspectives has long been overlooked. We design a local perspective module to regress a newly defined variable named keyedge-ratios as the parameterization of the local shape distortion to account for the local perspective, and derive the object depth and yaw angle from it. Theoretically, this module does not rely on the pixel-wise size or position in the image of the objects, therefore independent of the camera intrinsic parameters. By plugging this module in existing monocular 3D object detection frameworks, we incorporate the local perspective distortion with global perspective effect for monocular 3D reasoning, and we demonstrate the effectiveness and superior performance over strong baseline methods in multiple datasets."
More Control for Free! Image Synthesis With Semantic Diffusion Guidance,"Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell",2Picsart AI Research (PAIR); 1UC Berkeley4The University of Hong Kong; 2Picsart AI Research (PAIR)3University of Oregon; 1UC Berkeley,75,"Hong Kong, USA",25,USA,"Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from a reference image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We investigate fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores, without re-training the diffusion model. We explore CLIP-based language guidance as well as both content and style-based image guidance in a unified framework. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content reference image, and examples with both textual and image guidance.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf,xh-liu.github.io/sdg/,,2112.05744,main,Poster,https://ieeexplore.ieee.org/document/10030365/,"['Computer vision', 'Image synthesis', 'Annotations', 'Image matching', 'Semantics', 'Noise reduction', 'Probabilistic logic']","['Diffusion Model', 'Reference Image', 'Control Synthesis', 'Image Synthesis', 'Diverse Images', 'Unconditional Model', 'Image Guidance', 'Text Annotation', 'Instructional Text', 'Fine-tuned', 'Structural Information', 'Scaling Factor', 'Paired Data', 'Generative Adversarial Networks', 'Image Generation', 'Image Noise', 'Human Faces', 'Noisy Images', 'Forward Process', 'Gram Matrix', 'Curly Hair', 'StyleGAN', 'Image Encoder', 'Fine-grained Control', 'Guidance Signaling', 'Style Transfer', 'Framework Synthesis', 'Image Editing', 'Backward Process', 'Feature Maps']","['Algorithms: Computational photography', 'image and video synthesis', 'Vision + language and/or other modalities']",79,"Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from a reference image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We investigate fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores, without re-training the diffusion model. We explore CLIP-based language guidance as well as both content and style-based image guidance in a unified framework. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content reference image, and examples with both textual and image guidance.
<sup>1</sup>"
"More Knowledge, Less Bias: Unbiasing Scene Graph Generation With Explicit Ontological Adjustment","Zhanwen Chen, Saed Rezayi, Sheng Li","School of Data Science, University of Virginia; School of Computing, University of Georgia",100,USA,0,,"Scene graph generation (SGG) models seek to detect relationships between objects in a given image. One challenge in this area is the biased distribution of predicates in the dataset and the semantic space. Recent works incorporating knowledge graphs with scene graphs prove effective in improving recall for the tail predicate classes. Moreover, many recent SGG approaches with promising results explicitly redistribute the predicates in both the training process and in the prediction step. To incorporate external knowledge, we construct a commonsense knowledge graph by integrating ConceptNet and Wikidata. To explicitly unbias SGG with knowledge in the reasoning process, we propose a novel framework, Explicit Ontological Adjustment (EOA), to adjust the graph model predictions with knowledge priors. We use the edge matrix from the commonsense knowledge graph as a module in the graph neural network model to refine the relationship detection process. This module proves effective in alleviating the long-tail distribution of predicates. When combined, we show that these modules achieve state-of-the-art performance on the Visual Genome dataset in most cases. The source code will be made publicly available.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_More_Knowledge_Less_Bias_Unbiasing_Scene_Graph_Generation_With_Explicit_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_More_Knowledge_Less_Bias_Unbiasing_Scene_Graph_Generation_With_Explicit_WACV_2023_paper.pdf,,https://github.com/zhanwenchen/eoa,,main,Poster,https://ieeexplore.ieee.org/document/10030879/,"['Training', 'Visualization', 'Solid modeling', 'Source coding', 'Image edge detection', 'Semantics', 'Knowledge based systems']","['Ontology', 'Unbiased', 'Scene Graph', 'Neural Network', 'Biased Distribution', 'Graph Neural Networks', 'Reasoning Process', 'External Knowledge', 'Semantic Space', 'Long-tailed Distribution', 'Commonsense Knowledge', 'Knowledge Base', 'Classification Task', 'Random Selection', 'Cognitive Effects', 'Conditional Probability', 'Confusion Matrix', 'Bounding Box', 'Generative Adversarial Networks', 'Types Of Edges', 'Visual Question Answering', 'Recall Metrics', 'Object Labels', 'Gated Recurrent Unit', 'Performance Gain', 'Total Direct Effect', 'Object Classification', 'Knowledge-based Approach', 'Relevant Baseline']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Vision + language and/or other modalities']",7,"Scene graph generation (SGG) models seek to detect relationships between objects in a given image. One challenge in this area is the biased distribution of predicates in the dataset and the semantic space. Recent works incorporating knowledge graphs with scene graphs prove effective in improving recall for the tail predicate classes. Moreover, many recent SGG approaches with promising results explicitly redistribute the predicates in both the training process and in the prediction step. To incorporate external knowledge, we construct a commonsense knowledge graph by integrating ConceptNet and Wikidata. To explicitly unbias SGG with knowledge in the reasoning process, we propose a novel framework, Explicit Ontological Adjustment (EOA), to adjust the graph model predictions with knowledge priors. We use the edge matrix from the commonsense knowledge graph as a module in the graph neural network model to refine the relationship detection process. This module proves effective in alleviating the long-tail distribution of predicates. When combined, we show that these modules achieve state-of-the-art performance on the Visual Genome dataset in most cases. The source code is available at https://github.com/zhanwenchen/eoa."
More Than Just Attention: Improving Cross-Modal Attentions With Contrastive Constraints for Image-Text Matching,"Yuxiao Chen, Jianbo Yuan, Long Zhao, Tianlang Chen, Rui Luo, Larry Davis, Dimitris N. Metaxas","Rutgers University; Amazon.com Services, Inc",50,USA,50,USA,"Cross-modal attention mechanisms have been widely applied to the image-text matching task and have achieved remarkable improvements thanks to their capability of learning fine-grained relevance across different modalities. However, the cross-modal attention models of existing methods could be sub-optimal and inaccurate because there is no direct supervision provided during the training process. In this work, we propose two novel training strategies, namely Contrastive Content Re-sourcing (CCR) and Contrastive Content Swapping (CCS) constraints, to address such limitations. These constraints supervise the training of cross-modal attention models in a contrastive learning manner without requiring explicit attention annotations. They are plug-in training strategies and can be generally integrated into existing cross-modal attention models. Additionally, we introduce three metrics, including Attention Precision, Recall, and F1-Score, to quantitatively measure the quality of learned attention models. We evaluate the proposed constraints by incorporating them into four state-of-the-art cross-modal attention-based image-text matching models. Experimental results on both Flickr30k and MS-COCO datasets demonstrate that integrating these constraints generally improves the model performance in terms of both retrieval performance and attention metrics.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_More_Than_Just_Attention_Improving_Cross-Modal_Attentions_With_Contrastive_Constraints_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_More_Than_Just_Attention_Improving_Cross-Modal_Attentions_With_Contrastive_Constraints_WACV_2023_paper.pdf,,,2105.09597,main,Poster,https://ieeexplore.ieee.org/document/10030207/,"['Training', 'Measurement', 'Visualization', 'Computer vision', 'Annotations', 'Computational modeling', 'Question answering (information retrieval)']","['Image-text Matching', 'Learning Models', 'Model Quality', 'Attention Model', 'Direct Supervision', 'MS COCO Dataset', 'Similarity Score', 'Image Regions', 'Intersection Over Union', 'Small Weight', 'Large Weight', 'Relevant Regions', 'Image Retrieval', 'Attention Map', 'Individual Words', 'Attention Weights', 'Noun Phrase', 'Additional Annotations', 'Low Attention', 'Text Fragments', 'Key Fragments', 'Relevant Fragments', 'Text Encoder', 'Visual Question Answering', 'Image Retrieval Task']",['Algorithms: Vision + language and/or other modalities'],3,"Cross-modal attention mechanisms have been widely applied to the image-text matching task. They have achieved remarkable improvements thanks to their capability of learning fine-grained relevance across different modalities. However, the cross-modal attention models of existing methods could be sub-optimal and inaccurate because there is no direct supervision provided during the training process. In this work, we propose two novel training strategies, namely Contrastive Content Resourcing (CCR) and Contrastive Content Swapping (CCS) constraints, to address such limitations. These constraints supervise the training of cross-modal attention models in a contrastive learning manner without requiring explicit attention annotations. They are plug-in training strategies and can be generally integrated into existing cross-modal attention models. Additionally, we introduce three metrics, including Attention Precision, Recall, and F1-Score, to quantitatively measure the quality of learned attention models. We evaluate the proposed constraints by incorporating them into four state- of-the-art cross-modal attention-based image-text matching models. Experimental results on both Flickr30k and MS-COCO datasets demonstrate that integrating these constraints generally improves the model performance in terms of both retrieval performance and attention metrics."
Motif Mining: Finding and Summarizing Remixed Image Content,"William Theisen, Daniel Gonzalez Cedre, Zachariah Carmichael, Daniel Moreira, Tim Weninger, Walter Scheirer","Department of Computer Science and Engineering, University of Notre Dame",100,USA,0,,"On the internet, images are no longer static; they have become dynamic content. Thanks to the availability of smartphones with cameras and easy-to-use editing software, images can be remixed (i.e., redacted, edited, and recombined with other content) on-the-fly and with a worldwide audience that can repeat the process. From digital art to memes, the evolution of images through time is now an important topic of study for digital humanists, social scientists, and media forensics specialists. However, because typical data sets in computer vision are composed of static content, the development of automated algorithms to analyze remixed content has been limited. In this paper, we introduce the idea of Motif Mining -- the process of finding and summarizing remixed image content in large collections of unlabeled and unsorted data. In this paper, this idea is formalized and a reference implementation is introduced. Experiments are conducted on three meme-style data sets, including a newly collected set associated with the information war in the Russo-Ukrainian conflict. The proposed motif mining approach is able to identify related remixed content that, when compared to similar approaches, more closely aligns with the preferences and expectations of human observers.",https://openaccess.thecvf.com/content/WACV2023/html/Theisen_Motif_Mining_Finding_and_Summarizing_Remixed_Image_Content_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Theisen_Motif_Mining_Finding_and_Summarizing_Remixed_Image_Content_WACV_2023_paper.pdf,,,2203.08327,main,Poster,https://ieeexplore.ieee.org/document/10030753/,"['Computer vision', 'Social networking (online)', 'Pipelines', 'Observers', 'Media', 'Market research', 'Software']","['Motif Mining', 'Computer Vision', 'Human Observers', 'Unlabeled Data', 'Edition Software', 'Ukraine Crisis', 'Digital Humanities', 'Local Features', 'Clustering Algorithm', 'Combination Of Features', 'Image Dataset', 'Markov Chain Monte Carlo', 'Number Of Images', 'Global Features', 'Accuracy Scores', 'Images In Set', 'Similar Images', 'Distribution Of Images', 'Vertices', 'Cluster C', 'Content-based Image Retrieval', 'Spectral Clustering', 'Louvain Method', 'Duplicate Images', 'Global Descriptors', 'Graph Generation', 'Gallery Images', 'Rights Activists', 'Static Images', 'Image Features']","['Applications: Social good', 'Visualization']",3,"On the Internet, images are no longer static; they have become dynamic content. Thanks to the availability of smartphones with cameras and easy-to-use editing software, images can be remixed (i.e., redacted, edited, and re-combined with other content) on-the-fly, allowing a world-wide audience to repeat the process many times. From digital art to memes, the evolution of images through time is now an important topic of study for digital humanists, social scientists, and media forensics specialists. However, because typical data sets in computer vision are composed of static content, there has been limited development of automated algorithms for analyzing remixed content. In this paper, we propose the idea of Motif Mining: the process of finding and summarizing remixed image content in large collections of unlabeled and unsorted data. For the first time, this idea is formalized and a reference implementation grounded in that formalism is introduced. We conduct experiments on three meme-style data sets, including a newly collected set associated with the Russo-Ukrainian conflict. The proposed motif mining approach is able to identify related remixed content that, when compared to similar approaches, more closely aligns with the preferences and expectations of human observers."
Motion Aware Self-Supervision for Generic Event Boundary Detection,"Ayush K. Rai, Tarun Krishna, Julia Dietlmeier, Kevin McGuinness, Alan F. Smeaton, Noel E. O’Connor","Insight SFI Centre for Data Analytics, Dublin City University (DCU)",100,Ireland,0,,"The task of Generic Event Boundary Detection (GEBD) aims to detect moments in videos that are naturally perceived by humans as generic and taxonomy-free event boundaries. Modeling the dynamically evolving temporal and spatial changes in a video makes GEBD a difficult problem to solve. Existing approaches involve very complex and sophisticated pipelines in terms of architectural design choices, hence creating a need for more straightforward and simplified approaches. In this work, we address this issue by revisiting a simple and effective self-supervised method and augment it with a differentiable motion feature learning module to tackle the spatial and temporal diversities in the GEBD task. We perform extensive experiments on the challenging Kinetics-GEBD and TAPOS datasets to demonstrate the efficacy of the proposed approach compared to the other self-supervised state-of-the-art methods. We also show that this simple self-supervised approach learns motion features without any explicit motion-specific pretext task.",https://openaccess.thecvf.com/content/WACV2023/html/K._Motion_Aware_Self-Supervision_for_Generic_Event_Boundary_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/K._Motion_Aware_Self-Supervision_for_Generic_Event_Boundary_Detection_WACV_2023_paper.pdf,,https://github.com/...,2210.05574,main,Poster,https://ieeexplore.ieee.org/document/10030295/,"['Representation learning', 'Computer vision', 'Pipelines', 'Task analysis', 'Videos', 'Software development management']","['General Detection', 'Boundary Detection', 'General Boundary', 'Event Boundaries', 'Spatial Variation', 'Motion Features', 'Explicit Task', 'Pretext Task', 'Negative Samples', 'Representation Learning', 'Temporal Order', 'Action Recognition', 'Optical Flow', 'Gaussian Blur', 'Self-supervised Learning', 'Motion Estimation', 'Video Segments', 'Temporal Cues', 'Temporal Boundaries', 'Contrast Objective', 'Self-supervised Learning Methods', 'Temporal Diversity', 'Precise Boundaries', 'Soft Labels', 'Video Modeling', 'Video Understanding', 'Online Fashion', 'Input RGB', 'Semantic', 'Appearance Features']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"The task of Generic Event Boundary Detection (GEBD) aims to detect moments in videos that are naturally perceived by humans as generic and taxonomy-free event boundaries. Modeling the dynamically evolving temporal and spatial changes in a video makes GEBD a difficult problem to solve. Existing approaches involve very complex and sophisticated pipelines in terms of architectural design choices, hence creating a need for more straightforward and simplified approaches. In this work, we address this issue by revisiting a simple and effective self-supervised method and augment it with a differentiable motion feature learning module to tackle the spatial and temporal diversities in the GEBD task. We perform extensive experiments on the challenging Kinetics-GEBD and TAPOS datasets to demonstrate the efficacy of the proposed approach compared to the other self-supervised state-of-the-art methods. We also show that this simple self-supervised approach learns motion features without any explicit motion-specific pretext task. Our results can be reproduced on $github$."
MovieCLIP: Visual Scene Recognition in Movies,"Digbalay Bose, Rajat Hebbar, Krishna Somandepalli, Haoyang Zhang, Yin Cui, Kree Cole-McLaughlin, Huisheng Wang, Shrikanth Narayanan","University of Southern California, Los Angeles, CA; Google",50,USA,50,USA,"Longform media such as movies have complex narrative structures, with events spanning a rich variety of ambient visual scenes. Domain-specific challenges associated with visual scenes in movies include transitions, person coverage, and a wide array of real-life and fictional scenarios. Existing visual scene datasets in movies have limited taxonomies and don't consider the visual scene transition within movie clips. In this work, we address the problem of visual scene recognition in movies by first automatically curating a new and extensive movie-centric taxonomy of 179 scene labels derived from movie scripts and auxiliary web-based video datasets. Instead of manual annotations which can be expensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips based on our proposed taxonomy. We provide baseline visual models trained on the weakly labeled dataset called MovieCLIP and evaluate them on an independent dataset verified by human raters. We show that leveraging features from models pretrained on MovieCLIP benefits downstream tasks such as multi-label scene and genre classification of web videos and movie trailers.",https://openaccess.thecvf.com/content/WACV2023/html/Bose_MovieCLIP_Visual_Scene_Recognition_in_Movies_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bose_MovieCLIP_Visual_Scene_Recognition_in_Movies_WACV_2023_paper.pdf,,,2210.11065,main,Poster,https://ieeexplore.ieee.org/document/10030882/,"['Visualization', 'Computer vision', 'Annotations', 'Computational modeling', 'Taxonomy', 'Manuals', 'Media']","['Object Recognition', 'Visual Recognition', 'Visual Scene', 'Visual Scene Recognition', 'Video Dataset', 'Variety Of Scenes', 'Movie Clips', 'Movie Scenes', 'Time Of Day', 'Greater Than Or Equal', 'Transformer Model', 'Validation Purposes', 'Scene Images', 'Human Experts', 'Natural Scenes', 'Mean Average Precision', 'Scene Classification', 'Movie Frames', 'Human-in-the-loop', 'Learning Rate Of 1e-4', 'Text Encoder', 'Similar Labeling', 'Auxiliary Source', 'Cockpit', 'Label Space', 'YouTube Channel', 'Pretext Task', 'Transformer']","['Applications: Arts/games/social media', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",6,"Longform media such as movies have complex narrative structures, with events spanning a rich variety of ambient visual scenes. Domain specific challenges associated with visual scenes in movies include transitions, person coverage, and a wide array of real-life and fictional scenarios. Existing visual scene datasets in movies have limited taxonomies and don’t consider the visual scene transition within movie clips. In this work, we address the problem of visual scene recognition in movies by first automatically curating a new and extensive movie-centric taxonomy of 179 scene labels derived from movie scripts and auxiliary web-based video datasets. Instead of manual annotations which can be expensive, we use CLIP to weakly label 1.12 million shots from 32K movie clips based on our proposed taxonomy. We provide baseline visual models trained on the weakly labeled dataset called MovieCLIP and evaluate them on an independent dataset verified by human raters. We show that leveraging features from models pretrained on MovieCLIP benefits downstream tasks such as multi-label scene and genre classification of web videos and movie trailers."
Multi-Frame Attention With Feature-Level Warping for Drone Crowd Tracking,"Takanori Asanomi, Kazuya Nishimura, Ryoma Bise","Kyushu University, Fukuoka, Japan",100,Japan,0,,"Drone crowd tracking has various applications such as crowd management and video surveillance. Unlike in general multi-object tracking, the size of the objects to be tracked are small, and the ground truth is given by a point-level annotation, which has no region information. This causes the lack of discriminative features for finding the same objects from many similar objects. Thus, similarity-based trackingtechniques, which are widely used for multi-object tracking with bounding-box, are difficult to use. To deal with this problem, we take into account the temporal context of the local area. To aggregate temporal context in a local area, we propose a multi-frame attention with feature-level warping. The feature-level warping can align the features of the same object in multiple frame, and then multi-frame attention can effectively aggregate the temporal context from the warped features. The experimental results show the effectiveness of our method. Our method outperformed the state-of-the-art method in DroneCrowd dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Asanomi_Multi-Frame_Attention_With_Feature-Level_Warping_for_Drone_Crowd_Tracking_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Asanomi_Multi-Frame_Attention_With_Feature-Level_Warping_for_Drone_Crowd_Tracking_WACV_2023_paper.pdf,,https://github.com/asanomitakanori/mfa-feature-warping,,main,Poster,https://ieeexplore.ieee.org/document/10030745/,"['Computer vision', 'Head', 'Codes', 'Annotations', 'Aggregates', 'Video surveillance', 'Object tracking']","['Crowd Tracking', 'Bounding Box', 'Object Features', 'Object Size', 'Temporal Context', 'Multiple Frames', 'Video Surveillance', 'Multi-object Tracking', 'False Negative', 'Image Features', 'Input Image', 'Feature Maps', 'Detection Results', 'Localization Performance', 'Spatial Context', 'Tracking Performance', 'Object Position', 'Pose Estimation', 'Video Capture', 'Bird’s Eye', 'Object In Frame', 'Motion Estimation', 'Siamese Network', 'Minimum Cost Flow', 'Personal Appearance', 'Vision Transformer', 'Map Position', 'Temporal Attention', 'Object Detection', 'Object Motion']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Drone crowd tracking has various applications such as crowd management and video surveillance. Unlike in general multi-object tracking, the size of the objects to be tracked are small, and the ground truth is given by a point-level annotation, which has no region information. This causes the lack of discriminative features for finding the same objects from many similar objects. Thus, similarity-based tracking techniques, which are widely used for multi-object tracking with bounding-box, are difficult to use. To deal with this problem, we take into account the temporal context of the local area. To aggregate temporal context in a local area, we propose a multi-frame attention with feature-level warping. The feature-level warping can align the features of the same object in multiple frames, and then multi-frame attention can effectively aggregate the temporal context from the warped features. The experimental results show the effectiveness of our method. Our method outperformed the state-of-the-art method in DroneCrowd dataset. The code is publicly available in https://github.com/asanomitakanori/mfa-feature-warping."
Multi-Level Contrastive Learning for Self-Supervised Vision Transformers,"Shentong Mo, Zhun Sun, Chao Li","Center for Advanced Intelligence Project (AIP), RIKEN; Carnegie Mellon University; Tohoku University",100,"Japan, USA",0,,"Recent studies aim to establish contrastive self-supervised learning (CSL) algorithms specialized for the family of Vision Transformers (ViTs) to make them function normally as ordinary convolutional-based backbones in the training progress. Despite obtaining promising performance on related downstream tasks, one compelling property of the ViTs is ignored in those approaches. As previous studies have demonstrated, vision transformers benefit from the early stage global attention mechanics, obtaining feature representations that contain information from distant patches, even in their shallow layers. Motivated by this, we present a simple yet effective framework to facilitate the self-supervised feature learning of transformer-based vision architectures, namely, Multi-level Contrastive learning for Vision Transformers (MCVT). Specifically, we equip the vision transformers with individual-based (InfoNCE) and prototypical-based (ProtoNCE) contrastive loss in different stages of the architecture to capture low-level invariance and high-level invariance between views of samples, respectively. We conduct extensive experiments to demonstrate the effectiveness of the proposed method, using two well-known vision transformer backbones, on several vision downstream tasks, including linear classification, detection, and semantic segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Mo_Multi-Level_Contrastive_Learning_for_Self-Supervised_Vision_Transformers_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mo_Multi-Level_Contrastive_Learning_for_Self-Supervised_Vision_Transformers_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030962/,"['Training', 'Representation learning', 'Computer vision', 'Head', 'Semantic segmentation', 'Self-supervised learning', 'Computer architecture']","['Self-supervised Learning', 'Vision Transformer', 'Feature Representation', 'Semantic Segmentation', 'Linear Classifier', 'Shallow Layers', 'Contrastive Loss', 'Convolutional Neural Network', 'Batch Size', 'Image Classification', 'Deeper Layers', 'Object Detection', 'Multilayer Perceptron', 'Characteristics Of Cases', 'High-level Features', 'Low-level Features', 'Global View', 'Training Examples', 'Meaningful Representation', 'Top-5 Accuracy', 'Instance Segmentation', 'Transformer Block', 'Top-1 Accuracy', 'Invariant Features', 'Concurrent Work', 'Problem Setup', 'Performance Gain', 'Learning Rate', 'Data Augmentation']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Recent studies aim to establish contrastive self-supervised learning (CSL) algorithms specialized for the family of Vision Transformers (ViTs) to make them function normally as ordinary convolutional-based backbones in the training progress. Despite obtaining promising performance on related downstream tasks, one compelling property of the ViTs is ignored in those approaches. As previous studies have demonstrated, vision transformers benefit from the early stage global attention mechanics, obtaining feature representations that contain information from distant patches, even in their shallow layers. Motivated by this, we present a simple yet effective framework to facilitate the self-supervised feature learning of transformer based vision architectures, namely, Multi-level Contrastive learning for Vision Transformers (MCVT). Specifically, we equip the vision transformers with individual-based (InfoNCE) and prototypical-based (ProtoNCE) contrastive loss in different stages of the architecture to capture low-level invariance and high-level invariance between views of samples, respectively. We conduct extensive experiments to demonstrate the effectiveness of the proposed method, using two well-known vision transformer backbones, on several vision downstream tasks, including linear classification, detection, and semantic segmentation."
Multi-Scale Cell-Based Layout Representation for Document Understanding,"Yuzhi Shi, Mijung Kim, Yeongnam Chae",Rakuten Institute of Technology; Chubu University,100,Japan,0,,"Deep learning techniques have achieved remarkable progress in document understanding. Most models use coordinates to represent absolute or relative spatial information of components, but they are difficult to represent latent rules in the document layout. This makes learning layout representation to be more difficult. Unlike the previous researches which have employed the coordinate system, graph or grid to represent the document layout, we propose a novel layout representation, the cell-based layout, to provide easy-to-understand spatial information for backbone models. In line with human reading habits, it uses cell information, i.e. row and column index, to represent the position of components in a document, and makes the document layout easier to understand. Furthermore, we proposed the multi-scale layout to represent the hierarchical structure of layout, and developed a data augmentation method to improve the performance. Experiment results show that our method achieves the state-of-the-art performance in text-based tasks, including form understanding and receipt understanding, and improves the performance in image-based task such as document image classification. We released the code in the repo.",https://openaccess.thecvf.com/content/WACV2023/html/Shi_Multi-Scale_Cell-Based_Layout_Representation_for_Document_Understanding_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shi_Multi-Scale_Cell-Based_Layout_Representation_for_Document_Understanding_WACV_2023_paper.pdf,,https://github.com/mijungkim-rakuten/multi-scale-cell-based,,main,Poster,https://ieeexplore.ieee.org/document/10030432/,"['Deep learning', 'Computer vision', 'Adaptation models', 'Codes', 'Computational modeling', 'Layout', 'Graphics processing units']","['Layout Representation', 'Deep Learning', 'Hierarchical Structure', 'Spatial Information', 'Related Information', 'Data Augmentation', 'Information Of Cells', 'Column Index', 'Position Components', 'Row Index', 'Backbone Model', 'F1 Score', 'Attention Mechanism', 'Image Information', 'Bounding Box', 'Spatial Representation', 'Text Classification', 'Top Left Corner', 'Row Column', 'Named Entity Recognition', 'Optical Character Recognition', 'Position Representation', 'Latent Information', 'Blocks Of Text', 'Multimodal Tasks', 'Camera Motion', 'Rule-based Approach', 'Coordinate Range', 'Text Lines']",,1,"Deep learning techniques have achieved remarkable progress in document understanding. Most models use co-ordinates to represent absolute or relative spatial information of components, but they are difficult to represent latent rules in the document layout. This makes learning layout representation to be more difficult. Unlike the previous researches which have employed the coordinate system, graph or grid to represent the document layout, we propose a novel layout representation, the cell-based layout, to provide easy-to-understand spatial information for backbone models. In line with human reading habits, it uses cell information, i.e. row and column index, to represent the position of components in a document, and makes the document layout easier to understand. Furthermore, we proposed the multi-scale layout to represent the hierarchical structure of layout, and developed a data augmentation method to improve the performance. Experiment results show that our method achieves the state-of-the-art performance in text-based tasks, including form understanding and receipt understanding, and improves the performance in image-based task such as document image classification. We released the code in the repo
<sup>a</sup>
."
Multi-Scale Contrastive Learning for Complex Scene Generation,"Hanbit Lee, Youna Kim, Sang-goo Lee","Seoul National University, Seoul, Korea",100,South Korea,0,,"Recent advances in Generative Adversarial Networks (GANs) have enabled photo-realistic synthesis of single object images. Yet, modeling more complex distributions, such as scenes with multiple objects, remains challenging. The difficulty stems from the incalculable variety of scene configurations which contain multiple objects of different categories placed at various locations. In this paper, we aim to alleviate the difficulty by enhancing the discriminative ability of the discriminator through a locally defined self-supervised pretext task. To this end, we design a discriminator to leverage multi-scale local feedback that guides the generator to better model local semantic structures in the scene. Then, we require the discriminator to carry out pixel-level contrastive learning at multiple scales to enhance discriminative capability on local regions. Experimental results on several challenging scene datasets show that our method improves the synthesis quality by a substantial margin compared to state-of-the-art baselines.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Multi-Scale_Contrastive_Learning_for_Complex_Scene_Generation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Multi-Scale_Contrastive_Learning_for_Complex_Scene_Generation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030436/,"['Computer vision', 'Semantics', 'Generative adversarial networks', 'Generators', 'Data models', 'Task analysis']","['Self-supervised Learning', 'Local Structure', 'Multiple Scales', 'Generative Adversarial Networks', 'Multiple Objects', 'Complex Distribution', 'Local Feedback', 'Scene Structure', 'Pretext Task', 'Synthesis Quality', 'Self-supervised Task', 'Feature Maps', 'Positive Features', 'Representation Learning', 'Distance Threshold', 'Multi-scale Features', 'Scene Images', 'Objective Quality', 'Objects In The Scene', 'Irrelevant Features', 'Generative Adversarial Networks Training', 'Fake Images', 'Contrastive Loss', 'Generative Adversarial Networks Model', 'Auxiliary Task', 'Differential Transformation', 'Image X', 'Consistency Regularization', 'Global Representation', 'Classification Head']","['Algorithms: Computational photography', 'image and video synthesis']",2,"Recent advances in Generative Adversarial Networks (GANs) have enabled photo-realistic synthesis of single object images. Yet, modeling more complex distributions, such as scenes with multiple objects, remains challenging. The difficulty stems from the incalculable variety of scene configurations which contain multiple objects of different categories placed at various locations. In this paper, we aim to alleviate the difficulty by enhancing the discriminative ability of the discriminator through a locally defined self-supervised pretext task. To this end, we design a discriminator to leverage multi-scale local feedback that guides the generator to better model local semantic structures in the scene. Then, we require the discriminator to carry out pixel-level contrastive learning at multiple scales to enhance discriminative capability on local regions. Experimental results on several challenging scene datasets show that our method improves the synthesis quality by a substantial margin compared to state-of-the-art baselines."
Multi-View Action Recognition Using Contrastive Learning,"Ketul Shah, Anshul Shah, Chun Pong Lau, Celso M. de Melo, Rama Chellappa",DEVCOM Army Research Laboratory; Johns Hopkins University,50,USA,50,USA,"In this work, we present a method for RGB-based action recognition using multi-view videos. We present a supervised contrastive learning framework to learn a feature embedding robust to changes in viewpoint, by effectively leveraging multi-view data. We use an improved supervised contrastive loss and augment the positives with those coming from synchronized viewpoints. We also propose a new approach to use classifier probabilities to guide the selection of hard negatives in the contrastive loss, to learn a more discriminative representation. Negative samples from confusing classes based on posterior are weighted higher. We also show that our method leads to better domain generalization compared to the standard supervised training based on synthetic multi-view data. Extensive experiments on real (NTU-60, NTU-120, NUMA) and synthetic (RoCoG) data demonstrate the effectiveness of our approach.",https://openaccess.thecvf.com/content/WACV2023/html/Shah_Multi-View_Action_Recognition_Using_Contrastive_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shah_Multi-View_Action_Recognition_Using_Contrastive_Learning_WACV_2023_paper.pdf,,https://github.com/kshah33/ViewCon,,main,Poster,https://ieeexplore.ieee.org/document/10031025/,"['Training', 'Computer vision', 'Synchronization', 'Standards', 'Videos', 'Synthetic data']","['Action Recognition', 'Self-supervised Learning', 'Synchronization', 'Negative Samples', 'Extensive Experiments', 'Domain Generalization', 'Contrastive Loss', 'Viewpoint Changes', 'Multi-view Data', 'Loss Function', 'Gestures', 'Positive Samples', 'Hardness', 'Feature Space', 'Small Datasets', 'Feature Learning', 'Data Augmentation', 'Additional Modifications', 'Transfer Learning', 'Large-scale Datasets', 'Frontal View', 'Multiple Viewpoints', 'Learning Settings', 'Pose Estimation', 'Representation Learning', 'Color Jittering', 'Privacy Issues', 'Action Classes', 'Video Capture', 'Synthetic Training Data']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",14,"In this work, we present a method for RGB-based action recognition using multi-view videos. We present a supervised contrastive learning framework to learn a feature embedding robust to changes in viewpoint, by effectively leveraging multi-view data. We use an improved supervised contrastive loss and augment the positives with those coming from synchronized viewpoints. We also propose a new approach to use classifier probabilities to guide the selection of hard negatives in the contrastive loss, to learn a more discriminative representation. Negative samples from confusing classes based on posterior are weighted higher. We also show that our method leads to better domain generalization compared to the standard supervised training based on synthetic multi-view data. Extensive experiments on real (NTU-60, NTU-120, NUMA) and synthetic (RoCoG) data demonstrate the effectiveness of our approach."
Multi-View Photometric Stereo Revisited,"Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, Luc Van Gool",ETH Zürich; KU Leuven; Google Research,66.66666667,"Belgium, Switzerland",33.33333333,USA,"Multi-view photometric stereo (MVPS) is a preferred method for detailed and precise 3D acquisition of an object from images. Although popular methods for MVPS can provide outstanding results, they are often complex to execute and limited to isotropic material objects. To address such limitations, we present a simple, practical approach to MVPS, which works well for isotropic as well as other object material types such as anisotropic and glossy. The proposed approach in this paper exploits the benefit of uncertainty modeling in a deep neural network for a reliable fusion of photometric stereo (PS) and multi-view stereo (MVS) network predictions. Yet, contrary to the recently proposed state-of-the-art, we introduce neural volume rendering methodology for a trustworthy fusion of MVS and PS measurements. The advantage of introducing neural volume rendering is that it helps in the reliable modeling of objects with diverse material types, where existing MVS methods, PS methods, or both may fail. Furthermore, it allows us to work on neural 3D shape representation, which has recently shown outstanding results for many geometric processing tasks. Our suggested new loss function aims to fit the zero level set of the implicit neural function using the most certain MVS and PS network predictions coupled with weighted neural volume rendering cost. The proposed approach shows state-of-the-art results when tested extensively on several benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Kaya_Multi-View_Photometric_Stereo_Revisited_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kaya_Multi-View_Photometric_Stereo_Revisited_WACV_2023_paper.pdf,,,2210.0767,main,Poster,https://ieeexplore.ieee.org/document/10030947/,"['Solid modeling', 'Three-dimensional displays', 'Uncertainty', 'Shape', 'Volume measurement', 'Materials reliability', 'Benchmark testing']","['Multi-view Stereo', 'Photometric Stereo', 'Neural Network', 'Type Material', 'Model Uncertainty', 'Benchmark Datasets', 'Neural Representations', '3D Shape', 'Implicit Function', 'Shape Representation', '3D Acquisition', 'Volume Rendering', 'Light Source', '3D Reconstruction', 'Multilayer Perceptron', '3D Surface', 'Reconstruction Accuracy', '3D Position', 'Surface Profile', 'Anisotropic Materials', 'Surface Normals', 'Surface Anisotropy', 'Glossy Surface', 'Multi-view Images', 'Stereo Images', 'View Synthesis', 'Structure From Motion', 'Accurate 3D', 'Marching Cubes Algorithm', 'Multilayer Perceptron Layer']","['Algorithms: Low-level and physics-based vision', '3D computer vision']",5,"Multi-view photometric stereo (MVPS) is a preferred method for detailed and precise 3D acquisition of an object from images. Although popular methods for MVPS can provide outstanding results, they are often complex to execute and limited to isotropic material objects. To address such limitations, we present a simple, practical approach to MVPS, which works well for isotropic as well as other object material types such as anisotropic and glossy. The proposed approach in this paper exploits the benefit of uncertainty modeling in a deep neural network for a reliable fusion of photometric stereo (PS) and multi-view stereo (MVS) network predictions. Yet, contrary to the recently proposed state-of-the-art, we introduce neural volume rendering methodology for a trustworthy fusion of MVS and PS measurements. The advantage of introducing neural volume rendering is that it helps in the reliable modeling of objects with diverse material types, where existing MVS methods, PS methods, or both may fail. Furthermore, it allows us to work on neural 3D shape representation, which has recently shown outstanding results for many geometric processing tasks. Our suggested new loss function aims to fit the zero level set of the implicit neural function using the most certain MVS and PS network predictions coupled with weighted neural volume rendering cost. The proposed approach shows state-of-the-art results when tested extensively on several benchmark datasets."
Multi-View Tracking Using Weakly Supervised Human Motion Prediction,"Martin Engilberge, Weizhe Liu, Pascal Fua","EPFL, Lausanne, Switzerland; Tencent XR Vision Labs",50,Switzerland,50,China,"Multi-view approaches to people-tracking have the potential to better handle occlusions than single-view ones in crowded scenes. They often rely on the tracking-by-detection paradigm, which involves detecting people first and then connecting the detections. In this paper, we argue that an even more effective approach is to predict people motion over time and infer people's presence in individual frames from these. This enables to enforce consistency both over time and across views of a single temporal frame. We validate our approach on the PETS2009 and WILDTRACK datasets and demonstrate that it outperforms state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Engilberge_Multi-View_Tracking_Using_Weakly_Supervised_Human_Motion_Prediction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Engilberge_Multi-View_Tracking_Using_Weakly_Supervised_Human_Motion_Prediction_WACV_2023_paper.pdf,,https://github.com/cvlab-epfl/MVFlow,2210.10771,main,Poster,https://ieeexplore.ieee.org/document/10030506/,"['Computer vision', 'Tracking', 'Computational modeling']","['Human Motion', 'Multi-view Tracking', 'Single Frame', 'Crowded Scenes', 'Time Step', 'Convolutional Layers', 'Object Detection', 'Parametrized', 'Shortest Path', 'Video Frames', 'Ground Plane', 'Video Sequences', 'Network Flow', 'Temporal Distance', 'Tracking Problem', 'Spatial Aggregation', 'Project Characteristics', 'Non-maximum Suppression', 'Temporal Consistency', 'Flow Prediction', 'Multiple Object Tracking', 'People Tracking', 'Training Frames', 'Multiple Viewpoints', 'Assembly Mechanisms']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"Multi-view approaches to people-tracking have the potential to better handle occlusions than single-view ones in crowded scenes. They often rely on the tracking-by-detection paradigm, which involves detecting people first and then connecting the detections. In this paper, we argue that an even more effective approach is to predict people motion over time and infer people’s presence in individual frames from these. This enables to enforce consistency both over time and across views of a single temporal frame. We validate our approach on the PETS2009 and WILDTRACK datasets and demonstrate that it outperforms state-of-the-art methods."
Multimodal Multi-Head Convolutional Attention With Various Kernel Sizes for Medical Image Super-Resolution,"Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Andreea-Iuliana Miron, Olivian Savencu, Nicolae-Cătălin Ristea, Nicolae Verga, Fahad Shahbaz Khan","MBZ University of Artificial Intelligence, UAE; “Carol Davila” University of Medicine and Pharmacy, Romania; Linköping University, Sweden; University Politehnica of Bucharest, Romania; University of Bucharest, Romania",100,"Romania, Sweden, UAE",0,,"Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques capture several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multimodal multi-head convolutional attention module to super-resolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention, and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for super-resolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads. Our code is freely available at https://github.com/lilygeorgescu/MHCA.",https://openaccess.thecvf.com/content/WACV2023/html/Georgescu_Multimodal_Multi-Head_Convolutional_Attention_With_Various_Kernel_Sizes_for_Medical_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Georgescu_Multimodal_Multi-Head_Convolutional_Attention_With_Various_Kernel_Sizes_for_Medical_WACV_2023_paper.pdf,,https://github.com/lilygeorgescu/MHCA,2204.04218,main,Poster,https://ieeexplore.ieee.org/document/10030757/,"['Convolutional codes', 'Head', 'Magnetic resonance imaging', 'Computed tomography', 'Superresolution', 'Magnetic heads', 'Data models']","['Medical Imaging', 'Atrous Convolution', 'Medical Image Super-resolution', 'Magnetic Resonance Imaging', 'Computed Tomography', 'Ablation', 'Magnetic Resonance Imaging Scans', 'Attention Mechanism', 'Receptive Field', 'Convolution Operation', 'Attention Module', 'Spatial Attention', 'Number Of Filters', 'Channel Attention', 'Single Investigator', 'Number Of Heads', 'Attention Heads', 'Input Tensor', 'Multiple Heads', 'Convolutional Attention Module', 'Structural Similarity Index Measure', 'Conv Layer', 'Convolutional Layers', 'Peak Signal-to-noise Ratio', 'Computer Vision Community', 'Magnetic Resonance Imaging Slices', 'Common Practice', 'Target Modality', 'Multiple Scans', '3D Scanning']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Biomedical/healthcare/medicine']",40,"Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques capture several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multi-modal multi-head convolutional attention module to super-resolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention, and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for super-resolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads. Our code is freely available at https://github.com/lilygeorgescu/MHCA."
Multimodal Vision Transformers With Forced Attention for Behavior Analysis,"Tanay Agrawal, Michal Balazia, Philipp Müller, François Brémond","INRIA, Valbonne, France; DFKI, Saarbrücken, Germany",50,France,50,Germany,"Human behavior understanding requires looking at minute details in the large context of a scene containing multiple input modalities. It is necessary as it allows the design of more human-like machines. While transformer approaches have shown great improvements, they face multiple challenges such as lack of data or background noise. To tackle these, we introduce the Forced Attention (FAt) Transformer which utilize forced attention with a modified backbone for input encoding and a use of additional inputs. In addition to improving the performance on different tasks and inputs, the modification requires less time and memory resources. We provide a model for a generalised feature extraction for tasks concerning social signals and behavior analysis. Our focus is on understanding behavior in videos where people are interacting with each other or talking into the camera which simulates the first person point of view in social interaction. FAt Transformers are applied to two downstream tasks: personality recognition and body language recognition. We achieve state-of-the-art results for Udiva v0.5, First Impressions v2 and MPII Group Interaction datasets. We further provide an extensive ablation study of the proposed architecture.",https://openaccess.thecvf.com/content/WACV2023/html/Agrawal_Multimodal_Vision_Transformers_With_Forced_Attention_for_Behavior_Analysis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agrawal_Multimodal_Vision_Transformers_With_Forced_Attention_for_Behavior_Analysis_WACV_2023_paper.pdf,,https://github.com/Parapompadoo/FAt-Transformers,,main,Poster,https://ieeexplore.ieee.org/document/10031002/,"['Analytical models', 'Visualization', 'Face recognition', 'Memory management', 'Transformers', 'Feature extraction', 'Behavioral sciences']","['Behavioral Analysis', 'Vision Transformer', 'Multimodal Transformer', 'Human Behavior', 'Background Noise', 'Group Interaction', 'Multiple Modalities', 'Social Significance', 'Personal Recognition', 'Face Multiple Challenges', 'Behavioral Video', 'Convolutional Layers', 'Batch Size', 'Metadata', 'Attention Mechanism', 'Weight Decay', 'Crop Rotation', 'Attention Module', 'Action Recognition', 'Segmentation Map', 'Input Patch', 'CNN Backbone', 'Early Fusion', 'Feature-level Fusion', 'Positional Encoding', 'Choice Of Dataset', 'Self-supervised Learning', 'Linear Layer', 'Pre-trained Weights', 'Dyadic Interactions']","['Applications: Psychology and cognitive science', 'Vision + language and/or other modalities', 'Biomedical/healthcare/medicine']",2,"Human behavior understanding requires looking at minute details in the large context of a scene containing multiple input modalities. It is necessary as it allows the design of more human-like machines. While transformer approaches have shown great improvements, they face multiple challenges such as lack of data or background noise. To tackle these, we introduce the Forced Attention (FAt) Transformer which utilize forced attention with a modified backbone for input encoding and a use of additional inputs. In addition to improving the performance on different tasks and inputs, the modification requires less time and memory resources. We provide a model for a generalised feature extraction for tasks concerning social signals and behavior analysis. Our focus is on understanding behavior in videos where people are interacting with each other or talking into the camera which simulates the first person point of view in social interaction. FAt Transformers are applied to two downstream tasks: personality recognition and body language recognition. We achieve state-of-the-art results for Udiva v0.5, First Impressions v2 and MPII Group Interaction datasets. We further provide an extensive ablation study of the proposed architecture."
Multivariate Probabilistic Monocular 3D Object Detection,"Xuepeng Shi, Zhixiang Chen, Tae-Kyun Kim","Imperial College London, KAIST; Imperial College London",100,"South Korea, UK",0,,"In autonomous driving, monocular 3D object detection is an important but challenging task. Towards accurate monocular 3D object detection, some recent methods recover the distance of objects from the physical height and visual height of objects. Such decomposition framework can introduce explicit constraints on the distance prediction, thus improving its accuracy and robustness. However, the inaccurate physical height and visual height prediction still may exacerbate the inaccuracy of the distance prediction. In this paper, we improve the framework by multivariate probabilistic modeling. We explicitly model the joint probability distribution of the physical height and visual height. This is achieved by learning a full covariance matrix of the physical height and visual height during training, with the guide of a multivariate likelihood. Such explicit joint probability distribution modeling not only leads to robust distance prediction when both the predicted physical height and visual height are inaccurate, but also brings learned covariance matrices with expected behaviors. The experimental results on the challenging Waymo Open and KITTI datasets show the effectiveness of our framework.",https://openaccess.thecvf.com/content/WACV2023/html/Shi_Multivariate_Probabilistic_Monocular_3D_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shi_Multivariate_Probabilistic_Monocular_3D_Object_Detection_WACV_2023_paper.pdf,,https://github.com/Rock-100/MonoDet,,main,Poster,https://ieeexplore.ieee.org/document/10030988/,"['Training', 'Visualization', 'Solid modeling', 'Three-dimensional displays', 'Object detection', 'Probabilistic logic', 'Probability distribution']","['Object Detection', '3D Object Detection', 'Monocular 3D Object Detection', 'Probabilistic 3D', 'Covariance Matrix', 'Explicit Model', 'Object Distance', 'Open Dataset', 'Height Distribution', 'KITTI Dataset', 'Accurate Object Detection', 'Explicit Constraints', 'Object Height', 'Decomposition Framework', 'Distance Prediction', 'Physical Height', 'Model Uncertainty', 'Bounding Box', 'Multivariate Distribution', '3D Bounding Box', 'Objective Accuracy', 'Yaw Angle', '3D Detection', 'Physical Size', 'Monocular Depth Estimation', '3D Head', 'Front Camera', 'Variational Autoencoder', 'Laplace Distribution']",['Algorithms: 3D computer vision'],12,"In autonomous driving, monocular 3D object detection is an important but challenging task. Towards accurate monocular 3D object detection, some recent methods recover the distance of objects from the physical height and visual height of objects. Such decomposition framework can introduce explicit constraints on the distance prediction, thus improving its accuracy and robustness. However, the inaccurate physical height and visual height prediction still may exacerbate the inaccuracy of the distance prediction. In this paper, we improve the framework by multivariate probabilistic modeling. We explicitly model the joint probability distribution of the physical height and visual height. This is achieved by learning a full covariance matrix of the physical height and visual height during training, with the guide of a multivariate likelihood. Such explicit joint probability distribution modeling not only leads to robust distance prediction when both the predicted physical height and visual height are inaccurate, but also brings learned covariance matrices with expected behaviors. The experimental results on the challenging Waymo Open and KITTI datasets show the effectiveness of our framework
<sup>1</sup>
."
Mutual Learning for Long-Tailed Recognition,"Changhwa Park, Junho Yim, Eunji Jun","42dot Inc.; Institute of Advanced Technology Development, Hyundai Motor Group; AI Technology Division, LG Energy Solution",33.33333333,South Korea,66.66666667,USA,"Deep neural networks perform well in artificially-balanced datasets, but real-world data often has a long-tailed distribution. Recent studies have focused on developing unbiased classifiers to improve tail class performance. Despite the efforts to learn a fine classifier, we cannot guarantee a solid performance if the representations are of poor quality. However, learning high-quality representations in a long-tailed setting is difficult because the features of tail classes easily overfit the training dataset. In this work, we propose a mutual learning framework that generates high-quality representations in long-tailed settings by exchanging information between networks. We show that the proposed method can improve representation quality and establish a new state-of-the-art record on several long-tailed recognition benchmark datasets, including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018.",https://openaccess.thecvf.com/content/WACV2023/html/Park_Mutual_Learning_for_Long-Tailed_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Park_Mutual_Learning_for_Long-Tailed_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030903/,"['Training', 'Deep learning', 'Computer vision', 'Image recognition', 'Neural networks', 'Tail', 'Benchmark testing']","['Mutual Learning', 'Long-tailed Recognition', 'Deep Neural Network', 'Quality Of Representations', 'Long-tailed Distribution', 'Learning Models', 'Learning Rate', 'Classification Accuracy', 'Test Dataset', 'Softmax', 'Cross-entropy Loss', 'Representation Learning', 'Linear Classifier', 'Distribution Of Dataset', 'Classification Loss', 'General Representation', 'Number Of Experts', 'Independent Learning', 'Linearly Separable', 'Independent Networks', 'Imbalance Ratio', 'Inference Phase', 'Independent Training', 'Label Distribution', 'Learning Rate Schedule']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"Deep neural networks perform well in artificially- balanced datasets, but real-world data often has a long-tailed distribution. Recent studies have focused on developing unbiased classifiers to improve tail class performance. Despite the efforts to learn a fine classifier, we cannot guarantee a solid performance if the representations are of poor quality. However, learning high-quality representations in a long-tailed setting is difficult because the features of tail classes easily overfit the training dataset. In this work, we propose a mutual learning framework that generates high- quality representations in long-tailed settings by exchanging information between networks. We show that the proposed method can improve representation quality and establish a new state-of-the-art record on several long-tailed recognition benchmark datasets, including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018."
My Face My Choice: Privacy Enhancing Deepfakes for Social Media Anonymization,"Umur A. Çiftçi, Gokturk Yuksek, İlke Demir",Intel Labs; Binghamton University,50,USA,50,USA,"Recently, productization of face recognition and identification algorithms have become the most controversial topic about ethical AI. As new policies around digital identities are formed, we introduce three face access models in a hypothetical social network, where the user has the power to only appear in photos they approve. Our approach eclipses current tagging systems and replaces unapproved faces with quantitatively dissimilar deepfakes. In addition, we propose new metrics specific for this task, where the deepfake is generated at random with a guaranteed dissimilarity. We explain access models based on strictness of the data flow, and discuss impact of each model on privacy, usability, and performance. We evaluate our system on Facial Descriptor Dataset as the real dataset, and two synthetic datasets with random and equal class distributions. Running seven SOTA face recognizers on our results, MFMC reduces the average accuracy by 61%. Lastly, we extensively analyze similarity metrics, deepfake generators, and datasets in structural, visual, and generative spaces; supporting the design choices and verifying the quality.",https://openaccess.thecvf.com/content/WACV2023/html/Ciftci_My_Face_My_Choice_Privacy_Enhancing_Deepfakes_for_Social_Media_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ciftci_My_Face_My_Choice_Privacy_Enhancing_Deepfakes_for_Social_Media_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030263/,"['Measurement', 'Deepfakes', 'Privacy', 'Data privacy', 'Social networking (online)', 'Face recognition', 'Task analysis']","['Social Media', 'Deepfake', 'Privacy Enhancing', 'Similarity Measure', 'Random Distribution', 'Face Recognition', 'Identification Algorithm', 'Recognition Algorithm', 'Face Identity', 'Digital Identity', 'Social Media Platforms', 'Skin Color', 'Generative Adversarial Networks', 'Target Image', 'Latent Space', 'Image Space', 'Source Images', 'Gaze Direction', 'Face Detection', 'Access Rights', 'Real Faces', 'Adversarial Attacks', 'Target Face', 'Storage Demand', 'Access Rules', 'StyleGAN', 'Head Pose', 'Inpainting', 'Social Graph', 'Set Of Faces']","['Applications: Social good', 'Biometrics', 'face', 'gesture', 'body pose', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",9,"Recently, productization of face recognition and identification algorithms have become the most controversial topic about ethical AI. As new policies around digital identities are formed [22], we introduce three face access models in a hypothetical social network, where the user has the power to only appear in photos they approve. Our approach eclipses current tagging systems and replaces unapproved faces with quantitatively dissimilar deepfakes. In addition, we propose new metrics specific for this task, where the deepfake is generated at random with a guaranteed dissimilarity. We explain access models based on strictness of the data flow, and discuss impact of each model on privacy, usability, and performance. We evaluate our system on Facial Descriptor Dataset [61] as the real dataset, and two synthetic datasets with random and equal class distributions. Running seven SOTA face recognizers on our results, MFMC reduces the average accuracy by 61%. Lastly, we extensively analyze similarity metrics, deepfake generators, and datasets in structural, visual, and generative spaces; supporting the design choices and verifying the quality."
NAPReg: Nouns As Proxies Regularization for Semantically Aware Cross-Modal Embeddings,"Bhavin Jawade, Deen Dayal Mohan, Naji Mohamed Ali, Srirangaraj Setlur, Venu Govindaraju","Department of Computer Science and Engineering, University at Buffalo, SUNY",100,USA,0,,"Cross-Modal retrieval is a fundamental vision-language task with a broad range of practical applications. Text-to-image matching is the most common form of cross-modal retrieval where given a large database of images and a textual query, the task is to retrieve the most relevant set of images. Existing methods utilize dual encoders with an attention mechanism and a ranking loss for learning embeddings that can be used for retrieval based on cosine similarity. Despite the fact that existing methods attempt to perform semantic alignment across visual regions and textual words using tailored attention mechanisms, there is no explicit supervision from the training objective to enforce such alignment. To address this, we propose NAPReg, a novel regularization formulation that projects high-level semantic entities i.e Nouns into the embedding space as shared learnable proxies. We show that using such a formulation allows the attention mechanism to learn better word-region alignment while also utilizing region information from other samples to build a more generalized latent representation for semantic concepts. Experiments on three benchmark datasets i.e. MS-COCO, Flickr30k and Flickr8k demonstrates that our method achieves state-of-the-art results in cross-modal metric learning for text-image and image-text retrieval tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Jawade_NAPReg_Nouns_As_Proxies_Regularization_for_Semantically_Aware_Cross-Modal_Embeddings_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jawade_NAPReg_Nouns_As_Proxies_Regularization_for_Semantically_Aware_Cross-Modal_Embeddings_WACV_2023_paper.pdf,,https://github.com/bhavinjawade/NAPReq,,main,Poster,https://ieeexplore.ieee.org/document/10030970/,"['Training', 'Measurement', 'Visualization', 'Computer vision', 'Codes', 'Databases', 'Semantics']","['Attention Mechanism', 'Benchmark Datasets', 'Latent Space', 'Text Words', 'Broad Range Of Applications', 'Training Objective', 'Metric Learning', 'Ranking Loss', 'Training Set', 'Objective Function', 'Image Features', 'Feature Representation', 'Qualitative Results', 'Similarity Score', 'Visual Representation', 'Visual Features', 'Image Regions', 'Representation Learning', 'Regularization Term', 'Words In Sentences', 'Salient Regions', 'Feature Alignment', 'Triplet Loss', 'Deep Metric Learning', 'Gated Recurrent Unit', 'Textual Features', 'Feature Aggregation', 'Test Set Of Images']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Cross-modal retrieval is a fundamental vision-language task with a broad range of practical applications. Text-to-image matching is the most common form of cross-modal retrieval where, given a large database of images and a textual query, the task is to retrieve the most relevant set of images. Existing methods utilize dual encoders with an attention mechanism and a ranking loss for learning embeddings that can be used for retrieval based on cosine similarity. Despite the fact that these methods attempt to perform semantic alignment across visual regions and textual words using tailored attention mechanisms, there is no explicit supervision from the training objective to enforce such alignment. To address this, we propose NAPReg, a novel regularization formulation that projects high-level semantic entities i.e Nouns into the embedding space as shared learnable proxies. We show that using such a formulation allows the attention mechanism to learn better word-region alignment while also utilizing region information from other samples to build a more generalized latent representation for semantic concepts. Experiments on three benchmark datasets i.e. MS-COCO, Flickr30k and Flickr8k demonstrate that our method achieves state-of-the-art results in cross-modal metric learning for text-image and image-text retrieval tasks. Code: https://github.com/bhavinjawade/NAPReq"
Nearest Neighbors Meet Deep Neural Networks for Point Cloud Analysis,"Renrui Zhang, Liuhui Wang, Ziyu Guo, Jianbo Shi","Peking University; Heisenberg Robotics, University of Pennsylvania; Peking University, Heisenberg Robotics",100,"China, USA",0,,"Performances on standard 3D point cloud benchmarks have plateaued, resulting in oversized models and complex network design to make a fractional improvement. We present an alternative to enhance existing deep neural networks without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter SN-Adapter. Building on any trained 3D network, we utilize its learned encoding capability to extract features of the training dataset and summarize them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter retrieves k nearest neighbors (k-NN) from the pre-constructed spatial prototypes and linearly interpolates the k-NN prediction with that of the original 3D network. By providing complementary characteristics, the proposed SN-Adapter serves as a plug-and-play module to economically improve performance in a non-parametric manner. More importantly, our SN-Adapter can be effectively generalized to various 3D tasks, including shape classification, part segmentation, and 3D object detection, demonstrating its superiority and robustness. We hope our approach could show a new perspective for point cloud analysis and facilitate future research.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Nearest_Neighbors_Meet_Deep_Neural_Networks_for_Point_Cloud_Analysis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Nearest_Neighbors_Meet_Deep_Neural_Networks_for_Point_Cloud_Analysis_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030087/,"['Point cloud compression', 'Knowledge engineering', 'Deep learning', 'Three-dimensional displays', 'Shape', 'Neural networks', 'Prototypes']","['Deep Neural Network', 'Point Cloud', 'Point Cloud Analysis', 'Interpolation', 'Training Dataset', 'Network Training', 'K-nearest Neighbor', 'Object Detection', '3D Network', 'Object Segmentation', 'Spatial Cognition', 'Part Segmentation', 'Extra Parameters', 'Complementary Features', 'Segmentation Detection', 'Shape Classification', '3D Tasks', '3D Object Detection', '3D Training', 'Positional Encoding', 'Nearest Neighbor Algorithm', 'Spherical Region', 'Linear Layer', '3D Position', 'Distance Metrics', '3D Space', 'Extra Cost', 'Bounding Box']",['Algorithms: 3D computer vision'],4,"Performances on standard 3D point cloud benchmarks have plateaued, resulting in oversized models and complex network design to make a fractional improvement. We present an alternative to enhance existing deep neural networks without any redesigning or extra parameters, termed as Spatial-Neighbor Adapter (SN-Adapter). Building on any trained 3D network, we utilize its learned encoding capability to extract features of the training dataset and summarize them as prototypical spatial knowledge. For a test point cloud, the SN-Adapter retrieves k nearest neighbors (k-NN) from the pre-constructed spatial prototypes and linearly interpolates the k-NN prediction with that of the original 3D network. By providing complementary characteristics, the proposed SN-Adapter serves as a plug-and-play module to economically improve performance in a nonparametric manner. More importantly, our SN-Adapter can be effectively generalized to various 3D tasks, including shape classification, part segmentation, and 3D object detection, demonstrating its superiority and robustness. We hope our approach could show a new perspective for point cloud analysis and facilitate future research."
Nested Deformable Multi-Head Attention for Facial Image Inpainting,"Shruti S. Phutke, Subrahmanyam Murala","CVPR Lab, Indian Institute of Technology Ropar, Punjab, INDIA",100,India,0,,"Extracting adequate contextual information is an important aspect of any image inpainting method. To achieve this, ample image inpainting methods are available that aim to focus on large receptive fields. Recent advancements in the deep learning field with the introduction of transformers for image inpainting paved the way toward plausible results. Stacking multiple transformer blocks in a single layer causes the architecture to become computationally complex. In this context, we propose a novel lightweight architecture with a nested deformable attention based transformer layer for feature fusion. The nested attention helps the network to focus on long-term dependencies from encoder and decoder features. Also, multi head attention consisting of a deformable convolution is proposed to delve into the diverse receptive fields. With the advantage of nested and deformable attention, we propose a lightweight architecture for facial image inpainting. The results comparison on Celeb HQ [25] dataset using known (NVIDIA) and unknown (QD-IMD) masks and Places2 [57] dataset with NVIDIA masks along with extensive ablation study prove the superiority of the proposed approach for image inpainting tasks. The code is available at: https://github.com/shrutiphutke/NDMA_ Facial_Inpainting.",https://openaccess.thecvf.com/content/WACV2023/html/Phutke_Nested_Deformable_Multi-Head_Attention_for_Facial_Image_Inpainting_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Phutke_Nested_Deformable_Multi-Head_Attention_for_Facial_Image_Inpainting_WACV_2023_paper.pdf,,https://github.com/shrutiphutke/NDMA_Facial_Inpainting,,main,Poster,https://ieeexplore.ieee.org/document/10030421/,"['Deep learning', 'Convolution', 'Stacking', 'Computer architecture', 'Transformers', 'Feature extraction', 'Decoding']","['Attention Mechanism', 'Image Inpainting', 'Computational Complexity', 'Contextual Information', 'Receptive Field', 'Diverse Fields', 'Plausible Results', 'Transformer Layers', 'Deformable Convolution', 'Deformable Layer', 'Transformer Block', 'Decoder Features', 'Comparative Analysis', 'Semantic', 'Convolutional Neural Network', 'Feature Maps', 'Generative Adversarial Networks', 'Feed-forward Network', 'Query Sequence', 'Peak Signal-to-noise Ratio', 'Fréchet Inception Distance', 'Encoder Layer', 'Ground Truth Image', 'Feature Encoder', 'Quadratic Complexity', 'Computational Complexity Analysis', 'Linear Complexity', 'Attention Layer', 'Edge Loss', 'L1-norm']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"Extracting adequate contextual information is an important aspect of any image inpainting method. To achieve this, ample image inpainting methods are available that aim to focus on large receptive fields. Recent advancements in the deep learning field with the introduction of transformers for image inpainting paved the way toward plausible results. Stacking multiple transformer blocks in a single layer causes the architecture to become computationally complex. In this context, we propose a novel lightweight architecture with a nested deformable attention-based transformer layer for feature fusion. The nested attention helps the network to focus on long-term dependencies from encoder and decoder features. Also, multi-head attention consisting of a deformable convolution is proposed to delve into the diverse receptive fields. With the advantage of nested and deformable attention, we propose a lightweight architecture for facial image inpainting. The results comparison on Celeb HQ [25] dataset using known (NVIDIA) and unknown (QD-IMD) masks and Places2 [57] dataset with NVIDIA masks along with extensive ablation study prove the superiority of the proposed approach for image inpainting tasks. The code is available at: https://github.com/shrutiphutke/NDMA_Facial_Inpainting."
Neural Distributed Image Compression With Cross-Attention Feature Alignment,"Nitish Mital, Ezgi Özyilkan, Ali Garjani, Deniz Gündüz","Dept. of Electrical and Electronics Engineering, Imperial College London; Section of Mathematics, EPFL; Dept. of Electrical and Computer Engineering, New York University",100,"Canada, Switzerland, UK, USA",0,,"We consider the problem of compressing an information source when a correlated one is available as side information only at the decoder side, which is a special case of the distributed source coding problem in information theory. In particular, we consider a pair of stereo images, which have overlapping fields of view, and are captured by a synchronized and calibrated pair of cameras as correlated image sources. In previously proposed methods, the encoder transforms the input image to a latent representation using a deep neural network, and compresses the quantized latent representation losslessly using entropy coding. The decoder decodes the entropy-coded quantized latent representation, and reconstructs the input image using this representation and the available side information. In the proposed method, the decoder employs a cross-attention module to align the feature maps obtained from the received latent representation of the input image and a latent representation of the side information. We argue that aligning the correlated patches in the feature maps allows better utilization of the side information. We empirically demonstrate the competitiveness of the proposed algorithm on KITTI and Cityscape datasets of stereo image pairs. Our experimental results show that the proposed architecture is able to exploit the decoder-only side information in a more efficient manner compared to previous works.",https://openaccess.thecvf.com/content/WACV2023/html/Mital_Neural_Distributed_Image_Compression_With_Cross-Attention_Feature_Alignment_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mital_Neural_Distributed_Image_Compression_With_Cross-Attention_Feature_Alignment_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030831/,"['Image coding', 'Source coding', 'Pipelines', 'Neural networks', 'Transforms', 'Cameras', 'Entropy coding']","['Feature Alignment', 'Image Compression', 'Deep Neural Network', 'Input Image', 'Feature Maps', 'Image Pairs', 'Image Representation', 'Latent Representation', 'Stereo Images', 'Stereo Image Pairs', 'Entropy Coding', 'Pair Of Cameras', 'Time Step', 'Convolutional Layers', 'General Case', 'Data Privacy', 'Attention Mechanism', 'Generative Adversarial Networks', 'Bitrate', 'Left Image', 'Uniform Noise', 'Common Information', 'Image X', 'Camera Array', 'Compression Algorithm', 'Stereo Camera', 'Least Significant Bit', 'Lossless Compression', 'Compression Scheme', 'Decoder Output']",,12,"We consider the problem of compressing an information source when a correlated one is available as side information only at the decoder side, which is a special case of the distributed source coding problem in information theory. In particular, we consider a pair of stereo images, which have overlapping fields of view, and are captured by a synchronized and calibrated pair of cameras as correlated image sources. In previously proposed methods, the encoder transforms the input image to a latent representation using a deep neural network, and compresses the quantized latent representation losslessly using entropy coding. The decoder decodes the entropy-coded quantized latent representation, and reconstructs the input image using this representation and the available side information. In the proposed method, the decoder employs a cross-attention module to align the feature maps obtained from the received latent representation of the input image and a latent representation of the side information. We argue that aligning the correlated patches in the feature maps allows better utilization of the side information. We empirically demonstrate the competitiveness of the proposed algorithm on KITTI and Cityscape datasets of stereo image pairs. Our experimental results show that the proposed architecture is able to exploit the decoder-only side information in a more efficient manner compared to previous works."
Neural Implicit Representations for Physical Parameter Inference From a Single Video,"Florian Hofherr, Lukas Koestler, Florian Bernard, Daniel Cremers",Technical University of Munich; University of Bonn,100,Germany,0,,"Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible.",https://openaccess.thecvf.com/content/WACV2023/html/Hofherr_Neural_Implicit_Representations_for_Physical_Parameter_Inference_From_a_Single_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hofherr_Neural_Implicit_Representations_for_Physical_Parameter_Inference_From_a_Single_WACV_2023_paper.pdf,,,2204.1403,main,Poster,https://ieeexplore.ieee.org/document/10031018/,"['Training', 'Visualization', 'Three-dimensional displays', 'Computational modeling', 'Training data', 'Predictive models', 'Ordinary differential equations']","['Neural Representations', 'Single Video', 'Implicit Representation', 'Implicit Neural Representation', 'Neural Network', 'Training Data', 'State Space', 'Physical System', 'Ordinary Differential Equations', 'Physical Phenomena', 'Dynamic Representation', 'Long-term Prediction', 'Dynamic Scenes', 'High-resolution Video', 'Model Parameters', 'Machine Learning', 'Real-world Data', 'Angular Velocity', 'Unknown Parameters', 'Parametrized', 'Dynamic Objects', 'Sequence Of Frames', 'Training Frames', 'Affine Transformation', 'View Synthesis', 'Homography', 'Static Background', 'Variational Autoencoder', 'Deflection Angle', 'Set Method']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Computational photography', 'image and video synthesis']",3,"Neural networks have recently been used to analyze diverse physical systems and to identify the underlying dynamics. While existing methods achieve impressive results, they are limited by their strong demand for training data and their weak generalization abilities to out-of-distribution data. To overcome these limitations, we propose to combine neural implicit representations for appearance modeling with neural ordinary differential equations (ODEs) for modelling planar physical phenomena to obtain a dynamic scene representation that can be identified directly from visual observations. Our proposed model combines several unique advantages: (i) Contrary to existing approaches that require large training datasets, we are able to identify physical parameters from only a single video. (ii) The use of neural implicit representations enables the processing of high-resolution videos and the synthesis of photo-realistic images. (iii) The embedded neural ODE has a known parametric form that allows for the identification of interpretable physical parameters, and (iv) long-term prediction in state space. (v) Furthermore, the photo-realistic rendering of novel scenes with modified physical parameters becomes possible."
Neural Weight Search for Scalable Task Incremental Learning,"Jian Jiang, Oya Celiktutan","Department of Engineering, King’s College London, London, UK",100,UK,0,,"Task incremental learning aims to enable a system to maintain its performance on previously learned tasks while learning new tasks, solving the problem of catastrophic forgetting. One promising approach is to build an individual network or sub-network for future tasks. However, this leads to an ever-growing memory due to saving extra weights for new tasks and how to address this issue has remained an open problem in task incremental learning. In this paper, we introduce a novel Neural Weight Search technique that designs a fixed search space where the optimal combinations of frozen weights can be searched to build new models for novel tasks in an end-to-end manner, resulting in a scalable and controllable memory growth. Extensive experiments on two benchmarks, i.e., Split-CIFAR-100 and CUB-to-Sketches, show our method achieves state-of-the-art performance with respect to both average inference accuracy and total memory cost.",https://openaccess.thecvf.com/content/WACV2023/html/Jiang_Neural_Weight_Search_for_Scalable_Task_Incremental_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jiang_Neural_Weight_Search_for_Scalable_Task_Incremental_Learning_WACV_2023_paper.pdf,,https://github.com/JianJiangKCL/NeuralWeightSearch,2211.13823,main,Poster,https://ieeexplore.ieee.org/document/10030787/,"['Deep learning', 'Computer vision', 'Costs', 'Benchmark testing', 'Aerospace electronics', 'Inference algorithms', 'Task analysis']","['Scalable', 'Incremental Learning', 'Neural Weights', 'Task Incremental Learning', 'Benchmark', 'Average Accuracy', 'Search Space', 'Task Model', 'Inference Accuracy', 'Memory Cost', 'Catastrophic Forgetting', 'Fine-tuned', 'Convolutional Layers', 'Search Algorithm', 'Large-scale Datasets', 'Binary Data', 'ImageNet', 'Stochastic Gradient Descent', 'Exhaustive Search', 'Convolution Kernel', 'Backbone Model', 'Number Of Kernels', 'Neural Architecture Search', 'Kernel Weight', 'Increase In Storage', 'Significant Body Of Work', 'Large-scale Image Datasets', 'Classification Loss', 'Human-robot Interaction', 'Poor Scalability']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Task incremental learning aims to enable a system to maintain its performance on previously learned tasks while learning new tasks, solving the problem of catastrophic forgetting. One promising approach is to build an individual network or sub-network for future tasks. However, this leads to an ever-growing memory due to saving extra weights for new tasks and how to address this issue has remained an open problem in task incremental learning. In this paper, we introduce a novel Neural Weight Search technique that designs a fixed search space where the optimal combinations of frozen weights can be searched to build new models for novel tasks in an end-to-end manner, resulting in a scalable and controllable memory growth. Extensive experiments on two benchmarks, i.e., Split-CIFAR-100 and CUB-to-Sketches, show our method achieves state-of-the-art performance with respect to both average inference accuracy and total memory cost.
<sup>1</sup>"
NeuralBF: Neural Bilateral Filtering for Top-Down Instance Segmentation on Point Clouds,"Weiwei Sun, Daniel Rebain, Renjie Liao, Vladimir Tankovich, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi",University of British Columbia; Simon Fraser University; Google Research,66.66666667,Canada,33.33333333,USA,"We introduce a method for instance proposal generation for 3D point clouds. Existing techniques typically directly regress proposals in a single feed-forward step, leading to inaccurate estimation. We show that this serves as a critical bottleneck, and propose a method based on iterative bilateral filtering with learned kernels. Following the spirit of bilateral filtering, we consider both the deep feature embeddings of each point, as well as their locations in the 3D space. We show via synthetic experiments that our method brings drastic improvements when generating instance proposals for a given point of interest. We further validate our method on the challenging ScanNet benchmark, achieving the best instance segmentation performance amongst the sub-category of top-down methods.",https://openaccess.thecvf.com/content/WACV2023/html/Sun_NeuralBF_Neural_Bilateral_Filtering_for_Top-Down_Instance_Segmentation_on_Point_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sun_NeuralBF_Neural_Bilateral_Filtering_for_Top-Down_Instance_Segmentation_on_Point_WACV_2023_paper.pdf,https://neuralbf.github.io,https://github.com/neuralbf,2207.09978,main,Poster,https://ieeexplore.ieee.org/document/10030147/,"['Point cloud compression', 'Image segmentation', 'Three-dimensional displays', 'Filtering', 'Pipelines', 'Estimation', 'Proposals']","['Point Cloud', 'Instance Segmentation', 'Bilateral Filter', 'Top-down Methods', 'Kernel Learning', 'Proposal Generation', 'Low Confidence', 'Bounding Box', 'Confidence Score', 'Semantic Similarity', 'Semantic Features', 'Convex Hull', 'Ground Truth Labels', 'Bottom-up Methods', 'Non-maximum Suppression', 'Spatial Similarity', 'Convex Polytope', 'Segmentation Module', 'Spatial Kernel', 'Query Point', 'Query Features', 'Semantic Scores', 'Backbone Feature', 'Semantic Prediction', 'Convex Domain', 'Simple Datasets', 'Affinity Score', 'Background Points', 'Semantic Segmentation', '3D Bounding Box']","['Algorithms: 3D computer vision', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"We introduce a method for instance proposal generation for 3D point clouds. Existing techniques typically directly regress proposals in a single feed-forward step, leading to inaccurate estimation. We show that this serves as a critical bottleneck, and propose a method based on iterative bilateral filtering with learned kernels. Following the spirit of bilateral filtering, we consider both the deep feature embeddings of each point, as well as their locations in the 3D space. We show via synthetic experiments that our method brings drastic improvements when generating instance proposals for a given point of interest. We further validate our method on the challenging ScanNet benchmark, achieving the best instance segmentation performance amongst the sub-category of top-down methods."
No Reference Opinion Unaware Quality Assessment of Authentically Distorted Images,"Nithin C. Babu, Vignesh Kannan, Rajiv Soundararajan","Mercedes-Benz Research and Development India; Indian Institute of Science, Bengaluru, India",50,India,50,India,"The quality assessment (QA) of camera captured authentically distorted images is important on account of its ubiquitous applications and challenging due to the lack of a reference. While there exists a plethora of supervised no reference (NR) image QA (IQA) algorithms, there is a need to study unsupervised or opinion unaware algorithms on account of their superior generalization performance. We explore self-supervised learning (SSL) for the feature design on authentically distorted images to predict quality without training on human labels. While SSL on synthetic distortions has recently shown promise, there is a need to enrich the feature learning on authentic distortions. The key challenge in achieving this is in the learning of quality sensitive features with mitigated content dependence. We design a self-supervised contrastive learning approach which only requires positives and introduce a content separation loss by estimating a bound on the mutual information between the features learnt and the content information. We show on multiple authentically distorted datasets that our self-supervised features can predict image quality by comparing with a corpus of pristine images and achieve state-of-the-art performance.",https://openaccess.thecvf.com/content/WACV2023/html/Babu_No_Reference_Opinion_Unaware_Quality_Assessment_of_Authentically_Distorted_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Babu_No_Reference_Opinion_Unaware_Quality_Assessment_of_Authentically_Distorted_Images_WACV_2023_paper.pdf,,https://github.com/nithincbabu7/iqa-ContentSep,,main,Poster,https://ieeexplore.ieee.org/document/10030129/,"['Representation learning', 'Training', 'Image quality', 'Self-supervised learning', 'Prediction algorithms', 'Distortion', 'Minimization']","['Authentic Distortions', 'Image Quality', 'Information Content', 'Feature Learning', 'Mutual Information', 'Reference Image', 'Self-supervised Learning', 'Human Labeling', 'Loss Function', 'Unsupervised Learning', 'Quality Characteristics', 'Prediction Quality', 'Image Patches', 'Examples Of Methods', 'Content Features', 'Performance Of Different Methods', 'Dependent Characteristics', 'Contrastive Loss', 'Feature Encoder', 'Motion Blur', 'Mean Opinion Score', 'Natural Scene Statistics', 'Content Bias', 'Distortion Levels', 'Human Supervision', 'Unsupervised Feature Learning', 'Feature Learning Method', 'Fine-tuned Model', 'Hidden Layer Size', 'Conditional Distribution']","['Algorithms: Low-level and physics-based vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",8,"The quality assessment (QA) of camera captured authentically distorted images is important on account of its ubiquitous applications and challenging due to the lack of a reference. While there exists a plethora of supervised no reference (NR) image QA (IQA) algorithms, there is a need to study unsupervised or opinion unaware algorithms on account of their superior generalization performance. We explore self-supervised learning (SSL) for the feature design on authentically distorted images to predict quality without training on human labels. While SSL on synthetic distortions has recently shown promise, there is a need to enrich the feature learning on authentic distortions. The key challenge in achieving this is in the learning of quality sensitive features with mitigated content dependence. We design a self-supervised contrastive learning approach which only requires positives and introduce a content separation loss by estimating a bound on the mutual information between the features learnt and the content information. We show on multiple authentically distorted datasets that our self-supervised features can predict image quality by comparing with a corpus of pristine images and achieve state-of-the-art performance.
<sup>§</sup>"
No Shifted Augmentations (NSA): Compact Distributions for Robust Self-Supervised Anomaly Detection,"Mohamed Yousef, Marcel Ackermann, Unmesh Kurup, Tom Bishop","Intuition Machines, Inc.; Glass Imaging, Inc.",50,USA,50,USA,"Unsupervised Anomaly detection (AD) requires building a notion of normalcy, distinguishing in-distribution (ID) and out-of-distribution (OOD) data, using only available ID samples. Recently, large gains were made on this task for the domain of natural images using self-supervised contrastive feature learning as a first step followed by kNN or traditional one-class classifiers for feature scoring. Learned representations that are non-uniformly distributed on the unit hypersphere have been shown to be beneficial for this task. We go a step further and investigate how the ph  geometrical compactness  of the ID feature distribution makes isolating and detecting outliers easier, especially in the realistic situation when ID training data is polluted with OOD data. We propose novel architectural modifications to the self-supervised feature learning step, that enable such compact distributions for ID data to be learned. We show that the proposed modifications can be effectively applied to most existing self-supervised objectives, with large gains in performance. Furthermore, this improved OOD performance is obtained without resorting to tricks such as using strongly augmented ID images (e.g. by 90 degree rotations) as proxies for the unseen OOD data, as these impose overly prescriptive assumptions about ID data and its invariances. We perform extensive studies on benchmark datasets for one-class OOD detection and show state-of-the-art performance in the presence of pollution in the ID data, and comparable performance otherwise. We also propose and extensively evaluate a novel feature scoring technique based on the angular Mahalanobis distance, and propose a simple and novel technique for feature ensembling during evaluation that enables a big boost in performance at nearly zero run-time cost compared to the standard use of model ensembling or test time augmentations. Source code is available here: https://github.com/IntuitionMachines/NSA",https://openaccess.thecvf.com/content/WACV2023/html/Yousef_No_Shifted_Augmentations_NSA_Compact_Distributions_for_Robust_Self-Supervised_Anomaly_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yousef_No_Shifted_Augmentations_NSA_Compact_Distributions_for_Robust_Self-Supervised_Anomaly_WACV_2023_paper.pdf,,Here,2203.10344,main,Poster,https://ieeexplore.ieee.org/document/10030913/,"['Representation learning', 'Measurement', 'Pollution', 'Costs', 'Source coding', 'Training data', 'Feature extraction']","['Anomaly Detection', 'Pollution', 'Training Data', 'Feature Learning', 'Representation Learning', 'Mahalanobis Distance', 'Self-supervised Learning', 'Feature Scores', 'Hypersphere', 'Presence Of Pollutants', 'Architectural Modifications', 'One-class Classification', 'Validation Set', 'Batch Size', 'Feature Maps', 'Part Of Network', 'Quality Characteristics', 'Network Output', 'Prediction Network', 'Variety Of Scenarios', 'Quality Of Representations', 'Maximum Mean Discrepancy', 'Self-supervised Learning Methods', 'Metric Learning', 'Embedding Learning', 'Gradient Flow', 'External Dataset', 'Adaptive Feature', 'Small Batch Size']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Unsupervised Anomaly detection (AD) requires building a notion of normalcy, distinguishing in-distribution (ID) and out-of-distribution (OOD) data, using only available ID samples. Recently, large gains were made on this task for the domain of natural images using self-supervised contrastive feature learning as a first step followed by kNN or traditional one-class classifiers for feature scoring. Learned representations that are non-uniformly distributed on the unit hypersphere have been shown to be beneficial for this task. We go a step further and investigate how the geometrical compactness of the ID feature distribution makes isolating and detecting outliers easier, especially in the realistic situation when ID training data is polluted (i.e. ID data contains some OOD data that is used for learning the feature extractor parameters). We propose novel architectural modifications to the self-supervised feature learning step, that enable such compact distributions for ID data to be learned. We show that the proposed modifications can be effectively applied to most existing self-supervised objectives, with large gains in performance. Furthermore, this improved OOD performance is obtained without resorting to tricks such as using strongly augmented ID images (e.g. by 90 degree rotations) as proxies for the unseen OOD data, as these impose overly prescriptive assumptions about ID data and its invariances. We perform extensive studies on benchmark datasets for one-class OOD detection and show state-of-the-art performance in the presence of pollution in the ID data, and comparable performance otherwise. We also propose and extensively evaluate a novel feature scoring technique based on the angular Mahalanobis distance, and propose a simple and novel technique for feature ensembling during evaluation that enables a big boost in performance at nearly zero run-time cost compared to the standard use of model ensembling or test time augmentations. Source code is available Here"
Normality Guided Multiple Instance Learning for Weakly Supervised Video Anomaly Detection,"Seongheon Park, Hanjae Kim, Minsu Kim, Dahye Kim, Kwanghoon Sohn","Yonsei University, Korea Institute of Science and Technology (KIST); Yonsei University",100,South Korea,0,,"Weakly supervised Video Anomaly Detection (wVAD) aims to distinguish anomalies from normal events based on video-level supervision. Most existing works utilize Multiple Instance Learning (MIL) with ranking loss to tackle this task. These methods, however, rely on noisy predictions from a MIL-based classifier for target instance selection in ranking loss, degrading model performance. To overcome this problem, we propose Normality Guided Multiple Instance Learning (NG-MIL) framework, which encodes diverse normal patterns from noise-free normal videos into prototypes for constructing a similarity-based classifier. By ensembling predictions of two classifiers, our method could refine the anomaly scores, reducing training instability from weak labels. Moreover, we introduce normality clustering and normality guided triplet loss constraining inner bag instances to boost the effect of NG-MIL and increase the discriminability of classifiers. Extensive experiments on three public datasets (ShanghaiTech, UCF-Crime, XD-Violence) demonstrate that our method is comparable to or better than existing weakly supervised methods, achieving state-of-the-art results.",https://openaccess.thecvf.com/content/WACV2023/html/Park_Normality_Guided_Multiple_Instance_Learning_for_Weakly_Supervised_Video_Anomaly_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Park_Normality_Guided_Multiple_Instance_Learning_for_Weakly_Supervised_Video_Anomaly_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030221/,"['Training', 'Prototypes', 'Streaming media', 'Predictive models', 'Video surveillance', 'Real-time systems', 'Safety']","['Anomaly Detection', 'Multiple Instance Learning', 'Video Anomaly Detection', 'Normal Pattern', 'Normal Events', 'Weak Labels', 'Ranking Loss', 'Instance Selection', 'Anomaly Score', 'Class Prediction', 'Learning Objectives', 'Large Margin', 'Decision Boundary', 'Graph Convolutional Network', 'ReLU Activation Function', 'Video Surveillance', 'Abnormal Events', 'Pseudo Labels', 'Clustering Loss', 'One-class Classification', 'RGB Features']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Robotics']",13,"Weakly supervised Video Anomaly Detection (wVAD) aims to distinguish anomalies from normal events based on video-level supervision. Most existing works utilize Multiple Instance Learning (MIL) with ranking loss to tackle this task. These methods, however, rely on noisy predictions from a MIL-based classifier for target instance selection in ranking loss, degrading model performance. To overcome this problem, we propose Normality Guided Multiple Instance Learning (NG-MIL) framework, which encodes diverse normal patterns from noise-free normal videos into prototypes for constructing a similarity-based classifier. By ensembling predictions of two classifiers, our method could refine the anomaly scores, reducing training instability from weak labels. Moreover, we introduce normality clustering and normality guided triplet loss constraining inner bag instances to boost the effect of NG-MIL and increase the discriminability of classifiers. Extensive experiments on three public datasets (ShanghaiTech, UCF-Crime, XD-Violence) demonstrate that our method is comparable to or better than existing weakly supervised methods, achieving state-of-the-art results."
OCR-VQGAN: Taming Text-Within-Image Generation,"Juan A. Rodríguez, David Vazquez, Issam Laradji, Marco Pedersoli, Pau Rodriguez","Computer Vision Center, Barcelona; ´ETS Montr´eal; ServiceNow Research",66.66666667,"Canada, Spain",33.33333333,USA,"Synthetic image generation has recently experienced significant improvements in domains such as natural image or art generation. However, the problem of figure and diagram generation remains unexplored. A challenging aspect of generating figures and diagrams is effectively rendering readable texts within the images. To alleviate this problem, we present OCR-VQGAN, an image encoder, and decoder that leverages OCR pre-trained features to optimize a text perceptual loss, encouraging the architecture to preserve high-fidelity text and diagram structure. To explore our approach, we introduce the Paper2Fig100k dataset, with over 100k images of figures and texts from research papers. The figures show architecture diagrams and methodologies of articles available at arXiv.org from fields like artificial intelligence and computer vision. Figures usually include text and discrete objects, e.g., boxes in a diagram, with lines and arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by conducting several experiments on the task of figure reconstruction. Additionally, we explore the qualitative and quantitative impact of weighting different perceptual metrics in the overall loss function. We release code, models, and dataset at https://github.com/joanrod/ocr-vqgan.",https://openaccess.thecvf.com/content/WACV2023/html/Rodriguez_OCR-VQGAN_Taming_Text-Within-Image_Generation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rodriguez_OCR-VQGAN_Taming_Text-Within-Image_Generation_WACV_2023_paper.pdf,,https://github.com/joanrod/ocr-vqgan,,main,Poster,https://ieeexplore.ieee.org/document/10030860/,"['Measurement', 'Computer vision', 'Shape', 'Image synthesis', 'Optical character recognition', 'Computer architecture', 'Rendering (computer graphics)']","['Computer Vision', 'Research Papers', 'Natural Images', 'Image Generation', 'Optical Character Recognition', 'Perceptual Loss', 'Reconstruction Task', 'Image Encoder', 'Feature Space', 'Image Dataset', 'Image Reconstruction', 'Codebook', 'Object Detection', 'Latent Space', 'Plaintext', 'Figure Caption', 'Channel Dimension', 'Perceptual Similarity', 'Image Synthesis', 'Sequence Of Tokens', 'Text Generation', 'Vector Quantization', 'Tokenized', 'Conditional Random Field', 'PDF Format', 'Upsampling Layer', 'Caption Text', 'Plagiarism Detection']","['Algorithms: Computational photography', 'image and video synthesis', 'Vision + language and/or other modalities']",1,"Synthetic image generation has recently experienced significant improvements in domains such as natural image or art generation. However, the problem of figure and diagram generation remains unexplored. A challenging aspect of generating figures and diagrams is effectively rendering readable texts within the images. To alleviate this problem, we present OCR-VQGAN, an image encoder, and decoder that leverages OCR pre-trained features to optimize a text perceptual loss, encouraging the architecture to preserve high-fidelity text and diagram structure. To explore our approach, we introduce the Paper2Fig100k dataset, with over 100k images of figures and texts from research papers. The figures show architecture diagrams and methodologies of articles available at arXiv.org from fields like artificial intelligence and computer vision. Figures usually include text and discrete objects, e.g., boxes in a diagram, with lines and arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by conducting several experiments on the task of figure reconstruction. Additionally, we explore the qualitative and quantitative impact of weighting different perceptual metrics in the overall loss function. We release code, models, and dataset at https://github.com/joanrod/ocr-vqgan."
On Quantizing Implicit Neural Representations,"Cameron Gordon, Shin-Fang Chng, Lachlan MacDonald, Simon Lucey",University of Adelaide,100,Australia,0,,"The role of quantization within implicit/coordinate neural networks is still not fully understood. We note that using a canonical fixed quantization scheme during training produces poor performance at low bit-rates due to the network weight distributions changing over the course of training. In this work, we show that a non-uniform quantization of neural weights can lead to significant improvements. Specifically, we demonstrate that a clustered quantization enables improved reconstruction. Finally, by characterising a trade-off between quantization and network capacity, we demonstrate that it is possible (while memory inefficient) to reconstruct signals using binary neural networks. We demonstrate our findings experimentally on 2D image reconstruction and 3D radiance fields; and show that simple quantization methods and architecture search can achieve compression of NeRF to less than 16kb with minimal loss in performance (323x smaller than the original NeRF).",https://openaccess.thecvf.com/content/WACV2023/html/Gordon_On_Quantizing_Implicit_Neural_Representations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gordon_On_Quantizing_Implicit_Neural_Representations_WACV_2023_paper.pdf,,,2209.01019,main,Poster,https://ieeexplore.ieee.org/document/10031015/,"['Training', 'Computer vision', 'Quantization (signal)', 'Image coding', 'Three-dimensional displays', 'Computational modeling', 'Neural networks']","['Implicit Neural Representation', 'Neural Network', 'Quantification Method', 'Weight Distribution', 'Network Weights', 'Bitrate', 'Network Capacity', 'Quantization Scheme', 'High-resolution', 'Quantum', 'Deep Learning', 'Hidden Layer', 'Feed-forward Network', 'Quantization Error', 'ReLU Activation', 'K-means Algorithm', 'Peak Signal-to-noise Ratio', 'Decision Boundary', 'Implicit Function', 'Image Gradient', 'Uniform Quantization', 'Quantization Levels', 'Implicit Representation', 'Image Compression', 'Structural Similarity Index Measure', 'Network Quantization', 'MSE Loss']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"The role of quantization within implicit/coordinate neural networks is still not fully understood. We note that using a canonical fixed quantization scheme during training produces poor performance at low bit-rates due to the network weight distributions changing over the course of training. In this work, we show that a non-uniform quantization of neural weights can lead to significant improvements. Specifically, we demonstrate that a clustered quantization enables improved reconstruction. Finally, by characterising a trade-off between quantization and network capacity, we demonstrate that it is possible (while memory inefficient) to reconstruct signals using binary neural networks. We demonstrate our findings experimentally on 2D image reconstruction and 3D radiance fields; and show that simple quantization methods and architecture search can achieve compression of NeRF to less than 16kb with minimal loss in performance (323x smaller than the original NeRF)."
On the Importance of Denoising When Learning To Compress Images,"Benoit Brummer, Christophe De Vleeschouwer","University of Louvain, Louvain-la-Neuve, Belgium; intoPIX, University of Louvain, Mont-Saint-Guibert, Belgium",100,Belgium,0,,"Image noise is ubiquitous in photography. However, image noise is not compressible nor desirable, thus attempting to convey the noise in compressed image bitstreams yields sub-par results in both rate and distortion. We propose to explicitly learn the image denoising task when training the codec. Therefore, we leverage the Natural Image Noise Dataset, which offers a wide variety of scenes captured with various noise levels. Given this training set, we show that a single model trained based on a mixture of images with variable noise levels appears to yield best-in-class results with both noisy and clean images, achieving better rate-distortion than a compression-only model or even than a pair of denoising-then-compression models with almost one order of magnitude fewer GMac operations.",https://openaccess.thecvf.com/content/WACV2023/html/Brummer_On_the_Importance_of_Denoising_When_Learning_To_Compress_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Brummer_On_the_Importance_of_Denoising_When_Learning_To_Compress_Images_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030876/,"['Training', 'Adaptation models', 'Image coding', 'Noise reduction', 'Rate-distortion', 'Transform coding', 'Noise measurement']","['Image Pairs', 'Image Noise', 'Clear Image', 'Bitstream', 'Noisy Images', 'Natural Noise', 'Various Levels Of Noise', 'Training Data', 'Learning Methods', 'Standard Model', 'Input Image', 'Ground Truth Image', 'Training Input', 'Paired Datasets', 'Input Noise', 'Least Significant Bit', 'Compression Method', 'Image Compression', 'Noise Threshold', 'Artificial Noise', 'Compression Scheme', 'Supervision Strategy', 'Denoising Methods', 'Standard Compression', 'Denoising Autoencoder', 'Latent Distribution', 'Shutter Speed', 'Image Sensor', 'Training Dataset', 'Railway Noise']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Image noise is ubiquitous in photography. However, image noise is not compressible nor desirable, thus attempting to convey the noise in compressed image bitstreams yields sub-par results in both rate and distortion. We propose to explicitly learn the image denoising task when training a codec. Therefore, we leverage the Natural Image Noise Dataset, which offers a wide variety of scenes captured with various ISO numbers, leading to different noise levels, including insignificant ones. Given this training set, we supervise the codec with noisy-clean image pairs, and show that a single model trained based on a mixture of images with variable noise levels appears to yield best-in-class results with both noisy and clean images, achieving better rate-distortion than a compression-only model or even than a pair of denoising-then-compression models with almost one order of magnitude fewer GMac operations."
One-Shot Doc Snippet Detection: Powering Search in Document Beyond Text,"Abhinav Java, Shripad Deshmukh, Milan Aggarwal, Surgan Jandial, Mausoom Sarkar, Balaji Krishnamurthy","Adobe Media and Data Science Research Labs, Noida, India",0,,100,USA,"Active consumption of digital documents has yielded scope for research in various applications, including search. Traditionally, searching within a document has been cast as a text matching problem ignoring the rich layout and visual cues commonly present in structured documents, forms, etc. To that end, we ask a mostly unexplored question: ""Can we search for other similar snippets present in a target document page given a single query instance of a document snippet?"". We propose MONOMER to solve this as a one-shot snippet detection task. MONOMER fuses context from visual, textual, and spatial modalities of snippets and documents to find query snippet in target documents. We conduct extensive ablations and experiments showing MONOMER outperforms several baselines from one-shot object detection (BHRL), template matching, and document understanding (LayoutLMv3). Due to the scarcity of relevant data for the task at hand, we train MONOMER on programmatically generated data having many visually similar query snippets and target document pairs from two datasets - Flamingo Forms and PubLayNet. We also do a human study to validate the generated data.",https://openaccess.thecvf.com/content/WACV2023/html/Java_One-Shot_Doc_Snippet_Detection_Powering_Search_in_Document_Beyond_Text_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Java_One-Shot_Doc_Snippet_Detection_Powering_Search_in_Document_Beyond_Text_WACV_2023_paper.pdf,,,2209.06584,main,Poster,https://ieeexplore.ieee.org/document/10030778/,"['Visualization', 'Computer vision', 'Fuses', 'Layout', 'Object detection', 'Search problems', 'Task analysis']","['Human Studies', 'Object Detection', 'Template Matching', 'Visual Similarity', 'Flamingo', 'Text Modality', 'Target Region', 'Bounding Box', 'Natural Images', 'Target Image', 'Average Precision', 'Image Formation', 'Structural Constituent', 'Edit Distance', 'Aforementioned Techniques', 'Sum Of Squared Differences', 'Blank Space', 'Query Image', 'Normalized Cross-correlation', 'Blocks Of Text', 'Sequence Of Tokens', 'Detection Head', 'Feature Volume']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"Active consumption of digital documents has yielded scope for research in various applications, including search. Traditionally, searching within a document has been cast as a text matching problem ignoring the rich layout and visual cues commonly present in structured documents, forms, etc. To that end, we ask a mostly unexplored question: ""Can we search for other similar snippets present in a target document page given a single query instance of a document snippet?"". We propose MONOMER to solve this as a one-shot snippet detection task. MONOMER fuses context from visual, textual, and spatial modalities of snippets and documents to find query snippet in target documents. We conduct extensive ablations and experiments showing MONOMER outperforms several baselines from one-shot object detection (BHRL), template matching, and document understanding (LayoutLMv3). Due to the scarcity of relevant data for the task at hand, we train MONOMER on programmatically generated data having many visually similar query snippets and target document pairs from two datasets - Flamingo Forms and PubLayNet. We also do a human study to validate the generated data."
One-Shot Synthesis of Images and Segmentation Masks,"Vadim Sushko, Dan Zhang, Jürgen Gall, Anna Khoreva","Bosch Center for Artiﬁcial Intelligence; University of Bonn; Bosch Center for Artiﬁcial Intelligence, University of Tübingen",66.66666667,Germany,33.33333333,Germany,"Joint synthesis of images and segmentation masks with generative adversarial networks (GANs) is promising to reduce the effort needed for collecting image data with pixel-wise annotations. However, to learn high-fidelity image-mask synthesis, existing GAN approaches first need a pre-training phase requiring large amounts of image data, which limits their utilization in restricted image domains. In this work, we take a step to reduce this limitation, introducing the task of one-shot image-mask synthesis. We aim to generate diverse images and their segmentation masks given only a single labelled example, and assuming, contrary to previous models, no access to any pre-training data. To this end, inspired by the recent architectural developments of single-image GANs, we introduce our OSMIS model which enables the synthesis of segmentation masks that are precisely aligned to the generated images in the one-shot regime. Besides achieving the high fidelity of generated masks, OSMIS outperforms state-of-the-art single-image GAN models in image synthesis quality and diversity. In addition, despite not using any additional data, OSMIS demonstrates an impressive ability to serve as a source of useful data augmentation for one-shot segmentation applications, providing performance gains that are complementary to standard data augmentation techniques. Code is available at https://github.com/boschresearch/one-shot-synthesis.",https://openaccess.thecvf.com/content/WACV2023/html/Sushko_One-Shot_Synthesis_of_Images_and_Segmentation_Masks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sushko_One-Shot_Synthesis_of_Images_and_Segmentation_Masks_WACV_2023_paper.pdf,,https://github.com/boschresearch/one-shot-synthesis,2209.07547,main,Poster,https://ieeexplore.ieee.org/document/10030467/,"['Training', 'Image segmentation', 'Computer vision', 'Codes', 'Image synthesis', 'Performance gain', 'Generative adversarial networks']","['Image Synthesis', 'Segmentation Mask', 'Data Augmentation', 'Generative Adversarial Networks', 'Diverse Images', 'Generative Adversarial Networks Model', 'Pre-training Data', 'Large Datasets', 'Image Quality', 'Diverse Sample', 'Single Image', 'Test Phase', 'Increase In Performance', 'Segmentation Model', 'Video Frames', 'Object Of Interest', 'Manual Annotation', 'Training Details', 'Single Training', 'Generative Adversarial Networks Training', 'Video Segments', 'Image X', 'External Networks']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"Joint synthesis of images and segmentation masks with generative adversarial networks (GANs) is promising to reduce the effort needed for collecting image data with pixel-wise annotations. However, to learn high-fidelity image-mask synthesis, existing GAN approaches first need a pre-training phase requiring large amounts of image data, which limits their utilization in restricted image domains. In this work, we take a step to reduce this limitation, introducing the task of one-shot image-mask synthesis. We aim to generate diverse images and their segmentation masks given only a single labelled example, and assuming, contrary to previous models, no access to any pre-training data. To this end, inspired by the recent architectural developments of single-image GANs, we introduce our OSMIS model which enables the synthesis of segmentation masks that are precisely aligned to the generated images in the one-shot regime. Besides achieving the high fidelity of generated masks, OSMIS outperforms state-of-the-art single-image GAN models in image synthesis quality and diversity. In addition, despite not using any additional data, OSMIS demonstrates an impressive ability to serve as a source of useful data augmentation for one-shot segmentation applications, providing performance gains that are complementary to standard data augmentation techniques. Code is available at https://github.com/boschresearch/one-shot-synthesis."
Online Adaptive Temporal Memory With Certainty Estimation for Human Trajectory Prediction,"Manh Huynh, Gita Alaghband","Department of Computer Science and Engineering, University of Colorado Denver",100,USA,0,,"Pedestrian trajectory prediction is an essential component of autonomous systems and robot navigation. Recent research has shown promising predictive performance by designing trajectory prediction networks to model a variety of motion-related features. Different from existing works, our focus is on designing a novel online adaptation framework (OATMem) to exploit the temporal similarities among trajectory samples encountered during testing to improve the prediction accuracy of any such models (i.e., predictors) without knowing the details of these predictors. Our framework consists of two novel modules: an augmented temporal observe-target memory network (ATM) and a certainty-based selector (CS). Inspired by the concept of key-value memory networks [16], we design the ATM to learn the temporal information from short-term past frames by encoding the trajectory samples of past pedestrians in form of observe-target (i.e., key-value) during testing. In addition, we propose a certainty-based selector (CS) to enhance the predictive ability of our framework under scenarios where there are large temporal dissimilarities between current pedestrians' movements and those stored in memory. In dynamic scenes, these scenarios commonly occur due to abrupt changes in contexts, such as camera motions, scene contexts, and pedestrians' behaviors. We extensively evaluate our framework in commonly-used datasets for pedestrian trajectory prediction: JAAD [12] and PIE [19] and show that our framework significantly improves the prediction accuracy of state-of-the-art models. Finally, in-depth ablation studies and analyses are conducted to show on the importance of each proposed component.",https://openaccess.thecvf.com/content/WACV2023/html/Huynh_Online_Adaptive_Temporal_Memory_With_Certainty_Estimation_for_Human_Trajectory_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Huynh_Online_Adaptive_Temporal_Memory_With_Certainty_Estimation_for_Human_Trajectory_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030982/,"['Adaptation models', 'Computer vision', 'Navigation', 'Computational modeling', 'Dynamics', 'Estimation', 'Predictive models']","['Trajectory Prediction', 'Temporal Memory', 'Human Trajectory Prediction', 'Prediction Accuracy', 'Predictive Performance', 'Temporal Information', 'Memory Network', 'Robot Navigation', 'Dynamic Scenes', 'Camera Motion', 'Scene Context', 'Temporal Similarity', 'Pedestrian Trajectory', 'Past Frames', 'Walking', 'Prediction Model', 'Dynamic Changes', 'Prediction Error', 'Recurrent Neural Network', 'Recent Models', 'Pedestrian Movement', 'Memory Module', 'Paste Samples', 'Past Trajectories', 'Final Prediction', 'Gated Recurrent Unit', 'Bounding Box', 'Scene Changes', 'Pedestrian Walking', 'Motor Variability']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Robotics']",7,"Pedestrian trajectory prediction is an essential component of autonomous systems and robot navigation. Recent research has shown promising predictive performance by designing prediction networks to model a variety of motion-related features. Different from existing works, our focus is on designing a novel online adaptation framework (OAT-Mem) to exploit the temporal similarities among trajectory samples encountered during testing to improve the prediction accuracy of any such models (i.e., predictors) without knowing the details of these predictors. Our framework consists of two novel modules: an augmented temporal observation-target memory network (ATM) and a certainty-based selector (CS). Inspired by the concept of key-value memory networks [16], ATM is proposed to learn the temporal information from short-term past frames by encoding the trajectory samples of past pedestrians in form of observation-target (i.e., key-value) during testing. In addition, we propose a certainty-based selector (CS) to enhance the predictive ability of our framework under scenarios where there are large temporal dissimilarities between current pedestrians’ movements and those stored in memory. In dynamic scenes, these scenarios commonly occur due to abrupt changes in contexts, such as camera motions, scene contexts, and pedestrians’ behaviors. We extensively evaluate our framework in commonly-used datasets: JAAD [12] and PIE [19] and show that our framework significantly improves the prediction accuracy of state-of-the-art models. Finally, in-depth studies are conducted to show the importance of each proposed component."
Online Knowledge Distillation for Multi-Task Learning,"Geethu Miriam Jacob, Vishal Agarwal, Björn Stenger","Rakuten Institute of Technology, Rakuten Group, Inc.",100,Japan,0,,"Multi-task learning (MTL) has found wide application in computer vision tasks. It uses a common backbone network allowing shared feature computation for different tasks such as semantic segmentation, depth- and normal estimation. In many cases negative transfer, i.e. impaired performance in the target domain, causes the MTL accuracy to be lower than simply training the corresponding single-task networks. To mitigate this issue, we propose an online knowledge distillation method for MTL, where single-task networks are trained simultaneously with the MTL network, guiding the optimization process. We propose selectively training layers for each task using an adaptive feature distillation (AFD) loss with an online task weighting (OTW) scheme. This task-wise feature distillation enables the MTL network to be trained in a similar way to the single-task networks. On the NYUv2 and Cityscapes datasets we show improvements over a baseline MTL model by 6.22% and 9.19%, respectively, and better performance than recent MTL methods. We validate our design choices, including the use of the online task weighting and the adaptive feature distillation loss in ablative experiments.",https://openaccess.thecvf.com/content/WACV2023/html/Jacob_Online_Knowledge_Distillation_for_Multi-Task_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jacob_Online_Knowledge_Distillation_for_Multi-Task_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030872/,"['Training', 'Knowledge engineering', 'Computer vision', 'Semantic segmentation', 'Computational modeling', 'Estimation', 'Benchmark testing']","['Multi-task Learning', 'Online Knowledge Distillation', 'Computer Vision', 'Semantic Segmentation', 'Backbone Network', 'Weighting Scheme', 'Distillation Method', 'Negative Transfer', 'Distillation Loss', 'Multi-task Learning Model', 'Classification Task', 'Object Detection', 'Single Network', 'Multiple Tasks', 'Softmax Function', 'Transformer Model', 'Inference Time', 'Iterative Model', 'Depth Estimation', 'Intermediate Features', 'Multi-task Network', 'Multi-task Model', 'Vision Transformer', 'Transformer Layers', 'Transformer Encoder', 'Scene Understanding', 'Visual Question Answering', 'Task Loss', 'CNN Model', 'Single Head']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",8,"Multi-task learning (MTL) has found wide application in computer vision tasks. We train a backbone network to learn a shared representation for different tasks such as semantic segmentation, depth- and normal estimation. In many cases negative transfer, i.e. impaired performance in the target domain, causes the MTL accuracy to be lower than training the corresponding single-task networks. To mitigate this issue, we propose an online knowledge distillation method, where single-task networks are trained simultaneously with the MTL network to guide the optimization process. We propose selectively training layers for each task using an adaptive feature distillation (AFD) loss with an online task weighting (OTW) scheme. This task-wise feature distillation enables the MTL network to be trained in a similar way to the single-task networks. On the NYUv2 and Cityscapes datasets we show improvements over a baseline MTL model by 6.22% and 9.19%, respectively, outperforming recent MTL methods. We validate the design choices in ablative experiments, including the use of online task weighting and the adaptive feature distillation loss."
OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping,"Junshi Xia, Naoto Yokoya, Bruno Adriano, Clifford Broni-Bediako","The University of Tokyo, Japan; RIKEN AIP, Japan",100,Japan,0,,"We introduce OpenEarthMap, a benchmark dataset, for global high-resolution land cover mapping. OpenEarthMap consists of 2.2 million segments of 5000 aerial and satellite images covering 97 regions from 44 countries across 6 continents, with manually annotated 8-class land cover labels at a 0.25--0.5m ground sampling distance. Semantic segmentation models trained on the OpenEarthMap generalize worldwide and can be used as off-the-shelf models in a variety of applications. We evaluate the performance of state-of-the-art methods for unsupervised domain adaptation and present challenging problem settings suitable for further technical development. We also investigate lightweight models using automated neural architecture search for limited computational resources and fast mapping. The dataset will be made publicly available.",https://openaccess.thecvf.com/content/WACV2023/html/Xia_OpenEarthMap_A_Benchmark_Dataset_for_Global_High-Resolution_Land_Cover_Mapping_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xia_OpenEarthMap_A_Benchmark_Dataset_for_Global_High-Resolution_Land_Cover_Mapping_WACV_2023_paper.pdf,https://open-earth-map.org,,2210.10732,main,Poster,https://ieeexplore.ieee.org/document/10030160/,"['Adaptation models', 'Image segmentation', 'Computer vision', 'Satellites', 'Computational modeling', 'Computer architecture', 'Benchmark testing']","['Benchmark Datasets', 'Global Map', 'High-resolution Maps', 'Land Cover Map', 'Global Land Cover', 'Global Land Cover Map', 'Continent', 'Development Of Techniques', 'Segmentation Model', 'Semantic Segmentation', 'Domain Adaptation', 'Ground Sampling Distance', 'Lightweight Model', 'Semantic Segmentation Models', 'Neural Architecture Search', 'Fast Mapping', 'Training Set', 'Farmland', 'Image Regions', 'Number Of Images', 'Semantic Segmentation Task', 'Source Domain', 'Open Data', 'Rangeland', 'Domain Gap', 'Target Domain', 'Land Cover Classes', 'Spatial Details', 'Limited Training', 'Source Dataset']","['Applications: Remote Sensing', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",41,"We introduce OpenEarthMap, a benchmark dataset, for global high-resolution land cover mapping. OpenEarth-Map consists of 2.2 million segments of 5000 aerial and satellite images covering 97 regions from 44 countries across 6 continents, with manually annotated 8-class land cover labels at a 0.25–0.5m ground sampling distance. Se-mantic segmentation models trained on the OpenEarth-Map generalize worldwide and can be used as off-the-shelf models in a variety of applications. We evaluate the performance of state-of-the-art methods for unsupervised domain adaptation and present challenging problem settings suitable for further technical development. We also investigate lightweight models using automated neural architecture search for limited computational resources and fast mapping. The dataset is available at https: //open-earth-map.org."
Orthogonal Transforms for Learning Invariant Representations in Equivariant Neural Networks,"Jaspreet Singh, Chandan Singh, Ankur Rana","Punjabi University, Patiala",100,India,0,,"The convolutional layers of the standard convolutional neural networks (CNNs) are equivariant to translation. Recently, a new class of CNNs is introduced which is equivariant to other affine geometric transformations such as rotation and reflection by replacing the standard convolutional layer with the group convolutional layer or using the steerable filters in the convloutional layer. We propose to embed the 2D positional encoding which is invariant to rotation, reflection and translation using orthogonal polar harmonic transforms (PHTs) before flattening the feature maps for fully-connected or classification layer in the equivariant CNN architecture. We select the PHTs among several invariant transforms, as they are very efficient in performance and speed. The proposed 2D positional encoding scheme between the convolutional and fully-connected layers of the equivariant networks is shown to provide significant improvement in performance on the rotated MNIST, CIFAR-10 and CIFAR-100 datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Singh_Orthogonal_Transforms_for_Learning_Invariant_Representations_in_Equivariant_Neural_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Singh_Orthogonal_Transforms_for_Learning_Invariant_Representations_in_Equivariant_Neural_Networks_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030227/,"['Neural networks', 'Transforms', 'Computer architecture', 'Harmonic analysis', 'Encoding', 'Reflection', 'Convolutional neural networks']","['Neural Network', 'Orthogonal Matrix', 'Equivariant Neural Networks', 'Learning Invariant Representations', 'Convolutional Neural Network', 'Convolutional Layers', 'Feature Maps', 'Fully-connected Layer', 'Convolutional Neural Network Architecture', 'MNIST Dataset', 'Geometric Transformation', 'Standard Convolutional Neural Networks', 'CIFAR-100 Dataset', 'Training Set', 'Coordinate System', 'Validation Set', 'Data Augmentation', 'Kernel Function', 'Stochastic Gradient Descent', 'Batch Normalization', 'Rotation Invariance', 'Representation Learning', 'Radial Basis Function', 'Filter Response', 'Discrete Transformation', 'Test Set Size', 'Composition Operator', 'Translation Invariance', 'Dimensions Of Responsiveness', 'Intermediate Layer']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"The convolutional layers of the standard convolutional neural networks (CNNs) are equivariant to translation. Recently, a new class of CNNs is introduced which is equivariant to other affine geometric transformations such as rotation and reflection by replacing the standard convolutional layer with the group convolutional layer or using the steerable filters in the convloutional layer. We propose to embed the 2D positional encoding which is invariant to rotation, reflection and translation using orthogonal polar harmonic transforms (PHTs) before flattening the feature maps for fully-connected or classification layer in the equivariant CNN architecture. We select the PHTs among several invariant transforms, as they are very efficient in performance and speed. The proposed 2D positional encoding scheme between the convolutional and fully-connected layers of the equivariant networks is shown to provide significant improvement in performance on the rotated MNIST, CIFAR-10 and CIFAR-100 datasets."
Out-of-Distribution Detection With Reconstruction Error and Typicality-Based Penalty,"Genki Osada, Tsubasa Takahashi, Budrul Ahsan, Takashi Nishide","IBM Japan; University of Tsukuba, Japan; LINE Corporation, Japan",33.33333333,Japan,66.66666667,Japan,"The task of out-of-distribution (OOD) detection is vital to realize safe and reliable operation for real-world applications. After the failure of likelihood-based detection in high dimensions had been revealed, approaches based on the typical set have been attracting attention; however, they still have not achieved satisfactory performance. Beginning by presenting the failure case of the typicality-based approach, we propose a new reconstruction error-based approach that employs normalizing flow (NF). We further introduce a typicality-based penalty, and by incorporating it into the reconstruction error in NF, we propose a new OOD detection method, penalized reconstruction error (PRE). Because the PRE detects test inputs that lie off the in-distribution manifold, it also effectively detects adversarial examples. We show the effectiveness of our method through the evaluation using natural image datasets, CIFAR-10, TinyImageNet, and ILSVRC2012.",https://openaccess.thecvf.com/content/WACV2023/html/Osada_Out-of-Distribution_Detection_With_Reconstruction_Error_and_Typicality-Based_Penalty_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Osada_Out-of-Distribution_Detection_With_Reconstruction_Error_and_Typicality-Based_Penalty_WACV_2023_paper.pdf,,,2212.12641,main,Poster,https://ieeexplore.ieee.org/document/10030782/,"['Manifolds', 'Computer vision', 'Measurement uncertainty', 'Noise measurement', 'Reliability', 'Task analysis', 'Image reconstruction']","['High-dimensional', 'Set Of Types', 'Normal Flow', 'Adversarial Examples', 'High Performance', 'Training Data', 'Deep Neural Network', 'Detection Performance', 'Latent Space', 'Lipschitz Continuous', 'Separate Datasets', 'Variational Autoencoder', 'Five-dimensional', 'Latent Vector', 'Isotropic Gaussian', 'Latent Distribution', 'Deep Generative Models', 'Likelihood-based Methods', 'Projected Gradient Descent', 'Widely Applicable Information Criterion', 'Classification-based Methods', 'Data Manifold', 'Likelihood-based Approach', 'Precision-recall Curve', 'Notion Of Set', 'Invertible', 'Numerical Errors', 'Normed Space', 'Larger Image Size']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Adversarial learning', 'adversarial attack and defense methods']",2,"The task of out-of-distribution (OOD) detection is vital to realize safe and reliable operation for real-world applications. After the failure of likelihood-based detection in high dimensions had been shown, approaches based on the typical set have been attracting attention; however, they still have not achieved satisfactory performance. Beginning by presenting the failure case of the typicality-based approach, we propose a new reconstruction error-based approach that employs normalizing flow (NF). We further introduce a typicality-based penalty, and by incorporating it into the reconstruction error in NF, we propose a new OOD detection method, penalized reconstruction error (PRE). Because the PRE detects test inputs that lie off the in-distribution manifold, it effectively detects adversarial examples as well as OOD examples. We show the effectiveness of our method through the evaluation using natural image datasets, CIFAR-10, TinyImageNet, and ILSVRC2012."
Out-of-Distribution Detection via Frequency-Regularized Generative Models,"Mu Cai, Yixuan Li","Department of Computer Sciences, University of Wisconsin-Madison",100,USA,0,,"Modern deep generative models can assign high likelihood to inputs drawn from outside the training distribution, posing threats to models in open-world deployments. While much research attention has been placed on defining new test-time measures of OOD uncertainty, these methods do not fundamentally change how deep generative models are regularized and optimized in training. In particular, generative models are shown to overly rely on the background information to estimate the likelihood. To address the issue, we propose a novel frequency-regularized learning (FRL) framework for OOD detection, which incorporates high-frequency information into training and guides the model to focus on semantically relevant features. FRL effectively improves performance on a wide range of generative architectures, including variational auto-encoder, GLOW, and PixelCNN++. On a new large-scale evaluation task, FRL achieves the state-of-the-art performance, outperforming a strong baseline Likelihood Regret by 10.7% (AUROC) while achieving 147x faster inference speed. Extensive ablations show that FRL improves the OOD detection performance while preserving the image generation quality.",https://openaccess.thecvf.com/content/WACV2023/html/Cai_Out-of-Distribution_Detection_via_Frequency-Regularized_Generative_Models_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cai_Out-of-Distribution_Detection_via_Frequency-Regularized_Generative_Models_WACV_2023_paper.pdf,,https://github.com/mu-cai/FRL,2208.09083,main,Poster,https://ieeexplore.ieee.org/document/10030908/,"['Training', 'Uncertainty', 'Image synthesis', 'Measurement uncertainty', 'Estimation', 'Computer architecture', 'Particle measurements']","['Semantic', 'Deep Models', 'Background Information', 'General Architecture', 'Variational Autoencoder', 'Large-scale Evaluation', 'Deep Generative Models', 'Extensive Ablation', 'High Frequency Information', 'Training Data', 'Likelihood Ratio', 'Gaussian Kernel', 'Latent Variables', 'Log-likelihood', 'Scoring Function', 'Small Datasets', 'Autoregressive Model', 'Large-scale Datasets', 'Frequency Information', 'Distribution Of Training Data', 'Common Benchmark', 'Negative Log-likelihood', 'Latent Code', 'Simple Datasets', 'Online Optimization', 'Code Length', 'Background Pixels', 'Training Deep Models', 'Invertible']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Computational photography', 'image and video synthesis']",15,"Modern deep generative models can assign high likelihood to inputs drawn from outside the training distribution, posing threats to models in open-world deployments. While much research attention has been placed on defining new test-time measures of OOD uncertainty, these methods do not fundamentally change how deep generative models are regularized and optimized in training. In particular, generative models are shown to overly rely on the background information to estimate the likelihood. To address the issue, we propose a novel frequency-regularized learning (FRL) framework for OOD detection, which incorporates high-frequency information into training and guides the model to focus on semantically relevant features. FRL effectively improves performance on a wide range of generative architectures, including variational auto-encoder, GLOW, and PixelCNN++. On a new large-scale evaluation task, FRL achieves the state-of-the-art performance, outperforming a strong baseline Likelihood Regret by 10.7% (AUROC) while achieving 147× faster inference speed. Extensive ablations show that FRL improves the OOD detection performance while preserving the image generation quality. Code is available at https://github.com/mu-cai/FRL."
OutfitTransformer: Learning Outfit Representations for Fashion Recommendation,"Rohan Sarkar, Navaneeth Bodla, Mariya I. Vasileva, Yen-Liang Lin, Anurag Beniwal, Alan Lu, Gerard Medioni","Amazon; Purdue University, West Lafayette",50,USA,50,USA,"Learning an effective outfit-level representation is critical for predicting the compatibility of items in an outfit, and retrieving complementary items for a partial outfit. We present a framework, OutfitTransformer, that uses the proposed task-specific tokens and leverages the self-attention mechanism to learn effective outfit-level representations encoding the compatibility relations between all items in the entire outfit for addressing both compatibility prediction and complementary item retrieval. For compatibility prediction, we design an outfit token to capture a global outfit representation and train the framework using a classification loss. For complementary item retrieval, we design a target item token that additionally takes the target item specification (in the form of a category or text description) into consideration. We train our framework using a proposed set-wise outfit ranking loss to generate a target item embedding given an outfit, and a target item specification as inputs. The generated target item embedding is then used to retrieve compatible items that match the rest of the outfit. Additionally, we adopt a pre-training approach and a curriculum learning strategy to improve retrieval performance. Experiments show that our approach outperforms state-of-the-art methods on compatibility prediction, fill-in-the-blank, and complementary item retrieval tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Sarkar_OutfitTransformer_Learning_Outfit_Representations_for_Fashion_Recommendation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sarkar_OutfitTransformer_Learning_Outfit_Representations_for_Fashion_Recommendation_WACV_2023_paper.pdf,,,2204.04812,main,Poster,https://ieeexplore.ieee.org/document/10030975/,"['Computer vision', 'Task analysis']","['Fashion Recommendation', 'Prediction Task', 'Textual Descriptions', 'Classification Loss', 'Curriculum Learning', 'Global Representation', 'Target Items', 'Retrieval Performance', 'Ranking Loss', 'Compatibility Relations', 'Retrieval Of Items', 'Image Features', 'Negative Samples', 'Recommender Systems', 'Category Information', 'Graph Convolutional Network', 'Textual Features', 'Test Split', 'Item Pairs', 'Image Encoder', 'Transformer Encoder', 'Text Encoder', 'Vision Transformer', 'Disjoint Sets', 'Item Descriptions', 'Target Category', 'Higher-order Relationships', 'Encoder Output', 'High-level Categories']","['Applications: Commercial/retail', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",9,"Learning an effective outfit-level representation is critical for predicting the compatibility of items in an outfit, and retrieving complementary items for a partial outfit. We present a framework, OutfitTransformer, that uses the pro-posed task-specific tokens and leverages the self-attention mechanism to learn effective outfit-level representations en-coding the compatibility relations between all items in the entire outfit for addressing both compatibility prediction and complementary item retrieval. For compatibility pre-diction, we design an outfit token to capture a global out-fit representation and train the framework using a classification loss. For complementary item retrieval, we design a target item token that additionally takes the target item specification (in the form of a category or text description) into consideration. We train our framework using a pro-posed set-wise outfit ranking loss to generate a target item embedding given an outfit, and a target item specification as inputs. The generated target item embedding is then used to retrieve compatible items that match the rest of the out-fit. Additionally, we adopt a pre-training approach and a curriculum learning strategy to improve retrieval performance. Experiments show that our approach outperforms state-of-the-art methods on compatibility prediction, fill-in-the-blank, and complementary item retrieval tasks."
Overlap-Guided Gaussian Mixture Models for Point Cloud Registration,"Guofeng Mei, Fabio Poiesi, Cristiano Saltori, Jian Zhang, Elisa Ricci, Nicu Sebe","University of Trento, Trento, Italy; University of Technology Sydney, Sydney, Australia; University of Trento & FBK, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy",100,"Australia, Italy",0,,"Probabilistic 3D point cloud registration methods have shown competitive performance in overcoming noise, outliers, and density variations. However, registering point cloud pairs in the case of partial overlap is still a challenge. This paper proposes a novel overlap-guided probabilistic registration approach that computes the optimal transformation from matched Gaussian Mixture Model (GMM) parameters. We reformulate the registration problem as the problem of aligning two Gaussian mixtures such that a statistical discrepancy measure between the two corresponding mixtures is minimized. We introduce a Transformer-based detection module to detect overlapping regions and represent the input point clouds using GMMs by guiding their alignment through overlap scores computed by this detection module. Experiments show that our method achieves superior registration accuracy and efficiency than state-of-the-art methods when handling point clouds with partial overlap and different densities on synthetic and real-world datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Mei_Overlap-Guided_Gaussian_Mixture_Models_for_Point_Cloud_Registration_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mei_Overlap-Guided_Gaussian_Mixture_Models_for_Point_Cloud_Registration_WACV_2023_paper.pdf,,https://github.com/gfmei/ogmm,2210.09836,main,Poster,https://ieeexplore.ieee.org/document/10030418/,"['Point cloud compression', 'Computer vision', 'Three-dimensional displays', 'Neural networks', 'Probabilistic logic', 'Transformers', 'Minimization']","['Mixture Model', 'Point Cloud', 'Gaussian Mixture Model', 'Point Cloud Registration', 'Variation In Density', 'Noise Variance', 'Real-world Datasets', 'Probabilistic Approach', 'Detection Module', 'Partial Overlap', 'Overlap Region', 'Registration Method', 'Registration Accuracy', 'Input Point Cloud', 'Registration Approach', 'Overlap Score', 'Registration Problem', 'Transformer', 'Feature Space', 'Rigid Transformation', 'Exact Correspondence', 'Positional Encoding', '3D Space', 'Repeat Structure', 'Point Source', '3D Coordinates', 'Geometric Properties', 'Graph Convolutional Network', 'Global Features']",['Algorithms: 3D computer vision'],18,"Probabilistic 3D point cloud registration methods have shown competitive performance in overcoming noise, outliers, and density variations. However, registering point cloud pairs in the case of partial overlap is still a challenge. This paper proposes a novel overlap-guided probabilistic registration approach that computes the optimal transformation from matched Gaussian Mixture Model (GMM) parameters. We reformulate the registration problem as the problem of aligning two Gaussian mixtures such that a statistical discrepancy measure between the two corresponding mixtures is minimized. We introduce a Transformer-based detection module to detect overlapping regions, and represent the input point clouds using GMMs by guiding their alignment through overlap scores computed by this detection module. Experiments show that our method achieves superior registration accuracy and efficiency than state-of-the-art methods when handling point clouds with partial overlap and different densities on synthetic and real-world datasets. https://github.com/gfmei/ogmm"
PIDS: Joint Point Interaction-Dimension Search for 3D Point Cloud,"Tunhou Zhang, Mingyuan Ma, Feng Yan, Hai Li, Yiran Chen","Department of Computer Science, University of Houston, Houston, TX 77204; ECE Department, Duke University, Durham, NC 27708",100,USA,0,,"The interaction and dimension of points are two important axes in designing point operators to serve hierarchical 3D models. Yet, these two axes are heterogeneous and challenging to fully explore. Existing works craft point operator under a single axis and reuse the crafted operator in all parts of 3D models. This overlooks the opportunity to better combine point interactions and dimensions by exploiting varying geometry/density of 3D point clouds. In this work, we establish PIDS, a novel paradigm to jointly explore point interactions and point dimensions to serve semantic segmentation on point cloud data. We establish a large search space to jointly consider versatile point interactions and point dimensions. This supports point operators with various geometry/density considerations. The enlarged search space with heterogeneous search components calls for a better ranking of candidate models. To achieve this, we improve the search space exploration by leveraging predictor-based Neural Architecture Search (NAS), and enhance the quality of prediction by assigning unique encoding to heterogeneous search components based on their priors. We thoroughly evaluate the networks crafted by PIDS on two semantic segmentation benchmarks, showing  1% mIOU improvement on SemanticKITTI and S3DIS over state-of-the-art 3D models.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_PIDS_Joint_Point_Interaction-Dimension_Search_for_3D_Point_Cloud_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_PIDS_Joint_Point_Interaction-Dimension_Search_for_3D_Point_Cloud_WACV_2023_paper.pdf,,,2211.15759,main,Poster,https://ieeexplore.ieee.org/document/10030375/,"['Point cloud compression', 'Geometry', 'Solid modeling', 'Three-dimensional displays', 'Semantic segmentation', 'Computational modeling', 'Computer architecture']","['Point Cloud', '3D Point', '3D Point Cloud', 'Partial Model', 'Search Space', 'Neural Architecture', 'Prediction Quality', 'Semantic Segmentation', 'Point Cloud Data', 'Single Axis', 'Neural Architecture Search', 'Heterogeneous Components', 'Linear Correlation', 'Central Point', 'Distribution Of Points', 'Dot Product', 'Design Space', 'Neighboring Points', 'Second-order Interactions', 'First-order Interactions', 'Sparse Point Cloud', 'Efficient 3D', 'Sparse Feature', 'Expansion Factor', 'Geometric Distribution', 'Choice Of Dimensions', 'Voxel-based Methods', 'Combination Of Dimensions']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Robotics']",4,"The interaction and dimension of points are two important axes in designing point operators to serve hierarchical 3D models. Yet, these two axes are heterogeneous and challenging to fully explore. Existing works craft point operator under a single axis and reuse the crafted operator in all parts of 3D models. This overlooks the opportunity to better combine point interactions and dimensions by exploiting varying geometry/density of 3D point clouds. In this work, we establish PIDS, a novel paradigm to jointly explore point interactions and point dimensions to serve semantic segmentation on point cloud data. We establish a large search space to jointly consider versatile point interactions and point dimensions. This supports point operators with various geometry/density considerations. The enlarged search space with heterogeneous search components calls for a better ranking of candidate models. To achieve this, we improve the search space exploration by leveraging predictor-based Neural Architecture Search (NAS), and enhance the quality of prediction by assigning unique encoding to heterogeneous search components based on their priors. We thoroughly evaluate the networks crafted by PIDS on two semantic segmentation benchmarks, showing ~ 1% mIOU improvement on SemanticKITTI and S3DIS over state-of-the-art 3D models."
PINER: Prior-Informed Implicit Neural Representation Learning for Test-Time Adaptation in Sparse-View CT Reconstruction,"Bowen Song, Liyue Shen, Lei Xing",University of Michigan; Stanford University,100,USA,0,,"Recently, deep learning has been introduced to solve important medical image reconstruction problems such as sparse-view CT reconstruction. However, the developed deep reconstruction models are generally limited in generalization when applied to unseen testing samples in target domain. Furthermore, privacy concerns may impede the availability of source-domain training data to retrain or adapt the model to the target-domain testing data, which are quite common in real-world medical applications. To address these issues, we introduce a source-free black-box test-time adaptation method for sparse-view CT reconstruction with unknown noise levels based on prior-informed implicit neural representation learning (PINER). By leveraging implicit neural representation learning to generate the image representations at various noise levels, the proposed method is able to construct the adapted input representations at test time based on the inference of black-box model and output analysis. We performed experiments of source-free test-time adaptation for sparse-view CT reconstruction with unknown noise levels on multiple anatomical sites with different black-box deep reconstruction models, where our method outperforms the state-of-the-art algorithms.",https://openaccess.thecvf.com/content/WACV2023/html/Song_PINER_Prior-Informed_Implicit_Neural_Representation_Learning_for_Test-Time_Adaptation_in_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Song_PINER_Prior-Informed_Implicit_Neural_Representation_Learning_for_Test-Time_Adaptation_in_WACV_2023_paper.pdf,,https://github.com/efzero/PINER,,main,Poster,https://ieeexplore.ieee.org/document/10030867/,"['Representation learning', 'Adaptation models', 'Computed tomography', 'Closed box', 'Training data', 'Noise measurement', 'Image reconstruction']","['Representation Learning', 'Neural Representations', 'Implicit Neural Representation', 'Test-time Adaptation', 'Sparse-view Computed Tomography', 'Sparse-view Computed Tomography Reconstruction', 'Training Data', 'Deep Learning', 'Deep Models', 'Image Reconstruction', 'Anatomical Sites', 'Image Representation', 'Reconstruction Model', 'Input Representation', 'Implicit Learning', 'Implicit Representation', 'Unknown Noise', 'Unseen Domains', 'Neural Network', 'Ground Truth Image', 'Prior Imaging', 'Original Input', 'Low-dose Computed Tomography', 'Source Domain', 'Peak Signal-to-noise Ratio', 'Computed Tomography Measurements', 'Computed Tomography Images', 'Change Point Detection', 'Target Domain']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Biomedical/healthcare/medicine']",6,"Recently, deep learning has been introduced to solve important medical image reconstruction problems such as sparse-view CT reconstruction. However, the developed deep reconstruction models are generally limited in generalization when applied to out-of-distribution samples in unseen domains. Furthermore, privacy concerns may impede the availability of source-domain training data to retrain or adapt the model to the target-domain testing data, which are quite common in real-world medical applications. To address these issues, we introduce a source-free black-box test-time adaptation method for sparse-view CT reconstruction with unknown noise levels based on prior-informed implicit neural representation learning (PINER). By leveraging implicit neural representation learning to generate the image representations at various noise levels, the proposed method is able to construct the adapted input representations at test time based on the inference of black-box model and output analysis. We performed experiments of source-free test-time adaptation for sparse-view CT reconstruction with unknown noise levels on multiple anatomical sites with different black-box deep reconstruction models, where our method outperforms the state-of-the-art algorithms. Code: https://github.com/efzero/PINER"
PP4AV: A Benchmarking Dataset for Privacy-Preserving Autonomous Driving,"Linh Trinh, Phuong Pham, Hoang Trinh, Nguyen Bach, Dung Nguyen, Giang Nguyen, Huy Nguyen","FPT Software, AI center, Vietnam; VinFast, Hanoi, Vietnam",0,,100,Vietnam,"Massive data collected on public roads for autonomous driving has become more popular in many locations in the world. More collected data leads to more concerns about data privacy, including but not limited to pedestrian faces and surrounding vehicle license plates, which urges for robust solutions for detecting and anonymizing them in realistic road-driving scenarios. Existing public datasets for both face and license plate detection are either not focused on autonomous driving or only in parking lots. In this paper, we introduce a challenging public dataset for face and license plate detection in autonomous driving domain. The dataset is aggregated from visual data that is available in public domain, to cover scenarios from six European cities, including daytime and nighttime, annotated with both faces and license plates. All of the images feature a variety of poses and sizes for both faces and license plates. Our dataset offers not only a benchmark for evaluating data anonymization models but also data to get more insights about privacy-preserving autonomous driving. The experimental results showed that 1) current generic state-of-the-art face and/or license plate detection models do not perform well on a realistic and diverse road-driving dataset like ours, 2) our model trained with autonomous driving data (even with soft-labeling data) outperformed strong but generic models, and 3) the size of faces and license plates is an important factor for evaluating and optimizing the performance of privacy-preserving autonomous driving. The annotation of dataset as well as baseline model and results are available at our github: https://github.com/khaclinh/pp4av.",https://openaccess.thecvf.com/content/WACV2023/html/Trinh_PP4AV_A_Benchmarking_Dataset_for_Privacy-Preserving_Autonomous_Driving_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Trinh_PP4AV_A_Benchmarking_Dataset_for_Privacy-Preserving_Autonomous_Driving_WACV_2023_paper.pdf,,https://github.com/khaclinh/pp4av,,main,Poster,https://ieeexplore.ieee.org/document/10030803/,"['Data privacy', 'Visualization', 'Annotations', 'Benchmark testing', 'Data models', 'Information filtering', 'License plate recognition']","['Benchmark Datasets', 'Autonomous Vehicles', 'Public Datasets', 'Data Privacy', 'Major Roads', 'Detection Dataset', 'Face Detection', 'Face Model', 'License Plate', 'Locations In The World', 'Face Size', 'Training Set', 'Training Data', 'Image Size', 'Image Object', 'Teacher Model', 'Bounding Box', 'Confidence Score', 'Object Classification', 'Camera Images', 'Fisheye Lens', 'Normal Images', 'Front Camera', 'Face Dataset', 'Width Of Face', 'Average Recall', 'Pseudo Labels', 'Street Scenes', 'Object Size', 'Bounding Box Size']","['Applications: Robotics', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",4,"Massive data collected on public roads for autonomous driving has become more popular in many locations in the world. More collected data leads to more concerns about data privacy, including but not limited to pedestrian faces and surrounding vehicle license plates, which urges for robust solutions for detecting and anonymizing them in realistic road-driving scenarios. Existing public datasets for both face and license plate detection are either not focused on autonomous driving or only in parking lots. In this paper, we introduce a challenging public dataset for face and license plate detection in autonomous driving domain. The dataset is aggregated from visual data that is available in public domain, to cover scenarios from six European cities, including daytime and nighttime, annotated with both faces and license plates. All of the images feature a variety of poses and sizes for both faces and license plates. Our dataset offers not only a benchmark for evaluating data anonymization models but also data to get more insights about privacy-preserving autonomous driving. The experimental results showed that 1) current generic state-of-the-art face and/or license plate detection models do not perform well on a realistic and diverse road- driving dataset like ours, 2) our model trained with autonomous driving data (even with soft-labeling data) out- performed strong but generic models, and 3) the size of faces and license plates is an important factor for evaluating and optimizing the performance of privacy-preserving autonomous driving. The annotation of dataset as well as baseline model and results are available at our github: https://github.com/khaclinh/pp4av."
PRN: Panoptic Refinement Network,"Bo Sun, Jason Kuen, Zhe Lin, Philippos Mordohai, Simon Chen",Stevens Institute of Technology; Adobe Inc.,50,USA,50,USA,"Panoptic segmentation is the task of uniquely assigning every pixel in an image to either a semantic label or an individual object instance, generating a coherent and complete scene description. Many current panoptic segmentation methods, however, predict masks of semantic classes and object instances in separate branches, yielding inconsistent predictions. Moreover, because state-of-the-art panoptic segmentation models rely on box proposals, the instance masks predicted are often of low-resolution. To overcome these limitations, we propose the Panoptic Refinement Network (PRN), which takes masks from base panoptic segmentation models and refines them jointly to produce coherent results. PRN extends the offset map-based architecture of Panoptic-Deeplab with several novel ideas including a foreground mask and instance bounding box offsets, as well as coordinate convolutions for improved spatial prediction. Experimental results on COCO and Cityscapes show that PRN can significantly improve already accurate results from a variety of panoptic segmentation networks.",https://openaccess.thecvf.com/content/WACV2023/html/Sun_PRN_Panoptic_Refinement_Network_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sun_PRN_Panoptic_Refinement_Network_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030816/,"['Training', 'Image segmentation', 'Computer vision', 'Semantics', 'Refining', 'Computer architecture', 'Predictive models']","['Bounding Box', 'Object Instances', 'Central Point', 'Semantic Segmentation', 'RGB Images', 'Segmentation Results', 'Detection Module', 'Two-stage Method', 'Segmentation Map', 'Instance Segmentation', 'Encoder Layer', 'Semantic Map', 'Encoder-decoder Network', 'Feature Pyramid Network', 'Decoder Layer', 'Mask R-CNN', 'COCO Dataset', 'Predicted Bounding Box', 'Longer Side', 'Recognition Quality', 'Segmentation Branch', 'Stochastic Gradient Descent', 'Weight Decay', 'Refinement Method', 'Short Side', 'Results Of Network', 'Branch Network', 'Input Modalities', 'Supplement For Details']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Computational photography', 'image and video synthesis']",3,"Panoptic segmentation is the task of uniquely assigning every pixel in an image to either a semantic label or an individual object instance, generating a coherent and complete scene description. Many current panoptic segmentation methods, however, predict masks of semantic classes and object instances in separate branches, yielding inconsistent predictions. Moreover, because state-of-the-art panoptic segmentation models rely on box proposals, the instance masks predicted are often of low-resolution. To overcome these limitations, we propose the Panoptic Refinement Network (PRN), which takes masks from base panoptic segmentation models and refines them jointly to produce coherent results. PRN extends the offset map-based architecture of Panoptic-Deeplab with several novel ideas including a foreground mask and instance bounding box offsets, as well as coordinate convolutions for improved spatial prediction. Experimental results on COCO and Cityscapes show that PRN can significantly improve already accurate results from a variety of panoptic segmentation networks."
PSENet: Progressive Self-Enhancement Network for Unsupervised Extreme-Light Image Enhancement,"Hue Nguyen, Diep Tran, Khoi Nguyen, Rang Nguyen","VinAI Research, Vietnam",0,,100,Vietnam,"The extremes of lighting (e.g. too much or too little light) usually cause many troubles for machine and human vision. Many recent works have mainly focused on under-exposure cases where images are often captured in low-light conditions (e.g. nighttime) and achieved promising results for enhancing the quality of images. However, they are inferior to handling images under over-exposure. To mitigate this limitation, we propose a novel unsupervised enhancement framework which is robust against various lighting conditions while does not require any well-exposed images to serve as the ground-truths. Our main concept is to construct pseudo-ground-truth images synthesized from multiple source images that simulate all potential exposure scenarios to train the enhancement network. Our extensive experiments show that the proposed approach consistently outperforms the current state-of-the-art unsupervised counterparts in several public datasets in terms of both quantitative metrics and qualitative results. Our code is available at https://github.com/VinAIResearch/PSENet-Image-Enhancement",https://openaccess.thecvf.com/content/WACV2023/html/Nguyen_PSENet_Progressive_Self-Enhancement_Network_for_Unsupervised_Extreme-Light_Image_Enhancement_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nguyen_PSENet_Progressive_Self-Enhancement_Network_for_Unsupervised_Extreme-Light_Image_Enhancement_WACV_2023_paper.pdf,,https://github.com/VinAIResearch/PSENet-Image-Enhancement,2210.00712,main,Poster,https://ieeexplore.ieee.org/document/10030972/,"['Training', 'Measurement', 'Codes', 'Lighting', 'Face detection', 'Image enhancement']","['Image Enhancement', 'Image Quality', 'Light Conditions', 'Public Datasets', 'Human Vision', 'Low Light Conditions', 'Results Of Method', 'Input Image', 'Unsupervised Learning', 'Paired Data', 'Unsupervised Methods', 'Learning-based Methods', 'Reference Image', 'Low Contrast', 'Peak Signal-to-noise Ratio', 'Output Image', 'Human Visual System', 'Ground Truth Image', 'Unsupervised Manner', 'Face Detection', 'Structural Similarity Index Measure', 'Low-light Image', 'Gamma Value', 'Previous Epoch', 'Face Dataset', 'Local Contrast', 'Large Margin', 'Final Image', 'Reconstruction Loss', 'Visual Comparison']","['Algorithms: Low-level and physics-based vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",13,"The extremes of lighting (e.g. too much or too little light) usually cause many troubles for machine and human vision. Many recent works have mainly focused on under-exposure cases where images are often captured in low-light conditions (e.g. nighttime) and achieved promising results for enhancing the quality of images. However, they are inferior to handling images under over-exposure. To mitigate this limitation, we propose a novel unsupervised enhancement framework which is robust against various lighting conditions while does not require any well-exposed images to serve as the ground-truths. Our main concept is to construct pseudo-ground-truth images synthesized from multiple source images that simulate all potential exposure scenarios to train the enhancement network. Our extensive experiments show that the proposed approach consistently outperforms the current state-of-the-art unsupervised counterparts in several public datasets in terms of both quantitative metrics and qualitative results. Our code is available at https://github.com/VinAIResearch/PSENet-Image-Enhancement."
Panoptic-Aware Image-to-Image Translation,"Liyun Zhang, Photchara Ratsamee, Bowen Wang, Zhaojie Luo, Yuki Uranishi, Manabu Higashida, Haruo Takemura","Osaka University, Japan",100,Japan,0,,"Despite remarkable progress in image translation, the complex scene with multiple discrepant objects remains a challenging problem. The translated images have low fidelity and tiny objects in fewer details causing unsatisfactory performance in object recognition. Without thorough object perception (i.e., bounding boxes, categories, and masks) of images as prior knowledge, the style transformation of each object will be difficult to track in translation. We propose panoptic-aware generative adversarial networks (PanopticGAN) for image-to-image translation together with a compact panoptic segmentation dataset. The panoptic perception (i.e., foreground instances and background semantics of the image scene) is extracted to achieve alignment between object content codes of the input domain and panoptic-level style codes sampled from the target style space, then refined by a proposed feature masking module for sharping object boundaries. The image-level combination between content and sampled style codes is also merged for higher fidelity image generation. Our proposed method was systematically compared with different competing methods and obtained significant improvement in both image quality and object recognition performance.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Panoptic-Aware_Image-to-Image_Translation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Panoptic-Aware_Image-to-Image_Translation_WACV_2023_paper.pdf,,,2112.01926,main,Poster,https://ieeexplore.ieee.org/document/10030413,"['Image quality', 'Training', 'Image segmentation', 'Computer vision', 'Codes', 'Image synthesis', 'Semantics']","['Semantic', 'Image Quality', 'Image Object', 'Object Recognition', 'Bounding Box', 'Quality Performance', 'Generative Adversarial Networks', 'Multiple Objects', 'Improve Image Quality', 'Object Perception', 'Object Boundaries', 'Content Coding', 'Input Domain', 'Tiny Objects', 'Improve Recognition Performance', 'Infrared Imaging', 'Input Image', 'Feature Maps', 'Color Images', 'Fully-connected Layer', 'Fréchet Inception Distance', 'Recognition Quality', 'Target Domain', 'Objective Scores', 'Hinge Loss', 'Residual Block', 'Object Instances', 'Saliency Map', 'Pre-trained Network', 'Instance Segmentation']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"Despite remarkable progress in image translation, the complex scene with multiple discrepant objects remains a challenging problem. The translated images have low fidelity and tiny objects in fewer details causing unsatisfactory performance in object recognition. Without thorough object perception (i.e., bounding boxes, categories, and masks) of images as prior knowledge, the style transformation of each object will be difficult to track in translation. We propose panoptic-aware generative adversarial networks (PanopticGAN) for image-to-image translation together with a compact panoptic segmentation dataset. The panoptic perception (i.e., foreground instances and background semantics of the image scene) is extracted to achieve alignment between object content codes of the input domain and panoptic-level style codes sampled from the target style space, then refined by a proposed feature masking module for sharping object boundaries. The image-level combination between content and sampled style codes is also merged for higher fidelity image generation. Our proposed method was systematically compared with different competing methods and obtained significant improvement in both image quality and object recognition performance."
Partially Calibrated Semi-Generalized Pose From Hybrid Point Correspondences,"Snehal Bhayani, Torsten Sattler, Viktor Larsson, Janne Heikkilä, Zuzana Kukelova","Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague; Center for Machine Vision and Signal Analysis, University of Oulu, Finland; Lund University, Sweden; Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague",100,"Czech Republic, Finland, Sweden",0,,"In this paper we study the problem of estimating the semi-generalized pose of a partially calibrated camera, i.e., the pose of a perspective camera with unknown focal length w.r.t. a generalized camera, from a hybrid set of 2D-2D and 2D-3D point correspondences. We study all possible camera configurations within the generalized camera system. To derive practical solvers to previously unsolved challenging configurations, we test different parameterizations as well as different solving strategies based on the state-of-the-art methods for generating efficient polynomial solvers. We evaluate the three most promising solvers, i.e., the H51f solver with five 2D-2D correspondences and one 2D-3D correspondence viewed by the same camera inside generalized camera, the H32f solver with three 2D-2D and two 2D-3D correspondences, and the H13f solver with one 2D-2D and three 2D-3D correspondences, on synthetic and real data. We show that in the presence of noise in the 3D points these solvers provide better estimates than the corresponding absolute pose solvers.",https://openaccess.thecvf.com/content/WACV2023/html/Bhayani_Partially_Calibrated_Semi-Generalized_Pose_From_Hybrid_Point_Correspondences_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bhayani_Partially_Calibrated_Semi-Generalized_Pose_From_Hybrid_Point_Correspondences_WACV_2023_paper.pdf,,,2209.15072,main,Poster,https://ieeexplore.ieee.org/document/10030135/,"['Computer vision', 'Three-dimensional displays', 'Estimation', 'Cameras', 'Testing']","['Corresponding Points', 'Focal Length', '3D Point', 'Camera Pose', 'Perspective Camera', 'Coordinate System', 'Local System', 'Image Noise', 'Coordinate Transformation', 'Image Distortion', 'Unknown Variables', 'Pose Estimation', 'Polynomial Equation', 'Automatic Generation', 'Solution Strategy', 'Minimal Solution', 'Visual Localization', 'Translation Vector', 'Query Image', 'Homography', 'Relative Pose', 'Pinhole Camera', 'Calibration Matrix', 'Problem Posing']","['Algorithms: 3D computer vision', 'Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"We study the problem of estimating the semi-generalized pose of a partially calibrated camera, i.e., the pose of a perspective camera with unknown focal length w.r.t. a generalized camera, from a hybrid set of 2D-2D and 2D-3D point correspondences. We study all possible camera configurations within the generalized camera system. To derive practical solvers to previously unsolved challenging configurations, we test different parameterizations as well as different solving strategies based on state-of-the-art methods for generating efficient polynomial solvers. We evaluate the three most promising solvers, i.e., the H51f solver with five 2D-2D correspondences and one 2D-3D match viewed by the same camera inside the generalized camera, the H32f solver with three 2D-2D and two 2D-3D correspondences, and the H13f solver with one 2D-2D and three 2D-3D matches, on synthetic and real data. We show that in the presence of noise in the 3D points these solvers provide better estimates than the corresponding absolute pose solvers."
Patch-Based Privacy Preserving Neural Network for Vision Tasks,"Mitsuhiro Mabuchi, Tetsuya Ishikawa","Toyota Motor Corporation, Tokyo, Japan; Woven Core, Inc., Tokyo, Japan",0,,100,Japan,"As machine learning technology is increasingly adopted into a variety of application domains, the potential risks of data leakage are becoming more serious in certain cases where the data contains highly sensitive information. While some privacy-preserving learning mechanisms for image data, such as SplitNN, enable the training of models without sharing private data on a central server, there exists a trade-off between security and computational cost to a client device. We propose a new mechanism to achieve higher level security and lower computational cost on a client device while maintaining model performance. Our approach, called Patch SplitNN, is based on SplitNN architecture that divides a CNN into two networks, called upper and lower. The difference from that previous work is to input individual image patches into multiple upper models, before concatenating their outputs before the lower model. For further improvement of the upper model training, we introduce an additional network and a loss function into the training process. We demonstrate our Patch SplitNN can classify images as accurately as a ResNet18 on various image classification datasets (CIFAR-10, CIFAR-100, and PCam) under multiple conditions (e.g. patching patterns, dropping patches).",https://openaccess.thecvf.com/content/WACV2023/html/Mabuchi_Patch-Based_Privacy_Preserving_Neural_Network_for_Vision_Tasks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mabuchi_Patch-Based_Privacy_Preserving_Neural_Network_for_Vision_Tasks_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030336/,"['Training', 'Privacy', 'Adaptation models', 'Computational modeling', 'Face recognition', 'Neural networks', 'Computational efficiency']","['Neural Network', 'Vision Tasks', 'Privacy Preservation', 'Loss Function', 'Contralateral', 'Model Performance', 'Image Classification', 'Central Server', 'Risk Of Leakage', 'Deep Neural Network', 'Image Size', 'Encryption', 'Object Detection', 'Learning Network', 'Face Recognition', 'Generative Adversarial Networks', 'Competitive Performance', 'RGB Images', 'Patch Size', 'Area Under Curve', 'Federated Learning', 'Vision Transformer', 'Original Image Size', 'Data Augmentation Methods', 'Residual Block', 'Image Classification Tasks', 'Client-side', 'Intermediate Features', 'Number Of Training Data', 'Data Augmentation']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",,"As machine learning technology is increasingly adopted into a variety of application domains, the potential risks of data leakage are becoming more serious in certain cases where the data contains highly sensitive information. While some privacy-preserving learning mechanisms for image data, such as SplitNN, enable the training of models without sharing private data on a central server, there exists a trade-off between security and computational cost to a client device. We propose a new mechanism to achieve higher level security and lower computational cost on a client device while maintaining model performance. Our approach, called Patch SplitNN, is based on SplitNN architecture that divides a CNN into two networks, called upper and lower. The difference from that previous work is to input individual image patches into multiple upper models, before concatenating their outputs before the lower model. For further improvement of the upper model training, we introduce an additional network and a loss function into the training process. We demonstrate our Patch SplitNN can classify images as accurately as a ResNet18 on various image classification datasets (CIFAR-10, CIFAR-100, and PCam) under multiple conditions (e.g. patching patterns, dropping patches)."
Patch-Level Gaze Distribution Prediction for Gaze Following,"Qiaomu Miao, Minh Hoai, Dimitris Samaras","Stony Brook University, Stony Brook, NY 11794, USA",100,USA,0,,"Gaze following aims to predict where a person is looking in a scene, by predicting the target location, or indicating that the target is located outside the image. Recent works detect the gaze target by training a heatmap regression task with a pixel-wise mean-square error (MSE) loss, while formulating the in/out prediction task as a binary classification task. This training formulation puts a strict, pixel-level constraint in higher resolution on the single annotation available in training, and does not consider annotation variance and the correlation between the two subtasks. To address these issues, we introduce the patch distribution prediction (PDP) method. We replace the in/out prediction branch in previous models with the PDP branch, by predicting a patch-level gaze distribution that also considers the outside cases. Experiments show that our model regularizes the MSE loss by predicting better heatmap distributions on images with larger annotation variances, meanwhile bridging the gap between the target prediction and in/out prediction subtasks, showing a significant improvement in performance on both subtasks on public gaze following datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Miao_Patch-Level_Gaze_Distribution_Prediction_for_Gaze_Following_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Miao_Patch-Level_Gaze_Distribution_Prediction_for_Gaze_Following_WACV_2023_paper.pdf,,,2211.11062,main,Poster,https://ieeexplore.ieee.org/document/10030998/,"['Training', 'Heating systems', 'Computer vision', 'Image resolution', 'Correlation', 'Annotations', 'Mean square error methods']","['Gaze Distribution', 'Large Variation', 'Target Location', 'Target Prediction', 'Prediction Task', 'Binary Classification Task', 'Mean Square Error Loss', 'Distribution Of Patches', 'Training Set', 'Human Behavior', 'Convolutional Layers', 'Kullback-Leibler', 'Attention Module', 'Depth Map', 'Area Under Curve', 'Scene Images', 'Distance Metrics', 'Binary Cross Entropy', 'Binary Cross-entropy Loss', 'Temporal Modulation', 'Prediction Head', 'Gaze Direction', 'Human Gaze', 'Multiple Patches', 'Coarse Scale', 'Higher Area Under Curve', 'Eye Images', 'Patch Scale', 'Dot Product', 'Performance Gain']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",9,"Gaze following aims to predict where a person is looking in a scene, by predicting the target location, or indicating that the target is located outside the image. Recent works detect the gaze target by training a heatmap regression task with a pixel-wise mean-square error (MSE) loss, while formulating the in/out prediction task as a binary classification task. This training formulation puts a strict, pixel-level constraint in higher resolution on the single annotation available in training, and does not consider annotation variance and the correlation between the two subtasks. To address these issues, we introduce the patch distribution prediction (PDP) method. We replace the in/out prediction branch in previous models with the PDP branch, by predicting a patch-level gaze distribution that also considers the outside cases. Experiments show that our model regularizes the MSE loss by predicting better heatmap distributions on images with larger annotation variances, meanwhile bridging the gap between the target prediction and in/out prediction subtasks, showing a significant improvement in performance on both subtasks on public gaze following datasets."
PatchDropout: Economizing Vision Transformers Using Patch Dropout,"Yue Liu, Christos Matsoukas, Fredrik Strand, Hossein Azizpour, Kevin Smith","KTH Royal Institute of Technology, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Science for Life Laboratory, Stockholm, Sweden; AstraZeneca, Gothenburg, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Science for Life Laboratory, Stockholm, Sweden; Karolinska Institutet, Stockholm, Sweden; Karolinska University Hospital, Stockholm, Sweden",62.5,Sweden,37.5,Sweden,"Vision transformers have demonstrated the potential to outperform CNNs in a variety of vision tasks. But the computational and memory requirements of these models prohibit their use in many applications, especially those that depend on high-resolution images, such as medical image classification. Efforts to train ViTs more efficiently are overly complicated, necessitating architectural changes or intricate training schemes. In this work, we show that standard ViT models can be efficiently trained at high resolution by randomly dropping input image patches. This simple approach, PatchDropout, reduces FLOPs and memory by at least 50% in standard natural image datasets such as ImageNet, and those savings only increase with image size. On CSAW, a high-resolution medical dataset, we observe a 5x savings in computation and memory using PatchDropout, along with a boost in performance. For practitioners with a fixed computational or memory budget, PatchDropout makes it possible to choose image resolution, hyperparameters, or model size to get the most performance out of their model.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_PatchDropout_Economizing_Vision_Transformers_Using_Patch_Dropout_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_PatchDropout_Economizing_Vision_Transformers_Using_Patch_Dropout_WACV_2023_paper.pdf,,,2208.0722,main,Poster,https://ieeexplore.ieee.org/document/10030183/,"['Training', 'Image resolution', 'Computational modeling', 'Biological system modeling', 'Memory management', 'Transformers', 'Robustness']","['Vision Transformer', 'High-resolution Images', 'Image Classification', 'Image Size', 'ImageNet', 'Computational Requirements', 'Image Patches', 'Standard Datasets', 'Training Efficiency', 'Computational Savings', 'Use In Many Applications', 'Medical Image Classification', 'Input Image Patch', 'Predictive Performance', 'Sequence Length', 'Patch Size', 'Large Image', 'Augmentation Methods', 'Computational Demands', 'Transformer Block', 'Input Sequence Length', 'Theoretical Complexity', 'Sequence Of Tokens', 'Input Patch', 'Memory Reduction', 'Regularization Effect', 'Input Tokens', 'Multi-head Self-attention', 'Position Embedding']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Biomedical/healthcare/medicine']",9,"Vision transformers have demonstrated the potential to outperform CNNs in a variety of vision tasks. But the computational and memory requirements of these models prohibit their use in many applications, especially those that depend on high-resolution images, such as medical image classification. Efforts to train ViTs more efficiently are overly complicated, necessitating architectural changes or intricate training schemes. In this work, we show that standard ViT models can be efficiently trained at high resolution by randomly dropping input image patches. This simple approach, PatchDropout, reduces FLOPs and memory by at least 50% in standard natural image datasets such as ImageNet, and those savings only increase with image size. On CSAW, a high-resolution medical dataset, we observe a 5× savings in computation and memory using PatchDropout, along with a boost in performance. For practitioners with a fixed computational or memory budget, PatchDropout makes it possible to choose image resolution, hyperparameters, or model size to get the most performance out of their model."
PatchZero: Defending Against Adversarial Patch Attacks by Detecting and Zeroing the Patch,"Ke Xu, Yao Xiao, Zhaoheng Zheng, Kaijie Cai, Ram Nevatia",University of Southern California,100,USA,0,,"Adversarial patch attacks mislead neural networks by injecting adversarial pixels within a local region. Patch attacks can be highly effective in a variety of tasks and physically realizable via attachment (e.g. a sticker) to the real-world objects. Despite the diversity in attack patterns, adversarial patches tend to be highly textured and different in appearance from natural images. We exploit this property and present PatchZero, a general defense pipeline against white-box adversarial patches without retraining the downstream classifier or detector. Specifically, our defense detects adversaries at the pixel-level and ""zeros out"" the patch region by repainting with mean pixel values. We further design a two-stage adversarial training scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA defense performance on the image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and video classification (UCF101) tasks with little degradation in benign performance. In addition, PatchZero transfers to different patch shapes and attack types.",https://openaccess.thecvf.com/content/WACV2023/html/Xu_PatchZero_Defending_Against_Adversarial_Patch_Attacks_by_Detecting_and_Zeroing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xu_PatchZero_Defending_Against_Adversarial_Patch_Attacks_by_Detecting_and_Zeroing_WACV_2023_paper.pdf,,,2207.01795,main,Poster,https://ieeexplore.ieee.org/document/10030535/,"['Training', 'Degradation', 'Shape', 'Pipelines', 'Neural networks', 'Object detection', 'Robustness']","['Adversarial Attacks', 'Adversarial Patches', 'Patch Attacks', 'Classification Task', 'Image Classification', 'Object Detection', 'Training Strategy', 'ImageNet', 'Detection Task', 'Natural Images', 'Video Analysis', 'Types Of Attacks', 'Image Classification Tasks', 'Adversarial Training', 'Two-stage Strategy', 'Object Detection Task', 'Image Object Detection', 'Mean Pixel Value', 'Patch Shape', 'General Pipeline', 'Projected Gradient Descent', 'JPEG Compression', 'Patch Size', 'Top-1 Accuracy', 'Adversarial Examples', 'Defense Methods', 'Digital Watermarking', 'Video Compression', 'Image X', 'Knowledge Of Size']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",9,"Adversarial patch attacks mislead neural networks by injecting adversarial pixels within a local region. Patch attacks can be highly effective in a variety of tasks and physically realizable via attachment (e.g. a sticker) to the real-world objects. Despite the diversity in attack patterns, adversarial patches tend to be highly textured and different in appearance from natural images. We exploit this property and present PatchZero, a general defense pipeline against white-box adversarial patches without retraining the downstream classifier or detector. Specifically, our defense detects adversaries at the pixel-level and ""zeros out"" the patch region by repainting with mean pixel values. We further design a two-stage adversarial training scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA defense performance on the image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and video classification (UCF101) tasks with little degradation in benign performance. In addition, PatchZero transfers to different patch shapes and attack types."
Perceiver-VL: Efficient Vision-and-Language Modeling With Iterative Latent Attention,"Zineng Tang, Jaemin Cho, Jie Lei, Mohit Bansal",UNC Chapel Hill,100,USA,0,,"We present Perceiver-VL, a vision-and-language framework that efficiently handles high-dimensional multimodal inputs such as long videos and text. Powered by the iterative latent-cross-attention of Perceiver, our framework scales with linear complexity, in contrast to the quadratic complexity of self-attention used in many state-of-the-art transformer-based models. To further improve the efficiency of our framework, we also study applying LayerDrop on cross-attention layers and introduce a mixed-stream architecture for cross-modal retrieval. We evaluate Perceiver-VL on diverse video-text and image-text benchmarks, where Perceiver-VL achieves the lowest GFLOPs and latency, while maintaining competitive performance. In addition, we also provide comprehensive analyses over various aspects of our framework, including pretraining data, scalability of latent size and input size, dropping cross-attention layers at inference to reduce latency, modality aggregation strategy, positional encoding, and weight initialization strategy.",https://openaccess.thecvf.com/content/WACV2023/html/Tang_Perceiver-VL_Efficient_Vision-and-Language_Modeling_With_Iterative_Latent_Attention_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tang_Perceiver-VL_Efficient_Vision-and-Language_Modeling_With_Iterative_Latent_Attention_WACV_2023_paper.pdf,,https://github.com/zinengtang/Perceiver_VL,,main,Poster,https://ieeexplore.ieee.org/document/10030503/,"['Training', 'Analytical models', 'Scalability', 'Computer architecture', 'Benchmark testing', 'Transformers', 'Complexity theory']","['Benchmark', 'Competitive Performance', 'Input Size', 'Input Modalities', 'Linear Complexity', 'Weight Initialization', 'Positional Encoding', 'Quadratic Complexity', 'Image Size', 'Adam Optimizer', 'Recent Models', 'Latent Space', 'Question Answering', 'Video Frames', 'Visual Input', 'Inference Time', 'Training Details', 'Frame Size', 'Attention Block', 'Decoder Output', 'Input Array', 'Masked Language Model', 'Visual Question Answering', 'High-dimensional Input', 'Position Embedding', 'Pre-training Dataset', 'Self-attention Layer', 'Multimodal Input', 'Single Architecture', 'Attention Layer']",['Algorithms: Vision + language and/or other modalities'],4,"We present PERCEIVER-VL, a vision-and-language framework that efficiently handles high-dimensional multi- modal inputs such as long videos and text. Powered by the iterative latent-cross-attention of Perceiver, our framework scales with linear complexity, in contrast to the quadratic complexity of self-attention used in many state-of-the-art transformer-based models. To further improve the efficiency of our framework, we also study applying LayerDrop on cross-attention layers and introduce a mixed- stream architecture for cross-modal retrieval. We evaluate PERCEIVER-VL on diverse video-text and image-text benchmarks, where PERCEIVER-VL achieves the lowest GFLOPs and latency, while maintaining competitive performance. In addition, we also provide comprehensive analyses over various aspects of our framework, including pretraining data, scalability of latent size and input size, dropping cross-attention layers at inference to reduce latency, modality aggregation strategy, positional encoding, and weight initialization strategy.
<sup>1</sup>"
Perceptual Image Enhancement for Smartphone Real-Time Applications,"Marcos V. Conde, Florin Vasluianu, Javier Vazquez-Corral, Radu Timofte","Computer Vision Center and Computer Science Dept., Universitat Autònoma de Barcelona, Spain; Computer Vision Lab, CAIDAS, IFI, University of Würzburg, Germany",100,"Germany, Spain",0,,"Recent advances in camera designs and imaging pipelines allow us to capture high-quality images using smartphones. However, due to the small size and lens limitations of the smartphone cameras, we commonly find artifacts or degradation in the processed images. The most common unpleasant effects are noise artifacts, diffraction artifacts, blur, and HDR overexposure. Deep learning methods for image restoration can successfully remove these artifacts. However, most approaches are not suitable for real-time applications on mobile devices due to their heavy computation and memory requirements. In this paper, we propose LPIENet, a lightweight network for perceptual image enhancement, with the focus on deploying it on smartphones. Our experiments show that, with much fewer parameters and operations, our model can deal with the mentioned artifacts and achieve competitive performance compared with state-of-the-art methods on standard benchmarks. Moreover, to prove the efficiency and reliability of our approach, we deployed the model directly on commercial smartphones and evaluated its performance. Our model can process 2K resolution images under 1s without specific optimization for the mobile devices.",https://openaccess.thecvf.com/content/WACV2023/html/Conde_Perceptual_Image_Enhancement_for_Smartphone_Real-Time_Applications_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Conde_Perceptual_Image_Enhancement_for_Smartphone_Real-Time_Applications_WACV_2023_paper.pdf,,https://github.com/mv-lab/AISP,2210.13552,main,Poster,https://ieeexplore.ieee.org/document/10030836/,"['Performance evaluation', 'Runtime', 'Image resolution', 'Computational modeling', 'Memory management', 'Benchmark testing', 'Real-time systems']","['Image Enhancement', 'Deep Learning', 'Mobile Devices', 'Fewer Parameters', 'High Dynamic Range', 'Denoising', 'Dynamic Range', 'Input Image', 'Single Image', 'Deep Learning Models', 'Training Images', 'Image Sensor', 'Clear Image', 'Residual Block', 'Dirac Delta', 'Point Spread Function', 'Organic Light-emitting Diodes', 'Top Image', 'Traditional Problem', 'Image Deblurring', 'Restoration Problem', 'Smartphone Devices']","['Applications: Smartphones/end user devices', 'Embedded sensing/real-time techniques', 'Visualization']",19,"Recent advances in camera designs and imaging pipelines allow us to capture high-quality images using smartphones. However, due to the small size and lens limitations of the smartphone cameras, we commonly find artifacts or degradation in the processed images. The most common unpleasant effects are noise artifacts, diffraction artifacts, blur, and HDR overexposure. Deep learning methods for image restoration can successfully remove these artifacts. However, most approaches are not suitable for real-time applications on mobile devices due to their heavy computation and memory requirements.In this paper, we propose LPIENet, a lightweight network for perceptual image enhancement, with the focus on deploying it on smartphones. Our experiments show that, with much fewer parameters and operations, our model can deal with the mentioned artifacts and achieve competitive performance compared with state-of-the-art methods on standard benchmarks. Moreover, to prove the efficiency and reliability of our approach, we deployed the model directly on commercial smartphones and evaluated its performance. Our model can process 2K resolution images under 1 second in mid-level commercial smartphones."
Performance Comparison of DVS Data Spatial Downscaling Methods Using Spiking Neural Networks,"Amélie Gruel, Jean Martinet, Bernabé Linares-Barranco, Teresa Serrano-Gotarredona","IMSE-CNM, Sevilla, Spain; CNRS, i3S, Universit ´e Cˆote d’Azur, Sophi-Antipolis, France",100,"France, Spain",0,,"Dynamic Vision Sensors (DVS) are an unconventional type of camera that produces sparse and asynchronous event data, which has recently led to a strong increase in its use for computer vision tasks namely in robotics. Embedded systems face limitations in terms of energy resources, memory, computational power, and communication bandwidth. Hence, this application calls for a way to reduce the amount of data to be processed while keeping the relevant information for the task at hand. We thus believe that a formal definition of event data reduction methods will provide a step further towards sparse data processing. The contributions of this paper are twofold: we introduce two complementary neuromorphic methods based on Spiking Neural Networks for DVS data spatial reduction, which is to best of our knowledge the first proposal of neuromorphic event data reduction; then we study for each method the trade-off between the amount of information kept after reduction, the performance of gesture classification after reduction and their capacity to handle events in real time. We demonstrate here that the proposed SNN-based methods outperform existing methods in a classification task for most dividing factors and are significantly better at handling data in real time, and make therefore the optimal choice for fully-integrated energy-efficient event data reduction running dynamically on a neuromorphic platform. Our code is publicly available online at: https://github.com/amygruel/EvVisu.",https://openaccess.thecvf.com/content/WACV2023/html/Gruel_Performance_Comparison_of_DVS_Data_Spatial_Downscaling_Methods_Using_Spiking_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gruel_Performance_Comparison_of_DVS_Data_Spatial_Downscaling_Methods_Using_Spiking_WACV_2023_paper.pdf,,https://github.com/amygruel/EvVisu,,main,Poster,https://ieeexplore.ieee.org/document/10030433/,"['Computer vision', 'Embedded systems', 'Neuromorphics', 'Neural networks', 'Robot vision systems', 'Vision sensors', 'Real-time systems']","['Spiking Neural Networks', 'Downscaling Method', 'Dynamic Vision Sensor', 'Data Processing', 'Amount Of Information', 'Power Calculation', 'Event Data', 'Data Reduction', 'Sparse Data', 'Communication Bandwidth', 'Events In Real Time', 'Negative Consequences', 'Computation Time', 'Running Time', 'Positive Bias', 'Input Layer', 'Hyperparameter Tuning', 'Output Neurons', 'Connection Weights', 'Synaptic Weights', 'Temporal Density', 'Contrast Threshold', 'Input Events', 'Simulation Run Time', 'Embedded System', 'Event Stream', 'Neuromorphic Hardware', 'Leaky Integrate-and-fire', 'Sensor Size']","['Applications: Robotics', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Embedded sensing/real-time techniques']",3,"Dynamic Vision Sensors (DVS) are an unconventional type of camera that produces sparse and asynchronous event data, which has recently led to a strong increase in its use for computer vision tasks namely in robotics. Embedded systems face limitations in terms of energy resources, memory, computational power, and communication bandwidth. Hence, this application calls for a way to reduce the amount of data to be processed while keeping the relevant information for the task at hand. We thus believe that a formal definition of event data reduction methods will provide a step further towards sparse data processing.The contributions of this paper are twofold: we introduce two complementary neuromorphic methods based on Spiking Neural Networks for DVS data spatial reduction, which is to best of our knowledge the first proposal of neuromorphic event data reduction; then we study for each method the trade-off between the amount of information kept after reduction, the performance of gesture classification after reduction and their capacity to handle events in real time. We demonstrate here that the proposed SNN-based methods outperform existing methods in a classification task for most dividing factors and are significantly better at handling data in real time, and make therefore the optimal choice for fully-integrated energy-efficient event data reduction running dynamically on a neuromorphic platform. Our code is publicly available online at: https://github.com/amygruel/EvVisu."
Performer: A Novel PPG-to-ECG Reconstruction Transformer for a Digital Biomarker of Cardiovascular Disease Detection,Ella Lan,"The Harker School, 500 Saratoga Ave, San Jose, CA 95129",100,USA,0,,"Electrocardiography (ECG), an electrical measurement which captures cardiac activities, is the gold standard for diagnosing cardiovascular disease (CVD). However, ECG is infeasible for continuous cardiac monitoring due to its requirement for user participation. By contrast, photoplethysmography (PPG) provides easy-to-collect data, but its limited accuracy constrains its clinical usage. To combine the advantages of both signals, recent studies incorporate various deep learning techniques for the reconstruction of PPG signals to ECG; however, the lack of contextual information as well as the limited abilities to denoise biomedical signals ultimately constrain model performance. In this research, we propose Performer, a novel Transformer-based architecture that reconstructs ECG from PPG and combines the PPG and reconstructed ECG as multiple modalities for CVD detection. This method is the first time that Transformer sequence-to-sequence translation has been performed on biomedical waveform reconstruction, combining the advantages of both PPG and ECG. We also create Shifted Patch-based Attention (SPA), an effective method to encode/decode the biomedical waveforms. Through fetching the various sequence lengths and capturing cross-patch connections, SPA maximizes the signal processing for both local features and global contextual representations. The proposed architecture generates a state-of-the-art performance of 0.29 RMSE for the reconstruction of PPG to ECG on the BIDMC database, surpassing prior studies. We also evaluated this model on the MIMIC-III dataset, achieving a 95.9% accuracy in CVD detection, and on the PPG-BP dataset, achieving 75.9% accuracy in related CVD diabetes detection, indicating its generalizability. As a proof of concept, an earring wearable named PEARL (prototype), was designed to scale up the point-of-care (POC) healthcare system.",https://openaccess.thecvf.com/content/WACV2023/html/Lan_Performer_A_Novel_PPG-to-ECG_Reconstruction_Transformer_for_a_Digital_Biomarker_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lan_Performer_A_Novel_PPG-to-ECG_Reconstruction_Transformer_for_a_Digital_Biomarker_WACV_2023_paper.pdf,,,2204.11795,main,Poster,,,,,,
Phantom Sponges: Exploiting Non-Maximum Suppression To Attack Deep Object Detectors,"Avishag Shapira, Alon Zolfi, Luca Demetrio, Battista Biggio, Asaf Shabtai","Ben-Gurion University, Israel; University of Cagliari, Italy; University of Genoa, Italy; The Open University, Israel",100,"Israel, Italy",0,,"Adversarial attacks against deep learning-based object detectors have been studied extensively in the past few years. Most of the attacks proposed have targeted the model's integrity (i.e., caused the model to make incorrect predictions), while adversarial attacks targeting the model's availability, a critical aspect in safety-critical domains such as autonomous driving, have not yet been explored by the machine learning research community. In this paper, we propose a novel attack that negatively affects the decision latency of an end-to-end object detection pipeline. We craft a universal adversarial perturbation (UAP) that targets a widely used technique integrated in many object detector pipelines - non-maximum suppression (NMS). Our experiments demonstrate the proposed UAP's ability to increase the processing time of individual frames by adding ""phantom"" objects that overload the NMS algorithm while preserving the detection of the original objects which allows the attack to go undetected for a longer period of time.",https://openaccess.thecvf.com/content/WACV2023/html/Shapira_Phantom_Sponges_Exploiting_Non-Maximum_Suppression_To_Attack_Deep_Object_Detectors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shapira_Phantom_Sponges_Exploiting_Non-Maximum_Suppression_To_Attack_Deep_Object_Detectors_WACV_2023_paper.pdf,,,2205.13618,main,Poster,https://ieeexplore.ieee.org/document/10030526/,"['Perturbation methods', 'Pipelines', 'Phantoms', 'Detectors', 'Object detection', 'Predictive models', 'Prediction algorithms']","['Object Detection', 'Non-maximum Suppression', 'Deep Object Detection', 'Processing Time', 'Original Objective', 'Adversarial Attacks', 'Detection Pipeline', 'Adversarial Perturbations', 'Running Time', 'Adaptive Algorithm', 'Image Object', 'Intersection Over Union', 'Bounding Box', 'Common Categories', 'Final Prediction', 'Confidence Score', 'Target Class', 'Inference Time', 'Target Model', 'Deep Learning-based Models', 'You Only Look Once', 'Object Detection Model', 'Adversarial Examples', 'Intersection Over Union Value', 'Values Of L1', 'Projected Gradient Descent', 'Two-stage Detectors', 'Values Of L2', 'Loss Function', 'Loss Of Components']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods']",10,"Adversarial attacks against deep learning-based object detectors have been studied extensively in the past few years. Most of the attacks proposed have targeted the model’s integrity (i.e., caused the model to make incorrect predictions), while adversarial attacks targeting the model’s availability, a critical aspect in safety-critical domains such as autonomous driving, have not yet been explored by the machine learning research community. In this paper, we propose a novel attack that negatively affects the decision latency of an end-to-end object detection pipeline. We craft a universal adversarial perturbation (UAP) that targets a widely used technique integrated in many object detector pipelines – non-maximum suppression (NMS). Our experiments demonstrate the proposed UAP’s ability to increase the processing time of individual frames by adding ""phantom"" objects that overload the NMS algorithm while preserving the detection of the original objects which allows the attack to go undetected for a longer period of time."
Physically Plausible Animation of Human Upper Body From a Single Image,"Ziyuan Huang, Zhengping Zhou, Yung-Yu Chuang, Jiajun Wu, C. Karen Liu",Stanford University; National Taiwan University,100,"Taiwan, USA",0,,"We present a new method for generating controllable, dynamically responsive, and photorealistic human animations. Given an image of a person, our system allows the user to generate Physically plausible Upper Body Animation (PUBA) using interaction in the image space, such as dragging their hand to various locations. We formulate a reinforcement learning problem to train a dynamic model that predicts the person's next 2D state (i.e., keypoints on the image) conditioned on a 3D action (i.e., joint torque), and a policy that outputs optimal actions to control the person to achieve desired goals. The dynamic model leverages the expressiveness of 3D simulation and the visual realism of 2D videos. PUBA generates 2D keypoint sequences that achieve task goals while being responsive to forceful perturbation. The sequences of keypoints are then translated by a pose-to-image generator to produce the final photorealistic video.",https://openaccess.thecvf.com/content/WACV2023/html/Huang_Physically_Plausible_Animation_of_Human_Upper_Body_From_a_Single_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Huang_Physically_Plausible_Animation_of_Human_Upper_Body_From_a_Single_WACV_2023_paper.pdf,,,2212.04741,main,Poster,https://ieeexplore.ieee.org/document/10030849/,"['Solid modeling', 'Visualization', 'Three-dimensional displays', 'Torque', 'Perturbation methods', 'Reinforcement learning', 'Predictive models']","['Upper Body', 'Dynamic Model', '3D Simulation', 'Person Image', 'Joint Torque', '2D Keypoints', 'Training Data', 'Data Augmentation', 'Physical Force', '3D Point', 'Proportional-integral-derivative', 'Reward Function', 'Deep Reinforcement Learning', '3D Representation', 'Human Motion', '3D Method', 'Gated Recurrent Unit', 'Humanoid', '2D Representation', 'Temporal Coherence', '2D Pose', 'Proximal Policy Optimization', '3D Joint', '3D Pose', 'Training Policy', 'Photo-realistic Images', 'Baseline Methods', 'User Study', 'Flow Vector', 'Inverse Dynamics']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Computational photography', 'image and video synthesis', 'Virtual/augmented reality']",1,"We present a new method for generating controllable, dynamically responsive, and photorealistic human animations. Given an image of a person, our system allows the user to generate Physically plausible Upper Body Animation (PUBA) using interaction in the image space, such as dragging their hand to various locations. We formulate a reinforcement learning problem to train a dynamic model that predicts the person’s next 2D state (i.e., keypoints on the image) conditioned on a 3D action (i.e., joint torque), and a policy that outputs optimal actions to control the person to achieve desired goals. The dynamic model leverages the expressiveness of 3D simulation and the visual realism of 2D videos. PUBA generates 2D keypoint sequences that achieve task goals while being responsive to forceful perturbation. The sequences of keypoints are then translated by a pose-to-image generator to produce the final photorealistic video."
Pik-Fix: Restoring and Colorizing Old Photos,"Runsheng Xu, Zhengzhong Tu, Yuanqi Du, Xiaoyu Dong, Jinlong Li, Zibo Meng, Jiaqi Ma, Alan Bovik, Hongkai Yu","Innopeak Technology Inc.; University of Texas at Austin; University of California, Los Angeles; Northwestern University; Cornell University; Cleveland State University",83.33333333,USA,16.66666667,USA,"Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack of large-scale datasets of old photos makes addressing this restoration task very challenging. Here we present a novel reference-based end-to-end learning framework that is able to both repair and colorize old, degraded pictures. Our proposed framework consists of three modules: a restoration sub-network that conducts restoration from degradations, a similarity network that performs color histogram matching and color transfer, and a colorization subnet that learns to predict the chroma elements of images conditioned on chromatic reference signals. The overall system makes uses of color histogram priors from reference images, which greatly reduces the need for large-scale training data. We have also created a first-of-a-kind public dataset of real old photos that are paired with ground truth ""pristine"" photos that have been manually restored by PhotoShop experts. We conducted extensive experiments on this dataset and synthetic datasets, and found that our method significantly outperforms previous state-of-the-art models using both qualitative comparisons and quantitative measurements. The code is available at https://github.com/DerrickXuNu/Pik-Fix.",https://openaccess.thecvf.com/content/WACV2023/html/Xu_Pik-Fix_Restoring_and_Colorizing_Old_Photos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xu_Pik-Fix_Restoring_and_Colorizing_Old_Photos_WACV_2023_paper.pdf,,https://github.com/DerrickXuNu/Pik-Fix,,main,Poster,https://ieeexplore.ieee.org/document/10030385/,"['Degradation', 'Deep learning', 'Histograms', 'Visualization', 'Image color analysis', 'Training data', 'Maintenance engineering']","['Training Data', 'Deep Learning', 'Public Datasets', 'Reference Image', 'Similarity Network', 'Color Histogram', 'Restoration Tasks', 'Contralateral', 'Training Set', 'Feature Maps', 'Color Images', 'Receptive Field', 'Grayscale Images', 'Real-world Datasets', 'Deep Features', 'Color Features', 'Synthetic Images', 'Bilinear Interpolation', 'Intermediate Features', 'Similarity Map', 'PASCAL VOC', 'Encoder Block', 'Dense Block', 'Earth Mover’s Distance', 'Deblurring', 'Color Preference', 'Color Processing', 'Fusion Method', 'Multiscale Method', 'Neural Network']","['Applications: Arts/games/social media', 'Low-level and physics-based vision']",9,"Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack of large-scale datasets of old photos makes addressing this restoration task very challenging. Here we present a novel reference-based end-to-end learning framework that is able to both repair and colorize old, degraded pictures. Our proposed framework consists of three modules: a restoration sub-network that conducts restoration from degradations, a similarity network that performs color histogram matching and color transfer, and a colorization subnet that learns to predict the chroma elements of images conditioned on chromatic reference signals. The overall system makes uses of color histogram priors from reference images, which greatly reduces the need for large-scale training data. We have also created a first-of-a-kind public dataset of real old photos that are paired with ground truth ""pristine"" photos that have been manually restored by PhotoShop experts. We conducted extensive experiments on this dataset and synthetic datasets, and found that our method significantly outperforms previous state-of-the-art models using both qualitative comparisons and quantitative measurements. The code is available at https://github.com/DerrickXuNu/Pik-Fix."
Pixel-Wise Prediction Based Visual Odometry via Uncertainty Estimation,"Hao-Wei Chen, Ting-Hsuan Liao, Hsuan-Kung Yang, Chun-Yi Lee","Elsa Lab, Department of Computer Science, National Tsing Hua University, Hsinchu City, Taiwan",100,Taiwan,0,,"This paper introduces pixel-wise prediction based visual odometry(PWVO), which is a dense prediction task that evaluates the values of translation and rotation for every pixel in its input observations. PWVO employs uncertainty estimation to identify the noisy regions in the input observations, and adopts a selection mechanism to integrate pixel-wise predictions based on the estimated uncertainty maps to derive the final translation and rotation. In order to train PWVO in a comprehensive fashion, we further develop a data generation workflow for generating synthetic training data. The experimental results show that PWVO isable to deliver favorable results. In addition, our analyses validate the effectiveness of the designs adopted in PWVO, and demonstrate that the uncertainty mapsestimated by PWVO is capable of capturing the noises in its input observations.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Pixel-Wise_Prediction_Based_Visual_Odometry_via_Uncertainty_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Pixel-Wise_Prediction_Based_Visual_Odometry_via_Uncertainty_Estimation_WACV_2023_paper.pdf,,,2208.08892,main,Poster,https://ieeexplore.ieee.org/document/10030448/,"['Training', 'Visualization', 'Uncertainty', 'Training data', 'Estimation', 'Noise measurement', 'Task analysis']","['Uncertainty Estimation', 'Visual Odometry', 'Pixel-wise Prediction', 'Uncertainty Map', 'Noisy Regions', 'Convolutional Neural Network', 'Set Of Analyses', 'Validation Set', 'Quantitative Results', 'Qualitative Results', 'Effects Of Components', 'Attention Mechanism', 'Raw Images', 'Selective Modulators', 'Inertial Measurement Unit', 'Ground Truth Labels', 'Optical Flow', 'Loss Term', 'Saliency Map', 'Intermediate Representation', 'Camera Motion', 'Flow Map', 'Camera Pose', 'L2 Loss', 'Reprojection Error', 'Transformation Matrix', 'Reconstruction Loss', 'Error Map', 'Depth Map', 'Deep Neural Network']","['Visual odometry', 'uncertainty estimation', 'pixel-wised predictions']",1,"This paper introduces pixel-wise prediction based visual odometry (PWVO), which is a dense prediction task that evaluates the values of translation and rotation for every pixel in its input observations. PWVO employs uncertainty estimation to identify the noisy regions in the input observations, and adopts a selection mechanism to integrate pixel-wise predictions based on the estimated uncertainty maps to derive the final translation and rotation. In order to train PWVO in a comprehensive fashion, we further develop a data generation workflow for generating synthetic training data. The experimental results show that PWVO is able to deliver favorable results. In addition, our analyses validate the effectiveness of the designs adopted in PWVO, and demonstrate that the uncertainty maps estimated by PWVO is capable of capturing the noises in its input observations."
Placing Human Animations Into 3D Scenes by Learning Interaction- and Geometry-Driven Keyframes,"James F. Mullen, Divya Kothandaraman, Aniket Bera, Dinesh Manocha",Purdue University; University of Maryland,100,USA,0,,"We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call ""keyframes."" These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances of the scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5% compared to POSA. Our project website is available at https://gamma.umd.edu/paak/.",https://openaccess.thecvf.com/content/WACV2023/html/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.pdf,https://gamma.umd.edu/paak/,,,main,Poster,https://ieeexplore.ieee.org/document/10030961/,"['Computer vision', 'Three-dimensional displays', 'Affordances', 'Animation', 'Floors']","['3D Scene', 'Human Animation', 'Affordances', 'Human Evaluation', 'Human 3D', '3D Animation', 'Human Model', 'Active Learning', 'Deep Models', 'Real-world Data', 'Human Interaction', 'Semantic Information', 'Generative Adversarial Networks', 'Motion Capture', 'Geometric Information', 'Geometric Method', 'Diversity Score', 'Semantic Labels', 'Contact Probability', 'Mesh Vertices']","['Algorithms: Computational photography', 'image and video synthesis', 'Arts/games/social media', 'Virtual/augmented reality']",4,"We present a novel method for placing a 3D human animation into a 3D scene while maintaining any human-scene interactions in the animation. We use the notion of computing the most important meshes in the animation for the interaction with the scene, which we call ""keyframes."" These keyframes allow us to better optimize the placement of the animation into the scene such that interactions in the animations (standing, laying, sitting, etc.) match the affordances ofthe scene (e.g., standing on the floor or laying in a bed). We compare our method, which we call PAAK, with prior approaches, including POSA, PROX ground truth, and a motion synthesis method, and highlight the benefits of our method with a perceptual study. Human raters preferred our PAAK method over the PROX ground truth data 64.6% of the time. Additionally, in direct comparisons, the raters preferred PAAK over competing methods including 61.5% compared to POSA. Our project website is available at https://gamma.umd.edu/paak/."
Planar Object Tracking via Weighted Optical Flow,"Jonáš Šerých, Jiří Matas","CMP Visual Recognition Group, Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague",100,Czech Republic,0,,"We propose WOFT - a novel method for planar object tracking that estimates a full 8 degrees-of-freedom pose, i.e., the homography w.r.t. a reference view. The method uses a novel module that leverages dense optical flow and assigns a weight to each optical flow correspondence, estimating a homography by weighted least squares in a fully differentiable manner. The trained module assigns zero weights to incorrect correspondences (outliers) in most cases, making the method robust and eliminating the need of the typically used non-differentiable robust estimators like RANSAC. The proposed weighted optical flow tracker (WOFT) achieves state-of-the-art performance on two benchmarks, POT-210 and POIC, tracking consistently well across a wide range of scenarios.",https://openaccess.thecvf.com/content/WACV2023/html/Serych_Planar_Object_Tracking_via_Weighted_Optical_Flow_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Serych_Planar_Object_Tracking_via_Weighted_Optical_Flow_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030801/,"['Computer vision', 'Image motion analysis', 'Codes', 'Annotations', 'Computational modeling', 'Estimation', 'Benchmark testing']","['Optical Flow', 'Object Tracking', 'Object Plane', 'Optical Tracking', 'Deep Learning', 'Least-squares', 'Feature Maps', 'Rigid Body', 'Control Points', 'Singular Value Decomposition', 'Local Flow', 'Global Flows', 'Current Frame', 'Least Squares Problem', 'Illumination Changes', 'Support Set', 'Motion Blur', 'Optical Networks', 'Flow Vector', 'Object Pose', 'Object In Frame', 'Dense Correspondence', 'QR Decomposition', 'Standard Tracking', 'Scale Changes', 'L1 Loss', 'Alignment Errors', 'Input Image', 'Network Flow', 'Learning Rate Of 1e']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Virtual/augmented reality']",2,"We propose WOFT – a novel method for planar object tracking that estimates a full 8 degrees-of-freedom pose, i.e. the homography w.r.t. a reference view. The method uses a novel module that leverages dense optical flow and assigns a weight to each optical flow correspondence, estimating a homography by weighted least squares in a fully differentiable manner. The trained module assigns zero weights to incorrect correspondences (outliers) in most cases, making the method robust and eliminating the need of the typically used non-differentiable robust estimators like RANSAC. The proposed weighted optical flow tracker (WOFT) achieves state-of-the-art performance on two benchmarks, POT-210 [23] and POIC [7], tracking consistently well across a wide range of scenarios."
PointInverter: Point Cloud Reconstruction and Editing via a Generative Model With Shape Priors,"Jaeyeon Kim, Binh-Son Hua, Thanh Nguyen, Sai-Kit Yeung",Hong Kong University of Science and Technology; Deakin University; VinAI Research,66.66666667,"Australia, Hong Kong",33.33333333,Vietnam,"In this paper, we propose a new method for mapping a 3D point cloud to the latent space of a 3D generative adversarial network. Our generative model for 3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point cloud generator. We derive an efficient way to encode an input 3D point cloud to the latent space of the SP-GAN. Our point cloud encoder can resolve the point ordering issue during inversion, and thus can determine the correspondences between points in the generated 3D point cloud and those in the canonical sphere used by the generator. We show that our method outperforms previous GAN inversion methods for 3D point clouds, achieving the state-of-the-art results both quantitatively and qualitatively. Our code is available upon publication.",https://openaccess.thecvf.com/content/WACV2023/html/Kim_PointInverter_Point_Cloud_Reconstruction_and_Editing_via_a_Generative_Model_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_PointInverter_Point_Cloud_Reconstruction_and_Editing_via_a_Generative_Model_WACV_2023_paper.pdf,,https://github.com/hkust-vgd/point_inverter,2211.08702,main,Poster,https://ieeexplore.ieee.org/document/10030251/,"['Point cloud compression', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Codes', 'Shape', 'Generative adversarial networks']","['Point Cloud', 'Shape Priors', 'Point Cloud Reconstruction', 'Generative Adversarial Networks', 'Latent Space', 'Inverse Method', '3D Point Cloud', 'Neural Network', 'Deep Learning', 'Object Detection', 'Object Classification', 'Semantic Segmentation', '3D Coordinates', 'Reconstruction Results', 'Point Cloud Data', 'Global Vector', 'Optimization-based Methods', 'Latent Code', '3D Domain', 'Input Point Cloud', 'Point Cloud Generation', 'Global Code', 'Dense Correspondence', 'Reproductive Quality', 'StyleGAN', 'Use Of Shapes', 'Encoder Architecture', 'Reconstruction Quality', 'Unit Sphere']",['Algorithms: 3D computer vision'],5,"In this paper, we propose a new method for mapping a 3D point cloud to the latent space of a 3D generative adversarial network. Our generative model for 3D point clouds is based on SP-GAN, a state-of-the-art sphere-guided 3D point cloud generator. We derive an efficient way to encode an input 3D point cloud to the latent space of the SP-GAN. Our point cloud encoder can resolve the point ordering issue during inversion, and thus can determine the correspondences between points in the generated 3D point cloud and those in the canonical sphere used by the generator. We show that our method outperforms previous GAN inversion methods for 3D point clouds, achieving state-of-the-art results both quantitatively and qualitatively. Our code is available at https://github.com/hkust-vgd/point_inverter."
PointNeuron: 3D Neuron Reconstruction via Geometry and Topology Learning of Point Clouds,"Runkai Zhao, Heng Wang, Chaoyi Zhang, Weidong Cai",University of Sydney,100,Australia,0,,"Digital neuron reconstruction from 3D microscopy images is an essential technique for investigating brain connectomics and neuron morphology. Existing reconstruction frameworks use convolution-based segmentation networks to partition the neuron from noisy backgrounds before applying the tracing algorithm. The tracing results are sensitive to the raw image quality and segmentation accuracy. In this paper, we propose a novel framework for 3D neuron reconstruction. Our key idea is to use the geometric representation power of the point cloud to better explore the intrinsic structural information of neurons. Our proposed framework adopts one graph convolutional network to predict the neural skeleton points and another one to produce the connectivity of these points. We finally generate the target SWC file through the interpretation of the predicted point coordinates, radius, and connections. Evaluated on the Janelia-Fly dataset from the BigNeuron project, we show that our framework achieves competitive neuron reconstruction performance. Our geometry and topology learning of point clouds could further benefit 3D medical image analysis, such as cardiac surface reconstruction. Our code is available at https://github.com/RunkaiZhao/PointNeuron.",https://openaccess.thecvf.com/content/WACV2023/html/Zhao_PointNeuron_3D_Neuron_Reconstruction_via_Geometry_and_Topology_Learning_of_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhao_PointNeuron_3D_Neuron_Reconstruction_via_Geometry_and_Topology_Learning_of_WACV_2023_paper.pdf,,https://github.com/RunkaiZhao/PointNeuron,2210.08305,main,Poster,https://ieeexplore.ieee.org/document/10030223/,"['Point cloud compression', 'Geometry', 'Image segmentation', 'Surface reconstruction', 'Three-dimensional displays', 'Neurons', 'Skeleton']","['Point Cloud', 'Neuron Reconstruction', 'Geometry And Topology', 'Geometry Learning', '3D Neuron', 'Point Cloud Geometry', '3D Neuron Reconstruction', 'Microscopy Images', 'Medical Imaging', 'Image Segmentation', 'Raw Images', 'Neuronal Morphology', 'Graph Convolutional Network', 'Medical Image Analysis', 'Graph Convolution', 'Reconstruction Performance', 'Neuronal Information', 'Digital Reconstruction', 'Tracing Algorithm', 'Volumetric Imaging', 'Geometric Features', 'Prediction Module', 'Atrous Spatial Pyramid Pooling', 'Imaging Of Neurons', 'Input Point', 'Point Cloud Data', 'Neuronal Structures', 'Retrograde Tracing', 'Manual Annotation']","['Applications: Biomedical/healthcare/medicine', '3D computer vision']",5,"Digital neuron reconstruction from 3D microscopy images is an essential technique for investigating brain connectomics and neuron morphology. Existing reconstruction frameworks use convolution-based segmentation networks to partition the neuron from noisy backgrounds before applying the tracing algorithm. The tracing results are sensitive to the raw image quality and segmentation accuracy. In this paper, we propose a novel framework for 3D neuron reconstruction. Our key idea is to use the geometric representation power of the point cloud to better explore the intrinsic structural information of neurons. Our proposed framework adopts one graph convolutional network to predict the neural skeleton points and another one to produce the connectivity of these points. We finally generate the target SWC file through the interpretation of the predicted point coordinates, radius, and connections. Evaluated on the Janelia-Fly dataset from the BigNeuron project, we show that our framework achieves competitive neuron reconstruction performance. Our geometry and topology learning of point clouds could further benefit 3D medical image analysis, such as cardiac surface reconstruction. Our code is available at https://github.com/RunkaiZhao/PointNeuron."
PreViTS: Contrastive Pretraining With Video Tracking Supervision,"Brian Chen, Ramprasaath R. Selvaraju, Shih-Fu Chang, Juan Carlos Niebles, Nikhil Naik",Salesforce Research; Artera AI; Columbia University,33.33333333,USA,66.66666667,USA,"Videos are a rich source for self-supervised learning (SSL) of visual representations due to the presence of natural temporal transformations of objects. However, current methods typically randomly sample video clips for learning, which results in an imperfect supervisory signal. In this work, we propose PreViTS, an SSL framework that utilizes an unsupervised tracking signal for selecting clips containing the same object, which helps better utilize temporal transformations of objects. PreViTS further uses the tracking signal to spatially constrain the frame regions to learn from and trains the model to locate meaningful objects by providing supervision on Grad-CAM attention maps. To evaluate our approach, we train a momentum contrastive (MoCo) encoder on VGG-Sound and Kinetics-400 datasets with PreViTS. Training with PreViTS outperforms representations learnt by contrastive strategy alone on video downstream tasks, obtaining state-of-the-art performance on action classification. PreViTS helps learn feature representations that are more robust to changes in background and context, as seen by experiments on datasets with background changes. Learning from large-scale videos with PreViTS could lead to more accurate and robust visual feature representations.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_PreViTS_Contrastive_Pretraining_With_Video_Tracking_Supervision_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_PreViTS_Contrastive_Pretraining_With_Video_Tracking_Supervision_WACV_2023_paper.pdf,,,2112.00804,main,Poster,https://ieeexplore.ieee.org/document/10030959/,"['Training', 'Representation learning', 'Visualization', 'Video tracking', 'Video on demand', 'Computational modeling', 'Lighting']","['Video Tracking', 'Contrastive Pretraining', 'Visual Representation', 'Accurate Representation', 'Representation Learning', 'Action Classes', 'Self-supervised Learning', 'Signal Tracks', 'Data Augmentation', 'Bounding Box', 'Single Object', 'Action Recognition', 'Object Tracking', 'Temporal Distance', 'Saliency Map', 'Temporal Constraints', 'Image Augmentation', 'Video Segments', 'Object Regions', 'Video Object', 'Pre-training Dataset', 'Foreground Objects', 'Video Retrieval', 'Salient Regions', 'Viewpoint Changes', 'YouTube']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",1,"Videos are a rich source for self-supervised learning (SSL) of visual representations due to the presence of natural temporal transformations of objects. However, current methods typically randomly sample video clips for learning, which results in an imperfect supervisory signal. In this work, we propose PreViTS, an SSL framework that utilizes an unsupervised tracking signal for selecting clips containing the same object, which helps better utilize temporal transformations of objects. PreViTS further uses the tracking signal to spatially constrain the frame regions to learn from and trains the model to locate meaningful objects by providing supervision on Grad-CAM attention maps. To evaluate our approach, we train a momentum contrastive (MoCo) encoder on VGG-Sound and Kinetics-400 datasets with PreViTS. Training with PreViTS outperforms representations learnt by contrastive strategy alone on video downstream tasks, obtaining state-of-the-art performance on action classification. PreViTS helps learn feature representations that are more robust to changes in background and context, as seen by experiments on datasets with background changes. Our experiment also demonstrates various visual transformation invariance captured by our model. Learning from large-scale videos with PreViTS could lead to more accurate and robust visual feature representations."
Proactive Deepfake Defence via Identity Watermarking,"Yuan Zhao, Bo Liu, Ming Ding, Baoping Liu, Tianqing Zhu, Xin Yu","Data61, CSIRO; University of Technology Sydney",100,Australia,0,,"The explosive progress of Deepfake techniques poses unprecedented privacy and security risks toward our society by creating real-looking but fake visual content. However, the current Deepfake detection studies are still in their infancy, because they mainly rely on capturing artifacts left by a Deepfake synthesis process as detection clues. These artifacts could be easily obscured due to various distortions (e.g. blurring) and could also be removed with the development of advanced Deepfake techniques, rendering the artifacts-based detection methods less effective in achieving reliable forgery forensics. In this paper, we propose a novel Deepfake detection method that does not depend on identifying the synthesized artifacts, but resorts to a mechanism of anti-counterfeit labels. Specifically, we design a neural network with an encoder-decoder structure to embed messages as anti-Deepfake labels into the facial identity features. Since the injected label is entangled with the facial identity feature, it will be sensitive to face swap translations (i.e., Deepfake), but robust to conventional image modifications (e.g., resize and compress). Therefore, we can check whether the watermarked image has been tampered with by Deepfake methods according to the existence of the label. Experimental results demonstrate that our method can achieve an average detection accuracy of more than 80%, which validates the effectiveness of the proposed method to implement Deepfake detection.",https://openaccess.thecvf.com/content/WACV2023/html/Zhao_Proactive_Deepfake_Defence_via_Identity_Watermarking_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhao_Proactive_Deepfake_Defence_via_Identity_Watermarking_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030248/,"['Deepfakes', 'Visualization', 'Privacy', 'Image coding', 'Neural networks', 'Watermarking', 'Robustness']","['Identification Of Features', 'Face Images', 'Digital Watermarking', 'Detection Methods', 'Receiver Operating Characteristic Curve', 'Image Quality', 'Quantitative Results', 'Input Image', 'Sequence Typing', 'F1 Score', 'Image Reconstruction', 'Skin Color', 'Visual Quality', 'Peak Signal-to-noise Ratio', 'Bitstream', 'Representation Of Identity', 'Representation Of Properties', 'Pseudo-random Sequence', 'Structural Similarity Index Measure', 'Identity Vector', 'Fake Images', 'Different Types Of Sequences', 'Superior Performance', 'Horizontal Flip', 'Feature Maps', 'Gaussian Blur', 'Attention Mechanism', 'Impact Of Sequence']","['Applications: Social good', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Arts/games/social media']",12,"The explosive progress of Deepfake techniques poses unprecedented privacy and security risks to our society by creating real-looking but fake visual content. The current Deepfake detection studies are still in their infancy because they mainly rely on capturing artifacts left by a Deepfake synthesis process as detection clues, which can be easily removed by various distortions (e.g. blurring) or advanced Deepfake techniques. In this paper, we propose a novel method that does not depend on identifying the artifacts but resorts to the mechanism of anti-counterfeit labels to protect face images from malicious Deepfake tampering. Specifically, we design a neural network with an encoder-decoder structure to embed watermarks as anti-Deepfake labels into the facial identity features. The injected label is entangled with the facial identity feature, so it will be sensitive to face swap translations (i.e., Deepfake) and robust to conventional image modifications (e.g., resize and compress). Therefore, we can identify whether watermarked images have been tampered with by Deepfake methods according to the label’s existence. Experimental results demonstrate that our method can achieve average detection accuracy of more than 80%, which validates the proposed method’s effectiveness in implementing Deepfake detection."
Probabilistic Integration of Object Level Annotations in Chest X-Ray Classification,"Tom van Sonsbeek, Xiantong Zhen, Dwarikanath Mahapatra, Marcel Worring","University of Amsterdam, Amsterdam, the Netherlands; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE",100,"Netherlands, UAE",0,,"Medical image datasets and their annotations are not growing as fast as their equivalents in the general domain. This makes translation from the newest, more data-intensive methods that have made a large impact on the vision field increasingly more difficult and less efficient. In this paper, we propose a new probabilistic latent variable model for disease classification in chest X-ray images. Specifically we consider chest X-ray datasets that contain global disease labels, and for a smaller subset contain object level expert annotations in the form of eye gaze patterns and disease bounding boxes. We propose a two-stage optimization algorithm which is able to handle these different label granularities through a single training pipeline in a two-stage manner. In our pipeline global dataset features are learned in the lower level layers of the model. The specific details and nuances in the fine-grained expert object-level annotations are learned in the final layers of the model using a knowledge distillation method inspired by conditional variational inference. Subsequently, model weights are frozen to guide this learning process and prevent overfitting on the smaller richly annotated data subsets. The proposed method yields consistent classification improvement across different backbones on the common benchmark datasets Chest X-ray14 and MIMIC-CXR. This shows how two-stage learning of labels from coarse to fine-grained, in particular with object level annotations, is an effective method for more optimal annotation usage.",https://openaccess.thecvf.com/content/WACV2023/html/van_Sonsbeek_Probabilistic_Integration_of_Object_Level_Annotations_in_Chest_X-Ray_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/van_Sonsbeek_Probabilistic_Integration_of_Object_Level_Annotations_in_Chest_X-Ray_Classification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030981/,"['Training', 'Annotations', 'Pipelines', 'MIMICs', 'Probabilistic logic', 'Data models', 'X-ray imaging']","['Chest X-ray', 'Chest X-ray Classification', 'Object-level Annotations', 'Classification Of Diseases', 'Latent Variables', 'Probabilistic Model', 'Bounding Box', 'Model Weights', 'Eye Contact', 'Layer Model', 'Consistent Improvement', 'Domain Generalization', 'Variational Inference', 'Chest X-ray Images', 'Expert Annotations', 'Annotation Format', 'Conditional Inference', 'X-ray Dataset', 'Medical Image Datasets', 'Large Datasets', 'CNN Backbone', 'Variational Autoencoder', 'Chest X-ray Scan', 'Image X', 'Small Datasets', 'Stage 2', 'Image Representation', 'Bounding Box Annotations', 'Small Amount Of Data', 'Multilayer Perceptron']",['Applications: Biomedical/healthcare/medicine'],8,"Medical image datasets and their annotations are not growing as fast as their equivalents in the general domain. This makes translation from the newest, more data-intensive methods that have made a large impact on the vision field increasingly more difficult and less efficient. In this paper, we propose a new probabilistic latent variable model for disease classification in chest X-ray images. Specifically we consider chest X-ray datasets that contain global disease labels, and for a smaller subset contain object level expert annotations in the form of eye gaze patterns and disease bounding boxes. We propose a two-stage optimization algorithm which is able to handle these different label granularities through a single training pipeline in a two-stage manner. In our pipeline global dataset features are learned in the lower level layers of the model. The specific details and nuances in the fine-grained expert object-level annotations are learned in the final layers of the model using a knowledge distillation method inspired by conditional variational inference. Subsequently, model weights are frozen to guide this learning process and prevent overfitting on the smaller richly annotated data subsets. The proposed method yields consistent classification improvement across different back-bones on the common benchmark datasets Chest X-ray14 and MIMIC-CXR. This shows how two-stage learning of labels from coarse to fine-grained, in particular with object level annotations, is an effective method for more optimal annotation usage."
Probabilistic Volumetric Fusion for Dense Monocular SLAM,"Antoni Rosinol, John J. Leonard, Luca Carlone",Massachusetts Institute of Technology,100,USA,0,,"We present a novel method to reconstruct 3D scenes from images by leveraging deep dense monocular SLAM and fast uncertainty propagation. The proposed approach is able to 3D reconstruct scenes densely, accurately, and in real-time while being robust to extremely noisy depth estimates coming from dense monocular SLAM. Differently from previous approaches, that either use ad-hoc depth filters, or that estimate the depth uncertainty from RGB-D cameras' sensor models, our probabilistic depth uncertainty derives directly from the information matrix of the underlying bundle adjustment problem in SLAM. We show that the resulting depth uncertainty provides an excellent signal to weight the depth-maps for volumetric fusion. Without our depth uncertainty, the resulting mesh is noisy and with artifacts, while our approach generates an accurate 3D mesh with significantly fewer artifacts. We provide results on the challenging Euroc dataset, and show that our approach achieves 92% better accuracy than directly fusing depths from monocular SLAM, and up to 90% improvements compared to the best competing approach.",https://openaccess.thecvf.com/content/WACV2023/html/Rosinol_Probabilistic_Volumetric_Fusion_for_Dense_Monocular_SLAM_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rosinol_Probabilistic_Volumetric_Fusion_for_Dense_Monocular_SLAM_WACV_2023_paper.pdf,,,2210.01276,main,Poster,https://ieeexplore.ieee.org/document/10030932/,"['Geometry', 'Visualization', 'Uncertainty', 'Three-dimensional displays', 'Simultaneous localization and mapping', 'Semantics', 'Probabilistic logic']","['Monocular SLAM', 'Volumetric Fusion', 'Dense SLAM', '3D Reconstruction', 'Error Propagation', '3D Mesh', 'Depth Camera', 'Fisher Information', 'Depth Estimation', 'Sensor Model', 'Accurate 3D', '3D Scene', 'Bundle Adjustment', 'Depth Filtration', 'Probabilistic Uncertainty', 'Uncertainty Estimation', 'Point Cloud', 'Weight Function', 'Reconstruction Algorithm', 'Depth Map', 'Volumetric Reconstruction', 'Pose Estimation', 'Optical Flow', 'Probabilistic Formulation', 'Monocular Images', 'Stereo Camera', 'Camera Pose', 'Depth Values', 'Reconstruction Accuracy', 'Depth Measurements']","['Algorithms: 3D computer vision', 'Robotics']",11,"We present a novel method to reconstruct 3D scenes from images by leveraging deep dense monocular SLAM and fast uncertainty propagation. The proposed approach is able to 3D reconstruct scenes densely, accurately, and in realtime while being robust to extremely noisy depth estimates coming from dense monocular SLAM. Differently from previous approaches, that either use ad-hoc depth filters, or that estimate the depth uncertainty from RGB-D cameras’ sensor models, our probabilistic depth uncertainty derives directly from the information matrix of the underlying bundle adjustment problem in SLAM. We show that the resulting depth uncertainty provides an excellent signal to weight the depth-maps for volumetric fusion. Without our depth uncertainty, the resulting mesh is noisy and with artifacts, while our approach generates an accurate 3D mesh with significantly fewer artifacts. We provide results on the challenging Euroc dataset, and show that our approach achieves 92% better accuracy than directly fusing depths from monocular SLAM, and up to 90% improvements compared to the best competing approach."
Progressive Video Summarization via Multimodal Self-Supervised Learning,"Haopeng Li, Qiuhong Ke, Mingming Gong, Tom Drummond","School of Computing and Information Systems, The University of Melbourne; Department of Data Science & AI, Monash University; School of Mathematics and Statistics, The University of Melbourne",100,Australia,0,,"Modern video summarization methods are based on deep neural networks that require a large amount of annotated data for training. However, existing datasets for video summarization are small-scale, easily leading to over-fitting of the deep models. Considering that the annotation of large-scale datasets is time-consuming, we propose a multimodal self-supervised learning framework to obtain semantic representations of videos, which benefits the video summarization task. Specifically, the self-supervised learning is conducted by exploring the semantic consistency between the videos and text in both course-grained and fine-grained fashions, as well as recovering masked frames in the videos. The multimodal framework is trained on a newly-collected dataset that consists of video-text pairs. Additionally, we introduce a progressive video summarization method, where the important content in a video is pinpointed progressively to generate better summaries. Extensive experiments have proved the effectiveness and superiority of our method in rank correlation coefficients and F-score compared to the state of the art.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Progressive_Video_Summarization_via_Multimodal_Self-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Progressive_Video_Summarization_via_Multimodal_Self-Supervised_Learning_WACV_2023_paper.pdf,,,2201.02494,main,Poster,https://ieeexplore.ieee.org/document/10031001/,"['Training', 'Deep learning', 'Correlation coefficient', 'Computer vision', 'Annotations', 'Semantics', 'Neural networks']","['Self-supervised Learning', 'Multimodal Learning', 'Video Summarization', 'Rank Correlation', 'Deep Neural Network', 'Model Overfitting', 'Semantic Consistency', 'Long Short-term Memory', 'Global Information', 'Textual Information', 'Search Queries', 'Word Embedding', 'Temporal Dependencies', 'Individual Words', 'Coarse-grained Model', 'Video Information', 'Linear Projection', 'Text Types', 'Dictionary Learning', 'Pre-trained Encoder', 'Semantic Correlation', 'Video Encoding', 'Text Encoder', 'Frame Features', 'Limited GPU Memory', 'Google Trends', 'YouTube', 'Fine-grained Model', 'Text Sequence']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",17,"Modern video summarization methods are based on deep neural networks that require a large amount of annotated data for training. However, existing datasets for video summarization are small-scale, easily leading to over-fitting of the deep models. Considering that the annotation of large-scale datasets is time-consuming, we propose a multimodal self-supervised learning framework to obtain semantic representations of videos, which benefits the video summarization task. Specifically, the self-supervised learning is conducted by exploring the semantic consistency between the videos and text in both coarse-grained and fine-grained fashions, as well as recovering masked frames in the videos. The multimodal framework is trained on a newly-collected dataset that consists of video-text pairs. Additionally, we introduce a progressive video summarization method, where the important content in a video is pinpointed progressively to generate better summaries. Extensive experiments have proved the effectiveness and superiority of our method in rank correlation coefficients and F-score
<sup>1</sup>
."
ProtoSeg: Interpretable Semantic Segmentation With Prototypical Parts,"Mikołaj Sacha, Dawid Rymarczyk, Łukasz Struski, Jacek Tabor, Bartosz Zieliński","Faculty of Mathematics and Computer Science, Jagiellonian University, Krakow, Poland",100,Poland,0,,"We introduce ProtoSeg, a novel model for interpretable semantic image segmentation, which constructs its predictions using similar patches from the training set. To achieve accuracy comparable to baseline methods, we adapt the mechanism of prototypical parts and introduce a diversity loss function that increases the variety of prototypes within each class. We show that ProtoSeg discovers semantic concepts, in contrast to standard segmentation models. Experiments conducted on Pascal VOC and Cityscapes datasets confirm the precision and transparency of the presented method.",https://openaccess.thecvf.com/content/WACV2023/html/Sacha_ProtoSeg_Interpretable_Semantic_Segmentation_With_Prototypical_Parts_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sacha_ProtoSeg_Interpretable_Semantic_Segmentation_With_Prototypical_Parts_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030923/,"['Training', 'Adaptation models', 'Computer vision', 'Semantic segmentation', 'Semantics', 'Prototypes', 'Computer architecture']","['Semantic Segmentation', 'Prototype Parts', 'Training Set', 'Standard Model', 'Image Segmentation', 'Loss Of Diversity', 'Segmentation Model', 'Semantic Knowledge', 'Semantic Segmentation Models', 'Medical Imaging', 'Learning Rate', 'Classification Task', 'Feature Maps', 'Segmentation Task', 'Loss Term', 'Conditional Random Field', 'Distance Vector', 'Joint Training', 'Explainable Artificial Intelligence', 'Architecture For Segmentation', 'Atrous Spatial Pyramid Pooling', 'Class Prototypes', 'Feature Maps Of Images', 'ImageNet Pretraining', 'Warm-up Phase', 'Constant Learning Rate']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",11,"We introduce ProtoSeg, a novel model for interpretable semantic image segmentation, which constructs its predictions using similar patches from the training set. To achieve accuracy comparable to baseline methods, we adapt the mechanism of prototypical parts and introduce a diversity loss function that increases the variety of prototypes within each class. We show that ProtoSeg discovers semantic concepts, in contrast to standard segmentation models. Experiments conducted on Pascal VOC and Cityscapes datasets confirm the precision and transparency of the presented method."
Pruning-Guided Curriculum Learning for Semi-Supervised Semantic Segmentation,"Heejo Kong, Gun-Hee Lee, Suneung Kim, Seong-Whan Lee","Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Artificial Intelligence, Korea University, Seoul, South Korea; Department of Computer Science and Engineering, Korea University, Seoul, South Korea",100,South Korea,0,,"This study focuses on improving the quality of pseudo-labeling in the context of semi-supervised semantic segmentation. Previous studies have adopted confidence thresholding to reduce erroneous predictions in pseudo-labeled data and to enhance their qualities. However, numerous pseudo-labels with high confidence scores exist in the early training stages even though their predictions are incorrect, and this ambiguity limits confidence thresholding substantially. In this paper, we present a novel method to resolve the ambiguity of confidence scores with the guidance of network pruning. A recent finding showed that network pruning severely impairs the network generalization ability on samples that are not yet well learned or represented. Inspired by this finding, we refine the confidence scores by reflecting the extent to which the predictions are affected by pruning. Furthermore, we adopted a curriculum learning strategy for the confidence score, which enables the network to learn gradually from easy to hard samples. This approach resolves the ambiguity by suppressing the learning of noisy pseudo-labels, the confidence scores of which are difficult to trust owing to insufficient training in the early stages. Extensive experiments on various benchmarks demonstrate the superiority of our framework over state-of-the-art alternatives.",https://openaccess.thecvf.com/content/WACV2023/html/Kong_Pruning-Guided_Curriculum_Learning_for_Semi-Supervised_Semantic_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kong_Pruning-Guided_Curriculum_Learning_for_Semi-Supervised_Semantic_Segmentation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030133/,"['Training', 'Knowledge engineering', 'Computer vision', 'Semantic segmentation', 'Benchmark testing', 'Feature extraction', 'Noise measurement']","['Semantic Segmentation', 'Curriculum Learning', 'Semi-supervised Semantic Segmentation', 'Ambiguity', 'Generalization Ability', 'Extensive Experiments', 'Confidence Score', 'Confidence Threshold', 'Insufficient Training', 'Early Stage Of Training', 'High Confidence Score', 'Erroneous Predictions', 'Network Pruning', 'Deep Neural Network', 'Validation Set', 'Probability Density Function', 'Qualitative Results', 'Large-scale Datasets', 'Stochastic Gradient Descent', 'Student Network', 'Unlabeled Data', 'Teacher Network', 'ResNet-50 Backbone', 'Consistency Regularization', 'Self-supervised Learning', 'Softmax Probability', 'Semi-supervised Learning', 'Confirmation Bias', 'Curriculum Policy']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"This study focuses on improving the quality of pseudolabeling in the context of semi-supervised semantic segmentation. Previous studies have adopted confidence thresholding to reduce erroneous predictions in pseudo-labeled data and to enhance their qualities. However, numerous pseudolabels with high confidence scores exist in the early training stages even though their predictions are incorrect, and this ambiguity limits confidence thresholding substantially. In this paper, we present a novel method to resolve the ambiguity of confidence scores with the guidance of network pruning. A recent finding showed that network pruning severely impairs the network generalization ability on samples that are not yet well learned or represented. Inspired by this finding, we refine the confidence scores by reflecting the extent to which the predictions are affected by pruning. Furthermore, we adopted a curriculum learning strategy for the confidence score, which enables the network to learn gradually from easy to hard samples. This approach resolves the ambiguity by suppressing the learning of noisy pseudolabels, the confidence scores of which are difficult to trust owing to insufficient training in the early stages. Extensive experiments on various benchmarks demonstrate the superiority of our framework over state-of-the-art alternatives."
Pushing the Efficiency Limit Using Structured Sparse Convolutions,"Vinay Kumar Verma, Nikhil Mehta, Shijing Si, Ricardo Henao, Lawrence Carin","SEF, Shanghai International Studies University; Duke University and KAUST Saudi Arabia; Duke University",100,"China, Saudi Arabia, USA",0,,"Weight pruning is among the most popular approaches for compressing deep convolutional neural networks. Recent work suggests that in a randomly initialized deep neural network, there exist sparse subnetworks that achieve performance comparable to the original network. Unfortunately, finding these subnetworks involves iterative stages of training and pruning, which can be computationally expensive. We propose Structured Sparse Convolution (SSC), that leverages the inherent structure in images to reduce the parameters in the convolutional filter. This leads to improved efficiency of convolutional architectures compared to existing methods that perform pruning at initialization. We show that SSC is a generalization of commonly used layers (depthwise, groupwise, and pointwise convolution) in ""efficient architectures."" Extensive experiments on well-known CNN models and datasets show the effectiveness of the proposed method. Architectures based on SSC achieve state-of-the-art performance compared to baselines on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks.",https://openaccess.thecvf.com/content/WACV2023/html/Verma_Pushing_the_Efficiency_Limit_Using_Structured_Sparse_Convolutions_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Verma_Pushing_the_Efficiency_Limit_Using_Structured_Sparse_Convolutions_WACV_2023_paper.pdf,,https://github.com/vkvermaa/SSC,2210.12818,main,Poster,https://ieeexplore.ieee.org/document/10030101/,"['Deep learning', 'Training', 'Convolutional codes', 'Tensors', 'Convolution', 'Computational modeling', 'Neural networks']","['Neural Network', 'Convolutional Network', 'Deep Neural Network', 'ImageNet', 'Deep Convolutional Neural Network', 'Deep Convolutional Network', 'Original Network', 'Classification Benchmarks', 'Pointwise Convolution', 'Training Data', 'Computational Cost', 'Hyperparameters', 'Standard Model', 'Convolutional Layers', 'Training Time', 'Sparsity', 'Feature Maps', 'Dense Network', 'Deep Architecture', 'Standard Filter', 'Filtration Efficiency', 'Depthwise Convolution', 'Standard Architecture', 'Kernel Type', 'Standard Convolution', 'Network Pruning', 'Extensive Ablation', 'Pruning Method', 'Memory Cost', 'Transfer Learning']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",2,"Weight pruning is among the most popular approaches for compressing deep convolutional neural networks. Recent work suggests that in a randomly initialized deep neural network, there exist sparse subnetworks that achieve performance comparable to the original network. Unfortunately, finding these subnetworks involves iterative stages of training and pruning, which can be computationally expensive. We propose Structured Sparse Convolution (SSC), that leverages the inherent structure in images to reduce the parameters in the convolutional filter. This leads to improved efficiency of convolutional architectures compared to existing methods that perform pruning at initialization. We show that SSC is a generalization of commonly used layers (depthwise, groupwise and pointwise convolution) in ""efficient architectures."" Extensive experiments on well-known CNN models and datasets show the effectiveness of the proposed method. Architectures based on SSC achieve state-of-the-art performance compared to baselines on CIFAR10, CIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks. Our source code is publicly available at https://github.com/vkvermaa/SSC."
QMagFace: Simple and Accurate Quality-Aware Face Recognition,"Philipp Terhörst, Malte Ihlefeld, Marco Huber, Naser Damer, Florian Kirchbuchner, Kiran Raja, Arjan Kuijper","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Paderborn University, Paderborn, Germany; Technical University of Darmstadt, Darmstadt, Germany; Norwegian University of Science and Technology, Gjøvik, Norway",100,"Germany, Norway",0,,"In this work, we propose QMagFace, a simple and effective face recognition solution (QMagFace) that combines a quality-aware comparison score with a recognition model based on a magnitude-aware angular margin loss.The proposed approach includes model-specific face image qualities in the comparison process to enhance the recognition performance under unconstrained circumstances. Exploiting the linearity between the qualities and their comparison scores induced by the utilized loss, our quality-aware comparison function is simple and highly generalizable. The experiments conducted on several face recognition databases and benchmarks demonstrate that the introduced quality-awareness leads to consistent improvements in the recognition performance. Moreover, the proposed QMagFace approach performs especially well under challenging circumstances, such as cross-pose, cross-age, or cross-quality. Consequently, it leads to state-of-the-art performances on several face recognition benchmarks, such as 98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace is publicly available.",https://openaccess.thecvf.com/content/WACV2023/html/Terhorst_QMagFace_Simple_and_Accurate_Quality-Aware_Face_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Terhorst_QMagFace_Simple_and_Accurate_Quality-Aware_Face_Recognition_WACV_2023_paper.pdf,,https://github.com/pterhoer/QMagFace,,main,Poster,https://ieeexplore.ieee.org/document/10030931/,"['Training', 'Image quality', 'Computer vision', 'Image recognition', 'Databases', 'Face recognition', 'Computational modeling']","['Face Recognition', 'Benchmark', 'Image Quality', 'Face Images', 'Recognition Performance', 'Comparison Process', 'Linear Function', 'Sample Quality', 'Recognition Task', 'Latent Space', 'Quality Estimation', 'Optimal Weight', 'Area Under Curve', 'Estimates Of Use', 'Decision Threshold', 'High Generalization', 'Variety Of Potential Applications', 'Face Recognition Task', 'Low-quality Samples', 'Class Center', 'Improve Recognition Performance', 'Face Recognition Model', 'Face Recognition Performance', 'Simple Linear Function', 'Unconstrained Environment', 'Equal Error Rate', 'Challenging Situations', 'Low Quality']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",30,"In this work, we propose QMagFace, a simple and effective face recognition solution (QMagFace) that combines a quality-aware comparison score with a recognition model based on a magnitude-aware angular margin loss. The proposed approach includes model-specific face image qualities in the comparison process to enhance the recognition performance under unconstrained circumstances. Exploiting the linearity between the qualities and their comparison scores induced by the utilized loss, our quality-aware comparison function is simple and highly generalizable. The experiments conducted on several face recognition databases and benchmarks demonstrate that the introduced quality-awareness leads to consistent improvements in the recognition performance. Moreover, the proposed QMagFace approach performs especially well under challenging circumstances, such as cross-pose, cross-age, or cross-quality. Consequently, it leads to state-of-the-art performances on several face recognition benchmarks, such as 98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace is publicly available
<sup>1</sup>
."
RADIANT: Better rPPG Estimation Using Signal Embeddings and Transformer,"Anup Kumar Gupta, Rupesh Kumar, Lokendra Birla, Puneet Gupta",Indian Institute of Technology Indore,100,India,0,,"Remote photoplethysmography can provide non-contact heart rate (HR) estimation by analyzing the skin color variations obtained from face videos. These variations are subtle, imperceptible to human eyes, and easily affected by noise. Existing deep learning-based rPPG estimators are incompetent due to three reasons. Firstly, they suppress the noise by utilizing information from the whole face even though different facial regions contain different noise characteristics. Secondly, local noise characteristics inherently affect the convolutional neural network (CNN) architectures. Lastly, the CNN sequential architectures fail to preserve long temporal dependencies. To address these issues, we propose RADIANT, that is, rPPG estimation using Signal Embeddings and Transformer. Our Transformer utilizes a multi-head attention mechanism that facilitates the feature subspace learning to extract the multiple correlations among the color variations corresponding to the periodic pulse. Also, its global information processing ability helps to suppress local noise characteristics. Apart from Transformer, we propose novel signal embedding to enhance the rPPG feature representation and suppress noise. We have also improved the generalization of our architecture by adding a new training set. To this end, the effectiveness of synthetic temporal signals and data augmentations were explored. Experiments on extensively utilized UBFC-rPPG and COHFACE datasets demonstrate that our architecture outperforms previous well-known architectures. The implementation will be made publicly available upon paper acceptance.",https://openaccess.thecvf.com/content/WACV2023/html/Gupta_RADIANT_Better_rPPG_Estimation_Using_Signal_Embeddings_and_Transformer_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_RADIANT_Better_rPPG_Estimation_Using_Signal_Embeddings_and_Transformer_WACV_2023_paper.pdf,,https://github.com/Deep-Intelligence-Lab/RADIANT.git,,main,Poster,https://ieeexplore.ieee.org/document/10030331/,"['Heart rate', 'Image color analysis', 'Estimation', 'Computer architecture', 'Transformers', 'Feature extraction', 'Convolutional neural networks']","['Training Set', 'Convolutional Neural Network', 'Feature Representation', 'Data Augmentation', 'Color Variation', 'Convolutional Neural Network Architecture', 'Temporal Dependencies', 'Temporal Signal', 'Local Noise', 'Synthetic Signals', 'Multi-head Attention Mechanism', 'Heart Rate Estimation', 'Mean Square Error', 'Random Noise', 'Long Short-term Memory', 'Multilayer Perceptron', 'Video Clips', 'Video Frames', 'Blue Channel', 'Pulse Signal', 'Transformer Architecture', 'Mean Square Error Loss', 'Self-attention Mechanism', 'Heart Rate Range', 'Beats Per Minute', 'Facial Movements', 'Transformer Layers', 'Blind Source Separation', 'Error Loss Function', 'Limited Training Data']",['Applications: Biomedical/healthcare/medicine'],25,"Remote photoplethysmography can provide non-contact heart rate (HR) estimation by analyzing the skin color variations obtained from face videos. These variations are subtle, imperceptible to human eyes, and easily affected by noise. Existing deep learning-based rPPG estimators are incompetent due to three reasons. Firstly, they suppress the noise by utilizing information from the whole face even though different facial regions contain different noise characteristics. Secondly, local noise characteristics inherently affect the convolutional neural network (CNN) architectures. Lastly, the CNN sequential architectures fail to preserve long temporal dependencies. To address these issues, we propose RADIANT, that is, rPPG estimation using Signal Embeddings and Transformer. Our architecture utilizes a multi-head attention mechanism that facilitates feature subspace learning to extract the multiple correlations among the color variations corresponding to the periodic pulse. Also, its global information processing ability helps to suppress local noise characteristics. Furthermore, we propose novel signal embedding to enhance the rPPG feature representation and suppress noise. We have also improved the generalization of our architecture by adding a new training set. To this end, the effectiveness of synthetic temporal signals and data augmentations were explored. Experiments on extensively utilized rPPG datasets demonstrate that our architecture outperforms previous well-known architectures. Code: https://github.com/Deep-Intelligence-Lab/RADIANT.git"
RANCER: Non-Axis Aligned Anisotropic Certification With Randomized Smoothing,"Taras Rumezhak, Francisco Girbal Eiras, Philip H.S. Torr, Adel Bibi","Ukrainian Catholic University, SoftServe; University of Oxford",100,"UK, Ukraine",0,,"As modern networks have been proven to be unprotected from adversarial attacks and are applied in safety-critical applications, defense against them is very crucial. Many works were dedicated to this topic, but randomized smoothing has been recently proven to be an effective approach for the certified defense of deep neural networks and getting robust classifiers. Some prior results were obtained utilizing the techniques of adding extra parameters to extend the limits of the certification regions. In this way, sample-wise optimization was proposed to maximize the certification radius per input. The idea was further extended with the generalized anisotropic counterparts of l1 and l2 certificates which allow achieving larger certified region volume avoiding worst-case certification near potentially larger safe regions. However, anisotropic certification is limited by the aligned axis lacking the freedom to extend in any direction. To mitigate this constraint, in this work, we (i) revisit the anisotropic certification, provide an analysis of its non-axis aligned counterpart and propose its rotation-free extension, (ii) conduct experiments on the CIFAR-10 dataset to report the improved performance.",https://openaccess.thecvf.com/content/WACV2023/html/Rumezhak_RANCER_Non-Axis_Aligned_Anisotropic_Certification_With_Randomized_Smoothing_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rumezhak_RANCER_Non-Axis_Aligned_Anisotropic_Certification_With_Randomized_Smoothing_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031016/,"['Deep learning', 'Computer vision', 'Smoothing methods', 'Neural networks', 'Certification', 'Optimization']","['Deep Neural Network', 'Axis Parallel', 'Robust Classification', 'Adversarial Attacks', 'Safety-critical Applications', 'Safe Region', 'Diagonal', 'Large Datasets', 'Eigenvalues', 'Eigenvectors', 'Gaussian Noise', 'Ellipsoid', 'Time Complexity', 'ImageNet', 'Transformation Matrix', 'Positive Definite Matrix', 'Definite Matrix', 'Orthogonal Matrix', 'Average Radius', 'Adversarial Training', 'Adversarial Examples', 'Smooth Distribution', 'Anisotropic Diffusion', 'Adversarial Perturbations', 'Runtime Complexity', 'Symmetric Positive Definite Matrix', 'Covariance Matrix']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods']",,"As modern networks have been proven to be unprotected from adversarial attacks and are applied in safety-critical applications, defense against them is very crucial. Many works were dedicated to this topic, but randomized smoothing has been recently proven to be an effective approach for the certified defense of deep neural networks and getting robust classifiers. Some prior results were obtained utilizing the techniques of adding extra parameters to extend the limits of the certification regions. In this way, sample-wise optimization was proposed to maximize the certification radius per input. The idea was further extended with the generalized anisotropic counterparts of ℓ
<inf>1</inf>
 and ℓ
<inf>2</inf>
 certificates which allow achieving larger certified region volume avoiding worst-case certification near potentially larger safe regions. However, anisotropic certification is limited by the aligned axis lacking the freedom to extend in any direction. To mitigate this constraint, in this work, we (i) revisit the anisotropic certification, provide an analysis of its non-axis aligned counterpart and propose its rotation-free extension, (ii) conduct experiments on the CIFAR-10 dataset to report the improved performance."
RAST: Restorable Arbitrary Style Transfer via Multi-Restoration,"Yingnan Ma, Chenqiu Zhao, Xudong Li, Anup Basu","Dept. of Computing Science, University of Alberta, Canada",100,Canada,0,,"Arbitrary style transfer aims at reproducing the target image with provided artistic or photo-realistic styles. Even though existing approaches can successfully transfer style information, arbitrary style transfer still faces many challenges, such as the content leak issue. To be specific, the embedding of artistic style can lead to content changes. In this paper, we solve the content leak problem from the perspective of image restoration. In particular, an iterative architecture is proposed to achieve the restorable arbitrary style transfer (RAST), which can realize the transmission of both content and style information through the multi-restorations. We control the content-style balance in stylized images by the accuracy of image restoration. In order to ensure the effectiveness of the proposed RAST architecture, we design two novel loss functions: multi-restoration loss and style difference loss. In addition, we propose a new quantitative evaluation method to measure content preservation performance and style embedding performance. Comprehensive experiments comparing with state-of-the-art methods demonstrate that our proposed architecture can produce stylized images with superior performance on content preservation and style embedding.",https://openaccess.thecvf.com/content/WACV2023/html/Ma_RAST_Restorable_Arbitrary_Style_Transfer_via_Multi-Restoration_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ma_RAST_Restorable_Arbitrary_Style_Transfer_via_Multi-Restoration_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030201/,"['Computer vision', 'Computer architecture', 'Interference', 'Image restoration', 'Iterative methods', 'Faces']","['Style Transfer', 'Arbitrary Style', 'Arbitrary Style Transfer', 'Loss Function', 'Superior Performance', 'Information Content', 'Information Transmission', 'Leakage Of Contents', 'Content Transmission', 'Preservation Of Content', 'Transformer', 'Input Image', 'Feature Maps', 'User Study', 'Image Pairs', 'Feed-forward Network', 'Domain Adaptation', 'Content Features', 'Loss Of Identity', 'Self-supervised Learning', '2nd Row', 'Style Image', '3rd Row', 'Style Features', 'VGG-19 Network', 'Positional Encoding', 'Transformer-based Methods', 'Contrastive Loss', 'Transmission Method']","['Algorithms: Computational photography', 'image and video synthesis', 'Adversarial learning', 'adversarial attack and defense methods']",10,"Arbitrary style transfer aims to reproduce the target image with the artistic or photo-realistic styles provided. Even though existing approaches can successfully transfer style information, arbitrary style transfer still faces many challenges, such as the content leak issue. Specifically, the embedding of artistic style can lead to content changes. In this paper, we solve the content leak problem from the perspective of image restoration. In particular, an iterative architecture is proposed to achieve the Restorable Arbitrary Style Transfer (RAST), which can realize transmission of both content and style information through multi-restorations. We control the content-style balance in stylized images by the accuracy of image restoration. In order to ensure effectiveness of the proposed RAST architecture, we design two novel loss functions: multi-restoration loss and style difference loss. In addition, we propose a new quantitative evaluation method to measure content preservation performance and style embedding performance. Comprehensive experiments comparing with state-of-the-art methods demonstrate that our proposed architecture can produce stylized images with superior performance on content preservation and style embedding."
RIFT: Disentangled Unsupervised Image Translation via Restricted Information Flow,"Ben Usman, Dina Bashkirova, Kate Saenko",Boston University,100,USA,0,,"Unsupervised image-to-image translation methods aim to map images from one domain into plausible examples from another domain while preserving the structure shared across two domains. In the many-to-many setting, an additional guidance example from the target domain is used to determine the domain-specific factors of variation of the generated image. In the absence of attribute annotations, methods have to infer which factors of variation are specific to each domain from data during training. In this paper, we show that many state-of-the-art architectures implicitly treat textures and colors as always being domain-specific, and thus fail when they are not. We propose a new method called RIFT that does not rely on such inductive architectural biases and instead infers which attributes are domain-specific vs shared directly from data. As a result, RIFT achieves consistently high cross-domain manipulation accuracy across multiple datasets spanning a wide variety of domain-specific and shared factors of variation.",https://openaccess.thecvf.com/content/WACV2023/html/Usman_RIFT_Disentangled_Unsupervised_Image_Translation_via_Restricted_Information_Flow_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Usman_RIFT_Disentangled_Unsupervised_Image_Translation_via_Restricted_Information_Flow_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030560/,"['Training', 'Computer vision', 'Image resolution', 'Image color analysis', 'Annotations', 'Computer architecture']","['Unsupervised Translation', 'Restricted Information Flow', 'Variety Of Factors', 'Target Domain', 'Translation Method', 'Inductive Bias', 'Average Accuracy', 'Skin Color', 'Mutual Information', 'Unsupervised Methods', 'Loss Of Capacity', 'Image Formation', 'Source Images', 'Object Shape', 'Input Source', 'Hair Color', 'Respective Domains', 'Bigotry', 'Reconstruction Loss', 'Original Domain', 'Kinds Of Properties', 'Domain-specific Information', 'Style Transfer', 'Cycle Loss', 'Form Of Noise', 'Real-valued Vector', 'Input Image', 'Gaussian Noise', 'Semantic']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Computational photography', 'image and video synthesis', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",,"Unsupervised image-to-image translation methods aim to map images from one domain into plausible examples from another domain while preserving the structure shared across two domains. In the many-to-many setting, an additional guidance example from the target domain is used to determine the domain-specific factors of variation of the generated image. In the absence of attribute annotations, methods have to infer which factors of variation are specific to each domain from data during training. In this paper, we show that many state-of-the-art architectures implicitly treat textures and colors as always being domain-specific, and thus fail when they are not. We propose a new method called RIFT that does not rely on such inductive architectural biases and instead infers which attributes are domain-specific vs shared directly from data. As a result, RIFT achieves consistently high cross-domain manipulation accuracy across multiple datasets spanning a wide variety of domain-specific and shared factors of variation."
RNAS-MER: A Refined Neural Architecture Search With Hybrid Spatiotemporal Operations for Micro-Expression Recognition,"Monu Verma, Priyanka Lubal, Santosh Kumar Vipparthi, Mohamed Abdel-Mottaleb","Vision Intelligence Lab, Malaviya National Institute of Technology, Jaipur, India; Electrical and Computer Engineering, University of Miami, USA; CVPR Lab, Indian Institute of Technology, Ropar, India",100,"India, USA",0,,"Existing NAS methods comprise linear connected convolutional operations and used ample search space to search task-driven convolution neural networks (CNN). These CNN models are computationally expensive and diminish the quality of receptive fields for tasks like micro-expression recognition (MER) with limited training samples. Therefore, we proposed a refined neural architecture search strategy to search a tiny CNN architecture for MER. In addition, we introduced a refined hybrid module (RHM) for innerlevel search space and an optimal path explore network (OPEN) for outer-level search. The RHM focuses on discovering optimal cell structures by incorporating a multilateral hybrid spatiotemporal operation space. Also, spatiotemporal attention blocks are embedded to refine the aggregated cell features. The OPEN search space aims to trace an optimal path between the cells to generate a tiny spatiotemporal CNN architecture instead of covering all possible tracks. The aggregate mix of RHM and OPEN search space availed the NAS method to robustly search and design an effective and efficient framework for MER. Compared with contemporary works, experiments reveal that the RNAS-MER is capable of bridging the gap between NAS algorithms and MER tasks. Further, RNAS-MER achieves new state-of-the-art performances on challenging MER benchmarks, including % on CASME-2, % SMIC, % SAMM, and % on COMPOSITE.",https://openaccess.thecvf.com/content/WACV2023/html/Verma_RNAS-MER_A_Refined_Neural_Architecture_Search_With_Hybrid_Spatiotemporal_Operations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Verma_RNAS-MER_A_Refined_Neural_Architecture_Search_With_Hybrid_Spatiotemporal_Operations_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030537/,"['Training', 'Computer vision', 'Convolution', 'Microprocessors', 'Stacking', 'Computer architecture', 'Benchmark testing']","['Neural Architecture Search', 'Micro-expression Recognition', 'Spatiotemporal Operations', 'Convolutional Neural Network', 'Cell Structure', 'Search Space', 'Receptive Field', 'Convolution Operation', 'Convolutional Neural Network Model', 'Convolutional Neural Network Architecture', 'Optimal Path', 'Operational Space', 'Limited Training Samples', 'Computational Complexity', 'Computational Analysis', 'Feature Maps', 'Disgust', 'Hidden Nodes', 'Structure Search', 'Memory Footprint', 'Partial Connection', 'Hybrid Operating', 'Compact Footprint', 'Cell Nodes', 'Robust Architecture', 'Suitable Architecture', 'Final Architecture', 'Shallow Convolutional Neural Network', 'Size Of Operation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Biometrics', 'face', 'gesture', 'body pose', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",7,"Existing neural architecture search (NAS) methods comprise linear connected convolution operations and use ample search space to search task-driven convolution neural networks (CNN). These CNN models are computationally expensive and diminish the quality of receptive fields for tasks like micro-expression recognition (MER) with limited training samples. Therefore, we propose a refined neural architecture search strategy to search for a tiny CNN architecture for MER. In addition, we introduced a refined hybrid module (RHM) for inner-level search space and an optimal path explore network (OPEN) for outer-level search space. The RHM focuses on discovering optimal cell structures by incorporating a multilateral hybrid spatiotemporal operation space. Also, spatiotemporal attention blocks are embedded to refine the aggregated cell features. The OPEN search space aims to trace an optimal path between the cells to generate a tiny spatiotemporal CNN architecture instead of covering all possible tracks. The aggregate mix of RHM and OPEN search space availed the NAS method to robustly search and design an effective and efficient framework for MER. Compared with contemporary works, experiments reveal that the RNAS-MER is capable of bridging the gap between NAS algorithms and MER tasks. Furthermore, RNAS-MER achieves new state-of-the-art performances on challenging MER benchmarks, including 0.8511%, 0.7620%, 0.9078% and 0.8235% UAR on COMPOSITE, SMIC, CASME-II and SAMM datasets respectively."
ROMA: Run-Time Object Detection To Maximize Real-Time Accuracy,"JunKyu Lee, Blesson Varghese, Hans Vandierendonck","University of St Andrews, St Andrews, UK; Queen’s University Belfast, Belfast, UK",100,"Canada, UK",0,,"This paper analyzes the effects of dynamically varying video contents and detection latency on the real-time detection accuracy of a detector and proposes a new run-time accuracy variation model, ROMA, based on the findings from the analysis. ROMA is designed to select an optimal detector out of a set of detectors in real time without label information to maximize real-time object detection accuracy. ROMA utilizing four YOLOv4 detectors on an NVIDIA Jetson Nano shows real-time accuracy improvements by 4 to 37% for a scenario of dynamically varying video contents and detection latency consisting of MOT17Det and MOT20Det datasets, compared to individual YOLOv4 detectors and two state-of-the-art runtime techniques.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_ROMA_Run-Time_Object_Detection_To_Maximize_Real-Time_Accuracy_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_ROMA_Run-Time_Object_Detection_To_Maximize_Real-Time_Accuracy_WACV_2023_paper.pdf,,,2210.16083,main,Poster,https://ieeexplore.ieee.org/document/10030225/,"['Histograms', 'Computer vision', 'Analytical models', 'Runtime', 'Detectors', 'Object detection', 'Switches']","['Object Detection', 'Detection Accuracy', 'Real-time Detection', 'Variation In Accuracy', 'Label Information', 'Video Content', 'Real-time Object Detection', 'Frame Rate', 'Bounding Box', 'Video Frames', 'Number Of Objects', 'Object Size', 'Single Detection', 'Evaluation Dataset', 'Object Tracking', 'Computing Devices', 'Multiple Detection', 'Current Detection', 'Previous Frame', 'Correct Detection', 'Frames Per Second', 'Early Frames', 'Computation Latency', 'Time Overhead', 'Series Of Frames']",['Applications: Embedded sensing/real-time techniques'],3,"This paper analyzes the effects of dynamically varying video contents and detection latency on the real-time detection accuracy of a detector and proposes a new run-time accuracy variation model, ROMA, based on the findings from the analysis. ROMA is designed to select an optimal detector out of a set of detectors in real time without label information to maximize real-time object detection accuracy. ROMA utilizing four YOLOv4 detectors on an NVIDIA Jetson Nano shows real-time accuracy improvements by 4 to 37% for a scenario of dynamically varying video con-tents and detection latency consisting of MOT17Det and MOT20Det datasets, compared to individual YOLOv4 detectors and two state-of-the-art runtime techniques."
RSF: Optimizing Rigid Scene Flow From 3D Point Clouds Without Labels,"David Deng, Avideh Zakhor",UC Berkeley,100,USA,0,,"We present a method for optimizing object-level rigid 3D scene flow over two successive point clouds without any annotated labels in autonomous driving settings. Rather than using pointwise flow vectors, our approach represents scene flow as the composition a global ego-motion and a set of bounding boxes with their own rigid motions, exploiting the multi-body rigidity commonly present in dynamic scenes. We jointly optimize these parameters over a novel loss function based on the nearest neighbor distance using a differentiable bounding box formulation. Our approach achieves state-of-the-art accuracy on KITTI Scene Flow and nuScenes without requiring any annotations, outperforming even supervised methods. Additionally, we demonstrate the effectiveness of our approach on motion segmentation and ego-motion estimation. Lastly, we visualize our predictions and validate our loss function design with an ablation study.",https://openaccess.thecvf.com/content/WACV2023/html/Deng_RSF_Optimizing_Rigid_Scene_Flow_From_3D_Point_Clouds_Without_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Deng_RSF_Optimizing_Rigid_Scene_Flow_From_3D_Point_Clouds_Without_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030308/,"['Point cloud compression', 'Computer vision', 'Visualization', 'Three-dimensional displays', 'Laser radar', 'Motion segmentation', 'Dynamics']","['Point Cloud', 'Scene Flow', 'Rigid Flow', 'Rigid Scene Flow', 'Loss Function', 'Bounding Box', 'Rigid Transformation', 'Nearest Neighbor Distance', 'Flow Vector', 'Annotated Labels', 'Loss Function Design', 'Deep Learning', 'Percentage Points', 'Recurrent Network', 'Object Detection', 'Step Function', 'Membership Function', 'Final Loss', 'Static Objects', 'Boundary Line', 'Rotation Error', 'Deep Learning Features', 'LiDAR Scans', 'Dynamic Objects', 'Mass Term', 'Supplement For Details', 'Chamfer Distance', '3D Bounding Box', 'Translation Error', 'Direct Correspondence']",['Algorithms: 3D computer vision'],6,"We present a method for optimizing object-level rigid 3D scene flow over two successive point clouds without any annotated labels in autonomous driving settings. Rather than using pointwise flow vectors, our approach represents scene flow as the composition a global ego-motion and a set of bounding boxes with their own rigid motions, exploiting the multi-body rigidity commonly present in dynamic scenes. We jointly optimize these parameters over a novel loss function based on the nearest neighbor distance using a differentiable bounding box formulation. Our approach achieves state-of-the-art accuracy on KITTI Scene Flow and nuScenes without requiring any annotations, outperforming even supervised methods. Additionally, we demonstrate the effectiveness of our approach on motion segmentation and ego-motion estimation. Lastly, we visualize our predictions and validate our loss function design with an ablation study."
Randomness Is the Root of All Evil: More Reliable Evaluation of Deep Active Learning,"Yilin Ji, Daniel Kaestner, Oliver Wirth, Christian Wressnegger","KASTEL Security Research Labs, Karlsruhe Institute of Technology",100,Germany,0,,"Using deep neural networks for active learning (AL) poses significant challenges for the stability and the reproducibility of experimental results. Inconsistent settings continue to be the root causes for contradictory conclusions and in worst cases, for incorrect appraisal of methods. Our community is in search of a unified framework for exhaustive and fair evaluation of deep active learning. In this paper, we provide just such a framework, one which is built upon systematically fixing, containing and interpreting sources of randomness. We isolate different influence factors, such as neural-network initialization or hardware specifics, to assess their impact on the learning performance. We then use our framework to analyze the effects of basic AL settings, such as the query-batch size and the use of subset selection, and different datasets on AL performance. Our findings enable us to derive specific recommendations for the reliable evaluation of deep active learning, thus helping advance the community toward a more normative evaluation of results.",https://openaccess.thecvf.com/content/WACV2023/html/Ji_Randomness_Is_the_Root_of_All_Evil_More_Reliable_Evaluation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ji_Randomness_Is_the_Root_of_All_Evil_More_Reliable_Evaluation_WACV_2023_paper.pdf,https://intellisec.de/research/eval-al,,,main,Poster,https://ieeexplore.ieee.org/document/10030143/,"['Deep learning', 'Computer vision', 'Neural networks', 'Stability analysis', 'Reproducibility of results', 'Hardware', 'Appraisal']","['Active Learning', 'Reliable Evaluation', 'Learning Evaluation', 'Deep Learning', 'Source Of Randomness', 'Learning Rate', 'Comparative Evaluation', 'Data Augmentation', 'Active Strategies', 'Stochastic Gradient Descent', 'Hyperparameter Tuning', 'Early Stopping', 'Random Strategy', 'Rigorous Evaluation', 'Pairwise Matrix', 'Imbalanced Datasets', 'Ranking Method', 'Cold Start', 'Active Learning Strategies', 'Backbone Model', 'Warm Start', 'Active Learning Methods', 'Backbone Architecture', 'Execution Environment', 'Learning Setup', 'Prior Experience', 'Types Of Datasets', 'Weight Initialization', 'Random Initialization', 'Labeling Efficiency']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",3,"Using deep neural networks for active learning (AL) poses significant challenges for the stability and the reproducibility of experimental results. Inconsistent settings continue to be the root causes for contradictory conclusions and in worst cases, for incorrect appraisal of methods. Our community is in search of a unified framework for exhaustive and fair evaluation of deep active learning. In this paper, we provide just such a framework, one which is built upon systematically fixing, containing and interpreting sources of randomness. We isolate different influence factors, such as neural-network initialization or hardware specifics, to assess their impact on the learning performance. We then use our framework to analyze the effects of basic AL settings, such as the query-batch size and the use of subset selection, and different datasets on AL performance. Our findings enable us to derive specific recommendations for the reliable evaluation of deep active learning, thus helping advance the community toward a more normative evaluation of results."
ReEnFP: Detail-Preserving Face Reconstruction by Encoding Facial Priors,"Yasheng Sun, Jiangke Lin, Hang Zhou, Zhiliang Xu, Dongliang He, Hideki Koike","Zhejiang University, Hangzhou, China; Baidu Inc., Beijing, China; Baidu Inc., Shenzhen, China; Baidu Inc., Shanghai, China; Tokyo Institute of Technology, Tokyo, Japan",40,"China, Japan",60,China,"We address the problem of face modeling, which is still challenging in achieving high-quality reconstruction results efficiently. Neither previous regression-based nor optimization-based frameworks could well balance between the facial reconstruction fidelity and efficiency. We notice that the large amount of in-the-wild facial images contain diverse appearance information, however, their underlying knowledge is not fully exploited for face modeling. To this end, we propose our Reconstruction by Encoding Facial Priors (ReEnFP) pipeline to exploit the potential of unconstrained facial images for further improvement. Our key is to encode generative priors learned by a style-based texture generator on unconstrained data for fast and detail-preserving face reconstruction. With our texture generator pre-trained using a differentiable renderer, faces could be encoded to its latent space as opposed to the time-consuming optimization-based inversion. Our generative prior encoding is further enhanced with a pyramid fusion block for adaptive integration of input spatial information. Extensive experiments show that our method reconstructs photo-realistic facial textures and geometric details with precise identity recovery.",https://openaccess.thecvf.com/content/WACV2023/html/Sun_ReEnFP_Detail-Preserving_Face_Reconstruction_by_Encoding_Facial_Priors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sun_ReEnFP_Detail-Preserving_Face_Reconstruction_by_Encoding_Facial_Priors_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030820/,"['Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Image coding', 'Pipelines', 'Generators', 'Encoding']","['Oral And Maxillofacial Surgery', 'Facial Priors', 'Latent Space', 'Prior Learning', 'Face Model', 'Geometric Details', 'Appearance Information', 'Fusion Block', 'Model Parameters', 'Illumination', 'Image Features', 'Input Image', 'Single Image', 'Face Recognition', 'Generative Adversarial Networks', '3D Scanning', 'Ratio Images', 'Map Position', 'Spherical Harmonics', 'Facial Appearance', 'Facial Details', 'Latent Code', 'Mean Opinion Score', 'Displacement Maps', 'Spatial Feature Extraction', 'Dual Representation', 'Prior Network', 'Detail Preservation', 'Manhattan Distance', 'Feature Maps']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Adversarial learning', 'adversarial attack and defense methods', 'Computational photography', 'image and video synthesis']",,"We address the problem of face modeling, which is still challenging in achieving high-quality reconstruction results efficiently. Neither previous regression-based nor optimization-based frameworks could well balance between the facial reconstruction fidelity and efficiency. We notice that the large amount of in-the-wild facial images contain diverse appearance information, however, their underlying knowledge is not fully exploited for face modeling. To this end, we propose our Reconstruction by Encoding Facial Priors (ReEnFP) pipeline to exploit the potential of unconstrained facial images for further improvement. Our key is to encode generative priors learned by a style-based texture generator on unconstrained data for fast and detail-preserving face reconstruction. With our texture generator pre-trained using a differentiable renderer, faces could be encoded to its latent space as opposed to the time-consuming optimization-based inversion. Our generative prior encoding is further enhanced with a pyramid fusion block for adaptive integration of input spatial information. Extensive experiments show that our method reconstructs photo-realistic facial textures and geometric details with precise identity recovery."
Real-Time Concealed Weapon Detection on 3D Radar Images for Walk-Through Screening System,"Nagma S. Khan, Kazumine Ogura, Eric Cosatto, Masayuki Ariyoshi","R&D Division, NEC Corporation, Japan; NEC Laboratories America, Inc.",0,,100,Japan,"This paper presents a framework for real-time concealed weapon detection (CWD) on 3D radar images for walk-through screening systems. The walk-through screening system aims to ensure security in crowded areas by performing CWD on walking persons, hence it requires an accurate and real-time detection approach. To ensure accuracy, a weapon needs to be detected irrespective of its 3D orientation, thus we use the 3D radar images as detection input. For achieving real-time, we reformulate classic U-Net based segmentation networks to perform 3D detection tasks. Our 3D segmentation network predicts peak-shaped probability map, instead of voxel-wise masks, to enable position inference by elementary peak detection operation on the predicted map. In the peak-shaped probability map, the peak marks the weapon's position. So, weapon detection task translates to peak detection on the probability map. A Gaussian function is used to model weapons in the probability map. We experimentally validate our approach on realistic 3D radar images obtained from a walk-through weapon screening system prototype. Extensive ablation studies verify the effectiveness of our proposed approach over existing conventional approaches. The experimental results demonstrate that our proposed approach can perform accurate and real-time CWD, thus making it suitable for practical applications of walk-through screening.",https://openaccess.thecvf.com/content/WACV2023/html/Khan_Real-Time_Concealed_Weapon_Detection_on_3D_Radar_Images_for_Walk-Through_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Khan_Real-Time_Concealed_Weapon_Detection_on_3D_Radar_Images_for_Walk-Through_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030435/,"['Image segmentation', 'Three-dimensional displays', 'Weapons', 'Radar detection', 'Prototypes', 'Radar', 'Radar imaging']","['Real-time Detection', 'Screening System', 'Radar Images', 'Concealed Weapons', 'Weapon Detection', 'Concealed Weapon Detection', 'Gaussian Kernel', '3D Images', 'Probability Function', 'Detection Approach', 'Detection Task', 'Peak Detection', '3D Network', 'Real-time Approach', '3D Detection', 'Person Walking', 'Receiver Operating Characteristic Curve', 'Convolutional Neural Network', 'False Positive Rate', 'Point Cloud', 'Bounding Box', '3D Approach', 'Easy Cases', '2D Projection', '2D Approach', '3D Bounding Box', 'Body Scan', '3D Object Detection', 'Region Proposal Network']","['Applications: Commercial/retail', 'Embedded sensing/real-time techniques', 'Social good']",2,"This paper presents a framework for real-time concealed weapon detection (CWD) on 3D radar images for walk-through screening systems. The walk-through screening system aims to ensure security in crowded areas by performing CWD on walking persons, hence it requires an accurate and real-time detection approach. To ensure accuracy, a weapon needs to be detected irrespective of its 3D orientation, thus we use the 3D radar images as detection input. For achieving real-time, we reformulate classic U-Net based segmentation networks to perform 3D detection tasks. Our 3D segmentation network predicts peak-shaped probability map, instead of voxel-wise masks, to enable position inference by elementary peak detection operation on the predicted map. In the peak-shaped probability map, the peak marks the weapon’s position. So, weapon detection task translates to peak detection on the probability map. A Gaussian function is used to model weapons in the probability map. We experimentally validate our approach on realistic 3D radar images obtained from a walk-through weapon screening system prototype. Extensive ablation studies verify the effectiveness of our proposed approach over existing conventional approaches. The experimental results demonstrate that our proposed approach can perform accurate and real-time CWD, thus making it suitable for practical applications of walk-through screening."
Real-Time Restoration of Dark Stereo Images,"Mohit Lamba, M. V. A. Suhas Kumar, Kaushik Mitra",Indian Institute of Technology Madras,100,India,0,,"Low-light image enhancement has been an actively researched area for decades and has produced excellent night-time single-image, video, and Light Field restoration methods. Despite these comprehensive studies, the problem of extreme low-light stereo image enhancement has been mostly ignored. Addressing this problem can enable night-time capabilities to several applications such as smartphones and self-driving cars. We propose a light-weight and fast hybrid U-net architecture for low-light stereo image enhancement. In the initial few scale spaces, we process the left and right features individually, because the two features do not align well due to large disparity. At coarser scale-spaces, the disparity between left and right features decreases and the network's receptive field increases. We use this fact to reduce computations by simultaneously processing the left and right features, which also benefits epipole preservation. As our architecture does not use any 3D convolution for fast inference, we use an Epipole-Aware loss module to train our network. This module computes quick and coarse depth estimates to better enforce the epipolar constraints. Extensive benchmarking in terms of visual enhancement and downstream depth estimation shows that our architecture not only performs significantly better but also offers 4-60 xspeed-up with 15-100 xlower floating point operations, suitable for real-world deployment.",https://openaccess.thecvf.com/content/WACV2023/html/Lamba_Real-Time_Restoration_of_Dark_Stereo_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lamba_Real-Time_Restoration_of_Dark_Stereo_Images_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030097/,"['Visualization', 'Three-dimensional displays', 'Convolution', 'Estimation', 'Computer architecture', 'Real-time systems', 'Light fields']","['Stereo Images', 'Receptive Field', 'Light Field', 'Self-driving', 'Image Enhancement', 'Depth Estimation', 'Scale Space', '3D Convolution', 'Dark Images', 'Visual Enhancement', 'Low-light Image', 'Super-resolution', 'Semantic Segmentation', 'Attention Module', 'Depth Map', 'Monocular', 'Computational Overhead', 'Noise Suppression', 'Linear Space', 'Left View', 'KITTI Dataset', 'Disparity Map', 'Stereo Pairs', 'Confidence Map', 'Hybrid Architecture', 'Poor Contrast', 'Ground Truth Depth', 'Inference Speed', 'Histogram Equalization']","['Applications: Commercial/retail', 'Embedded sensing/real-time techniques', 'Robotics']",,"Low-light image enhancement has been an actively researched area for decades and has produced excellent night-time single-image, video, and Light Field restoration methods. Despite these advances, the problem of extreme low-light stereo image restoration has been mostly ignored and addressing it can enable night-time capabilities to several applications such as smartphones and self-driving cars. We propose an especially light-weight and fast hybrid U-net architecture for extreme low-light stereo image restoration. In the initial few scale spaces, we process the left and right features individually, because the two features do not align well due to large disparity. At coarser scale-spaces, the disparity between left and right features decreases and the network’s receptive field increases. We use this fact to reduce computations by simultaneously processing the left and right features, which also benefits epipole preservation. As our architecture does not use any 3D convolution for fast inference, we use a Depth-Aware loss module to train our network. This module computes quick and coarse depth estimates to better enforce the stereo epipolar constraints. Extensive benchmarking in terms of visual enhancement and downstream depth estimation shows that our architecture not only restores dark stereo images faithfully but also offers 4−60× speed-up with 15−100× lower floating point operations, necessary for real-world applications."
Realistic Full-Body Anonymization With Surface-Guided GANs,"Håkon Hukkelås, Morten Smebye, Rudolf Mester, Frank Lindseth",Norwegian University of Science and Technology,100,Norway,0,,"Recent work on image anonymization has shown that generative adversarial networks (GANs) can generate near-photorealistic faces to anonymize individuals. However, scaling up these networks to the entire human body has remained a challenging and yet unsolved task. We propose a new anonymization method that generates realistic humans for in-the-wild images. A key part of our design is to guide adversarial nets by dense pixel-to-surface correspondences between an image and a canonical 3D surface. We introduce Variational Surface-Adaptive Modulation (V-SAM) that embeds surface information throughout the generator. Combining this with our novel discriminator surface supervision loss, the generator can synthesize high quality humans with diverse appearances in complex and varying scenes. We demonstrate that surface guidance significantly improves image quality and diversity of samples, yielding a highly practical generator. Finally, we show that our method preserves data usability without infringing privacy when collecting image datasets for training computer vision models.",https://openaccess.thecvf.com/content/WACV2023/html/Hukkelas_Realistic_Full-Body_Anonymization_With_Surface-Guided_GANs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hukkelas_Realistic_Full-Body_Anonymization_With_Surface-Guided_GANs_WACV_2023_paper.pdf,,github.com/hukkelas/full body anonymization,,main,Poster,https://ieeexplore.ieee.org/document/10030530/,"['Data privacy', 'Deepfakes', 'Computational modeling', 'Diversity reception', 'Generators', 'Information filtering', 'Usability']","['Generative Adversarial Networks', 'Computer Vision', 'User Data', '3D Surface', 'Constant Function', 'Improve Image Quality', 'Convolution', 'Latent Variables', 'Feature Maps', 'Latent Space', 'Peak Signal-to-noise Ratio', 'Affine Transformation', 'Abuse Potential', 'Camera View', 'Future Vision', 'Image Synthesis', 'Compact Representation', 'Validation Images', 'Face Detection', 'Modulation Parameters', 'Fréchet Inception Distance', 'Adaptive Modulation', 'COCO Dataset', 'Human Figure', 'Image Inpainting', 'Intermediate Space', 'Personal Information', 'Latent Code', 'Average Precision']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Biometrics', 'face', 'gesture', 'body pose', 'Computational photography', 'image and video synthesis']",10,"Recent work on image anonymization has shown that generative adversarial networks (GANs) can generate near-photorealistic faces to anonymize individuals. However, scaling up these networks to the entire human body has remained a challenging and yet unsolved task. We propose a new anonymization method that generates realistic humans for in-the-wild images. A key part of our design is to guide adversarial nets by dense pixel-to-surface correspondences between an image and a canonical 3D surface. We introduce Variational Surface-Adaptive Modulation (V-SAM) that embeds surface information throughout the generator. Combining this with our novel discriminator surface supervision loss, the generator can synthesize high quality humans with diverse appearances in complex and varying scenes. We demonstrate that surface guidance significantly improves image quality and diversity of samples, yielding a highly practical generator. Finally, we show that our method preserves data usability without infringing privacy when collecting image datasets for training computer vision models. Source code and appendix is available at: github.com/hukkelas/full_body_anonymization"
"Rebalancing Gradient To Improve Self-Supervised Co-Training of Depth, Odometry and Optical Flow Predictions","Marwane Hariat, Antoine Manzanera, David Filliat","INRIA FLOWERS; U2IS, ENSTA Paris, Institut Polytechnique de Paris, Palaiseau, France",100,France,0,,"We present CoopNet, an approach that improves the cooperation of co-trained networks by dynamically adapting the apportionment of gradient, to ensure equitable learning progress. It is applied to motion-aware self-supervised prediction of depth maps, by introducing a new hybrid loss, based on a distribution model of photo-metric reconstruction errors made by, on the one hand the depth + odometry paired networks, and on the other hand the optical flow network. This model essentially assumes that the pixels from moving objects (that must be discarded for training depth and odometry), correspond to those where the two reconstructions strongly disagree. We justify this model by theoretical considerations and experimental evidences, and show that its implementation improves or is comparable to the state of the art in depth, odometry and optical flow predictions. Our code is available here: https://github.com/mhariat/CoopNet.",https://openaccess.thecvf.com/content/WACV2023/html/Hariat_Rebalancing_Gradient_To_Improve_Self-Supervised_Co-Training_of_Depth_Odometry_and_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hariat_Rebalancing_Gradient_To_Improve_Self-Supervised_Co-Training_of_Depth_Odometry_and_WACV_2023_paper.pdf,,https://github.com/mhariat/CoopNet,,main,Poster,https://ieeexplore.ieee.org/document/10030266/,"['Training', 'Optical losses', 'Visualization', 'Computer vision', 'Adaptation models', 'Codes', 'Estimation']","['Optical Flow', 'Odometry', 'Flow Prediction', 'Depth Prediction', 'Optical Flow Prediction', 'Depth Map', 'Network Flow', 'Optical Networks', 'Visual Task', 'Training Strategy', 'Tail Of Distribution', 'Pose Estimation', 'Depth Estimation', 'Network Depth', 'Self-supervised Learning', 'Camera Pose', 'Tail Part', 'Intrinsic Bias', 'Warped Image', 'Optical Flow Estimation', 'Camera Pose Estimation', 'Intermediate Estimates', 'Monocular Images', 'Substantial Margin', 'Supervision Signal', 'Pixel Displacement']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', '3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"We present CoopNet, an approach that improves the co-operation of co-trained networks by dynamically adapting the apportionment of gradient, to ensure equitable learning progress. It is applied to motion-aware self-supervised prediction of depth maps, by introducing a new hybrid loss, based on a distribution model of photo-metric reconstruction errors made by, on the one hand the depth + odometry paired networks, and on the other hand the optical flow network. This model essentially assumes that the pixels from moving objects (that must be discarded for training depth and odometry), correspond to those where the two reconstructions strongly disagree. We justify this model by theoretical considerations and experimental evidences. A comparative evaluation on KITTI and CityScapes datasets shows that CoopNet improves or is comparable to the state-of-the-art in depth, odometry and optical flow predictions. Our code is available here: https://github.com/mhariat/CoopNet."
Recipe2Video: Synthesizing Personalized Videos From Recipe Texts,"Prateksha Udhayanan, Suryateja BV, Parth Laturia, Dev Chauhan, Darshan Khandelwal, Stefano Petrangeli, Balaji Vasan Srinivasan",Morgan Stanley; Avanti Fellows; Graviton Research Capital LLP; Adobe Research; Goldman Sachs,0,,100,USA,"Procedural texts are a special type of documents that contain complex textual descriptions for carrying out a sequence of instructions. Due to the lack of visual cues, it often becomes difficult to consume the textual information effectively. In this paper, we focus on recipes - a particular type of procedural document and introduce a novel deep-learning driven system - Recipe2Video that automatically converts a recipe document into a multimodal illustrative video. Our method employs novel retrieval and re-ranking methods to select the best set of images and videos that can provide the desired illustration. We formulate a Viterbi-based optimization algorithm to stitch together a coherent video that combines the visual cues, text and voice-over to present an enhanced mode of consumption. We design automated metrics and compare performance across several baselines on two recipe datasets (RecipeQA, Tasty Videos). Our results on downstream tasks and human studies indicate that Recipe2Video captures the semantic and sequential information of the input in the generated video.",https://openaccess.thecvf.com/content/WACV2023/html/Udhayanan_Recipe2Video_Synthesizing_Personalized_Videos_From_Recipe_Texts_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Udhayanan_Recipe2Video_Synthesizing_Personalized_Videos_From_Recipe_Texts_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030241/,"['Measurement', 'Visualization', 'Computer vision', 'Semantics', 'Layout', 'Quality assessment', 'Task analysis']","['Sequence Of Instructions', 'Quantitative Evaluation', 'Kullback-Leibler', 'Singular Value Decomposition', 'Temporal Aspects', 'Video Quality', 'Input Text', 'Forms Of Consumption', 'Key Phrases', 'Video Duration', 'Weak Supervision', 'Cloze Test', 'Final Video', 'Semantic Coherence', 'Number Of Verbs']","['Applications: Visualization', 'Vision + language and/or other modalities']",1,"Procedural texts are a special type of documents that contain complex textual descriptions for carrying out a sequence of instructions. Due to the lack of visual cues, it often becomes difficult to consume the textual information effectively. In this paper, we focus on recipes - a particular type of procedural document and introduce a novel deep-learning driven system - Recipe2Video that automatically converts a recipe document into a multimodal illustrative video. Our method employs novel retrieval and re-ranking methods to select the best set of images and videos that can provide the desired illustration. We formulate a Viterbi-based optimization algorithm to stitch together a coherent video that combines the visual cues, text and voice-over to present an enhanced mode of consumption. We design automated metrics and compare performance across several baselines on two recipe datasets (RecipeQA, Tasty Videos). Our results on downstream tasks and human studies indicate that Recipe2Video captures the semantic and sequential information of the input in the generated video."
Reconstructing Humpty Dumpty: Multi-Feature Graph Autoencoder for Open Set Action Recognition,"Dawei Du, Ameya Shringi, Anthony Hoogs, Christopher Funk",Kitware,100,USA,0,,"Most action recognition datasets and algorithms assume a closed world, where all test samples are instances of the known classes. In open set problems, test samples may be drawn from either known or unknown classes. Existing open set action recognition methods are typically based on extending closed set methods by adding post hoc analysis of classification scores or feature distances and do not capture the relations among all the video clip elements. Our approach uses the reconstruction error to determine the novelty of the video since unknown classes are harder to put back together and thus have a higher reconstruction error than videos from known classes. We refer to our solution to the open set action recognition problem as ""Humpty Dumpty"", due to its reconstruction abilities. Humpty Dumpty is a novel graph-based autoencoder that accounts for contextual and semantic relations among the clip pieces for improved reconstruction. A larger reconstruction error leads to an increased likelihood that the action can not be reconstructed, i.e., can not put Humpty Dumpty back together again, indicating that the action has never been seen before and is novel/unknown. Extensive experiments are performed on two publicly available action recognition datasets including HMDB-51 and UCF-101, showing the state-of-the-art performance for open set action recognition.",https://openaccess.thecvf.com/content/WACV2023/html/Du_Reconstructing_Humpty_Dumpty_Multi-Feature_Graph_Autoencoder_for_Open_Set_Action_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Du_Reconstructing_Humpty_Dumpty_Multi-Feature_Graph_Autoencoder_for_Open_Set_Action_WACV_2023_paper.pdf,,https://github.com/Kitware/graphautoencoder,2212.06023,main,Poster,https://ieeexplore.ieee.org/document/10030468/,"['Computer vision', 'Semantics', 'Classification algorithms']","['Action Recognition', 'Open Set', 'Humpty Dumpty', 'Video Clips', 'Semantic Similarity', 'Recognition Algorithm', 'Action Recognition Datasets', 'Neural Network', 'Convolutional Layers', 'Latent Space', 'Area Under Curve', 'Gaussian Mixture Model', 'Graph Convolutional Network', 'Node Features', 'Graph Construction', 'Temporal Distance', 'Video Information', 'Graph Convolution', 'Score Map', 'Zero-shot', 'Video Action Recognition', 'Bayesian Neural Network', 'Novelty Detection', 'Temporal Similarity', 'Self-attention Layer', 'Attention Heads', 'Average Pooling', 'Feature Pooling', 'Convolutional Network']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",1,"Most action recognition datasets and algorithms assume a closed world, where all test samples are instances of the known classes. In open set problems, test samples may be drawn from either known or unknown classes. Existing open set action recognition methods are typically based on extending closed set methods by adding post hoc analysis of classification scores or feature distances and do not capture the relations among all the video clip elements. Our approach uses the reconstruction error to determine the novelty of the video since unknown classes are harder to put back together and thus have a higher reconstruction error than videos from known classes. We refer to our solution to the open set action recognition problem as ""Humpty Dumpty"", due to its reconstruction abilities. Humpty Dumpty is a novel graph-based autoencoder that accounts for contextual and semantic relations among the clip pieces for improved reconstruction. A larger reconstruction error leads to an increased likelihood that the action can not be reconstructed, i.e., can not put Humpty Dumpty back together again, indicating that the action has never been seen before and is novel/unknown. Extensive experiments are performed on two publicly available action recognition datasets including HMDB-51 and UCF-101, showing the state-of-the-art performance for open set action recognition."
Recovering Fine Details for Neural Implicit Surface Reconstruction,"Decai Chen, Peng Zhang, Ingo Feldmann, Oliver Schreer, Peter Eisert",TU Berlin; HU Berlin; Fraunhofer HHI,33.33333333,Germany,66.66666667,Germany,"Recent works on implicit neural representations have made significant strides. Learning implicit neural surfaces using volume rendering has gained popularity in multi-view reconstruction without 3D supervision. However, accurately recovering fine details is still challenging, due to the underlying ambiguity of geometry and appearance representation. In this paper, we present D-NeuS, a volume rendering-base neural implicit surface reconstruction method capable to recover fine geometry details, which extends NeuS by two additional loss functions targeting enhanced reconstruction quality. First, we encourage the rendered surface points from alpha compositing to have zero signed distance values, alleviating the geometry bias arising from transforming SDF to density for volume rendering. Second, we impose multi-view feature consistency on the surface points, derived by interpolating SDF zero-crossings from sampled points along rays. Extensive quantitative and qualitative results demonstrate that our method reconstructs high-accuracy surfaces with details, and outperforms the state of the art.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Recovering_Fine_Details_for_Neural_Implicit_Surface_Reconstruction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Recovering_Fine_Details_for_Neural_Implicit_Surface_Reconstruction_WACV_2023_paper.pdf,,https://github.com/fraunhoferhhi/D-NeuS,2211.1132,main,Poster,https://ieeexplore.ieee.org/document/10030434/,"['Geometry', 'Surface reconstruction', 'Interpolation', 'Computer vision', 'Three-dimensional displays', 'Reconstruction algorithms', 'Rendering (computer graphics)']","['Fine Details', 'Surface Reconstruction', 'Implicit Surface', 'Implicit Surface Reconstruction', 'Neural Implicit Surfaces', 'Reconstruction Quality', 'Surface Points', 'Neural Network', 'Convolutional Neural Network', 'Input Image', 'Linear Interpolation', 'Point Cloud', 'Multilayer Perceptron', 'Depth Map', '3D Point', 'Object Surface', 'Unit Sphere', 'Surface Geometry', 'Ray Tracing', 'Homography', 'Multi-view Stereo', 'Cost Volume', 'Sum Of Absolute Differences', 'Camera Center', 'View Direction', 'Normalized Cross-correlation', 'Signed Distance Function', 'Feature Maps', 'Computer Vision', 'Surface Normals']",['Algorithms: 3D computer vision'],6,"Recent works on implicit neural representations have made significant strides. Learning implicit neural surfaces using volume rendering has gained popularity in multi-view reconstruction without 3D supervision. However, accurately recovering fine details is still challenging, due to the underlying ambiguity of geometry and appearance representation. In this paper, we present D-NeuS, a volume rendering-base neural implicit surface reconstruction method capable to recover fine geometry details, which extends NeuS by two additional loss functions targeting enhanced reconstruction quality. First, we encourage the rendered surface points from alpha compositing to have zero signed distance values, alleviating the geometry bias arising from transforming SDF to density for volume rendering. Second, we impose multi-view feature consistency on the surface points, derived by interpolating SDF zerocrossings from sampled points along rays. Extensive quantitative and qualitative results demonstrate that our method reconstructs high-accuracy surfaces with details, and outperforms the state of the art. 
<sup>1</sup>"
"Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition","Sofia Broomé, Ernest Pokropek, Boyu Li, Hedvig Kjellström","KTH, Sweden; KTH, Sweden and Silo AI, Sweden",100,Sweden,0,,"Most action recognition models today are highly parameterized, and evaluated on datasets with appearance-wise distinct classes. It has also been shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward texture rather than shape in still image recognition tasks, in contrast to humans. Taken together, this raises suspicion that large video models partly learn spurious spatial texture correlations rather than to track relevant shapes over time to infer generalizable semantics from their movement. A natural way to avoid parameter explosion when learning visual patterns over time is to make use of recurrence. Biological vision consists of abundant recurrent circuitry, and is superior to computer vision in terms of domain shift generalization. In this article, we empirically study whether the choice of low-level temporal modeling has consequences for texture bias and cross-domain robustness. In order to enable a light-weight and systematic assessment of the ability to capture temporal structure, not revealed from single frames, we provide the Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing for the investigation of spatial texture bias in video models. The combined results of our experiments indicate that sound physical inductive bias such as recurrence in temporal modeling may be advantageous when robustness to domain shift is important for the task.",https://openaccess.thecvf.com/content/WACV2023/html/Broome_Recur_Attend_or_Convolve_On_Whether_Temporal_Modeling_Matters_for_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Broome_Recur_Attend_or_Convolve_On_Whether_Temporal_Modeling_Matters_for_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030781/,"['Computer vision', 'Visualization', 'Systematics', 'Shape', 'Tracking', 'Computational modeling', 'Semantics']","['Neural Network', 'Convolutional Neural Network', 'Computer Vision', 'Domain Shift', 'Action Recognition', 'Still Images', 'Temporal Model', 'Spatial Bias', 'Inductive Bias', 'Video Modeling', 'Temporal Shape', 'Time And Space', 'Results Of Experiments', 'Types Of Models', 'Artificial Neural Network', 'Validation Set', 'Layered Structure', 'Data Augmentation', 'Graphics Processing Unit', 'Model Size', 'Recurrent Model', 'Source Domain', 'Qualitative Examples', 'Target Domain', 'Validation Accuracy', 'Domain Adaptation', 'Video Understanding', 'Parameter Count', '3D Convolution', 'Number Of Performance']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",4,"Most action recognition models today are highly parameterized, and evaluated on datasets with appearance-wise distinct classes. It has also been shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward texture rather than shape in still image recognition tasks [19], in contrast to humans. Taken together, this raises suspicion that large video models partly learn spurious spatial texture correlations rather than to track relevant shapes over time to infer generalizable semantics from their movement. A natural way to avoid parameter explosion when learning visual patterns over time is to make use of recurrence. Biological vision consists of abundant recurrent circuitry, and is superior to computer vision in terms of domain shift generalization. In this article, we empirically study whether the choice of low-level temporal modeling has consequences for texture bias and cross-domain robustness. In order to enable a light-weight and systematic assessment of the ability to capture temporal structure, not revealed from single frames, we provide the Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing for the investigation of spatial texture bias in video models. The combined results of our experiments indicate that sound physical inductive bias such as recurrence in temporal modeling may be advantageous when robustness to domain shift is important for the task."
Reducing Annotation Effort by Identifying and Labeling Contextually Diverse Classes for Semantic Segmentation Under Domain Shift,"Sharat Agarwal, Saket Anand, Chetan Arora","IIIT Delhi, India; Indian Institute of Technology Delhi, India",100,India,0,,"In Active Domain Adaptation (ADA), one uses Active Learning (AL) to select target domain frames to annotate for Domain Adaptation (DA). Thus, ADA creates a continuum of cost-performance trade-off models, with unsupervised, and fully supervised DA techniques at the two ends. We observe that in ADA not all regions of a selected frame contribute equally to a model's performance, and there is a strong correlation between annotating certain hard/unique/novel object/stuff instances, and a model's performance. E.g., road regions in a target dataset may look mostly similar to source domain except for certain curved instances, where annotation may be more useful. Based on the observation, we propose Anchor-based and Augmentation-based ADA techniques, which, given a selected frame, determine certain 'hard' semantic regions to be annotated in that frame, such that the selected regions are complementary and diverse in the context of the current labeled set. The proposed techniques carefully avoid the pitfall of region based AL techniques which try to choose most uncertain regions in a frame, but ends up selecting all edge pixels, and similar annotation cost as the whole frame. We show that our approach achieves 66.6 \miou on \gta->\cityscapes dataset with a budget of 4.7% in comparison to 64.9 \miou by MADA [??]. Our technique can also be used as a decorator for any existing frame-based AL technique. E.g., we report 1.5% performance improvement for CDAL [??] on \cityscapes using our approach.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Reducing_Annotation_Effort_by_Identifying_and_Labeling_Contextually_Diverse_Classes_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Reducing_Annotation_Effort_by_Identifying_and_Labeling_Contextually_Diverse_Classes_WACV_2023_paper.pdf,,,2210.06749,main,Poster,https://ieeexplore.ieee.org/document/10030486/,"['Training', 'Computer vision', 'Costs', 'Uncertainty', 'Annotations', 'Semantic segmentation', 'Measurement uncertainty']","['Domain Shift', 'Semantic Segmentation', 'Active Learning', 'Performance Gap', 'Target Domain', 'Domain Adaptation', 'Object Boundaries', 'Domain Adaptation Techniques', 'Deep Neural Network', 'Feature Space', 'Random Selection', 'Object Detection', 'Generative Adversarial Networks', 'Segmentation Model', 'Diverse Contexts', 'Representative Class', 'Semi-supervised Learning', 'Training Objective', 'Source Domain', 'Target Dataset', 'Frame Selection', 'Unlabeled Target Domain', 'Classification Confusion', 'Pseudo Labels', 'Active Learning Strategies', 'Source Dataset', 'Active Learning Approach', 'Pixel-level Labels', 'Class Instances']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Commercial/retail', 'Visualization']",2,"In Active Domain Adaptation (ADA), one uses Active Learning (AL) to select a subset of images from the target domain, which are then annotated and used for supervised domain adaptation (DA). Given the large performance gap between supervised and unsupervised DA techniques, ADA allows for an excellent trade-off between annotation cost and performance. Prior art makes use of measures of uncertainty or disagreement of models to identify ‘regions' to be annotated by the human oracle. However, these regions frequently comprise of pixels at object boundaries which are hard and tedious to annotate. Hence, even if the fraction of image pixels annotated reduces, the overall annotation time and the resulting cost still remain high. In this work, we propose an ADA strategy, which given a frame, identifies a set of classes that are hardest for the model to predict accurately, thereby recommending semantically meaningful regions to be annotated in a selected frame. We show that these set of ‘hard' classes are context-dependent and typically vary across frames, and when annotated help the model generalize better. We propose two ADA techniques: the Anchor-based and Augmentation-based approaches to select complementary and diverse regions in the context of the current training set. Our approach achieves 66.6 mIoU on GTA5 →Cityscapes dataset with an annotation budget of 4.7% in comparison to 64.9 mIoU by MADA [22] using 5% of annotations. Our technique can also be used as a decorator for any existing frame-based AL technique, e.g., we report 1.5% performance improvement for CDAL [1] on Cityscapes using our approach."
Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions,"David Brüggemann, Christos Sakaridis, Prune Truong, Luc Van Gool","ETH Zurich, Switzerland",100,Switzerland,0,,"Due to the scarcity of dense pixel-level semantic annotations for images recorded in adverse visual conditions, there has been a keen interest in unsupervised domain adaptation (UDA) for the semantic segmentation of such images. UDA adapts models trained on normal conditions to the target adverse-condition domains. Meanwhile, multiple datasets with driving scenes provide corresponding images of the same scenes across multiple conditions, which can serve as a form of weak supervision for domain adaptation. We propose Refign, a generic extension to self-training-based UDA methods which leverages these cross-domain correspondences. Refign consists of two steps: (1) aligning the normal-condition image to the corresponding adverse-condition image using an uncertainty-aware dense matching network, and (2) refining the adverse prediction with the normal prediction using an adaptive label correction mechanism. We design custom modules to streamline both steps and set the new state of the art for domain-adaptive semantic segmentation on several adverse-condition benchmarks, including ACDC and Dark Zurich. The approach introduces no extra training parameters, minimal computational overhead--during training only--and can be used as a drop-in extension to improve any given self-training-based UDA method. Code is available at https://github.com/brdav/refign.",https://openaccess.thecvf.com/content/WACV2023/html/Bruggemann_Refign_Align_and_Refine_for_Adaptation_of_Semantic_Segmentation_to_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bruggemann_Refign_Align_and_Refine_for_Adaptation_of_Semantic_Segmentation_to_WACV_2023_paper.pdf,,https://github.com/brdav/refign,,main,Poster,https://ieeexplore.ieee.org/document/10030987/,"['Training', 'Visualization', 'Computer vision', 'Uncertainty', 'Semantic segmentation', 'Semantics', 'Refining']","['Adverse Conditions', 'Semantic Segmentation', 'State Of The Art', 'Computational Overhead', 'Domain Adaptation', 'Correct Label', 'Matching Network', 'Unsupervised Domain Adaptation Methods', 'Low Confidence', 'Target Prediction', 'Large Class', 'Target Image', 'Reference Image', 'Target Domain', 'Self-driving', 'Source Domain', 'Convex Combination', 'Schwarz Inequality', 'Dynamic Objects', 'Structure From Motion', 'Alignment Module', 'Adaptive Refinement', 'Noisy Labels', 'Refinement Strategy', 'Network Alignment', 'Confidence Map', 'Intermediate Domain', 'Homography', 'Uncertainty Estimation', 'Confirmation Bias']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Robotics']",25,"Due to the scarcity of dense pixel-level semantic annotations for images recorded in adverse visual conditions, there has been a keen interest in unsupervised domain adaptation (UDA) for the semantic segmentation of such images. UDA adapts models trained on normal conditions to the target adverse-condition domains. Meanwhile, multiple datasets with driving scenes provide corresponding images of the same scenes across multiple conditions, which can serve as a form of weak supervision for domain adaptation. We propose Refign, a generic extension to self-training-based UDA methods which leverages these cross-domain correspondences. Refign consists of two steps: (1) aligning the normal-condition image to the corresponding adverse-condition image using an uncertainty-aware dense matching network, and (2) refining the adverse prediction with the normal prediction using an adaptive label correction mechanism. We design custom modules to streamline both steps and set the new state of the art for domain-adaptive semantic segmentation on several adverse-condition benchmarks, including ACDC and Dark Zurich. The approach introduces no extra training parameters, minimal computational overhead—during training only—and can be used as a drop-in extension to improve any given self-training-based UDA method. Code is available at https://github.com/brdav/refign."
Relation Preserving Triplet Mining for Stabilising the Triplet Loss In re-Identification Systems,"Adhiraj Ghosh, Kuruparan Shanmugalingam, Wen-Yan Lin","Singapore Management University; Singapore Management University, University of Tübingen; Singapore Management University, University of New South Wales",100,"Germany, Singapore",0,,"Object appearances change dramatically with pose variations. This creates a challenge for embedding schemes that seek to map instances with the same object ID to locations that are as close as possible. This issue becomes significantly heightened in complex computer vision tasks such as re-identification(reID). In this paper, we suggest that these dramatic appearance changes are indications that an object ID is composed of multiple natural groups, and it is counterproductive to forcefully map instances from different groups to a common location. This leads us to introduce Relation Preserving Triplet Mining (RPTM), a feature matching guided triplet mining scheme, that ensures that triplets will respect the natural subgroupings within an object ID. We use this triplet mining mechanism to establish a pose-aware, well-conditioned triplet loss by implicitly enforcing view consistency. This allows a single network to be trained with fixed parameters across datasets while providing state-of-the-art results. Code is available at https: //github.com/adhirajghosh/RPTM_reid.",https://openaccess.thecvf.com/content/WACV2023/html/Ghosh_Relation_Preserving_Triplet_Mining_for_Stabilising_the_Triplet_Loss_In_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ghosh_Relation_Preserving_Triplet_Mining_for_Stabilising_the_Triplet_Loss_In_WACV_2023_paper.pdf,,https://github.com/adhirajghosh/RPTM_reid,2110.07933,main,Poster,https://ieeexplore.ieee.org/document/10031008/,"['Training', 'Computer vision', 'Codes', 'Pipelines', 'Image retrieval', 'Feature extraction', 'Stability analysis']","['Triplet Loss', 'Re-identification System', 'Triplet Mining', 'Computer Vision', 'Changes In Appearance', 'Feature Matching', 'Object Appearance', 'Image Size', 'Stochastic Gradient Descent', 'Image Pairs', 'Trainable Parameters', 'Pose Estimation', 'Image Retrieval', 'Metric Learning', 'Retrieval Results', 'Number Of Training Images', 'Vision Transformer']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"Object appearances change dramatically with pose variations. This creates a challenge for embedding schemes that seek to map instances with the same object ID to locations that are as close as possible. This issue becomes significantly heightened in complex computer vision tasks such as re-identification(reID). In this paper, we suggest that these dramatic appearance changes are indications that an object ID is composed of multiple natural groups, and it is counterproductive to forcefully map instances from different groups to a common location. This leads us to introduce Relation Preserving Triplet Mining (RPTM), a feature matching guided triplet mining scheme, that ensures that triplets will respect the natural subgroupings within an object ID. We use this triplet mining mechanism to establish a pose-aware, well-conditioned triplet loss by implicitly enforcing view consistency. This allows a single network to be trained with fixed parameters across datasets, while providing state-of-the-art results. Code is available at https://github.com/adhirajghosh/RPTM_reid."
Relaxing Contrastiveness in Multimodal Representation Learning,"Zudi Lin, Erhan Bas, Kunwar Yashraj Singh, Gurumurthy Swaminathan, Rahul Bhotika",Amazon Alexa Science; Optum Labs; Scale AI; AWS AI Labs,25,USA,75,USA,"Multimodal representation learning for images with paired raw texts can improve the usability and generality of the learned semantic concepts while significantly reducing annotation costs. In this paper, we explore the design space of loss functions in visual-linguistic pretraining frameworks and propose a novel Relaxed Contrastive (ReCo) objective, which acts as a drop-in replacement of the widely used InfoNCE loss. The key insight of ReCo is to allow a relaxed negative space by not penalizing unpaired multimodal samples (ie, negative pairs) that are already orthogonal or negatively correlated. Unlike the widely-used InfoNCE, which keeps repelling negative pairs as long as they are not anti-correlated, ReCo by design embraces more diversity and flexibility of the learned embeddings. We conduct extensive experiments using ReCo with state-of-the-art models by pretraining on the MIMIC-CXR dataset that consists of chest radiographs and free-text radiology reports, and evaluating on the CheXpert dataset for multimodal retrieval and disease classification. Our ReCo achieves an absolute improvement of 2.9% over the InfoNCE baseline on the CheXpert Retrieval dataset in average retrieval precision and reports better or comparable performance in the linear evaluation and finetuning for classification. We further show that ReCo outperforms InfoNCE on the Flickr30K dataset by 1.7% in retrieval Recall@1, demonstrating the generalizability of our approach to natural images.",https://openaccess.thecvf.com/content/WACV2023/html/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lin_Relaxing_Contrastiveness_in_Multimodal_Representation_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030328/,"['Representation learning', 'Radiography', 'Computer vision', 'Costs', 'Semantics', 'MIMICs', 'Radiology']","['Representation Learning', 'Multimodal Learning', 'Multimodal Representation Learning', 'Chest X-ray', 'Average Precision', 'Embedding Learning', 'Unpaired Samples', 'Paired Samples', 'Batch Size', 'Visual Representation', 'Transfer Learning', 'Similarity Matrix', 'Latent Space', 'Training Protocol', 'Embedding Dimension', 'Negative Terms', 'Image Retrieval', 'Self-supervised Learning', 'Visual Learning', 'Masked Images', 'Text Retrieval', 'Unsupervised Representation Learning', 'Retrieval Performance', 'Text Encoder', 'Hypersphere', 'Contrastive Loss', 'Embedding Matrix', 'Energy Function', 'Similar Distribution']","['Applications: Biomedical/healthcare/medicine', 'Vision + language and/or other modalities']",4,"Multimodal representation learning for images with paired raw texts can improve the usability and generality of the learned semantic concepts while significantly reducing annotation costs. In this paper, we explore the design space of loss functions in visual-linguistic pretraining frameworks and propose a novel Relaxed Contrastive (ReCo) objective, which act as a drop-in replacement of the widely used InfoNCE loss. The key insight of ReCo is to allow a relaxed negative space by not penalizing unpaired multimodal samples (i.e., negative pairs) that are already orthogonal or negatively correlated. Unlike the widely-used InfoNCE, which keeps repelling negative pairs as long as they are not anti-correlated, ReCo by design embraces more diversity and flexibility of the learned embeddings. We conduct exten-sive experiments using ReCo with state-of-the-art models by pretraining on the MIMIC-CXR dataset that consists of chest radiographs and free-text radiology reports, and eval-uating on the CheXpert dataset for multimodal retrieval and disease classification. Our ReCo achieves an absolute improvement of 2.9% over the InfoNCE baseline on the CheXpert Retrieval dataset in average retrieval precision and re-ports better or comparable performance in the linear evaluation and finetuning for classification. We further show that ReCo outperforms InfoNCE on the Flickr30K dataset by 1.7% in retrieval Recall@1, demonstrating the generalizability of our approach to natural images."
Representation Disentanglement in Generative Models With Contrastive Learning,"Shentong Mo, Zhun Sun, Chao Li","Center for Advanced Intelligence Project (AIP), RIKEN; Carnegie Mellon University; Tohoku University",100,"Japan, USA",0,,"Contrastive learning has shown its effectiveness in image classification and generation. Recent works apply the contrastive learning on the discriminator of the Generative Adversarial Networks, and there exists little work on exploring if contrastive learning can be applied on encoders to learn disentangled representations. In this work, we propose a simple yet effective method via incorporating contrastive learning into latent optimization, where we name it. Specifically, we first use a generator to learn discriminative and disentangled embeddings via latent optimization. Then an encoder and two momentum encoders are applied to dynamically learn disentangled information across large amount of samples with content-level and residual-level contrastive loss. In the meanwhile, we tune the encoder with the learned embeddings in an amortized manner. We evaluate our approach on ten benchmarks in terms of representation disentanglement and linear classification. Extensive experiments demonstrate the effectiveness of our ContraLORD on learning both discriminative and generative representations.",https://openaccess.thecvf.com/content/WACV2023/html/Mo_Representation_Disentanglement_in_Generative_Models_With_Contrastive_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mo_Representation_Disentanglement_in_Generative_Models_With_Contrastive_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030844/,"['Computer vision', 'Scalability', 'Training data', 'Benchmark testing', 'Generative adversarial networks', 'Generators', 'Task analysis']","['Self-supervised Learning', 'Disentangled Representation', 'Generative Adversarial Networks', 'Linear Classifier', 'Contrastive Loss', 'Embedding Learning', 'Batch Size', 'Negative Samples', 'Unsupervised Learning', 'Data Augmentation', 'Mutual Information', 'Representation Learning', 'Training Examples', 'Variational Autoencoder', 'Problem Setup', 'Gradient Loss', 'Discriminative Representations', 'Query Sample', 'Large Batch Size', 'Pre-training Stage', 'Parameters Of The Encoder']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Contrastive learning has shown its effectiveness in image classification and generation. Recent works apply contrastive learning to the discriminator of the Generative Adversarial Networks. However, there is little work exploring if contrastive learning can be applied to the encoderdecoder structure to learn disentangled representations. In this work, we propose a simple yet effective method via incorporating contrastive learning into latent optimization, where we name it ContraLORD. Specifically, we first use a generator to learn discriminative and disentangled embeddings via latent optimization. Then an encoder and two momentum encoders are applied to dynamically learn disentangled information across a large number of samples with content-level and residual-level contrastive loss. In the meanwhile, we tune the encoder with the learned embeddings in an amortized manner. We evaluate our approach on ten benchmarks regarding representation disentanglement and linear classification. Extensive experiments demonstrate the effectiveness of our ContraLORD on learning both discriminative and generative representations."
Representation Recovering for Self-Supervised Pre-Training on Medical Images,"Xiangyi Yan, Junayed Naushad, Shanlin Sun, Kun Han, Hao Tang, Deying Kong, Haoyu Ma, Chenyu You, Xiaohui Xie","University of California, Irvine; Yale University",100,USA,0,,"Advances in self-supervised learning, especially in contrastive learning, have drawn attention to investigating these techniques in providing effective visual representations from unlabeled images. It enables the models' ability of extracting highly consistent features by generating different views. Due to the recent success of Masked Autoencoders (MAE), an emerging trend of exploring generative modeling in self-supervised learning has come back into sight of the community. The generative approaches encode the input into a compact embedding and empower the models' ability of recovering the original input. However, in our experiments, we found vanilla MAE mainly recovers course high level semantic information and barely recovers detailed low level information. We show that in dense downstream prediction tasks like multi-organ segmentation, directly applying MAE is not ideal. In this paper, we propose RepRec, a hybrid visual representation learning framework for self-supervised pre-training on large-scale unlabelled medical datasets, which takes advantage of both contrastive and generative modeling. In our method, to solve the aforementioned dilemma that MAE encounters, a convolutional encoder is pre-trained to provide low-level feature information, in a contrastive way; and a transformer encoder is pre-trained to produce high level semantic dependency, in a generative way -- by recovering masked representations from the convolutional encoder. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Yan_Representation_Recovering_for_Self-Supervised_Pre-Training_on_Medical_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yan_Representation_Recovering_for_Self-Supervised_Pre-Training_on_Medical_Images_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030120/,"['Representation learning', 'Visualization', 'Image segmentation', 'Semantics', 'Self-supervised learning', 'Feature extraction', 'Transformers']","['Self-supervised Pretraining', 'Representation Learning', 'Self-supervised Learning', 'Visual Learning', 'Transformer Encoder', 'High-level Semantic Information', 'Visual Framework', 'Convolutional Encoder', 'Compact Embedding', 'Computed Tomography', 'Feature Maps', 'Segmentation Task', 'Skip Connections', 'Marginal Utility', 'Dice Similarity Coefficient', 'Random Initialization', 'Submandibular Glands', 'Contrastive Loss', 'Object Detection Task', 'Histogram Of Oriented Gradients', 'Fine-tuning Stage', 'Pre-training Stage', 'Discriminative Approach', 'Organ Segmentation', 'Pretext Task', 'Raw Pixel', 'Memory Bank', 'Pre-training Tasks', 'Argument In Section', 'Negative Samples']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"Advances in self-supervised learning have drawn attention to developing techniques to extract effective visual representations from unlabeled images. Contrastive learning (CL) trains a model to extract consistent features by generating different views. Recent success of Masked Autoencoders (MAE) highlights the benefit of generative modeling in self-supervised learning. The generative approaches encode the input into a compact embedding and empower the model’s ability of recovering the original input. However, in our experiments, we found vanilla MAE mainly recovers coarse high level semantic information and is inadequate in recovering detailed low level information. We show that in dense downstream prediction tasks like multi-organ segmentation, directly applying MAE is not ideal. Here, we propose RepRec, a hybrid visual representation learning framework for self-supervised pre-training on large-scale unlabelled medical datasets, which takes advantage of both contrastive and generative modeling. To solve the aforementioned dilemma that MAE encounters, a convolutional encoder is pre-trained to provide low-level feature information, in a contrastive way; and a transformer encoder is pre-trained to produce high level semantic dependency, in a generative way – by recovering masked representations from the convolutional encoder. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods."
Resolving Class Imbalance for LiDAR-Based Object Detector by Dynamic Weight Average and Contextual Ground Truth Sampling,"Daeun Lee, Jinkyu Kim","Statistics, Korea University, Seoul 02841, Korea; Computer Science and Engineering, Korea University, Seoul 02841, Korea",100,South Korea,0,,"An autonomous driving system requires a 3D object detector, which must perceive all present road agents reliably to navigate an environment safely. However, real-world driving datasets often suffer from the problem of data imbalance, which causes difficulties in training a model that works well across all classes, resulting in an undesired imbalanced sub-optimal performance. In this work, we propose a method to address this data imbalance problem. Our method consists of two main components: (i) a LiDAR-based 3D object detector with per-class multiple detection heads where losses from each head are modified by dynamic weight average to be balanced. (ii) Contextual ground truth (GT) sampling, where we improve conventional GT sampling techniques by leveraging semantic information to augment point cloud with sampled ground truth GT objects. Our experiment with KITTI and nuScenes datasets confirms our proposed method's effectiveness in dealing with the data imbalance problem, producing better detection accuracy compared to existing approaches. Our implementation will be publicly available upon publication.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Resolving_Class_Imbalance_for_LiDAR-Based_Object_Detector_by_Dynamic_Weight_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Resolving_Class_Imbalance_for_LiDAR-Based_Object_Detector_by_Dynamic_Weight_WACV_2023_paper.pdf,,,2210.03331,main,Poster,https://ieeexplore.ieee.org/document/10030909/,"['Weight measurement', 'Training', 'Three-dimensional displays', 'Roads', 'Semantics', 'Detectors', 'Solids']","['Object Detection', 'Ground Truth Samples', 'Dynamic Weight Average', 'Detection Accuracy', 'Point Cloud', 'Semantic Information', 'Imbalanced Data', 'Imbalance Problem', 'Conventional Sampling', 'KITTI Dataset', '3D Detection', 'Detection Head', 'Data Imbalance Problem', 'Multiple Heads', 'Ground Truth Object', '3D Object Detection', 'Detection Performance', 'Pedestrian', 'Data Augmentation', 'Large-scale Datasets', 'Rare Classes', 'Ground Truth Points', 'Multi-task Learning', 'Minority Class', 'LiDAR Point', 'Bounding Box', 'LiDAR Point Clouds', 'Balancing Techniques', 'Data Augmentation Techniques', 'Landmark Work']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Robotics']",3,"An autonomous driving system requires a 3D object detector, which must perceive all present road agents reliably to navigate an environment safely. However, real-world driving datasets often suffer from the problem of data imbalance, which causes difficulties in training a model that works well across all classes, resulting in an undesired imbalanced sub-optimal performance. In this work, we propose a method to address this data imbalance problem. Our method consists of two main components: (i) a LiDAR-based 3D object detector with per-class multiple detection heads where losses from each head are modified by dynamic weight average to be balanced. (ii) Contextual ground truth (GT) sampling, where we improve conventional GT sampling techniques by leveraging semantic information to augment point cloud with sampled ground truth GT objects. Our experiment with KITTI and nuScenes datasets confirms our proposed method’s effectiveness in dealing with the data imbalance problem, producing better detection accuracy compared to existing approaches."
Rethinking Rotation in Self-Supervised Contrastive Learning: Adaptive Positive or Negative Data Augmentation,"Atsuyuki Miyai, Qing Yu, Daiki Ikami, Go Irie, Kiyoharu Aizawa","NTT Corporation, Japan; The University of Tokyo",50,Japan,50,Japan,"Rotation is frequently listed as a candidate for data augmentation in contrastive learning but seldom provides satisfactory improvements. We argue that this is because the rotated image is always treated as either positive or negative. The semantics of an image can be rotation-invariant or rotation-variant, so whether the rotated image is treated as positive or negative should be determined based on the content of the image. Therefore, we propose a novel augmentation strategy, adaptive Positive or Negative Data Augmentation (PNDA), in which an original and its rotated image are a positive pair if they are semantically close and a negative pair if they are semantically different. To achieve PNDA, we first determine whether rotation is positive or negative on an image-by-image basis in an unsupervised way. Then, we apply PNDA to contrastive learning frameworks. Our experiments showed that PNDA improves the performance of contrastive learning.",https://openaccess.thecvf.com/content/WACV2023/html/Miyai_Rethinking_Rotation_in_Self-Supervised_Contrastive_Learning_Adaptive_Positive_or_Negative_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Miyai_Rethinking_Rotation_in_Self-Supervised_Contrastive_Learning_Adaptive_Positive_or_Negative_WACV_2023_paper.pdf,,https://github.com/AtsuMiyai/rethinking_rotation,2210.12681,main,Poster,https://ieeexplore.ieee.org/document/10030915/,"['Computer vision', 'Codes', 'Semantics', 'Task analysis']","['Data Augmentation', 'Self-supervised Learning', 'Negative Data Augmentation', 'Learning Performance', 'Augmentation Strategy', 'Image Rotation', 'Unsupervised Way', 'Semantic Image', 'Training Data', 'Positive Samples', 'Convolutional Layers', 'Supervised Learning', 'Negative Samples', 'Detailed Explanation', 'Stochastic Gradient Descent', 'Representation Learning', 'Vanilla', 'Rotation Invariance', 'Image X', 'Sampling Framework', 'Difficulty Of Prediction', 'Differences In Difficulty', 'Rotation Characteristics']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Rotation is frequently listed as a candidate for data augmentation in contrastive learning but seldom provides satisfactory improvements. We argue that this is because the rotated image is always treated as either positive or negative. The semantics of an image can be rotation-invariant or rotation-variant, so whether the rotated image is treated as positive or negative should be determined based on the content of the image. Therefore, we propose a novel augmentation strategy, adaptive Positive or Negative Data Augmentation (PNDA), in which an original and its rotated image are a positive pair if they are semantically close and a negative pair if they are semantically different. To achieve PNDA, we first determine whether rotation is positive or negative on an image-by-image basis in an unsupervised way. Then, we apply PNDA to contrastive learning frameworks. Our experiments showed that PNDA improves the performance of contrastive learning. The code is available at https://github.com/AtsuMiyai/rethinking_rotation."
Rethinking the Data Annotation Process for Multi-View 3D Pose Estimation With Active Learning and Self-Training,"Qi Feng, Kun He, He Wen, Cem Keskin, Yuting Ye",Meta Reality Labs,0,,100,USA,"Pose estimation of the human body and hands is a fundamental problem in computer vision, and learning-based solutions require a large amount of annotated data. In this work, we improve the efficiency of the data annotation process for 3D pose estimation problems with Active Learning (AL) in a multi-view setting. AL selects examples with the highest value to annotate under limited annotation budgets (time and cost), but choosing the selection strategy is often nontrivial. We present a framework to efficiently extend existing single-view AL strategies. We then propose two novel AL strategies that make full use of multi-view geometry. Moreover, we demonstrate additional performance gains by incorporating pseudo-labels computed during the AL process, which is a form of self-training. Our system significantly outperforms simulated annotation baselines in 3D body and hand pose estimation on two large-scale benchmarks: CMU Panoptic Studio and InterHand2.6M. Notably, on CMU Panoptic Studio, we are able to reduce the turn-around time by 60% and annotation cost by 80% when compared to the conventional annotation process.",https://openaccess.thecvf.com/content/WACV2023/html/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.pdf,,,,main,Poster,,,,,,
Revisiting Training-Free NAS Metrics: An Efficient Training-Based Method,"Taojiannan Yang, Linjie Yang, Xiaojie Jin, Chen Chen","Bytedance Inc.; Center for Research in Computer Vision, University of Central Florida",50,USA,50,China,"Recent neural architecture search (NAS) works proposed training-free metrics to rank networks which largely reduced the search cost in NAS. In this paper, we revisit these training-free metrics and find that: (1) the number of parameters (#Param), which is the most straightforward training-free metric, is overlooked in previous works but is surprisingly effective, (2) recent training-free metrics largely rely on the #Param information to rank networks. Our experiments show that the performance of recent training-free metrics drops dramatically when the #Param information is not available. Motivated by these observations, we argue that metrics less correlated with the #Param are desired to provide additional information for NAS. We propose a light-weight training-based metric which has a weak correlation with the #Param while achieving better performance than training-free metrics at a lower search cost. Specifically, on DARTS search space, our method completes searching directly on ImageNet in only 2.6 GPU hours and achieves a top-1/top-5 error rate of 24.1%/7.1%, which is competitive among state-of-the-art NAS methods.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Revisiting_Training-Free_NAS_Metrics_An_Efficient_Training-Based_Method_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Revisiting_Training-Free_NAS_Metrics_An_Efficient_Training-Based_Method_WACV_2023_paper.pdf,,,2211.08666,main,Poster,https://ieeexplore.ieee.org/document/10030994/,"['Measurement', 'Computer vision', 'Costs', 'Systematics', 'Correlation', 'Error analysis', 'Graphics processing units']","['Neural Architecture Search', 'Weak Correlation', 'Performance Metrics', 'Search Space', 'Search Costs', 'Deep Neural Network', 'Highly Correlated', 'Training Loss', 'Training Iterations', 'Random Search', 'Target Dataset', 'Architecture Parameters', 'Feature Extraction Layer', 'Dynamic Training', 'Effective Metrics', 'Good Metric', 'Candidate Network']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Recent neural architecture search (NAS) works proposed training-free metrics to rank networks which largely reduced the search cost in NAS. In this paper, we revisit these training-free metrics and find that: (1) the number of parameters (#Param), which is the most straightforward training-free metric, is overlooked in previous works but is surprisingly effective, (2) recent training-free metrics largely rely on the #Param information to rank networks. Our experiments show that the performance of recent training-free metrics drops dramatically when the #Param information is not available. Motivated by these observations, we argue that metrics less correlated with the #Param are desired to provide additional information for NAS. We propose a light-weight training-based metric which has a weak correlation with the #Param while achieving better performance than training-free metrics at a lower search cost. Specifically, on DARTS search space, our method completes searching directly on ImageNet in only 2.6 GPU hours and achieves a top-1/top-5 error rate of 24.1%/7.1%, which is competitive among state-of-the-art NAS methods."
Robust Real-World Image Enhancement Based on Multi-Exposure LDR Images,"Haoyu Ren, Yi Fan, Stephen Huang",Oppo Mobile Telecommunications Corp.,0,,100,China,"Robust real-world image enhancement from multi-exposure low dynamic range (LDR) images is a challenging task due to the unexpected inconsistency among the input images, such as the large motion or various exposures. In this paper, we propose a novel end-to-end image enhancement network to solve this problem. After extracting contextual information from the LDR images, we design a novel matching volume to align them by considering the motion and exposure differences among the input images. A stacked hourglass with dilated convolution is further utilized to aggregate the matched feature maps to the final enhanced image. In addition, we design a weakly-supervised pairwise loss function to evaluate the color consistency in the enhanced image, which further boosts the performance. We show the effectiveness of our methods on high dynamic ranging imaging (HDR) and End-to-End image signal processing (E2E-ISP). Experimental results demonstrate that our model achieves state-of-the-art enhancement performance.",https://openaccess.thecvf.com/content/WACV2023/html/Ren_Robust_Real-World_Image_Enhancement_Based_on_Multi-Exposure_LDR_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ren_Robust_Real-World_Image_Enhancement_Based_on_Multi-Exposure_LDR_Images_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030317/,"['Image color analysis', 'Convolution', 'Volume measurement', 'Imaging', 'Dynamic range', 'Feature extraction', 'Distance measurement']","['Image Enhancement', 'Low Dynamic Range', 'Low Dynamic Range Images', 'Loss Function', 'Dynamic Range', 'Input Image', 'Feature Maps', 'High Dynamic Range', 'Large Motion', 'Dilated Convolution', 'Convolutional Neural Network', 'Convolutional Layers', 'Raw Images', 'Generative Adversarial Networks', 'Reference Image', 'Peak Signal-to-noise Ratio', 'Human Vision', 'Output Feature Map', 'L1 Loss', 'Prior Art', 'High Dynamic Range Image', 'Patch Pairs', 'Mean Opinion Score', 'Cost Volume', 'Motion Vector', 'Optical Flow Estimation', 'Disparity Estimation', '4K Resolution', 'Aggregation Module', 'Dense Block']","['Algorithms: Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Robust real-world image enhancement from multi-exposure low dynamic range (LDR) images is a challenging task due to the unexpected inconsistency among the input images, such as the large motion or various exposures. In this paper, we propose a novel end-to-end image enhancement network to solve this problem. After extracting contextual information from the LDR images, we design a novel matching volume to align them by considering the motion and exposure differences among the input images. A stacked hourglass with dilated convolution is further utilized to aggregate the matched feature maps to the final enhanced image. In addition, we design a weakly-supervised pairwise loss function to evaluate the color consistency in the enhanced image, which further boosts the performance. We show the effectiveness of our methods on high dynamic ranging imaging (HDR) and End-to-End image signal processing (E2E-ISP) tasks. Experimental results demonstrate that our model achieves state-of-the-art enhancement performance."
Robust and Efficient Alignment of Calcium Imaging Data Through Simultaneous Low Rank and Sparse Decomposition,"Junmo Cho, Seungjae Han, Eun-Seo Cho, Kijung Shin, Young-Gyu Yoon","Kim Jaechul Graduate School of AI, KAIST; KAIST Institute for Health Science and Technology; School of Electrical Engineering, KAIST",100,South Korea,0,,"Accurate alignment of calcium imaging data, which is critical for the extraction of neuronal activity signals, is often hindered by the image noise and the neuronal activity itself. To address the problem, we propose an algorithm named REALS for robust and efficient batch image alignment through simultaneous geometric transformation and low rank and sparse decomposition. REALS is constructed upon our finding that the low rank subspace can be recovered via linear projection, which allows us to perform simultaneous image alignment and decomposition with gradient-based updates. REALS achieves orders-of magnitude improvement in terms of accuracy and speed compared to the state-of-the-art robust image alignment algorithms. In addition, we introduce two extended versions of REALS that achieve even higher accuracy than REALS under challenging conditions. First, multi-resolution REALS achieves up to 5 times higher alignment accuracy than REALS. Second, deformable REALS generalizes REALS for non-rigid registration. Furthermore, REALS can be combined with downstream tasks such as unsupervised image segmentation owing to its differentiability.",https://openaccess.thecvf.com/content/WACV2023/html/Cho_Robust_and_Efficient_Alignment_of_Calcium_Imaging_Data_Through_Simultaneous_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cho_Robust_and_Efficient_Alignment_of_Calcium_Imaging_Data_Through_Simultaneous_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030282/,"['Backpropagation', 'Computer vision', 'Calcium', 'Scalability', 'Imaging', 'Transforms', 'Robustness']","['Calcium Imaging', 'Alignment Data', 'Low-rank Decomposition', 'Robust Alignment', 'Rank Decomposition', 'Neuronal Activity', 'Robust Algorithm', 'Image Noise', 'Alignment Algorithm', 'Alignment Accuracy', 'Image Alignment', 'Linear Projection', 'Robust Image', 'Optimization Problem', 'Objective Function', 'Computation Time', 'Hyperparameters', 'Gaussian Noise', 'Input Image', 'Image Pixels', 'Affine Transformation', 'Set Of Transformations', 'Non-negative Matrix Factorization', 'Shot Noise', 'Sparse Matrix', 'Differencing', 'Image Pyramid', 'Activity In Zebrafish', 'Rigid Transformation', 'Target Image']",['Applications: Biomedical/healthcare/medicine'],,"Accurate alignment of calcium imaging data, which is critical for the extraction of neuronal activity signals, is often hindered by the image noise and the neuronal activity itself. To address the problem, we propose an algorithm named REALS for robust and efficient batch image alignment through simultaneous transformation and low rank and sparse decomposition. REALS is constructed upon our finding that the low rank subspace can be recovered via linear projection, which allows us to perform simultaneous image alignment and decomposition with gradient-based updates. REALS achieves orders-of-magnitude improvement in terms of accuracy and speed compared to the state-of-the-art robust image alignment algorithms."
Robustness of Trajectory Prediction Models Under Map-Based Attacks,"Zhihao Zheng, Xiaowen Ying, Zhen Yao, Mooi Choo Chuah",Lehigh University,100,USA,0,,"Trajectory Prediction (TP) is a critical component in the control system of an Autonomous Vehicle (AV). It predicts future motion of traffic agents based on observations of their past trajectories. Existing works have studied the vulnerability of TP models when the perception systems are under attacks and proposed corresponding mitigation schemes. Recent TP designs have incorporated context map information for performance enhancements. Such designs are subjected to a new type of attacks where an attacker can interfere with these TP models by attacking the context maps. In this paper, we study the robustness of TP models under our newly proposed map-based adversarial attacks. We show that such attacks can compromise state-of-the-art TP models that use either image-based or node-based map representation while keeping the adversarial examples imperceptible. We also demonstrate that our attacks can still be launched under the black-box settings without any knowledge of the TP models running underneath. Our experiments on the NuScene dataset show that the proposed map-based attacks can increase the trajectory prediction errors by 29-110%. Finally, we demonstrate that two defense mechanisms are effective in defending against such map-based attacks.",https://openaccess.thecvf.com/content/WACV2023/html/Zheng_Robustness_of_Trajectory_Prediction_Models_Under_Map-Based_Attacks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_Robustness_of_Trajectory_Prediction_Models_Under_Map-Based_Attacks_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030215/,"['Visualization', 'Computer vision', 'Image coding', 'Sensitivity analysis', 'Computational modeling', 'Predictive models', 'Control systems']","['Trajectory Prediction', 'Robust Prediction Model', 'Trajectory Prediction Model', 'Defense Mechanisms', 'Prediction Error', 'Autonomous Vehicles', 'Perceptual System', 'Types Of Attacks', 'Imperceptible', 'Map Information', 'Map Representation', 'Adversarial Attacks', 'Adversarial Examples', 'Convolutional Neural Network', 'Object Detection', 'Point Cloud', 'Binary Image', 'Local Map', 'Image Representation', 'Submodule', 'Black-box Attacks', 'White-box Attack', 'Trajectories Of Agents', 'Adversarial Perturbations', 'Impact Of Attacks', 'Future Trajectories', 'Original Map', 'Binary Map', 'Minor Perturbations', 'White Box']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods']",7,"Trajectory Prediction (TP) is a critical component in the control system of an Autonomous Vehicle (AV). It predicts future motion of traffic agents based on observations of their past trajectories. Existing works have studied the vulnerability of TP models when the perception systems are under attacks and proposed corresponding mitigation schemes. Recent TP designs have incorporated context map information for performance enhancements. Such designs are subjected to a new type of attacks where an attacker can interfere with these TP models by attacking the context maps. In this paper, we study the robustness of TP models under our newly proposed map-based adversarial attacks. We show that such attacks can compromise state-of-the-art TP models that use either image-based or node-based map representation while keeping the adversarial examples imperceptible. We also demonstrate that our attacks can still be launched under the black-box settings without any knowledge of the TP models running underneath. Our experiments on the NuScene dataset show that the proposed map-based attacks can increase the trajectory prediction errors by 29-110%. Finally, we demonstrate that two defense mechanisms are effective in defending against such map-based attacks."
SAILOR: Scaling Anchors via Insights Into Latent Object Representation,"Dušan Malić, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof","Institute of Computer Graphics and Vision, Graz University of Technology; Institute of Computer Graphics and Vision, Graz University of Technology; Christian Doppler Laboratory for Embedded Machine Learning",100,Austria,0,,"LiDAR 3D object detection models are inevitably biased towards their training dataset. The detector clearly exhibits this bias when employed on a target dataset, particularly towards object sizes. However, object sizes vary heavily between domains due to, for instance, different labeling policies or geographical locations. State-of-the-art unsupervised domain adaptation approaches outsource methods to overcome the object size bias. Mainstream size adaptation approaches exploit target domain statistics, contradicting the original unsupervised assumption. Our novel unsupervised anchor calibration method addresses this limitation. Given a model trained on the source data, we estimate the optimal target anchors in a completely unsupervised manner. The main idea stems from an intuitive observation: by varying the anchor sizes for the target domain, we inevitably introduce noise or even remove valuable object cues. The latent object representation, perturbed by the anchor size, is closest to the learned source features only under the optimal target anchors. We leverage this observation for anchor size optimization. Our experimental results show that, without any retraining, we achieve competitive results even compared to state-of-the-art weakly-supervised size adaptation approaches. In addition, our anchor calibration can be combined with such existing methods, making them completely unsupervised.",https://openaccess.thecvf.com/content/WACV2023/html/Malic_SAILOR_Scaling_Anchors_via_Insights_Into_Latent_Object_Representation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Malic_SAILOR_Scaling_Anchors_via_Insights_Into_Latent_Object_Representation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030785/,"['Training', 'Adaptation models', 'Solid modeling', 'Three-dimensional displays', 'Laser radar', 'Object detection', 'Detectors']","['Latent Representation', 'Data Sources', 'Unsupervised Learning', 'Object Detection', 'Object Size', 'Target Domain', 'Source Characteristics', 'Domain Adaptation', 'Target Dataset', '3D Object Detection', 'Labelling Policy', 'Pedestrian', 'Intersection Over Union', 'Point Cloud', 'Bounding Box', 'Gaussian Mixture Model', 'Target Data', 'Source Model', 'Vector Populations', 'Joint Optimization', 'Source Domain', 'Source Dataset', 'Region Proposal Network', 'Feature Database', 'Domain Gap', 'Candidate Solutions', 'Pseudo Labels', '3D Detection', 'Multi-object Tracking', 'Detection Head']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"LiDAR 3D object detection models are inevitably biased towards their training dataset. The detector clearly exhibits this bias when employed on a target dataset, particularly towards object sizes. However, object sizes vary heavily between domains due to, for instance, different labeling policies or geographical locations. State-of-the-art unsupervised domain adaptation approaches outsource methods to overcome the object size bias. Mainstream size adaptation approaches exploit target domain statistics, contradicting the original unsupervised assumption. Our novel unsupervised anchor calibration method addresses this limitation. Given a model trained on the source data, we estimate the optimal target anchors in a completely unsupervised manner. The main idea stems from an intuitive observation: by varying the anchor sizes for the target domain, we inevitably introduce noise or even remove valuable object cues. The latent object representation, perturbed by the anchor size, is closest to the learned source features only under the optimal target anchors. We leverage this observation for anchor size optimization. Our experimental results show that, without any retraining, we achieve competitive results even compared to state-of-the-art weakly-supervised size adaptation approaches. In addition, our anchor calibration can be combined with such existing methods, making them completely unsupervised."
"SALAD: Source-Free Active Label-Agnostic Domain Adaptation for Classification, Segmentation and Detection","Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, Dinesh Manocha",University of Maryland College Park; Carnegie Mellon University; Adobe Research,66.66666667,USA,33.33333333,USA,"We present a novel method, SALAD, for the challenging vision task of adapting a pre-trained ""source"" domain network to a ""target"" domain, with a small budget for annotation in the ""target"" domain and a shift in the label space. Further, the task assumes that the source data is not available for adaptation, due to privacy concerns or otherwise. We postulate that such systems need to jointly optimize the dual task of (i) selecting fixed number of samples from the target domain for annotation and (ii) transfer of knowledge from the pre-trained network to the target domain. To do this, SALAD consists of a novel Guided Attention Transfer Network (GATN) and an active learning function, HAL. The GATN enables feature distillation from pre-trained network to the target network, complemented with the target samples mined by HAL using transfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it is task-agnostic, and can be applied across various visual tasks such as classification, segmentation and detection; (ii) it can handle shifts in output label space from the pre-trained source network to the target domain; (iii) it does not require access to source data for adaptation. We conduct extensive experiments across 3 visual tasks, viz. digits classification (MNIST, SVHN, VISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document layout detection (PubLayNet to DSSE). We show that our source-free approach, SALAD, results in an improvement of 0.5%-31.3% (across datasets and tasks) over prior adaptation methods that assume access to large amounts of annotated source data for adaptation.",https://openaccess.thecvf.com/content/WACV2023/html/Kothandaraman_SALAD_Source-Free_Active_Label-Agnostic_Domain_Adaptation_for_Classification_Segmentation_and_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kothandaraman_SALAD_Source-Free_Active_Label-Agnostic_Domain_Adaptation_for_Classification_Segmentation_and_WACV_2023_paper.pdf,,Available here,2205.1284,main,Poster,,,,,,
SAT: Scale-Augmented Transformer for Person Search,"Mustansar Fiaz, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan","Department of computer Vision, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE.",100,UAE,0,,"Person search is a challenging computer vision problem where the objective is to simultaneously detect and reidentify a target person from the gallery of whole scene images captured from multiple cameras. Here, the challenges related to underlying detection and re-identification tasks need to be addressed along with a joint optimization of these two tasks. In this paper, we propose a three-stage cascaded Scale-Augmented Transformer (SAT) person search framework. In the three-stage design of our SAT framework, the first stage performs person detection whereas the last two stages performs both detection and re-identification. Considering the contradictory nature of detection and identification, in the last two stages, we introduce separate norm feature embeddings for the two tasks to reconcile the relationship between them in a joint person search model. Our SAT framework benefits from the attributes of convolutional neural networks and transformers by introducing a convolutional encoder and a scale modulator within each stage. Here, the convolutional encoder increases the generalization ability of the model whereas the scale modulator performs context aggregation at different granularity levels to aid in handling pose/scale variations within a region of interest. To further improve the performance during occlusion, we apply shifting augmentation operations at each granularity level within the scale modulator. Experimental results on challenging CUHK-SYSU [35] and PRW [47] datasets demonstrate the favorable performance of our method compared to state-of-the-art methods. Our source code and trained models are available at this https URL.",https://openaccess.thecvf.com/content/WACV2023/html/Fiaz_SAT_Scale-Augmented_Transformer_for_Person_Search_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Fiaz_SAT_Scale-Augmented_Transformer_for_Person_Search_WACV_2023_paper.pdf,,https URL provided in the paper,,main,Poster,https://ieeexplore.ieee.org/document/10030219/,"['Convolutional codes', 'Uniform resource locators', 'Computer vision', 'Source coding', 'Modulation', 'Transformers', 'Feature extraction']","['Transformer', 'Level Of Granularity', 'Separate Features', 'Target Person', 'Convolutional Encoder', 'Gallery Images', 'Re-identification Task', 'Training Set', 'Convolutional Layers', 'Pedestrian', 'Scale Variation', 'Bounding Box', 'Average Precision', 'Shift Operator', 'Convolutional Block', 'Linear Layer', 'Residual Connection', 'Faster R-CNN', 'Robust Representation', 'Two-step Model', 'Depthwise Convolution', 'Score Map', 'One-step Model', 'Gallery Set', 'Region Proposal Network']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"Person search is a challenging computer vision problem where the objective is to simultaneously detect and reidentify a target person from the gallery of whole scene images captured from multiple cameras. Here, the challenges related to underlying detection and re-identification tasks need to be addressed along with a joint optimization of these two tasks. In this paper, we propose a three-stage cascaded Scale-Augmented Transformer (SAT) person search framework. In the three-stage design of our SAT framework, the first stage performs person detection whereas the last two stages performs both detection and re-identification. Considering the contradictory nature of detection and re-identification, in the last two stages, we introduce separate norm feature embeddings for the two tasks to reconcile the relationship between them in a joint person search model. Our SAT framework benefits from the attributes of convolutional neural networks and transformers by introducing a convolutional encoder and a scale modulator within each stage. Here, the convolutional encoder increases the generalization ability of the model whereas the scale modulator performs context aggregation at different granularity levels to aid in handling pose/scale variations within a region of interest. To further improve the performance during occlusion, we apply shifting augmentation operations at each granularity level within the scale modulator. Experimental results on challenging CUHK-SYSU [35] and PRW [47] datasets demonstrate the favorable performance of our method compared to state-of-the-art methods. Our source code and trained models are available at this https URL."
SCTS: Instance Segmentation of Single Cells Using a Transformer-Based Semantic-Aware Model and Space-Filling Augmentation,"Yating Zhou, Wenjing Li, Ge Yang","Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences",100,China,0,,"Instance segmentation of single cells from microscopy images is critical to quantitative analysis of their spatial and morphological features for many important biomedical applications, such as disease diagnosis and drug screening. However, the high densities, tight contacts, and weak boundaries of the cells pose substantial technical challenges. To overcome these challenges, we have developed a new instance segmentation model, which we refer to as single-cell Transformer segmenter (SCTS). It utilizes a Swin Transformer as its backbone, combining the global modeling capabilities of a Transformer and the local modeling capabilities of a convolutional neural network (CNN) to ensure model adaptability to different cell sizes, shapes, and textures. It also embeds a three-class (background, cell interior, and cell boundary) semantic segmentation branch to classify pixels and to provide semantic features for downstream tasks. The prediction of boundary semantics improves boundary awareness, and the differentiation between foreground and background semantics improves segmentation integrity in regions with weak signals. To reduce the need for annotated training data, we have developed an augmentation strategy that randomly fills instances of single cells into open spaces of training images. Experiments show that our model outperforms several state-of-the-art models on the LIVECell dataset and an in-house dataset. The code and dataset of this work are openly accessible at https://github.com/cbmi-group/SCTS.",https://openaccess.thecvf.com/content/WACV2023/html/Zhou_SCTS_Instance_Segmentation_of_Single_Cells_Using_a_Transformer-Based_Semantic-Aware_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhou_SCTS_Instance_Segmentation_of_Single_Cells_Using_a_Transformer-Based_Semantic-Aware_WACV_2023_paper.pdf,,https://github.com/cbmi-group/SCTS,,main,Poster,https://ieeexplore.ieee.org/document/10030765/,"['Training', 'Image segmentation', 'Adaptation models', 'Statistical analysis', 'Shape', 'Semantics', 'Training data']","['Cell Segmentation', 'Instance Segmentation', 'Microscopy Images', 'Convolutional Neural Network', 'Cell Size', 'Drug Screening', 'Weak Signal', 'Cell Shape', 'Inside Cells', 'Cell Binding', 'Training Images', 'Semantic Segmentation', 'Regional Integration', 'Augmentation Strategy', 'Tight Contact', 'Annotated Training Data', 'Working Dataset', 'Interior Boundary', 'Training Set', 'Convolutional Layers', 'Mask R-CNN', 'Semantic Labels', 'Vision Transformer', 'Number Of Insertions', 'Individual Instances', 'Fully Convolutional Network', 'Watershed Segmentation', 'Cell Shape Changes', 'Region Proposal Network', 'Intersection Over Union']","['Applications: Biomedical/healthcare/medicine', 'Low-level and physics-based vision']",5,"Instance segmentation of single cells from microscopy images is critical to quantitative analysis of their spatial and morphological features for many important biomedical applications, such as disease diagnosis and drug screening. However, the high densities, tight contacts, and weak boundaries of the cells pose substantial technical challenges. To overcome these challenges, we have developed a new instance segmentation model, which we refer to as single-cell Transformer segmenter (SCTS). It utilizes a Swin Transformer as its backbone, combining the global modeling capabilities of a Transformer and the local modeling capabilities of a convolutional neural network (CNN) to ensure model adaptability to different cell sizes, shapes, and textures. It also embeds a three-class (background, cell interior, and cell boundary) semantic segmentation branch to classify pixels and to provide semantic features for downstream tasks. The prediction of boundary semantics improves boundary awareness, and the differentiation between foreground and background semantics improves segmentation integrity in regions with weak signals. To reduce the need for annotated training data, we have developed an augmentation strategy that randomly fills instances of single cells into open spaces of training images. Experiments show that our model outperforms several state-of-the-art models on the LIVECell dataset and an in-house dataset. The code and dataset of this work are openly accessible at https://github.com/cbmigroup/SCTS."
SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution,"Shwai He, Chenbo Jiang, Daize Dong, Liang Ding",Nanjing University of Science and Technology Zijin College; JD Explore Academy; University of Electronic Science and Technology of China,100,China,0,,"Dynamic convolution achieves better performance for efficient CNNs at the cost of negligible FLOPs increase. However, the performance increase can not match the significantly expanded number of parameters, which is the main bottleneck in real-world applications. Contrastively, mask-based unstructured pruning obtains a lightweight network by removing redundancy in the heavy network. In this paper, we propose a new framework, Sparse Dynamic Convolution (SD-Conv), to naturally integrate these two paths such that it can inherit the advantage of dynamic mechanism and sparsity. We first design a binary mask derived from a learnable threshold to prune static kernels, significantly reducing the parameters and computational cost but achieving higher performance in Imagenet-1K. We further transfer pretrained models into a variety of downstream tasks, showing consistently better results than baselines. We hope our SD-Conv could be an efficient alternative to conventional dynamic convolutions.",https://openaccess.thecvf.com/content/WACV2023/html/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030938/,"['Computer vision', 'Costs', 'Convolution', 'Computational modeling', 'Redundancy', 'Computational efficiency', 'Task analysis']","['Dynamic Convolution', 'Computational Cost', 'Neural Network', 'Learning Rate', 'Deep Neural Network', 'Convolutional Layers', 'Image Classification', 'Dense Network', 'Dynamic Network', 'Binary Data', 'ImageNet', 'Convolution Operation', 'Convolution Kernel', 'Softmax Function', 'Attention Scores', 'Convolutional Architecture', 'Extra Computational Cost', 'Kernel Computation', 'Network Pruning']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",1,"Dynamic convolution achieves better performance for efficient CNNs at the cost of negligible FLOPs increase. However, the performance increase can not match the significantly expanded number of parameters, which is the main bottleneck in real-world applications. Contrastively, mask-based unstructured pruning obtains a lightweight network by removing redundancy in the heavy network. In this paper, we propose a new framework, Sparse Dynamic Convolution (SD-CONV), to naturally integrate these two paths such that it can inherit the advantage of dynamic mechanism and sparsity. We first design a binary mask derived from a learnable threshold to prune static kernels, significantly reducing the parameters and computational cost but achieving higher performance in Imagenet-1K. We further transfer pretrained models into a variety of downstream tasks, showing consistently better results than baselines. We hope our SD-Conv could be an efficient alternative to conventional dynamic convolutions."
SD-Pose: Structural Discrepancy Aware Category-Level 6D Object Pose Estimation,"Guowei Li, Dongchen Zhu, Guanghui Zhang, Wenjun Shi, Tianyu Zhang, Xiaolin Zhang, Jiamao Li","Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Xiongan Institute of Innovation, Xiongan, 071700, China; University of Science and Technology of China, Hefei, Anhui, 230027, China; ShanghaiTech University, Shanghai 201210, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Bionic Vision System Laboratory, State Key Laboratory of Transducer Technology, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai 200050, China; University of Chinese Academy of Sciences, Beijing 100049, China; Xiongan Institute of Innovation, Xiongan, 071700, China",100,China,0,,"Category-level 6D object pose estimation aims to predict the full pose and size information for previously unseen instances from known categories, which is an essential portion of robot grasping and augmented reality. However, the core challenge of this task still is the enormous shape variation within each category. With regard to the challenge, we propose a novel framework SD-Pose, which utilizes the instance-category structural discrepancy and the potential geometric-semantic association to enhance the exploration of the intra-class shape information. Specifically, an information exchange augmentation (IEA) module is introduced to supplement the instance-category structural information by their structural discrepancy, thus facilitating the enhanced geometric information to contain both the character of instance shape and the commonality of category structure. For complementing the deficiencies of structural information adaptively, a semantic dynamic fusion (SDF) module is further designed to fuse semantic and geometric features. Finally, the proposed SD-Pose framework equipped with the IEA and SDF modules hierarchically supplements instance-category structural information in a stacked manner and achieves state-of-the-art performance on the CAMERA25 and REAL275 datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030202/,"['Geometry', 'Point cloud compression', 'Computer vision', 'Shape', 'Fuses', 'Semantics', 'Pose estimation']","['Pose Estimation', 'Human Pose Estimation', '6D Object Pose', '6D Object Pose Estimation', 'Structural Information', 'Shape Variation', 'Geometric Features', 'Semantic Features', 'Geometric Information', 'Pose Information', 'Object Detection', 'Intersection Over Union', 'Point Cloud', 'Semantic Information', 'Multilayer Perceptron', 'Structure Of Matrix', 'Fusion Method', 'Segmentation Results', 'Feature Categories', 'Dynamic Adjustment', 'CAD Model', 'Results In Row', 'Object Instances', 'Matching Network', 'Geometry Information', 'Intra-class Variance', 'Point Cloud Model', 'Influential Points', 'Chamfer Distance']",['Algorithms: 3D computer vision'],3,"Category-level 6D object pose estimation aims to predict the full pose and size information for previously unseen instances from known categories, which is an essential portion of robot grasping and augmented reality. However, the core challenge of this task still is the enormous shape variation within each category. With regard to the challenge, we propose a novel framework SD-Pose, which utilizes the instance-category structural discrepancy and the potential geometric-semantic association to enhance the exploration of the intra-class shape information. Specifically, an information exchange augmentation (IEA) module is introduced to supplement the instance-category structural information by their structural discrepancy, thus facilitating the enhanced geometric information to contain both the character of instance shape and the commonality of category structure. For complementing the deficiencies of structural information adaptively, a semantic dynamic fusion (SDF) module is further designed to fuse semantic and geometric features. Finally, the proposed SD-Pose framework equipped with the IEA and SDF modules hierarchically supplements instance-category structural information in a stacked manner and achieves state-of-the-art performance on the CAMERA25 and REAL275 datasets."
SERF: Towards Better Training of Deep Neural Networks Using Log-Softplus ERror Activation Function,"Sayan Nag, Mayukh Bhattacharyya, Anuraag Mukherjee, Rohit Kundu",UCR Riverside; Stony Brook University; University of Toronto; IISER Mohali,100,"Canada, India, USA",0,,"Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and nonmonotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multimodal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of preconditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster.",https://openaccess.thecvf.com/content/WACV2023/html/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030482/,"['Training', 'Deep learning', 'Computer vision', 'Neural networks', 'Computer architecture', 'Object detection', 'Machine translation']","['Neural Network', 'Activation Function', 'Deep Neural Network', 'Neural Network Training', 'Learning Rate', 'Classification Task', 'Image Classification', 'Object Detection', 'Network Performance', 'Vision Tasks', 'Entailment', 'Sentiment Analysis', 'Machine Translation', 'Natural Language Processing Tasks', 'Regularization Effect', 'Rectified Linear Unit Activation', 'Language Processing Tasks', 'Computer Vision Processing', 'Transformer', 'Leaky ReLU', 'Variety Of Tasks', 'CIFAR-100 Dataset', 'Object Detection Task', 'PASCAL VOC', 'Mean Average Precision', 'Effect Of Preconditioning', 'Nonlinear Function', 'Computational Efficiency', 'Error Function']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",6,"Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and non-monotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multi-modal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of pre-conditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster."
SGPCR: Spherical Gaussian Point Cloud Representation and Its Application To Object Registration and Retrieval,"Driton Salihu, Eckehard Steinbach","Chair of Media Technology and Munich Institute of Robotics and Machine Intelligence (MIRMI), Technical University of Munich, School of Computation, Information and Technology, Department of Computer Engineering",100,Germany,0,,"Retrieving and aligning CAD models from databases with scanned real-world point clouds remains an important topic for 3D reconstruction. Due to zero point-to-point correspondences between the sampled CAD model and the scanned real-world object, an information-rich representation of point clouds is needed. We propose SGPCR, a novel method for representing 3D point clouds by Spherical Gaussians for efficient, stable, and rotation-equivariant representation. We also propose a rotation-invariant convolution to improve the representation quality through a trainable optimization process. In addition, we demonstrate the strengths of SGPCR-based point cloud representation using the fundamental challenge of shape retrieval and point cloud registration on point clouds with zero point-to-point correspondences. Under these conditions, our approach improves registration quality by reducing chamfer distance by up to 90% and rotation root mean square error by up to 86% compared to the state of the art. Furthermore, the proposed SGCPR is used for one-shot shape retrieval and registration and improves retrieval precision by up to 58% over comparable methods.",https://openaccess.thecvf.com/content/WACV2023/html/Salihu_SGPCR_Spherical_Gaussian_Point_Cloud_Representation_and_Its_Application_To_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Salihu_SGPCR_Spherical_Gaussian_Point_Cloud_Representation_and_Its_Application_To_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030833/,"['Point cloud compression', 'Training', 'Solid modeling', 'Three-dimensional displays', 'Shape', 'Convolution', 'Transformers']","['Point Cloud', 'Point Cloud Representation', 'Spherical Representation', 'Spherical Gaussian', 'Root Mean Square Error', 'Mean Square Error', 'Convolution', '3D Reconstruction', '3D Point Cloud', 'CAD Model', 'Stable Representation', 'Point Cloud Registration', 'Chamfer Distance', 'Degrees Of Freedom', 'Transformer', 'Gaussian Kernel', 'K-nearest Neighbor', 'Mixture Model', 'Bounding Box', 'Indoor Environments', 'Semantic Segmentation', 'Registration Method', '3D Object Detection', 'Gaussian Mixture Model', 'Rigid Transformation', 'Correct Object', 'Object Point Cloud', 'Surface Reconstruction', 'Registration Procedure', 'Alignment Quality']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"Retrieving and aligning CAD models from databases with scanned real-world point clouds remains an important topic for 3D reconstruction. Due to zero point-to-point correspondences between the sampled CAD model and the scanned real-world object, an information-rich representation of point clouds is needed. We propose SGPCR, a novel method for representing 3D point clouds by Spherical Gaussians for efficient, stable, and rotation-equivariant representation. We also propose a rotation-invariant convolution to improve the representation quality through a trainable optimization process. In addition, we demonstrate the strengths of SGPCR-based point cloud representation using the fundamental challenge of shape retrieval and point cloud registration on point clouds with zero point-to-point correspondences. Under these conditions, our approach improves registration quality by reducing chamfer distance by up to 90% and rotation root mean square error by up to 86% compared to the state of the art. Furthermore, the proposed SGCPR is used for one-shot shape retrieval and registration and improves retrieval precision by up to 58% over comparable methods."
SHARDS: Efficient Shadow Removal Using Dual Stage Network for High-Resolution Images,"Mrinmoy Sen, Sai Pradyumna Chermala, Nazrinbanu Nurmohammad Nagori, Venkat Peddigari, Praful Mathur, B. H. Pawan Prasad, Moonhwan Jeong",Samsung Electronics; Samsung R&D Institute India - Bangalore,50,India,50,South Korea,"Shadow Removal is an important and widely researched topic in computer vision. Recent advances in deep learning have resulted in addressing this problem by using convolutional neural networks (CNNs) similar to other vision tasks. But these existing works are limited to low-resolution images. Furthermore, the existing methods rely on heavy network architectures which cannot be deployed on resource-constrained platforms like smartphones. In this paper, we propose SHARDS, a shadow removal method for high-resolution images. The proposed method solves shadow removal for high-resolution images in two stages using two lightweight networks: a Low-resolution Shadow Removal Network (LSRNet) followed by a Detail Refinement Network (DRNet). LSRNet operates at low-resolution and computes a low-resolution, shadow-free output. It achieves state-of-the-art results on standard datasets with 65x lesser network parameters than existing methods. This is followed by DRNet, which is tasked to refine the low-resolution output to a high-resolution output using the high-resolution input shadow image as guidance. We construct high-resolution shadow removal datasets and through our experiments, prove the effectiveness of our proposed method on them. It is then demonstrated that this method can be deployed on modern day smartphones and is the first of its kind solution that can efficiently (2.4secs) perform shadow removal for high-resolution images (12MP) in these devices. Like many existing approaches, our shadow removal network relies on a shadow region mask as input to the network. To complement the lightweight shadow removal network, we also propose a lightweight shadow detector in this paper.",https://openaccess.thecvf.com/content/WACV2023/html/Sen_SHARDS_Efficient_Shadow_Removal_Using_Dual_Stage_Network_for_High-Resolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sen_SHARDS_Efficient_Shadow_Removal_Using_Dual_Stage_Network_for_High-Resolution_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030238/,"['Performance evaluation', 'Deep learning', 'Computer vision', 'Computer architecture', 'Detectors', 'Network architecture', 'Convolutional neural networks']","['High-resolution Images', 'Dual Stage', 'Shadow Removal', 'Convolutional Neural Network', 'Computer Vision', 'Network Parameters', 'Low-resolution Images', 'Standard Datasets', 'Advances In Deep Learning', 'High-resolution Dataset', 'Lightweight Network', 'Shadow Regions', 'Shadow Images', 'Root Mean Square Error', 'False Negative', 'Computational Efficiency', 'Convolutional Layers', 'Early Work', 'Receptive Field', 'Training Details', 'Lab Color Space', 'Perceptual Loss', 'Early Techniques', 'Gamma Correction', 'Number Of Network Parameters']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",3,"Shadow Removal is an important and widely researched topic in computer vision. Recent advances in deep learning have resulted in addressing this problem by using convolutional neural networks (CNNs) similar to other vision tasks. But these existing works are limited to low-resolution images. Furthermore, the existing methods rely on heavy network architectures which cannot be deployed on resource-constrained platforms like smartphones. In this paper, we propose SHARDS, a shadow removal method for high-resolution images. The proposed method solves shadow removal for high-resolution images in two stages using two lightweight networks: a Low-resolution Shadow Removal Network (LSRNet) followed by a Detail Refinement Network (DRNet). LSRNet operates at low-resolution and computes a low-resolution, shadow-free output. It achieves state-of-the-art results on standard datasets with 65x lesser network parameters than existing methods. This is followed by DRNet, which is tasked to refine the low-resolution output to a high-resolution output using the high-resolution input shadow image as guidance. We construct high-resolution shadow removal datasets and through our experiments, prove the effectiveness of our proposed method on them. It is then demonstrated that this method can be deployed on modern day smartphones and is the first of its kind solution that can efficiently (2.4secs) perform shadow removal for high-resolution images (12MP) in these devices. Like many existing approaches, our shadow removal network relies on a shadow region mask as input to the network. To complement the lightweight shadow removal network, we also propose a lightweight shadow detector in this paper."
SIRA: Relightable Avatars From a Single Image,"Pol Caselles, Eduard Ramon, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer, Gil Triginer","Universitat Politècnica de Catalunya; Crisalix SA; Institut de Robòtica i Informàtica Industrial, CSIC-UPC",100,"France, Spain",0,,"Recovering the geometry of a human head from a single image, while factorizing the materials and illumination is a severely ill-posed problem that requires prior information to be solved. Methods based on 3D Morphable Models (3DMM), and their combination with differentiable renderers, have shown promising results. However, the expressiveness of 3DMMs is limited, and they typically yield over-smoothed and identity-agnostic 3D shapes limited to the face region. Highly accurate full head reconstructions have recently been obtained with neural fields that parameterize the geometry using multilayer perceptrons. The versatility of these representations has also proved effective for disentangling geometry, materials and lighting. However, these methods require several tens of input images. In this paper, we introduce SIRA, a method which, from a single image, reconstructs human head avatars with high fidelity geometry and factorized lights and surface materials. Our key ingredients are two data-driven statistical models based on neural fields that resolve the ambiguities of single-view 3D surface reconstruction and appearance factorization. Experiments show that SIRA obtains state of the art results in 3D head reconstruction while at the same time it successfully disentangles the global illumination, and the diffuse and specular albedos. Furthermore, our reconstructions are amenable to physically-based appearance editing and head model relighting.",https://openaccess.thecvf.com/content/WACV2023/html/Caselles_SIRA_Relightable_Avatars_From_a_Single_Image_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Caselles_SIRA_Relightable_Avatars_From_a_Single_Image_WACV_2023_paper.pdf,,,2209.03027,main,Poster,https://ieeexplore.ieee.org/document/10030265/,"['Geometry', 'Surface reconstruction', 'Solid modeling', 'Head', 'Three-dimensional displays', 'Shape', 'Avatars']","['Single Image', 'Statistical Models', 'Factorization', 'Input Image', '3D Reconstruction', 'Multilayer Perceptron', '3D Surface', 'Reconstruction Accuracy', '3D Shape', 'Humeral Head', 'Neural Field', 'Full Head', '3D Head', 'Deep Neural Network', 'Latent Space', 'Implicit Bias', 'Inference Time', '3D Geometry', 'Signed Distance Function', 'Latent Vector', 'Multi-view Images', 'Scene Geometry', 'Shape Space', 'Surface Points', 'View Synthesis', 'Reference Space', 'Specular Component', 'Internal Parameters']","['Algorithms: 3D computer vision', 'Biometrics', 'face', 'gesture', 'body pose', 'Computational photography', 'image and video synthesis']",9,"Recovering the geometry of a human head from a single image, while factorizing the materials and illumination, is a severely ill-posed problem that requires prior information to be solved. Methods based on 3D Morphable Models (3DMM), and their combination with differentiable renderers, have shown promising results. However, the expressiveness of 3DMMs is limited, and they typically yield over-smoothed and identity-agnostic 3D shapes limited to the face region. Highly accurate full head reconstructions have recently been obtained with neural fields that parameterize the geometry using multilayer perceptrons. The versatility of these representations has also proved effective for disentangling geometry, materials and lighting. However, these methods require several tens of input images. In this paper, we introduce SIRA, a method which, from a single image, reconstructs human head avatars with high fidelity geometry and factorized lights and surface materials. Our key ingredients are two data-driven statistical models based on neural fields that resolve the ambiguities of single-view 3D surface reconstruction and appearance factorization. Experiments show that SIRA obtains state of the art results in 3D head reconstruction while at the same time it successfully disentangles the global illumination, and the diffuse and specular albedos. Furthermore, our reconstructions are amenable to physically-based appearance editing and head model relighting."
SIUNet: Sparsity Invariant U-Net for Edge-Aware Depth Completion,"Avinash Nittur Ramesh, Fabio Giovanneschi, María A. González-Huici","Fraunhofer FHR, Wachtberg, Germany",0,,100,Germany,"Depth completion is the task of generating dense depth images from sparse depth measurements, e.g., LiDARs. Existing unguided approaches fail to recover dense depth images with sharp object boundaries due to depth bleeding, especially from extremely sparse measurements. State-of-the-art guided approaches require additional processing for spatial and temporal alignment of multi-modal inputs, and sophisticated architectures for data fusion, making them non-trivial for customized sensor setup. To address these limitations, we propose an unguided approach based on UNet that is invariant to sparsity of inputs. Boundary consistency in reconstruction is explicitly enforced through auxiliary learning on a synthetic dataset with dense depth and depth contour images as targets, followed by fine-tuning on a real-world dataset. With our network architecture and simple implementation approach, we achieve competitive results among unguided approaches on KITTI benchmark and show that the reconstructed image has sharp boundaries and is robust even towards extremely sparse LiDAR measurements.",https://openaccess.thecvf.com/content/WACV2023/html/Ramesh_SIUNet_Sparsity_Invariant_U-Net_for_Edge-Aware_Depth_Completion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ramesh_SIUNet_Sparsity_Invariant_U-Net_for_Edge-Aware_Depth_Completion_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030185/,"['Training', 'Laser radar', 'Image edge detection', 'Computer architecture', 'Network architecture', 'Sensor systems', 'Task analysis']","['Depth Completion', 'Real-world Datasets', 'Bathymetry', 'Depth Images', 'Data Fusion', 'Density Imaging', 'Object Boundaries', 'Sharp Boundaries', 'Sparse Measurements', 'Lidar Measurements', 'Dense Depth', 'Transfer Learning', 'Point Cloud', 'Noisy Data', 'Semantic Segmentation', 'RGB Images', 'Primary Task', 'Target Domain', 'Source Domain', 'Ground Truth Image', 'KITTI Dataset', 'Auxiliary Task', 'Sparse Input', 'Zero-shot', 'Naive Way', 'Valid Pixels', 'Sparse Imaging', 'Lidar Data', 'Sparsity Level', 'U-Net Architecture']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', '3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",6,"Depth completion is the task of generating dense depth images from sparse depth measurements, e.g., LiDARs. Existing unguided approaches fail to recover dense depth images with sharp object boundaries due to depth bleeding, especially from extremely sparse measurements. State-of-the-art guided approaches require additional processing for spatial and temporal alignment of multi-modal inputs, and sophisticated architectures for data fusion, making them non-trivial for customized sensor setup. To address these limitations, we propose an unguided approach based on U-Net that is invariant to sparsity of inputs. Boundary consistency in reconstruction is explicitly enforced through auxiliary learning on a synthetic dataset with dense depth and depth contour images as targets, followed by fine-tuning on a real-world dataset. With our network architecture and simple implementation approach, we achieve competitive results among unguided approaches on KITTI benchmark and show that the reconstructed image has sharp boundaries and is robust even towards extremely sparse LiDAR measurements."
SLI-pSp: Injecting Multi-Scale Spatial Layout in pSp,"Aradhya Neeraj Mathur, Anish Madan, Ojaswa Sharma",IIITD,100,India,0,,"We propose SLI-pSp, a general purpose Image-to-Image (I2I) translation model that encodes spatial layout information as well as style in the generator, using pSp as the base architecture. Previous methods like pSp have shown promising results by leveraging StyleGAN as a generator in various I2I tasks but they seem to miss finer or under-represented details in facial images like earrings and caps, and break down on complex datasets due to their solely global approach. To address these shortcomings, we propose a technique termed Spatial Layout Injection (SLI-pSp) that encodes spatial layout information in the input image in the StyleGAN generator along with style. We do so without modifying the style vector injection in the generator through pSp's map2style network, but rather by combining SLI with noise layers in the StyleGAN generator at multiple spatial scales. Such an approach helps preserve global aspects of image generation as well as enhance spatial layout details in the output. We experiment on several challenging datasets and across several I2I tasks that highlight the effectiveness of our approach over previous methods with respect to finer details in the generated image and overall visual quality.",https://openaccess.thecvf.com/content/WACV2023/html/Mathur_SLI-pSp_Injecting_Multi-Scale_Spatial_Layout_in_pSp_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mathur_SLI-pSp_Injecting_Multi-Scale_Spatial_Layout_in_pSp_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030567/,"['Visualization', 'Computer vision', 'Image synthesis', 'Layout', 'Computer architecture', 'Generators', 'Task analysis']","['Spatial Layout', 'Image Quality', 'Spatial Information', 'Input Image', 'Feature Maps', 'Multiple Scales', 'Network Layer', 'Super-resolution', 'Image Information', 'Semantic Information', 'Variety Of Tasks', 'Generative Adversarial Networks', 'Latent Space', 'Image Generation', 'RGB Images', 'Fine Details', 'Face Images', 'Visual Quality', 'Global Approach', 'Tinnitus', 'Latent Code', 'Conditional Generative Adversarial Network', 'Latent Vector', 'Synthesis Network', 'Segmentation Map', 'Translation Task', 'Semantic Segmentation', 'High-resolution Images', 'Image Edge', 'Convolution']","['Algorithms: Computational photography', 'image and video synthesis', 'Adversarial learning', 'adversarial attack and defense methods']",,"We propose SLI-pSp, a general purpose Image-to-Image (I2I) translation model that encodes spatial layout information as well as style in the generator, using pSp as the base architecture. Previous methods like pSp have shown promising results by leveraging StyleGAN as a generator in various I2I tasks but they seem to miss finer or underrepresented details in facial images like earrings and caps, and break down on complex datasets due to their solely global approach. To address these shortcomings, we propose a technique termed Spatial Layout Injection (SLI-pSp) that encodes spatial layout information in the input image in the StyleGAN generator along with style. We do so without modifying the style vector injection in the generator through pSp’s map2style network, but rather by combining SLI with noise layers in the StyleGAN generator at multiple spatial scales. Such an approach helps preserve global aspects of image generation as well as enhance spatial layout details in the output. We experiment on several challenging datasets and across several I2I tasks that highlight the effectiveness of our approach over previous methods with respect to finer details in the generated image and overall visual quality."
SONGs: Self-Organizing Neural Graphs,"Łukasz Struski, Tomasz Danel, Marek Śmieja, Jacek Tabor, Bartosz Zieliński","Faculty of Mathematics and Computer Science, Jagiellonian University, Krakow, Poland; Faculty of Mathematics and Computer Science, Jagiellonian University, Krakow, Poland; IDEAS NCBR, Warsaw, Poland",66.66666667,Poland,33.33333333,Poland,"Recent years have seen a surge in research on combining deep neural networks with other methods, including decision trees and graphs. There are at least three advantages of incorporating decision trees and graphs: they are easy to interpret since they are based on sequential decisions, they can make decisions faster, and they provide a hierarchy of classes. However, one of the well-known drawbacks of decision trees, as compared to decision graphs, is that decision trees cannot reuse the decision nodes. Nevertheless, decision graphs were not commonly used in deep learning due to the lack of efficient gradient-based training techniques. In this paper, we fill this gap and provide a general paradigm based on Markov processes, which allows for efficient training of the special type of decision graphs, which we call Self-Organizing Neural Graphs (SONG). We provide a theoretical study on SONG, complemented by experiments conducted on Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our method performs on par or better than existing decision models.",https://openaccess.thecvf.com/content/WACV2023/html/Struski_SONGs_Self-Organizing_Neural_Graphs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Struski_SONGs_Self-Organizing_Neural_Graphs_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030404/,"['Training', 'Deep learning', 'Computer vision', 'Pipelines', 'Neural networks', 'Directed graphs', 'Markov processes']","['Neural Network', 'Deep Learning', 'Markov Chain', 'Deep Neural Network', 'Decision Tree', 'Decision Model', 'Training Efficiency', 'MNIST Dataset', 'Lack Of Techniques', 'Types Of Graphs', 'Surge In Research', 'Including Decision Tree', 'Convolutional Neural Network', 'Number Of Steps', 'Multi-label', 'Tree Structure', 'Directed Graph', 'Pair Of Nodes', 'Graph Structure', 'Additional Loss', 'Back Edge', 'Binary Cross-entropy Loss', 'Binary Tree', 'Directed Acyclic Graph', 'Shallow Model', 'Edge Probability', 'Backbone Network', 'Subtree', 'CIFAR-100 Dataset']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",,"Recent years have seen a surge in research on combining deep neural networks with other methods, including decision trees and graphs. There are at least three advantages of incorporating decision trees and graphs: they are easy to interpret since they are based on sequential decisions, they can make decisions faster, and they provide a hierarchy of classes. However, one of the well-known drawbacks of decision trees, as compared to decision graphs, is that decision trees cannot reuse the decision nodes. Nevertheless, decision graphs were not commonly used in deep learning due to the lack of efficient gradient-based training techniques. In this paper, we fill this gap and provide a general paradigm based on Markov processes, which allows for efficient training of the special type of decision graphs, which we call Self-Organizing Neural Graphs (SONG). We provide a theoretical study on SONG, complemented by experiments conducted on Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our method performs on par or better than existing decision models."
SPIQ: Data-Free Per-Channel Static Input Quantization,"Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly","Sorbonne Universit ´e1, CNRS, ISIR, f-75005, 4 Place Jussieu 75005 Paris, France; Datakalab2, 114 boulevard Malesherbes, 75017 Paris, France",50,France,50,France,"Computationally expensive neural networks are ubiquitous in computer vision and solutions for efficient inference have drawn a growing attention in the machine learning community. Examples of such solutions comprise quantization, i.e. converting the processing values (weights and inputs) from floating point into integers e.g. int8 or int4. Concurrently, the rise of privacy concerns motivated the study of less invasive acceleration methods, such as data-free quantization of pre-trained models weights and activations. Previous approaches either exploit statistical information to deduce scalar ranges and scaling factors for the activations in a static manner, or dynamically adapt this range on-the-fly for each input of each layers (also referred to as activations): the latter generally being more accurate at the expanse of significantly slower inference. In this work, we argue that static input quantization can reach the accuracy levels of dynamic methods by means of a per-channel input quantization scheme that allows one to more finely preserve cross-channel dynamics. We show through a thorough empirical evaluation on multiple computer vision problems (e.g. ImageNet classification, Pascal VOC object detection as well as CityScapes semantic segmentation) that the proposed method, dubbed SPIQ, achieves accuracies rivalling dynamic approaches with static-level inference speed, significantly outperforming state-of-the-art quantization methods on every benchmark.",https://openaccess.thecvf.com/content/WACV2023/html/Yvinec_SPIQ_Data-Free_Per-Channel_Static_Input_Quantization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yvinec_SPIQ_Data-Free_Per-Channel_Static_Input_Quantization_WACV_2023_paper.pdf,,,2203.14642,main,Poster,https://ieeexplore.ieee.org/document/10030914/,"['Computer vision', 'Data privacy', 'Quantization (signal)', 'Semantic segmentation', 'Neural networks', 'Estimation', 'Object detection']","['Input State', 'Input Quantization', 'Neural Network', 'Scaling Factor', 'Computer Vision', 'Quantification Method', 'Input Layer', 'Object Detection', 'ImageNet', 'Dynamic Approach', 'Semantic Segmentation', 'Inference Speed', 'Quantization Scheme', 'PASCAL VOC', 'Small Values', 'Deep Neural Network', 'Challenging Task', 'Quantization Error', 'Static Method', 'Output Channels', 'Static Approach', 'Scalar Value', 'BN Layer', 'Input Tensor', 'Input Scale', 'Accuracy Drop', 'Input Distribution', 'Quantification Process', 'Empirical Validation']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'low-shot', 'semi-', 'self-', 'and un-supervised learning']",5,"Computationally expensive neural networks are ubiquitous in computer vision and solutions for efficient inference have drawn a growing attention in the machine learning community. Examples of such solutions comprise quantization, i.e. converting the processing values (weights and inputs) from floating point into integers e.g. int8 or int4. Concurrently, the rise of privacy concerns motivated the study of less invasive acceleration methods, such as data-free quantization of pre-trained models weights and activations. Previous approaches either exploit statistical information to deduce scalar ranges and scaling factors for the activations in a static manner, or dynamically adapt this range on-the-fly for each input of each layer (also referred to as activations): the latter generally being more accurate at the expense of significantly slower inference. In this work, we argue that static input quantization can reach the accuracy levels of dynamic methods by means of a per-channel input quantization scheme that allows one to more finely preserve cross-channel dynamics. We show through a thorough empirical evaluation on multiple computer vision problems (e.g. ImageNet classification, Pascal VOC object detection as well as CityScapes semantic segmentation) that the proposed method, dubbed SPIQ, achieves accuracies rivalling dynamic approaches with static-level inference speed, significantly outperforming state-of-the-art quantization methods on every benchmark."
SSFE-Net: Self-Supervised Feature Enhancement for Ultra-Fine-Grained Few-Shot Class Incremental Learning,"Zicheng Pan, Xiaohan Yu, Miaohua Zhang, Yongsheng Gao","School of Engineering and Built Environment, Griffith University, QLD, 4111, Australia",100,Australia,0,,"Ultra-Fine-Grained Visual Categorization (ultra-FGVC) has become a popular problem due to its great real-world potential for classifying the same or closely related species with very similar layouts. However, there present many challenges for the existing ultra-FGVC methods, firstly there are always not enough samples in the existing ultra-FGVC datasets based on which the models can easily get overfitting. Secondly, in practice, we are likely to find new species that we have not seen before and need to add them to existing models, which is known as incremental learning. The existing methods solve these problems by Few-Shot Class Incremental Learning (FSCIL), but the main challenge of the FSCIL models on ultra-FGVC tasks lies in their inferior discrimination detection ability since they usually use low-capacity networks to extract features, which leads to insufficient discriminative details extraction from ultra-fine-grained images. In this paper, a self-supervised feature enhancement for the few-shot incremental learning network (SSFE-Net) is proposed to solve this problem. Specifically, a self-supervised learning (SSL) and knowledge distillation (KD) framework is developed to enhance the feature extraction of the low-capacity backbone network for ultra-FGVC few-shot class incremental learning tasks. Besides, we for the first time create a series of benchmarks for FSCIL tasks on two public ultra-FGVC datasets and three normal fine-grained datasets, which will facilitate the development of the Ultra-FGVC community. Extensive experimental results on public ultra-FGVC datasets and other state-of-the-art benchmarks consistently demonstrate the effectiveness of the proposed method.",https://openaccess.thecvf.com/content/WACV2023/html/Pan_SSFE-Net_Self-Supervised_Feature_Enhancement_for_Ultra-Fine-Grained_Few-Shot_Class_Incremental_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Pan_SSFE-Net_Self-Supervised_Feature_Enhancement_for_Ultra-Fine-Grained_Few-Shot_Class_Incremental_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030400/,"['Knowledge engineering', 'Visualization', 'Computer vision', 'Layout', 'Self-supervised learning', 'Computer architecture', 'Benchmark testing']","['Incremental Learning', 'Feature Enhancement', 'Class-incremental Learning', 'Few-shot Class-incremental Learning', 'Learning Task', 'Community Development', 'Self-supervised Learning', 'Visual Classification', 'Few-shot Learning', 'Insufficient Detail', 'Neural Network', 'Performance Of Method', 'Feature Maps', 'Feature Representation', 'Attention In Recent Years', 'Mutual Information', 'Discriminative Features', 'Precision Agriculture', 'Dataset Split', 'Feature Extraction Ability', 'Prediction Head', 'Class Activation Maps', 'Distillation Loss']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Agriculture']",11,"Ultra-Fine-Grained Visual Categorization (ultra-FGVC) has become a popular problem due to its great real-world potential for classifying the same or closely related species with very similar layouts. However, there present many challenges for the existing ultra-FGVC methods, firstly there are always not enough samples in the existing ultraFGVC datasets based on which the models can easily get overfitting. Secondly, in practice, we are likely to find new species that we have not seen before and need to add them to existing models, which is known as incremental learning. The existing methods solve these problems by Few-Shot Class Incremental Learning (FSCIL), but the main challenge of the FSCIL models on ultra-FGVC tasks lies in their inferior discrimination detection ability since they usually use low-capacity networks to extract features, which leads to insufficient discriminative details extraction from ultrafine-grained images. In this paper, a self-supervised feature enhancement for the few-shot incremental learning network (SSFE-Net) is proposed to solve this problem. Specifically, a self-supervised learning (SSL) and knowledge distillation (KD) framework is developed to enhance the feature extraction of the low-capacity backbone network for ultra-FGVC few-shot class incremental learning tasks. Besides, we for the first time create a series of benchmarks for FSCIL tasks on two public ultra-FGVC datasets and three normal finegrained datasets, which will facilitate the development of the Ultra-FGVC community. Extensive experimental results on public ultra-FGVC datasets and other state-of-the-art benchmarks consistently demonstrate the effectiveness of the proposed method."
SSSD: Self-Supervised Self Distillation,"Wei-Chi Chen, Wei-Ta Chu",,,,,,"With labeled data, self distillation (SD) has been proposed to develop compact but effective models without a complex teacher model available in advance. Such approaches need labeled data to guide the self distillation process. Inspired by self-supervised (SS) learning, we propose a self-supervised self distillation (SSSD) approach in this work. Based on an unlabeled image dataset, a model is constructed to learn visual representations in a self-supervised manner. This pre-trained model is then adopted to extract visual representations of the target dataset and generates pseudo labels via clustering. The pseudo labels guide the SD process, and thus enable SD to proceed in an unsupervised way (no data labels are required at all). We verify this idea based on evaluations on the CIFAR-10, CIFAR-100, and ImageNet-1K datasets, and demonstrate the effectiveness of this unsupervised SD approach. Performance outperforming similar frameworks is also shown.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_SSSD_Self-Supervised_Self_Distillation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_SSSD_Self-Supervised_Self_Distillation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030174/,"['Visualization', 'Computer vision', 'Computational modeling', 'Clustering algorithms', 'Self-supervised learning', 'Feature extraction', 'Data models']","['Visual Representation', 'Teacher Model', 'Labeled Data', 'Self-supervised Learning', 'Approach In This Work', 'Pseudo Labels', 'Distillation Process', 'Self-supervised Manner', 'CIFAR-100 Dataset', 'Classification Accuracy', 'Cross-entropy', 'Object Detection', 'Data Augmentation', 'Kullback-Leibler', 'Fully-connected Layer', 'Residual Block', 'Backbone Network', 'Classification Output', 'Pre-trained Network', 'Semi-supervised Learning', 'Pretext Task', 'Distillation Method', 'Student Model', 'Random Labeling', 'Teacher Network', 'Cluster Concept']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"With labeled data, self distillation (SD) has been proposed to develop compact but effective models without a complex teacher model available in advance. Such approaches need labeled data to guide the self distillation process. Inspired by self-supervised (SS) learning, we propose a self-supervised self distillation (SSSD) approach in this work. Based on an unlabeled image dataset, a model is constructed to learn visual representations in a self-supervised manner. This pre-trained model is then adopted to extract visual representations of the target dataset and generates pseudo labels via clustering. The pseudo labels guide the SD process, and thus enable SD to proceed in an unsupervised way (no data labels are required at all). We verify this idea based on evaluations on the CIFAR-10, CIFAR-100, and ImageNet-1K datasets, and demonstrate the effectiveness of this unsupervised SD approach. Performance outperforming similar frameworks is also shown."
STAR-Transformer: A Spatio-Temporal Cross Attention Transformer for Human Action Recognition,"Dasom Ahn, Sangwon Kim, Hyunsu Hong, Byoung Chul Ko","Dept. of Computer Engineering, Keimyung University, Daegu, South Korea; Difine, Seongnam, South Korea",50,South Korea,50,South Korea,"In action recognition, although the combination of spatio-temporal videos and skeleton features can improve the recognition performance, a separate model and balancing feature representation for cross-modal data are required. To solve these problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can effectively represent two cross-modal features as a recognizable vector. First, from the input video and skeleton sequence, video frames are output as global grid tokens and skeletons are output as joint map tokens, respectively. These tokens are then aggregated into multi-class tokens and input into STAR-transformer. The STAR-transformer encoder consists of a full spatio-temporal attention (FAttn) module and a proposed zigzag spatio-temporal attention (ZAttn) module. Similarly, the continuous decoder consists of a FAttn module and a proposed binary spatio-temporal attention (BAttn) module. STAR-transformer learns an efficient multi-feature representation of the spatio-temporal features by properly arranging pairings of the FAttn, ZAttn, and BAttn modules. Experimental results on the Penn-Action, NTU-RGB+D 60, and 120 datasets show that the proposed method achieves a promising improvement in performance in comparison to previous state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Ahn_STAR-Transformer_A_Spatio-Temporal_Cross_Attention_Transformer_for_Human_Action_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ahn_STAR-Transformer_A_Spatio-Temporal_Cross_Attention_Transformer_for_Human_Action_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030830/,"['Representation learning', 'Computational modeling', 'Pose estimation', 'Transformers', 'Skeleton', 'Data models', 'Trajectory']","['Action Recognition', 'Human Activity Recognition', 'Video Frames', 'Attention Module', 'Recognition Performance', 'Convolutional Neural Network', 'Local Features', 'Feature Maps', 'Deep Learning Models', 'Spatial Dimensions', 'Attention Mechanism', 'Difference In Accuracy', 'Stochastic Gradient Descent', 'Encoder-decoder', 'Global Map', 'Transformer Model', 'Pose Estimation', 'Action Classes', 'Graph Convolutional Network', 'Vision Transformer', 'Cross-view', 'Input Tokens', 'Joint Trajectories', 'Action Recognition Model', 'Deep Learning', 'Evaluation Protocol', 'Spatiotemporal Modulation']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",74,"In action recognition, although the combination of spatiotemporal videos and skeleton features can improve the recognition performance, a separate model and balancing feature representation for cross-modal data are required. To solve these problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can effectively represent two cross-modal features as a recognizable vector. First, from the input video and skeleton sequence, video frames are output as global grid tokens and skeletons are output as joint map tokens, respectively. These tokens are then aggregated into multi-class tokens and input into STAR-transformer. The STAR-transformer encoder consists of a full spatio-temporal attention (FAttn) module and a proposed zigzag spatio-temporal attention (ZAttn) module. Similarly, the continuous decoder consists of a FAttn module and a proposed binary spatio-temporal attention (BAttn) module. STAR-transformer learns an efficient multi-feature representation of the spatio-temporal features by properly arranging pairings of the FAttn, ZAttn, and BAttn modules. Experimental results on the Penn-Action, NTU-RGB+D 60, and 120 datasets show that the proposed method achieves a promising improvement in performance in comparison to previous state-of-the-art methods."
SVD-NAS: Coupling Low-Rank Approximation and Neural Architecture Search,"Zhewen Yu, Christos-Savvas Bouganis",Imperial College London,100,UK,0,,"The task of compressing pre-trained Deep Neural Networks has attracted wide interest of the research community due to its great benefits in freeing practitioners from data access requirements. In this domain, low-rank approximation is a promising method, but existing solutions considered a restricted number of design choices and failed to efficiently explore the design space, which lead to severe accuracy degradation and limited compression ratio achieved. To address the above limitations, this work proposes the SVD-NAS framework that couples the domains of low-rank approximation and neural architecture search. SVD-NAS generalises and expands the design choices of previous works by introducing the Low-Rank architecture space, LR-space, which is a more fine-grained design space of low-rank approximation. Afterwards, this work proposes a gradient-descent-based search for efficiently traversing the LR-space. This finer and more thorough exploration of the possible design choices results in improved accuracy as well as reduction in parameters, FLOPS, and latency of a CNN model. Results demonstrate that the SVD-NAS achieves 2.06-12.85pp higher accuracy on ImageNet than state-of-the-art methods under the data-limited problem settings. SVD-NAS is open-sourced at https://github.com/Yu-Zhewen/SVD-NAS.",https://openaccess.thecvf.com/content/WACV2023/html/Yu_SVD-NAS_Coupling_Low-Rank_Approximation_and_Neural_Architecture_Search_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yu_SVD-NAS_Coupling_Low-Rank_Approximation_and_Neural_Architecture_Search_WACV_2023_paper.pdf,,https://github.com/Yu-Zhewen/SVD-NAS,,main,Poster,https://ieeexplore.ieee.org/document/10030490/,"['Degradation', 'Deep learning', 'Couplings', 'Computer vision', 'Neural networks', 'Computer architecture', 'Space exploration']","['Low-rank Approximation', 'Neural Architecture Search', 'Neural Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Design Choices', 'Design Space', 'Problem Setting', 'Compression Ratio', 'Mean Square Error', 'Training Data', 'Hyperparameters', 'Building Blocks', 'Gradient Descent', 'Convolutional Layers', 'Single Layer', 'Design Parameters', 'Singular Value', 'Singular Value Decomposition', 'Lookup Table', 'Low-rank Tensor', 'Top-1 Accuracy', 'Design Space Exploration', 'Batch Normalization Layer', 'Metrics Of Interest', 'Pointwise Convolution', 'Synthetic Images', 'Accuracy Of Network']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",3,"The task of compressing pre-trained Deep Neural Networks has attracted wide interest of the research community due to its great benefits in freeing practitioners from data access requirements. In this domain, low-rank approximation is a promising method, but existing solutions considered a restricted number of design choices and failed to efficiently explore the design space, which lead to severe accuracy degradation and limited compression ratio achieved. To address the above limitations, this work proposes the SVD-NAS framework that couples the domains of low-rank approximation and neural architecture search. SVD-NAS generalises and expands the design choices of previous works by introducing the Low-Rank architecture space, LR-space, which is a more fine-grained design space of low-rank approximation. Afterwards, this work proposes a gradient-descent-based search for efficiently traversing the LR-space. This finer and more thorough exploration of the possible design choices results in improved accuracy as well as reduction in parameters, FLOPS, and latency of a CNN model. Results demonstrate that the SVD-NAS achieves 2.06-12.85pp higher accuracy on ImageNet than state-of-the-art methods under the data-limited problem setting. SVD-NAS is open-sourced at https://github.com/Yu-Zhewen/SVD-NAS."
Saliency Guided Experience Packing for Replay in Continual Learning,"Gobinda Saha, Kaushik Roy","Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA",100,USA,0,,"Artificial learning systems aspire to mimic human intelligence by continually learning from a stream of tasks without forgetting past knowledge. One way to enable such learning is to store past experiences in the form of input examples in episodic memory and replay them when learning new tasks. However, performance of such method suffers as the size of the memory becomes smaller. In this paper, we propose a new approach for experience replay, where we select the past experiences by looking at the saliency maps which provide visual explanations for the model's decision. Guided by these saliency maps, we pack the memory with only the parts or patches of the input images important for the model's prediction. While learning a new task, we replay these memory patches with appropriate zero-padding to remind the model about its past decisions. We evaluate our algorithm on CIFAR-100, miniImageNet and CUB datasets and report better performance than the state-of-the-art approaches. With qualitative and quantitative analyses we show that our method captures richer summaries of past experiences without any memory increase, and hence performs well with small episodic memory.",https://openaccess.thecvf.com/content/WACV2023/html/Saha_Saliency_Guided_Experience_Packing_for_Replay_in_Continual_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Saha_Saliency_Guided_Experience_Packing_for_Replay_in_Continual_Learning_WACV_2023_paper.pdf,,,2109.04954,main,Poster,https://ieeexplore.ieee.org/document/10030142/,"['Location awareness', 'Learning systems', 'Visualization', 'Statistical analysis', 'Reinforcement learning', 'Learning (artificial intelligence)', 'Predictive models']","['Incremental Learning', 'Performance Of Method', 'Episodic Memory', 'Memory Size', 'Saliency Map', 'Small Memory', 'Past Decisions', 'Experience Replay', 'Past Knowledge', 'Neural Network', 'Fine-tuned', 'Artificial Neural Network', 'Image Classification', 'Correct Predictions', 'Patch Size', 'Part Of The Image', 'Image Patches', 'Computational Overhead', 'Single Pass', 'Coordinate Values', 'Catastrophic Forgetting', 'Patch Quality', 'Patch Selection', 'Shapley Value', 'Exact Placement', 'Accuracy Drop']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"Artificial learning systems aspire to mimic human intelligence by continually learning from a stream of tasks without forgetting past knowledge. One way to enable such learning is to store past experiences in the form of input examples in episodic memory and replay them when learning new tasks. However, performance of such method suffers as the size of the memory becomes smaller. In this paper, we propose a new approach for experience replay, where we select the past experiences by looking at the saliency maps which provide visual explanations for the model’s decision. Guided by these saliency maps, we pack the memory with only the parts or patches of the input images important for the model’s prediction. While learning a new task, we replay these memory patches with appropriate zero-padding to remind the model about its past decisions. We evaluate our algorithm on CIFAR-100, miniImageNet and CUB datasets and report better performance than the state-of-the-art approaches. With qualitative and quantitative analyses we show that our method captures richer summaries of past experiences without any memory increase, and hence performs well with small episodic memory."
Scaling Neural Face Synthesis to High FPS and Low Latency by Neural Caching,"Frank Yu, Sid Fels, Helge Rhodin",University of British Columbia (UBC),100,Canada,0,,"Recent neural rendering approaches greatly improve image quality, reaching near photorealism. However, the underlying neural networks have high runtime, precluding telepresence and virtual reality applications that require high resolution at low latency. The sequential dependency of layers in deep networks makes their optimization difficult. We break this dependency by caching information from the previous frame to speed up the processing of the current one with an implicit warp. The warping with a shallow network reduces latency and the caching operations can further be parallelized to improve the frame rate. In contrast to existing temporal neural networks, ours is tailored for the task of rendering novel views of faces by conditioning on the change of the underlying surface mesh. We test the approach on view-dependent rendering of 3D portrait avatars, as needed for telepresence, on established benchmark sequences. Warping reduces latency by 70% (from 49.4ms to 14.9ms on commodity GPUs) and scales frame rates accordingly over multiple GPUs while reducing image quality by only 1%, making it suitable as part of end-to-end view-dependent 3D teleconferencing applications.",https://openaccess.thecvf.com/content/WACV2023/html/Yu_Scaling_Neural_Face_Synthesis_to_High_FPS_and_Low_Latency_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yu_Scaling_Neural_Face_Synthesis_to_High_FPS_and_Low_Latency_WACV_2023_paper.pdf,,,2211.05773,main,Poster,https://ieeexplore.ieee.org/document/10030371/,"['Image quality', 'Teleconferencing', 'Three-dimensional displays', 'Telepresence', 'Runtime', 'Neural networks', 'Rendering (computer graphics)']","['Low Latency', 'Face Synthesis', 'Neural Face', 'Neural Network', 'Deep Network', 'Image Quality', 'Parallelization', 'Frame Rate', '3D Mesh', 'Shallow Network', 'Previous Frame', 'Deep Network Layers', 'Face View', 'Multiple GPUs', 'Deep Neural Network', 'Facial Expressions', 'Head Motion', 'Image Generation', 'Skip Connections', 'Parallel Execution', 'Current Frame', 'Single GPU', 'View Synthesis', 'Head Model', 'View Direction', 'Spherical Harmonics', 'Talking Head', 'Past Frames', 'High Frame Rate']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Computational photography', 'image and video synthesis']",,"Recent neural rendering approaches greatly improve image quality, reaching near photorealism. However, the underlying neural networks have high runtime, precluding telepresence and virtual reality applications that require high resolution at low latency. The sequential dependency of layers in deep networks makes their optimization difficult. We break this dependency by caching information from the previous frame to speed up the processing of the current one with an implicit warp. The warping with a shallow network reduces latency and the caching operations can further be parallelized to improve the frame rate. In contrast to existing temporal neural networks, ours is tailored for the task of rendering novel views of faces by conditioning on the change of the underlying surface mesh. We test the approach on view-dependent rendering of 3D portrait avatars, as needed for telepresence, on established benchmark sequences. Warping reduces latency by 70% (from 49.4ms to 14.9ms on commodity GPUs) and scales frame rates accordingly over multiple GPUs while reducing image quality by only 1%, making it suitable as part of end-to-end view-dependent 3D teleconferencing applications."
Scaling Novel Object Detection With Weakly Supervised Detection Transformers,"Tyler LaBonte, Yale Song, Xin Wang, Vibhav Vineet, Neel Joshi","Meta AI, FAIR; Georgia Institute of Technology; Microsoft Research",33.33333333,USA,66.66666667,USA,"A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study reveals that class quantity is more important than image quantity for WSOD pretraining.",https://openaccess.thecvf.com/content/WACV2023/html/LaBonte_Scaling_Novel_Object_Detection_With_Weakly_Supervised_Detection_Transformers_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/LaBonte_Scaling_Novel_Object_Detection_With_Weakly_Supervised_Detection_Transformers_WACV_2023_paper.pdf,,,2207.05205,main,Poster,https://ieeexplore.ieee.org/document/10030386/,"['Training', 'Costs', 'Surveillance', 'Object detection', 'Detectors', 'Transformers', 'Data models']","['Object Detection', 'Fine-tuned', 'Knowledge Transfer', 'Learning Framework', 'Bounding Box', 'Rounds Of Refinement', 'Bounding Box Annotations', 'Object Detection Dataset', 'Multiple Instance Learning', 'Large-scale Object', 'Image-level Labels', 'Pre-training Dataset', 'Distinct Features', 'Softmax', 'Number Of Images', 'Transfer Learning', 'Learning Objectives', 'Joint Probability', 'Variety Of Domains', 'Largest Dataset', 'Source Dataset', 'Objective Scores', 'Object Proposals', 'iNaturalist', 'Proposal Generation', 'Faster R-CNN', 'Target Dataset', 'Weight Initialization', 'Joint Objective', 'Position Embedding']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",7,"A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study reveals that class quantity is more important than image quantity for WSOD pretraining."
ScanNeRF: A Scalable Benchmark for Neural Radiance Fields,"Luca De Luigi, Damiano Bolognini, Federico Domeniconi, Daniele De Gregorio, Matteo Poggi, Luigi Di Stefano",University of Bologna; Eyecan.ai and University of Bologna; Eyecan.ai,66.66666667,Italy,33.33333333,USA,"In this paper, we propose the first-ever real benchmark thought for evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering (NR) frameworks. We design and implement an effective pipeline for scanning real objects in quantity and effortlessly. Our scan station is built with less than 500 hardware budget and can collect roughly 4000 images of a scanned object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset characterized by several train/val/test splits aimed at benchmarking the performance of modern NeRF methods under different conditions. Accordingly, we evaluate three cutting-edge NeRF variants on it to highlight their strengths and weaknesses. The dataset is available on our project page, together with an online benchmark to foster the development of better and better NeRFs.",https://openaccess.thecvf.com/content/WACV2023/html/De_Luigi_ScanNeRF_A_Scalable_Benchmark_for_Neural_Radiance_Fields_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/De_Luigi_ScanNeRF_A_Scalable_Benchmark_for_Neural_Radiance_Fields_WACV_2023_paper.pdf,https://eyecan-ai.github.io/scannerf/,,2211.13762,main,Poster,https://ieeexplore.ieee.org/document/10030989/,"['Computer vision', 'Pipelines', 'Benchmark testing', 'Rendering (computer graphics)', 'Hardware', 'Faces']","['Neural Radiance Fields', 'Neural Network', 'Image Acquisition', '3D Reconstruction', 'Multilayer Perceptron', 'Training Images', 'Virtual World', 'Azimuth Angle', 'Hash Function', 'Peak Signal-to-noise Ratio', 'Zenith Angle', 'Explicit Representation', 'Camera Pose', 'Digital Twin', 'Pixel Color', 'Amount Of Images', 'Voxel Grid', 'Implicit Representation', 'Neural Field', 'Multi-view Stereo']","['Applications: Virtual/augmented reality', 'Computational photography', 'image and video synthesis']",9,"In this paper, we propose the first-ever real benchmark thought for evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering (NR) frameworks. We design and implement an effective pipeline for scanning real objects in quantity and effortlessly. Our scan station is built with less than 500$ hardware budget and can collect roughly 4000 images of a scanned object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset characterized by several train/val/test splits aimed at benchmarking the performance of modern NeRF methods under different conditions. Accordingly, we evaluate three cuttingedge NeRF variants on it to highlight their strengths and weaknesses. The dataset is available on our project page, together with an online benchmark to foster the development of better and better NeRFs."
ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification,"Thomas Stegmüller, Behzad Bozorgtabar, Antoine Spahr, Jean-Philippe Thiran","EPFL, Switzerland; EPFL, Switzerland; CHUV, Switzerland; CIBM, Switzerland",75,Switzerland,25,Switzerland,"Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning MIL to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and current MIL-based approaches often process images uniformly, discarding the inter-patches interactions. To alleviate these issues, we propose ScoreNet, a new efficient transformer that exploits a differentiable recommendation stage to extract discriminative image regions and dedicate computational resources accordingly. The proposed transformer leverages the local and global attention of a few dynamically recommended high-resolution regions at an efficient computational cost. We further introduce a novel mixing data-augmentation, namely ScoreMix, by leveraging the image's semantic distribution to guide the data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly simple and mitigates the pitfalls of previous augmentations, which assume a uniform semantic distribution and risk mislabeling the samples. Thorough experiments and ablation studies on three breast cancer histology datasets of Haematoxylin & Eosin (H&E) have validated the superiority of our approach over prior arts, including transformer-based models on tumour regions-of-interest TRoIs classification. ScoreNet equipped with proposed ScoreMix augmentation demonstrates better generalization capabilities and achieves new state-of-the-art (SOTA) results with only 50% of the data compared to other mixing augmentation variants. Finally, ScoreNet yields high efficacy and outperforms SOTA efficient transformers, namely TransPath and SwinTransformer, with throughput around 3x and 4x higher than the aforementioned architectures, respectively.",https://openaccess.thecvf.com/content/WACV2023/html/Stegmuller_ScoreNet_Learning_Non-Uniform_Attention_and_Augmentation_for_Transformer-Based_Histopathological_Image_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Stegmuller_ScoreNet_Learning_Non-Uniform_Attention_and_Augmentation_for_Transformer-Based_Histopathological_Image_WACV_2023_paper.pdf,,https://github.com/stegmuel/ScoreNet,,main,Poster,https://ieeexplore.ieee.org/document/10030379/,"['Computer vision', 'Costs', 'Histopathology', 'Semantics', 'Computer architecture', 'Transformers', 'Throughput']","['Histopathological Images', 'Histopathological Image Classification', 'High-resolution Images', 'Image Regions', 'Data Augmentation', 'Generalization Capability', 'Transformation Efficiency', 'Relevant Regions', 'Digital Pathology', 'Pathological Images', 'Local Attention', 'Multiple Instance Learning', 'Tumor Regions Of Interest', 'Differences In Outcomes', 'Factorization', 'Convolutional Neural Network', 'Contextual Information', 'Attention Mechanism', 'Bounding Box', 'Downscaling', 'Vision Transformer', 'Cut-and-paste', 'Self-supervised Learning Methods', 'Ductal Carcinoma In Situ', 'Self-supervised Learning', 'Convolutional Neural Networks Backbone', 'Graph Neural Networks', 'Transformer Model', 'Representation Learning', 'First Row Of Matrix']","['Applications: Biomedical/healthcare/medicine', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",21,"Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning (MIL) to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and current MIL-based approaches often process images uniformly, discarding the inter-patches interactions. To alleviate these issues, we propose ScoreNet, a new efficient transformer that exploits a differentiable recommendation stage to extract discriminative image regions and dedicate computational resources accordingly. The proposed transformer leverages the local and global attention of a few dynamically recommended high-resolution regions at an efficient computational cost. We further introduce a novel mixing data-augmentation, namely ScoreMix, by leveraging the image’s semantic distribution to guide the data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly simple and mitigates the pitfalls of previous augmentations, which assume a uniform semantic distribution and risk mislabeling the samples. Thorough experiments and ablation studies on three breast cancer histology datasets of Haematoxylin & Eosin (H&E) have validated the superiority of our approach over prior arts, including transformer-based models on tumour regions-of-interest (TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation demonstrates better generalization capabilities and achieves new state-of-the-art (SOTA) results with only 50% of the data compared to other mixing augmentation variants. Finally, ScoreNet yields high efficacy and outperforms SOTA efficient transformers, namely TransPath [37] and SwinTransformer [20], with throughput around 3× and 4× higher than the aforementioned architectures, respectively. Our code is publicly available
<sup>1</sup>
."
SeCo: Separating Unknown Musical Visual Sounds With Consistency Guidance,"Xinchi Zhou, Dongzhan Zhou, Wanli Ouyang, Hang Zhou, Di Hu","Gaoling School of Artificial Intelligence, Renmin University of China; Baidu Inc.; The University of Sydney",66.66666667,"Australia, China",33.33333333,China,"Recent years have witnessed the success of deep learning on the visual sound separation task. However, existing works follow similar settings where the training and testing datasets share the same musical instrument categories, which to some extent limits the versatility of this task. In this work, we focus on a more general and challenging scenario, namely the separation of unknown musical instruments, where the categories in training and testing phases have no overlap with each other. To tackle this new setting, we propose the ""Separation-with-Consistency"" (SeCo) framework, which can accomplish the separation on unknown categories by exploiting the consistency constraints. Furthermore, to capture richer characteristics of the novel melodies, we devise an online matching strategy, which can bring stable enhancements with no cost of extra parameters. Experiments demonstrate that our SeCo framework exhibits strong adaptation ability on the novel musical categories and outperforms the baseline methods by a notable margin.",https://openaccess.thecvf.com/content/WACV2023/html/Zhou_SeCo_Separating_Unknown_Musical_Visual_Sounds_With_Consistency_Guidance_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhou_SeCo_Separating_Unknown_Musical_Visual_Sounds_With_Consistency_Guidance_WACV_2023_paper.pdf,,,2203.13535,main,Poster,https://ieeexplore.ieee.org/document/10030869/,"['Training', 'Deep learning', 'Visualization', 'Computer vision', 'Costs', 'Instruments', 'System performance']","['Musical Sounds', 'Deep Learning', 'Visual Task', 'Baseline Methods', 'Musical Instruments', 'Adaptive Ability', 'Extra Parameters', 'Challenging Scenarios', 'Consistency Constraint', 'Training Categories', 'Direct Overlap', 'Training Set', 'Visual Cues', 'Visual Features', 'Video Clips', 'New Instrument', 'Visual Modality', 'Optical Flow', 'Types Of Instruments', 'Signal Separation', 'Consistency Loss', 'Mixed Signals', 'Effective Guidance', 'Short-time Fourier Transform', 'Signal-to-interference Ratio', 'Zero-shot', 'Visual Guidance', 'Auditory Modality', 'Self-supervised Manner', 'Motion Information']",['Algorithms: Vision + language and/or other modalities'],2,"Recent years have witnessed the success of deep learning on the visual sound separation task. However, existing works follow similar settings where the training and testing datasets share the same musical instrument categories, which to some extent limits the versatility of this task. In this work, we focus on a more general and challenging scenario, namely the separation of unknown musical instruments, where the categories in training and testing phases have no direct overlap with each other. To tackle this new setting, we propose the ""Separation-with-Consistency"" (SeCo) framework, which can accomplish the separation on unknown categories by exploiting the consistency constraints. Furthermore, to capture richer characteristics of the novel melodies, we devise an online matching strategy, which can bring stable enhancements with no cost of extra parameters. Experiments demonstrate that our SeCo framework exhibits strong adaptation ability on the novel musical categories and outperforms the baseline methods by a notable margin."
Searching Efficient Neural Architecture With Multi-Resolution Fusion Transformer for Appearance-Based Gaze Estimation,"Vikrant Nagpure, Kenji Okuma","Honda Motor Co., Ltd., Tokyo, Japan",0,,100,Japan,"For aiming at a more accurate appearance-based gaze estimation, a series of recent works propose to use transformers or high-resolution networks in several ways which achieve state-of-the-art, but such works lack efficiency for real-time applications on edge computing devices. In this paper, we propose a compact model to precisely and efficiently solve gaze estimation. The proposed model includes 1) a Neural Architecture Search(NAS)-based multi-resolution feature extractor for extracting feature maps with global and local information which are essential for this task and 2) a novel multi-resolution fusion transformer as the gaze estimation head for efficiently estimating gaze values by fusing the extracted feature maps. We search our proposed model, called GazeNAS-ETH, on the ETH-XGaze dataset. We confirmed through experiments that GazeNAS-ETH achieved state-of-the-art on Gaze360, MPIIFaceGaze, RTGENE, and EYEDIAP datasets, while having only about 1M parameters and using only 0.28 GFLOPs, which is significantly less compared to previous state-of-the-art models, making it easier to deploy for real-time applications.",https://openaccess.thecvf.com/content/WACV2023/html/Nagpure_Searching_Efficient_Neural_Architecture_With_Multi-Resolution_Fusion_Transformer_for_Appearance-Based_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nagpure_Searching_Efficient_Neural_Architecture_With_Multi-Resolution_Fusion_Transformer_for_Appearance-Based_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030991/,"['Fuses', 'Computational modeling', 'Estimation', 'Computer architecture', 'Feature extraction', 'Transformers', 'Real-time systems']","['Neural Architecture', 'Gaze Estimation', 'Efficient Neural Architecture', 'Appearance-based Gaze Estimation', 'Feature Maps', 'Global Information', 'Neural Architecture Search', 'Extracted Feature Maps', 'Computational Cost', 'Convolutional Neural Network', 'Search Space', 'Face Images', 'Feature Fusion', 'Input Channels', 'Evaluation Dataset', 'Resolution Feature', 'Eye Images', 'Branch Network', 'High-resolution Features', 'Stage Of Network', 'Vision Transformer', 'Head Pose', 'Transformer Encoder', 'Parallel Modules', 'Transformer Architecture', 'Low-resolution Feature', 'Depthwise Convolution', 'Human Gaze', 'Object Detection', 'Simple Concatenation']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose']",15,"For aiming at a more accurate appearance-based gaze estimation, a series of recent works propose to use transformers or high-resolution networks in several ways which achieve state-of-the-art, but such works lack efficiency for real-time applications on edge computing devices. In this paper, we propose a compact model to precisely and efficiently solve gaze estimation. The proposed model includes 1) a Neural Architecture Search(NAS)-based multi-resolution feature extractor for extracting feature maps with global and local information which are essential for this task and 2) a novel multi-resolution fusion transformer as the gaze estimation head for efficiently estimating gaze values by fusing the extracted feature maps. We search our proposed model, called GazeNAS-ETH, on the ETH-XGaze dataset. We confirmed through experiments that GazeNAS-ETH achieved state-of-the-art on Gaze360, MPIIFaceGaze, RTGENE, and EYEDIAP datasets, while having only about 1M parameters and using only 0.28 GFLOPs, which is significantly less compared to previous state-of-the-art models, making it easier to deploy for real-time applications."
Searching for Robust Binary Neural Networks via Bimodal Parameter Perturbation,"Daehyun Ahn, Hyungjun Kim, Taesu Kim, Eunhyeok Park, Jae-Joon Kim",Pohang University of Science and Technology; Seoul National University; SqueezeBits Inc.,66.66666667,South Korea,33.33333333,USA,"Binary neural networks (BNNs) are advantageous in performance and memory footprint but suffer from low accuracy due to their limited expression capability. Recent works have tried to enhance the accuracy of BNNs via a gradient-based search algorithm and showed promising results. However, the mixture of architecture search and binarization induce the instability of the search process, resulting in convergence to the suboptimal point. To address this issue, we propose a BNN architecture search framework with bimodal parameter perturbation. The bimodal parameter perturbation can improve the stability of gradient-based architecture search by reducing the sharpness of the loss surface along both weight and architecture parameter axes. In addition, we refine the inverted bottleneck convolution block for having robustness with BNNs. The synergy of the refined space and the stabilized search process allows us to find out the accurate BNNs with high computation efficiency. Experimental results show that our framework finds the best architecture on CIFAR-100 and ImageNet datasets in the existing search space for BNNs. We also tested our framework on another search space based on the inverted bottleneck convolution block, and the selected BNN models using our approach achieved the highest accuracy on both datasets with a much smaller number of equivalent operations than previous works.",https://openaccess.thecvf.com/content/WACV2023/html/Ahn_Searching_for_Robust_Binary_Neural_Networks_via_Bimodal_Parameter_Perturbation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ahn_Searching_for_Robust_Binary_Neural_Networks_via_Bimodal_Parameter_Perturbation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030986/,"['Gradient methods', 'Convolution', 'Perturbation methods', 'Neural networks', 'Computer architecture', 'Search problems', 'Stability analysis']","['Neural Network', 'Bimodal', 'Perturbation Parameter', 'Binary Network', 'Binary Neural Networks', 'Search Algorithm', 'Search Space', 'Weight Parameters', 'ImageNet Dataset', 'Architecture Parameters', 'CIFAR-100 Dataset', 'Learning Rate', 'Convolutional Layers', 'Adam Optimizer', 'Random Noise', 'Weight Decay', 'Convolution Operation', 'Difference In Loss', 'Floating-point Operations', 'Hypervolume', 'Neural Architecture Search', 'Partial Duplication', 'Gradient-based Methods', 'Learning Rate Schedule', 'Depthwise Convolution', 'Instability Problem', 'Perturbation Conditions', 'Binary Search', 'Backbone Architecture', 'Projected Gradient Descent']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",1,"Binary neural networks (BNNs) are advantageous in performance and memory footprint but suffer from low accuracy due to their limited expression capability. Recent works have tried to enhance the accuracy of BNNs via a gradient-based search algorithm and showed promising results. However, the mixture of architecture search and binarization induce the instability of the search process, resulting in convergence to the suboptimal point. To address this issue, we propose a BNN architecture search framework with bimodal parameter perturbation. The bimodal parameter perturbation can improve the stability of gradient-based architecture search by reducing the sharpness of the loss surface along both weight and architecture parameter axes. In addition, we refine the inverted bottleneck convolution block for having robustness with BNNs. The synergy of the refined space and the stabilized search process allows us to find out the accurate BNNs with high computation efficiency. Experimental results show that our framework finds the best architecture on CIFAR-100 and ImageNet datasets in the existing search space for BNNs. We also tested our framework on another search space based on the inverted bottleneck convolution block, and the selected BNN models using our approach achieved the highest accuracy on both datasets with a much smaller number of equivalent operations than previous works."
Seg&Struct: The Interplay Between Part Segmentation and Structure Inference for 3D Shape Parsing,"Jeonghyun Kim, Kaichun Mo, Minhyuk Sung, Woontack Woo",KAIST; Stanford University,100,"South Korea, USA",0,,"We propose Seg&Struct, a supervised learning framework leveraging the interplay between part segmentation and structure inference and demonstrating their synergy in an integrated framework. Both part segmentation and structure inference have been extensively studied in the recent deep learning literature, while the supervisions used for each task have not been fully exploited to assist the other task. Namely, structure inference has been typically conducted with an autoencoder that does not leverage the point-to-part associations. Also, segmentation has been mostly performed without structural priors that tell the plausibility of the output segments. We present how these two tasks can be best combined while fully utilizing supervision to improve performance. Our framework first decomposes a raw input shape into part segments using an off-the-shelf algorithm, whose outputs are then mapped to nodes in a part hierarchy, establishing point-to-part associations. Following this, ours predicts the structural information, e.g., part bounding boxes and part relationships. Lastly, the segmentation is rectified by examining the confusion of part boundaries using the structure-based part features. Our experimental results based on the StructureNet and PartNet demonstrate that the interplay between the two tasks results in remarkable improvements in both tasks: 27.91% in structure inference and 0.5% in segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Kim_SegStruct_The_Interplay_Between_Part_Segmentation_and_Structure_Inference_for_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_SegStruct_The_Interplay_Between_Part_Segmentation_and_Structure_Inference_for_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030476/,"['Deep learning', 'Computer vision', 'Three-dimensional displays', 'Shape', 'Supervised learning', 'Prediction algorithms', 'Task analysis']","['3D Shape', 'Part Segmentation', 'Structure Inference', 'Bounding Box', 'Raw Input', 'Segmentation Output', 'Input Shape', 'Decoding', 'Computer Vision', 'Quantitative Evaluation', 'Structure Prediction', 'Intersection Over Union', 'Point Cloud', 'Shape Structure', 'Leaf Node', 'Candidate Features', '3D Segmentation', 'Structural Hierarchy', 'Hierarchical Representation', '3D Graph', 'Input Geometry', 'Latent Code', 'Merge Operation', '3D Parts', 'Intersection Over Union Threshold', 'Feature Merging', 'Backbone Segments', 'Root Node', 'Part Geometry', 'Previous Step']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"We propose Seg&Struct, a supervised learning framework leveraging the interplay between part segmentation and structure inference and demonstrating their synergy in an integrated framework. Both part segmentation and structure inference have been extensively studied in the recent deep learning literature, while the supervisions used for each task have not been fully exploited to assist the other task. Namely, structure inference has been typically conducted with an autoencoder that does not lever-age the point-to-part associations. Also, segmentation has been mostly performed without structural priors that tell the plausibility of the output segments. We present how these two tasks can be best combined while fully utilizing super-vision to improve performance. Our framework first decomposes a raw input shape into part segments using an off-the-shelf algorithm, whose outputs are then mapped to nodes in a part hierarchy, establishing point-to-part associations. Following this, ours predicts the structural information, e.g., part bounding boxes and part relationships. Lastly, the segmentation is rectified by examining the confusion of part boundaries using the structure-based part features. Our experimental results based on the StructureNet and PartNet demonstrate that the interplay between two tasks results in remarkable improvements in both tasks: 27.91% in structure inference and 0.5% in segmentation."
Segmentation-Free Direct Iris Localization Networks,"Takahiro Toizumi, Koichi Takahashi, Masato Tsukada","NEC corporation, University of Tsukuba; NEC corporation",50,Japan,50,Japan,"This paper proposes an efficient iris localization method without using iris segmentation and circle fitting. Conventional iris localization methods first extract iris regions by using semantic segmentation methods such as U-Net. Afterward, the inner and outer iris circles are localized using the traditional circle fitting algorithm. However, this approach requires high-resolution encoder-decoder networks for iris segmentation, so it causes computational costs to be high. In addition, traditional circle fitting tends to be sensitive to noise in input images and fitting parameters, causing the iris recognition performance to be poor. To solve these problems, we propose an iris localization network (ILN), that can directly localize pupil and iris circles with eyelid points from a low-resolution iris image. We also introduce a pupil refinement network (PRN) to improve the accuracy of pupil localization. Experimental results show that the combination of ILN and PRN works in 34.5 ms for one iris image on a CPU, and its localization performance outperforms conventional iris segmentation methods. In addition, generalized evaluation results show that the proposed method has higher robustness for datasets in different domain than other segmentation methods. Furthermore, we also confirm that the proposed ILN and PRN improve the iris recognition accuracy.",https://openaccess.thecvf.com/content/WACV2023/html/Toizumi_Segmentation-Free_Direct_Iris_Localization_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Toizumi_Segmentation-Free_Direct_Iris_Localization_Networks_WACV_2023_paper.pdf,,,2210.10403,main,Poster,https://ieeexplore.ieee.org/document/10030850/,"['Location awareness', 'Image segmentation', 'Computer vision', 'Semantic segmentation', 'Fitting', 'Eyelids', 'Detectors']","['Input Image', 'Localization Accuracy', 'Localization Performance', 'Segmentation Method', 'Recognition Performance', 'Outer Circle', 'Semantic Segmentation Methods', 'Performance Of Method', 'Aspect Ratio', 'Horizontal Axis', 'Object Detection', 'Data Augmentation', 'Generalization Performance', 'Stochastic Gradient Descent', 'Bounding Box', 'Output Vector', 'Segmentation Map', 'CNN-based Methods', 'Eyelashes', 'Hausdorff Distance', 'Region Of Interest Image', 'Landmark Detection', 'Equal Error Rate', 'Original Input Image', 'Rubber Sheet', 'Network Width', 'Horizontal Shift', 'Segmentation Results', 'Center Of The Circle', 'Biometric']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"This paper proposes an efficient iris localization method without using iris segmentation and circle fitting. Conventional iris localization methods first extract iris regions by using semantic segmentation methods such as U-Net. Afterward, the inner and outer iris circles are localized using the traditional circle fitting algorithm. However, this approach requires high-resolution encoder-decoder networks for iris segmentation, so it causes computational costs to be high. In addition, traditional circle fitting tends to be sensitive to noise in input images and fitting parameters, causing the iris recognition performance to be poor. To solve these problems, we propose an iris localization network (ILN), that can directly localize pupil and iris circles with eyelid points from a low-resolution iris image. We also introduce a pupil refinement network (PRN) to improve the accuracy of pupil localization. Experimental results show that the combination of ILN and PRN works in 34.5 ms for one iris image on a CPU, and its localization performance out-performs conventional iris segmentation methods. In addition, generalized evaluation results show that the proposed method has higher robustness for datasets in different domain than other segmentation methods. Furthermore, we also confirm that the proposed ILN and PRN improve the iris recognition accuracy."
"Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation","Aadarsh Sahoo, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das",IIT Kharagpur; MIT-IBM Watson AI Lab; Boston University,100,"India, USA",0,,"Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability, and domain invariance in the latent space. To alleviate the above issues, we develop a novel 'Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. First, we present an efficient ""select"" module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the ""label"" module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space. Finally, the ""mix"" module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed framework over state-of-the-art methods. Project page: https://cvir.github.io/projects/slm.",https://openaccess.thecvf.com/content/WACV2023/html/Sahoo_Select_Label_and_Mix_Learning_Discriminative_Invariant_Feature_Representations_for_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sahoo_Select_Label_and_Mix_Learning_Discriminative_Invariant_Feature_Representations_for_WACV_2023_paper.pdf,https://cvir.github.io/projects/slm,,2012.03358,main,Poster,https://ieeexplore.ieee.org/document/10030127/,"['Computer vision', 'Benchmark testing']","['Domain Adaptation', 'Partial Adaptation', 'Invariant Representation', 'Discriminative Feature Representation', 'Discriminative Feature Learning', 'Partial Domain Adaptation', 'Latent Space', 'Target Domain', 'Source Domain', 'Intrinsic Structure', 'Label Space', 'Negative Transfer', 'Lack Of Discrimination', 'Source Domain Data', 'Labeled Source Domain', 'Average Accuracy', 'Target Sample', 'Selective Modulators', 'Domain Classifier', 'Mixed Strategy', 'Domain Discriminator', 'Source Domain Samples', 'Network Selection', 'Challenging Dataset', 'Target Domain Samples', 'Target Domain Images', 'Unlabeled Target Domain', 'Hausdorff Distance', 'Relevant Samples', 'Triplet Loss']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",6,"Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability, and domain invariance in the latent space. To alleviate the above issues, we develop a novel ‘Select, Label, and Mix’ (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. First, we present an efficient ""select"" module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the ""label"" module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space. Finally, the ""mix"" module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation. Extensive experiments on several benchmark datasets for partial domain adaptation demonstrate the superiority of our proposed framework over state-of-the-art methods. Project page: https://cvir.github.io/projects/slm."
Self Supervised Low Dose Computed Tomography Image Denoising Using Invertible Network Exploiting Inter Slice Congruence,"Sutanu Bera, Prabir Kumar Biswas","Indian Institute of Technology Kharagpur, India",100,India,0,,"The resurgence of deep neural networks has created an alternative pathway for low-dose computed tomography denoising by learning a nonlinear transformation function between low-dose CT (LDCT) and normal-dose CT (NDCT) image pairs. However, those paired LDCT and NDCT images are rarely available in the clinical environment, making deep neural network deployment infeasible. This study proposes a novel method for self-supervised low-dose CT denoising to alleviate the requirement of paired LDCT and NDCT images. Specifically, we have trained an invertible neural network to minimize the pixel-based mean square distance between a noisy slice and the average of its two immediate adjacent noisy slices. We have shown the aforementioned is similar to training a neural network to minimize the distance between clean NDCT and noisy LDCT image pairs. Again, during the reverse mapping of the invertible network, the output image is mapped to the original input image, similar to cycle consistency loss. Finally, the trained invertible network's forward mapping is used for denoising LDCT images. Extensive experiments on two publicly available datasets showed that our method performs favourably against other existing unsupervised methods.",https://openaccess.thecvf.com/content/WACV2023/html/Bera_Self_Supervised_Low_Dose_Computed_Tomography_Image_Denoising_Using_Invertible_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bera_Self_Supervised_Low_Dose_Computed_Tomography_Image_Denoising_Using_Invertible_WACV_2023_paper.pdf,,,2211.01618,main,Poster,https://ieeexplore.ieee.org/document/10030355/,"['Training', 'Deep learning', 'Computer vision', 'Correlation', 'Computed tomography', 'Noise reduction', 'Neural networks']","['Computed Tomography Images', 'Low-dose Computed Tomography', 'Low-dose Computed Tomography Images', 'Invertible Network', 'Neural Network', 'Deep Network', 'Deep Neural Network', 'Input Image', 'Image Pairs', 'Unsupervised Methods', 'Output Image', 'Noisy Images', 'Forward Mapping', 'Cycle Consistency Loss', 'Reverse Mapping', 'Feature Maps', 'Super-resolution', 'Bounding Box', 'Original Signal', 'Noise Variance', 'Structural Similarity Index Measure', 'Visualization Of Lesions', 'Clear Image', 'Noisy Signal', 'Granular Pattern', 'Noisy Observations', 'Denoising Methods', 'Computed Tomography Slices']","['Applications: Biomedical/healthcare/medicine', 'Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"The resurgence of deep neural networks has created an alternative pathway for low-dose computed tomography denoising by learning a nonlinear transformation function between low-dose CT (LDCT) and normal-dose CT (NDCT) image pairs. However, those paired LDCT and NDCT images are rarely available in the clinical environment, making deep neural network deployment infeasible. This study proposes a novel method for self-supervised low-dose CT denoising to alleviate the requirement of paired LDCT and NDCT images. Specifically, we have trained an invertible neural network to minimize the pixel-based mean square distance between a noisy slice and the average of its two immediate adjacent noisy slices. We have shown the aforementioned is similar to training a neural network to minimize the distance between clean NDCT and noisy LDCT image pairs. Again, during the reverse mapping of the invertible network, the output image is mapped to the original input image, similar to cycle consistency loss. Finally, the trained invertible network’s forward mapping is used for denoising LDCT images. Extensive experiments on two publicly available datasets showed that our method performs favourably against other existing unsupervised methods."
Self-Attention Message Passing for Contrastive Few-Shot Learning,"Ojas Kishorkumar Shirekar, Anuj Singh, Hadi Jamali-Rad","Delft University of Technology (TU Delft), The Netherlands; Shell Global Solutions International B.V., Amsterdam, The Netherlands",100,Netherlands,0,,"Humans have a unique ability to learn new representations from just a handful of examples with little to no supervision. Deep learning models, however, require an abundance of data and supervision to perform at a satisfactory level. Unsupervised few-shot learning (U-FSL) is the pursuit of bridging this gap between machines and humans. Inspired by the capacity of graph neural networks (GNNs) in discovering complex inter-sample relationships, we propose a novel self-attention based message passing contrastive learning approach (coined as SAMP-CLR) for U-FSL pre-training. We also propose an optimal transport (OT) based fine-tuning strategy (we call OpT-Tune) to efficiently induce task awareness into our novel end-to-end unsupervised few-shot classification framework (SAMPTransfer). Our extensive experimental results corroborate the efficacy of SAMPTransfer in a variety of downstream few-shot classification scenarios, setting a new state-of-the-art for U-FSL on both miniImageNet and tieredImageNet benchmarks, offering up to 7%+ and 5%+ improvements, respectively. Our further investigations also confirm that SAMPTransfer remains on-par with some supervised baselines on miniImageNet and outperforms all existing U-FSL baselines in a challenging cross-domain scenario.",https://openaccess.thecvf.com/content/WACV2023/html/Shirekar_Self-Attention_Message_Passing_for_Contrastive_Few-Shot_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shirekar_Self-Attention_Message_Passing_for_Contrastive_Few-Shot_Learning_WACV_2023_paper.pdf,,https://github.com/ojss/SAMPTransfer/,2210.06339,main,Poster,https://ieeexplore.ieee.org/document/10030742/,"['Deep learning', 'Computer vision', 'Codes', 'Message passing', 'Benchmark testing', 'Graph neural networks', 'Data models']","['Self-supervised Learning', 'Few-shot Learning', 'Message Passing', 'Benchmark', 'Deep Learning Models', 'Optimal Transport', 'Graph Neural Networks', 'Few-shot Classification', 'Handful Of Examples', 'Data Structure', 'Transfer Learning', 'Latent Space', 'Source Images', 'Matrix M', 'Node Features', 'Linear Layer', 'Crop Diseases', 'Base Classes', 'Contrastive Loss', 'Support Set', 'Attention Heads', 'Metric Learning', 'Pre-training Phase', 'Query Set', 'Convolutional Features', 'Batch Of Images', 'Query Sample', 'Cost Matrix', 'Graph Attention', 'Unsupervised Methods']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",1,"Humans have a unique ability to learn new representations from just a handful of examples with little to no supervision. Deep learning models, however, require an abundance of data and supervision to perform at a satisfactory level. Unsupervised few-shot learning (U-FSL) is the pursuit of bridging this gap between machines and humans. Inspired by the capacity of graph neural networks (GNNs) in discovering complex inter-sample relationships, we propose a novel self-attention based message passing contrastive learning approach (coined as SAMP-CLR) for U-FSL pre-training. We also propose an optimal transport (OT) based fine-tuning strategy (we call OpT-Tune) to efficiently induce task awareness into our novel end-to-end unsupervised few-shot classification framework (SAMPTransfer). Our extensive experimental results corroborate the efficacy of SAMPTransferin a variety of downstream few-shot classification scenarios, setting a new state-of-the-art for U-FSL on both miniImageNet and tieredImageNet benchmarks, offering up to 7%+ and 5%+ improvements, respectively. Our further investigations also confirm that SAMPTransferremains on-par with some supervised baselines on miniImageNet and outperforms all existing U-FSL baselines in a challenging cross-domain scenario. Our code can be found in our GitHub repository: https://github.com/ojss/SAMPTransfer/."
Self-Attentive Pooling for Efficient Deep Learning,"Fang Chen, Gourav Datta, Souvik Kundu, Peter A. Beerel","Intel Labs, USA; Universiy of Southern California, Los Angeles, USA",50,USA,50,USA,"Efficient custom pooling techniques that can aggressively trim the dimensions of a feature map for resource-constrained computer vision applications have recently gained significant traction. However, prior pooling works extract only the local context of the activation maps, limiting their effectiveness. In contrast, we propose a novel non-local self-attentive pooling method that can be used as a drop-in replacement to the standard pooling layers, such as max/average pooling or strided convolution. The proposed self-attention module uses patch embedding, multi-head self-attention, and spatial-channel restoration, followed by sigmoid activation and exponential soft-max. This self-attention mechanism efficiently aggregates dependencies between non-local activation patches during down-sampling. Extensive experiments on standard object classification and detection tasks with various convolutional neural network (CNN) architectures demonstrate the superiority of our proposed mechanism over the state-of-the-art (SOTA) pooling techniques. In particular, we surpass the test accuracy of existing pooling techniques on different variants of MobileNet-V2 on ImageNet by an average of 1.2%. With the aggressive down-sampling of the activation maps in the initial layers (providing up to 22x reduction in memory consumption), our approach achieves 1.43% higher test accuracy compared to SOTA techniques with iso-memory footprints. This enables the deployment of our models in memory-constrained devices, such as micro-controllers without losing significant accuracy, because the initial activation maps consume a significant amount of on-chip memory for high-resolution images required for complex vision tasks. Our pooling method also leverages channel pruning to further reduce memory footprints. Codes are available at https://github.com/C-Fun/Non-Local-Pooling.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Self-Attentive_Pooling_for_Efficient_Deep_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Self-Attentive_Pooling_for_Efficient_Deep_Learning_WACV_2023_paper.pdf,,https://github.com/C-Fun/Non-Local-Pooling,2209.07659,main,Poster,https://ieeexplore.ieee.org/document/10030316/,"['Deep learning', 'Computer vision', 'Limiting', 'Memory management', 'Feature extraction', 'System-on-chip', 'Image restoration']","['Convolutional Neural Network', 'Feature Maps', 'Test Accuracy', 'Object Detection', 'ImageNet', 'Detection Task', 'Pooling Layer', 'Activation Maps', 'Convolutional Neural Network Architecture', 'Sigmoid Activation', 'Memory Consumption', 'Amount Of Memory', 'Self-attention Mechanism', 'Memory Footprint', 'Multi-head Self-attention', 'Self-attention Module', 'On-chip Memory', 'Strided Convolution', 'Local Information', 'Convolutional Layers', 'Pooling Approach', 'Convolutional Neural Networks Backbone', 'Batch Normalization Layer', 'Local Pool', 'Backbone Network', 'Feature Aggregation', 'Large Receptive Field', 'Exponential Function', 'Sigmoid Function', 'Convolutional Neural Network Model']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Embedded sensing/real-time techniques']",4,"Efficient custom pooling techniques that can aggressively trim the dimensions of a feature map for resource-constrained computer vision applications have recently gained significant traction. However, prior pooling works extract only the local context of the activation maps, limiting their effectiveness. In contrast, we propose a novel non-local self-attentive pooling method that can be used as a drop-in replacement to the standard pooling layers, such as max/average pooling or strided convolution. The proposed self-attention module uses patch embedding, multihead self-attention, and spatial-channel restoration, followed by sigmoid activation and exponential soft-max. This self-attention mechanism efficiently aggregates dependencies between non-local activation patches during downsampling. Extensive experiments on standard object classification and detection tasks with various convolutional neural network (CNN) architectures demonstrate the superiority of our proposed mechanism over the state-of-the-art (SOTA) pooling techniques. In particular, we surpass the test accuracy of existing pooling techniques on different variants of MobileNet-V2 on ImageNet by an average of ~1.2%. With the aggressive down-sampling of the activation maps in the initial layers (providing up to 22x reduction in memory consumption), our approach achieves 1.43% higher test accuracy compared to SOTA techniques with iso-memory footprints. This enables the deployment of our models in memory-constrained devices, such as micro-controllers (without losing significant accuracy), because the initial activation maps consume a significant amount of on-chip memory for high-resolution images required for complex vision tasks. Our pooling method also leverages channel pruning to further reduce memory footprints. Codes are available at https://github.com/CFun/Non-Local-Pooling."
Self-Distillation for Unsupervised 3D Domain Adaptation,"Adriano Cardace, Riccardo Spezialetti, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano","Department of Computer Science and Engineering (DISI), University of Bologna, Italy",100,Italy,0,,"Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Cardace_Self-Distillation_for_Unsupervised_3D_Domain_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cardace_Self-Distillation_for_Unsupervised_3D_Domain_Adaptation_WACV_2023_paper.pdf,,,2210.08226,main,Poster,https://ieeexplore.ieee.org/document/10030179/,"['Point cloud compression', 'Training', 'Computer vision', 'Three-dimensional displays', 'Art', 'Benchmark testing', 'Graph neural networks']","['Domain Adaptation', 'Neural Network', 'Feature Space', 'Training Time', 'Point Cloud', 'Target Domain', 'Source Domain', 'Graph Neural Networks', 'Part Segmentation', 'Feature Alignment', 'Auxiliary Task', 'Augmented Version', 'Unlabeled Target Domain', 'Point Cloud Classification', 'Point Cloud Reconstruction', 'Data Sources', 'Similar Shape', 'Object Detection', 'Data Augmentation', 'Target Sample', 'Graph Convolutional Network', 'Similar Embeddings', 'Semantic Segmentation', 'Self-supervised Learning', 'Nodes In The Graph', 'Decision Boundary', 'Target Data', 'Graph Structure', 'Sample Isolation', 'Main Backbone']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",10,"Point cloud classification is a popular task in 3D vision. However, previous works, usually assume that point clouds at test time are obtained with the same procedure or sensor as those at training time. Unsupervised Domain Adaptation (UDA) instead, breaks this assumption and tries to solve the task on an unlabeled target domain, leveraging only on a supervised source domain. For point cloud classification, recent UDA methods try to align features across domains via auxiliary tasks such as point cloud reconstruction, which however do not optimize the discriminative power in the target domain in feature space. In contrast, in this work, we focus on obtaining a discriminative feature space for the target domain enforcing consistency between a point cloud and its augmented version. We then propose a novel iterative self-training methodology that exploits Graph Neural Networks in the UDA context to refine pseudo-labels. We perform extensive experiments and set the new state-of-the art in standard UDA benchmarks for point cloud classification. Finally, we show how our approach can be extended to more complex tasks such as part segmentation."
Self-Distilled Self-Supervised Representation Learning,"Jiho Jang, Seonhoon Kim, Kiyoon Yoo, Chaerin Kong, Jangho Kim, Nojun Kwak",Coupang; Seoul National University; Kookmin University,66.66666667,South Korea,33.33333333,South Korea,"State-of-the-art frameworks in self-supervised learning have recently shown that fully utilizing transformer-based models can lead to performance boost compared to conventional CNN models. Striving to maximize the mutual information of two views of an image, existing works apply a contrastive loss to the final representations. Motivated by self-distillation in the supervised regime, we further exploit this by allowing the intermediate representations to learn from the final layer via the contrastive loss. Through self-distillation, the intermediate layers are better suited for instance discrimination, making the performance of an early-exited sub-network not much degraded from that of the full network. This renders the pretext task easier also for the final layer, lead to better representations. Our method, Self-Distilled Self-Supervised Learning (SDSSL), outperforms competitive baselines (SimCLR, BYOL and MoCo v3) using ViT on various tasks and datasets. In the linear evaluation and k-NN protocol, SDSSL not only leads to superior performance in the final layers, but also in most of the lower layers. Furthermore, qualitative and quantative analyses show how representations are formed more effectively along the transformer layers. Code will be available.",https://openaccess.thecvf.com/content/WACV2023/html/Jang_Self-Distilled_Self-Supervised_Representation_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jang_Self-Distilled_Self-Supervised_Representation_Learning_WACV_2023_paper.pdf,,https://github.com/hagiss/SDSSL,2111.12958,main,Poster,https://ieeexplore.ieee.org/document/10030333/,"['Representation learning', 'Computer vision', 'Protocols', 'Codes', 'Statistical analysis', 'Self-supervised learning', 'Transformers']","['Representation Learning', 'Self-supervised Learning', 'Self-supervised Representation Learning', 'Mutual Information', 'Lower Layer', 'Final Layer', 'Intermediate Layer', 'Contrastive Loss', 'Intermediate Representation', 'Pretext Task', 'Convolutional Network', 'Deep Neural Network', 'Positive Samples', 'Supervised Learning', 'Batch Size', 'Negative Samples', 'ImageNet', 'Projector', 'Performance Gain', 'Higher Layers', 'Representation Layer', 'Image Retrieval', 'Low Representation', 'Teacher Network', 'Linearly Separable', 'Video Segments', 'Separate Representations', 'Self-supervised Learning Methods', 'Visual Domain', 'Intermediate Features']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",4,"State-of-the-art frameworks in self-supervised learning have recently shown that fully utilizing transformer-based models can lead to performance boost compared to conventional CNN models. Striving to maximize the mutual information of two views of an image, existing works apply a contrastive loss to the final representations. Motivated by self-distillation in the supervised regime, we further exploit this by allowing the intermediate representations to learn from the final layer via the contrastive loss. Through self-distillation, the intermediate layers are better suited for instance discrimination, making the performance of an early-exited sub-network not much degraded from that of the full network. This renders the pretext task easier also for the final layer, leading to better representations. Our method, Self-Distilled Self-Supervised Learning (SDSSL), outperforms competitive baselines (SimCLR, BYOL and MoCo v3) using ViT on various tasks and datasets. In the linear evaluation and k-NN protocol, SDSSL not only leads to superior performance in the final layers, but also in most of the lower layers. Furthermore, qualitative and quantitative analyses show how representations are formed more effectively along the transformer layers. Code is available at https://github.com/hagiss/SDSSL."
Self-Improving Multiplane-To-Layer Images for Novel View Synthesis,"Pavel Solovev, Taras Khakhulin, Denis Korzhenkov",Samsung AI Center – Moscow; Yandex Research; Skolkovo Institute of Science and Technology,33.33333333,Russia,66.66666667,Russia,"We present a new method for lightweight novel-view synthesis that generalizes to an arbitrary forward-facing scene. Recent approaches are computationally expensive, require per-scene optimization, or produce a memory-expensive representation. We start by representing the scene with a set of fronto-parallel semitransparent planes and afterwards convert them to deformable layers in an end-to-end manner. Additionally, we employ a feed-forward refinement procedure that corrects the estimated representation by aggregating information from input views. Our method does not require any fine-tuning when a new scene is processed and can handle an arbitrary number of views without any restrictions. Experimental results show that our approach surpasses recent models in terms of both common metrics and human evaluation, with the noticeable advantage in inference speed and compactness of the inferred layered geometry.",https://openaccess.thecvf.com/content/WACV2023/html/Solovev_Self-Improving_Multiplane-To-Layer_Images_for_Novel_View_Synthesis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Solovev_Self-Improving_Multiplane-To-Layer_Images_for_Novel_View_Synthesis_WACV_2023_paper.pdf,https://samsunglabs.github.io/MLI/,https://github.com/SamsungLabs/MLI,2210.01602,main,Poster,https://ieeexplore.ieee.org/document/10030993/,"['Measurement', 'Geometry', 'Computer vision', 'Adaptive systems', 'Pipelines', 'Rendering (computer graphics)', 'Optimization']","['View Synthesis', 'Common Metrics', 'Set Of Planes', 'Deformable Layer', 'New Method For The Synthesis', 'Neural Network', 'Number Of Images', 'Multiple Images', 'Peak Signal-to-noise Ratio', 'Ground Truth Image', 'Correction Step', 'Perceptual Loss', 'Camera Pose', 'Input Frames', 'Scene Representation', 'Scene Dataset', 'Scene Geometry', 'Frustum', 'Number Of Planes', 'Set Of Views', 'Virtual Camera', 'Feature Tensor', 'Pinhole Camera']","['Algorithms: 3D computer vision', 'Computational photography', 'image and video synthesis']",9,"We present a new method for lightweight novel-view synthesis that generalizes to an arbitrary forward-facing scene. Recent approaches are computationally expensive, require per-scene optimization, or produce a memory-expensive representation. We start by representing the scene with a set of fronto-parallel semitransparent planes and afterwards convert them to deformable layers in an end-to-end manner. Additionally, we employ a feed-forward refinement procedure that corrects the estimated representation by aggregating information from input views. Our method does not require any fine-tuning when a new scene is processed and can handle an arbitrary number of views without any restrictions. Experimental results show that our approach surpasses recent models in terms of both common metrics and human evaluation, with the noticeable advantage in inference speed and compactness of the inferred layered geometry."
Self-Pair: Synthesizing Changes From Single Source for Object Change Detection in Remote Sensing Imagery,"Minseok Seo, Hakjin Lee, Yongjin Jeon, Junghoon Seo",SI Analytics,0,,100,USA,"For change detection in remote sensing, constructing a training dataset for deep learning models is quite difficult due to the requirements of bi-temporal supervision. To overcome this issue, single-temporal supervision which treats change labels as the difference of two semantic masks has been proposed. This novel method trains a change detector using two spatially unrelated images with corresponding semantic labels. However, training with unpaired dataset shows not enough performance compared with other methods based on bi-temporal supervision. We suspect this phenomenon caused by ignorance of meaningful information in the actual bi-temporal pairs.In this paper, we emphasize that the change originates from the source image and show that manipulating the source image as an after-image is crucial to the performance of change detection. Our method achieves state-of-the-art performance in a large gap than existing methods.",https://openaccess.thecvf.com/content/WACV2023/html/Seo_Self-Pair_Synthesizing_Changes_From_Single_Source_for_Object_Change_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Seo_Self-Pair_Synthesizing_Changes_From_Single_Source_for_Object_Change_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030197/,"['Training', 'Deep learning', 'Visualization', 'Costs', 'Biological system modeling', 'Semantics', 'Supervised learning']","['Change Detection', 'Remote Sensing', 'Remote Sensing Imagery', 'Object Change Detection', 'Deep Learning', 'Training Dataset', 'Source Images', 'Semantic Labels', 'Single Image', 'Object Detection', 'Semantic Information', 'Image Pairs', 'Semantic Segmentation', 'Gaussian Blur', 'Synthetic Images', 'Interesting Changes', 'Object Segmentation', 'Change Map', 'Instantaneous Change', 'Augmentation Strategy', 'Pseudo Labels', 'Earth Mover’s Distance', 'Change Detection Methods', 'Random Cropping', 'XOR Operation', 'Geometric Transformation', 'Segmentation Dataset', 'Semantic Segmentation Datasets', 'True Positive']","['Applications: Remote Sensing', 'Environmental monitoring/climate change/ecology']",13,"For change detection in remote sensing, constructing a training dataset for deep learning models is difficult due to the requirements of bi-temporal supervision. To overcome this issue, single-temporal supervision which treats change labels as the difference of two semantic masks has been proposed. This novel method trains a change detector using two spatially unrelated images with corresponding semantic labels such as building. However, training on unpaired datasets could confuse the change detector in the case of pixels that are labeled unchanged but are visually significantly different. In order to maintain the visual similarity in unchanged area, in this paper, we emphasize that the change originates from the source image and show that manipulating the source image as an after-image is crucial to the performance of change detection. Extensive experiments demonstrate the importance of maintaining visual information between pre- and post-event images, and our method outperforms existing methods based on single-temporal supervision."
Self-Supervised 2D/3D Registration for X-Ray to CT Image Fusion,"Srikrishna Jaganathan, Maximilian Kukla, Jian Wang, Karthik Shetty, Andreas Maier","FAU Erlangen-Nürnberg, Erlangen, Germany; Siemens Healthineers AG, Forchheim, Germany",50,Germany,50,Germany,"Deep Learning-based 2D/3D registration enables fast, robust, and accurate X-ray to CT image fusion when large annotated paired datasets are available for training. However, the need for paired CT volume and X-ray images with ground truth registration limits the applicability in interventional scenarios. An alternative is to use simulated X-ray projections from CT volumes, thus removing the need for paired annotated datasets. Deep Neural Networks trained exclusively on simulated X-ray projections can perform significantly worse on real X-ray images due to the domain gap. We propose a self-supervised 2D/3D registration framework combining simulated training with unsupervised feature and pixel space domain adaptation to overcome the domain gap and eliminate the need for paired annotated datasets. Our framework achieves a registration accuracy of 1.83 +-1.16 mm with a high success ratio of 90.1% on real X-ray images showing a 23.9% increase in success ratio compared to reference annotation-free algorithms.",https://openaccess.thecvf.com/content/WACV2023/html/Jaganathan_Self-Supervised_2D3D_Registration_for_X-Ray_to_CT_Image_Fusion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jaganathan_Self-Supervised_2D3D_Registration_for_X-Ray_to_CT_Image_Fusion_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030411/,"['Training', 'Representation learning', 'Deep learning', 'Computer vision', 'Computed tomography', 'Neural networks', 'Self-supervised learning']","['Computed Tomography Images', 'Feature Space', 'Adaptive Feature', 'Domain Adaptation', 'Simulation Training', 'Annotated Dataset', 'Pixel Spacing', 'Registration Accuracy', 'Intervention Scenarios', 'Domain Gap', 'Unsupervised Feature', 'Success Ratio', 'X-ray Projection', 'Computed Tomography Volumes', 'Simulation Project', 'Computed Tomography', 'Training Data', 'Simulated Data', '3D Images', '2D Images', 'Style Transfer', 'Fluoroscopic Images', 'Multimodal Learning', 'Self-supervised Learning', 'Simulated Images', 'Registration Techniques', 'Preoperative Volume', 'Registration Error', 'Generative Adversarial Networks', 'Loss Of Flow']","['Applications: Biomedical/healthcare/medicine', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",11,"Deep Learning-based 2D/3D registration enables fast, robust, and accurate X-ray to CT image fusion when large annotated paired datasets are available for training. However, the need for paired CT volume and X-ray images with ground truth registration limits the applicability in interventional scenarios. An alternative is to use simulated X-ray projections from CT volumes, thus removing the need for paired annotated datasets. Deep Neural Networks trained exclusively on simulated X-ray projections can perform significantly worse on real X-ray images due to the domain gap. We propose a self-supervised 2D/3D registration framework combining simulated training with unsupervised feature and pixel space domain adaptation to overcome the domain gap and eliminate the need for paired annotated datasets. Our framework achieves a registration accuracy of 1.83 ± 1.16 mm with a high success ratio of 90.1% on real X-ray images showing a 23.9% increase in success ratio compared to reference annotation-free algorithms."
Self-Supervised Clustering Based on Manifold Learning and Graph Convolutional Networks,"Leonardo Tadeu Lopes, Daniel Carlos Guimarães Pedronette","State University of São Paulo (UNESP), Brazil",100,Brazil,0,,"In spite of the huge advances in supervised learning, the common requirement for extensive labeled datasets represents a severe bottleneck. In this scenario, other learning paradigms capable of addressing the challenge associated with the scarcity of labeled data represent a relevant alternative solution. This paper presents a novel clustering method called Self-Supervised Graph Convolutional Clustering (SGCC), which aims to exploit the strengths of different learning paradigms, combining unsupervised, semi-supervised, and self-supervised perspectives. An unsupervised manifold learning algorithm based on hypergraphs and ranking information is used to provide more effective and global similarity information. The hypergraph structures allow identifying representative items for each cluster, which are used to derive a set of small but high confident clusters. Such clusters are taken as soft-labels for training a Graph Convolutional Network (GCN) in a semi-supervised classification task. Once trained in a self-supervised setting, the GCN is used to predict the cluster of remaining items. The proposed SGCC method was evaluated both in image and citation networks datasets and compared with classic and recent clustering methods, obtaining high-effective results in all scenarios.",https://openaccess.thecvf.com/content/WACV2023/html/Lopes_Self-Supervised_Clustering_Based_on_Manifold_Learning_and_Graph_Convolutional_Networks_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lopes_Self-Supervised_Clustering_Based_on_Manifold_Learning_and_Graph_Convolutional_Networks_WACV_2023_paper.pdf,,https://github.com/lopes-leonardo/sgcc,,main,Poster,https://ieeexplore.ieee.org/document/10030544/,"['Training', 'Visualization', 'Computer vision', 'Parameter estimation', 'Clustering methods', 'Supervised learning', 'Clustering algorithms']","['Graph Convolutional Network', 'Graph Convolution', 'Self-supervised Clustering', 'Clustering Method', 'Image Dataset', 'Unsupervised Learning', 'Similar Information', 'Citation Network', 'Semi-supervised Classification', 'Ranking Information', 'Similarity Measure', 'Agglomerates', 'Binary Vector', 'Original Features', 'Feature Matrix', 'Semi-supervised Learning', 'Ranked List', 'Self-supervised Learning', 'Incidence Matrix', 'Neighborhood Size', 'Graph Convolutional Network Model', 'Reliable Clusters', 'List Of Sets', 'Representative Element', 'Outcome Metrics', 'Cluster Configuration', 'Object Pairs', 'Cartesian Product', 'Machine Learning Tasks', 'Unsupervised Learning Methods']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"In spite of the huge advances in supervised learning, the common requirement for extensive labeled datasets represents a severe bottleneck. In this scenario, other learning paradigms capable of addressing the challenge associated with the scarcity of labeled data represent a relevant alternative solution. This paper presents a novel clustering method called Self-Supervised Graph Convolutional Clustering (SGCC)
<sup>1</sup>
, which aims to exploit the strengths of different learning paradigms, combining unsupervised, semi-supervised, and self-supervised perspectives. An unsupervised manifold learning algorithm based on hypergraphs and ranking information is used to provide more effective and global similarity information. The hypergraph structures allow identifying representative items for each cluster, which are used to derive a set of small but high-confident clusters. Such clusters are taken as soft-labels for training a Graph Convolutional Network (GCN) in a semi-supervised classification task. Once trained in a self-supervised setting, the GCN is used to predict the cluster of remaining items. The proposed SGCC method was evaluated both in image and citation networks datasets and compared with classic and recent clustering methods, obtaining high-effective results in all scenarios."
Self-Supervised Correspondence Estimation via Multiview Registration,"Mohamed El Banani, Ignacio Rocco, David Novotny, Andrea Vedaldi, Natalia Neverova, Justin Johnson, Ben Graham","University of Michigan, Meta AI; University of Michigan; Meta AI",66.66666667,USA,33.33333333,USA,"Video provides us with the spatio-temporal consistency needed for visual learning. Recent approaches have utilized this signal to learn correspondence estimation from close-by frame pairs. However, by only relying on close-by frame pairs, those approaches miss out on the richer long-range consistency between more distant overlapping frames. To address this, we propose a self-supervised approach for correspondence estimation that learns from multiview consistency in short RGB-D video sequences. Our approach combines pairwise correspondence estimation and registration with a novel SE(3) transformation synchronization algorithm. Our key insight is that self-supervised multiview registration allows us to obtain correspondences over longer time frames, which increases both the diversity and difficulty of sampled pairs. We evaluate our approach on indoor scenes for correspondence estimation and RGB-D pointcloud registration and find that we can perform on-par with prior supervised approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Banani_Self-Supervised_Correspondence_Estimation_via_Multiview_Registration_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Banani_Self-Supervised_Correspondence_Estimation_via_Multiview_Registration_WACV_2023_paper.pdf,,,2212.03236,main,Poster,https://ieeexplore.ieee.org/document/10031026/,"['Visualization', 'Computer vision', 'Three-dimensional displays', 'Pipelines', 'Video sequences', 'Estimation', 'Training data']","['Multi-view Registration', 'Synchronization', 'Short Sequences', 'Longer Time Frame', 'Pairwise Estimates', 'Point Cloud Registration', 'Self-supervised Approach', 'Transformer', 'Ratio Test', 'Feature Space', '3D Reconstruction', 'Video Frames', 'Handcrafted Features', 'Image Point', 'Cyanoacrylate', 'Feature Matching', 'Pairwise Matrix', 'Alignment Algorithm', 'Training Videos', 'Adjacent Pairs', 'Geometric Consistency', '3D Error', 'Camera Pose', 'Camera Intrinsics', 'Large Outliers', 'Eigendecomposition', 'Translation Error', '3D Coordinates', 'Matching Algorithm']","['Algorithms: 3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"Video provides us with the spatio-temporal consistency needed for visual learning. Recent approaches have utilized this signal to learn correspondence estimation from closeby frame pairs. However, by only relying on close-by frame pairs, those approaches miss out on the richer long-range consistency between distant overlapping frames. To address this, we propose a self-supervised approach for correspondence estimation that learns from multiview consistency in short RGB-D video sequences. Our approach combines pairwise correspondence estimation and registration with a novel SE(3) transformation synchronization algorithm. Our key insight is that self-supervised multiview registration allows us to obtain correspondences over longer time frames; increasing both the diversity and difficulty of sampled pairs. We evaluate our approach on indoor scenes for correspondence estimation and RGB-D pointcloud registration and find that we perform on-par with supervised approaches."
Self-Supervised Distilled Learning for Multi-Modal Misinformation Identification,"Michael Mu, Sreyasee Das Bhattacharjee, Junsong Yuan","The State University of New York at Buffalo, NY, USA",100,USA,0,,"Rapid dissemination of misinformation is a major societal problem receiving increasing attention. Unlike Deepfake, Out-of-Context misinformation, in which the unaltered unimode contents (e.g. image, text) of a multi-modal news sample are combined in an out-of-context manner to generate deception, requires limited technical expertise to create. Therefore, it is more prevalent a means to confuse readers. Most existing approaches extract features from its uni-mode counterparts to concatenate and train a model for the misinformation classification task. In this paper, we design a self-supervised feature representation learning strategy that aims to attain the multi-task objectives: (1) task-agnostic, which evaluates the intra- and inter-mode representational consistencies for improved alignments across related models; (2) task-specific, which estimates the category-specific multi-modal knowledge to enable the classifier to derive more discriminative predictive distributions. To compensate for the dearth of annotated data representing varied types of misinformation, the proposed Self-Supervised Distilled Learner (SSDL) utilizes a Teacher network to weakly guide a Student network to mimic a similar decision pattern as the teacher. The two-phased learning of SSDL can be summarized as: initial pretraining of the Student model using a combination of contrastive self-supervised task-agnostic objective and supervised task-specific adjustment in parallel; finetuning the Student model via self-supervised knowledge distillation blended with the supervised objective of decision alignment. In addition to the consistent out-performances over the existing baselines that demonstrate the feasibility of our approach, the explainability capacity of the proposed SSDL also helps users visualize the reasoning behind a specific prediction made by the model.",https://openaccess.thecvf.com/content/WACV2023/html/Mu_Self-Supervised_Distilled_Learning_for_Multi-Modal_Misinformation_Identification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Mu_Self-Supervised_Distilled_Learning_for_Multi-Modal_Misinformation_Identification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030784/,"['Representation learning', 'Training data', 'Predictive models', 'Streaming media', 'Semisupervised learning', 'Multitasking', 'Feature extraction']","['Self-supervised Learning', 'Learning Strategies', 'Representation Learning', 'Predictive Distribution', 'Student Model', 'Teacher Network', 'Student Network', 'Deepfake', 'Semantic', 'Pairing', 'Large Networks', 'Plot In Fig', 'Unimodal', 'External Information', 'Semi-supervised Learning', 'Multi-task Learning', 'Classification Decision', 'Component Of Image', 'Visual Components', 'Entire Collection', 'Pre-trained Encoder', 'News Content', 'Distillation Loss', 'Augmented Samples', 'Self-supervised Manner', 'Multimodal Information', 'Learning Scenarios', 'Multimodal Learning', 'Caption Text', 'Multilayer Perceptron']","['Applications: Social good', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Vision + language and/or other modalities']",5,"Rapid dissemination of misinformation is a major societal problem receiving increasing attention. Unlike Deep-fake, Out-of-Context misinformation, in which the unaltered unimode contents (e.g. image, text) of a multi-modal news sample are combined in an out-of-context manner to generate deception, requires limited technical expertise to create. Therefore, it is more prevalent a means to confuse readers. Most existing approaches extract features from its uni-mode counterparts to concatenate and train a model for the misinformation classification task. In this paper, we design a self-supervised feature representation learning strategy that aims to attain the multi-task objectives: (1) task-agnostic, which evaluates the intra- and inter-mode representational consistencies for improved alignments across related models; (2) task-specific, which estimates the category-specific multi-modal knowledge to enable the classifier to derive more discriminative predictive distributions. To compensate for the dearth of annotated data representing varied types of misinformation, the proposed Self-Supervised Distilled Learner (SSDL) utilizes a Teacher network to weakly guide a Student network to mimic a similar decision pattern as the teacher. The two-phased learning of SSDL can be summarized as: initial pretraining of the Student model using a combination of contrastive self-supervised task-agnostic objective and supervised task-specific adjustment in parallel; finetuning the Student model via self-supervised knowledge distillation blended with the supervised objective of decision alignment. In addition to the consistent out-performances over the existing baselines that demonstrate the feasibility of our approach, the explainability capacity of the proposed SSDL also helps users visualize the reasoning behind a specific prediction made by the model."
Self-Supervised Learning With Local Contrastive Loss for Detection and Semantic Segmentation,"Ashraful Islam, Benjamin Lundell, Harpreet Sawhney, Sudipta N. Sinha, Peter Morales, Richard J. Radke",; Rensselaer Polytechnic Institute; Microsoft Mixed Reality; Nvidia,33.33333333,USA,66.66666667,USA,"We present a self-supervised learning (SSL) method suitable for semi-global tasks such as object detection and semantic segmentation. We enforce local consistency between self-learned features, representing corresponding image locations of transformed versions of the same image, by minimizing a pixel-level local contrastive (LC) loss during training. LC-loss can be added to existing self-supervised learning methods with minimal overhead. We evaluate our SSL approach on two downstream tasks -- object detection and semantic segmentation, using COCO, PASCAL VOC, and CityScapes datasets. Our method outperforms the existing state-of-the-art SSL approaches by 1.9% on COCO object detection, 1.4% on PASCAL VOC detection, and 0.6% on CityScapes segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Islam_Self-Supervised_Learning_With_Local_Contrastive_Loss_for_Detection_and_Semantic_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Islam_Self-Supervised_Learning_With_Local_Contrastive_Loss_for_Detection_and_Semantic_WACV_2023_paper.pdf,,,2207.04398,main,Poster,https://ieeexplore.ieee.org/document/10030301/,"['Training', 'Computer vision', 'Semantic segmentation', 'Self-supervised learning', 'Object detection', 'Task analysis']","['Semantic Segmentation', 'Local Loss', 'Self-supervised Learning', 'Contrastive Loss', 'Local Contrastive Loss', 'Object Detection', 'COCO Dataset', 'Minimal Overhead', 'Self-supervised Learning Methods', 'VOC Dataset', 'PASCAL VOC Dataset', 'Fine-tuned', 'Local Features', 'Input Image', 'Image Classification', 'Feature Representation', 'Transfer Learning', 'Good Characteristics', 'Corresponding Points', 'Image Point', 'Pretext Task', 'Negative Log-likelihood', 'Target Network', 'Mask R-CNN', 'ImageNet Pretraining', 'Color Transformation', 'Approaches In The Literature', 'Self-supervised Training', 'Image Transformation', 'Color Distortion']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",2,"We present a self-supervised learning (SSL) method suitable for semi-global tasks such as object detection and semantic segmentation. We enforce local consistency between self-learned features that represent corresponding image locations of transformed versions of the same image, by minimizing a pixel-level local contrastive (LC) loss during training. LC-loss can be added to existing self-supervised learning methods with minimal overhead. We evaluate our SSL approach on two downstream tasks – object detection and semantic segmentation, using COCO, PASCAL VOC, and CityScapes datasets. Our method outperforms the existing state-of-the-art SSL approaches by 1.9% on COCO object detection, 1.4% on PASCAL VOC detection, and 0.6% on CityScapes segmentation."
"Self-Supervised Learning With Masked Image Modeling for Teeth Numbering, Detection of Dental Restorations, and Instance Segmentation in Dental Panoramic Radiographs","Amani Almalki, Longin Jan Latecki","Department of Computer and Information Sciences, Temple University, Philadelphia, USA",100,USA,0,,"The computer-assisted radiologic informative report is currently emerging in dental practice to facilitate dental care and reduce time consumption in manual panoramic radiographic interpretation. However, the amount of dental radiographs for training is very limited, particularly from the point of view of deep learning. This study aims to utilize recent self-supervised learning methods like SimMIM and UM-MAE to increase the model efficiency and understanding of the limited number of dental radiographs. We use the Swin Transformer for teeth numbering, detection of dental restorations, and instance segmentation tasks. To the best of our knowledge, this is the first study that applied self-supervised learning methods to Swin Transformer on dental panoramic radiographs. Our results show that the SimMIM method obtained the highest performance of 90.4% and 88.9% on detecting teeth and dental restorations and instance segmentation, respectively, increasing the average precision by 13.4 and 12.8 over the random initialization baseline. Moreover, we augment and correct the existing dataset of panoramic radiographs. The code and the dataset are available at https://github.com/AmaniHAlmalki/DentalMIM.",https://openaccess.thecvf.com/content/WACV2023/html/Almalki_Self-Supervised_Learning_With_Masked_Image_Modeling_for_Teeth_Numbering_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Almalki_Self-Supervised_Learning_With_Masked_Image_Modeling_for_Teeth_Numbering_Detection_WACV_2023_paper.pdf,,https://github.com/AmaniHAlmalki/DentalMIM,2210.11404,main,Poster,https://ieeexplore.ieee.org/document/10030848/,"['Radiography', 'Training', 'Image segmentation', 'Teeth', 'Self-supervised learning', 'Transformers', 'Dentistry']","['Self-supervised Learning', 'Instance Segmentation', 'Masked Images', 'Dental Restorations', 'Panoramic Radiographs', 'Average Precision', 'Random Initialization', 'Self-supervised Learning Methods', 'Learning Rate', 'Convolutional Neural Network', 'Computer Vision', 'Systematic Errors', 'Object Detection', 'Data Augmentation', 'Weight Decay', 'Annotated Dataset', 'Root Canal', 'Mask R-CNN', 'Learning Rate Schedule', 'Base Learning Rate', 'Dental Imaging', 'Dental Restorative Materials', 'Dental Providers']",['Applications: Biomedical/healthcare/medicine'],7,"The computer-assisted radiologic informative report is currently emerging in dental practice to facilitate dental care and reduce time consumption in manual panoramic radiographic interpretation. However, the amount of dental radiographs for training is very limited, particularly from the point of view of deep learning. This study aims to utilize recent self-supervised learning methods like SimMIM and UM-MAE to increase the model efficiency and understanding of the limited number of dental radiographs. We use the Swin Transformer for teeth numbering, detection of dental restorations, and instance segmentation tasks. To the best of our knowledge, this is the first study that applied self-supervised learning methods to Swin Transformer on dental panoramic radiographs. Our results show that the SimMIM method obtained the highest performance of 90.4% and 88.9% on detecting teeth and dental restorations and instance segmentation, respectively, increasing the average precision by 13.4 and 12.8 over the random initialization baseline. Moreover, we augment and correct the existing dataset of panoramic radiographs. The code and the dataset are available at https://github.com/AmaniHAlmalki/DentalMIM."
Self-Supervised Monocular Depth Estimation From Thermal Images via Adversarial Multi-Spectral Adaptation,"Ukcheol Shin, Kwanyong Park, Byeong-Uk Lee, Kyunghyun Lee, In So Kweon",Korea Advanced Institute of Science and Technology (KAIST),100,South Korea,0,,"Recently, thermal image based 3D understanding is gradually attracting attention for an illumination condition agnostic machine vision. However, the difficulty of the thermal image lies in insufficient training supervision due to its low-contrast and textureless properties. Also, introducing additional modality requires further constraints such as complicated multi-sensor calibration and synchronized data acquisition. To leverage additional modality information without such constraints, we propose a novel training framework that consists of self-supervised learning of unpaired multi-spectral images and feature-level adversarial adaptation. In the training stage, we utilize unpaired RGB/thermal video and partially shared network architecture consisting of modality-specific feature extractors and modality-independent decoder. Through the shared network design, the depth decoder can leverage the self-supervised signal of the unpaired RGB images. Feature-level adversarial adaptation minimizes the gap between RGB and thermal features and eventually makes the thermal encoder extract representative and informative features. Based on the proposed method, the trained depth network shows outperformed results than previous state-of-the-art methods.",https://openaccess.thecvf.com/content/WACV2023/html/Shin_Self-Supervised_Monocular_Depth_Estimation_From_Thermal_Images_via_Adversarial_Multi-Spectral_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shin_Self-Supervised_Monocular_Depth_Estimation_From_Thermal_Images_via_Adversarial_Multi-Spectral_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030213/,"['Training', 'Three-dimensional displays', 'Data acquisition', 'Estimation', 'Self-supervised learning', 'Network architecture', 'Feature extraction']","['Infrared Imaging', 'Depth Estimation', 'Monocular Depth Estimation', 'Self-supervised Monocular Depth Estimation', 'RGB Images', 'Thermal Characteristics', 'Illumination Conditions', 'Network Depth', 'Self-supervised Learning', 'Training Framework', 'Extract Feature Information', 'RGB Features', 'Light Conditions', 'Feature Space', 'Image Reconstruction', 'High Contrast', 'High Noise', 'Image Pairs', 'Semantic Segmentation', 'Depth Map', 'Pose Estimation', 'Relative Pose', 'Domain Adaptation', 'RGB Video', 'Reconstruction Loss', 'RGB Camera', 'Smoothness Loss', 'Low Texture', 'Target Domain', 'Spatial Reconstruction']","['Applications: Robotics', '3D computer vision']",10,"Recently, thermal image based 3D understanding is gradually attracting attention for an illumination condition agnostic machine vision. However, the difficulty of the thermal image lies in insufficient training supervision due to its low-contrast and texturesless properties. Also, introducing additional modality requires further constraints such as complicated multi-sensor calibration and synchronized data acquisition. To leverage additional modality information without such constraints, we propose a novel training framework that consists of self-supervised learning of unpaired multi-spectral images and feature-level adversarial adaptation. In the training stage, we utilize unpaired RGB/thermal video and partially shared network architecture consisting of modality-specific feature extractors and modality-independent decoder. Through the shared network design, the depth decoder can leverage the self-supervised signal of the unpaired RGB images. Feature-level adversarial adaptation minimizes the gap between RGB and thermal features and eventually makes the thermal encoder extract representative and informative features. Based on the proposed method, the trained depth network shows outperformed results than previous state-of-the-art methods."
Self-Supervised Monocular Depth Estimation: Solving the Edge-Fattening Problem,"Xingyu Chen, Ruonan Zhang, Ji Jiang, Yan Wang, Ge Li, Thomas H. Li","Advanced Institute of Information Technology, Peking University; Information Technology R&D Innovation Center of Peking University; School of Electronic and Computer Engineering, Peking University",100,China,0,,"Self-supervised monocular depth estimation (MDE) models universally suffer from the notorious edge-fattening issue. Triplet loss, popular for metric learning, has made a great success in many computer vision tasks. In this paper, we redesign the patch-based triplet loss in MDE to alleviate the ubiquitous edge-fattening issue. We show two drawbacks of the raw triplet loss in MDE and demonstrate our problem-driven redesigns. First, we present a min. operator based strategy applied to all negative samples, to prevent well-performing negatives sheltering the error of edge-fattening negatives. Second, we split the anchor-positive distance and anchor-negative distance from within the original triplet, which directly optimizes the positives without any mutual effect with the negatives. Extensive experiments show the combination of these two small redesigns can achieve unprecedented results: Our powerful and versatile triplet loss not only makes our model outperform all previous SoTA by a large margin, but also provides substantial performance boosts to a large number of existing models, while introducing no extra inference computation at all.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Self-Supervised_Monocular_Depth_Estimation_Solving_the_Edge-Fattening_Problem_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Self-Supervised_Monocular_Depth_Estimation_Solving_the_Edge-Fattening_Problem_WACV_2023_paper.pdf,,https://github.com/xingyuuchen/tri-depth,2210.00411,main,Poster,https://ieeexplore.ieee.org/document/10030156/,"['Measurement', 'Computer vision', 'Computational modeling', 'Estimation', 'Optimization']","['Depth Estimation', 'Self-supervised Monocular Depth Estimation', 'Redesign', 'Computer Vision', 'Large Margin', 'Mutual Effect', 'Metric Learning', 'Triplet Loss', 'Extra Computation', 'Feature Space', 'Image Pixels', 'Point Cloud', 'Target Image', 'Regression Problem', 'Source Images', 'Differences In Depth', 'Background Pixels', 'Object Boundaries', 'Contrastive Loss', 'Similar Depth', 'Deep Metric Learning', 'Depth Prediction', 'Unprecedented Performance', 'Stereo Pairs', 'Windshield', 'KITTI Dataset', 'LiDAR Point Clouds']","['Algorithms: 3D computer vision', 'Low-level and physics-based vision']",8,"Self-supervised monocular depth estimation (MDE) models universally suffer from the notorious edge-fattening issue. Triplet loss, as a widespread metric learning strategy, has largely succeeded in many computer vision applications. In this paper, we redesign the patch-based triplet loss in MDE to alleviate the ubiquitous edge-fattening issue. We show two drawbacks of the raw triplet loss in MDE and demonstrate our problem-driven redesigns. First, we present a min. operator based strategy applied to all negative samples, to prevent well-performing negatives sheltering the error of edge-fattening negatives. Second, we split the anchor-positive distance and anchor-negative distance from within the original triplet, which directly optimizes the positives without any mutual effect with the negatives. Extensive experiments show the combination of these two small redesigns can achieve unprecedented results: Our powerful and versatile triplet loss not only makes our model outperform all previous SoTA by a large margin, but also provides substantial performance boosts to a large number of existing models, while introducing no extra inference computation at all."
Self-Supervised Pyramid Representation Learning for Multi-Label Visual Analysis and Beyond,"Cheng-Yen Hsieh, Chih-Jung Chang, Fu-En Yang, Yu-Chiang Frank Wang",National Taiwan University,100,Taiwan,0,,"While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with additional learners to observe and relate inherent semantic information within an image. In particular, we present a cross-scale patch-level correlation learning in SS-PRL, which allows the model to aggregate and associate information learned across patch scales. We show that, with our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection, and instance segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hsieh_Self-Supervised_Pyramid_Representation_Learning_for_Multi-Label_Visual_Analysis_and_Beyond_WACV_2023_paper.pdf,,,2208.14439,main,Poster,https://ieeexplore.ieee.org/document/10030415/,"['Representation learning', 'Adaptation models', 'Visualization', 'Correlation', 'Aggregates', 'Semantics', 'Prototypes']","['Self-supervised Learning', 'Pyramid Representation', 'Object Detection', 'Image Scale', 'Pixel Level', 'Instance Segmentation', 'Patch Level', 'Patch Scale', 'Self-supervised Learning Methods', 'Classification Task', 'Input Image', 'Image Classification', 'Multiple Scales', 'Visual Task', 'ImageNet', 'Semantic Segmentation', 'Objective Presentation', 'Segmentation Task', 'Unlabeled Data', 'Linear Classifier', 'Multi-label Classification Task', 'Downstream Classification', 'Pretext Task', 'Global Image', 'COCO Dataset', 'Object Detection Task', 'Image Object Detection', 'Dense Prediction', 'Semantic Labels', 'Training Data']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",2,"While self-supervised learning has been shown to benefit a number of vision tasks, existing techniques mainly focus on image-level manipulation, which may not generalize well to downstream tasks at patch or pixel levels. Moreover, existing SSL methods might not sufficiently describe and associate the above representations within and across image scales. In this paper, we propose a Self-Supervised Pyramid Representation Learning (SS-PRL) framework. The proposed SS-PRL is designed to derive pyramid representations at patch levels via learning proper prototypes, with additional learners to observe and relate inherent semantic information within an image. In particular, we present a cross-scale patch-level correlation learning in SS-PRL, which allows the model to aggregate and associate information learned across patch scales. We show that, with our proposed SS-PRL for model pre-training, one can easily adapt and fine-tune the models for a variety of applications including multi-label classification, object detection, and instance segmentation."
Self-Supervised Relative Pose With Homography Model-Fitting in the Loop,"Bruce R. Muller, William A. P. Smith","Department of Computer Science, University of York, UK",100,UK,0,,"We propose a self-supervised method for relative pose estimation for road scenes. By exploiting the approximate planarity of the local ground plane, we can extract a self-supervision signal via cross-projection between images using a homography derived from estimated ground-relative pose. We augment cross-projected perceptual loss by including classical image alignment in the network training loop. We use pretrained semantic segmentation and optical flow to extract ground plane correspondences between approximately aligned images and RANSAC to find the best fitting homography. By decomposing to ground-relative pose, we obtain pseudo labels that can be used for direct supervision. We show that this extremely simple geometric model is competitive for visual odometry with much more complex self-supervised methods that must learn depth estimation in conjunction with relative pose. Code and result videos: github.com/brucemuller/homographyVO.",https://openaccess.thecvf.com/content/WACV2023/html/Muller_Self-Supervised_Relative_Pose_With_Homography_Model-Fitting_in_the_Loop_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Muller_Self-Supervised_Relative_Pose_With_Homography_Model-Fitting_in_the_Loop_WACV_2023_paper.pdf,,https://github.com/brucemuller/homographyVO,,main,Poster,https://ieeexplore.ieee.org/document/10030804/,"['Training', 'Roads', 'Ultraviolet sources', 'Semantic segmentation', 'Pose estimation', 'Fitting', 'Semantics']","['Relative Pose', 'Semantic Segmentation', 'Ground Plane', 'Geometric Model', 'Optical Flow', 'Pose Estimation', 'Depth Estimation', 'Perceptual Loss', 'Local Plane', 'Pseudo Labels', 'Street Scenes', 'Visual Odometry', 'Pose Estimation Methods', 'Image Pairs', 'Road Surface', 'Network Output', 'Optical Axis', 'Monocular', 'Training Loss', 'Inference Time', 'Feature Matching', 'Dense Depth', 'Pre-trained Network', 'Bundle Adjustment', 'Rotation Error', 'Camera Height', 'Training Pairs', 'Scale Ambiguity', 'Network Flow', 'Supervision Signal']",['Algorithms: 3D computer vision'],1,"We propose a self-supervised method for relative pose estimation for road scenes. By exploiting the approximate planarity of the local ground plane, we can extract a self-supervision signal via cross-projection between images using a homography derived from estimated ground-relative pose. We augment cross-projected perceptual loss by including classical image alignment in the network training loop. We use pretrained semantic segmentation and optical flow to extract ground plane correspondences between approximately aligned images and RANSAC to find the best fitting homography. By decomposing to ground-relative pose, we obtain pseudo labels that can be used for direct supervision. We show that this extremely simple geometric model is competitive for visual odometry with much more complex self-supervised methods that must learn depth estimation in conjunction with relative pose. Code and result videos: github.com/brucemuller/homographyVO."
Semantic Guided Latent Parts Embedding for Few-Shot Learning,"Fengyuan Yang, Ruiping Wang, Xilin Chen","Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Beijing Academy of Artiﬁcial Intelligence, Beijing, 100084, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China",100,China,0,,"The ability of few-shot learning (FSL) is a basic requirement of intelligent agent learning in the open visual world. However, existing deep learning systems rely too heavily on large numbers of training samples, making it hard to learn new categories efficiently from limited size of training data. Two key challenges of FSL are insufficient comprehension and imperfect modeling of the few-shot novel class. For insufficient visual comprehension, semantic knowledge which is information from other modalities can help replenish the understanding of novel classes. But even so, most works still suffer from the second challenge because the single global class prototype they adopted is extremely unstable and imperfect given the larger intra-class variation and harder inter-class discrimination in FSL scenario. Thus, we propose to represent each class by its several different parts with the help of class semantic knowledge. Since we can never pre-define parts for unknown novel classes, we embed them in a latent manner. Concretely, we train a generator that takes the class semantic knowledge as input and outputs several filters of class-specific semantic latent parts. By applying each part filter, our model can pay attention to corresponding local regions containing each part. At the inference stage, the classification is conducted by comparing the similarities between those parts. Experiments on several FSL benchmarks demonstrate the effectiveness of our proposed method and show its potential to go beyond class recognition to class understanding. Furthermore, we also find when semantic knowledge is more visualized and customized, it will be more helpful in the FSL task.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Semantic_Guided_Latent_Parts_Embedding_for_Few-Shot_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Semantic_Guided_Latent_Parts_Embedding_for_Few-Shot_Learning_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031012/,"['Training', 'Deep learning', 'Visualization', 'Semantics', 'Training data', 'Prototypes', 'Generators']","['Few-shot Learning', 'Latent Parts', 'Intelligence Agencies', 'Semantic Knowledge', 'Intra-class Variance', 'Deep Learning System', 'Feature Maps', 'Cross-entropy Loss', 'Similar Classification', 'Activation Maps', 'Weight Coefficient', 'Part Of Class', 'Cognitive Categories', 'Visual Space', 'Base Classes', 'Follow-up Work', 'Class Weights', 'Benchmark For Comparison', 'Objective Understanding', 'Semantic Space', 'Semantic Annotation', 'Zero-shot', 'Standard Cross-entropy Loss', 'Query Sample', 'Tail Part', 'Few-shot Classification', 'Limited Training Samples']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",9,"The ability of few-shot learning (FSL) is a basic requirement of intelligent agent learning in the open visual world. However, existing deep learning systems rely too heavily on large numbers of training samples, making it hard to learn new categories efficiently from limited size of training data. Two key challenges of FSL are insufficient comprehension and imperfect modeling of the few-shot novel class. For insufficient visual comprehension, semantic knowledge which is information from other modalities can help replenish the understanding of novel classes. But even so, most works still suffer from the second challenge because the single global class prototype they adopted is extremely unstable and imperfect given the larger intra-class variation and harder inter-class discrimination in FSL scenario. Thus, we propose to represent each class by its several different parts with the help of class semantic knowledge. Since we can never pre-define parts for unknown novel classes, we embed them in a latent manner. Concretely, we train a generator that takes the class semantic knowledge as input and outputs several filters of class-specific semantic latent parts. By applying each part filter, our model can pay attention to corresponding local regions containing each part. At the inference stage, the classification is conducted by comparing the similarities between those parts. Experiments on several FSL benchmarks demonstrate the effectiveness of our proposed method and show its potential to go beyond class recognition to class understanding. Furthermore, we also find when semantic knowledge is more visualized and customized, it will be more helpful in the FSL task."
Semantic Segmentation With Active Semi-Supervised Learning,"Aneesh Rangnekar, Christopher Kanan, Matthew Hoffman","Rochester Institute of Technology, Rochester, NY, USA",100,USA,0,,"Using deep learning, we now have the ability to create exceptionally good semantic segmentation systems; however, collecting the prerequisite pixel-wise annotations for training images remains expensive and time-consuming. Therefore, it would be ideal to minimize the number of human annotations needed when creating a new dataset. Here, we address this problem by proposing a novel algorithm that combines active learning and semi-supervised learning. Active learning is an approach for identifying the best unlabeled samples to annotate. While there has been work on active learning for segmentation, most methods require annotating all pixel objects in each image, rather than only the most informative regions. We argue that this is inefficient. Instead, our active learning approach aims to minimize the number of annotations per image. Our method is enriched with semi-supervised learning, where we use pseudo labels generated with a teacher-student framework to identify image regions that help disambiguate confused classes. We also integrate mechanisms that enable better performance on imbalanced label distributions, which have not been studied previously for active learning in semantic segmentation. In experiments on the CamVid and CityScapes datasets, our method obtains over 95% of the network's performance on the full-training set using less than 17% of the training data, whereas the previous state of the art required 40% of the training data.",https://openaccess.thecvf.com/content/WACV2023/html/Rangnekar_Semantic_Segmentation_With_Active_Semi-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rangnekar_Semantic_Segmentation_With_Active_Semi-Supervised_Learning_WACV_2023_paper.pdf,,,2203.1073,main,Poster,https://ieeexplore.ieee.org/document/10030538/,"['Training', 'Deep learning', 'Head', 'Costs', 'Annotations', 'Semantic segmentation', 'Training data']","['Active Learning', 'Semantic Segmentation', 'Semi-supervised Learning', 'Active Semi-supervised Learning', 'Deep Learning', 'Training Images', 'Number Of Annotations', 'Pseudo Labels', 'Learning For Segmentation', 'Active Learning Approach', 'Training Set', 'Random Sampling', 'Regional Level', 'Softmax', 'Pooled Data', 'Pedestrian', 'Data Augmentation', 'Generative Adversarial Networks', 'Head And Tail', 'Traffic Light', 'Dataset Bias', 'Unlabeled Data', 'Replay Buffer', 'Student Network', 'Teacher Network', 'Unlabeled Images', 'Regularization Scheme', 'Labeling Cost', 'Validation Images', 'Random Flipping']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",14,"Using deep learning, we now have the ability to create exceptionally good semantic segmentation systems; however, collecting the prerequisite pixel-wise annotations for training images remains expensive and time-consuming. Therefore, it would be ideal to minimize the number of human annotations needed when creating a new dataset. Here, we address this problem by proposing a novel algorithm that combines active learning and semi-supervised learning. Active learning is an approach for identifying the best unlabeled samples to annotate. While there has been work on active learning for segmentation, most methods require annotating all pixel objects in each image, rather than only the most informative regions. We argue that this is inefficient. Instead, our active learning approach aims to minimize the number of annotations per-image. Our method is enriched with semi-supervised learning, where we use pseudo labels generated with a teacher-student framework to identify image regions that help disambiguate confused classes. We also integrate mechanisms that enable better performance on imbalanced label distributions, which have not been studied previously for active learning in semantic segmentation. In experiments on the CamVid and CityScapes datasets, our method obtains over 95% of the network’s performance on the full-training set using less than 17% of the training data, whereas the previous state of the art required 40% of the training data."
Semantic Segmentation in Aerial Imagery Using Multi-Level Contrastive Learning With Local Consistency,"Maofeng Tang, Konstantinos Georgiou, Hairong Qi, Cody Champion, Marc Bosch","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville; Accenture Federal Services",100,USA,0,,"Semantic segmentation in large-scale aerial images is an extremely challenging task. On one hand, the limited ground truth, as compared to the vast area the images cover, greatly hinders the development of supervised representation learning. On the other hand, the large footprint from remote sensing raises new challenges for semantic segmentation. In addition, the complex and ever changing image acquisition conditions further complicate the problem where domain shifting commonly occurs. In this paper, we exploit self-supervised contrastive learning (CL) methodologies for semantic segmentation in aerial imagery. In addition to performing CL at the feature level as most practices do, we add another level of contrastive learning, at the semantic level, taking advantage of the segmentation output from the downstream task. Further, we embed local mutual information in the semantic-level CL to enforce local consistency. This has largely enhanced the representation power at each pixel and improved the generalization capacity of the trained model. We refer to the proposed approach as multi-level contrastive learning with local consistency (mCL-LC). The experimental results on different benchmarks indicate that the proposed mCL-LC exhibits superior performance as compared to other state-of-the-art contrastive learning frameworks for the semantic segmentation task. mCL-LC also carries better generalization capacity especially when domain shifting exists.",https://openaccess.thecvf.com/content/WACV2023/html/Tang_Semantic_Segmentation_in_Aerial_Imagery_Using_Multi-Level_Contrastive_Learning_With_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tang_Semantic_Segmentation_in_Aerial_Imagery_Using_Multi-Level_Contrastive_Learning_With_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030272/,"['Representation learning', 'Computer vision', 'Image analysis', 'Semantic segmentation', 'Semantics', 'Decoding', 'Task analysis']","['Aerial Images', 'Semantic Segmentation', 'Self-supervised Learning', 'Segmentation In Aerial Imagery', 'Learning Framework', 'Mutual Information', 'Domain Shift', 'Representation Learning', 'Generation Capacity', 'Segmentation Task', 'Semantic Level', 'Large Footprint', 'Acquisition Conditions', 'Data Augmentation', 'Natural Images', 'High-level Features', 'Augmentation Methods', 'Local Details', 'Pixel Level', 'Pixel Location', 'Contrastive Loss', 'Local Smoothing', 'Local Loss', 'Cloud Formation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Remote Sensing']",11,"Semantic segmentation in large-scale aerial images is an extremely challenging task. On one hand, the limited ground truth, as compared to the vast area the images cover, greatly hinders the development of supervised representation learning. On the other hand, the large footprint from remote sensing raises new challenges for semantic segmentation. In addition, the complex and ever changing image acquisition conditions further complicate the problem where domain shifting commonly occurs. In this paper, we exploit self-supervised contrastive learning (CL) methodologies for semantic segmentation in aerial imagery. In addition to performing CL at the feature level as most practices do, we add another level of contrastive learning, at the semantic level, taking advantage of the segmentation output from the downstream task. Further, we embed local mutual information in the semantic-level CL to enforce local consistency. This has largely enhanced the representation power at each pixel and improved the generalization capacity of the trained model. We refer to the proposed approach as multi-level contrastive learning with local consistency (mCL-LC). The experimental results on different benchmarks indicate that the proposed mCL-LC exhibits superior performance as compared to other state-of-the-art contrastive learning frameworks for the semantic segmentation task. mCL-LC also carries better generalization capacity especially when domain shifting exists."
Semantic Segmentation of Degraded Images Using Layer-Wise Feature Adjustor,"Kazuki Endo, Masayuki Tanaka, Masatoshi Okutomi","Tokyo Institute of Technology, Meguro-ku, Tokyo, Japan; Teikyo Heisei University, Nakano-ku, Tokyo, Japan",100,Japan,0,,"Semantic segmentation of degraded images is important for practical applications such as autonomous driving and surveillance systems. The degradation level, which represents the strength of degradation, is usually unknown in practice. Therefore, the semantic segmentation algorithm needs to take account of various levels of degradation. In this paper, we propose a convolutional neural network of semantic segmentation which can cope with various levels of degradation. The proposed network is based on the knowledge distillation from a source network trained with only clean images. More concretely, the proposed network is trained to acquire multi-layer features keeping consistency with the source network, while adjusting for various levels of degradation. The effectiveness of the proposed method is confirmed for different types of degradations: JPEG distortion, Gaussian blur and salt&pepper noise. The experimental comparisons validate that the proposed network outperforms existing networks for semantic segmentation of degraded images with various degradation levels.",https://openaccess.thecvf.com/content/WACV2023/html/Endo_Semantic_Segmentation_of_Degraded_Images_Using_Layer-Wise_Feature_Adjustor_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Endo_Semantic_Segmentation_of_Degraded_Images_Using_Layer-Wise_Feature_Adjustor_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030815/,"['Degradation', 'Knowledge engineering', 'Image recognition', 'Semantic segmentation', 'Surveillance', 'Transform coding', 'Object detection']","['Semantic Segmentation', 'Convolutional Neural Network', 'Surveillance System', 'Clear Image', 'Gaussian Blur', 'Level Of Degradation', 'Semantic Segmentation Network', 'Typical Degradation', 'Source Network', 'Multilayer Feature', 'Pepper Noise', 'Semantic Segmentation Algorithms', 'Root Mean Square Error', 'Mean Square Error', 'Digital Images', 'Image Features', 'Data Augmentation', 'High-resolution Data', 'Training Status', 'Imaging Performance', 'Low-resolution Data', 'Mixed Training', 'Target Network', 'Semantic Network']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",2,"Semantic segmentation of degraded images is important for practical applications such as autonomous driving and surveillance systems. The degradation level, which represents the strength of degradation, is usually unknown in practice. Therefore, the semantic segmentation algorithm needs to take account of various levels of degradation. In this paper, we propose a convolutional neural network of semantic segmentation which can cope with various levels of degradation. The proposed network is based on the knowledge distillation from a source network trained with only clean images. More concretely, the proposed network is trained to acquire multi-layer features keeping consistency with the source network, while adjusting for various levels of degradation. The effectiveness of the proposed method is confirmed for different types of degradations: JPEG distortion, Gaussian blur and salt&pepper noise. The experimental comparisons validate that the proposed network outperforms existing networks for semantic segmentation of degraded images with various degradation levels."
Semantics Guided Contrastive Learning of Transformers for Zero-Shot Temporal Activity Detection,"Sayak Nag, Orpaz Goldstein, Amit K. Roy-Chowdhury","University of California, Riverside, USA; Amazon, USA",50,USA,50,USA,"Zero-shot temporal activity detection (ZSTAD) is the problem of simultaneous temporal localization and classification of activity segments that are previously unseen during training. This is achieved by transferring the knowledge learned from semantically-related seen activities. This ability to reason about unseen concepts without supervision makes ZSTAD very promising for applications where the acquisition of annotated training videos is difficult. In this paper, we design a transformer-based framework titled TranZAD, which streamlines the detection of unseen activities by casting ZSTAD as a direct set-prediction problem, removing the need for hand-crafted designs and manual post-processing. We show how a semantic information-guided contrastive learning strategy can effectively train TranZAD for the zero-shot setting, enabling the efficient transfer of knowledge from the seen to the unseen activities. To reduce confusion between unseen activities and unrelated background information in videos, we introduce a more efficient method of computing the background class embedding by dynamically adapting it as part of the end-to-end learning. Additionally, unlike existing work on ZSTAD, we do not assume the knowledge of which classes are unseen during training and use the visual and semantic information of only the seen classes for the knowledge transfer. This makes TranZAD more viable for practical scenarios, which we evaluate by conducting extensive experiments on Thumos'14 and Charades.",https://openaccess.thecvf.com/content/WACV2023/html/Nag_Semantics_Guided_Contrastive_Learning_of_Transformers_for_Zero-Shot_Temporal_Activity_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nag_Semantics_Guided_Contrastive_Learning_of_Transformers_for_Zero-Shot_Temporal_Activity_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030581/,"['Training', 'Location awareness', 'Visualization', 'Adaptation models', 'Computational modeling', 'Semantics', 'Manuals']","['Self-supervised Learning', 'Visual Information', 'Knowledge Transfer', 'Background Information', 'Detection Of Activity', 'Semantic Information', 'Temporal Localization', 'Video Information', 'Activation Segment', 'Background Class', 'Training Data', 'Training Phase', 'Object Detection', 'Visual Features', 'Multilayer Perceptron', 'Semantic Similarity', 'Activity Prediction', 'Activity Classification', 'Contrastive Loss', 'Temporal Overlap', 'Semantic Embedding', 'Unseen Classes', 'Transformer Decoder', 'Semantic Space', 'Proposal Generation', 'Temporal Coordination', 'Two-stage Detectors', 'Detection Head', '3D Convolutional Network', 'Temporal Coherence']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",5,"Zero-shot temporal activity detection (ZSTAD) is the problem of simultaneous temporal localization and classification of activity segments that are previously unseen during training. This is achieved by transferring the knowledge learned from semantically-related seen activities. This ability to reason about unseen concepts without supervision makes ZSTAD very promising for applications where the acquisition of annotated training videos is difficult. In this paper, we design a transformer-based framework titled TranZAD, which streamlines the detection of unseen activities by casting ZSTAD as a direct set-prediction problem, removing the need for hand-crafted designs and manual post-processing. We show how a semantic information-guided contrastive learning strategy can effectively train TranZAD for the zero-shot setting, enabling the efficient transfer of knowledge from the seen to the unseen activities. To reduce confusion between unseen activities and unrelated background information in videos, we introduce a more efficient method of computing the background class embedding by dynamically adapting it as part of the end-to-end learning. Additionally, unlike existing work on ZSTAD, we do not assume the knowledge of which classes are unseen during training and use the visual and semantic information of only the seen classes for the knowledge transfer. This makes TranZAD more viable for practical scenarios, which we evaluate by conducting extensive experiments on Thumos’14 and Charades."
Semantics-Depth-Symbiosis: Deeply Coupled Semi-Supervised Learning of Semantics and Depth,"Nitin Bansal, Pan Ji, Junsong Yuan, Yi Xu","OPPO US Research Center, USA; State University of New York at Buffalo, USA",50,USA,50,USA,"Multi-task learning (MTL) paradigm focuses on jointly learning two or more tasks, aiming for an improvement w.r.t model's generalizability, performance, and training/inference memory footprint. The aforementioned benefits become ever so indispensable in the case of training for vision-related dense prediction tasks. In this work, we tackle the MTL problem of two dense tasks, i.e., semantic segmentation and depth estimation, and present a novel attention module called Cross-Channel Attention Module (CCAM), which facilitates effective feature sharing along each channel between the two tasks, leading to mutual performance gain with a negligible increase in trainable parameters. In a symbiotic spirit, we also formulate novel data augmentations for the semantic segmentation task using predicted depth called AffineMix, and one using predicted semantics called ColorAug, for depth estimation task. Finally, we validate the performance gain of the proposed method on the Cityscapes and ScanNet dataset. which helps us achieve state-of-the-art results for a semi-supervised joint model based on depth estimation and semantic segmentation.",https://openaccess.thecvf.com/content/WACV2023/html/Bansal_Semantics-Depth-Symbiosis_Deeply_Coupled_Semi-Supervised_Learning_of_Semantics_and_Depth_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bansal_Semantics-Depth-Symbiosis_Deeply_Coupled_Semi-Supervised_Learning_of_Semantics_and_Depth_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030271/,"['Training', 'Symbiosis', 'Computer vision', 'Semantic segmentation', 'Semantics', 'Estimation', 'Performance gain']","['Semantic', 'Data Augmentation', 'Semantic Segmentation', 'Performance Gain', 'Attention Module', 'Semantic Task', 'Depth Estimation', 'Multi-task Learning', 'Semantic Segmentation Task', 'Convolutional Neural Network', 'Material For Details', 'ImageNet', 'Supplementary Materials For Details', 'Transfer Characteristics', 'Spatial Attention', 'Matrix M', 'Object Motion', 'Channel Dimension', 'Convolutional Block', 'Intermediate Features', 'Decoder Module', 'Relative Absolute Error', 'Pseudo Labels', 'Joint Training', 'Channel Attention', 'Label Space', 'Semantic Labels', 'Multi-task Learning Model', 'Basic Architecture']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"Multi-task learning (MTL) paradigm focuses on jointly learning two or more tasks, aiming for an improvement w.r.t model’s generalizability, performance, and training/inference memory footprint. The aforementioned benefits become ever so indispensable in the case of training for vision-related dense prediction tasks. In this work, we tackle the MTL problem of two dense tasks, i.e., semantic segmentation and depth estimation, and present a novel attention module called Cross-Channel Attention Module (CCAM), which facilitates effective feature sharing along each channel between the two tasks, leading to mutual performance gain with a negligible increase in trainable parameters. In a symbiotic spirit, we also formulate novel data augmentations for the semantic segmentation task using predicted depth called AffineMix, and one using predicted semantics called ColorAug, for depth estimation task. Finally, we validate the performance gain of the proposed method on the Cityscapes and ScanNet dataset. which helps us achieve state-of-the-art results for a semi-supervised joint model based on depth estimation and semantic segmentation."
Semi-Supervised Domain Adaptation With Auto-Encoder via Simultaneous Learning,"Md Mahmudur Rahman, Rameswar Panda, Mohammad Arif Ul Alam",MIT-IBM Watson AI Lab; University of Massachusetts Lowell,100,USA,0,,"We present a new semi-supervised domain adaptation framework that combines a novel auto-encoder-based domain adaptation model with a simultaneous learning scheme providing stable improvements over state-of-the-art domain adaptation models. Our framework holds strong distribution matching property by training both source and target auto-encoders using a novel simultaneous learning scheme on a single graph with an optimally modified MMD loss objective function. Additionally, we design a semi-supervised classification approach by transferring the aligned domain invariant feature spaces from source domain to the target domain. We evaluate on three datasets and show proof that our framework can effectively solve both fragile convergence (adversarial) and weak distribution matching problems between source and target feature space (discrepancy) with a high 'speed' of adaptation requiring a very low number of iterations.",https://openaccess.thecvf.com/content/WACV2023/html/Rahman_Semi-Supervised_Domain_Adaptation_With_Auto-Encoder_via_Simultaneous_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rahman_Semi-Supervised_Domain_Adaptation_With_Auto-Encoder_via_Simultaneous_Learning_WACV_2023_paper.pdf,,,2210.09486,main,Poster,https://ieeexplore.ieee.org/document/10030756/,"['Training', 'Adaptation models', 'Computer vision', 'Computational modeling', 'Linear programming', 'Convergence']","['Domain Adaptation', 'Simultaneous Learning', 'Feature Space', 'Target Features', 'Target Domain', 'Source Characteristics', 'Source Domain', 'Target Training', 'Distribution Matching', 'Domain-invariant Features', 'Loss Function', 'Data Sources', 'Dimensional Space', 'Loss Of Diversity', 'Classification Network', 'Generative Adversarial Networks', 'Classifier Training', 'Representation Of Space', 'Model Discrimination', 'Source Images', 'Unlabeled Data', 'Bottleneck Layer', 'Target Domain Data', 'Maximum Mean Discrepancy', 'Autoencoder Training', 'Course Of Learning', 'Labeled Source Domain', 'Reconstruction Loss', 'Source Domain Data', 'Target Data']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Vision + language and/or other modalities']",2,"We present a new semi-supervised domain adaptation framework that combines a novel auto-encoder-based domain adaptation model with a simultaneous learning scheme providing stable improvements over state-of-the-art domain adaptation models. Our framework holds strong distribution matching property by training both source and target auto-encoders using a novel simultaneous learning scheme on a single graph with an optimally modified MMD loss objective function. Additionally, we design a semi-supervised classification approach by transferring the aligned domain invariant feature spaces from source domain to the target domain. We evaluate on three datasets and show proof that our framework can effectively solve both fragile convergence (adversarial) and weak distribution matching problems between source and target feature space (discrepancy) with a high ‘speed’ of adaptation requiring a very low number of iterations."
Semi-Supervised Learning for Low-Light Image Restoration Through Quality Assisted Pseudo-Labeling,"Sameer Malik, Rajiv Soundararajan","Indian Institute of Science, Bengaluru, India",100,India,0,,"Convolutional neural networks have been successful in restoring images captured under poor illumination conditions addressing multiple challenges such as contrast enhancement, denoising, and color cast removal. Nevertheless, such approaches require a large number of paired low-light and ground truth images for training. Thus, we study the problem of semi-supervised learning for low-light image restoration when limited low-light images have ground truth labels. Our main contributions in this work are twofold. We first deploy an ensemble of low-light restoration networks to restore the unlabeled images and generate a set of potential pseudo-labels. We model the contrast distortions in the labeled set to generate different sets of training data and create the ensemble of networks. We then design a contrastive self-supervised learning based image quality measure to obtain the pseudo-label among the images restored by the ensemble. We show that training the restoration network with the pseudo-labels allows us to achieve excellent restoration performance even with very few labeled pairs. We conduct extensive experiments on three popular low-light image restoration datasets to show the superior performance of our semi-supervised low-light image restoration compared to other approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Malik_Semi-Supervised_Learning_for_Low-Light_Image_Restoration_Through_Quality_Assisted_Pseudo-Labeling_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Malik_Semi-Supervised_Learning_for_Low-Light_Image_Restoration_Through_Quality_Assisted_Pseudo-Labeling_WACV_2023_paper.pdf,,https://github.com/sameerIISc/SSL-LLR,,main,Poster,https://ieeexplore.ieee.org/document/10030337/,"['Training', 'Image quality', 'Training data', 'Lighting', 'Self-supervised learning', 'Semisupervised learning', 'Distortion']","['Semi-supervised Learning', 'Low-light Image', 'Training Dataset', 'Convolutional Neural Network', 'Image Quality', 'Self-supervised Learning', 'Ground Truth Image', 'Ensemble Of Networks', 'Unlabeled Images', 'Restoration Performance', 'Structural Similarity', 'Model Performance', 'Quality Assessment', 'Convolutional Neural Network Model', 'Ensemble Model', 'Unlabeled Data', 'Output Image', 'Simple Architecture', 'Ensemble Approach', 'Structural Similarity Index', 'Model Quality Assessment', 'Distortion Model', 'Natural Scene Statistics', 'Multi-scale Architecture', 'Semi-supervised Learning Approach', 'Quality Assessment Methods', 'Poor Contrast', 'Multi-scale Network']","['Algorithms: Computational photography', 'image and video synthesis', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",10,"Convolutional neural networks have been successful in restoring images captured under poor illumination conditions. Nevertheless, such approaches require a large number of paired low-light and ground truth images for training. Thus, we study the problem of semi-supervised learning for low-light image restoration when limited low-light images have ground truth labels. Our main contributions in this work are twofold. We first deploy an ensemble of low-light restoration networks to restore the unlabeled images and generate a set of potential pseudo-labels. We model the contrast distortions in the labeled set to generate different sets of training data and create the ensemble of networks. We then design a contrastive self-supervised learning based image quality measure to obtain the pseudo-label among the images restored by the ensemble. We show that training the restoration network with the pseudo-labels allows us to achieve excellent restoration performance even with very few labeled pairs. We conduct extensive experiments on three popular low-light image restoration datasets to show the superior performance of our semi-supervised low-light image restoration compared to other approaches. Project page is available at https://github.com/sameerIISc/SSL-LLR."
Semi-Supervised Learning for Sparsely-Labeled Sequential Data: Application to Healthcare Video Processing,"Florian Dubost, Erin Hong, Siyi Tang, Nandita Bhaskhar, Christopher Lee-Messer, Daniel Rubin",Stanford University,100,USA,0,,"Labeled data is a critical resource for training and evaluating machine learning models. However, many real-life datasets are only partially labeled. We propose a semi-supervised machine learning training strategy to improve event detection performance on sequential data, such as video recordings, when only sparse labels are available, such as event start times without their corresponding end times. Our method uses noisy guesses of the events' end times to train event detection models. Depending on how conservative these guesses are, mislabeled samples may be introduced into the training set. We further propose a mathematical model for explaining and estimating the evolution of the classification performance for increasingly noisier end time estimates. We show that neural networks can improve their detection performance by leveraging more training data with less conservative approximations despite the higher proportion of incorrect labels. We adapt sequential versions of CIFAR-10 and MNIST, and use the Berkeley MHAD and HMBD51 video datasets to empirically evaluate our method, and find that our risk-tolerant strategy outperforms conservative estimates by 3.5 points of mean average precision for CIFAR, 30 points for MNIST, 3 points for MHAD, and 14 points for HMBD51. Then, we leverage the proposed training strategy to tackle a real-life application: processing continuous video recordings of epilepsy patients, and show that our method outperforms baseline labeling methods by 17 points of average precision, and reaches a classification performance similar to that of fully supervised models.",https://openaccess.thecvf.com/content/WACV2023/html/Dubost_Semi-Supervised_Learning_for_Sparsely-Labeled_Sequential_Data_Application_to_Healthcare_Video_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dubost_Semi-Supervised_Learning_for_Sparsely-Labeled_Sequential_Data_Application_to_Healthcare_Video_WACV_2023_paper.pdf,,https://github.com/fpgdubost/CIFAR-10-Sparsely-Labeled-Sequential-Data,2011.14101,main,Poster,https://ieeexplore.ieee.org/document/10030874/,"['Training', 'Event detection', 'Neural networks', 'Training data', 'Machine learning', 'Semisupervised learning', 'Mathematical models']","['Semi-supervised Learning', 'Training Set', 'Classification Performance', 'Detection Performance', 'End Time', 'Start Time', 'Training Strategy', 'Event Detection', 'Average Precision', 'Development Performance', 'Continuous Recording', 'Mean Average Precision', 'Video Dataset', 'Incorrect Labels', 'Sparse Labeling', 'Evaluation Of Machine Learning Models', 'Convolutional Neural Network', 'Positive Samples', 'Validation Set', 'Convolutional Layers', 'Structural Similarity Index Measure', 'Semi-supervised Methods', 'Pseudo Labels', 'Frames Per Second', 'Risk Level', 'Parameters A1', 'Negative Elements', 'Negative Training', 'Suction', 'End Of Event']","['Applications: Biomedical/healthcare/medicine', 'Social good']",,"Labeled data is a critical resource for training and evaluating machine learning models. However, many real-life datasets are only partially labeled. We propose a semi-supervised machine learning training strategy to improve event detection performance on sequential data, such as video recordings, when only sparse labels are available, such as event start times without their corresponding end times. Our method uses noisy guesses of the events’ end times to train event detection models. Depending on how conservative these guesses are, mislabeled samples may be introduced into the training set. We further propose a mathematical model for explaining and estimating the evolution of the classification performance for increasingly noisier end time estimates. We show that neural networks can improve their detection performance by leveraging more training data with less conservative approximations despite the higher proportion of incorrect labels. We adapt sequential versions of CIFAR-10 and MNIST, and use the Berkeley MHAD and HMBD51 video datasets to empirically evaluate our method, and find that our risk-tolerant strategy outperforms conservative estimates by 3.5 points of mean average precision for CIFAR, 30 points for MNIST, 3 points for MHAD, and 14 points for HMBD51. Then, we leverage the proposed training strategy to tackle a real-life application: processing continuous video recordings of epilepsy patients, and show that our method outperforms baseline labeling methods by 17 points of average precision, and reaches a classification performance similar to that of fully supervised models. We share part of the code for this article at the following repository: fpgdubost/CIFAR-10-Sparsely-Labeled-Sequential-Data."
Separating Partially-Polarized Diffuse and Specular Reflection Components Under Unpolarized Light Sources,"Soma Kajiyama, Taihe Piao, Ryo Kawahara, Takahiro Okabe","Department of Artificial Intelligence, Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka 820-8502, Japan",100,Japan,0,,"Separating diffuse and specular reflection components observed on an object surface is important for preprocessing of various computer vision techniques. Conventionally, diffuse-specular separation based on the polarimetric and color clues assumes that the diffuse/specular reflection components are unpolarized/partially polarized under unpolarized light sources. However, the diffuse reflection component is partially polarized in fact, because the diffuse reflectance is maximal when the polarization direction is parallel to the outgoing plane. Accordingly, we propose a method for separating partially-polarized diffuse and specular reflection components on the basis of the polarization reflection model and the dichromatic reflection model. In particular, our method enables us not only to achieve diffuse-specular separation but also to estimate the polarimetric properties of the object surface from a single color polarization image. We experimentally confirmed that our method performs better than the method assuming unpolarized diffuse reflection components.",https://openaccess.thecvf.com/content/WACV2023/html/Kajiyama_Separating_Partially-Polarized_Diffuse_and_Specular_Reflection_Components_Under_Unpolarized_Light_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kajiyama_Separating_Partially-Polarized_Diffuse_and_Specular_Reflection_Components_Under_Unpolarized_Light_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030760/,"['Reflectivity', 'Computer vision', 'Polarization', 'Image color analysis', 'Reflection', 'Light sources']","['Light Source', 'Diffuse Reflectance', 'Specular Reflection', 'Reflection Component', 'Specular Component', 'Specular Reflection Components', 'Diffuse Reflection Components', 'Surface Properties', 'Single Image', 'Polar Angle', 'Polarization Direction', 'Object Surface', 'Computer Vision Techniques', 'Polarization Imaging', 'Differences In Use', 'Linearly Polarized', 'Polarized Light', 'Independent Component Analysis', 'Light Color', 'Synthetic Images', 'Surface Normals', 'Color Information', 'RGB Color Space', 'Inliers', 'White Light Source', 'Random Sample Consensus', 'Peak Signal-to-noise Ratio', 'Dc Component', 'Structural Similarity Index Measure']","['Algorithms: Low-level and physics-based vision', 'Computational photography', 'image and video synthesis']",1,"Separating diffuse and specular reflection components observed on an object surface is important for preprocessing of various computer vision techniques. Conventionally, diffuse-specular separation based on the polarimetric and color clues assumes that the diffuse/specular reflection components are unpolarized/partially polarized under unpolarized light sources. However, the diffuse reflection component is partially polarized in fact, because the diffuse reflectance is maximal when the polarization direction is parallel to the outgoing plane. Accordingly, we propose a method for separating partially-polarized diffuse and specular reflection components on the basis of the polarization reflection model and the dichromatic reflection model. In particular, our method enables us not only to achieve diffuse-specular separation but also to estimate the polarimetric properties of the object surface from a single color polarization image. We experimentally confirmed that our method performs better than the method assuming unpolarized diffuse reflection components."
Seq-UPS: Sequential Uncertainty-Aware Pseudo-Label Selection for Semi-Supervised Text Recognition,"Gaurav Patel, Jan P. Allebach, Qiang Qiu","School of Electrical and Computer Engineering, Purdue University, USA",100,USA,0,,"This paper looks at semi-supervised learning (SSL) for image-based text recognition. One of the most popular SSL approaches is pseudo-labeling (PL). PL approaches assign labels to unlabeled data before re-training the model with a combination of labeled and pseudo-labeled data. However, PL methods are severely degraded by noise and are prone to over-fitting to noisy labels, due to the inclusion of erroneous high confidence pseudo-labels generated from poorly calibrated models, thus, rendering threshold-based selection ineffective. Moreover, the combinatorial complexity of the hypothesis space and the error accumulation due to multiple incorrect autoregressive steps posit pseudo-labeling challenging for sequential self-training. To this end, we propose a pseudo-label generation and an uncertainty-based data selection framework for semi-supervised text recognition. We first use Beam-Search inference to yield highly probable hypotheses to assign pseudo-labels to the unlabelled examples. Then we adopt an ensemble of models, sampled by applying dropout, to obtain a robust estimate of the uncertainty associated with the prediction, considering both the character-level and word-level predictive distribution to select good quality pseudo-labels. Extensive experiments on several benchmark handwriting and scene-text datasets show that our method outperforms the baseline approaches and the previous state-of-the-art semi-supervised text-recognition methods.",https://openaccess.thecvf.com/content/WACV2023/html/Patel_Seq-UPS_Sequential_Uncertainty-Aware_Pseudo-Label_Selection_for_Semi-Supervised_Text_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Patel_Seq-UPS_Sequential_Uncertainty-Aware_Pseudo-Label_Selection_for_Semi-Supervised_Text_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030491/,"['Computer vision', 'Uncertainty', 'Text recognition', 'Semisupervised learning', 'Predictive models', 'Benchmark testing', 'Rendering (computer graphics)']","['Optical Character Recognition', 'Pseudo-label Selection', 'Uncertainty Estimation', 'Handwritten', 'Ensemble Model', 'Predictive Distribution', 'Unlabeled Data', 'Semi-supervised Learning', 'Semi-supervised Methods', 'Pseudo Labels', 'Recognition Framework', 'Hypothesis Space', 'Unlabeled Examples', 'Error Rate', 'Supplemental Material', 'Training Time', 'Input Image', 'Feature Maps', 'Measurement Uncertainty', 'Trainable Parameters', 'Word Error Rate', 'Recognition Model', 'Prediction Task', 'Word Predictability', 'Prior Art', 'Structure Of Space', 'Recognition Network', 'Semi-supervised Learning Framework', 'Low Uncertainty', 'Prediction Uncertainty']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'low-shot', 'semi-', 'self-', 'and un-supervised learning']",4,"This paper looks at semi-supervised learning (SSL) for image-based text recognition. One of the most popular SSL approaches is pseudo-labeling (PL). PL approaches assign labels to unlabeled data before re-training the model with a combination of labeled and pseudo-labeled data. However, PL methods are severely degraded by noise and are prone to over-fitting to noisy labels, due to the inclusion of erroneous high confidence pseudo-labels generated from poorly calibrated models, thus, rendering threshold-based selection ineffective. Moreover, the combinatorial complexity of the hypothesis space and the error accumulation due to multiple incorrect autoregressive steps posit pseudo-labeling challenging for sequence models. To this end, we propose a pseudo-label generation and an uncertainty-based data selection framework for semi-supervised text recognition. We first use Beam-Search inference to yield highly probable hypotheses to assign pseudo-labels to the unlabelled examples. Then we adopt an ensemble of models, sampled by applying dropout, to obtain a robust estimate of the uncertainty associated with the prediction, considering both the character-level and word-level predictive distribution to select good quality pseudo-labels. Extensive experiments on several benchmark handwriting and scene-text datasets show that our method outperforms the baseline approaches and the previous state-of- the-art semi-supervised text-recognition methods."
Sim2RealVS: A New Benchmark for Video Stabilization With a Strong Baseline,"Qi Rao, Xin Yu, Shant Navasardyan, Humphrey Shi","ReLER Lab, AAII, University of Technology Sydney; Picsart AI Research (PAIR); University of Technology Sydney",66.66666667,Australia,33.33333333,Ukraine,"Video stabilization is highly desirable when videos undergo severe jittering artifacts. The difficulty of obtaining sufficient training data obstructs the development of video stabilization. In this work, we address this issue by presenting a Sim2RealVS benchmark with more than 1,300 pairs of shaky and stable videos. Our benchmark is curated by an in-game simulator with diverse scenes and various jittering effects. Moreover, we propose a simple yet strong baseline approach, named Motion-Trajectory Smoothing Network (MTSNet), by fully exploiting our Sim2RealVS data. Our MTSNet consists of three main steps: motion estimation, global trajectory smoothing and frame warping. In motion estimation, we design a Motion Correction and Completion (MCC) module to rectify the optical flow with low confidence, such as in textureless regions, thus providing more consistent motion estimation for next steps. Benefiting from our synthetic data, we can explicitly learn a Trajectory Smoothing Transformer (TST) with ground-truth supervision to smooth global trajectories. In training TST, we propose two fully-supervised losses, i.e., a motion magnitude similarity loss and a motion tendency similarity loss. After training, our TST is able to produce smooth motion trajectories for the shaky input videos. Extensive qualitative and quantitative results demonstrate that our MTSNet achieves superior performance on both synthetic and real-world data.",https://openaccess.thecvf.com/content/WACV2023/html/Rao_Sim2RealVS_A_New_Benchmark_for_Video_Stabilization_With_a_Strong_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Rao_Sim2RealVS_A_New_Benchmark_for_Video_Stabilization_With_a_Strong_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030794/,"['Training', 'Optical losses', 'Smoothing methods', 'Motion estimation', 'Training data', 'Benchmark testing', 'Propulsion']","['Strong Baseline', 'Video Stabilization', 'Transformer', 'Quantitative Results', 'Qualitative Results', 'Real-world Data', 'Motion Correction', 'Optical Flow', 'Similar Loss', 'Motion Estimation', 'Motion Trajectory', 'Smooth Trajectory', 'Variety Of Scenes', 'Deep Learning', 'Low-pass', 'Random Noise', 'Training Loss', 'Bilinear Interpolation', 'Stable Trajectory', 'Accurate Motion', 'Synthetic Benchmark', 'Smoothing Step', 'Parallax', 'Stable Pairs', 'Confidence Map', 'Motion Field', 'Block Stacking', 'Homography']",['Algorithms: Low-level and physics-based vision'],5,"Video stabilization is highly desirable when videos undergo severe jittering artifacts. The difficulty of obtaining sufficient training data obstructs the development of video stabilization. In this work, we address this issue by presenting a Sim2RealVS benchmark with more than 1,300 pairs of shaky and stable videos. Our benchmark is curated by an in-game simulator with diverse scenes and various jittering effects. Moreover, we propose a simple yet strong baseline approach, named Motion-Trajectory Smoothing Network (MTSNet), by fully exploiting our Sim2RealVS data. Our MTSNet consists of three main steps: motion estimation, global trajectory smoothing and frame warping. In motion estimation, we design a Motion Correction and Completion (MCC) module to rectify the optical flow with low confidence, such as in textureless regions, thus providing more consistent motion estimation for next steps. Benefiting from our synthetic data, we can explicitly learn a Trajectory Smoothing Transformer (TST) with ground-truth supervision to smooth global trajectories. In training TST, we propose two fully-supervised losses, i.e., a motion magnitude similarity loss and a motion tendency similarity loss. After training, our TST is able to produce smooth motion trajectories for the shaky input videos. Extensive qualitative and quantitative results demonstrate that our MTSNet achieves superior performance on both synthetic and real-world data."
Sim2real Transfer Learning for Point Cloud Segmentation: An Industrial Application Case on Autonomous Disassembly,"Chengzhi Wu, Xuelei Bi, Julius Pfrommer, Alexander Cebulla, Simon Mangold, Jürgen Beyerer","Fraunhofer Institute of Optronics, System Technologies and Image Exploitation IOSB, Germany; Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany; wbk Institute of Production Science, Karlsruhe Institute of Technology, Germany",100,Germany,0,,"On robotics computer vision tasks, generating and annotating large amounts of data from real-world for the use of deep learning-based approaches is often difficult or even impossible. A common strategy for solving this problem is to apply simulation-to-reality (sim2real) approaches with the help of simulated scenes. While the majority of current robotics vision sim2real work focuses on image data, we present an industrial application case that uses sim2real transfer learning for point cloud data. We provide insights on how to generate and process synthetic point cloud data in order to achieve better performance when the learned model is transferred to real-world data. The issue of imbalanced learning is investigated using multiple strategies. A novel patch-based attention network is proposed additionally to tackle this problem.",https://openaccess.thecvf.com/content/WACV2023/html/Wu_Sim2real_Transfer_Learning_for_Point_Cloud_Segmentation_An_Industrial_Application_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wu_Sim2real_Transfer_Learning_for_Point_Cloud_Segmentation_An_Industrial_Application_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030362/,"['Point cloud compression', 'Learning systems', 'Computer vision', 'Service robots', 'Transfer learning', 'Pipelines', 'Data models']","['Transfer Learning', 'Point Cloud', 'Point Cloud Segmentation', 'Industrial Application Cases', 'Autonomous Disassembly', 'Real-world Data', 'Data In Order', 'Vision Tasks', 'Point Cloud Data', 'Simulated Scene', 'Imbalanced Learning', 'Neural Network', 'Deep Learning', 'Network Model', 'Data Augmentation', 'Augmentation Methods', 'Synthetic Images', 'Domain Adaptation', 'Neighboring Points', 'Imbalance Problem', 'Pre-training Process', 'Point Cloud Dataset', 'Point Cloud Generation', 'Fine-tuning Process', 'Ground Truth Segmentation', 'Clamping System', 'Fine-tuning Step', 'Synthetic Generation', 'Motor Type', 'Real-world Scenes']",['Applications: Robotics'],6,"On robotics computer vision tasks, generating and annotating large amounts of data from real-world for the use of deep learning-based approaches is often difficult or even impossible. A common strategy for solving this problem is to apply simulation-to-reality (sim2real) approaches with the help of simulated scenes. While the majority of current robotics vision sim2real work focuses on image data, we present an industrial application case that uses sim2real transfer learning for point cloud data. We provide insights on how to generate and process synthetic point cloud data in order to achieve better performance when the learned model is transferred to real-world data. The issue of imbalanced learning is investigated using multiple strategies. A novel patch-based attention network is proposed additionally to tackle this problem."
SimGlim: Simplifying Glimpse Based Active Visual Reconstruction,"Abhishek Jha, Soroush Seifi, Tinne Tuytelaars","ESAT-PSI, KU Leuven",100,Belgium,0,,"An agent with a limited field of view needs to sample the most informative local observations of an environment in order to model the global context. Current works train this selection strategy by defining a complex architecture built upon features learned through convolutional encoders. In this paper, we first discuss why vision transformers are better suited than CNNs for such an agent. Next, we propose a simple transformer based active visual sampling model, called ""SimGlim"", which utilises transformer's inherent self-attention architecture to sequentially predict the best next location based on the current observable environment. We show the efficacy of our proposed method on the task of image reconstruction in the partial observable setting and compare our model against existing state-of-the-art active visual reconstruction methods. Finally, we provide ablations for the parameters of our design choice to understand their importance in the overall architecture.",https://openaccess.thecvf.com/content/WACV2023/html/Jha_SimGlim_Simplifying_Glimpse_Based_Active_Visual_Reconstruction_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jha_SimGlim_Simplifying_Glimpse_Based_Active_Visual_Reconstruction_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030774/,"['Visualization', 'Computer vision', 'Reconstruction algorithms', 'Predictive models', 'Transformers', 'Data models', 'Task analysis']","['Image Reconstruction', 'Global Context', 'Partial Observation', 'Limited Field Of View', 'Vision Transformer', 'Number Of Observations', 'Random Selection', 'Attention Mechanism', 'Semantic Segmentation', 'Fully-connected Layer', 'Local Image', 'Transformer Model', 'Words In Sentences', 'Max Values', 'Output Of Module', 'Objects In The Scene', 'Reconstruction Loss', 'Error Map', 'Outdoor Scenes', 'Salient Object', 'Multi-head Self-attention', 'Pretext Task', 'Salient Regions', 'MS COCO Dataset', 'Reconstruction Task', 'Critical Region', 'Active Agents', 'Part Of The Image', 'Position Embedding', 'Intermediate Representation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Computational photography', 'image and video synthesis']",1,"In active visual exploration, an agent with a limited field of view needs to sample the most informative local observations of an environment in order to model the global context. Current works train this selection strategy by defining a complex architecture built upon features learned through convolutional encoders. In this paper, we first discuss why vision transformers are better suited than CNNs for such an agent. Next, we propose a simple transformer-based active visual sampling model, called ""SimGlim"", which utilises transformer’s inherent self-attention architecture to sequentially predict the best next location based on the current observable environment. We show the efficacy of our proposed method on the task of image reconstruction in the partial observable setting and compare our model against existing state-of-the-art active visual reconstruction methods. Finally, we provide ablations for the parameters of our design choice to understand their importance in the overall architecture."
Similarity Contrastive Estimation for Self-Supervised Soft Contrastive Learning,"Julien Denize, Jaonary Rabarisoa, Astrid Orcesi, Romain Hérault, Stéphane Canu","Normandie Univ, INSA Rouen, LITIS, 76801, Saint Etienne du Rouvray, France; Universit´e Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France",100,France,0,,"Contrastive representation learning has proven to be an effective self-supervised learning method. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations, or semantic similarity, between the instances. Contrastive learning implicitly learns relations but considering all negatives as noise harms the quality of the learned relations. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive learning one. Instead of hard classifying positives and negatives, we estimate from one view of a batch a continuous distribution to push or pull instances based on their semantic similarities. This target similarity distribution is sharpened to eliminate noisy relations. The model predicts for each instance, from another view, the target distribution while contrasting its positive with negatives. Experimental results show that SCE is Top-1 on the ImageNet linear evaluation protocol at 100 pretraining epochs with 72.1% accuracy and is competitive with state-of-the-art algorithms by reaching 75.4% for 200 epochs with multi-crop. We also show that SCE is able to generalize to several tasks. Source code is available here: https://github.com/CEA-LIST/SCE.",https://openaccess.thecvf.com/content/WACV2023/html/Denize_Similarity_Contrastive_Estimation_for_Self-Supervised_Soft_Contrastive_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Denize_Similarity_Contrastive_Estimation_for_Self-Supervised_Soft_Contrastive_Learning_WACV_2023_paper.pdf,,https://github.com/CEA-LIST/SCE,2111.14585,main,Poster,https://ieeexplore.ieee.org/document/10030549/,"['Training', 'Representation learning', 'Protocols', 'Source coding', 'Semantics', 'Estimation', 'Self-supervised learning']","['Self-supervised Learning', 'Contrast Estimates', 'Similar Distribution', 'Semantic Similarity', 'Evaluation Protocol', 'Target Distribution', 'Instances In The Dataset', 'Batch Size', 'State Of The Art', 'Implementation Details', 'Small Datasets', 'Object Detection', 'Data Augmentation', 'Transfer Learning', 'Stochastic Gradient Descent', 'Training Images', 'Projector', 'Temperature Parameters', 'Instance Segmentation', 'Contrast Objective', 'Pretext Task', 'Soft Approach', 'Soft Objects']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",11,"Contrastive representation learning has proven to be an effective self-supervised learning method. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations, or semantic similarity, between the instances. Contrastive learning implicitly learns relations but considering all negatives as noise harms the quality of the learned relations. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive learning one. Instead of hard classifying positives and negatives, we estimate from one view of a batch a continuous distribution to push or pull instances based on their semantic similarities. This target similarity distribution is sharpened to eliminate noisy relations. The model predicts for each instance, from another view, the target distribution while contrasting its positive with negatives. Experimental results show that SCE is Top-1 on the ImageNet linear evaluation protocol at 100 pretraining epochs with 72.1% accuracy and is competitive with state-of-the-art algorithms by reaching 75.4% for 200 epochs with multi-crop. We also show that SCE is able to generalize to several tasks. Source code is available here: https://github.com/CEA-LIST/SCE."
Simultaneous Acquisition of High Quality RGB Image and Polarization Information Using a Sparse Polarization Sensor,"Teppei Kurita, Yuhi Kondo, Legong Sun, Yusuke Moriuchi",Sony Group Corporation,0,,100,Japan,"This paper proposes a novel polarization sensor structure and network architecture to obtain a high-quality RGB image and polarization information. Conventional polarization sensors can simultaneously acquire RGB images and polarization information, but the polarizers on the sensor degrade the quality of the RGB images. There is a trade-off between the quality of the RGB image and polarization information as fewer polarization pixels reduce the degradation of the RGB image but decrease the resolution of polarization information. Therefore, we propose an approach that resolves the trade-off by sparsely arranging polarization pixels on the sensor and compensating for low-resolution polarization information with higher resolution using the RGB image as a guide. Our proposed network architecture consists of an RGB image refinement network and a polarization information compensation network. We confirmed the superiority of our proposed network in compensating the differential component of polarization intensity by comparing its performance with state-of-the-art methods for similar tasks: depth completion. Furthermore, we confirmed that our approach could simultaneously acquire higher quality RGB images and polarization information than conventional polarization sensors, resolving the trade-off between the quality of RGB images and polarization information. The baseline code and newly generated real and synthetic large-scale polarization image datasets are available for further research and development.",https://openaccess.thecvf.com/content/WACV2023/html/Kurita_Simultaneous_Acquisition_of_High_Quality_RGB_Image_and_Polarization_Information_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kurita_Simultaneous_Acquisition_of_High_Quality_RGB_Image_and_Polarization_Information_WACV_2023_paper.pdf,,https://github.com/sony/polar-densification,2209.13106,main,Poster,https://ieeexplore.ieee.org/document/10031007/,"['Degradation', 'Computer vision', 'Image resolution', 'Codes', 'Network architecture', 'Task analysis', 'Research and development']","['Image Information', 'RGB Images', 'Polarization Information', 'Polar Sensor', 'Image Quality', 'Polarizability', 'Information Quality', 'Resolution Information', 'Conventional Sensors', 'Refinement Network', 'Compensation Network', 'Spatial Resolution', 'Large Amount Of Data', 'Real-world Data', 'Transformation Matrix', 'Image Sensor', 'Real-world Datasets', 'Polar Angle', 'Real Sense', 'Polar Components', 'Stokes Parameters', 'Noise Factors', 'Pixel Binning', 'White Filter', 'Number Of Training Images']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",5,"This paper proposes a novel polarization sensor structure and network architecture to obtain a high-quality RGB image and polarization information. Conventional polarization sensors can simultaneously acquire RGB images and polarization information, but the polarizers on the sensor degrade the quality of the RGB images. There is a trade-off between the quality of the RGB image and polarization information as fewer polarization pixels reduce the degradation of the RGB image but decrease the resolution of polarization information. Therefore, we propose an approach that resolves the trade-off by sparsely arranging polarization pixels on the sensor and compensating for low-resolution polarization information with higher resolution using the RGB image as a guide. Our proposed network architecture consists of an RGB image refinement network and a polarization information compensation network. We confirmed the superiority of our proposed network in compensating the differential component of polarization intensity by comparing its performance with state-of-the-art methods for similar tasks: depth completion. Furthermore, we confirmed that our approach could simultaneously acquire higher quality RGB images and polarization information than conventional polarization sensors, resolving the tradeoff between the quality of RGB images and polarization information. The baseline code and newly generated real and synthetic large-scale polarization image datasets are available for further research and development."
Single Image Super-Resolution via a Dual Interactive Implicit Neural Network,"Quan H. Nguyen, William J. Beksi",The University of Texas at Arlington,100,USA,0,,"In this paper, we introduce a novel implicit neural network for the task of single image super-resolution at arbitrary scale factors. To do this, we represent an image as a decoding function that maps locations in the image along with their associated features to their reciprocal pixel attributes. Since the pixel locations are continuous in this representation, our method can refer to any location in an image of varying resolution. To retrieve an image of a particular resolution, we apply a decoding function to a grid of locations each of which refers to the center of a pixel in the output image. In contrast to other techniques, our dual interactive neural network decouples content and positional features. As a result, we obtain a fully implicit representation of the image that solves the super-resolution problem at (real-valued) elective scales using a single model. We demonstrate the efficacy and flexibility of our approach against the state of the art on publicly available benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Nguyen_Single_Image_Super-Resolution_via_a_Dual_Interactive_Implicit_Neural_Network_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nguyen_Single_Image_Super-Resolution_via_a_Dual_Interactive_Implicit_Neural_Network_WACV_2023_paper.pdf,,,2210.12593,main,Poster,https://ieeexplore.ieee.org/document/10030562/,"['Computer vision', 'Adaptation models', 'Superresolution', 'Neural networks', 'Transforms', 'Computer architecture', 'Benchmark testing']","['Neural Network', 'Super-resolution', 'Dual Network', 'Single Image Super-resolution', 'Image Resolution', 'Scaling Factor', 'State Of The Art', 'Image Pixels', 'Local Image', 'Image Representation', 'Content Features', 'Position Features', 'Arbitrary Scale', 'Implicit Representation', 'Decoding Function', 'Deep Learning', 'Convolutional Neural Network', 'High-resolution Images', 'Convolutional Layers', 'Feature Maps', 'Low-resolution Images', 'Bicubic Interpolation', 'Global Coordinates', 'L1 Loss', 'Peak Signal-to-noise Ratio', 'Structural Similarity Index Measure', 'Implicit Function', 'Multilayer Perceptron', 'Target Resolution', 'Network Modules']","['Algorithms: Low-level and physics-based vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",9,"In this paper, we introduce a novel implicit neural network for the task of single image super-resolution at arbitrary scale factors. To do this, we represent an image as a decoding function that maps locations in the image along with their associated features to their reciprocal pixel attributes. Since the pixel locations are continuous in this representation, our method can refer to any location in an image of varying resolution. To retrieve an image of a particular resolution, we apply a decoding function to a grid of locations each of which refers to the center of a pixel in the output image. In contrast to other techniques, our dual interactive neural network decouples content and positional features. As a result, we obtain a fully implicit representation of the image that solves the super-resolution problem at (real-valued) elective scales using a single model. We demonstrate the efficacy and flexibility of our approach against the state of the art on publicly available benchmark datasets."
Single Stage Weakly Supervised Semantic Segmentation of Complex Scenes,"Peri Akiva, Kristin Dana",Rutgers University,100,USA,0,,"The costly process of obtaining semantic segmentation labels has driven research towards weakly supervised semantic segmentation (WSSS) methods, using only image-level, point, or box labels. Such annotations introduce limitations and challenges that results in overly-tuned methods specialized in specific domains or scene types. The over reliance of image-level based methods on generation of high quality class activation maps (CAMs) results in limited applicable dataset complexity range, mostly focusing on object centric scenes. Additionally, the lack of dense annotations requires methods to increase network complexity to obtain additional semantic information, often done through multiple stages of training and refinement. Here, we present a single-stage approach generalizable to a wide range of dataset complexities, that is trainable from scratch, without any dependency on pre-trained backbones, classification, or separate refinement tasks. We utilize point annotations to generate reliable, on-the-fly pseudo-masks through refined and spatially filtered features. We are to demonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as significantly outperform other SOTA WSSS methods on recent real-world datasets (CRAID, CityPersons, IAD, ADE20K, CityScapes) with up to 28.1% and 22.6% performance boosts compared to our single-stage and multi-stage baselines respectively.",https://openaccess.thecvf.com/content/WACV2023/html/Akiva_Single_Stage_Weakly_Supervised_Semantic_Segmentation_of_Complex_Scenes_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Akiva_Single_Stage_Weakly_Supervised_Semantic_Segmentation_of_Complex_Scenes_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030835/,"['Training', 'Computer vision', 'Annotations', 'Semantic segmentation', 'Semantics', 'Focusing', 'Benchmark testing']","['Semantic Segmentation', 'Single Stage', 'Weakly Supervised Semantic Segmentation', 'Classification Task', 'Benchmark Datasets', 'Training Stage', 'Segmentation Method', 'Spatial Filter', 'Complex Range', 'Class Activation Maps', 'Semantic Segmentation Methods', 'Wide Range Of Complexes', 'Active Region', 'Random Walk', 'Intersection Over Union', 'Object Location', 'Distance Map', 'Object Parts', 'Neighboring Pixels', 'New Ground', 'Early Iterations', 'Objects In The Scene', 'Refiner', 'Refinement Network', 'Ground Truth Points', 'Expansion Mechanism', 'Background Points', 'Image-level Labels', 'Object Scale', 'Standard Benchmark Datasets']","['Applications: Agriculture', 'Remote Sensing']",3,"The costly process of obtaining semantic segmentation labels has driven research towards weakly supervised semantic segmentation (WSSS) methods, using only image-level, point, or box labels. Such annotations introduce limitations and challenges that results in overly-tuned methods specialized in specific domains or scene types. The over reliance of image-level based methods on generation of high quality class activation maps (CAMs) results in limited applicable dataset complexity range, mostly focusing on object centric scenes. Additionally, the lack of dense annotations requires methods to increase network complexity to obtain additional semantic information, often done through multiple stages of training and refinement. Here, we present a single-stage approach generalizable to a wide range of dataset complexities, that is trainable from scratch, without any dependency on pre-trained backbones, classification, or separate refinement tasks. We utilize point annotations to generate reliable, on-the-fly pseudo-masks through refined and spatially filtered features. We are to demonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as significantly outperform other SOTA WSSS methods on recent real-world datasets (CRAID, CityPersons, IAD, ADE20K, CityScapes) with up to 28.1% and 22.6% performance boosts compared to our single-stage and multi-stage baselines respectively."
Single-Image HDR Reconstruction by Multi-Exposure Generation,"Phuoc-Hieu Le, Quynh Le, Rang Nguyen, Binh-Son Hua","VinAI Research, University of California San Diego; VinAI Research",50,USA,50,Vietnam,"High dynamic range (HDR) imaging is an indispensable technique in modern photography. Traditional methods focus on HDR reconstruction from multiple images, solving the core problems of image alignment, fusion, and tone mapping, yet having a perfect solution due to ghosting and other visual artifacts in the reconstruction. Recent attempts at single-image HDR reconstruction show a promising alternative: by learning to map pixel values to their irradiance using a neural network, one can bypass the align-and-merge pipeline completely yet still obtain a high-quality HDR image. In this work, we propose a weakly supervised learning method that inverts the physical image formation process for HDR reconstruction via learning to generate multiple exposures from a single image. Our neural network can invert the camera response to reconstruct pixel irradiance before synthesizing multiple exposures and hallucinating details in under- and over-exposed regions from a single input image. To train the network, we propose a representation loss, a reconstruction loss, and a perceptual loss applied on pairs of under- and over-exposure images and thus do not require HDR images for training. Our experiments show that our proposed model can effectively reconstruct HDR images. Our qualitative and quantitative results show that our method achieves state-of-the-art performance on the DrTMO dataset. Our code is available at https://github.com/VinAIResearch/single_image_hdr.",https://openaccess.thecvf.com/content/WACV2023/html/Le_Single-Image_HDR_Reconstruction_by_Multi-Exposure_Generation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Le_Single-Image_HDR_Reconstruction_by_Multi-Exposure_Generation_WACV_2023_paper.pdf,,https://github.com/VinAIResearch/single_image_hdr,2210.15897,main,Poster,https://ieeexplore.ieee.org/document/10030738/,"['Training', 'Photography', 'Image quality', 'Visualization', 'Computer vision', 'Neural networks', 'Supervised learning']","['High Dynamic Range', 'High Dynamic Range Reconstruction', 'Neural Network', 'Dynamic Range', 'Input Image', 'Single Image', 'Qualitative Results', 'Hallucinations', 'Multiple Images', 'Image Formation', 'Multiple Exposures', 'Reconstruction Loss', 'Perceptual Loss', 'Reconstruction Artifacts', 'High Dynamic Range Image', 'Single Input Image', 'Loss Function', 'Structural Similarity', 'Exposure Time', 'Low Dynamic Range', 'Peak Signal-to-noise Ratio', 'Convolutional Neural Network', 'Ground Truth Image', 'Generative Adversarial Networks', 'Human Visual System', 'Saturation Region', 'Training Phase', 'Missing Details', 'Image Pairs']","['Algorithms: Computational photography', 'image and video synthesis', 'Low-level and physics-based vision']",17,"High dynamic range (HDR) imaging is an indispensable technique in modern photography. Traditional methods focus on HDR reconstruction from multiple images, solving the core problems of image alignment, fusion, and tone mapping, yet having a perfect solution due to ghosting and other visual artifacts in the reconstruction. Recent attempts at single-image HDR reconstruction show a promising alternative: by learning to map pixel values to their irradiance using a neural network, one can bypass the align-and-merge pipeline completely yet still obtain a high-quality HDR image. In this work, we propose a weakly supervised learning method that inverts the physical image formation process for HDR reconstruction via learning to generate multiple exposures from a single image. Our neural network can invert the camera response to reconstruct pixel irradiance before synthesizing multiple exposures and hallucinating details in under- and over-exposed regions from a single input image. To train the network, we propose a representation loss, a reconstruction loss, and a perceptual loss applied on pairs of under- and over-exposure images and thus do not require HDR images for training. Our experiments show that our proposed model can effectively reconstruct HDR images. Our qualitative and quantitative results show that our method achieves state-of-the-art performance on the DrTMO dataset. Our code is available at https://github.com/VinAIResearch/single_image_hdr."
SketchInverter: Multi-Class Sketch-Based Image Generation via GAN Inversion,"Zirui An, Jingbo Yu, Runtao Liu, Chuang Wang, Qian Yu",Beihang University; Johns Hopkins University,100,"China, USA",0,,"This paper proposes the first GAN inversion-based method for multi-class sketch-based image generation (MC-SBIG). MC-SBIG is a challenging task that requires strong prior knowledge due to the significant domain gap between sketches and natural images. Existing learning-based approaches rely on a large-scale paired dataset to learn the mapping between these two image modalities. However, since the public paired sketch-photo data are scarce, it is struggling for learning-based methods to achieve satisfactory results. In this work, we introduce a new approach based on GAN inversion, which can utilize a powerful pretrained generator to facilitate image generation from a given sketch. Our GAN inversion-based method has two advantages: 1. it can freely take advantage of the prior knowledge of a pretrained image generator; 2. it allows the proposed model to focus on learning the mapping from a sketch to a low-dimension latent code, which is a much easier task than directly mapping to a high-dimension natural image. We also present a novel shape loss to improve generation quality further. Extensive experiments are conducted to show that our method can produce sketch-faithful and photo-realistic images and significantly outperform the baseline methods.",https://openaccess.thecvf.com/content/WACV2023/html/An_SketchInverter_Multi-Class_Sketch-Based_Image_Generation_via_GAN_Inversion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/An_SketchInverter_Multi-Class_Sketch-Based_Image_Generation_via_GAN_Inversion_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030825/,"['Training', 'Learning systems', 'Computer vision', 'Codes', 'Image synthesis', 'Shape', 'Generators']","['Generative Adversarial Networks', 'Image Generation', 'Generative Adversarial Networks Inversion', 'Paired Data', 'Natural Images', 'Baseline Methods', 'Domain Gap', 'Latent Code', 'Photo-realistic Images', 'Image Quality', 'Class Labels', 'Training Strategy', 'Latent Space', 'ImageNet Dataset', 'Reconstruction Loss', 'Conditional Generative Adversarial Network', 'Generative Adversarial Networks Model', 'Multi-class Model', 'Fréchet Inception Distance', 'Large-scale Image Datasets']","['Algorithms: Computational photography', 'image and video synthesis', 'Arts/games/social media']",2,"This paper proposes the first GAN inversion-based method for multi-class sketch-based image generation (MCSBIG). MC-SBIG is a challenging task that requires strong prior knowledge due to the significant domain gap between sketches and natural images. Existing learning-based approaches rely on a large-scale paired dataset to learn the mapping between these two image modalities. However, since the public paired sketch-photo data are scarce, it is struggling for learning-based methods to achieve satisfactory results. In this work, we introduce a new approach based on GAN inversion, which can utilize a powerful pretrained generator to facilitate image generation from a given sketch. Our GAN inversion-based method has two advantages: 1. it can freely take advantage of the prior knowledge of a pretrained image generator; 2. it allows the proposed model to focus on learning the mapping from a sketch to a low-dimension latent code, which is a much easier task than directly mapping to a high-dimension natural image. We also present a novel shape loss to improve generation quality further. Extensive experiments are conducted to show that our method can produce sketch-faithful and photo-realistic images and significantly outperform the baseline methods."
Skew-Robust Human-Object Interactions in Videos,"Apoorva Agarwal, Rishabh Dabral, Arjun Jain, Ganesh Ramakrishnan",IIT Bombay; UA VIO Labs,50,India,50,USA,"Humans are, arguably, one of the most important regions of interest in a visual analysis pipeline. Detecting how the human interacts with the surrounding environment, thus, becomes an important problem and has several potential use-cases. While this has been adequately addressed in the literature in the image setting, there exist very few methods addressing the case for in-the-wild videos. The problem is further exacerbated by the high degree of label skew. To this end, we propose SeRVo-HOI, a robust end-to-end framework for recognizing human-object interactions from a video, particularly in high label-skew settings. The network contextualizes multiple image representations and is trained to explicitly handle dataset skew. We propose and analyse methods to address the long-tail distribution of the labels and show improvements on the tail-labels. SeRVo-HOI outperforms the state-of-the-art by a significant margin 21.1% vs 17.6% mAP on the large-scale, in-the-wild VidHOI dataset while particularly demonstrating solid improvements in the tail-classes 19.9% vs 17.3% mAP.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Skew-Robust_Human-Object_Interactions_in_Videos_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Skew-Robust_Human-Object_Interactions_in_Videos_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030949/,"['Visualization', 'Computer vision', 'Protocols', 'Pipelines', 'Image representation', 'Solids', 'Videos']","['Interactive Video', 'Human-object Interaction', 'Long-tailed Distribution', 'Spatial Information', 'Cross-entropy', 'Visual Features', 'Cross-entropy Loss', 'Bounding Box', 'Visual Detection', 'Handcrafted Features', 'Key Performance Indicators', 'Detection Mode', 'Focal Loss', 'Mean Average Precision', 'Challenging Dataset', 'Binary Cross-entropy Loss', 'Conditional Random Field', 'Score Map', 'Human Pose', 'Rare Classes', 'Scene Graph', 'Noisy Set', 'Detection In Videos', 'Body Joints', 'Detection In Images', 'Spatial Cues', 'Visual Detection Task', 'Validation Set', 'Entropy Loss']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Humans are, arguably, one of the most important regions of interest in a visual analysis pipeline. Detecting how the human interacts with the surrounding environment, thus, becomes an important problem and has several potential use-cases. While this has been adequately addressed in the literature in the image setting, there exist very few methods addressing the case for in-the-wild videos. The problem is further exacerbated by the high degree of label skew. To this end, we propose SERVO-HOI, a robust end-to-end framework for recognizing human-object interactions from a video, particularly in high label-skew settings. The network contextualizes multiple image representations and is trained to explicitly handle dataset skew. We propose and analyse methods to address the long-tail distribution of the labels and show improvements on the tail-labels. SeRVo-HOI outperforms the state-of-the-art by a significant margin (21.1% vs 17.6% mAP) on the large-scale, in-the-wild VidHOI dataset while particularly demonstrating solid improvements in the tail-classes (19.9% vs 17.3% mAP)."
Sparsity Agnostic Depth Completion,"Andrea Conti, Matteo Poggi, Stefano Mattoccia","Department of Computer Science and Engineering (DISI), University of Bologna, Italy",100,Italy,0,,"We present a novel depth completion approach agnostic to the sparsity of depth points, that is very likely to vary in many practical applications. State-of-the-art approaches yield accurate results only when processing a specific density and distribution of input points, i.e. the one observed during training, narrowing their deployment in real use cases. On the contrary, our solution is robust to uneven distributions and extremely low densities never witnessed during training. Experimental results on standard indoor and outdoor benchmarks highlight the robustness of our framework, achieving accuracy comparable to state-of-the-art methods when tested with density and distribution equal to the training one while being much more accurate in the other cases. Our pretrained models and further material are available in our project page.",https://openaccess.thecvf.com/content/WACV2023/html/Conti_Sparsity_Agnostic_Depth_Completion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Conti_Sparsity_Agnostic_Depth_Completion_WACV_2023_paper.pdf,https://andreaconti.github.io/projects/sparsity_agnostic_depth_completion,,2212.0079,main,Poster,https://ieeexplore.ieee.org/document/10030924/,"['Training', 'Computer vision', 'Computer architecture', 'Benchmark testing', 'Robustness', 'Standards']","['Sparsity', 'Depth Completion', 'Uneven Distribution', 'Input Point', 'Depth Point', 'Project Page', 'Real Use Case', 'Neural Network', 'Convolutional Layers', 'Multiple Scales', 'Sparse Data', 'Light Detection And Ranging', 'RGB Images', 'Depth Map', 'Skip Connections', 'Depth Camera', 'Generation Module', 'Depth Perception', 'Sparse Point', 'Time-of-flight Sensors', 'Depth Prediction', 'Sparse Input', 'Confidence Map', 'Dense Depth', 'Sparse Map', 'Scene Point', 'Depth Values', 'Specific Scale', 'Iteration Step']","['Algorithms: 3D computer vision', 'Computational photography', 'image and video synthesis', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",13,"We present a novel depth completion approach agnostic to the sparsity of depth points, that is very likely to vary in many practical applications. State-of-the-art approaches yield accurate results only when processing a specific density and distribution of input points, i.e. the one observed during training, narrowing their deployment in real use cases. On the contrary, our solution is robust to uneven distributions and extremely low densities never witnessed during training. Experimental results on standard indoor and outdoor benchmarks highlight the robustness of our framework, achieving accuracy comparable to state-of-the-art methods when tested with density and distribution equal to the training one while being much more accurate in the other cases. Our pretrained models and further material are available in our project page."
Spatial Consistency Loss for Training Multi-Label Classifiers From Single-Label Annotations,"Thomas Verelst, Paul K. Rubenstein, Marcin Eichner, Tinne Tuytelaars, Maxim Berman","Apple; ESAT-PSI, KU Leuven, Belgium",50,Belgium,50,USA,"Multi-label image classification is more applicable 'in the wild' than single-label classification, as natural images usually contain multiple objects. However, exhaustively annotating images with every object of interest is costly and time-consuming. We train multi-label classifiers from datasets where each image is annotated with a single positive label only. As the presence of all other classes is unknown, we propose an Expected Negative loss that builds a set of expected negative labels in addition to the annotated positives. This set is determined based on prediction consistency, by averaging predictions over consecutive training epochs to build robust targets. Moreover, the crop data-augmentation leads to additional label noise by cropping out the single annotated object. Our novel spatial consistency loss improves supervision and ensures consistency of the spatial feature maps by maintaining per-class running-average heatmaps for each training image. We use MS-COCO, Pascal VOC, NUS-WIDE and CUB-Birds datasets to demonstrate the gains of the Expected Negative loss in combination with consistency and spatial consistency losses. We also demonstrate improved multi-label classification mAP on ImageNet-1K using the ReaL multi-label validation set.",https://openaccess.thecvf.com/content/WACV2023/html/Verelst_Spatial_Consistency_Loss_for_Training_Multi-Label_Classifiers_From_Single-Label_Annotations_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Verelst_Spatial_Consistency_Loss_for_Training_Multi-Label_Classifiers_From_Single-Label_Annotations_WACV_2023_paper.pdf,,,2203.06127,main,Poster,https://ieeexplore.ieee.org/document/10030948/,"['Training', 'Heating systems', 'Computer vision', 'Annotations', 'Text categorization', 'Crops', 'Standards']","['Consistency Loss', 'Spatial Consistency Loss', 'Average Time', 'Validation Set', 'Image Classification', 'Feature Maps', 'Data Augmentation', 'Training Epochs', 'Single Label', 'Positive Labels', 'Negative Labels', 'PASCAL VOC', 'Consecutive Epochs', 'Label Noise', 'Loss Function', 'Adam Optimizer', 'Cross-entropy Loss', 'Image Object', 'Network Output', 'Binary Cross-entropy Loss', 'Concept Drift', 'Single Annotation', 'Score Map', 'Semi-supervised Learning', 'Cosine Annealing', 'Partial Labels', 'Mean Average Precision', 'Score Estimation', 'Annotated Labels']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",7,"Multi-label image classification is more applicable ""in the wild"" than single-label classification, as natural images usually contain multiple objects. However, exhaustively annotating images with every object of interest is costly and time-consuming. We train multi-label classifiers from datasets where each image is annotated with a single positive label only. As the presence of all other classes is unknown, we propose an Expected Negative loss that builds a set of expected negative labels in addition to the annotated positives. This set is determined based on prediction consistency, by averaging predictions over consecutive training epochs to build robust targets. Moreover, the ‘crop’ data augmentation leads to additional label noise by cropping out the single annotated object. Our novel spatial consistency loss improves supervision and ensures consistency of the spatial feature maps by maintaining per-class running-average heatmaps for each training image. We use MS-COCO, Pascal VOC, NUS-WIDE and CUB-Birds datasets to demonstrate the gains of the Expected Negative loss in combination with consistency and spatial consistency losses. We also demonstrate improved multi-label classification mAP on ImageNet-1K using the ReaL multi-label validation set."
Spatially Multi-Conditional Image Generation,"Nikola Popović, Ritika Chakraborty, Danda Pani Paudel, Thomas Probst, Luc Van Gool","Computer Vision Laboratory, ETH Zurich, Switzerland; Computer Vision Laboratory, ETH Zurich, Switzerland; VISICS, ESAT/PSI, KU Leuven, Belgium",100,"Belgium, Switzerland",0,,"In most scenarios, conditional image generation can be thought of as an inversion of the image understanding process. Since generic image understanding involves solving multiple tasks, it is natural to aim at generating images via multi conditioning. However, multi-conditional image generation is a very challenging problem due to the heterogeneity and the sparsity of the (in practice) available conditioning labels. In this work, we propose a novel neural architecture to address the problem of heterogeneity and sparsity of the spatially multi-conditional labels. Our choice of spatial conditioning, such as by semantics and depth, is driven by the promise it holds for better control of the image generation process. The proposed method uses a transformer-like architecture operating pixel-wise, which receives the available labels as input tokens to merge them in a learned homogeneous space of labels. The merged labels are then used for image generation via conditional generative adversarial training. In this process, the sparsity of the labels is handled by simply dropping the input tokens corresponding to the missing labels at the desired locations, thanks to the proposed pixel-wise operating architecture. Our experiments on three benchmark datasets demonstrate the clear superiority of our method over the state-of-the-art and compared baselines.",https://openaccess.thecvf.com/content/WACV2023/html/Popovic_Spatially_Multi-Conditional_Image_Generation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Popovic_Spatially_Multi-Conditional_Image_Generation_WACV_2023_paper.pdf,,https://github.com/96ritika/TLAM,,main,Poster,https://ieeexplore.ieee.org/document/10030479/,"['Training', 'Visualization', 'Tensors', 'TV', 'Image synthesis', 'Process control', 'Computer architecture']","['Image Generation', 'Problem Of Heterogeneity', 'Homogeneous Space', 'Input Tokens', 'Neural Network', 'Qualitative Results', 'Multilayer Perceptron', 'Latent Space', 'Semantic Segmentation', 'Pixel Location', 'Image Synthesis', 'Semantic Labels', 'User Control', 'Multiple Labels', 'Labeling Density', 'Image Editing', 'Vision Transformer', 'Sparse Labeling', 'Surface Normals', 'Fréchet Inception Distance', 'Input Label']","['Algorithms: Computational photography', 'image and video synthesis', 'Adversarial learning', 'adversarial attack and defense methods']",,"In most scenarios, conditional image generation can be thought of as an inversion of the image understanding process. Since generic image understanding involves solving multiple tasks, it is natural to aim at generating images via multi-conditioning. However, multi-conditional image generation is a very challenging problem due to the heterogeneity and the sparsity of the (in practice) available conditioning labels. In this work, we propose a novel neural architecture to address the problem of heterogeneity and sparsity of the spatially multi-conditional labels. Our choice of spatial conditioning, such as by semantics and depth, is driven by the promise it holds for better control of the image generation process. The proposed method uses a transformer-like architecture operating pixel-wise, which receives the available labels as input tokens to merge them in a learned homogeneous space of labels. The merged labels are then used for image generation via conditional generative adversarial training. In this process, the sparsity of the labels is handled by simply dropping the input tokens corresponding to the missing labels at the desired locations, thanks to the proposed pixel-wise operating architecture. Our experiments on three benchmark datasets demonstrate the clear superiority of our method over the state-of-the-art and compared baselines. The source code can be found at https://github.com/96ritika/TLAM."
Spatio-Temporal Action Detection Under Large Motion,"Gurkirt Singh, Vasileios Choutas, Suman Saha, Fisher Yu, Luc Van Gool","Computer Vision Lab, ETH Zürich",100,Switzerland,0,,"Current methods for spatiotemporal action tube detection often extend a bounding box proposal at a given key-frame into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatiotemporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to the cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Singh_Spatio-Temporal_Action_Detection_Under_Large_Motion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Singh_Spatio-Temporal_Action_Detection_Under_Large_Motion_WACV_2023_paper.pdf,,,2209.0225,main,Poster,https://ieeexplore.ieee.org/document/10030795/,"['Three-dimensional displays', 'Tracking', 'Shape', 'Detectors', 'Feature extraction', 'Cameras', 'Electron tubes']","['Large Motion', 'Action Detection', 'Spatio-temporal Action', 'Spatio-temporal Action Detection', 'Bounding Box', 'Feature Aggregation', 'Fast Activity', 'Camera Motion', 'Feature Pooling', 'Temporal Aggregation', 'Time And Space', 'Low Quality', 'Learning Rate', 'Temporal Scales', 'Stochastic Gradient Descent', 'Types Of Modes', 'Baseline Methods', 'Action Recognition', 'Motor Speed', 'Backbone Network', 'Temporal Convolutional Network', 'Action Classes', 'Action Instances', 'Temporal Convolution', 'Video Quality']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",7,"Current methods for spatio-temporal action tube detection often extend a bounding box proposal at a given key-frame into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatio-temporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset."
Spike-Based Anytime Perception,"Matthew Dutson, Yin Li, Mohit Gupta",University of Wisconsin–Madison,100,USA,0,,"In many emerging computer vision applications, it is critical to adhere to stringent latency and power constraints. The current neural network paradigm of frame-based, floating-point inference is often ill-suited to these resource-constrained applications. Spike-based perception - enabled by spiking neural networks (SNNs) - is one promising alternative. Unlike conventional neural networks (ANNs), spiking networks exhibit smooth tradeoffs between latency, power, and accuracy. SNNs are the archetype of an ""anytime algorithm"" whose accuracy improves smoothly over time. This property allows SNNs to adapt their computational investment in response to changing resource constraints. Unfortunately, mainstream algorithms for training SNNs (i.e., those based on ANN-to-SNN conversion) tend to produce models that are inefficient in practice. To mitigate this problem, we propose a set of principled optimizations that reduce latency and power consumption by 1-2 orders of magnitude in converted SNNs. These optimizations leverage a set of novel efficiency metrics designed for anytime algorithms. We also develop a state-of-the-art simulator, SaRNN, which can simulate SNNs using commodity GPU hardware and neuromorphic platforms. We hope that the proposed optimizations, metrics, and tools will facilitate the future development of spike-based vision systems.",https://openaccess.thecvf.com/content/WACV2023/html/Dutson_Spike-Based_Anytime_Perception_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dutson_Spike-Based_Anytime_Perception_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030469/,"['Measurement', 'Training', 'Computer vision', 'Power demand', 'Neuromorphics', 'Machine vision', 'Neural networks']","['Neural Network', 'Power Consumption', 'Spiking Neural Networks', 'Time Step', 'Convolution', 'Object Detection', 'Large-scale Datasets', 'ImageNet', 'Firing Rate', 'Batch Normalization', 'Supplementary Materials For Details', 'Optimal Formulation', 'Neuron Model', 'ReLU Activation', 'Image Classification Tasks', 'Optimal Phase', 'Message Passing Interface', 'Trade-off Curve', 'Sparse Weight']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Embedded sensing/real-time techniques']",1,"In many emerging computer vision applications, it is critical to adhere to stringent latency and power constraints. The current neural network paradigm of frame-based, floating-point inference is often ill-suited to these resource-constrained applications. Spike-based perception – enabled by spiking neural networks (SNNs) – is one promising alternative. Unlike conventional neural networks (ANNs), spiking networks exhibit smooth tradeoffs between latency, power, and accuracy. SNNs are the archetype of an ""anytime algorithm"" whose accuracy improves smoothly over time. This property allows SNNs to adapt their computational investment in response to changing resource constraints. Unfortunately, mainstream algorithms for training SNNs (i.e., those based on ANN-to-SNN conversion) tend to produce models that are inefficient in practice. To mitigate this problem, we propose a set of principled optimizations that reduce latency and power consumption by 1–2 orders of magnitude in converted SNNs. These optimizations leverage a set of novel efficiency metrics designed for anytime algorithms. We also develop a state-of-the-art simulator, SaRNN, which can simulate SNNs using commodity GPU hardware and neuromorphic platforms. We hope that the proposed optimizations, metrics, and tools will facilitate the future development of spike-based vision systems."
Splatting-Based Synthesis for Video Frame Interpolation,"Simon Niklaus, Ping Hu, Jiawen Chen",Adobe Research; Boston University; Adobe Inc,33.33333333,USA,66.66666667,USA,"Frame interpolation is an essential video processing technique that adjusts the temporal resolution of an image sequence. While deep learning has brought great improvements to the area of video frame interpolation, techniques that make use of neural networks can typically not easily be deployed in practical applications like a video editor since they are either computationally too demanding or fail at high resolutions. In contrast, we propose a deep learning approach that solely relies on splatting to synthesize interpolated frames. This splatting-based synthesis for video frame interpolation is not only much faster than similar approaches, especially for multi-frame interpolation, but can also yield new state-of-the-art results at high resolutions.",https://openaccess.thecvf.com/content/WACV2023/html/Niklaus_Splatting-Based_Synthesis_for_Video_Frame_Interpolation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Niklaus_Splatting-Based_Synthesis_for_Video_Frame_Interpolation_WACV_2023_paper.pdf,,,2201.10075,main,Poster,https://ieeexplore.ieee.org/document/10030935/,"['Deep learning', 'Interpolation', 'Computer vision', 'Image resolution', 'Neural networks', 'Image sequences', 'Computational efficiency']","['Frame Interpolation', 'High-resolution', 'Neural Network', 'Deep Learning', 'Use Of Neural Networks', 'Video Editing', 'Input Image', 'Softmax', 'Target Location', 'Source Images', 'Optical Flow', 'Ablation Experiments', 'Numerical Stability', 'Numerical Instability', 'Subsequent Purification', 'Flow Estimation', 'Motion Estimation', 'Brightness Changes', 'Input Frames', 'Multiple Pixels', 'Motion Compensation', 'Optical Flow Estimation', 'Interpolation Results', 'Video Compression', 'Refinement Network', 'Interpolation Approach', 'Intermediate Frames', 'Baseline For Comparison', 'Low Resolution']","['Algorithms: Computational photography', 'image and video synthesis']",11,"Frame interpolation is an essential video processing technique that adjusts the temporal resolution of an image sequence. While deep learning has brought great improvements to the area of video frame interpolation, techniques that make use of neural networks can typically not easily be deployed in practical applications like a video editor since they are either computationally too demanding or fail at high resolutions. In contrast, we propose a deep learning approach that solely relies on splatting to synthesize interpolated frames. This splatting-based synthesis for video frame interpolation is not only much faster than similar approaches, especially for multi-frame interpolation, but can also yield new state-of-the-art results at high resolutions."
Split To Learn: Gradient Split for Multi-Task Human Image Analysis,"Weijian Deng, Yumin Suh, Xiang Yu, Masoud Faraki, Liang Zheng, Manmohan Chandraker","NEC Labs America, University of California, San Diego; Australian National University; NEC Labs America",66.66666667,"Australia, USA",33.33333333,Japan,"This paper presents an approach to train a unified deep network that simultaneously solves multiple human-related tasks. A multi-task framework is favorable for sharing information across tasks under restricted computational resources. However, tasks not only share information but may also compete for resources and conflict with each other, making the optimization of shared parameters difficult and leading to suboptimal performance. We propose a simple but effective training scheme called GradSplit that alleviates this issue by utilizing asymmetric inter-task relations. Specifically, at each convolution module, it splits features into T groups for T tasks and trains each group only using the gradient back-propagated from the task losses with which it does not have conflicts. During training, we apply GradSplit to a series of convolution modules. As a result, each module is trained to generate a set of task-specific features using the shared features from the previous module. This enables a network to use complementary information across tasks while circumventing gradient conflicts. Experimental results show that GradSplit achieves a better accuracy-efficiency trade-off than existing methods. It minimizes accuracy drop caused by task conflicts while significantly saving compute resources in terms of both FLOPs and memory at inference. We further show that GradSplit achieves higher cross-dataset accuracy compared to single-task and other multi-task networks.",https://openaccess.thecvf.com/content/WACV2023/html/Deng_Split_To_Learn_Gradient_Split_for_Multi-Task_Human_Image_Analysis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Deng_Split_To_Learn_Gradient_Split_for_Multi-Task_Human_Image_Analysis_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030388/,"['Training', 'Computer vision', 'Image analysis', 'Convolution', 'Computational modeling', 'Estimation', 'Interference']","['Multiple Tasks', 'Asymmetric Relationship', 'Convolution Module', 'Task Conflict', 'Task Loss', 'Task-specific Features', 'Multi-task Network', 'Training Dataset', 'Parsing', 'Multiple Datasets', 'Task Model', 'Output Channels', 'Regularization Method', 'Pose Estimation', 'Pairwise Relationships', 'Multi-task Learning', 'Forward Pass', 'Domain Gap', 'Multi-task Model', 'Pedestrian Detection', 'Group Normalization', 'Backbone Architecture']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"This paper presents an approach to train a unified deep network that simultaneously solves multiple human-related tasks. A multi-task framework is favorable for sharing information across tasks under restricted computational resources. However, tasks not only share information but may also compete for resources and conflict with each other, making the optimization of shared parameters difficult and leading to suboptimal performance. We propose a simple but effective training scheme called GradSplit that alleviates this issue by utilizing asymmetric inter-task relations. Specifically, at each convolution module, it splits features into T groups for T tasks and trains each group only using the gradient back-propagated from the task losses with which it does not have conflicts. During training, we apply GradSplit to a series of convolution modules. As a result, each module is trained to generate a set of task-specific features using the shared features from the previous module. This enables a network to use complementary information across tasks while circumventing gradient conflicts. Experimental results show that GradSplit achieves a better accuracy-efficiency trade-off than existing methods. It minimizes accuracy drop caused by task conflicts while significantly saving compute resources in terms of both FLOPs and memory at inference. We further show that GradSplit achieves higher cross-dataset accuracy compared to single-task and other multi-task networks."
Stop or Forward: Dynamic Layer Skipping for Efficient Action Recognition,"Jonghyeon Seon, Jaedong Hwang, Jonghwan Mun, Bohyung Han",Massachusetts Institute of Technology; Kakao Brain; Seoul National University,66.66666667,"South Korea, USA",33.33333333,South Korea,"One of the challenges for analyzing video contents (e.g., actions) is high computational cost, especially for the tasks that require processing densely sampled frames in a long video. We present a novel efficient action recognition algorithm, which allocates computational resources adaptively to individual frames depending on their relevance and significance. Specifically, our algorithm adopts LSTM-based policy modules and sequentially estimates the usefulness of each frame based on their intermediate representations. If a certain frame is unlikely to be helpful for recognizing actions, our model stops forwarding the features to the rest of the layers and starts to consider the next sampled frame. We further reduce the computational cost of our approach by introducing a simple yet effective early termination strategy during the inference procedure. We evaluate the proposed algorithm on three public benchmarks: ActivityNet-v1.3, Mini-Kinetics, and THUMOS'14. Our experiments show that the proposed approach achieves outstanding trade-off between accuracy and efficiency in action recognition.",https://openaccess.thecvf.com/content/WACV2023/html/Seon_Stop_or_Forward_Dynamic_Layer_Skipping_for_Efficient_Action_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Seon_Stop_or_Forward_Dynamic_Layer_Skipping_for_Efficient_Action_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030737/,"['Location awareness', 'Knowledge engineering', 'Heuristic algorithms', 'Semantics', 'Memory management', 'Termination of employment', 'Filtering algorithms']","['Action Recognition', 'Efficient Recognition', 'Efficient Action Recognition', 'Computational Cost', 'Sampling Frame', 'Recognition Accuracy', 'Video Frames', 'Video Content', 'Inference Procedure', 'Deep Neural Network', 'Validation Set', 'Network Layer', 'Visual Features', 'Cross-entropy Loss', 'Lookup Table', 'Network Efficiency', 'Hidden State', 'Residual Block', 'Loss Of Efficiency', 'Intermediate Layer', 'Backbone Network', 'Frame Selection', 'Input Frames', 'Action Labels', 'Semantic Level', 'Temporal Model', 'Online Processing', 'Frame Information']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",5,"One of the challenges for analyzing video contents (e.g., actions) is high computational cost, especially for the tasks that require processing densely sampled frames in a long video. We present a novel efficient action recognition algorithm, which allocates computational resources adaptively to individual frames depending on their relevance and significance. Specifically, our algorithm adopts LSTM-based policy modules and sequentially estimates the usefulness of each frame based on their intermediate representations. If a certain frame is unlikely to be helpful for recognizing actions, our model stops forwarding the features to the rest of the layers and starts to consider the next sampled frame. We further reduce the computational cost of our approach by introducing a simple yet effective early termination strategy during the inference procedure. We evaluate the proposed algorithm on three public benchmarks: ActivityNet-v1.3, Mini-Kinetics, and THUMOS’14. Our experiments show that the proposed approach achieves outstanding trade-off between accuracy and efficiency in action recognition."
Structure-Encoding Auxiliary Tasks for Improved Visual Representation in Vision-and-Language Navigation,"Chia-Wen Kuo, Chih-Yao Ma, Judy Hoffman, Zsolt Kira",Georgia Tech,100,USA,0,,"In Vision-and-Language Navigation (VLN), researchers typically take an image encoder pre-trained on ImageNet without fine-tuning on the environments that the agent will be trained or tested on. However, the distribution shift between the training images from ImageNet and the views in the navigation environments may render the ImageNet pre-trained image encoder suboptimal. Therefore, in this paper, we design a set of structure-encoding auxiliary tasks (SEA) that leverage the data in the navigation environments to pre-train and improve the image encoder. Specifically, we design and customize (1) 3D jigsaw, (2) traversability prediction, and (3) instance classification to pre-train the image encoder. Through rigorous ablations, our SEA pre-trained features are shown to better encode structural information of the scenes, which ImageNet pre-trained features fail to properly encode but is crucial for the target navigation task. The SEA pre-trained features can be easily plugged into existing VLN agents without any tuning. For example, on Test-Unseen environments, the VLN agents combined with our SEA pre-trained features achieve absolute success rate improvement of 12% for Speaker-Follower [14], 5% for Env-Dropout [37], and 4% for AuxRN [50].",https://openaccess.thecvf.com/content/WACV2023/html/Kuo_Structure-Encoding_Auxiliary_Tasks_for_Improved_Visual_Representation_in_Vision-and-Language_Navigation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kuo_Structure-Encoding_Auxiliary_Tasks_for_Improved_Visual_Representation_in_Vision-and-Language_Navigation_WACV_2023_paper.pdf,,,2211.11116,main,Poster,https://ieeexplore.ieee.org/document/10030910/,"['Training', 'Visualization', 'Computer vision', 'Three-dimensional displays', 'Navigation', 'Source coding', 'Task analysis']","['Visual Representation', 'Auxiliary Task', '3D Classification', 'Class Instances', 'Scene Information', 'Absolute Improvement', 'Navigation In Environments', 'Image Encoder', 'Pre-trained Encoder', 'Pre-trained Feature', 'Computer Vision', 'Visual Information', 'Object Detection', 'Representation Learning', 'Semantic Segmentation', 'Textual Information', 'State Representation', 'Affine Transformation', 'Training Examples', 'Normal Approximation', 'Unseen Environments', 'Scene Classification', 'Training Environment', 'Pre-training Tasks', 'Self-supervised Learning', 'Target Task', 'Image Augmentation', 'Conduct Ablation Studies', 'Target Environment', 'Color Jittering']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"In Vision-and-Language Navigation (VLN), researchers typically take an image encoder pre-trained on ImageNet without fine-tuning on the environments that the agent will be trained or tested on. However, the distribution shift between the training images from ImageNet and the views in the navigation environments may render the ImageNet pre-trained image encoder suboptimal. Therefore, in this paper, we design a set of structure-encoding auxiliary tasks (SEA) that leverage the data in the navigation environments to pre-train and improve the image encoder. Specifically, we design and customize (1) 3D jigsaw, (2) traversability prediction, and (3) instance classification to pre-train the image encoder. Through rigorous ablations, our SEA pre-trained features are shown to better encode structural information of the scenes, which ImageNet pre-trained features fail to properly encode but is crucial for the target navigation task. The SEA pre-trained features can be easily plugged into existing VLN agents without any tuning. For example, on Test-Unseen environments, the VLN agents combined with our SEA pre-trained features achieve absolute success rate improvement of 12% for Speaker-Follower [14], 5% for Env-Dropout [37], and 4% for AuxRN [50]."
Style-Guided Inference of Transformer for High-Resolution Image Synthesis,"Jonghwa Yim, Minjae Kim","Vision AI Lab., AI Center, NCSOFT, South Korea",0,,100,South Korea,"Transformer is eminently suitable for auto-regressive image synthesis which predicts discrete value from the past values recursively to make up full image. Especially, combined with vector quantised latent representation, the state-of-the-art auto-regressive transformer displays realistic high-resolution images. However, sampling the latent code from discrete probability distribution makes the output unpredictable. Therefore, it requires to generate lots of diverse samples to acquire desired outputs. To alleviate the process of generating lots of samples repetitively, in this article, we propose to take a desired output, a style image, as an additional condition without re-training the transformer. To this end, our method transfers the style to a probability constraint to re-balance the prior, thereby specifying the target distribution instead of the original prior. Thus, generated samples from the re-balanced prior have similar styles to reference style. In practice, we can choose either an image or a category of images as an additional condition. In our qualitative assessment, we show that styles of majority of outputs are similar to the input style.",https://openaccess.thecvf.com/content/WACV2023/html/Yim_Style-Guided_Inference_of_Transformer_for_High-Resolution_Image_Synthesis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yim_Style-Guided_Inference_of_Transformer_for_High-Resolution_Image_Synthesis_WACV_2023_paper.pdf,,,2210.05533,main,Poster,https://ieeexplore.ieee.org/document/10031010/,"['Computer vision', 'Codes', 'Image synthesis', 'Transformers', 'Probability distribution']","['High-resolution Images', 'Image Synthesis', 'Additional Conditions', 'Latent Representation', 'Image Categories', 'Latent Code', 'Style Image', 'Lot Of Samples', 'Training Dataset', 'Single Image', 'Recurrent Neural Network', 'Autoregressive Model', 'Receptive Field', 'Image Generation', 'Semantic Segmentation', 'Density Model', 'Sunglasses', 'Variational Autoencoder', 'Semantic Labels', 'Semantic Map', 'Style Transfer', 'Fréchet Inception Distance', 'Semantic Regions', 'Vector Quantization', 'Low-level Vision', 'Transformer Architecture', 'Deep Generative Models']","['Algorithms: Computational photography', 'image and video synthesis']",,"Transformer is eminently suitable for auto-regressive image synthesis which predicts discrete value from the past values recursively to make up full image. Especially, combined with vector quantised latent representation, the state-of-the-art auto-regressive transformer displays realistic high-resolution images. However, sampling the latent code from discrete probability distribution makes the output unpredictable. Therefore, it requires to generate lots of diverse samples to acquire desired outputs. To alleviate the process of generating lots of samples repetitively, in this article, we propose to take a desired output, a style image, as an additional condition without re-training the transformer. To this end, our method transfers the style to a probability constraint to re-balance the prior, thereby specifying the target distribution instead of the original prior. Thus, generated samples from the re-balanced prior have similar styles to reference style. In practice, we can choose either an image or a category of images as an additional condition. In our qualitative assessment, we show that styles of majority of outputs are similar to the input style."
Surface Normal Estimation From Optimized and Distributed Light Sources Using DNN-Based Photometric Stereo,"Takafumi Iwaguchi, Hiroshi Kawasaki","Kyushu University, Fukuoka, Japan",100,Japan,0,,"Photometric stereo (PS) is a major technique to recover surface normal for each pixel. However, since it assumes Lambertian surface and directional light to estimate the value, a large number of images are usually required to avoid the effects of outliers and noise. In this paper, we propose a technique to reduce the number of images by using distributed light sources, where the patterns are optimized by a deep neural network (DNN). In addition, to efficiently realize the distributed light, we use an optical diffuser with a video projector, where the diffuser is illuminated by the projector from behind, the illuminated area on the diffuser works as if an arbitrary-shaped area light. To estimate the surface normal using the distributed light source, we propose a near-light photometric stereo (NLPS) using DNN. Since optimization of the pattern of distributed light is achieved by a differentiable renderer, it is connected with NLPS network, achieving end-to-end learning. The experiments are conducted to show the successful estimation of the surface normal by our method from a small number of images.",https://openaccess.thecvf.com/content/WACV2023/html/Iwaguchi_Surface_Normal_Estimation_From_Optimized_and_Distributed_Light_Sources_Using_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Iwaguchi_Surface_Normal_Estimation_From_Optimized_and_Distributed_Light_Sources_Using_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030930/,"['Shape', 'Neural networks', 'Estimation', 'Optical computing', 'Optical variables control', 'Optical imaging', 'Optical noise']","['Light Source', 'Normal Approximation', 'Surface Normals', 'Photometric Stereo', 'Surface Normal Estimation', 'Deep Neural Network', 'Number Of Images', 'Direct Light', 'Large Number Of Images', 'Training Data', 'Light Conditions', '3D Reconstruction', 'Generalization Performance', 'Bounding Box', 'Depth Map', 'Optimal Distribution', 'Source Distribution', 'Learning Patterns', 'Specular Reflection', 'Light Distribution', 'Point Light Source', 'Light Patterns', 'Optimal Pattern', 'Cast Shadows', 'Real-world Images', 'Metal Objects', 'Liquid Crystal Display']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision']",3,"Photometric stereo (PS) is a major technique to recover surface normal for each pixel. However, since it assumes Lambertian surface and directional light to estimate the value, a large number of images are usually required to avoid the effects of outliers and noise. In this paper, we propose a technique to reduce the number of images by using distributed light sources, where the patterns are optimized by a deep neural network (DNN). In addition, to efficiently realize the distributed light, we use an optical diffuser with a video projector, where the diffuser is illuminated by the projector from behind, the illuminated area on the diffuser works as if an arbitrary-shaped area light. To estimate the surface normal using the distributed light source, we propose a near-light photometric stereo (NLPS) using DNN. Since optimization of the pattern of distributed light is achieved by a differentiable renderer, it is connected with NLPS network, achieving end-to-end learning. The experiments are conducted to show the successful estimation of the surface normal by our method from a small number of images."
Switching to Discriminative Image Captioning by Relieving a Bottleneck of Reinforcement Learning,"Ukyo Honda, Taro Watanabe, Yuji Matsumoto","Nara Institute of Science and Technology; RIKEN; CyberAgent, Inc. and RIKEN",100,Japan,0,,"Discriminativeness is a desirable feature of image captions: captions should describe the characteristic details of input images. However, recent high-performing captioning models, which are trained with reinforcement learning (RL), tend to generate overly generic captions despite their high performance in various other criteria. First, we investigate the cause of the unexpectedly low discriminativeness and show that RL has a deeply rooted side effect of limiting the output words to high-frequency words. The limited vocabulary is a severe bottleneck for discriminativeness as it is difficult for a model to describe the details beyond its vocabulary. This identification of the bottleneck allows us to drastically recast discriminative image captioning as a much simpler task of encouraging low-frequency word generation. Hinted by long-tail classification and debiasing methods, we propose the methods that easily switch off-the-shelf RL models to discriminativeness-aware models with only a single-epoch fine-tuning on the part of the parameters. Extensive experiments demonstrate that our methods significantly enhance the discriminativeness of off-the-shelf RL models and even outperform previous discriminativeness-aware methods with much smaller computational costs. Detailed analysis and human evaluation also verify that our methods boost the discriminativeness without sacrificing the overall quality of captions.",https://openaccess.thecvf.com/content/WACV2023/html/Honda_Switching_to_Discriminative_Image_Captioning_by_Relieving_a_Bottleneck_of_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Honda_Switching_to_Discriminative_Image_Captioning_by_Relieving_a_Bottleneck_of_WACV_2023_paper.pdf,,https://github.com/ukyh/switch-disc-caption.git,2212.0323,main,Poster,https://ieeexplore.ieee.org/document/10030291/,"['Vocabulary', 'Computer vision', 'Limiting', 'Computational modeling', 'Switches', 'Reinforcement learning', 'Control systems']","['Image Captioning', 'Computational Cost', 'Reinforcement Learning Model', 'Low-frequency Words', 'Transformer', 'Hyperparameters', 'Natural Language', 'Cross-entropy Loss', 'Model Architecture', 'Representation Learning', 'Reward Function', 'Classification Parameters', 'Vocabulary Size', 'Text Generation', 'Properties Of Words', 'Frequency Bias', 'Pre-trained Language Models', 'Frequency Rank', 'Standard Reinforcement Learning']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",6,"Discriminativeness is a desirable feature of image captions: captions should describe the characteristic details of input images. However, recent high-performing captioning models, which are trained with reinforcement learning (RL), tend to generate overly generic captions despite their high performance in various other criteria. First, we investigate the cause of the unexpectedly low discriminativeness and show that RL has a deeply rooted side effect of limiting the output words to high-frequency words. The limited vocabulary is a severe bottleneck for discriminativeness as it is difficult for a model to describe the details beyond its vocabulary. Then, based on this identification of the bottleneck, we drastically recast discriminative image captioning as a much simpler task of encouraging low-frequency word generation. Hinted by long-tail classification and debiasing methods, we propose methods that easily switch off-the-shelf RL models to discriminativeness-aware models with only a single-epoch fine-tuning on the part of the parameters. Extensive experiments demonstrate that our methods significantly enhance the discriminative-ness of off-the-shelf RL models and even outperform previous discriminativeness-aware methods with much smaller computational costs. Detailed analysis and human evaluation also verify that our methods boost the discriminativeness without sacrificing the overall quality of captions.
<sup>1</sup>"
Synthetic Latent Fingerprint Generator,"André Brasil Vieira Wyzykowski, Anil K. Jain",Michigan State University,100,USA,0,,"Given a full fingerprint image (rolled or slap), we present CycleGAN models to generate multiple latent impressions of the same identity as the full print. Our models can control the degree of distortion, noise, blurriness and occlusion in the generated latent print images to obtain Good, Bad and Ugly latent image categories as introduced in the NIST SD27 latent database. The contributions of our work are twofold: (i) demonstrate the similarity of synthetically generated latent fingerprint images to crime scene latents in NIST SD27 and MSP databases as evaluated by the NIST NFIQ 2 quality measure and recognition accuracies obtained by a SOTA fingerprint matcher, and (ii) use of synthetic latents to augment small-size latent training databases in the public domain to improve the performance of DeepPrint, a SOTA fingerprint matcher designed for rolled to rolled fingerprint matching on three latent databases (NIST SD27, NIST SD302, and IIITD-SLF). As an example, with synthetic latent data augmentation, the Rank-1 retrieval performance of DeepPrint is improved from 15.50% to 29.07% on challenging NIST SD27 latent database. Our approach for generating synthetic latent fingerprints can be used to improve the recognition performance of any latent matcher and its individual components (e.g., enhancement, segmentation and feature extraction). https://prip-lab.github.io/Synthetic-Latent-Fingerprint-Generator/",https://openaccess.thecvf.com/content/WACV2023/html/Wyzykowski_Synthetic_Latent_Fingerprint_Generator_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wyzykowski_Synthetic_Latent_Fingerprint_Generator_WACV_2023_paper.pdf,https://prip-lab.github.io/Synthetic-Latent-Fingerprint-Generator/,https://github.com/prip-lab/Synthetic-Latent-Fingerprint-Generator,2208.13811,main,Poster,https://ieeexplore.ieee.org/document/10030917/,"['Training', 'Computer vision', 'Image recognition', 'Databases', 'Image matching', 'Fingerprint recognition', 'NIST']","['Synthetic Generation', 'Latent Fingerprints', 'Data Augmentation', 'Latent Image', 'Fine-tuned', 'Image Quality', 'Stage 2', 'Generative Adversarial Networks', 'Synthetic Images', 't-SNE Plot', 'Visual Similarity', 'Cycle Consistency Loss']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Computational photography', 'image and video synthesis']",9,"Given a full fingerprint image (rolled or slap), we present CycleGAN models to generate multiple latent impressions of the same identity as the full print. Our models can control the degree of distortion, noise, blurriness and occlusion in the generated latent print images to obtain Good, Bad and Ugly latent image categories as introduced in the NIST SD27 latent database. The contributions of our work are twofold: (i) demonstrate the similarity of synthetically generated latent fingerprint images to crime scene latents in NIST SD27 and MSP databases as evaluated by the NIST NFIQ 2 quality measure and recognition accuracies obtained by a SOTA fingerprint matcher, and (ii) use of synthetic latents to augment small-size latent training databases in the public domain to improve the performance of DeepPrint, a SOTA fingerprint matcher designed for rolled to rolled fingerprint matching on three latent databases (NIST SD27, NIST SD302, and IIITD-SLF). As an example, with synthetic latent data augmentation, the Rank-1 retrieval performance of DeepPrint is improved from 15.50% to 29.07% on challenging NIST SD27 latent database. Our approach for generating synthetic latent fingerprints can be used to improve the recognition performance of any latent matcher and its individual components (e.g., enhancement, segmentation and feature extraction). https://prip-lab.github.io/Synthetic-Latent-Fingerprint-Generator/"
TCAM: Temporal Class Activation Maps for Object Localization in Weakly-Labeled Unconstrained Videos,"Soufiane Belharbi, Ismail Ben Ayed, Luke McCaffrey, Eric Granger","LIVIA, Dept. of Systems Engineering, École de technologie supérieure, Montreal, Canada; Goodman Cancer Research Centre, Dept. of Oncology, McGill University, Montreal, Canada",100,Canada,0,,"Weakly supervised video object localization (WSVOL) allows locating object in videos using only global video tags such as object classes. State-of-art methods rely on multiple independent stages, where initial spatio-temporal proposals are generated using visual and motion cues, and then prominent objects are identified and refined. The localization involves solving an optimization problem over one or more videos, and video tags are typically used for video clustering. This process requires a model per video or per class making for costly inference. Moreover, localized regions are not necessary discriminant because these methods rely on unsupervised motion methods like optical flow, or discarded video tags from optimization. In this paper, we leverage the successful class activation mapping (CAM) methods, designed for WSOL based on still images. A new Temporal CAM (TCAM) method is introduced for training a discriminant deep learning (DL) model to exploit spatio-temporal information in videos, using an CAM-Temporal Max Pooling (CAM-TMP) aggregation mechanism over consecutive CAMs. In particular, activations of regions of interest (ROIs) are collected from CAMs produced by a pretrained CNN classifier, and generate pixel-wise pseudo-labels for training a decoder. In addition, a global unsupervised size constraint, and local constraint such as CRF are used to yield more accurate CAMs. Inference over single independent frames allows parallel processing of a clip of frames, and real-time localization. Extensive experiments on two challenging YouTube-Objects datasets with unconstrained videos indicate that CAM methods (trained on independent frames) can yield decent localization accuracy. Our proposed TCAM method achieves a new state-of-art in WSVOL accuracy, and visual results suggest that it can be adapted for subsequent tasks, such as object detection and tracking.",https://openaccess.thecvf.com/content/WACV2023/html/Belharbi_TCAM_Temporal_Class_Activation_Maps_for_Object_Localization_in_Weakly-Labeled_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Belharbi_TCAM_Temporal_Class_Activation_Maps_for_Object_Localization_in_Weakly-Labeled_WACV_2023_paper.pdf,,https://github.com/sbelharbi/tcam-wsol-video,2208.14542,main,Poster,https://ieeexplore.ieee.org/document/10030472/,"['Location awareness', 'Training', 'Visualization', 'Streaming media', 'Parallel processing', 'Real-time systems', 'Proposals']","['Object Location', 'Class Activation Maps', 'Unconstrained Videos', 'Object Detection', 'Single Frame', 'Optical Flow', 'Object Tracking', 'Still Images', 'Local Constraints', 'Conditional Random Field', 'Video Information', 'Spatiotemporal Information', 'Size Constraints', 'Global Constraints', 'Motion Cues', 'Class Labels', 'Intersection Over Union', 'Stochastic Gradient Descent', 'Bounding Box', 'Localization Performance', 'Foreground Regions', 'Video Object', 'Small Region Of Interest', 'Fully Convolutional Network', 'Sequence Of Frames', 'Video Frames', 'Long-range Dependencies', 'Temporal Dependencies', 'Forward Pass', 'Final Segmentation']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Weakly supervised video object localization (WSVOL) allows locating object in videos using only global video tags such as object classes. State-of-art methods rely on multiple independent stages, where initial spatio-temporal proposals are generated using visual and motion cues, and then prominent objects are identified and refined. The localization involves solving an optimization problem over one or more videos, and video tags are typically used for video clustering. This process requires a model per video or per class making for costly inference. Moreover, localized regions are not necessary discriminant because these methods rely on unsupervised motion methods like optical flow, or discarded video tags from optimization. In this paper, we leverage the successful class activation mapping (CAM) methods, designed for WSOL based on still images. A new Temporal CAM (TCAM) method is introduced for training a discriminant deep learning (DL) model to exploit spatio-temporal information in videos, using an CAM-Temporal Max Pooling (CAM-TMP) aggregation mechanism over consecutive CAMs. In particular, activations of regions of interest (ROIs) are collected from CAMs produced by a pretrained CNN classifier, and generate pixel-wise pseudo-labels for training a decoder. In addition, a global unsupervised size constraint, and local constraint such as CRF are used to yield more accurate CAMs. Inference over single independent frames allows parallel processing of a clip of frames, and real-time localization. Extensive experiments
<sup>1</sup>
 on two challenging YouTube-Objects datasets with unconstrained videos indicate that CAM methods (trained on independent frames) can yield decent localization accuracy. Our proposed TCAM method achieves a new state-of-art in WSVOL accuracy, and visual results suggest that it can be adapted for subsequent tasks, such as object detection and tracking."
THOR-Net: End-to-End Graformer-Based Realistic Two Hands and Object Reconstruction With Self-Supervision,"Ahmed Tawfik Aboukhadra, Jameel Malik, Ahmed Elhayek, Nadia Robertini, Didier Stricker","DFKI-A V Kaiserslautern; DFKI-A V Kaiserslautern, NUST-SEECS Pakistan; DFKI-A V Kaiserslautern, TU Kaiserslautern; UPM Saudi Arabia",75,"Germany, Pakistan, Saudi Arabia",25,Germany,"Realistic reconstruction of two hands interacting with objects is a new and challenging problem that is essential for building personalized Virtual and Augmented Reality environments. Graph Convolutional networks (GCNs) allow for the preservation of the topologies of hands poses and shapes by modeling them as a graph. In this work, we propose the THOR-Net which combines the power of GCNs, Transformer, and self-supervision to realistically reconstruct two hands and an object from a single RGB image. Our network comprises two stages; namely the features extraction stage and the reconstruction stage. In the features extraction stage, a Keypoint RCNN is used to extract 2D poses, features maps, heatmaps, and bounding boxes from a monocular RGB image. Thereafter, this 2D information is modeled as two graphs and passed to the two branches of the reconstruction stage. The shape reconstruction branch estimates meshes of two hands and an object using our novel coarse-to-fine GraFormer shape network. The 3D poses of the hands and objects are reconstructed by the other branch using a GraFormer network. Finally, a self-supervised photometric loss is used to directly regress the realistic textured of each vertex in the hands' meshes. Our approach achieves State-of-the-art results in Hand shape estimation on the HO3D dataset (10.0mm) exceeding ArtiBoost (10.8mm). It also surpasses other methods in hand pose estimation on the challenging two hands and object (H2O) dataset by 5mm on the left-hand pose and 1 mm on the right-hand pose. The code base of THOR-Net will be released soon under https://github.com/ATAboukhadra/THOR-Net.",https://openaccess.thecvf.com/content/WACV2023/html/Aboukhadra_THOR-Net_End-to-End_Graformer-Based_Realistic_Two_Hands_and_Object_Reconstruction_With_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Aboukhadra_THOR-Net_End-to-End_Graformer-Based_Realistic_Two_Hands_and_Object_Reconstruction_With_WACV_2023_paper.pdf,,https://github.com/ATAboukhadra/THOR-Net,,main,Poster,,,,,,
TI2Net: Temporal Identity Inconsistency Network for Deepfake Detection,"Baoping Liu, Bo Liu, Ming Ding, Tianqing Zhu, Xin Yu","Data61, CSIRO, Sydney, NSW, Australia; University of Technology Sydney, NSW, Australia",100,Australia,0,,"In this paper, we propose a Temporal Identity Inconsistency Network (TI2Net), a Deepfake detector that focuses on temporal identity inconsistency. Specifically, TI2Net recognizes fake videos by capturing the dissimilarities of human faces among video frames of the same identity. Therefore, TI2Net is a reference-agnostic detector and can be used on unseen datasets. For a video clip of a given identity, identity information in all frames will first be encoded to identity vectors. TI2Net learns the temporal identity embedding from the temporal difference of the identity vectors. The temporal embedding, representing the identity inconsistency in the video clip, is finally used to determine the authenticity of the video clip. During training, TI2Net incorporates triplet loss to learn more discriminative temporal embeddings. We conduct comprehensive experiments to evaluate the performance of the proposed TI2Net. Experimental results indicate that TI2Net generalizes well to unseen manipulations and datasets with unseen identities. Besides, TI2Net also shows robust performance against compression and additive noise.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_TI2Net_Temporal_Identity_Inconsistency_Network_for_Deepfake_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_TI2Net_Temporal_Identity_Inconsistency_Network_for_Deepfake_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030936/,"['Training', 'Additive noise', 'Deepfakes', 'Computer vision', 'Image coding', 'Face recognition', 'Detectors']","['Temporal Identity', 'Deepfake Detection', 'Additive Noise', 'Identity Information', 'Video Clips', 'Video Frames', 'Temporal Differences', 'Human Faces', 'Triplet Loss', 'Identity Vector', 'Spatial Patterns', 'Random Sampling', 'Deep Neural Network', 'Positive Samples', 'Sequence Length', 'Negative Samples', 'Identification Of Features', 'Temporal Information', 'Differencing', 'Consecutive Frames', 'Reset Gate', 'Gated Recurrent Unit', 'Update Gate', 'Real Videos', 'Regular Items', 'Image Compression', 'Temporal Representation', 'Degree Of Compression', 'Original Video', 'Real Ones']","['Applications: Social good', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Arts/games/social media']",16,"In this paper, we propose the Temporal Identity Inconsistency Network (TI
<sup>2</sup>
Net), a Deepfake detector that focuses on temporal identity inconsistency. Specifically, TI
<sup>2</sup>
Net recognizes fake videos by capturing the dissimilarities of human faces among video frames of the same identity. Therefore, TI
<sup>2</sup>
Net is a reference-agnostic detector and can be used on unseen datasets. For a video clip of a given identity, identity information in all frames will first be encoded to identity vectors. TI
<sup>2</sup>
Net learns the temporal identity embedding from the temporal difference of the identity vectors. The temporal embedding, representing the identity inconsistency in the video clip, is finally used to determine the authenticity of the video clip. During training, TI
<sup>2</sup>
Net incorporates triplet loss to learn more discriminative temporal embeddings. We conduct comprehensive experiments to evaluate the performance of the proposed TI
<sup>2</sup>
Net. Experimental results indicate that TI
<sup>2</sup>
Net generalizes well to unseen manipulations and datasets with unseen identities. Besides, TI
<sup>2</sup>
Net also shows robust performance against compression and additive noise."
TTTFlow: Unsupervised Test-Time Training With Normalizing Flow,"David Osowiechi, Gustavo A. Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed, Christian Desrosiers","ETS Montr´eal, Canada; International Laboratory on Learning Systems (ILLS), McGILL - ETS - MILA - CNRS - Universit´e Paris-Saclay - CentraleSup´elec, Canada; LIVIA, ETS Montr´eal, Canada",100,Canada,0,,"A major problem of deep neural networks for image classification is their vulnerability to domain changes at test-time. Recent methods have proposed to address this problem with test-time training (TTT), where a two-branch model is trained to learn a main classification task and also a self-supervised task used to perform test-time adaptation. However, these techniques require defining a proxy task specific to the target application. To tackle this limitation, we propose TTTFlow: a Y-shaped architecture using an unsupervised head based on Normalizing Flows to learn the normal distribution of latent features and detect domain shifts in test examples. At inference, keeping the unsupervised head fixed, we adapt the model to domain-shifted examples by maximizing the log likelihood of the Normalizing Flow. Our results show that our method can significantly improve the accuracy with respect to previous works.",https://openaccess.thecvf.com/content/WACV2023/html/Osowiechi_TTTFlow_Unsupervised_Test-Time_Training_With_Normalizing_Flow_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Osowiechi_TTTFlow_Unsupervised_Test-Time_Training_With_Normalizing_Flow_WACV_2023_paper.pdf,,,2210.11389,main,Poster,https://ieeexplore.ieee.org/document/10030446/,"['Training', 'Adaptation models', 'Head', 'Sensitivity', 'Computational modeling', 'Computer architecture', 'Predictive models']","['Normal Flow', 'Test-time Training', 'Deep Network', 'Domain Shift', 'Self-supervised Task', 'Data Sources', 'Corruption', 'Feature Maps', 'Improvement In Accuracy', 'Object Recognition', 'Latent Space', 'Jacobian Matrix', 'Source Model', 'Domain Adaptation', 'Pre-trained Network', 'Source Domain', 'Self-supervised Learning', 'Unsupervised Manner', 'Domain Generalization', 'Contrastive Loss', 'Joint Training', 'Unknown Distribution', 'Secondary Task', 'Negative Log-likelihood', 'Domain-specific Information', 'Target Data', 'Hyperparameters', 'Flow Model', 'Formation Of Domains', 'Batch Normalization']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",7,"A major problem of deep neural networks for image classification is their vulnerability to domain changes at test-time. Recent methods have proposed to address this problem with test-time training (TTT), where a two-branch model is trained to learn a main classification task and also a self-supervised task used to perform test-time adaptation. However, these techniques require defining a proxy task specific to the target application. To tackle this limitation, we propose TTTFlow: a Y-shaped architecture using an unsupervised head based on Normalizing Flows to learn the nor-mal distribution of latent features and detect domain shifts in test examples. At inference, keeping the unsupervised head fixed, we adapt the model to domain-shifted examples by maximizing the log likelihood of the Normalizing Flow. Our results show that our method can significantly improve the accuracy with respect to previous works."
TVCalib: Camera Calibration for Sports Field Registration in Soccer,"Jonas Theiner, Ralph Ewerth","L3S Research Center, Leibniz University Hannover, Hannover, Germany; TIB – Leibniz Information Centre for Science and Technology, Hannover, Germany",50,Germany,50,Germany,"Sports field registration in broadcast videos is typically interpreted as the task of homography estimation, which provides a mapping between a planar field and the corresponding visible area of the image. In contrast to previous approaches, we consider the task as a camera calibration problem. First, we introduce a differentiable objective function that is able to learn the camera pose and focal length from segment correspondences (e.g., lines, point clouds), based on pixel-level annotations for segments of a known calibration object. The calibration module iteratively minimizes the segment reprojection error induced by the estimated camera parameters. Second, we propose a novel approach for 3D sports field registration from broadcast soccer images. Compared to the typical solution, which subsequently refines an initial estimation, our solution does it in one step. The proposed method is evaluated for sports field registration on two datasets and achieves superior results compared to two state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Theiner_TVCalib_Camera_Calibration_for_Sports_Field_Registration_in_Soccer_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Theiner_TVCalib_Camera_Calibration_for_Sports_Field_Registration_in_Soccer_WACV_2023_paper.pdf,https://mm4spa.github.io/tvcalib,,2207.11709,main,Poster,https://ieeexplore.ieee.org/document/10030585/,"['Point cloud compression', 'Image segmentation', 'Three-dimensional displays', 'Neural networks', 'Estimation', 'Cameras', 'Calibration']","['Sports Field', 'Camera Calibration', 'Point Cloud', 'Focal Length', 'Camera Pose', '3D Field', 'Reprojection Error', '3D Registration', 'Camera Focal Length', 'Input Image', 'Intersection Over Union', 'Raw Images', 'Semantic Segmentation', 'Line Segment', 'Image Distortion', 'Related Tasks', 'Camera Position', 'Instance Segmentation', 'Multiple Cameras', 'Polyline', 'Homography Matrix', 'World Space', 'Ground Truth Segmentation', 'Disc Segmentation', 'Pinhole Camera', 'Soccer Field', 'Projection Error', 'Point Cloud Segmentation', 'Pinhole Camera Model', 'Team Sports']","['Algorithms: 3D computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Arts/games/social media']",13,"Sports field registration in broadcast videos is typically interpreted as the task of homography estimation, which provides a mapping between a planar field and the corresponding visible area of the image. In contrast to previous approaches, we consider the task as a camera calibration problem. First, we introduce a differentiable objective function that is able to learn the camera pose and focal length from segment correspondences (e.g., lines, point clouds), based on pixel-level annotations for segments of a known calibration object. The calibration module iteratively minimizes the segment reprojection error induced by the estimated camera parameters. Second, we propose a novel approach for 3D sports field registration from broadcast soccer images. Compared to the typical solution, which subsequently refines an initial estimation, our solution does it in one step. The proposed method is evaluated for sports field registration on two datasets and achieves superior results compared to two state-of-the-art approaches."
TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation,"Jinyu Yang, Jingjing Liu, Ning Xu, Junzhou Huang",Kuaishou Technology; University Of Texas at Arlington,50,USA,50,China,"Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViT's intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned transferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_TVT_Transferable_Vision_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_TVT_Transferable_Vision_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2023_paper.pdf,,https://github.com/uta-smile/TVT,2108.05988,main,Poster,https://ieeexplore.ieee.org/document/10030518/,"['Computer vision', 'Computer architecture', 'Benchmark testing', 'Image representation', 'Transformers', 'Convolutional neural networks', 'Task analysis']","['Domain Adaptation', 'Vision Transformer', 'Convolutional Neural Network', 'Generalization Ability', 'Knowledge Transfer', 'Attention Mechanism', 'Discriminative Features', 'Transfer Characteristics', 'Vision Tasks', 'Target Domain', 'Source Domain', 'Unlabeled Target Domain', 'Unsupervised Domain Adaptation Methods', 'Labeled Source Domain', 'Domain-invariant Representations', 'Deep Neural Network', 'Average Accuracy', 'Object Detection', 'Feature Learning', 'Transfer Learning', 'Domain Discriminator', 'Multi-head Self-attention', 'Generative Adversarial Networks', 'Fine-grained Features', 'Unlabeled Target Data', 'Optical Character Recognition', 'Source Characteristics', 'Domain-invariant Features', 'Subspace Representation', 'Contemporary Work']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",59,"Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViT’s intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned trans- ferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods."
Task Agnostic and Post-Hoc Unseen Distribution Detection,"Radhika Dua, Seongjun Yang, Yixuan Li, Edward Choi",KAIST; University of Wisconsin-Madison,100,"South Korea, USA",0,,"Despite the recent advances in out-of-distribution(OOD) detection, anomaly detection, and uncertainty estimation tasks, there do not exist a task-agnostic and post-hoc approach. To address this limitation, we design a novel clustering-based ensembling method, called Task Agnostic and Post-hoc Unseen Distribution Detection (TAPUDD) that utilizes the features extracted from the model trained on a specific task. Explicitly, it comprises of TAP-Mahalanobis, which clusters the training datasets' features and determines the minimum Mahalanobis distance of the test sample from all clusters. Further, we propose the Ensembling module that aggregates the computation of iterative TAP-Mahalanobis for a different number of clusters to provide reliable and efficient cluster computation. Through extensive experiments on synthetic and real-world datasets, we observe that our task-agnostic approach can detect unseen samples effectively across diverse tasks and performs better or on-par with the existing task-specific baselines. We also demonstrate that our method is more viable even for large-scale classification tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Dua_Task_Agnostic_and_Post-Hoc_Unseen_Distribution_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dua_Task_Agnostic_and_Post-Hoc_Unseen_Distribution_Detection_WACV_2023_paper.pdf,,,2207.13083,main,Poster,https://ieeexplore.ieee.org/document/10030862/,"['Training', 'Uncertainty', 'Three-dimensional displays', 'Aggregates', 'Medical services', 'Feature extraction', 'Natural language processing']","['Unseen Distributions', 'Training Dataset', 'Classification Task', 'Uncertainty Estimation', 'Mahalanobis Distance', 'Distance Sampling', 'Anomaly Detection', 'Diverse Tasks', 'Different Numbers Of Clusters', 'Characteristics Of Data', 'Deep Neural Network', 'Experimental Details', 'Binary Regression', 'Binary Classification', 'Detection Performance', 'Multi-label', 'Gaussian Mixture Model', 'Agglomerative Clustering', 'Energy Score', 'Binary Classification Task', 'Ensemble Strategy', 'MSE Loss', 'Multi-class Classification Task', 'Post-hoc Method', 'Extensive Details']","['Algorithms: Adversarial learning', 'adversarial attack and defense methods', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Vision + language and/or other modalities']",2,"Despite the recent advances in out-of-distribution(OOD) detection, anomaly detection, and uncertainty estimation tasks, there do not exist a task-agnostic and post-hoc approach. To address this limitation, we design a novel clustering-based ensembling method, called Task Agnostic and Post-hoc Unseen Distribution Detection (TAPUDD) that utilizes the features extracted from the model trained on a specific task. Explicitly, it comprises of TAP-Mahalanobis, which clusters the training datasets’ features and determines the minimum Mahalanobis distance of the test sample from all clusters. Further, we propose the Ensembling module that aggregates the computation of iterative TAP-Mahalanobis for a different number of clusters to provide reliable and efficient cluster computation. Through extensive experiments on synthetic and real-world datasets, we observe that our task-agnostic approach can detect unseen samples effectively across diverse tasks and performs better or on-par with the existing task-specific baselines. We also demonstrate that our method is more viable even for large-scale classification tasks."
TeST: Test-Time Self-Training Under Distribution Shift,"Samarth Sinha, Peter Gehler, Francesco Locatello, Bernt Schiele",AWS Tubingen; University of Toronto,100,"Canada, Germany",0,,"Despite their recent success, deep neural networks continue to perform poorly when they encounter distribution shifts at test time. Many recently proposed approaches try to counter this by aligning the model to the new distribution prior to inference. With no labels available this requires unsupervised objectives to adapt the model on the observed test data. In this paper, we propose Test-Time Self-Training (TeST): a technique that takes as input a model trained on some source data and a novel data distribution at test time, and learns invariant and robust representations using a student-teacher framework. We find that models adapted using TeST significantly improve over baseline test-time adaptation algorithms. TeST achieves competitive performance to modern domain adaptation algorithms [4,43], while having access to 5-10x less data at time of adaption. We thoroughly evaluate a variety of baselines on two tasks: object detection and image segmentation and find that models adapted with TeST. We find that TeST sets the new state-of-the art for test-time domain adaptation algorithms.",https://openaccess.thecvf.com/content/WACV2023/html/Sinha_TeST_Test-Time_Self-Training_Under_Distribution_Shift_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sinha_TeST_Test-Time_Self-Training_Under_Distribution_Shift_WACV_2023_paper.pdf,,,2209.11459,main,Poster,https://ieeexplore.ieee.org/document/10030118/,"['Training', 'Adaptation models', 'Image segmentation', 'Neural networks', 'Object detection', 'Predictive models', 'Prediction algorithms']","['Domain Shift', 'Data Sources', 'Data Distribution', 'Test Data', 'Image Segmentation', 'Object Detection', 'Adaptive Algorithm', 'Domain Adaptation', 'Robust Representation', 'Invariant Representation', 'Deep Learning', 'Training Time', 'Image Classification', 'Teacher Training', 'Bounding Box', 'Representation Learning', 'Semantic Segmentation', 'Representation Of Space', 'Target Domain', 'Source Domain', 'Consistency Regularization', 'Target Distribution', 'Minimum Entropy', 'Unsupervised Adaptation', 'Source Model', 'Bounding Box Regression', 'Training Distribution', 'Self-supervised Learning', 'Default Hyperparameters']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"Despite their recent success, deep neural networks continue to perform poorly when they encounter distribution shifts at test time. Many recently proposed approaches try to counter this by aligning the model to the new distribution prior to inference. With no labels available this requires unsupervised objectives to adapt the model on the observed test data. In this paper, we propose Test-Time Self-Training (TeST): a technique that takes as input a model trained on some source data and a novel data distribution at test time, and learns invariant and robust representations using a student-teacher framework. We find that models adapted using TeST significantly improve over baseline test-time adaptation algorithms. TeST achieves competitive performance to modern domain adaptation algorithms [4], [43], while having access to 5-10x less data at time of adaption. We thoroughly evaluate a variety of baselines on two tasks: object detection and image segmentation and find that models adapted with TeST. We find that TeST sets the new state-of-the art for test-time domain adaptation algorithms."
Temporal Feature Enhancement Dilated Convolution Network for Weakly-Supervised Temporal Action Localization,"Jianxiong Zhou, Ying Wu","Department of Electrical and Computer Engineering, Northwestern University",100,USA,0,,"Weakly-supervised Temporal Action Localization (WTAL) aims to classify and localize action instances in untrimmed videos with only video-level labels. Existing methods typically use snippet-level RGB and optical flow features extracted from pre-trained extractors directly. Because of two limitations: the short temporal span of snippets and the inappropriate initial features, these WTAL methods suffer from the lack of effective use of temporal information and have limited performance. In this paper, we propose the Temporal Feature Enhancement Dilated Convolution Network (TFE-DCN) to address these two limitations. The proposed TFE-DCN has an enlarged receptive field that covers a long temporal span to observe the full dynamics of action instances, which makes it powerful to capture temporal dependencies between snippets. Furthermore, we propose the Modality Enhancement Module that can enhance RGB features with the help of enhanced optical flow features, making the overall features appropriate for the WTAL task. Experiments conducted on THUMOS'14 and ActivityNet v1.3 datasets show that our proposed approach far outperforms state-of-the-art WTAL methods.",https://openaccess.thecvf.com/content/WACV2023/html/Zhou_Temporal_Feature_Enhancement_Dilated_Convolution_Network_for_Weakly-Supervised_Temporal_Action_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhou_Temporal_Feature_Enhancement_Dilated_Convolution_Network_for_Weakly-Supervised_Temporal_Action_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030572/,"['Location awareness', 'Computer vision', 'Convolution', 'Feature extraction', 'Task analysis', 'Optical flow', 'Videos']","['Convolutional Network', 'Temporal Features', 'Temporal Localization', 'Dilated Convolution', 'Action Localization', 'Temporal Action Localization', 'Dilated Convolutional Network', 'Weakly-supervised Temporal Action Localization', 'Use Of Information', 'Receptive Field', 'Optical Characteristics', 'Temporal Information', 'Optical Flow', 'Action Instances', 'Temporal Span', 'RGB Features', 'Convolutional Layers', 'Sigmoid Function', 'Action Classes', 'Graph Convolutional Network', 'Attention Weights', 'Temporal Weights', 'Multiple Instance Learning', 'Temporal Convolutional Network', 'Original RGB', 'Final Objective Function', 'Action Proposals', 'Filter Module', 'Element-wise Multiplication', 'Temporal Convolution']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",11,"Weakly-supervised Temporal Action Localization (WTAL) aims to classify and localize action instances in untrimmed videos with only video-level labels. Existing methods typically use snippet-level RGB and optical flow features extracted from pre-trained extractors directly. Because of two limitations: the short temporal span of snippets and the inappropriate initial features, these WTAL methods suffer from the lack of effective use of temporal information and have limited performance. In this paper, we propose the Temporal Feature Enhancement Dilated Convolution Network (TFE-DCN) to address these two limitations. The proposed TFE-DCN has an enlarged receptive field that covers a long temporal span to observe the full dynamics of action instances, which makes it powerful to capture temporal dependencies between snippets. Furthermore, we propose the Modality Enhancement Module that can enhance RGB features with the help of enhanced optical flow features, making the overall features appropriate for the WTAL task. Experiments conducted on THUMOS’14 and ActivityNet v1.3 datasets show that our proposed approach far outperforms state-of-the-art WTAL methods."
Temporally Consistent Online Depth Estimation in Dynamic Scenes,"Zhaoshuo Li, Wei Ye, Dilin Wang, Francis X. Creighton, Russell H. Taylor, Ganesh Venkatesh, Mathias Unberath","Reality Labs, Meta Inc.; Johns Hopkins University",50,USA,50,USA,"Temporally consistent depth estimation is crucial for online applications such as augmented reality. While stereo depth estimation has received substantial attention as a promising way to generate 3D information, there is relatively little work focused on maintaining temporal stability. Indeed, based on our analysis, current techniques still suffer from poor temporal consistency. Stabilizing depth temporally in dynamic scenes is challenging due to concurrent object and camera motion. In an online setting, this process is further aggravated because only past frames are available. We present a framework named Consistent Online Dynamic Depth (CODD) to produce temporally consistent depth estimates in dynamic scenes in an online setting. CODD augments per-frame stereo networks with novel motion and fusion networks. The motion network accounts for dynamics by predicting a per-pixel SE3 transformation and aligning the observations. The fusion network improves temporal depth consistency by aggregating the current and past estimates. We conduct extensive experiments and demonstrate quantitatively and qualitatively that CODD outperforms competing methods in terms of temporal consistency and performs on par in terms of per-frame accuracy.",https://openaccess.thecvf.com/content/WACV2023/html/Li_Temporally_Consistent_Online_Depth_Estimation_in_Dynamic_Scenes_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Li_Temporally_Consistent_Online_Depth_Estimation_in_Dynamic_Scenes_WACV_2023_paper.pdf,,,2111.09337,main,Poster,https://ieeexplore.ieee.org/document/10030519/,"['Measurement', 'Computer vision', 'Three-dimensional displays', 'Dynamics', 'Estimation', 'Benchmark testing', 'Cameras']","['Depth Estimation', 'Temporal Consistency', 'Dynamic Scenes', 'Temporal Stability', 'Online Setting', 'Fusion Network', 'Camera Motion', 'Poor Consistency', 'Past Frames', 'Kalman Filter', 'Temporal Information', 'Semantic Features', 'Fusion Process', 'Optical Flow', 'Current Frame', 'Memory State', 'Motion Estimation', 'Previous Frame', 'Simultaneous Localization And Mapping', 'Camera Pose', 'Stereo Images', 'Disparity Estimation', 'Fusion Weights', 'Changes In Disparity', 'Motion Prediction', 'Depth Prediction', 'Deformable Objects', 'Qualitative Visualization', 'Static Scenes', '3D Motion']",['Applications: Virtual/augmented reality'],6,"Temporally consistent depth estimation is crucial for online applications such as augmented reality. While stereo depth estimation has received substantial attention as a promising way to generate 3D information, there is relatively little work focused on maintaining temporal stability. Indeed, based on our analysis, current techniques still suffer from poor temporal consistency. Stabilizing depth temporally in dynamic scenes is challenging due to concurrent object and camera motion. In an online setting, this process is further aggravated because only past frames are available. We present a framework named Consistent Online Dynamic Depth (CODD) to produce temporally consistent depth estimates in dynamic scenes in an online setting. CODD augments per-frame stereo networks with novel motion and fusion networks. The motion network accounts for dynamics by predicting a per-pixel SE3 transformation and aligning the observations. The fusion network improves temporal depth consistency by aggregating the current and past estimates. We conduct extensive experiments and demonstrate quantitatively and qualitatively that CODD outperforms competing methods in terms of temporal consistency and performs on par in terms of per-frame accuracy."
Text and Image Guided 3D Avatar Generation and Manipulation,"Zehranaz Canfes, M. Furkan Atasoy, Alara Dirik, Pinar Yanardag","Bo˘gazic ¸i University, Istanbul, Turkey",100,Turkey,0,,"The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as 'a young face' or 'a surprised face'. We leverage the power of Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars, and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced, while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons.",https://openaccess.thecvf.com/content/WACV2023/html/Canfes_Text_and_Image_Guided_3D_Avatar_Generation_and_Manipulation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Canfes_Text_and_Image_Guided_3D_Avatar_Generation_and_Manipulation_WACV_2023_paper.pdf,,,2202.06079,main,Poster,https://ieeexplore.ieee.org/document/10030861/,"['Solid modeling', 'Three-dimensional displays', 'Shape', 'Avatars', 'Source coding', 'Pipelines', 'Process control']","['3D Manipulation', 'Generative Adversarial Networks', 'Latent Space', 'Generative Adversarial Networks Model', 'Latent Code', 'Analysis Of Changes', 'Facial Expressions', 'Rigid Body', 'Point Cloud', 'Target Image', 'Human Faces', '3D Mesh', '3D Shape', 'Use Of Complexes', 'Simple Use', 'Simple Shapes', 'Latent Vector', 'L2 Loss', 'Older Humans', 'Use Of Shapes', 'Target Text', 'Implicit Representation', 'StyleGAN', 'Vector C', 'Unsupervised Methods', '3D Reconstruction', 'Default Hyperparameters', 'Facial Shape']","['Algorithms: 3D computer vision', 'Biometrics', 'face', 'gesture', 'body pose']",13,"The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as ’a young face’ or ’a surprised face’. We leverage the power of Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons."
Text-Guided Object Detector for Multi-Modal Video Question Answering,"Ruoyue Shen, Nakamasa Inoue, Koichi Shinoda",Tokyo Institute of Technology,100,Japan,0,,"Video Question Answering (Video QA) is a task to answer a text-format question based on the understanding of linguistic semantics, visual information, and also linguistic-visual alignment in the video. In Video QA, an object detector pre-trained with large-scale datasets, such as Faster R-CNN, has been widely used to extract visual representations from video frames. However, it is not always able to precisely detect the objects needed to answer the question because of the domain gaps between the datasets for training the object detector and those for Video QA. In this paper, we propose a text-guided object detector (TGOD), which takes text question-answer pairs and video frames as inputs, detects the objects relevant to the given text, and thus provides intuitive visualization and interpretable results. Our experiments using the STAGE framework on the TVQA+ dataset show the effectiveness of our proposed detector. It achieves a 2.02 points improvement in accuracy of QA, 12.13 points improvement in object detection (mAP50), 1.1 points improvement in temporal location, and 2.52 points improvement in ASA over the STAGE original detector.",https://openaccess.thecvf.com/content/WACV2023/html/Shen_Text-Guided_Object_Detector_for_Multi-Modal_Video_Question_Answering_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Shen_Text-Guided_Object_Detector_for_Multi-Modal_Video_Question_Answering_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030907/,"['Training', 'Measurement', 'Visualization', 'Annotations', 'Semantics', 'Detectors', 'Object detection']","['Object Detection', 'Question Answering', 'Interpretation Of Results', 'Visual Information', 'Visual Representation', 'Video Frames', 'Faster R-CNN', 'Convolutional Neural Network', 'Feature Maps', 'Visual Features', 'Bounding Box', 'Feed-forward Network', 'Visual Input', 'Words In Sentences', 'Encoder Layer', 'Contrastive Loss', 'Linguistic Information', 'Inference Speed', 'Decoder Layer', 'Position Embedding', 'Transformer Decoder', 'Convolutional Neural Networks Backbone', 'Transformer Encoder', 'Text Encoder', 'Word Tokens', 'Visual Encoding', 'Prediction Head', 'Label Prediction', 'Postage', 'Object Detection Task']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Vision + language and/or other modalities']",4,"Video Question Answering (Video QA) is a task to answer a text-format question based on the understanding of linguistic semantics, visual information, and also linguistic-visual alignment in the video. In Video QA, an object detector pre-trained with large-scale datasets, such as Faster R-CNN, has been widely used to extract visual representations from video frames. However, it is not always able to precisely detect the objects needed to answer the question be-cause of the domain gaps between the datasets for training the object detector and those for Video QA. In this paper, we propose a text-guided object detector (TGOD), which takes text question-answer pairs and video frames as inputs, detects the objects relevant to the given text, and thus provides intuitive visualization and interpretable results. Our experiments using the STAGE framework on the TVQA+ dataset show the effectiveness of our proposed detector. It achieves a 2.02 points improvement in accuracy of QA, 12.13 points improvement in object detection (mAP50), 1.1 points improvement in temporal location, and 2.52 points improvement in ASA over the STAGE original detector."
The Box Size Confidence Bias Harms Your Object Detector,"Johannes Gilg, Torben Teepe, Fabian Herzog, Gerhard Rigoll",Technical University of Munich,100,Germany,0,,"Countless applications depend on accurate predictions with reliable confidence estimates from modern object detectors. However, it is well known that neural networks, including object detectors, produce miscalibrated confidence estimates. Recent work even suggests that detectors' confidence predictions are biased with respect to object size and position. In object detection, the issues of conditional biases, confidence calibration, and task performance are usually explored in isolation, but, as we aim to show, they are closely related. We formally prove that the conditional confidence bias harms the performance of object detectors and empirically validate these findings. Specifically, to quantify the performance impact of the confidence bias on object detectors, we modify the histogram binning calibration to avoid performance impairment and instead improve it through calibration conditioned on the bounding box size. We further find that the confidence bias is also present in detections generated on the training data of the detector, which can be leveraged to perform the de-biasing. Moreover, we show that Test Time Augmentation (TTA) confounds this bias, which results in even more significant performance impairments on the detectors. Finally, we use our proposed algorithm to analyze a diverse set of object detection architectures and show that the conditional confidence bias harms their performance by up to 0.6 mAP and 0.8 mAP50. Code available at https://github.com/Blueblue4/Object-Detection-Confidence-Bias.",https://openaccess.thecvf.com/content/WACV2023/html/Gilg_The_Box_Size_Confidence_Bias_Harms_Your_Object_Detector_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gilg_The_Box_Size_Confidence_Bias_Harms_Your_Object_Detector_WACV_2023_paper.pdf,,https://github.com/Blueblue4/Object-Detection-Confidence-Bias,2112.01901,main,Poster,https://ieeexplore.ieee.org/document/10031023/,"['Histograms', 'Computer vision', 'Neural networks', 'Training data', 'Detectors', 'Object detection', 'Computer architecture']","['Object Detection', 'Box Size', 'Confidence Bias', 'Neural Network', 'Training Data', 'Detection Performance', 'Bounding Box', 'Impact Of Bias', 'Prediction Confidence', 'Confidence Estimation', 'Histogram Bins', 'Bounding Box Size', 'Set Of Architectures', 'Loss Function', 'Standard Curve', 'Training Set', 'Deep Learning', 'Conditional Probability', 'Intersection Over Union', 'Precision And Recall', 'Calibration Method', 'True Positive Detection', 'Ground-truth Bounding Box', 'Calibration Strategy', 'True Positive Predictions', 'Predicted Bounding Box', 'Deep Object Detection', 'Two-stage Detectors', 'Average Precision', 'Formal Proof']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"Countless applications depend on accurate predictions with reliable confidence estimates from modern object detectors. However, it is well known that neural networks, including object detectors, produce miscalibrated confidence estimates. Recent work even suggests that detectors’ confidence predictions are biased with respect to object size and position. In object detection, the issues of conditional biases, confidence calibration, and task performance are usually explored in isolation, but, as we aim to show, they are closely related. We formally prove that the conditional confidence bias harms the performance of object detectors and empirically validate these findings. Specifically, to quantify the performance impact of the confidence bias on object detectors, we modify the histogram binning calibration to avoid performance impairment and instead improve it through calibration conditioned on the bounding box size. We further find that the confidence bias is also present in detections generated on the training data of the detector, which can be leveraged to perform the de-biasing. Moreover, we show that Test Time Augmentation (TTA) confounds this bias, which results in even more significant performance impairments on the detectors. Finally, we use our proposed algorithm to analyze a diverse set of object detection architectures and show that the conditional confidence bias harms their performance by up to 0.6 mAP and 0.8 mAP
<inf>50</inf>
. Code available at https://github.com/Blueblue4/Object-Detection-Confidence-Bias."
The Change You Want To See,"Ragav Sachdeva, Andrew Zisserman","Visual Geometry Group, Dept. of Engineering Science, University of Oxford",100,UK,0,,"We live in a dynamic world where things change all the time. Given two images of the same scene, being able to automatically detect the changes in them has practical applications in a variety of domains. In this paper, we tackle the change detection problem with the goal of detecting ""object-level"" changes in an image pair despite differences in their viewpoint and illumination. To this end, we make the following four contributions: (i) we propose a scalable methodology for obtaining a large-scale change detection training dataset by leveraging existing object segmentation benchmarks; (ii) we introduce a co-attention based novel architecture that is able to implicitly determine correspondences between an image pair and find changes in the form of bounding box predictions; (iii) we contribute four evaluation datasets that cover a variety of domains and transformations, including synthetic image changes, real surveillance images of a 3D scene, and synthetic 3D scenes with camera motion; (iv) we evaluate our model on these four datasets and demonstrate zero-shot and beyond training transformation generalization. The code, datasets and pre-trained model can be found at our project page: https://www.robots.ox.ac.uk/ vgg/research/cyws/",https://openaccess.thecvf.com/content/WACV2023/html/Sachdeva_The_Change_You_Want_To_See_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sachdeva_The_Change_You_Want_To_See_WACV_2023_paper.pdf,https://www.robots.ox.ac.uk/~vgg/research/cyws/,,2209.14341,main,Poster,https://ieeexplore.ieee.org/document/10030458/,"['Training', 'Solid modeling', 'Three-dimensional displays', 'Surveillance', 'Training data', 'Detectors', 'Benchmark testing']","['Training Dataset', 'Change Detection', 'Large-scale Datasets', 'Bounding Box', 'Image Pairs', 'Scene Images', 'Evaluation Dataset', '3D Scene', 'Camera Motion', 'Prediction Of Formation', 'Predicted Bounding Box', 'Things Change', 'Changes In Activity', 'Training Data', 'Feature Maps', 'Nuisance', 'Image Object', 'Average Precision', 'Affine Transformation', 'Geometric Transformation', 'Scene Changes', 'Inpainting', 'Ground-truth Bounding Box', 'Image Inpainting', 'Color Jittering', 'Street Scenes', 'Parallax']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",7,"We live in a dynamic world where things change all the time. Given two images of the same scene, being able to automatically detect the changes in them has practical applications in a variety of domains. In this paper, we tackle the change detection problem with the goal of detecting ""object-level"" changes in an image pair despite differences in their viewpoint and illumination. To this end, we make the following four contributions: (i) we pro-pose a scalable methodology for obtaining a large-scale change detection training dataset by leveraging existing object segmentation benchmarks; (ii) we introduce a co-attention based novel architecture that is able to implicitly determine correspondences between an image pair and find changes in the form of bounding box predictions; (iii) we contribute four evaluation datasets that cover a variety of domains and transformations, including synthetic image changes, real surveillance images of a 3D scene, and synthetic 3D scenes with camera motion; (iv) we evaluate our model on these four datasets and demonstrate zero-shot and beyond training transformation generalization. The code, datasets and pre-trained model can be found at our project page: https://www.robots.ox.ac.uk/~vgg/research/cyws/."
The CropAndWeed Dataset: A Multi-Modal Learning Approach for Efficient Crop and Weed Manipulation,"Daniel Steininger, Andreas Trondl, Gerardus Croonen, Julia Simon, Verena Widhalm",AIT Austrian Institute of Technology,100,Austria,0,,"Precision Agriculture and especially the application of automated weed intervention represents an increasingly essential research area, as sustainability and efficiency considerations are becoming more and more relevant. While the potentials of Convolutional Neural Networks for detection, classification and segmentation tasks have successfully been demonstrated in other application areas, this relatively new field currently lacks the required quantity and quality of training data for such a highly data-driven approach. Therefore, we propose a novel large-scale image dataset specializing in the fine-grained identification of 74 relevant crop and weed species with a strong emphasis on data variability. We provide annotations of labeled bounding boxes, semantic masks and stem positions for about 112k instances in more than 8k high-resolution images of both real-world agricultural sites and specifically cultivated outdoor plots of rare weed types. Additionally, each sample is enriched with an extensive set of meta-annotations regarding environmental conditions and recording parameters. We furthermore conduct benchmark experiments for multiple learning tasks on different variants of the dataset to demonstrate its versatility and provide examples of useful mapping schemes for tailoring the annotated data to the requirements of specific applications. In the course of the evaluation, we furthermore demonstrate how incorporating multiple species of weeds into the learning process increases the accuracy of crop detection. Overall, the evaluation clearly demonstrates that our dataset represents an essential step towards overcoming the data gap and promoting further research in the area of Precision Agriculture.",https://openaccess.thecvf.com/content/WACV2023/html/Steininger_The_CropAndWeed_Dataset_A_Multi-Modal_Learning_Approach_for_Efficient_Crop_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Steininger_The_CropAndWeed_Dataset_A_Multi-Modal_Learning_Approach_for_Efficient_Crop_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030808/,"['Location awareness', 'Training', 'Image segmentation', 'Annotations', 'Crops', 'Training data', 'Benchmark testing']","['Training Data', 'Convolutional Neural Network', 'Learning Task', 'Bounding Box', 'Crop Species', 'Annotation Data', 'Multiple Tasks', 'Precision Agriculture', 'Weed Species', 'Evaluation Of Course', 'Variant Dataset', 'Relevant Crops', 'Model Performance', 'Test Data', 'Imaging Data', 'Light Conditions', 'Crop Growth', 'Detection Performance', 'Object Detection', 'Detection Results', 'Single Class', 'Semantic Segmentation', 'Visual Similarity', 'Early Growth Stages', 'Instance Size', 'Cultivated Area', 'Crop Classification', 'Input Modalities', 'Crop Types', 'Detection Model']","['Applications: Agriculture', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",8,"Precision Agriculture and especially the application of automated weed intervention represents an increasingly essential research area, as sustainability and efficiency considerations are becoming more and more relevant. While the potentials of Convolutional Neural Networks for detection, classification and segmentation tasks have successfully been demonstrated in other application areas, this relatively new field currently lacks the required quantity and quality of training data for such a highly data-driven approach. Therefore, we propose a novel large-scale image dataset specializing in the fine-grained identification of 74 relevant crop and weed species with a strong emphasis on data variability. We provide annotations of labeled bounding boxes, semantic masks and stem positions for about 112k instances in more than 8k high-resolution images of both real-world agricultural sites and specifically cultivated outdoor plots of rare weed types. Additionally, each sample is enriched with an extensive set of meta-annotations regarding environmental conditions and recording parameters. We furthermore conduct benchmark experiments for multiple learning tasks on different variants of the dataset to demonstrate its versatility and provide examples of useful mapping schemes for tailoring the annotated data to the requirements of specific applications. In the course of the evaluation, we furthermore demonstrate how incorporating multiple species of weeds into the learning process increases the accuracy of crop detection. Overall, the evaluation clearly demonstrates that our dataset represents an essential step towards overcoming the data gap and promoting further research in the area of Precision Agriculture."
The Fully Convolutional Transformer for Medical Image Segmentation,"Athanasios Tragakis, Chaitanya Kaul, Roderick Murray-Smith, Dirk Husmeier","Mathematics and Statistics, University of Glasgow, United Kingdom, G12 8QW; School of Computing Science, University of Glasgow, United Kingdom, G12 8RZ",100,UK,0,,"We propose a novel transformer model, capable of segmenting medical images of varying modalities. Challenges posed by the fine-grained nature of medical image analysis mean that the adaptation of the transformer for their analysis is still at nascent stages. The overwhelming success of the UNet lay in its ability to appreciate the fine-grained nature of the segmentation task, an ability which existing transformer based models do not currently posses. To address this shortcoming, we propose The Fully Convolutional Transformer (FCT), which builds on the proven ability of Convolutional Neural Networks to learn effective image representations, and combines them with the ability of Transformers to effectively capture long-term dependencies in its inputs. The FCT is the first fully convolutional Transformer model in medical imaging literature. It processes its input in two stages, where first, it learns to extract long range semantic dependencies from the input image, and then learns to capture hierarchical global attributes from the features. FCT is compact, accurate and robust. Our results show that it outperforms all existing transformer architectures by large margins across multiple medical image segmentation datasets of varying data modalities without the need for any pre-training. FCT outperforms its immediate competitor on the ACDC dataset by 1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC 2017 dataset by 1.1% on the dice metric, with up to five times fewer parameters. On the ACDC Post-2017-MICCAI-Challenge online test set, our model sets a new state-of-the-art on unseen MRI test cases outperforming large ensemble models as well as nnUNet with considerably fewer parameters. Our code, environments and models will be available via GitHub.",https://openaccess.thecvf.com/content/WACV2023/html/Tragakis_The_Fully_Convolutional_Transformer_for_Medical_Image_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tragakis_The_Fully_Convolutional_Transformer_for_Medical_Image_Segmentation_WACV_2023_paper.pdf,,https://github.com/Thanos-DB/FullyConvolutionalTransformer,2206.00566,main,Poster,https://ieeexplore.ieee.org/document/10030969/,"['Convolutional codes', 'Image segmentation', 'Technological innovation', 'Semantic segmentation', 'Semantics', 'Transformers', 'Feature extraction']","['Medical Imaging', 'Medical Image Segmentation', 'Synapse', 'Convolutional Neural Network', 'Input Image', 'Fewer Parameters', 'Segmentation Task', 'Transformer Model', 'Segmentation Dataset', 'Convolutional Layers', 'Image Dataset', 'Attention Mechanism', 'Receptive Field', 'Convolution Operation', 'Semantic Segmentation', 'Spatial Context', 'Skip Connections', 'Segmentation Map', 'Linear Projection', 'Binary Segmentation', 'Multi-head Self-attention', 'Atrous Convolution', 'Transformer Layers', 'Deep Supervision', 'Transformer Block', 'Concurrent Work', 'Image Context', 'Image Pyramid', 'Positional Encoding', 'Decoder Layer']",['Applications: Biomedical/healthcare/medicine'],58,"We propose a novel transformer, capable of segmenting medical images of varying modalities. Challenges posed by the fine-grained nature of medical image analysis mean that the adaptation of the transformer for their analysis is still at nascent stages. The overwhelming success of the UNet lay in its ability to appreciate the fine-grained nature of the segmentation task, an ability which existing transformer based models do not currently posses. To address this shortcoming, we propose The Fully Convolutional Transformer (FCT), which builds on the proven ability of Convolutional Neural Networks to learn effective image representations, and combines them with the ability of Transformers to effectively capture long-term dependencies in its inputs. The FCT is the first fully convolutional Transformer model in medical imaging literature. It processes its input in two stages, where first, it learns to extract long range semantic dependencies from the input image, and then learns to capture hierarchical global attributes from the features. FCT is compact, accurate and robust. Our results show that it outperforms all existing transformer architectures by large margins across multiple medical image segmentation datasets of varying data modalities without the need for any pre-training. FCT outperforms its immediate competitor on the ACDC dataset by 1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC 2017 dataset by 1.1% on the dice metric, with up to five times fewer parameters. On the ACDC Post-2017-MICCAI-Challenge online test set, our model sets a new state-of-the-art on unseen MRI test cases out-performing large ensemble models as well as nnUNet with considerably fewer parameters. Our code, environments and models will be available via GitHub
<sup>†</sup>
."
TinyHD: Efficient Video Saliency Prediction With Heterogeneous Decoders Using Hierarchical Maps Distillation,"Feiyan Hu, Simone Palazzo, Federica Proietto Salanitri, Giovanni Bellitto, Morteza Moradi, Concetto Spampinato, Kevin McGuinness","Insight SFI Research Centre for Data Analytics, Dublin City University, Dublin, Ireland; PeRCeiVe Lab, University of Catania, Catania, Italy",100,"Ireland, Italy",0,,"Video saliency prediction has recently attracted attention of the research community, as it is an upstream task for several practical applications. However, current solutions are particurly computationally demanding, especially due to the wide usage of spatio-temporal 3D convolutions. We observe that, while different model architectures achieve similar performance on benchmarks, visual variations between predicted saliency maps are still significant. Inspired by this intuition, we propose a lightweight model that employs multiple simple heterogeneous decoders and adopts several practical approaches to improve accuracy while keeping computational costs low, such as hierarchical multi-map knowledge distillation, multi-output saliency prediction, unlabeled auxiliary datasets and channel reduction with teacher assistant supervision. Our approach achieves saliency prediction accuracy on par or better than state-of-the-art methods on DFH1K, UCF-Sports and Hollywood2 benchmarks, while enhancing significantly the efficiency of the model.",https://openaccess.thecvf.com/content/WACV2023/html/Hu_TinyHD_Efficient_Video_Saliency_Prediction_With_Heterogeneous_Decoders_Using_Hierarchical_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Hu_TinyHD_Efficient_Video_Saliency_Prediction_With_Heterogeneous_Decoders_Using_Hierarchical_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030222/,"['Visualization', 'Three-dimensional displays', 'Computational modeling', 'Computer architecture', 'Predictive models', 'Benchmark testing', 'Semisupervised learning']","['Hierarchical Map', 'Saliency Prediction', 'Video Saliency', 'Computational Cost', 'Teaching Assistants', 'Saliency Map', '3D Convolution', 'Channel Reduction', 'Auxiliary Dataset', 'Temporal Dimension', 'Final Output', 'Final Prediction', 'Video Sequences', 'Multiple Predictors', 'Human Visual System', 'Encoder Layer', 'Ground Truth Map', 'Fusion Layer', 'Decoder Architecture', 'Intermediate Maps', 'Decoding Scheme', 'Multiple-input Single-output', 'Decoding Path', 'Abstraction Layer']","['Applications: Psychology and cognitive science', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",4,"Video saliency prediction has recently attracted attention of the research community, as it is an upstream task for several practical applications. However, current solutions are particurly computationally demanding, especially due to the wide usage of spatio-temporal 3D convolutions. We observe that, while different model architectures achieve similar performance on benchmarks, visual variations between predicted saliency maps are still significant. Inspired by this intuition, we propose a lightweight model that employs multiple simple heterogeneous decoders and adopts several practical approaches to improve accuracy while keeping computational costs low, such as hierarchical multi-map knowledge distillation, multi-output saliency prediction, unlabeled auxiliary datasets and channel reduction with teacher assistant supervision. Our approach achieves saliency prediction accuracy on par or better than state-of-the-art methods on DFH1K, UCF-Sports and Hollywood2 benchmarks, while enhancing significantly the efficiency of the model."
Token Pooling in Vision Transformers for Image Classification,"Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, Oncel Tuzel",Apple; University of Waterloo,50,Canada,50,USA,"Pooling is commonly used to improve the computation-accuracy trade-off of convolutional networks. By aggregating neighboring feature values on the image grid, pooling layers downsample feature maps while maintaining accuracy. In transformers, however, tokens are processed individually and do not necessarily lie on regular grids. Utilizing pooling methods designed for image grids (e.g., average pooling) can thus be sub-optimal for transformers, as shown by our experiments. In this paper, we propose Token Pooling to downsample tokens in vision transformers. We take a new perspective --- instead of assuming tokens form a regular grid, we treat them as discrete (and irregular) samples of a continuous signal. Given a target number of tokens, Token Pooling finds the set of tokens that best approximates the underlying continuous signal. We rigorously evaluate the proposed method on the standard transformer architecture (ViT/DeiT), and our experiments show that Token Pooling significantly improves the computation-accuracy trade-off without any further modifications to the architecture. On ImageNet-1k, Token Pooling enables DeiT-Ti to achieve the same top-1 accuracy while using 42% fewer computations.",https://openaccess.thecvf.com/content/WACV2023/html/Marin_Token_Pooling_in_Vision_Transformers_for_Image_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Marin_Token_Pooling_in_Vision_Transformers_for_Image_Classification_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030157/,"['Filtering', 'Semantic segmentation', 'Pose estimation', 'Computer architecture', 'Transformers', 'Encoding', 'Convolutional neural networks']","['Image Classification', 'Vision Transformer', 'Feature Values', 'Feature Maps', 'Continuous-time', 'Regular Grid', 'Signal Samples', 'Average Pooling', 'Discrete Samples', 'Standard Transformation', 'Top-1 Accuracy', 'Standard Architecture', 'Transformer Architecture', 'Image Grid', 'Image Classification Problems', 'Computational Cost', 'Convolutional Neural Network', 'Clustering Algorithm', 'Random Selection', 'Transformer Block', 'Input Tokens', 'Transformation Efficiency', 'Downsampling Layer', 'Learning Rate Schedule', 'Query Vector', 'Time Complexity', 'Multilayer Perceptron', 'Number Of Heads', 'Unordered Set']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)']",14,"Pooling is commonly used to improve the computation-accuracy trade-off of convolutional networks. By aggregating neighboring feature values on the image grid, pooling layers downsample feature maps while maintaining accuracy. In standard vision transformers, however, tokens are processed individually and do not necessarily lie on regular grids. Utilizing pooling methods designed for image grids (e.g., average pooling) thus can be sub-optimal for transformers, as shown by our experiments. In this paper, we propose Token Pooling to downsample token sets in vision transformers. We take a new perspective — instead of assuming tokens form a regular grid, we treat them as discrete (and irregular) samples of an implicit continuous signal. Given a target number of tokens, Token Pooling finds the set of tokens that best approximates the underlying continuous signal. We rigorously evaluate the proposed method on the standard transformer architecture (ViT/DeiT) and on the image classification problem using ImageNet-1k. Our experiments show that Token Pooling significantly improves the computation-accuracy trade-off without any further modifications to the architecture. Token Pooling enables DeiT-Ti to achieve the same top-1 accuracy while using 42% fewer computations."
Toward Edge-Efficient Dense Predictions With Synergistic Multi-Task Neural Architecture Search,"Thanh Vu, Yanqi Zhou, Chunfeng Wen, Yueqi Li, Jan-Michael Frahm","UNC at Chapel Hill; X, The Moonshot Factory; Google Research",33.33333333,USA,66.66666667,USA,"In this work, we propose a novel and scalable solution to address the challenges of developing efficient dense predictions on edge platforms. Our first key insight is that MultiTask Learning (MTL) and hardware-aware Neural Architecture Search (NAS) can work in synergy to greatly benefit on-device Dense Predictions (DP). Empirical results reveal that the joint learning of the two paradigms is surprisingly effective at improving DP accuracy, achieving superior performance over both the transfer learning of single-task NAS and prior state-of-the-art approaches in MTL, all with just 1/10th of the computation. To the best of our knowledge, our framework, named EDNAS, is the first to successfully leverage the synergistic relationship of NAS and MTL for DP. Our second key insight is that the standard depth training for multi-task DP can cause significant instability and noise to MTL evaluation. Instead, we propose JAReD, an improved, easy-to-adopt Joint Absolute-Relative Depth loss, that reduces up to 88% of the undesired noise while simultaneously boosting accuracy. We conduct extensive evaluations on standard datasets, benchmark against strong baselines and state-of-the-art approaches, as well as provide an analysis of the discovered optimal architectures.",https://openaccess.thecvf.com/content/WACV2023/html/Vu_Toward_Edge-Efficient_Dense_Predictions_With_Synergistic_Multi-Task_Neural_Architecture_Search_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Vu_Toward_Edge-Efficient_Dense_Predictions_With_Synergistic_Multi-Task_Neural_Architecture_Search_WACV_2023_paper.pdf,,,2210.01384,main,Poster,https://ieeexplore.ieee.org/document/10030152/,"['Training', 'Computer vision', 'Transfer learning', 'Computer architecture', 'Benchmark testing', 'Multitasking', 'Boosting']","['Neural Architecture Search', 'Dense Prediction', 'Multi-task Architecture', 'Transfer Learning', 'Key Insights', 'Standard Training', 'Multi-task Learning', 'Strong Baseline', 'Joint Learning', 'Objective Function', 'Learning Rate', 'Image Classification', 'Search Space', 'Semantic Segmentation', 'Multiple Tasks', 'Individual Tasks', 'Reward Function', 'Depth Estimation', 'Relative Depth', 'Basic Architecture', 'Absolute Depth', 'Relative Gain', 'Edge Devices', 'Depth Prediction', 'Multi-task Model', 'Inference Speed', 'Target Edge', 'Negative Transfer', 'L1 Loss', 'Learning For Segmentation']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Embedded sensing/real-time techniques']",4,"In this work, we propose a novel and scalable solution to address the challenges of developing efficient dense predictions on edge platforms. Our first key insight is that MultiTask Learning (MTL) and hardware-aware Neural Architecture Search (NAS) can work in synergy to greatly benefit on-device Dense Predictions (DP). Empirical results reveal that the joint learning of the two paradigms is surprisingly effective at improving DP accuracy, achieving superior performance over both the transfer learning of single-task NAS and prior state-of-the-art approaches in MTL, all with just 1/10th of the computation. To the best of our knowledge, our framework, named EDNAS, is the first to successfully leverage the synergistic relationship of NAS and MTL for DP. Our second key insight is that the standard depth training for multi-task DP can cause significant instability and noise to MTL evaluation. Instead, we propose JAReD, an improved, easy-to-adopt Joint Absolute-Relative Depth loss, that reduces up to 88% of the undesired noise while simultaneously boosting accuracy. We conduct extensive evaluations on standard datasets, benchmark against strong baselines and state-of-the-art approaches, as well as provide an analysis of the discovered optimal architectures."
Towards Discriminative and Transferable One-Stage Few-Shot Object Detectors,"Karim Guirguis, Mohamed Abdelsamad, George Eskandar, Ahmed Hendawy, Matthias Kayser, Bin Yang, Jürgen Beyerer",University of Stuttgart; Robert Bosch GmbH†Karlsruhe Institute of Technology; Karlsruhe Institute of Technology†Fraunhofer IOSB,100,Germany,0,,"Recent object detection models have proved valuable for many robotics and manufacturing tasks, but they require large amounts of annotated data for each new class of objects they are trained for. Few-shot object detection (FSOD) aims to address this problem by learning novel classes given only a few samples of annotated data. While competitive results have been achieved using two-stage FSOD detectors, typically faster one-stage FSODs underperform in comparison. We make the discovery that the large gap in performance between two-stage and one-stage FSODs is mainly due to their weak discriminability, which is explained away by a small post-fusion receptive field and a small number of foreground samples in the loss function. We propose a new one-stage FSOD framework to address these limitations - Few-shot RetinaNet (FSRN). Specifically, we propose: (1) a multi-way support training strategy to augment the number of foreground samples for dense meta-detectors during training, (2) an early multi-level feature fusion providing a wide receptive field that covers the whole anchor area, (3) two augmentation techniques on query and source images to enhance transferability. Extensive experiments demonstrate that the proposed approach addresses the limitations of previous methods and boosts both discriminability and transferability. FSRN is two times faster than twostage FSODs while remaining competitive in accuracy, and it triples the state-of-the-art of one-stage meta-detectors on the competitive 10-shot MS-COCO benchmark. On the PASCAL VOC benchmark, the proposed approach consistently outperforms one-stage meta-detectors and many two-stage FSODs.",https://openaccess.thecvf.com/content/WACV2023/html/Guirguis_Towards_Discriminative_and_Transferable_One-Stage_Few-Shot_Object_Detectors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Guirguis_Towards_Discriminative_and_Transferable_One-Stage_Few-Shot_Object_Detectors_WACV_2023_paper.pdf,,,2210.05783,main,Poster,https://ieeexplore.ieee.org/document/10030509/,"['Training', 'Computer vision', 'Annotations', 'Object detection', 'Detectors', 'Benchmark testing', 'Data models']","['Object Detection', 'Receptive Field', 'Feature Fusion', 'Augmentation Techniques', 'Query Image', 'Early Fusion', 'Small Receptive Field', 'Data Augmentation', 'Transfer Learning', 'Bounding Box', 'Global Average Pooling', 'Focal Loss', 'Base Classes', 'Support Set', 'Feature Pyramid Network', 'Low Discrimination', 'Region Proposal Network', 'Number Of Annotations', 'Few-shot Learning', 'Detection Head', 'Query Features', 'Class Prototypes', 'PASCAL VOC Dataset', 'MS COCO Dataset', 'Multi-scale Feature Fusion', 'One-stage Detectors', 'Two-stage Detectors', 'Self-driving', 'Region Proposal', 'Feature Learning']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",2,"Recent object detection models require large amounts of annotated data for training a new classes of objects. Few-shot object detection (FSOD) aims to address this problem by learning novel classes given only a few samples. While competitive results have been achieved using two-stage FSOD detectors, typically one-stage FSODs under-perform compared to them. We make the observation that the large gap in performance between two-stage and one-stage FSODs are mainly due to their weak discriminability, which is explained by a small post-fusion receptive field and a small number of foreground samples in the loss function. To address these limitations, we propose the Few-shot RetinaNet (FSRN) that consists of: a multi-way support training strategy to augment the number of foreground samples for dense meta-detectors, an early multi-level feature fusion providing a wide receptive field that covers the whole anchor area and two augmentation techniques on query and source images to enhance transferability. Extensive experiments show that the proposed approach addresses the limitations and boosts both discriminability and transferability. FSRN is almost two times faster than two-stage FSODs while remaining competitive in accuracy, and it outperforms the state-of-the-art of one-stage meta-detectors and also some two-stage FSODs on the MS-COCO and PASCAL VOC benchmarks."
Towards Disturbance-Free Visual Mobile Manipulation,"Tianwei Ni, Kiana Ehsani, Luca Weihs, Jordi Salvador",PRIOR @ Allen Institute for AI; Universit ´e de Montr ´eal & Mila – Quebec AI Institute,100,"Canada, USA",0,,"Deep reinforcement learning has shown promising results on an abundance of robotic tasks in simulation, including visual navigation and manipulation. Prior work generally aims to build embodied agents that solve their assigned tasks as quickly as possible, while largely ignoring the problems caused by collision with objects during interaction. This lack of prioritization is understandable: there is no inherent cost in breaking virtual objects. As a result, ""well-trained"" agents frequently collide with objects before achieving their primary goals, a behavior that would be catastrophic in the real world. In this paper, we study the problem of training agents to complete the task of visual mobile manipulation in the ManipulaTHOR environment while avoiding unnecessary collision (disturbance) with objects. We formulate disturbance avoidance as a penalty term in the reward function, but find that directly training with such penalized rewards often results in agents being unable to escape poor local optima. Instead, we propose a two-stage training curriculum where an agent is first allowed to freely explore and build basic competencies without penalization, after which a disturbance penalty is introduced to refine the agent's behavior. Results on testing scenes show that our curriculum not only avoids these poor local optima, but also leads to 10% absolute gains in success rate without disturbance, compared to our state-of-the-art baselines. Moreover, our curriculum is significantly more performant than a safe RL algorithm that casts collision avoidance as a constraint. Finally, we propose a novel disturbance-prediction auxiliary task that accelerates learning.",https://openaccess.thecvf.com/content/WACV2023/html/Ni_Towards_Disturbance-Free_Visual_Mobile_Manipulation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ni_Towards_Disturbance-Free_Visual_Mobile_Manipulation_WACV_2023_paper.pdf,https://sites.google.com/view/disturb-freethe,,2112.12612,main,Poster,https://ieeexplore.ieee.org/document/10030790/,"['Training', 'Deep learning', 'Visualization', 'Computer vision', 'Costs', 'Navigation', 'Reinforcement learning']","['Mobile Manipulator', 'Reward Function', 'Deep Reinforcement Learning', 'Machine Vision', 'Auxiliary Task', 'Trained Agent', 'Two-stage Training', 'Lack Of Prioritization', 'Deep Learning', 'Validation Set', 'Transfer Learning', 'High Success Rate', 'Target Object', 'Path Planning', 'Sampling Efficiency', 'Model-based Approach', 'Design Decisions', 'Sequential Task', 'Original Objective', 'Collision Detection', 'Reward Structure', 'Curriculum Learning', 'Reinforcement Learning Agent', 'Inverse Dynamics', 'Collision Impact', 'Real-world Deployment', 'Privileged Information']","['Algorithms: Vision + language and/or other modalities', 'Robotics']",2,"Deep reinforcement learning has shown promising results on an abundance of robotic tasks in simulation, including visual navigation and manipulation. Prior work generally aims to build embodied agents that solve their assigned tasks as quickly as possible, while largely ignoring the problems caused by collision with objects during interaction. This lack of prioritization is understandable: there is no inherent cost in breaking virtual objects. As a result, ""well-trained"" agents frequently collide with objects before achieving their primary goals, a behavior that would be catastrophic in the real world. In this paper, we study the problem of training agents to complete the task of visual mobile manipulation in the ManipulaTHOR environment while avoiding unnecessary collision (disturbance) with objects. We formulate disturbance avoidance as a penalty term in the reward function, but find that directly training with such penalized rewards often results in agents being unable to escape poor local optima. Instead, we propose a two-stage training curriculum where an agent is first allowed to freely explore and build basic competencies without penalization, after which a disturbance penalty is introduced to refine the agent’s behavior. Results on testing scenes show that our curriculum not only avoids these poor local optima, but also leads to 10% absolute gains in success rate without disturbance, compared to our state-of-the-art baselines. Moreover, our curriculum is significantly more performant than a safe RL algorithm that casts collision avoidance as a constraint. Finally, we propose a novel disturbance-prediction auxiliary task that accelerates learning.
<sup>1</sup>"
Towards Equivariant Optical Flow Estimation With Deep Learning,"Stefano Savian, Pietro Morerio, Alessio Del Bue, Andrea A. Janes, Tammam Tillo","Free University of Bozen-Bolzano, Bolzano, Italy; Indraprastha Institute of Information Technology Delhi (IIITD), Delhi, India; Pattern Analysis & Computer Vision (PA VIS), Istituto Italiano di Tecnologia, Genova, Italy",100,"India, Italy",0,,"Methods for Optical Flow (OF) estimation based on Deep Learning have considerably improved traditional approaches in challenging and realistic conditions. However, data-driven approaches can inherently be biased, leading to unexpected under-performance in real application scenarios. In this paper, we first observe that the OF estimation accuracy varies with motion direction, and name this phenomenon 'OF sign imbalance'. The sign imbalance cannot be assessed by means of the endpoint-error (EPE), the typical training and evaluation metric for Deep Optical Flow estimators. This paper tackles this issue by proposing a new metric to assess the sign imbalance, which is compared to the endpoint-error. We provide an extensive evaluation of the sign imbalance for the state-of-the-art optical flow estimators. Based on the evaluation, we propose two strategies to mitigate the phenomenon, i) by constraining the model estimations during inference, and, ii) by constraining the loss function during training. Testing and training code is available at: www.github.com/stsavian/equivariant_of_estimation.",https://openaccess.thecvf.com/content/WACV2023/html/Savian_Towards_Equivariant_Optical_Flow_Estimation_With_Deep_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Savian_Towards_Equivariant_Optical_Flow_Estimation_With_Deep_Learning_WACV_2023_paper.pdf,,www.github.com/stsavian/equivariant_of_estimation,,main,Poster,https://ieeexplore.ieee.org/document/10030957/,"['Training', 'Measurement', 'Deep learning', 'Optical losses', 'Computer vision', 'Codes', 'Computational modeling']","['Deep Learning', 'Equivalency', 'Optical Flow', 'Flow Estimation', 'Optical Flow Estimation', 'Imbalance', 'Loss Function', 'Direction Of Motion', 'Considerable Improvement', 'Training Data', 'Fine-tuned', 'Convolutional Neural Network', 'Average Error', 'Data Augmentation', 'Inference Time', 'Arithmetic Average', 'Training Schedule', 'Iterative Refinement', 'Forward Propagation', 'Simultaneous Localization And Mapping', 'Cost Volume', 'Dark Pixels']","['Algorithms: Low-level and physics-based vision', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",,"Methods for Optical Flow (OF) estimation based on Deep Learning have considerably improved traditional approaches in challenging and realistic conditions. However, data-driven approaches can inherently be biased, leading to unexpected under-performance in real application scenarios. In this paper, we first observe that the OF estimation accuracy varies with motion direction, and name this phenomenon ‘OF sign imbalance’. The sign imbalance cannot be assessed by means of the endpoint-error (EPE), the typical training and evaluation metric for Deep Optical Flow estimators. This paper tackles this issue by proposing a new metric to assess the sign imbalance, which is compared to the endpoint-error. We provide an extensive evaluation of the sign imbalance for the state-of-the-art optical flow estimators. Based on the evaluation, we propose two strategies to mitigate the phenomenon, i) by constraining the model estimations during inference, and, ii) by constraining the loss function during training. Testing and training code is available at: www.github.com/stsavian/equivariant_of_estimation."
Towards Few-Annotation Learning for Object Detection: Are Transformer-Based Models More Efficient?,"Quentin Bouniot, Angélique Loesch, Romaric Audigier, Amaury Habrard","Universit´e Paris-Saclay, CEA, LIST, F-91120, Palaiseau, France; Universit´e de Lyon, UJM-Saint-Etienne, CNRS, IOGS, Laboratoire Hubert Curien UMR 5516, F-42023, Saint-Etienne, France",100,France,0,,"For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilities to adapt similar object detection methods in this setup as well.",https://openaccess.thecvf.com/content/WACV2023/html/Bouniot_Towards_Few-Annotation_Learning_for_Object_Detection_Are_Transformer-Based_Models_More_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bouniot_Towards_Few-Annotation_Learning_for_Object_Detection_Are_Transformer-Based_Models_More_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10031004/,"['Deformable models', 'Adaptation models', 'Education', 'Object detection', 'Detectors', 'Predictive models', 'Transformers']","['Object Detection', 'Teacher Model', 'Labeled Data', 'Object Detection Methods', 'PASCAL VOC', 'Training Set', 'Transformer', 'Data Augmentation', 'Class Prediction', 'Bounding Box', 'Methods In The Literature', 'Unlabeled Data', 'Semi-supervised Learning', 'Self-supervised Learning', 'Student Model', 'Geometric Transformation', 'Region Proposal Network', 'Non-maximum Suppression', 'Few-shot Learning', 'Unlabeled Images', 'Matching Cost', 'Bipartite Matching', 'Unlabeled Set', 'Computer Vision', 'Post-processing Step', 'Rate Parameters', 'Confidence Threshold', 'Semi-supervised Learning Approach']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",3,"For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilities to adapt similar object detection methods in this setup as well."
Towards Generating Ultra-High Resolution Talking-Face Videos With Lip Synchronization,"Anchit Gupta, Rudrabha Mukhopadhyay, Sindhu Balachandra, Faizan Farooq Khan, Vinay P. Namboodiri, C. V. Jawahar",IIIT-Hyderabad; University of Bath,100,"India, UK",0,,"Talking-face video generation works have achieved state-of-the-art results in synthesizing videos with lip synchronization. However, most of the previous works deal with low-resolution talking-face videos (up to 256x256 pixels), thus, generating extremely high-resolution videos still remains a challenge. We take a giant leap in this work and propose a novel method to synthesize talking-face videos at resolutions as high as 4K! Our task presents several key challenges: (i) Scaling the existing methods to such high resolutions is resource-constrained, both in terms of compute and the availability of very high-resolution datasets, (ii) The synthesized videos need to be spatially and temporally coherent. The sheer number of pixels that the model needs to generate while maintaining the temporal consistency at the video level makes this task non-trivial and has never been attempted before in literature. To address these issues, we propose to train the lip-sync generator in a compact Vector Quantized (VQ) space for the first time. Our core idea to encode the faces in a compact 16x16 representation allows us to model high-resolution videos. In our framework, we learn the lip movements in the quantized space on the newly collected 4K Talking Faces (4KTF) dataset. Our approach is speaker agnostic and can handle various languages and voices. We benchmark our technique against several competitive works and show that we can achieve a remarkable 64-times more pixels than the current state-of-the-art! Our supplementary demo video depicts additional qualitative results, comparisons, and several real-world applications, like professional movie editing enabled by our model.",https://openaccess.thecvf.com/content/WACV2023/html/Gupta_Towards_Generating_Ultra-High_Resolution_Talking-Face_Videos_With_Lip_Synchronization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gupta_Towards_Generating_Ultra-High_Resolution_Talking-Face_Videos_With_Lip_Synchronization_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030232/,"['Industries', 'Computer vision', 'Shape', 'Lips', 'Motion pictures', 'Generators', 'Synchronization']","['Ultra-high Resolution', 'Supplementary Video', 'Temporal Coherence', 'Vector Quantization', 'Lip Movements', 'Time Step', 'Stage 2', 'Quantitative Evaluation', 'Perception Of Quality', 'Latent Space', 'Video Content', 'Face Detection', 'Random Pairs', 'Speech Segments', 'Accurate Shape', 'Target Speech', 'Head Pose', 'Fréchet Inception Distance', 'Speech Coding', 'Lip Region', '4K Resolution', 'Multimedia Applications', 'Movie Scenes', 'YouTube', 'Half Of The Face', 'High-quality Results', 'Facial Features', 'Webinars', 'Head Motion']","['Algorithms: Vision + language and/or other modalities', 'Commercial/retail', 'Education']",3,"Talking-face video generation works have achieved state-of-the-art results in synthesizing videos with lip synchronization. However, most of the previous works deal with low-resolution talking-face videos (up to 256×256 pixels), thus, generating extremely high-resolution videos still remains a challenge. We take a giant leap in this work and propose a novel method to synthesize talking-face videos at resolutions as high as 4K! Our task presents several key challenges: (i) Scaling the existing methods to such high resolutions is resource-constrained, both in terms of compute and the availability of very high-resolution datasets, (ii) The synthesized videos need to be spatially and temporally coherent. The sheer number of pixels that the model needs to generate while maintaining the temporal consistency at the video level makes this task non-trivial and has never been attempted before in literature. To address these issues, we propose to train the lip-sync generator in a compact Vector Quantized (VQ) space for the first time. Our core idea to encode the faces in a compact 16× 16 representation allows us to model high-resolution videos. In our framework, we learn the lip movements in the quantized space on the newly collected 4K Talking Faces (4KTF) dataset. Our approach is speaker agnostic and can handle various languages and voices. We benchmark our technique against several competitive works and show that we can achieve a remarkable 64-times more pixels than the current state-of-the-art! Our supplementary demo video depicts additional qualitative results, comparisons, and several real-world applications, like professional movie editing enabled by our model."
Towards Interpretable Video Anomaly Detection,"Keval Doshi, Yasin Yilmaz",University of South Florida,100,USA,0,,"Most video anomaly detection approaches are based on data-intensive end-to-end trained neural networks, which extract spatiotemporal features from videos. The extracted feature representations in such approaches are not interpretable, which prevents the automatic identification of anomaly cause. To this end, we propose a novel framework which can explain the detected anomalous event in a surveillance video. In addition to monitoring objects independently, we also monitor the interactions between them to detect anomalous events and explain their root causes. Specifically, we demonstrate that the scene graphs obtained by monitoring the object interactions provide an interpretation for the context of the anomaly while performing competitively with respect to the recent state-of-the-art approaches. Moreover, the proposed interpretable method enables cross-domain adaptability (i.e., transfer learning in another surveillance scene), which is not feasible for most existing end-to-end methods due to the lack of sufficient labeled training data for every surveillance scene. The quick and reliable detection performance of the proposed method is evaluated both theoretically (through an asymptotic optimality proof) and empirically on the popular benchmark datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Doshi_Towards_Interpretable_Video_Anomaly_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Doshi_Towards_Interpretable_Video_Anomaly_Detection_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030193/,"['Surveillance', 'Transfer learning', 'Pipelines', 'Training data', 'Detectors', 'Benchmark testing', 'Reliability theory']","['Anomaly Detection', 'Video Anomaly', 'Video Anomaly Detection', 'Neural Network', 'Training Data', 'Transfer Learning', 'Benchmark Datasets', 'Neural Network Training', 'Video Surveillance', 'Object Interaction', 'Objective Monitoring', 'Anomalous Events', 'Extract Representative Features', 'Scene Graph', 'Asymptotic Optimality', 'Convolutional Neural Network', 'False Alarm', 'Multilayer Perceptron', 'Bounding Box', 'Generative Adversarial Networks', 'Appearance Features', 'Positive Instances', 'Metric Learning', 'Global Monitoring', 'Semantic Embedding', 'Gated Recurrent Unit', 'Embedding Learning', 'Viewpoint Variations', 'Objects In The Scene', 'Contrastive Loss']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",14,"Most video anomaly detection approaches are based on data-intensive end-to-end trained neural networks, which extract spatiotemporal features from videos. The extracted feature representations in such approaches are not interpretable, which prevents the automatic identification of anomaly cause. To this end, we propose a novel framework which can explain the detected anomalous event in a surveillance video. In addition to monitoring objects independently, we also monitor the interactions between them to detect anomalous events and explain their root causes. Specifically, we demonstrate that the scene graphs obtained by monitoring the object interactions provide an interpretation for the context of the anomaly while performing competitively with respect to the recent state-of-the-art approaches. Moreover, the proposed interpretable method enables cross-domain adaptability (i.e., transfer learning in another surveillance scene), which is not feasible for most existing end-to-end methods due to the lack of sufficient labeled training data for every surveillance scene. The quick and reliable detection performance of the proposed method is evaluated both theoretically (through an asymptotic optimality proof) and empirically on the popular benchmark datasets."
Towards MOOCs for Lipreading: Using Synthetic Talking Heads To Train Humans in Lipreading at Scale,"Aditya Agarwal, Bipasha Sen, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C. V. Jawahar",IIIT Hyderabad; IIIT Hydearbad; University of Bath,100,"India, UK",0,,"Many people with some form of hearing loss consider lipreading as their primary mode of day-to-day communication. However, finding resources to learn or improve one's lipreading skills can be challenging. This is further exacerbated in the COVID19 pandemic due to restrictions on direct interactions with peers and speech therapists. Today, online MOOCs platforms like Coursera and Udemy have become the most effective form of training for many types of skill development. However, online lipreading resources are scarce as creating such resources is an extensive process needing months of manual effort to record hired actors. Because of the manual pipeline, such platforms are also limited in vocabulary, supported languages, accents, and speakers and have a high usage cost. In this work, we investigate the possibility of replacing real human talking videos with synthetically generated videos. Synthetic data can easily incorporate larger vocabularies, variations in accent, and even local languages and many speakers. We propose an end-to-end automated pipeline to develop such a platform using state-of-the-art talking head video generator networks, text-to-speech models, and computer vision techniques. We then perform an extensive human evaluation using carefully thought out lipreading exercises to validate the quality of our designed platform against the existing lipreading platforms. Our studies concretely point toward the potential of our approach in developing a large-scale lipreading MOOC platform that can impact millions of people with hearing loss.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Towards_MOOCs_for_Lipreading_Using_Synthetic_Talking_Heads_To_Train_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Towards_MOOCs_for_Lipreading_Using_Synthetic_Talking_Heads_To_Train_WACV_2023_paper.pdf,,,2208.09796,main,Poster,https://ieeexplore.ieee.org/document/10030770/,"['Training', 'Vocabulary', 'Computer vision', 'Statistical analysis', 'Veins', 'Pipelines', 'Auditory system']","['Massive Open Online Courses', 'Talking Head', 'Vocabulary', 'Hearing Loss', 'Local Language', 'Millions Of People', 'Speech Therapy', 'Manual Effort', 'Differences In Performance', 'Formal Education', 'Diverse Backgrounds', 'Multiple-choice', 'Statistically Insignificant', 'User Study', 'Disambiguation', 'Online Courses', 'Words In Sentences', 'Online Sources', 'Hearing Disability', 'Sentence Context', 'Speech Utterances', 'Lip Movements', 'Real Videos', 'Mean Opinion Score', 'Mouth Movements', 'Highest Density Interval', 'Computer Vision Community', 'Pose Changes', 'Original Video', 'Lip-sync']","['Applications: Social good', 'Education']",1,"Many people with some form of hearing loss consider lipreading as their primary mode of day-to-day communication. However, finding resources to learn or improve one’s lipreading skills can be challenging. This is further exacerbated in the COVID19 pandemic due to restrictions on direct interactions with peers and speech therapists. Today, online MOOCs platforms like Coursera and Udemy have become the most effective form of training for many types of skill development. However, online lipreading resources are scarce as creating such resources is an extensive process needing months of manual effort to record hired ac-tors. Because of the manual pipeline, such platforms are also limited in vocabulary, supported languages, accents, and speakers and have a high usage cost. In this work, we investigate the possibility of replacing real human talking videos with synthetically generated videos. Synthetic data can easily incorporate larger vocabularies, variations in accent, and even local languages and many speakers. We propose an end-to-end automated pipeline to develop such a platform using state-of-the-art talking head video generator networks, text-to-speech models, and computer vision techniques. We then perform an extensive human evaluation using carefully thought out lipreading exercises to validate the quality of our designed platform against the existing lipreading platforms. Our studies concretely point toward the potential of our approach in developing a large-scale lipreading MOOC platform that can impact millions of people with hearing loss."
Towards Online Domain Adaptive Object Detection,"Vibashan VS, Poojan Oza, Vishal M. Patel","Johns Hopkins University, Baltimore, MD, USA",100,USA,0,,"Existing object detection models assume both the training and test data are sampled from the same source domain. This assumption does not hold true when these detectors are deployed in real-world applications, where they encounter new visual domains. Unsupervised Domain Adaptation (UDA) methods are generally employed to mitigate the adverse effects caused by domain shift. Existing UDA methods operate in an offline manner where the model is first adapted toward the target domain and then deployed in real-world applications. However, this offline adaptation strategy is not suitable for real-world applications as the model frequently encounters new domain shifts. Hence, it is critical to develop a feasible UDA method that generalizes to the new domain shifts encountered during deployment time in a continuous online manner. To this end, we propose a novel unified adaptation framework that adapts and improves generalization on the target domain in both offline and online settings. Specifically, we introduce MemXformer - a cross-attention transformer-based memory module where items in the memory take advantage of domain shifts and record prototypical patterns of the target distribution. Further, MemXformer produces strong positive and negative pairs to guide a novel contrastive loss, which enhances target-specific representation learning. Experiments on diverse detection benchmarks show that the proposed strategy producs state-of-the-art performance in both offline and online settings. To the best of our knowledge, this is the first work to address online and offline adaptation settings for object detection. Source code will be released after review.",https://openaccess.thecvf.com/content/WACV2023/html/VS_Towards_Online_Domain_Adaptive_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/VS_Towards_Online_Domain_Adaptive_Object_Detection_WACV_2023_paper.pdf,,https://github.com/Vibashan/memXformer-online-da,2204.05289,main,Poster,https://ieeexplore.ieee.org/document/10030126/,"['Training', 'Representation learning', 'Adaptation models', 'Visualization', 'Source coding', 'Object detection', 'Detectors']","['Object Detection', 'Domain Adaptation', 'Domain Adaptive Object Detection', 'Training Data', 'Test Data', 'Adaptive Method', 'Real-world Applications', 'Detection Model', 'Domain Shift', 'Representation Learning', 'Target Domain', 'Target Distribution', 'Source Domain', 'Online Setting', 'Contrastive Loss', 'Memory Items', 'Object Detection Model', 'Domain Adaptation Methods', 'Online Manner', 'Unsupervised Domain Adaptation Methods', 'Self-supervised Learning', 'Unlabeled Data', 'Teacher Network', 'Large Batch Size', 'Data Streams', 'Training Images', 'Unlabeled Target Data', 'Student Network', 'Real-world Deployment', 'Robust Feature Representation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",17,"Existing object detection models assume both the training and test data are sampled from the same source do-main. This assumption does not hold true when these detectors are deployed in real-world applications, where they en-counter new visual domains. Unsupervised Domain Adaptation (UDA) methods are generally employed to mitigate the adverse effects caused by domain shift. Existing UDA methods operate in an offline manner where the model is first adapted toward the target domain and then deployed in real-world applications. However, this offline adaptation strategy is not suitable for real-world applications as the model frequently encounters new domain shifts. Hence, it is critical to develop a feasible UDA method that generalizes to the new domain shifts encountered during deployment time in a continuous online manner. To this end, we propose a novel unified adaptation framework that adapts and improves generalization on the target domain in both offline and online settings. Specifically, we introduce MemXformer - a cross-attention transformer-based memory module where items in the memory take advantage of domain shifts and record prototypical patterns of the target distribution. Further, MemXformer produces strong positive and negative pairs to guide a novel contrastive loss, which enhances target-specific representation learning. Experiments on diverse detection benchmarks show that the proposed strategy producs state-of-the-art performance in both offline and online settings. To the best of our knowledge, this is the first work to address online and offline adaptation settings for object detection. Source code: https://github.com/Vibashan/memXformer-online-da"
Towards a Framework for Privacy-Preserving Pedestrian Analysis,"Anil Kunchala, Mélanie Bouroche, Bianca Schoen-Phelan",Trinity College Dublin; Technological University Dublin,100,Ireland,0,,"The design of pedestrian-friendly infrastructures plays a crucial role in creating sustainable transportation in urban environments. Analyzing pedestrian behaviour in response to existing infrastructure is pivotal to planning, maintaining, and creating more pedestrian-friendly facilities. Many approaches have been proposed to extract such behaviour by applying deep learning models to video data. Video data, however, includes an broad spectrum of privacy-sensitive information about individuals, such as their location at a given time or who they are with. Most of the existing models use privacy-invasive methodologies to track, detect, and analyse individual or group pedestrian behaviour patterns. As a step towards privacy-preserving pedestrian analysis, this paper introduces a framework to anonymize all pedestrians before analyzing their behaviors. The proposed framework leverages recent developments in 3D wireframe reconstruction and digital in-painting to represent pedestrians with quantitative wireframes by removing their images while preserving pose, shape, and background scene context. To evaluate the proposed framework, a generic metric is introduced for each of privacy and utility. Experimental evaluation on widely-used datasets shows that the proposed framework outperforms traditional and state-of-the-art image filtering approaches by generating best privacy utility trade-off.",https://openaccess.thecvf.com/content/WACV2023/html/Kunchala_Towards_a_Framework_for_Privacy-Preserving_Pedestrian_Analysis_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kunchala_Towards_a_Framework_for_Privacy-Preserving_Pedestrian_Analysis_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030965/,"['Measurement', 'Deep learning', 'Privacy', 'Three-dimensional displays', 'Shape', 'Urban areas', 'Transportation']","['Sustainable Transport', 'Image Filtering', 'General Metrics', 'Pedestrian Behavior', 'Convolutional Neural Network', 'Personal Information', 'Shape Parameter', 'Subjective Evaluation', 'Body Shape', 'Visual Methods', 'Privacy Protection', '3D Mesh', 'Video Sequences', 'Pose Estimation', 'Shape Estimation', 'Images Of People', 'Triplet Loss', 'Foreground Objects', 'Translation Parameters', 'Static Information', 'Keypoint Detection', 'Pedestrian Detection', 'Background Objects', 'Privacy Level', 'Majority Of Research', 'Similarity Index', 'Bounding Box', 'Axis Angle', 'Privacy Preservation', 'Dynamic Background']",['Applications: Social good'],6,"The design of pedestrian-friendly infrastructures plays a crucial role in creating sustainable transportation in urban environments. Analyzing pedestrian behaviour in response to existing infrastructure is pivotal to planning, maintaining, and creating more pedestrian-friendly facilities. Many approaches have been proposed to extract such behaviour by applying deep learning models to video data. Video data, however, includes an broad spectrum of privacy-sensitive information about individuals, such as their location at a given time or who they are with. Most of the existing models use privacy-invasive methodologies to track, detect, and analyse individual or group pedestrian behaviour patterns. As a step towards privacy-preserving pedestrian analysis, this paper introduces a framework to anonymize all pedestrians before analyzing their behaviors. The proposed framework leverages recent developments in 3D wireframe reconstruction and digital in-painting to represent pedestrians with quantitative wireframes by removing their images while preserving pose, shape, and background scene context. To evaluate the proposed framework, a generic metric is introduced for each of privacy and utility. Experimental evaluation on widely-used datasets shows that the proposed framework outperforms traditional and state-of-the-art image filtering approaches by generating best privacy utility trade-off."
Tracking Growth and Decay of Plant Roots in Minirhizotron Images,"Alexander Gillert, Bo Peters, Uwe Freiherr von Lukas, Jürgen Kreyling, Gesche Blume-Werry","Fraunhofer Institute for Computer Graphics Research IGD, Rostock; Department of Ecology and Environmental Science, Ume ˚a University; Institute of Botany and Landscape Ecology, Greifswald University; Institute for Visual & Analytic Computing, University of Rostock",100,"Germany, Sweden",0,,"Plant roots are difficult to monitor and study since they are hidden belowground. Minirhizotrons offer an in-situ monitoring solution but their widespread adoption is still limited by the capabilities of automatic analysis methods. These capabilities so far consist only of estimating a single number (total root length) per image.\nWe propose a method for a more fine-grained analysis which estimates the root turnover, i.e. the amount of root growth and decay between two minirhizotron images. It consists of a neural network that computes which roots are visible in both images and is trained in an unsupervised manner without additional annotations.\nOur code is available as a part of an analysis tool with a user interface ready to be used by ecologists.",https://openaccess.thecvf.com/content/WACV2023/html/Gillert_Tracking_Growth_and_Decay_of_Plant_Roots_in_Minirhizotron_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Gillert_Tracking_Growth_and_Decay_of_Plant_Roots_in_Minirhizotron_Images_WACV_2023_paper.pdf,,https://github.com/alexander-g/Root-Tracking,,main,Poster,https://ieeexplore.ieee.org/document/10030766/,"['Computer vision', 'Codes', 'Neural networks', 'User interfaces', 'Monitoring']","['Root Growth', 'Minirhizotron Images', 'Neural Network', 'Root Length', 'Unsupervised Manner', 'Total Root Length', 'Root Turnover', 'Stage 2', 'Image Pairs', 'Training Stage', 'Local Image', 'Negative Views', 'Segmentation Map', 'Fine Roots', 'Self-supervised Learning', 'Mesocosm', 'Output Stage', 'Image X', 'Scale-invariant Feature Transform', 'Root Segments', 'Root Locus', 'Root Length Measurements', 'Semantic Segmentation Network']","['Applications: Environmental monitoring/climate change/ecology', 'Agriculture']",2,"Plant roots are difficult to monitor and study since they are hidden belowground. Minirhizotrons offer an in-situ monitoring solution but their widespread adoption is still limited by the capabilities of automatic analysis methods. These capabilities so far consist only of estimating a single number (total root length) per image.We propose a method for a more fine-grained analysis which estimates the root turnover, i.e. the amount of root growth and decay between two minirhizotron images. It consists of a neural network that computes which roots are visible in both images and is trained in an unsupervised manner without additional annotations.Our code is available as a part of an analysis tool with a user interface ready to be used by ecologists.
<sup>1</sup>"
Training Auxiliary Prototypical Classifiers for Explainable Anomaly Detection in Medical Image Segmentation,"Wonwoo Cho, Jeonghoon Park, Jaegul Choo","KAIST, Daejeon, Republic of Korea; KAIST, Daejeon, Republic of Korea; Letsur Inc., Seoul, Republic of Korea",66.66666667,South Korea,33.33333333,South Korea,"Machine learning-based algorithms using fully convolutional networks (FCNs) have been a promising option for medical image segmentation. However, such deep networks silently fail if input samples are drawn far from the training data distribution, thus causing critical problems in automatic data processing pipelines. To overcome such out-of-distribution (OoD) problems, we propose a novel OoD score formulation and its regularization strategy by applying an auxiliary add-on classifier to an intermediate layer of an FCN, where the auxiliary module is helfpul for analyzing the encoder output features by taking their class information into account. Our regularization strategy train the module along with the FCN via the principle of outlier exposure so that our model can be trained to distinguish OoD samples from normal ones without modifying the original network architecture. Our extensive experiment results demonstrate that the proposed approach can successfully conduct effective OoD detection without loss of segmentation performance. In addition, our module can provide reasonable explanation maps along with OoD scores, which can enable users to analyze the reliability of predictions.",https://openaccess.thecvf.com/content/WACV2023/html/Cho_Training_Auxiliary_Prototypical_Classifiers_for_Explainable_Anomaly_Detection_in_Medical_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cho_Training_Auxiliary_Prototypical_Classifiers_for_Explainable_Anomaly_Detection_in_Medical_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030758/,"['Training', 'Image segmentation', 'Machine learning algorithms', 'Pipelines', 'Training data', 'Network architecture', 'Data processing']","['Medical Imaging', 'Anomaly Detection', 'Medical Image Segmentation', 'Training Data', 'Output Feature', 'Class Information', 'Segmentation Performance', 'Fully Convolutional Network', 'Distribution Of Training Data', 'Automatic Pipeline', 'Magnetic Resonance Imaging', 'Training Set', 'Deep Neural Network', '3D Images', 'Real-world Applications', 'Latent Space', 'Segmentation Results', 'Mahalanobis Distance', 'Segmentation Task', 'Successful Detection', 'Score Map', 'Inclusion Probability', 'Image X', 'Latent Features', 'Foreground Pixels', 'Perspective Of Model', 'Segmentation Labels', 'Auxiliary Classifier', 'Intermediate Features', 'Class Prototypes']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"Machine learning-based algorithms using fully convolutional networks (FCNs) have been a promising option for medical image segmentation. However, such deep networks silently fail if input samples are drawn far from the training data distribution, thus causing critical problems in automatic data processing pipelines. To overcome such out-of-distribution (OoD) problems, we propose a novel OoD score formulation and its regularization strategy by applying an auxiliary add-on classifier to an intermediate layer of an FCN, where the auxiliary module is helfpul for analyzing the encoder output features by taking their class information into account. Our regularization strategy train the module along with the FCN via the principle of outlier exposure so that our model can be trained to distinguish OoD samples from normal ones without modifying the original network architecture. Our extensive experiment results demonstrate that the proposed approach can successfully conduct effective OoD detection without loss of segmentation performance. In addition, our module can provide reasonable explanation maps along with OoD scores, which can enable users to analyze the reliability of predictions."
Trans4Map: Revisiting Holistic Bird's-Eye-View Mapping From Egocentric Images to Allocentric Semantics With Vision Transformers,"Chang Chen, Jiaming Zhang, Kailun Yang, Kunyu Peng, Rainer Stiefelhagen","CV:HCI Lab, Karlsruhe Institute of Technology",100,Germany,0,,"Humans have an innate ability to sense their surroundings, as they can extract the spatial representation from the egocentric perception and form an allocentric semantic map via spatial transformation and memory updating. However, endowing mobile agents with such a spatial sensing ability is still a challenge, due to two difficulties: (1) the previous convolutional models are limited by the local receptive field, thus, struggling to capture holistic long-range dependencies during observation; (2) the excessive computational budgets required for success, often lead to a separation of the mapping pipeline into stages, resulting the entire mapping process inefficient. To address these issues, we propose an end-to-end one-stage Transformer-based framework for Mapping, termed Trans4Map. Our egocentric-to-allocentric mapping process includes three steps: (1) the efficient transformer extracts the contextual features from a batch of egocentric images; (2) the proposed Bidirectional Allocentric Memory (BAM) module projects egocentric features into the allocentric memory; (3) the map decoder parses the accumulated memory and predicts the top-down semantic segmentation map. In contrast, Trans4Map achieves state-of-the-art results, reducing 67.2% parameters, yet gaining a +3.25% mIoU and a +4.09% mBF1 improvements on the Matterport3D dataset.",https://openaccess.thecvf.com/content/WACV2023/html/Chen_Trans4Map_Revisiting_Holistic_Birds-Eye-View_Mapping_From_Egocentric_Images_to_Allocentric_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Trans4Map_Revisiting_Holistic_Birds-Eye-View_Mapping_From_Egocentric_Images_to_Allocentric_WACV_2023_paper.pdf,,https://github.com/jamycheung/Trans4Map,,main,Poster,,,,,,
TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking,"Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, Zicheng Liu",Microsoft; Stony Brook University,50,USA,50,USA,"Tracking multiple objects in videos relies on modeling the spatial-temporal interactions of the objects. In this paper, we propose TransMOT, which leverages powerful graph transformers to efficiently model the spatial and temporal interactions among the objects. TransMOT is capable of effectively modeling the interactions of a large number of objects by arranging the trajectories of the tracked targets and detection candidates as a set of sparse weighted graphs, and constructing a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial graph transformer decoder layer based on the graphs. Through end-to-end learning, TransMOT can exploit the spatial-temporal clues to directly estimate association from a large number of loosely filtered detection predictions for robust MOT in complex scenes. The proposed method is evaluated on multiple benchmark datasets, including MOT15, MOT16, MOT17, and MOT20, and it achieves state-of-the-art performance on all the datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Chu_TransMOT_Spatial-Temporal_Graph_Transformer_for_Multiple_Object_Tracking_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Chu_TransMOT_Spatial-Temporal_Graph_Transformer_for_Multiple_Object_Tracking_WACV_2023_paper.pdf,,,2104.00194,main,Poster,https://ieeexplore.ieee.org/document/10030267/,"['Computer vision', 'Target tracking', 'Computational modeling', 'Benchmark testing', 'Transformers', 'Trajectory', 'Decoding']","['Multiple Objects', 'Multiple Object Tracking', 'Graph Transformation', 'Spatial Interaction', 'Spatial Layer', 'Transformer Encoder', 'Video Object', 'Spatial Encoding', 'Temporal Layer', 'Spatial Information', 'Object Detection', 'Visual Features', 'Attention Mechanism', 'Bounding Box', 'Temporal Information', 'Graph Convolutional Network', 'Node Features', 'Current Frame', 'Attention Weights', 'Graph Convolution', 'Virtual Source', 'Tracking Framework', 'Feature Tensor', 'Virtual Nodes', 'Ground-truth Bounding Box', 'Color Histogram', 'Inference Speed', 'Candidate Boxes', 'Online Tracking', 'Spatial Features']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)']",103,"Tracking multiple objects in videos relies on modeling the spatial-temporal interactions of the objects. In this paper, we propose TransMOT, which leverages powerful graph transformers to efficiently model the spatial and temporal interactions among the objects. TransMOT is capable of effectively modeling the interactions of a large number of objects by arranging the trajectories of the tracked targets and detection candidates as a set of sparse weighted graphs, and constructing a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial graph transformer decoder layer based on the graphs. Through end-to-end learning, TransMOT can exploit the spatial-temporal clues to directly estimate association from a large number of loosely filtered detection predictions for robust MOT in complex scenes. The proposed method is evaluated on multiple benchmark datasets, including MOT15, MOT16, MOT17, and MOT20, and it achieves state-of-the-art performance on all the datasets."
TransPillars: Coarse-To-Fine Aggregation for Multi-Frame 3D Object Detection,"Zhipeng Luo, Gongjie Zhang, Changqing Zhou, Tianrui Liu, Shijian Lu, Liang Pan","S-Lab, Nanyang Technological University; Nanyang Technological University",100,Singapore,0,,"3D object detection using point clouds has attracted increasing attention due to its wide applications in autonomous driving and robotics. However, most existing studies focus on single point cloud frames without harnessing the temporal information in point cloud sequences. In this paper, we design TransPillars, a novel transformer-based feature aggregation technique that exploits temporal features of consecutive point cloud frames for multi-frame 3D object detection. TransPillars aggregates spatial-temporal point cloud features from two perspectives. First, it fuses voxel-level features directly from multi-frame feature maps instead of pooled instance features to preserve instance details with contextual information that are essential to accurate object localization. Second, it introduces a hierarchical coarse-to-fine strategy to fuse multi-scale features progressively to effectively capture the motion of moving objects and guide the aggregation of fine features. Besides, a variant of deformable transformer is introduced to improve the effectiveness of cross-frame feature matching. Extensive experiments show that our proposed TransPillars achieves state-of-art performance as compared to existing multi-frame detection approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Luo_TransPillars_Coarse-To-Fine_Aggregation_for_Multi-Frame_3D_Object_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Luo_TransPillars_Coarse-To-Fine_Aggregation_for_Multi-Frame_3D_Object_Detection_WACV_2023_paper.pdf,,,2208.03141,main,Poster,https://ieeexplore.ieee.org/document/10030528/,"['Point cloud compression', 'Location awareness', 'Computer vision', 'Three-dimensional displays', 'Fuses', 'Object detection', 'Benchmark testing']","['Object Detection', '3D Object Detection', 'Contextual Information', 'Feature Maps', 'Point Cloud', 'Multi-scale Features', 'Consecutive Frames', 'Feature Matching', 'Feature Aggregation', 'Frame Features', 'Point Cloud Features', 'Contralateral', 'Sampling Locations', 'Attention Mechanism', 'Entire Sequence', 'Explicit Model', 'Feature Fusion', 'Matching Process', 'Mean Average Precision', 'Current Frame', 'Positional Encoding', 'Query Features', 'Transformer Layers', 'Past Frames', 'Attention Matrix', 'Input Frames', 'Attention Weights', 'Key Frames', 'Loss Of Details', 'Voxel-based Methods']",['Algorithms: 3D computer vision'],8,"3D object detection using point clouds has attracted increasing attention due to its wide applications in autonomous driving and robotics. However, most existing studies focus on single point cloud frames without harnessing the temporal information in point cloud sequences. In this paper, we design TransPillars, a novel transformer-based feature aggregation technique that exploits temporal features of consecutive point cloud frames for multi-frame 3D object detection. TransPillars aggregates spatial-temporal point cloud features from two perspectives. First, it fuses voxel-level features directly from multi-frame feature maps instead of pooled instance features to preserve instance details with contextual information that are essential to accurate object localization. Second, it introduces a hierarchical coarse-to-fine strategy to fuse multi-scale features progressively to effectively capture the motion of moving objects and guide the aggregation of fine features. Besides, a variant of deformable transformer is introduced to improve the effectiveness of cross-frame feature matching. Extensive experiments show that our proposed TransPillars achieves state-of-art performance as compared to existing multi-frame detection approaches."
TransVLAD: Multi-Scale Attention-Based Global Descriptors for Visual Geo-Localization,"Yifan Xu, Pourya Shamsolmoali, Eric Granger, Claire Nicodeme, Laurent Gardes, Jie Yang",ETS Montreal; SNCF Paris; East China Normal University; Shanghai Jiao Tong University,75,"Canada, China",25,France,"Visual geo-localization remains a challenging task due to variations in the appearance and perspective among captured images. This paper introduces an efficient TransVLAD module, which aggregates attention-based feature maps into a discriminative and compact global descriptor. Unlike existing methods that generate feature maps using only convolutional neural networks (CNNs), we propose a sparse transformer to encode global dependencies and compute attention-based feature maps, which effectively reduces visual ambiguities that occurs in large-scale geo-localization problems. A positional embedding mechanism is used to learn the corresponding geometric configurations between query and gallery images. A grouped VLAD layer is also introduced to reduce the number of parameters, and thus construct an efficient module. Finally, rather than only learning from the global descriptors on entire images, we propose a self-supervised learning method to further encode more information from multi-scale patches between the query and positive gallery images. Extensive experiments on three challenging large-scale datasets indicate that our model outperforms state-of-the-art models, and has lower computational complexity.",https://openaccess.thecvf.com/content/WACV2023/html/Xu_TransVLAD_Multi-Scale_Attention-Based_Global_Descriptors_for_Visual_Geo-Localization_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Xu_TransVLAD_Multi-Scale_Attention-Based_Global_Descriptors_for_Visual_Geo-Localization_WACV_2023_paper.pdf,,https://github.com/wacv-23/TVLAD,,main,Poster,https://ieeexplore.ieee.org/document/10030454/,"['Visualization', 'Computer vision', 'Codes', 'Computational modeling', 'Image retrieval', 'Self-supervised learning', 'Transformers']","['Global Descriptors', 'Convolutional Neural Network', 'Feature Maps', 'Positive Image', 'Low Computational Complexity', 'Geometric Configuration', 'Self-supervised Learning', 'Appearance Variations', 'Query Image', 'Gallery Images', 'Self-supervised Learning Methods', 'Generate Feature Maps', 'Feature Representation', 'Pedestrian', 'Dimensional Vector', 'Multilayer Perceptron', 'Stochastic Gradient Descent', 'Representation Of Space', 'Image Retrieval', 'Convolutional Neural Networks Backbone', 'Local Descriptors', 'High-dimensional Feature Vector', 'Image Retrieval Task', 'Vision Transformer', 'Triplet Loss', 'Compact Devices', 'Low-dimensional Vector', 'Perform Ablation Studies', 'Descriptor Vector']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",11,"Visual geo-localization remains a challenging task due to variations in the appearance and perspective among captured images. This paper introduces an efficient TransVLAD module, which aggregates attention-based feature maps into a discriminative and compact global descriptor. Unlike existing methods that generate feature maps using only convolutional neural networks (CNNs), we propose a sparse transformer to encode global dependencies and compute attention-based feature maps, which effectively reduces visual ambiguities that occurs in large-scale geo-localization problems. A positional embedding mechanism is used to learn the corresponding geometric configurations between query and gallery images. A grouped VLAD layer is also introduced to reduce the number of parameters, and thus construct an efficient module. Finally, rather than only learning from the global descriptors on entire images, we propose a self-supervised learning method to further encode more information from multi-scale patches between the query and positive gallery images. Extensive experiments on three challenging large-scale datasets indicate that our model outperforms state-of-the-art models, and has lower computational complexity. The code is available at: https://github.com/wacv-23/TVLAD."
Transformers for Recognition in Overhead Imagery: A Reality Check,"Francesco Luzi, Aneesh Gupta, Leslie Collins, Kyle Bradbury, Jordan Malof",University of Montana; Duke University,100,USA,0,,"There is evidence that transformers offer state-of-the-art recognition performance on tasks involving overhead imagery (e.g., satellite imagery). However, it is difficult to make unbiased empirical comparisons between competing deep learning models, making it unclear whether, and to what extent, transformer-based models are beneficial. In this paper we systematically compare the impact of adding transformer structures into state-of-the-art segmentation models for overhead imagery. Each model is given a similar budget of free parameters, and their hyperparameters are optimized using Bayesian Optimization with a fixed quantity of data and computation time. We conduct our experiments with a large and diverse dataset comprising two large public benchmarks: Inria and DeepGlobe. We perform additional ablation studies to explore the impact of specific transformer-based modeling choices. Our results suggest that transformers provide consistent, but modest, performance improvements. We only observe this advantage however in hybrid models that combine convolutional and transformer-based structures, while fully transformer-based models achieve relatively poor performance.",https://openaccess.thecvf.com/content/WACV2023/html/Luzi_Transformers_for_Recognition_in_Overhead_Imagery_A_Reality_Check_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Luzi_Transformers_for_Recognition_in_Overhead_Imagery_A_Reality_Check_WACV_2023_paper.pdf,,,2210.12599,main,Poster,https://ieeexplore.ieee.org/document/10030966/,"['Training', 'Image segmentation', 'Systematics', 'Image recognition', 'Satellites', 'Computational modeling', 'Transformers']","['Imagery', 'Ablation', 'Segmentation Model', 'Bayesian Optimization', 'Convolutional Structure', 'Learning Rate', 'Validation Set', 'Convolutional Layers', 'Window Size', 'Subset Of Data', 'Object Detection', 'Intersection Over Union', 'Model Architecture', 'Trainable Parameters', 'Layer Model', 'Amount Of Training Data', 'Visual Model', 'Segmentation Performance', 'Differences In Dimensions', 'Hyperparameter Settings', 'Transformer Layers', 'Parameter Count', 'Long-range Dependencies', 'Number Of Heads', 'Trained Model Parameters', 'Hyperparameter Tuning', 'Ground Truth Labels', 'Training Dataset', 'Training Set', 'Gaussian Process']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Remote Sensing']",2,"There is evidence that transformers offer state-of-the-art recognition performance on tasks involving overhead imagery (e.g., satellite imagery). However, it is difficult to make unbiased empirical comparisons between competing deep learning models, making it unclear whether, and to what extent, transformer-based models are beneficial. In this paper we systematically compare the impact of adding transformer structures into state-of-the-art segmentation models for overhead imagery. Each model is given a similar budget of free parameters, and their hyperparameters are optimized using Bayesian Optimization with a fixed quantity of data and computation time. We conduct our experiments with a large and diverse dataset comprising two large public benchmarks: Inria and DeepGlobe. We perform additional ablation studies to explore the impact of specific transformer-based modeling choices. Our results suggest that transformers provide consistent, but modest, performance improvements. We only observe this advantage however in hybrid models that combine convolutional and transformer-based structures, while fully transformer-based models achieve relatively poor performance."
Treating Motion as Option To Reduce Motion Dependency in Unsupervised Video Object Segmentation,"Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Chaewon Park, Donghyeong Kim, Sangyoun Lee","Yonsei University, Korea Institute of Science and Technology (KIST); Yonsei University",100,South Korea,0,,"Unsupervised video object segmentation (VOS) aims to detect the most salient object in a video sequence at the pixel level. In unsupervised VOS, most state-of-the-art methods leverage motion cues obtained from optical flow maps in addition to appearance cues to exploit the property that salient objects usually have distinctive movements compared to the background. However, as they are overly dependent on motion cues, which may be unreliable in some cases, they cannot achieve stable prediction. To reduce this motion dependency of existing two-stream VOS methods, we propose a novel motion-as-option network that optionally utilizes motion cues. Additionally, to fully exploit the property of the proposed network that motion is not always required, we introduce a collaborative network learning strategy. On all the public benchmark datasets, our proposed network affords state-of-the-art performance with real-time inference speed.",https://openaccess.thecvf.com/content/WACV2023/html/Cho_Treating_Motion_as_Option_To_Reduce_Motion_Dependency_in_Unsupervised_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Cho_Treating_Motion_as_Option_To_Reduce_Motion_Dependency_in_Unsupervised_WACV_2023_paper.pdf,,https://github.com/suhwan-cho/TMO,2209.03138,main,Poster,https://ieeexplore.ieee.org/document/10030769/,"['Federated learning', 'Video sequences', 'Object segmentation', 'Benchmark testing', 'Streaming media', 'Solids', 'Real-time systems']","['Video Object Segmentation', 'Unsupervised Video Object Segmentation', 'Learning Strategies', 'Optical Flow', 'Video Sequences', 'Pixel Level', 'Collaborative Strategies', 'Inference Speed', 'Flow Map', 'Salient Object', 'Motion Cues', 'Public Benchmark Datasets', 'Training Set', 'Training Data', 'Training Dataset', 'Quantitative Evaluation', 'Network Training', 'Video Frames', 'RGB Images', 'Training Protocol', 'Salient Object Detection', 'Appearance Features', 'Motion Features', 'Input Encoding', 'Decoder Block', 'Motion Information', 'Target Frame', 'Temporal Coherence', 'Appearance Information', 'Encoder Block']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Embedded sensing/real-time techniques']",22,"Unsupervised video object segmentation (VOS) aims to detect the most salient object in a video sequence at the pixel level. In unsupervised VOS, most state-of-the-art methods leverage motion cues obtained from optical flow maps in addition to appearance cues to exploit the property that salient objects usually have distinctive movements compared to the background. However, as they are overly dependent on motion cues, which may be unreliable in some cases, they cannot achieve stable prediction. To reduce this motion dependency of existing two-stream VOS methods, we propose a novel motion-as-option network that optionally utilizes motion cues. Additionally, to fully exploit the property of the proposed network that motion is not always required, we introduce a collaborative network learning strategy. On all the public benchmark datasets, our proposed network affords state-of-the-art performance with real-time inference speed. Code and models are available at https://github.com/suhwan-cho/TMO."
Treatment Learning Causal Transformer for Noisy Image Classification,"Chao-Han Huck Yang, I-Te Hung, Yi-Chieh Liu, Pin-Yu Chen",IBM Research AI; Georgia Institute of Technology; Columbia University,66.66666667,USA,33.33333333,USA,"Current top-notch deep learning (DL) based vision models are primarily based on exploring and exploiting the inherent correlations between training data samples and their associated labels. However, a known practical challenge is their degraded performance against ""noisy"" data, induced by different circumstances such as spurious correlations, irrelevant contexts, domain shift, and adversarial attacks. In this work, we incorporate this binary information of ""existence of noise"" as treatment into image classification tasks to improve prediction accuracy by jointly estimating their treatment effects. Motivated from causal variational inference, we propose a transformer-based architecture, Treatment Learning Causal Transformer (TLT), that uses a latent generative model to estimate robust feature representations from current observational input for noise image classification. Depending on the estimated noise level (modeled as a binary treatment factor), TLT assigns the corresponding inference network trained by the designed causal loss for prediction. We also create new noisy image datasets incorporating a wide range of noise factors (e.g., object masking, style transfer, and adversarial perturbation) for performance benchmarking. The superior performance of TLT in noisy image classification is further validated by several refutation evaluation metrics. As a by-product, TLT also improves visual salience methods for perceiving noisy images.",https://openaccess.thecvf.com/content/WACV2023/html/Yang_Treatment_Learning_Causal_Transformer_for_Noisy_Image_Classification_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yang_Treatment_Learning_Causal_Transformer_for_Noisy_Image_Classification_WACV_2023_paper.pdf,,https://github.com/huckiyang/treatment-causal-transformer,,main,Poster,https://ieeexplore.ieee.org/document/10030146/,"['Deep learning', 'Visualization', 'Correlation', 'Perturbation methods', 'Training data', 'Benchmark testing', 'Transformers']","['Transformer', 'Image Classification', 'Noisy Images', 'Benchmark', 'Training Data', 'Causal Inference', 'Network Inference', 'Variational Inference', 'Adversarial Attacks', 'Style Transfer', 'Binary Treatment', 'Adversarial Perturbations', 'Existence Of Noise', 'Noise Level Estimation', 'Causal Effect', 'Deep Neural Network', 'Additive Noise', 'Attention Mechanism', 'Proxy Measure', 'Latent Space', 'Causal Model', 'Visual Patterns', 'Adversarial Examples', 'Visual Learning', 'Noisy Input', 'Average Treatment Effect', 'Artificial Noise', 'Visual Recognition', 'Training Objective', 'Fast Gradient Sign Method']","['Applications: Psychology and cognitive science', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"Current top-notch deep learning (DL) based vision models are primarily based on exploring and exploiting the inherent correlations between training data samples and their associated labels. However, a known practical challenge is their degraded performance against ""noisy"" data, induced by different circumstances such as spurious correlations, irrelevant contexts, domain shift, and adversarial attacks. In this work, we incorporate this binary information of ""existence of noise"" as treatment into image classification tasks to improve prediction accuracy by jointly estimating their treatment effects. Motivated from causal variational inference, we propose a transformer-based architecture, Treatment Learning Causal Transformer (TLT), that uses a latent generative model to estimate robust feature representations from current observational input for noise image classification. Depending on the estimated noise level (modeled as a binary treatment factor), TLT assigns the corresponding inference network trained by the designed causal loss for prediction. We also create new noisy image datasets incorporating a wide range of noise factors (e.g., object masking, style transfer, and adversarial perturbation) for performance benchmarking. The superior performance of TLT in noisy image classification is further validated by several refutation evaluation metrics. As a by-product, TLT also improves visual salience methods for perceiving noisy images."
Two-Level Data Augmentation for Calibrated Multi-View Detection,"Martin Engilberge, Haixin Shi, Zhiye Wang, Pascal Fua","EPFL, Lausanne, Switzerland",100,Switzerland,0,,"Data augmentation has proven its usefulness to improve model generalization and performance. While it is commonly applied in computer vision application when it comes to multi-view systems, it is rarely used. Indeed geometric data augmentation can break the alignment among views. This is problematic since multi-view data tend to be scarce and it is expensive to annotate. In this work we propose to solve this issue by introducing a new multi-view data augmentation pipeline that preserves alignment among views. Additionally to traditional augmentation of the input image we also propose a second level of augmentation applied directly at the scene level. When combined with our simple multi-view detection model, our two-level augmentation pipeline outperforms all existing baselines by a significant margin on the two main multi-view multi-person detection datasets WILDTRACK and MultiviewX.",https://openaccess.thecvf.com/content/WACV2023/html/Engilberge_Two-Level_Data_Augmentation_for_Calibrated_Multi-View_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Engilberge_Two-Level_Data_Augmentation_for_Calibrated_Multi-View_Detection_WACV_2023_paper.pdf,,https://github.com/cvlab-epfl/MVAug,2210.10756,main,Poster,https://ieeexplore.ieee.org/document/10030512/,"['Computer vision', 'Computational modeling', 'Pipelines', 'Data models']","['Data Augmentation', 'Multi-view Detection', 'Input Image', 'Multi-view Data', 'Overfitting', 'Transformer', 'Training Dataset', 'Convolutional Layers', 'Part Of Network', 'Bounding Box', 'Batch Normalization', 'Ground Plane', 'Projection Matrix', 'Affine Transformation', 'Bilinear Interpolation', 'Conditional Random Field', 'Final Detection', 'Geometric Transformation', 'Non-maximum Suppression', 'Homography', 'Type Of Augmentation', 'Detection In Scenes', 'Perspective Transformation', 'World Coordinate System', 'Deep Learning', 'Detection Function', 'Detection Task', 'Top View']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",7,"Data augmentation has proven its usefulness to improve model generalization and performance. While it is commonly applied in computer vision application when it comes to multi-view systems, it is rarely used. Indeed geometric data augmentation can break the alignment among views. This is problematic since multi-view data tend to be scarce and it is expensive to annotate.In this work we propose to solve this issue by introducing a new multi-view data augmentation pipeline that preserves alignment among views. Additionally to traditional augmentation of the input image we also propose a second level of augmentation applied directly at the scene level. When combined with our simple multi-view detection model, our two-level augmentation pipeline outperforms all existing baselines by a significant margin on the two main multi-view multi-person detection datasets WILD-TRACK and MultiviewX."
UPAR: Unified Pedestrian Attribute Recognition and Person Retrieval,"Andreas Specker, Mickael Cormier, Jürgen Beyerer","Karlsruhe Institute of Technology, Fraunhofer IOSB, Fraunhofer Center for Machine Learning; Fraunhofer IOSB, Karlsruhe Institute of Technology, Fraunhofer Center for Machine Learning",100,Germany,0,,"Recognizing soft-biometric pedestrian attributes is essential in video surveillance and fashion retrieval. Recent works show promising results on single datasets. Nevertheless, the generalization ability of these methods under different attribute distributions, viewpoints, varying illumination, and low resolutions remains rarely understood due to strong biases and varying attributes in current datasets. To close this gap and support a systematic investigation, we present UPAR, the Unified Person Attribute Recognition Dataset. It is based on four well-known person attribute recognition datasets: PA100K, PETA, RAPv2, and Market1501. We unify those datasets by providing 3,3M additional annotations to harmonize 40 important binary attributes over 12 attribute categories across the datasets. We thus enable research on generalizable pedestrian attribute recognition as well as attribute-based person retrieval for the first time. Due to the vast variance of the image distribution, pedestrian pose, scale, and occlusion, existing approaches are greatly challenged both in terms of accuracy and efficiency. Furthermore, we develop a strong baseline for PAR and attribute-based person retrieval based on a thorough analysis of regularization methods. Our models achieve state-of-the-art performance in cross-domain and specialization settings on PA100k, PETA, RAPv2, Market1501-Attributes, and UPAR. We believe UPAR and our strong baseline will contribute to the artificial intelligence community and promote research on large-scale, generalizable attribute recognition systems.",https://openaccess.thecvf.com/content/WACV2023/html/Specker_UPAR_Unified_Pedestrian_Attribute_Recognition_and_Person_Retrieval_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Specker_UPAR_Unified_Pedestrian_Attribute_Recognition_and_Person_Retrieval_WACV_2023_paper.pdf,,https://github.com/speckean/upar_dataset,2209.02522,main,Poster,https://ieeexplore.ieee.org/document/10030414/,"['Computer vision', 'Systematics', 'Image recognition', 'Annotations', 'Lighting', 'Video surveillance', 'Artificial intelligence']","['Attribute Recognition', 'Person Retrieval', 'Pedestrian Attribute', 'Pedestrian Attribute Recognition', 'Generalization Ability', 'Single Dataset', 'Regularization Method', 'Video Surveillance', 'Strong Baseline', 'Semantic', 'Training Data', 'Batch Size', 'Small Datasets', 'Public Datasets', 'Data Augmentation', 'Real-world Scenarios', 'Online Shopping', 'Relevant Regions', 'Adversarial Training', 'Version Of Dataset', 'Dataset Bias', 'Backbone Model', 'Simple Baseline', 'Person Image', 'Vision Transformer', 'Unseen Domains', 'Surveillance Cameras', 'Hair Length', 'Outdoor Scenes', 'Privacy Issues']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",16,"Recognizing soft-biometric pedestrian attributes is essential in video surveillance and fashion retrieval. Recent works show promising results on single datasets. Nevertheless, the generalization ability of these methods under different attribute distributions, viewpoints, varying illumination, and low resolutions remains rarely understood due to strong biases and varying attributes in current datasets. To close this gap and support a systematic investigation, we present UPAR, the Unified Person Attribute Recognition Dataset. It is based on four well-known person attribute recognition datasets: PA100K, PETA, RAPv2, and Market1501. We unify those datasets by providing 3,3M additional annotations to harmonize 40 important binary attributes over 12 attribute categories across the datasets. We thus enable research on generalizable pedestrian attribute recognition as well as attribute-based person retrieval for the first time. Due to the vast variance of the image distribution, pedestrian pose, scale, and occlusion, existing approaches are greatly challenged both in terms of accuracy and efficiency. Furthermore, we develop a strong baseline for PAR and attribute-based person retrieval based on a thorough analysis of regularization methods. Our models achieve state-of-the-art performance in cross-domain and specialization settings on PA100k, PETA, RAPv2, Market1501-Attributes, and UPAR. We believe UPAR and our strong baseline will contribute to the artificial intelligence community and promote research on large-scale, generalizable attribute recognition systems. The dataset is available here: https://github.com/speckean/upar_dataset"
UVCGAN: UNet Vision Transformer Cycle-Consistent GAN for Unpaired Image-to-Image Translation,"Dmitrii Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shinjae Yoo, Meifeng Lin, Brett Viren, Yihui Ren","Brookhaven National Laboratory, Upton, NY, USA",100,USA,0,,"Unpaired image-to-image translation has broad applications in art, design, and scientific simulations. One early breakthrough was CycleGAN that emphasizes one-to-one mappings between two unpaired image domains via generative-adversarial networks (GAN) coupled with the cycle-consistency constraint, while more recent works promote one-to-many mapping to boost diversity of the translated images. Motivated by scientific simulation and one-to-one needs, this work revisits the classic CycleGAN framework and boosts its performance to outperform more contemporary models without relaxing the cycle-consistency constraint. To achieve this, we equip the generator with a Vision Transformer (ViT) and employ necessary training and regularization techniques. Compared to previous best-performing models, our model performs better and retains a strong correlation between the original and translated image. An accompanying ablation study shows that both the gradient penalty and self-supervised pre-training are crucial to the improvement. To promote reproducibility and open science, the source code, hyperparameter configurations, and pre-trained model are available at https: //github.com/LS4GAN/uvcgan.",https://openaccess.thecvf.com/content/WACV2023/html/Torbunov_UVCGAN_UNet_Vision_Transformer_Cycle-Consistent_GAN_for_Unpaired_Image-to-Image_Translation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Torbunov_UVCGAN_UNet_Vision_Transformer_Cycle-Consistent_GAN_for_Unpaired_Image-to-Image_Translation_WACV_2023_paper.pdf,,https://github.com/LS4GAN/uvcgan,2203.02557,main,Poster,https://ieeexplore.ieee.org/document/10030802/,"['Training', 'Correlation', 'Computational modeling', 'Source coding', 'Benchmark testing', 'Inspection', 'Transformers']","['Generative Adversarial Networks', 'Vision Transformer', 'Technical Training', 'Image Domain', 'Gradient Penalty', 'Loss Function', 'Convolutional Neural Network', 'Computer Vision', 'Small Datasets', 'Benchmark Datasets', 'Variational Autoencoder', 'Hair Color', 'Loss Of Identity', 'Random Flipping', 'Random Cropping', 'High Energy Physics', 'Deep Generative Models', 'Generative Adversarial Networks Training', 'Image Inpainting', 'Fréchet Inception Distance', 'Generative Adversarial Networks Loss', 'Wasserstein Generative Adversarial Networks', 'Cycle Consistency Loss', 'Benchmark Tasks', 'Attention Matrix', 'Discriminator Loss', 'Sequence Of Tokens', 'Attention Weights', 'Input Image', 'Skip Connections']","['Algorithms: Computational photography', 'image and video synthesis', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",57,"Unpaired image-to-image translation has broad applications in art, design, and scientific simulations. One early breakthrough was CycleGAN that emphasizes one-to-one mappings between two unpaired image domains via generative-adversarial networks (GAN) coupled with the cycle-consistency constraint, while more recent works promote one-to-many mapping to boost diversity of the translated images. Motivated by scientific simulation and one-to-one needs, this work revisits the classic CycleGAN framework and boosts its performance to outperform more contemporary models without relaxing the cycle-consistency constraint. To achieve this, we equip the generator with a Vision Transformer (ViT) and employ necessary training and regularization techniques. Compared to previous best-performing models, our model performs better and retains a strong correlation between the original and translated image. An accompanying ablation study shows that both the gradient penalty and self-supervised pre-training are crucial to the improvement. To promote reproducibility and open science, the source code, hyperparameter configurations, and pre-trained model are available at https://github.com/LS4GAN/uvcgan."
Uncertainty-Aware Interactive LiDAR Sampling for Deep Depth Completion,"Kensuke Taguchi, Shogo Morita, Yusuke Hayashi, Wataru Imaeda, Hironobu Fujiyoshi",KYOCERA Corporation; Chubu University,50,Japan,50,Japan,"Programmable scan LiDAR is able to measure arbitrary areas and is expected to be used in various applications. In this paper, we study a LiDAR sampling strategy for deep depth completion of a programmable scan LiDAR with an RGB camera. General data sampling strategies include adaptive approaches such as active learning, in which candidate data are assessed through a task model for data selection and then the selected data pool is updated sequentially. Although it is an effective approach, the adaptive approach requires many iterations involving the inference process to assess the candidate data, which is not suitable for LiDAR systems. Therefore, we propose a novel interactive LiDAR sampling method without each inference process. Our key insights are that we assess sampling candidates by depth estimation uncertainty and virtually update the uncertainty by an approximation of the candidate assessment. This enables us to add interactivity to the model state without requiring each inference process. We demonstrate the effectiveness of our method on the KITTI dataset and the generalization performance on the NYU-Depth-v2 dataset in comparison with a conventional adaptive LiDAR sampling method, and we find superior results in the depth completion task. We also show ablation studies to analyze our approach.",https://openaccess.thecvf.com/content/WACV2023/html/Taguchi_Uncertainty-Aware_Interactive_LiDAR_Sampling_for_Deep_Depth_Completion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Taguchi_Uncertainty-Aware_Interactive_LiDAR_Sampling_for_Deep_Depth_Completion_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030181/,"['Adaptation models', 'Computer vision', 'Laser radar', 'Uncertainty', 'Computational modeling', 'Estimation', 'Sampling methods']","['Interactive', 'Light Detection And Ranging', 'Depth Completion', 'Active Learning', 'Depth Estimation', 'RGB Camera', 'Adaptive Sampling', 'KITTI Dataset', 'Deep Neural Network', 'Local Context', 'State Of The Art', 'Optical Axis', 'RGB Images', 'Depth Map', 'Outdoor Scenes', 'Scan Position', 'Output Uncertainty', 'Sparse Map', 'Dense Depth']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision']",1,"Programmable scan LiDAR is able to measure arbitrary areas and is expected to be used in various applications. In this paper, we study a LiDAR sampling strategy for deep depth completion of a programmable scan LiDAR with an RGB camera. General data sampling strategies include adaptive approaches such as active learning, in which candidate data are assessed through a task model for data selection and then the selected data pool is updated sequentially. Although it is an effective approach, the adaptive approach requires many iterations involving the inference process to assess the candidate data, which is not suitable for LiDAR systems. Therefore, we propose a novel interactive LiDAR sampling method without each inference process. Our key insights are that we assess sampling candidates by depth estimation uncertainty and virtually update the uncertainty by an approximation of the candidate assessment. This enables us to add interactivity to the model state without requiring each inference process. We demonstrate the effectiveness of our method on the KITTI dataset and the generalization performance on the NYU-Depth-v2 dataset in comparison with a conventional adaptive LiDAR sampling method, and we find superior results in the depth completion task. We also show ablation studies to analyze our approach."
Uncertainty-Aware Label Distribution Learning for Facial Expression Recognition,"Nhat Le, Khanh Nguyen, Quang Tran, Erman Tjiputra, Bac Le, Anh Nguyen","Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam; Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam; Vietnam National University, Ho Chi Minh City, Vietnam; AIOZ, Singapore; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam; Vietnam National University, Ho Chi Minh City, Vietnam; FPT Software AI Center, Vietnam; AIOZ, Singapore; Department of Computer Science, University of Liverpool, Liverpool, UK",70,"UK, Vietnam",30,Singapore,"Despite significant progress over the past few years, ambiguity is still a key challenge in Facial Expression Recognition (FER). It can lead to noisy and inconsistent annotation, which hinders the performance of deep learning models in real-world scenarios. In this paper, we propose a new uncertainty-aware label distribution learning method to improve the robustness of deep models against uncertainty and ambiguity. We leverage neighborhood information in the valence-arousal space to adaptively construct emotiona distributions for training samples. We also consider the uncertainty of provided labels when incorporating them into the label distributions. Our method can be easily integrated into a deep network to obtain more training supervision and improve recognition accuracy. Intensive experiments on several datasets under various noisy and ambiguous settings show that our method achieves competitive results and outperforms recent state-of-the-art approaches. Our code and models are available at https://github.com/minhnhatvt/label-distribution-learning-fer-tf.",https://openaccess.thecvf.com/content/WACV2023/html/Le_Uncertainty-Aware_Label_Distribution_Learning_for_Facial_Expression_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Le_Uncertainty-Aware_Label_Distribution_Learning_for_Facial_Expression_Recognition_WACV_2023_paper.pdf,,https://github.com/minhnhatvt/label-distribution-learning-fer-tf,2209.10448,main,Poster,https://ieeexplore.ieee.org/document/10030187/,"['Training', 'Learning systems', 'Deep learning', 'Adaptation models', 'Emotion recognition', 'Uncertainty', 'Face recognition']","['Facial Expressions', 'Face Recognition', 'Label Distribution', 'Facial Expression Recognition', 'Label Distribution Learning', 'Information In Space', 'Neighborhood Information', 'Supervision Training', 'Neural Network', 'Training Set', 'Arousal', 'Noisy Data', 'Face Images', 'Emotion Categories', 'Basic Emotions', 'Facial Action', 'Single Label', 'Uncertainty Factors', 'Distribution Of Aggregates', 'Discrete Emotions', 'Noisy Labels', 'Ambiguous Expressions', 'Action Units', 'Discriminator Loss', 'Degree Of Contribution', 'CNN Backbone', 'Supervision Information', 'Facial Action Units']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose']",21,"Despite significant progress over the past few years, ambiguity is still a key challenge in Facial Expression Recognition (FER). It can lead to noisy and inconsistent annotation, which hinders the performance of deep learning models in real-world scenarios. In this paper, we propose a new uncertainty-aware label distribution learning method to improve the robustness of deep models against uncertainty and ambiguity. We leverage neighborhood information in the valence-arousal space to adaptively construct emotiona distributions for training samples. We also consider the uncertainty of provided labels when incorporating them into the label distributions. Our method can be easily integrated into a deep network to obtain more training supervision and improve recognition accuracy. Intensive experiments on several datasets under various noisy and ambiguous settings show that our method achieves competitive results and outperforms recent state-of-the-art approaches. Our code and models are available at https://github.com/minhnhatvt/label-distribution-learning-fer-tf."
Understanding the Role of Mixup in Knowledge Distillation: An Empirical Study,"Hongjun Choi, Eun Som Jeon, Ankita Shukla, Pavan Turaga","School of Arts, Media and Engineering, Arizona State University; School of Electrical, Computer and Energy Engineering, Arizona State University; Geometric Media Lab, School of Arts, Media and Engineering, Arizona State University",100,USA,0,,"Mixup is a popular data augmentation technique based on creating new samples by linear interpolation between two given data samples, to improve both the generalization and robustness of the trained model. Knowledge distillation (KD), on the other hand, is widely used for model compression and transfer learning, which involves using a larger network's implicit knowledge to guide the learning of a smaller network. At first glance, these two techniques seem very different, however, we found that ""smoothness"" is the connecting link between the two and is also a crucial attribute in understanding KD's interplay with mixup. Although many mixup variants and distillation methods have been proposed, much remains to be understood regarding the role of a mixup in knowledge distillation. In this paper, we present a detailed empirical study on various important dimensions of compatibility between mixup and knowledge distillation. We also scrutinize the behavior of the networks trained with a mixup in the light of knowledge distillation through extensive analysis, visualizations, and comprehensive experiments on image classification. Finally, based on our findings, we suggest improved strategies to guide the student network to enhance its effectiveness. Additionally, the findings of this study provide insightful suggestions to researchers and practitioners that commonly use techniques from KD. Our code is available at https://github.com/hchoi71/MIX-KD.",https://openaccess.thecvf.com/content/WACV2023/html/Choi_Understanding_the_Role_of_Mixup_in_Knowledge_Distillation_An_Empirical_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Choi_Understanding_the_Role_of_Mixup_in_Knowledge_Distillation_An_Empirical_WACV_2023_paper.pdf,,https://github.com/hchoi71/MIX-KD,2211.03946,main,Poster,https://ieeexplore.ieee.org/document/10030285/,"['Knowledge engineering', 'Training', 'Interpolation', 'Computer vision', 'Codes', 'Transfer learning', 'Robustness']","['Image Classification', 'Data Augmentation', 'Transfer Learning', 'Linear Interpolation', 'Student Network', 'Distillation Method', 'Connecting Link', 'Training Set', 'Deep Neural Network', 'Control Parameters', 'Feature Representation', 'Test Accuracy', 'Teacher Model', 'ImageNet', 'Student Performance', 'Kullback-Leibler', 'Similar Classification', 'Softmax Function', 'Beta Distribution', 'Adversarial Examples', 'Student Model', 'Top-1 Accuracy', 'Teacher Network', 'Fast Gradient Sign Method', 'CIFAR-100 Dataset', 'Output Logits', 'Gray Bars', 'Empirical Analysis', 'Adversarial Attacks']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Adversarial learning', 'adversarial attack and defense methods']",5,"Mixup is a popular data augmentation technique based on creating new samples by linear interpolation between two given data samples, to improve both the generalization and robustness of the trained model. Knowledge distillation (KD), on the other hand, is widely used for model compression and transfer learning, which involves using a larger network’s implicit knowledge to guide the learning of a smaller network. At first glance, these two techniques seem very different, however, we found that ""smoothness"" is the connecting link between the two and is also a crucial attribute in understanding KD’s interplay with mixup. Although many mixup variants and distillation methods have been proposed, much remains to be understood regarding the role of a mixup in knowledge distillation. In this paper, we present a detailed empirical study on various important dimensions of compatibility between mixup and knowledge distillation. We also scrutinize the behavior of the networks trained with a mixup in the light of knowledge distillation through extensive analysis, visualizations, and comprehensive experiments on image classification. Finally, based on our findings, we suggest improved strategies to guide the student network to enhance its effectiveness. Additionally, the findings of this study provide insightful suggestions to researchers and practitioners that commonly use techniques from KD. Our code is available at https://github.com/hchoi71/MIX-KD."
Unifying Distribution Alignment as a Loss for Imbalanced Semi-Supervised Learning,"Justin Lazarow, Kihyuk Sohn, Chen-Yu Lee, Chun-Liang Li, Zizhao Zhang, Tomas Pfister",Google Cloud AI Research; UC San Diego,50,USA,50,USA,"While remarkable progress has been made in imbalanced supervised learning, less attention has been given to the setting of imbalanced semi-supervised learning (SSL) where not only is few labeled data provided, but the underlying data distribution can be severely imbalanced. Recent work requires both complicated sampling strategies of pseudo-labeled unlabeled data and distribution alignment of the pseudo-label distribution to accommodate this imbalance. We present a novel approach that relies only on a form of a distribution alignment but no sampling strategy where rather than aligning the pseudo-labels during inference, we move the distribution alignment component into the respective cross entropy loss computations for both the supervised and unsupervised losses. This alignment compensates for both imbalance in the data and the eventual distributional shift present during evaluation. Altogether, this provides a unified strategy that offers both significantly reduced training requirements and improved performance across both low and richly labeled regimes and over varying degrees of imbalance. In experiments, we validate the efficacy of our method on SSL variants of CIFAR10-LT, CIFAR100-LT, and ImageNet-127. On ImageNet-127, our method shows 1.6% accuracy improvement over CReST with an 80% training time reduction and is competitive with other SOTA methods.",https://openaccess.thecvf.com/content/WACV2023/html/Lazarow_Unifying_Distribution_Alignment_as_a_Loss_for_Imbalanced_Semi-Supervised_Learning_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lazarow_Unifying_Distribution_Alignment_as_a_Loss_for_Imbalanced_Semi-Supervised_Learning_WACV_2023_paper.pdf,,https://github.com/google-research/crest,,main,Poster,https://ieeexplore.ieee.org/document/10030270/,"['Training', 'Computer vision', 'Codes', 'Supervised learning', 'Semisupervised learning', 'Entropy']","['Semi-supervised Learning', 'Distribution Alignment', 'Data Distribution', 'Supervised Learning', 'Training Time', 'Cross-entropy Loss', 'Learning Settings', 'Unlabeled Data', 'Reduce Training Time', 'Degree Of Imbalance', 'Underlying Data Distribution', 'Uniform Distribution', 'Hyperparameters', 'General Approach', 'Unique Approach', 'Class Distribution', 'Marginal Distribution', 'Class Imbalance', 'Point Increase', 'Pseudo Labels', 'Unlabeled Set', 'Progressive Alignment', 'Balanced Distribution', 'Imbalanced Learning', 'Unlabeled Examples', 'Imbalance Ratio', 'Label Distribution', 'Target Distribution', 'Training Examples']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",5,"While remarkable progress has been made in imbalanced supervised learning, less attention has been given to the setting of imbalanced semi-supervised learning (SSL) where not only are few labeled data provided, but the underlying data distribution can be severely imbalanced. Recent work requires both complicated sampling strategies of pseudo-labeled unlabeled data and distribution alignment of the pseudo-label distribution to accommodate this imbalance. We present a novel approach that relies only on a form of a distribution alignment but no sampling strategy where rather than aligning the pseudo-labels during inference, we move the distribution alignment component into the respective cross entropy loss computations for both the supervised and unsupervised losses. This alignment compensates for both imbalance in the data and the eventual distributional shift present during evaluation. Altogether, this provides a unified strategy that offers both significantly reduced training requirements and improved performance across both low and richly labeled regimes and over varying degrees of imbalance. In experiments, we validate the efficacy of our method on SSL variants of CIFAR10-LT, CIFAR100-LT, and ImageNet-127. On ImageNet-127, our method shows 1.6% accuracy improvement over CReST with an 80% training time reduction and is competitive with other SOTA methods. Code is available at https://github.com/google-research/crest"
Unifying Margin-Based Softmax Losses in Face Recognition,"Yang Zhang, Simao Herdade, Kapil Thadani, Eric Dodds, Jack Culpepper, Yueh-Ning Ku",Yahoo Research,0,,100,USA,"In this work, we develop a theoretical and experimental framework to study the effect of margin penalties on angular softmax losses, which have led to state-of-the-art performance in face recognition. We also introduce a new multiplicative margin which performs comparably to previously proposed additive margins when the model is trained to convergence. A regime of the margin parameters can lead to degenerate minima, but these can be reliably avoided through the use of two regularization techniques that we propose. Our theory predicts the minimal angular distance between sample embeddings and the correct and wrong class prototype vectors learned during training, and it suggests a new method to identify optimal margin parameters without expensive tuning. Finally, we conduct a thorough ablation study of the margin parameters in our proposed framework, and we characterize the sensitivity of generalization to each parameter both theoretically and through experiments on standard face recognition benchmarks.",https://openaccess.thecvf.com/content/WACV2023/html/Zhang_Unifying_Margin-Based_Softmax_Losses_in_Face_Recognition_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_Unifying_Margin-Based_Softmax_Losses_in_Face_Recognition_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030583/,"['Training', 'Manifolds', 'Sensitivity', 'Face recognition', 'Prototypes', 'Benchmark testing', 'Reliability theory']","['Face Recognition', 'Softmax Loss', 'Benchmark', 'Theoretical Framework', 'Correct Classification', 'Angular Distance', 'Wrong Classification', 'Margin Parameter', 'Face Recognition Performance', 'Loss Function', 'Training Set', 'Deep Learning', 'Hyperparameters', 'Batch Size', 'Unit Vector', 'Random Vector', 'Symmetric Distribution', 'Training Schedule', 'Marginal Value', 'Metric Learning', 'Intra-class Distance', 'Spherical Cap', 'Values Of M1', 'Learning Rate Schedule', 'Triplet Loss', 'Margin Setting', 'Dynamic Training', 'Cosine Of The Angle', 'Gradient Loss']","['Algorithms: Biometrics', 'face', 'gesture', 'body pose', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",5,"In this work, we develop a theoretical and experimental framework to study the effect of margin penalties on angular softmax losses, which have led to state-of-the-art performance in face recognition. We also introduce a new multiplicative margin which performs comparably to previously proposed additive margins when the model is trained to convergence. A regime of the margin parameters can lead to degenerate minima, but these can be reliably avoided through the use of two regularization techniques that we propose. Our theory predicts the minimal angular distance between sample embeddings and the correct and wrong class prototype vectors learned during training, and it suggests a new method to identify optimal margin parameters without expensive tuning. Finally, we conduct a thorough ablation study of the margin parameters in our proposed framework, and we characterize the sensitivity of generalization to each parameter both theoretically and through experiments on standard face recognition benchmarks."
Universal Deep Image Compression via Content-Adaptive Optimization With Adapters,"Koki Tsubota, Hiroaki Akutsu, Kiyoharu Aizawa","The University of Tokyo; Hitachi, Ltd.",50,Japan,50,Japan,"Deep image compression performs better than conventional codecs, such as JPEG, on natural images. However, deep image compression is learning-based and encounters a problem: the compression performance deteriorates significantly for out-of-domain images. In this study, we highlight this problem and address a novel task: universal deep image compression. This task aims to compress images belonging to arbitrary domains, such as natural images, line drawings, and comics. To address this problem, we propose a content-adaptive optimization framework; this framework uses a pre-trained compression model and adapts the model to a target image during compression. Adapters are inserted into the decoder of the model. For each input image, our framework optimizes the latent representation extracted by the encoder and the adapter parameters in terms of rate-distortion. The adapter parameters are additionally transmitted per image. For the experiments, a benchmark dataset containing uncompressed images of four domains (natural images, line drawings, comics, and vector arts) is constructed and the proposed universal deep compression is evaluated. Finally, the proposed model is compared with non-adaptive and existing adaptive compression models. The comparison reveals that the proposed model outperforms these. The code and dataset are publicly available at https://github.com/kktsubota/universal-dic.",https://openaccess.thecvf.com/content/WACV2023/html/Tsubota_Universal_Deep_Image_Compression_via_Content-Adaptive_Optimization_With_Adapters_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tsubota_Universal_Deep_Image_Compression_via_Content-Adaptive_Optimization_With_Adapters_WACV_2023_paper.pdf,,https://github.com/kktsubota/universal-dic,2211.00918,main,Poster,https://ieeexplore.ieee.org/document/10030094/,"['Adaptation models', 'Computer vision', 'Image coding', 'Codes', 'Transform coding', 'Rate-distortion', 'Benchmark testing']","['Image Compression', 'Deep Image Compression', 'Natural Images', 'Target Image', 'Line Drawings', 'Latent Representation', 'Natural Vector', 'Decoding Model', 'Universal Image', 'Gradient Descent', 'Baseline Methods', 'Peak Signal-to-noise Ratio', 'Parameter Update', 'Bitstream', 'Optimal Order', 'Entropy Model', 'Adaptive Optimization', 'Video Compression', 'Uniform Quantization', 'Entropy Coding', 'Decoder Parameters', 'Cartoon Images']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Computational photography', 'image and video synthesis']",8,"Deep image compression performs better than conventional codecs, such as JPEG, on natural images. However, deep image compression is learning-based and en-counters a problem: the compression performance deteriorates significantly for out-of-domain images. In this study, we highlight this problem and address a novel task: universal deep image compression. This task aims to compress images belonging to arbitrary domains, such as natural images, line drawings, and comics. To address this problem, we propose a content-adaptive optimization framework; this framework uses a pre-trained compression model and adapts the model to a target image during compression. Adapters are inserted into the decoder of the model. For each input image, our framework optimizes the latent representation extracted by the encoder and the adapter parameters in terms of rate-distortion. The adapter parameters are additionally transmitted per image. For the experiments, a benchmark dataset containing uncompressed images of four domains (natural images, line drawings, comics, and vector arts) is constructed and the proposed universal deep compression is evaluated. Finally, the proposed model is compared with non-adaptive and existing adaptive compression models. The comparison reveals that the proposed model outperforms these. The code and dataset are publicly available at https://github.com/kktsubota/universal-dic."
Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings With Multivariate Occupancy Time Series,"Thomas Kreutz, Max Mühlhäuser, Alejandro Sanchez Guinea","Telekooperation Lab, Technical University Darmstadt",100,Germany,0,,"In this work, we address the problem of unsupervised moving object segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no ground truth annotations are involved. Deep learning-based state-of-the-art methods for LiDAR MOS strongly depend on annotated ground truth data, which is expensive to obtain and scarce in existence. To close this gap in the stationary setting, we propose a novel 4D LiDAR representation based on multivariate time series that relaxes the problem of unsupervised MOS to a time series clustering problem. More specifically, we propose modeling the change in occupancy of a voxel by a multivariate occupancy time series (MOTS), which captures spatio-temporal occupancy changes on the voxel level and its surrounding neighborhood. To perform unsupervised MOS, we train a neural network in a self-supervised manner to encode MOTS into voxel-level feature representations, which can be partitioned by a clustering algorithm into moving or stationary. Experiments on stationary scenes from the Raw KITTI dataset show that our fully unsupervised approach achieves performance that is comparable to that of supervised state-of-the-art approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Kreutz_Unsupervised_4D_LiDAR_Moving_Object_Segmentation_in_Stationary_Settings_With_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kreutz_Unsupervised_4D_LiDAR_Moving_Object_Segmentation_in_Stationary_Settings_With_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030114/,"['Point cloud compression', 'Computer vision', 'Laser radar', 'Annotations', 'Time series analysis', 'Neural networks', 'Clustering algorithms']","['Time Series', 'Multivariate Time Series', 'Static Set', 'Moving Object Segmentation', 'Neural Network', 'Clustering Algorithm', 'Feature Representation', 'Unsupervised Learning', 'Annotation Data', 'Balance Of System', 'Lidar Data', 'Voxel Level', 'Changes In Occupancy', 'Maximum And Minimum', 'Window Size', 'State Data', 'Static Conditions', 'Pedestrian', 'Point Cloud', 'Receptive Field', 'LiDAR Sensor', 'Neighborhood Change', 'Self-supervised Learning', 'Gaussian Mixture Model', 'Intelligent Transportation Systems', 'Wrong Predictions', 'Embedding Dimension', 'Enormous Amount Of Data', 'Semantic Segmentation', 'Surveillance Cameras']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', '3D computer vision', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",11,"In this work, we address the problem of unsupervised moving object segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no ground truth annotations are involved. Deep learning-based state-of-the-art methods for LiDAR MOS strongly depend on annotated ground truth data, which is expensive to obtain and scarce in existence. To close this gap in the stationary setting, we propose a novel 4D LiDAR representation based on multivariate time series that relaxes the problem of unsupervised MOS to a time series clustering problem. More specifically, we propose modeling the change in occupancy of a voxel by a multivariate occupancy time series (MOTS), which captures spatio-temporal occupancy changes on the voxel level and its surrounding neighborhood. To perform unsupervised MOS, we train a neural network in a self-supervised manner to encode MOTS into voxel-level feature representations, which can be partitioned by a clustering algorithm into moving or stationary. Experiments on stationary scenes from the Raw KITTI dataset show that our fully unsupervised approach achieves performance that is comparable to that of supervised state-of-the-art approaches."
Unsupervised Audio-Visual Lecture Segmentation,"Darshan Singh S., Anchit Gupta, C. V. Jawahar, Makarand Tapaswi","CVIT, IIIT Hyderabad",100,India,0,,"Over the last decade, online lecture videos have become increasingly popular and have experienced a meteoric rise during the pandemic. However, video-language research has primarily focused on instructional videos or movies, and tools to help students navigate the growing online lectures are lacking. Our first contribution is to facilitate research in the educational domain, by introducing AVLectures, a large-scale dataset consisting of 86 courses with over 2,350 lectures covering various STEM subjects. Each course contains video lectures, transcripts, OCR outputs for lecture frames, and optionally lecture notes, slides, assignments, and related educational content that can inspire a variety of tasks. Our second contribution is introducing video lecture segmentation that splits lectures into bite-sized topics that show promise in improving learner engagement. We formulate lecture segmentation as an unsupervised task that leverages visual, textual, and OCR cues from the lecture, while clip representations are fine-tuned on a pretext self-supervised task of matching the narration with the temporally aligned visual content. We use these representations to generate segments using a temporally consistent 1-nearest neighbor algorithm, TW-FINCH. We evaluate our method on 15 courses and compare it against various visual and textual baselines, outperforming all of them. Our comprehensive ablation studies also identify the key factors driving the success of our approach.",https://openaccess.thecvf.com/content/WACV2023/html/S._Unsupervised_Audio-Visual_Lecture_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/S._Unsupervised_Audio-Visual_Lecture_Segmentation_WACV_2023_paper.pdf,https://cvit.iiit.ac.in/research/projects/cvit-projects/avlectures,,2210.16644,main,Poster,https://ieeexplore.ieee.org/document/10030327/,"['Visualization', 'Computer vision', 'Three-dimensional displays', 'Pandemics', 'Navigation', 'Optical character recognition', 'Motion pictures']","['Narrative', 'Large-scale Datasets', 'Video For Instructions', 'Educational Domain', 'Video Lectures', 'Online Lectures', 'Unsupervised Tasks', 'Visual Representation', 'Visual Features', 'Intersection Over Union', 'Video Clips', 'Representation Learning', 'Online Courses', 'Language Model', 'Content Table', 'Textual Features', 'Segment Boundaries', 'Massive Open Online Courses', 'Text Query', 'Temporal Proximity', 'Fine-tuning Strategy', 'Web Scraping', 'Digital Board', 'Self-supervised Manner', 'Lecture Content', 'Embedding Learning', 'Unsupervised Manner', '3D Features']","['Applications: Education', 'Vision + language and/or other modalities']",1,"Over the last decade, online lecture videos have become increasingly popular and have experienced a meteoric rise during the pandemic. However, video-language research has primarily focused on instructional videos or movies, and tools to help students navigate the growing online lectures are lacking. Our first contribution is to facilitate research in the educational domain by introducing AVLectures, a large-scale dataset consisting of 86 courses with over 2,350 lectures covering various STEM subjects. Each course contains video lectures, transcripts, OCR outputs for lecture frames, and optionally lecture notes, slides, assignments, and related educational content that can inspire a variety of tasks. Our second contribution is introducing video lecture segmentation that splits lectures into bite-sized topics. Lecture clip representations leverage visual, textual, and OCR cues and are trained on a pretext self-supervised task of matching the narration with the temporally aligned visual content. We formulate lecture segmentation as an unsupervised task and use these representations to generate segments using a temporally consistent 1-nearest neighbor algorithm, TW-FINCH [44]. We evaluate our method on 15 courses and compare it against various visual and textual baselines, outperforming all of them. Our comprehensive ablation studies also identify the key factors driving the success of our approach."
Unsupervised Multi-Object Segmentation Using Attention and Soft-Argmax,"Bruno Sauvalle, Arnaud de La Fortelle","Centre de Robotique, Mines ParisTech PSL University",100,France,0,,"We introduce a new architecture for unsupervised object-centric representation learning and multi-object detection and segmentation, which uses a translation-equivariant attention mechanism to predict the coordinates of the objects present in the scene and to associate a feature vector to each object. A transformer encoder handles occlusions and redundant detections, and a convolutional autoencoder is in charge of background reconstruction. We show that this architecture significantly outperforms the state of the art on complex synthetic benchmarks.",https://openaccess.thecvf.com/content/WACV2023/html/Sauvalle_Unsupervised_Multi-Object_Segmentation_Using_Attention_and_Soft-Argmax_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Sauvalle_Unsupervised_Multi-Object_Segmentation_Using_Attention_and_Soft-Argmax_WACV_2023_paper.pdf,,,2205.13271,main,Poster,https://ieeexplore.ieee.org/document/10030554/,['Portable document format'],,"['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",4,"We introduce a new architecture for unsupervised object-centric representation learning and multi-object detection and segmentation, which uses a translation-equivariant attention mechanism to predict the coordinates of the objects present in the scene and to associate a feature vector to each object. A transformer encoder handles occlusions and redundant detections, and a convolutional autoencoder is in charge of background reconstruction. We show that this architecture significantly outperforms the state of the art on complex synthetic benchmarks."
Unsupervised Video Object Segmentation via Prototype Memory Network,"Minhyeok Lee, Suhwan Cho, Seunghoon Lee, Chaewon Park, Sangyoun Lee","Yonsei University, Korea Institute of Science and Technology (KIST); Yonsei University",100,South Korea,0,,"Unsupervised video object segmentation aims to segment a target object in the video without a ground truth mask in the initial frame. This challenging task requires extracting features for the most salient common objects within a video sequence. This difficulty can be solved by using motion information such as optical flow, but using only the information between adjacent frames results in poor connectivity between distant frames and poor performance. To solve this problem, we propose a novel prototype memory network architecture. The proposed model effectively extracts the RGB and motion information by extracting superpixel-based component prototypes from the input RGB images and optical flow maps. In addition, the model scores the usefulness of the component prototypes in each frame based on a self-learning algorithm and adaptively stores the most useful prototypes in memory and discards obsolete prototypes. We use the prototypes in the memory bank to predict the next query frame's mask, which enhances the association between distant frames to help with accurate mask prediction. Our method is evaluated on three datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed model with various ablation studies.",https://openaccess.thecvf.com/content/WACV2023/html/Lee_Unsupervised_Video_Object_Segmentation_via_Prototype_Memory_Network_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lee_Unsupervised_Video_Object_Segmentation_via_Prototype_Memory_Network_WACV_2023_paper.pdf,,,2209.03712,main,Poster,https://ieeexplore.ieee.org/document/10030128/,"['Adaptation models', 'Video sequences', 'Prototypes', 'Object segmentation', 'Feature extraction', 'Prediction algorithms', 'Data mining']","['Video Object Segmentation', 'Unsupervised Video Object Segmentation', 'Prototype Memory', 'RGB Images', 'Target Object', 'Common Objects', 'Optical Flow', 'Video Sequences', 'Motion Information', 'Flow Map', 'Salient Object', 'Video Object', 'Memory Bank', 'Input RGB Image', 'Intersection Over Union', 'Multilayer Perceptron', 'Pixel Coordinates', 'Conditional Random Field', 'Feature Encoder', 'Transformer Block', 'Popular Datasets', 'Vision Transformer', 'Simple Linear Iterative Clustering', 'Feature Pyramid Network', 'Optical Flow Estimation']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",18,"Unsupervised video object segmentation aims to segment a target object in the video without a ground truth mask in the initial frame. This challenging task requires extracting features for the most salient common objects within a video sequence. This difficulty can be solved by using motion information such as optical flow, but using only the information between adjacent frames results in poor connectivity between distant frames and poor performance. To solve this problem, we propose a novel prototype memory network architecture. The proposed model effectively extracts the RGB and motion information by extracting superpixel-based component prototypes from the input RGB images and optical flow maps. In addition, the model scores the usefulness of the component prototypes in each frame based on a self-learning algorithm and adaptively stores the most useful prototypes in memory and discards obsolete proto-types. We use the prototypes in the memory bank to predict the next query frame’s mask, which enhances the association between distant frames to help with accurate mask prediction. Our method is evaluated on three datasets, achieving state-of-the-art performance. We prove the effectiveness of the proposed model with various ablation studies."
Uplift and Upsample: Efficient 3D Human Pose Estimation With Uplifting Transformers,"Moritz Einfalt, Katja Ludwig, Rainer Lienhart","Machine Learning and Computer Vision Lab, University of Augsburg",100,Germany,0,,"The state-of-the-art for monocular 3D human pose estimation in videos is dominated by the paradigm of 2D-to-3D pose uplifting. While the uplifting methods themselves are rather efficient, the true computational complexity depends on the per-frame 2D pose estimation. In this paper, we present a Transformer-based pose uplifting scheme that can operate on temporally sparse 2D pose sequences but still produce temporally dense 3D pose estimates. We show how masked token modeling can be utilized for temporal upsampling within Transformer blocks. This allows to decouple the sampling rate of input 2D poses and the target frame rate of the video and drastically decreases the total computational complexity. Additionally, we explore the option of pre-training on large motion capture archives, which has been largely neglected so far. We evaluate our method on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. With an MPJPE of 45.0 mm and 46.9 mm, respectively, our proposed method can compete with the state-of-the-art while reducing inference time by a factor of 12. This enables real-time throughput with variable consumer hardware in stationary and mobile applications. We release our code and models at https://github.com/goldbricklemon/uplift-upsample-3dhpe",https://openaccess.thecvf.com/content/WACV2023/html/Einfalt_Uplift_and_Upsample_Efficient_3D_Human_Pose_Estimation_With_Uplifting_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Einfalt_Uplift_and_Upsample_Efficient_3D_Human_Pose_Estimation_With_Uplifting_WACV_2023_paper.pdf,,https://github.com/goldbricklemon/uplift-upsample-3dhpe,2210.0611,main,Poster,https://ieeexplore.ieee.org/document/10030339/,"['Solid modeling', 'Three-dimensional displays', 'Pose estimation', 'Transformers', 'Throughput', 'Motion capture', 'Real-time systems']","['Pose Estimation', 'Human Pose Estimation', 'Efficient 3D', 'Computational Complexity', 'Frame Rate', 'Motion Capture', '3D Pose', 'Transformer Block', '2D Pose', 'Data Augmentation', 'Entire Sequence', 'Input Sequence', 'Final Prediction', 'Video Frames', 'Over Space', 'Bilinear Interpolation', 'Graph Convolutional Network', 'Human Motion', 'Spatial Accuracy', 'Target Dataset', 'Transformer Architecture', 'Input Rate', 'Frame Index', 'Center Of Frame', 'Factor XIII', 'Single Camera', 'Sparse Input', 'Temporal Convolutional Network', 'RGB Camera', 'Human Joint']","['Algorithms: 3D computer vision', 'Biometrics', 'face', 'gesture', 'body pose', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",24,"The state-of-the-art for monocular 3D human pose estimation in videos is dominated by the paradigm of 2D-to-3D pose uplifting. While the uplifting methods themselves are rather efficient, the true computational complexity depends on the per-frame 2D pose estimation. In this paper, we present a Transformer-based pose uplifting scheme that can operate on temporally sparse 2D pose sequences but still produce temporally dense 3D pose estimates. We show how masked token modeling can be utilized for temporal upsampling within Transformer blocks. This allows to decouple the sampling rate of input 2D poses and the target frame rate of the video and drastically decreases the total computational complexity. Additionally, we explore the option of pre-training on large motion capture archives, which has been largely neglected so far We evaluate our method on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. With an MPJPE of 45.0 mm and 46.9 mm, respectively, our proposed method can compete with the state-of-the-art while reducing inference time by a factor of 12. This enables real-time throughput with variable consumer hardware in stationary and mobile applications. We release our code and models at https://github.com/goldbricklemon/uplift-upsample-3dhpe"
Urban Scene Semantic Segmentation With Low-Cost Coarse Annotation,"Anurag Das, Yongqin Xian, Yang He, Zeynep Akata, Bernt Schiele","ETH Zürich; MPI for Informatics, Saarland Informatics Campus; MPI for Intelligent Systems, University of Tübingen; CISPA",100,"Germany, Switzerland",0,,"For best performance, today's semantic segmentation methods use large and carefully labeled datasets, requiring expensive annotation budgets. In this work, we show that coarse annotation is a low-cost but highly effective alternative for training semantic segmentation models. Considering the urban scene segmentation scenario, we leverage cheap coarse annotations for real-world captured data, as well as synthetic data to train our model and show competitive performance compared with fully annotated real-world data. Specifically, we propose a coarse-to fine self-training framework that generates pseudo labels for unlabeled regions of the coarsely annotated data, using synthetic data to improve predictions around the boundaries between semantic classes, and using cross-domain data augmentation to increase diversity. Our extensive experimental results on Cityscapes and BDD100k datasets demonstrate that our method achieves a significantly better performance vs annotation cost tradeoff, yielding a comparable performance to fully annotated data with only a small fraction of the annotation budget. Also, when used as pretraining, our framework performs better compared to the standard fully supervised setting.",https://openaccess.thecvf.com/content/WACV2023/html/Das_Urban_Scene_Semantic_Segmentation_With_Low-Cost_Coarse_Annotation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Das_Urban_Scene_Semantic_Segmentation_With_Low-Cost_Coarse_Annotation_WACV_2023_paper.pdf,,,2212.07911,main,Poster,https://ieeexplore.ieee.org/document/10030587/,"['Training', 'Computer vision', 'Costs', 'Annotations', 'Semantic segmentation', 'Semantics', 'Data models']","['Semantic Segmentation', 'Urban Scenes', 'Coarse Annotations', 'Urban Scene Semantic Segmentation', 'Data Augmentation', 'Annotation Data', 'Competitive Performance', 'Pseudo Labels', 'Region Labels', 'Training Set', 'Training Data', 'Primary Source', 'Image Pixels', 'Bounding Box', 'Traffic Light', 'Unlabeled Data', 'Synthetic Images', 'Domain Adaptation', 'Semi-supervised Learning', 'Granular Data', 'Label Of Pixel', 'Boundary Information', 'Unlabeled Images', 'Scribble', 'Zero-shot', 'Overhead Costs', 'Object Boundaries', 'Training Batch', 'Training Examples']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)']",6,"For best performance, today’s semantic segmentation methods use large and carefully labeled datasets, requiring expensive annotation budgets. In this work, we show that coarse annotation is a low-cost but highly effective alternative for training semantic segmentation models. Considering the urban scene segmentation scenario, we lever-age cheap coarse annotations for real-world captured data, as well as synthetic data to train our model and show competitive performance compared with finely annotated real-world data. Specifically, we propose a coarse-to-fine self-training framework that generates pseudo labels for unlabeled regions of the coarsely annotated data, using synthetic data to improve predictions around the boundaries between semantic classes, and using cross-domain data augmentation to increase diversity. Our extensive experimental results on Cityscapes and BDD100k datasets demonstrate that our method achieves a significantly better performance vs annotation cost tradeoff, yielding a comparable performance to fully annotated data with only a small fraction of the annotation budget. Also, when used as pre-training, our framework performs better compared to the standard fully supervised setting."
VLC-BERT: Visual Question Answering With Contextualized Commonsense Knowledge,"Sahithya Ravi, Aditya Chinchure, Leonid Sigal, Renjie Liao, Vered Shwartz","University of British Columbia; University of British Columbia, Vector Institute for AI",100,Canada,0,,"There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work, we focus on questions that require commonsense reasoning. In contrast to previous methods which inject knowledge from static knowledge bases, we investigate the incorporation of contextualized knowledge using Commonsense Transformer (COMET), an existing knowledge model trained on human-curated knowledge bases. We propose a method to generate, select, and encode external commonsense knowledge alongside visual and textual cues in a new pre-trained Vision-Language-Commonsense transformer model, VLC-BERT. Through our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets, we show that VLC-BERT is capable of outperforming existing models that utilize static knowledge bases. Furthermore, through a detailed analysis, we explain which questions benefit, and which don't, from contextualized commonsense knowledge from COMET.",https://openaccess.thecvf.com/content/WACV2023/html/Ravi_VLC-BERT_Visual_Question_Answering_With_Contextualized_Commonsense_Knowledge_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Ravi_VLC-BERT_Visual_Question_Answering_With_Contextualized_Commonsense_Knowledge_WACV_2023_paper.pdf,,https://github.com/aditya10/VLC-BERT,,main,Poster,https://ieeexplore.ieee.org/document/10030877/,"['Comets', 'Visualization', 'Analytical models', 'Knowledge based systems', 'Linguistics', 'Transformers', 'Question answering (information retrieval)']","['Visual Question Answering', 'Commonsense Knowledge', 'Knowledge Base', 'Visual Cues', 'Transformer Model', 'External Knowledge', 'Training Set', 'Validation Set', 'Image Regions', 'Attention Mechanism', 'Large-scale Datasets', 'Input Sequence', 'Additional Input', 'Language Model', 'Large-scale Models', 'Attention Weights', 'Factual Knowledge', 'Sentence Type', 'Weak Supervision', 'Google Images', 'Semantic Search', 'Word Tokens', 'Subset Of Questions', 'Masked Language Model']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",16,"There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work, we focus on questions that require commonsense reasoning. In contrast to previous methods which inject knowledge from static knowledge bases, we investigate the incorporation of contextualized knowledge using Commonsense Transformer (COMET), an existing knowledge model trained on human-curated knowledge bases. We propose a method to generate, select, and encode external commonsense knowledge alongside visual and textual cues in a new pre-trained Vision-Language-Commonsense transformer model, VLC-BERT. Through our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets, we show that VLC-BERT is capable of outperforming existing models that utilize static knowledge bases. Furthermore, through a detailed analysis, we explain which questions benefit, and which don’t, from contextualized commonsense knowledge from COMET. Code: https://github.com/aditya10/VLC-BERT"
VSGD-Net: Virtual Staining Guided Melanocyte Detection on Histopathological Images,"Kechun Liu, Beibin Li, Wenjun Wu, Caitlin May, Oliver Chang, Stevan Knezevich, Lisa Reisch, Joann Elmore, Linda Shapiro","University of Washington, Microsoft Research; University of California, Los Angeles; University of Washington; University of Washington, V A Puget Sound; Dermatopathology Northwest; Pathology Associates",66.66666667,USA,33.33333333,USA,"Detection of melanocytes serves as a critical prerequisite in assessing melanocytic growth patterns when diagnosing melanoma and its precursor lesions on skin biopsy specimens. However, this detection is challenging due to the visual similarity of melanocytes to other cells in routine Hematoxylin and Eosin (H&E) stained images, leading to the failure of current nuclei detection methods. Stains such as Sox10 can mark melanocytes, but they require an additional step and expense and thus are not regularly used in clinical practice. To address these limitations, we introduce VSGD-Net, a novel detection network that learns melanocyte identification through virtual staining from H&E to Sox10. The method takes only routine H&E images during inference, resulting in a promising approach to support pathologists in the diagnosis of melanoma. To the best of our knowledge, this is the first study that investigates the detection problem using image synthesis features between two distinct pathology stainings. Extensive experimental results show that our proposed model outperforms state-of-the-art nuclei detection methods.",https://openaccess.thecvf.com/content/WACV2023/html/Liu_VSGD-Net_Virtual_Staining_Guided_Melanocyte_Detection_on_Histopathological_Images_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Liu_VSGD-Net_Virtual_Staining_Guided_Melanocyte_Detection_on_Histopathological_Images_WACV_2023_paper.pdf,,https://github.com/kechunl/VSGD-Net,,main,Poster,https://ieeexplore.ieee.org/document/10030364/,"['Visualization', 'Pathology', 'Image synthesis', 'Biological system modeling', 'Source coding', 'Biopsy', 'Melanoma']","['Histopathological Images', 'Virtual Staining', 'Hematoxylin And Eosin', 'Melanoma', 'Skin Biopsies', 'Image Synthesis', 'Visual Similarity', 'Nuclei Detection', 'Skin Biopsy Specimens', 'Convolutional Neural Network', 'High Precision', 'Detection Performance', 'Intersection Over Union', 'Deep Convolutional Neural Network', 'Generative Adversarial Networks', 'Ground Truth Labels', 'Peak Signal-to-noise Ratio', 'Skip Connections', 'High Recall', 'Intermediate Features', 'Region Proposal Network', 'Mask R-CNN', 'Attention Block', 'Generative Adversarial Networks Model', 'Certain Types Of Cells', 'Feature Pyramid Network', 'U-Net Structure', 'ResNet-50 Backbone', 'Generative Adversarial Networks Loss', 'Bounding Box']","['Applications: Biomedical/healthcare/medicine', 'Adversarial learning', 'adversarial attack and defense methods', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)']",4,"Detection of melanocytes serves as a critical prerequisite in assessing melanocytic growth patterns when diagnosing melanoma and its precursor lesions on skin biopsy specimens. However, this detection is challenging due to the visual similarity of melanocytes to other cells in routine Hematoxylin and Eosin (H&E) stained images, leading to the failure of current nuclei detection methods. Stains such as Sox10 can mark melanocytes, but they require an additional step and expense and thus are not regularly used in clinical practice. To address these limitations, we introduce VSGD-Net, a novel detection network that learns melanocyte identification through virtual staining from H&E to Sox10. The method takes only routine H&E images during inference, resulting in a promising approach to support pathologists in the diagnosis of melanoma. To the best of our knowledge, this is the first study that investigates the detection problem using image synthesis features between two distinct pathology stainings. Extensive experimental results show that our proposed model outperforms state-of-the-art nuclei detection methods for melanocyte detection. The source code and pre-trained model are available at: https://github.com/kechunl/VSGD-Net"
Video Joint Denoising and Demosaicing With Recurrent CNNs,"Valéry Dewil, Adrien Courtois, Mariano Rodríguez, Thibaud Ehret, Nicola Brandonisio, Denis Bujoreanu, Gabriele Facciolo, Pablo Arias","Université Paris-Saclay, CNRS, ENS Paris-Saclay, Centre Borelli, 91190, Gif-sur-Yvette, France; Huawei Technologies France SASU",100,France,0,,"Denoising and demosaicing are two critical components of the image/video processing pipeline. While historically these two tasks have mainly been considered separately, current neural network approaches allow to obtain state-of-the-art results by treating them jointly. However, most existing research focuses in single image or burst joint denoising and demosaicing (JDD). Although related to burst JDD, video JDD deserves its own treatment. In this work we present an empirical exploration of different design aspects of video joint denoising and demosaicing using neural networks. We compare recurrent and non-recurrent approaches and explore aspects such as type of propagated information in recurrent networks, motion compensation, video stabilization, and network architecture. We found that recurrent networks with motion compensation achieve best results. Our work should serve as a strong baseline for future research in video JDD.",https://openaccess.thecvf.com/content/WACV2023/html/Dewil_Video_Joint_Denoising_and_Demosaicing_With_Recurrent_CNNs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Dewil_Video_Joint_Denoising_and_Demosaicing_With_Recurrent_CNNs_WACV_2023_paper.pdf,https://centreborelli.github.io/RVDD/,,,main,Poster,https://ieeexplore.ieee.org/document/10030104/,"['Computer vision', 'Recurrent neural networks', 'Noise reduction', 'Pipelines', 'Network architecture', 'Motion compensation', 'Task analysis']","['Joint Denoising', 'Neural Network', 'Recurrent Network', 'Motion Compensation', 'Learning Rate', 'Gaussian Noise', 'Feature Maps', 'Optical Flow', 'Consecutive Frames', 'Training Details', 'Current Frame', 'Multiple Frames', 'Gamma Correction', 'White Balance', 'Camera Motion', 'Input Frames', 'Previous Output', 'Raw Video', 'Future Frames', 'Denoising Methods', 'Raw Frames', 'Tensor Of Size']","['Applications: Smartphones/end user devices', 'Visualization']",3,"Denoising and demosaicing are two critical components of the image/video processing pipeline. While historically these two tasks have mainly been considered separately, current neural network approaches allow to obtain state-of-the-art results by treating them jointly. However, most existing research focuses in single image or burst joint denoising and demosaicing (JDD). Although related to burst JDD, video JDD deserves its own treatment. In this work we present an empirical exploration of different design aspects of video joint denoising and demosaicing using neural networks. We compare recurrent and non-recurrent approaches and explore aspects such as type of propagated information in recurrent networks, motion compensation, video stabilization, and network architecture. We found that recurrent networks with motion compensation achieve best results. Our work should serve as a strong baseline for future research in video JDD."
Video Object Matting via Hierarchical Space-Time Semantic Guidance,"Yumeng Wang, Bo Xu, Ziwen Li, Han Huang, Cheng Lu, Yandong Guo","Xmotors; OPPO Research Institute, Northwestern Polytechnical University; OPPO Research Institute",66.66666667,China,33.33333333,USA,"Different from most existing approaches that require trimap generation for each frame, we reformulate video object matting (VOM) by introducing improved semantic guidance propagation. The proposed approach can achieve a higher degree of temporal coherence between frames with only a single coarse mask as reference. In this paper, we adapt the hierarchical memory matching mechanism into the space-time baseline to build an efficient and robust framework for semantic guidance propagation and alpha prediction. To enhance the temporal smoothness, we also propose a cross-frame attention refinement (CFAR) module that can refine the feature representations across multiple adjacent frames (both historical and current frames) based on the spatio-temporal correlation among the cross-frame pixels. Extensive experiments demonstrate the effectiveness of hierarchical spatio-temporal semantic guidance and the cross-video-frame attention refinement module, and our model outperforms the state-of-the-art VOM methods. We also analyze the significance of different components in our model.",https://openaccess.thecvf.com/content/WACV2023/html/Wang_Video_Object_Matting_via_Hierarchical_Space-Time_Semantic_Guidance_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Video_Object_Matting_via_Hierarchical_Space-Time_Semantic_Guidance_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030502/,"['Adaptation models', 'Computer vision', 'Analytical models', 'Correlation', 'Three-dimensional displays', 'Computational modeling', 'Semantics']","['Semantic Guidance', 'Feature Representation', 'Current Frame', 'Multiple Frames', 'Effective Guidance', 'Temporal Coherence', 'Spatiotemporal Correlation', 'Pixel In Frame', 'Hierarchical Mechanism', 'Training Set', 'Low Resolution', 'Attention Mechanism', 'Semantic Information', 'Video Frames', 'Temporal Correlation', 'Encoder-decoder', 'RGB Color', 'Content Creation', 'Attention Map', 'Hierarchical Features', 'Alpha Matte', 'Memory Reading', 'Memory Bank', 'Unknown Regions', 'State-of-the-art Models']","['Algorithms: Vision + language and/or other modalities', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",1,"Different from most existing approaches that require trimap generation for each frame, we reformulate video object matting (VOM) by introducing improved semantic guidance propagation. The proposed approach can achieve a higher degree of temporal coherence between frames with only a single coarse mask as a reference. In this paper, we adapt the hierarchical memory matching mechanism into the space-time baseline to build an efficient and robust framework for semantic guidance propagation and alpha prediction. To enhance the temporal smoothness, we also propose a cross-frame attention refinement (CFAR) module that can refine the feature representations across multiple adjacent frames (both historical and current frames) based on the spatio-temporal correlation among the cross- frame pixels. Extensive experiments demonstrate the effectiveness of hierarchical spatio-temporal semantic guidance and the cross-video-frame attention refinement module, and our model outperforms the state-of-the-art VOM methods. We also analyze the significance of different components in our model."
ViewCLR: Learning Self-Supervised Video Representation for Unseen Viewpoints,"Srijan Das, Michael S. Ryoo",University of North Carolina at Charlotte; Stony Brook University,100,USA,0,,"Learning self-supervised video representation predominantly focuses on discriminating instances generated from simple data augmentation schemes. However, the learned representation often fails to generalize over unseen camera viewpoints. To this end, we propose ViewCLR, that learns self-supervised video representation invariant to camera viewpoint changes. We introduce a viewpoint-generator that can be considered as a learnable augmentation for any self-supervised pre-text tasks, to generate latent viewpoint representation of a video. ViewCLR maximizes the similarities between the representation of the latent viewpoint and that of the original viewpoint, enabling the learned video encoder to generalize over unseen camera viewpoints. Experiments on cross-view benchmark datasets including NTU RGB+D dataset show that ViewCLR stands as a state-of-the-art viewpoint invariant self-supervised method.",https://openaccess.thecvf.com/content/WACV2023/html/Das_ViewCLR_Learning_Self-Supervised_Video_Representation_for_Unseen_Viewpoints_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Das_ViewCLR_Learning_Self-Supervised_Video_Representation_for_Unseen_Viewpoints_WACV_2023_paper.pdf,,,2112.03905,main,Poster,https://ieeexplore.ieee.org/document/10030791/,"['Computer vision', 'Benchmark testing', 'Cameras', 'Robustness', 'Task analysis']","['Unseen Viewpoints', 'Data Augmentation', 'Representation Learning', 'Latent Representation', 'Viewpoint Changes', 'Pretext Task', 'Camera Viewpoint', 'Video Encoding', 'Feature Maps', 'Dimensional Vector', 'Large-scale Datasets', 'Action Recognition', 'Optical Flow', 'Action Classes', '3D Representation', 'Camera View', 'Self-supervised Learning', '2D Projection', 'Contrastive Loss', 'Camera Angle', '3D Pose', 'Invariant Representation', 'Appearance Information', 'Self-supervised Learning Methods', '3D World', 'Input RGB', 'Video Samples', 'Action Labels', 'Gaussian Blur', 'Generative Adversarial Networks']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",6,"Learning self-supervised video representation predominantly focuses on discriminating instances generated from simple data augmentation schemes. However, the learned representation often fails to generalize over unseen camera viewpoints. To this end, we propose ViewCLR, that learns self-supervised video representation invariant to camera viewpoint changes. We introduce a viewpoint-generator that can be considered as a learnable augmentation for any self-supervised pre-text tasks, to generate latent viewpoint representation of a video. ViewCLR maximizes the similarities between the representation of the latent viewpoint and that of the original viewpoint, enabling the learned video encoder to generalize over unseen camera viewpoints. Experiments on cross-view benchmark datasets including NTU RGB+D dataset show that ViewCLR stands as a state-of-the-art viewpoint invariant self-supervised method."
VirtualHome Action Genome: A Simulated Spatio-Temporal Scene Graph Dataset With Consistent Relationship Labels,"Yue Qiu, Yoshiki Nagasaki, Kensho Hara, Hirokatsu Kataoka, Ryota Suzuki, Kenji Iwata, Yutaka Satoh",National Institute of Advanced Industrial Science and Technology (AIST),100,Japan,0,,"Spatio-temporal scene graph generation is an essential task in household activity recognition that aims to identify human-object interactions. Constructing a dataset with per-frame object region and consistent relationship annotations requires extremely high labor costs. Existing datasets sparsely annotate frames sampled from videos, resulting in the lack of dense spatio-temporal correlation in videos. Additionally, existing datasets contain inconsistent relationship annotations, leading to the problem of learning ambiguous temporal associations. Moreover, existing datasets mainly discuss relationships that can be inferred from a single frame, ignoring the significance of temporal associations. To resolve those issues, we created a simulated dataset with per-frame consistent annotations and introduced a range of relationships requiring both spatial and temporal context. Most existing methods explore spatial correlations within single images and do not explicitly consider the dynamic changes across frames. Therefore, we proposed a tracking-based approach that explicitly grasps spatio-temporal human-object interactions while simultaneously localizing humans and objects. Our proposed approach achieved state-of-the-art performance on scene graph generation and outperformed existing methods in scene graph localization by large margins on the proposed dataset. Moreover, the experiments show the efficacy of pre-training on the proposed dataset while adapting to a previous benchmark consisting of real daily videos, indicating the potential of the proposed dataset in real-world scenarios.",https://openaccess.thecvf.com/content/WACV2023/html/Qiu_VirtualHome_Action_Genome_A_Simulated_Spatio-Temporal_Scene_Graph_Dataset_With_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qiu_VirtualHome_Action_Genome_A_Simulated_Spatio-Temporal_Scene_Graph_Dataset_With_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030905/,"['Location awareness', 'Limiting', 'Correlation', 'Costs', 'Annotations', 'Genomics', 'Transformers']","['Simulated Datasets', 'Scene Graph', 'Spatiotemporal Datasets', 'Spatio-temporal Scene Graphs', 'Single Image', 'Large Margin', 'Single Frame', 'Spatial Context', 'Action Recognition', 'Temporal Association', 'Temporal Context', 'Household Activities', 'Object Regions', 'Consistent Annotation', 'Daily Activities', 'Spatial Information', 'Temporal Changes', 'Bounding Box', 'Object Classification', 'Multiple Objects', 'Previous Frame', 'Current Frame', 'Object Tracking', 'Video Frames', 'Spatiotemporal Context', 'Tracking Framework', 'Genomic Datasets', 'Image Captioning', 'Linear Projection', 'Track Loss']","['Algorithms: Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Robotics']",3,"Spatio-temporal scene graph generation is an essential task in household activity recognition that aims to identify human-object interactions. Constructing a dataset with per-frame object region and consistent relationship annotations requires extremely high labor costs. Existing datasets sparsely annotate frames sampled from videos, resulting in the lack of dense spatio-temporal correlation in videos. Additionally, existing datasets contain inconsistent relationship annotations, leading to the problem of learning ambiguous temporal associations. Moreover, existing datasets mainly discuss relationships that can be inferred from a single frame, ignoring the significance of temporal associations. To resolve those issues, we created a simulated dataset with per-frame consistent annotations and introduced a range of relationships requiring both spatial and temporal context. Most existing methods explore spatial correlations within single images and do not explicitly consider the dynamic changes across frames. Therefore, we proposed a tracking-based approach that explicitly grasps spatio-temporal human-object interactions while simultaneously localizing humans and objects. Our proposed approach achieved state-of-the-art performance on scene graph generation and outperformed existing methods in scene graph localization by large margins on the proposed dataset. Moreover, the experiments show the efficacy of pre-training on the proposed dataset while adapting to a previous benchmark consisting of real daily videos, indicating the potential of the proposed dataset in real-world scenarios."
Vis2Rec: A Large-Scale Visual Dataset for Visit Recommendation,"Michaël Soumm, Adrian Popescu, Bertrand Delezoide","Amanda, 34 Avenue Des Champs Elysées, F-75008, Paris, France; Université Paris-Saclay, CEA, List, F-91120, Palaiseau, France",50,France,50,France,"Most recommendation datasets for tourism are restricted to one world region and rely on explicit data such as check-ins. However, in reality, tourists visit various places worldwide and document their trips primarily through photos. These images contain a wealth of raw information that can be used to capture users' preferences and recommend personalized content. Visual content was already used in past works, but no large-scale publicly-available dataset that gives access to users' personal images exists for recommender systems. As such a resource would open-up possibilities for new image-based recommendation algorithms, we introduce Vis2Rec, a new dataset based on visit data extracted from users' Flickr photographic streams, which includes over 7 million photos, 36k recognizable points of interest, and 14k user profiles. Google Landmarks v2 is used as an auxiliary dataset to identify points of interest in users' photos, using a state-of-the-art image-matching deep architecture. Image-based user profiles are then constituted by aggregating the points of interest detected for each user. In addition, ground truth visits were determined for the test subset in order to enable accurate evaluation. Finally, we benchmark Vis2Rec using various existing recommender systems, and discuss the possibilities opened up by the availability of user images, as well as the societal issues that come with them. Following good practice in dataset sharing, Vis2Rec is created using only freely distributable content, and additional anonymization is performed to ensure the privacy of users. The raw dataset and the preprocessed user profiles will be publicly available at https://github.com/MSoumm/Vis2Rec.",https://openaccess.thecvf.com/content/WACV2023/html/Soumm_Vis2Rec_A_Large-Scale_Visual_Dataset_for_Visit_Recommendation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Soumm_Vis2Rec_A_Large-Scale_Visual_Dataset_for_Visit_Recommendation_WACV_2023_paper.pdf,,https://github.com/MSoumm/Vis2Rec,,main,Poster,https://ieeexplore.ieee.org/document/10030739/,"['Visualization', 'Data privacy', 'Image databases', 'Semantics', 'Multimedia Web sites', 'Benchmark testing', 'Streaming media']","['Large-scale Datasets', 'Benchmark', 'Deep Architecture', 'Recommender Systems', 'User Profile', 'Test Subset', 'User Interest', 'Tourists Visit', 'Recommendation Algorithm', 'Training Set', 'Deep Learning', 'Data Visualization', 'Matrix Factorization', 'Types Of Errors', 'Text Data', 'Point Of Interest', 'Latent Space', 'Reference Image', 'Distribution Of Images', 'Daily Visits', 'Visual Matching', 'Creative Commons License', 'Implicit Feedback', 'Matching Score', 'Sparse Vector', 'Recommendation Method', 'Geolocated', 'User Side', 'Item Characteristics', 'Percentage Points']","['Algorithms: Vision + language and/or other modalities', 'Applications: Arts/games/social media']",1,"Most recommendation datasets for tourism are restricted to one world region and rely on explicit data such as checkins. However, in reality, tourists visit various places world-wide and document their trips primarily through photos. These images contain a wealth of raw information that can be used to capture users’ preferences and recommend personalized content. Visual content was already used in past works, but no large-scale publicly-available dataset that gives access to users’ personal images exists for recommender systems. As such a resource would open-up possibilities for new image-based recommendation algorithms, we introduce Vis2Rec, a new dataset based on visit data extracted from users’ Flickr photographic streams, which includes over 7 million photos, 36k recognizable points of interest, and 14k user profiles. Google Landmarks v2 is used as an auxiliary dataset to identify points of interest in users’ photos, using a state-of-the-art image-matching deep architecture. Image-based user profiles are then constituted by aggregating the points of interest detected for each user. In addition, ground truth visits were determined for the test subset in order to enable accurate evaluation. Finally, we benchmark Vis2Rec using various existing recommender systems, and discuss the possibilities opened up by the availability of user images, as well as the societal issues that come with them. Following good practice in dataset sharing, Vis2Rec is created using only freely distributable content, and additional anonymization is performed to ensure the privacy of users. The raw dataset and the preprocessed user profiles will be publicly available at https://github.com/MSoumm/Vis2Rec."
Vision Transformer for NeRF-Based View Synthesis From a Single Input Image,"Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, Ravi Ramamoorthi",MIT; NVIDIA; Google; UC San Diego,50,USA,50,USA,"Although neural radiance fields (NeRF) have shown impressive advances in novel view synthesis, most methods require multiple input images of the same scene with accurate camera poses. In this work, we seek to substantially reduce the inputs to a single unposed image. Existing approaches using local image features to reconstruct a 3D object often render blurry predictions at viewpoints distant from the source view. To address this, we propose to leverage both the global and local features to form an expressive 3D representation. The global features are learned from a vision transformer, while the local features are extracted from a 2D convolutional network. To synthesize a novel view, we train a multi-layer perceptron (MLP) network conditioned on the learned 3D representation to perform volume rendering. This novel 3D representation allows the network to reconstruct unseen regions without enforcing constraints like symmetry or canonical coordinate systems. Our method renders novel views from just a single input image, and generalizes across multiple object categories using a single model. Quantitative and qualitative evaluations demonstrate that the proposed method achieves state-of-the-art performance and renders richer details than existing approaches.",https://openaccess.thecvf.com/content/WACV2023/html/Lin_Vision_Transformer_for_NeRF-Based_View_Synthesis_From_a_Single_Input_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Lin_Vision_Transformer_for_NeRF-Based_View_Synthesis_From_a_Single_Input_WACV_2023_paper.pdf,https://cseweb.ucsd.edu/~viscomp/projects/VisionNeRF/,,2207.05736,main,Poster,https://ieeexplore.ieee.org/document/10030901/,"['Computer vision', 'Three-dimensional displays', 'Shape', 'Pose estimation', 'Feature extraction', 'Transformers', 'Cameras']","['Input Image', 'Single Image', 'Vision Transformer', 'View Synthesis', 'Single Input Image', 'Convolutional Network', 'Image Features', 'Local Features', 'Global Features', 'Multilayer Perceptron', '3D Representation', 'Camera Pose', 'Local Image Features', 'Neural Field', '2D Convolutional Network', 'Training Set', 'Convolutional Neural Network', 'Feature Maps', '2D Images', 'Global Information', 'Target View', '3D Point', 'Fine Details', 'Latent Code', 'Local Feature Extraction', 'Volume Rendering', '3D Position', 'Transformer Encoder', 'Occluded Regions']","['Algorithms: Computational photography', 'image and video synthesis', '3D computer vision']",46,"Although neural radiance fields (NeRF) have shown impressive advances in novel view synthesis, most methods require multiple input images of the same scene with accurate camera poses. In this work, we seek to substantially reduce the inputs to a single unposed image. Existing approaches using local image features to reconstruct a 3D object often render blurry predictions at viewpoints distant from the source view. To address this, we propose to leverage both the global and local features to form an expressive 3D representation. The global features are learned from a vision transformer, while the local features are extracted from a 2D convolutional network. To synthesize a novel view, we train a multi-layer perceptron (MLP) network conditioned on the learned 3D representation to perform volume rendering. This novel 3D representation allows the network to reconstruct unseen regions without enforcing constraints like symmetry or canonical coordinate systems. Our method renders novel views from just a single input image, and generalizes across multiple object categories using a single model. Quantitative and qualitative evaluations demonstrate that the proposed method achieves state-of-the-art performance and renders richer details than existing approaches. https://cseweb.ucsd.edu/ %7eviscomp/projects/VisionNeRF/"
Visualizing Global Explanations of Point Cloud DNNs,Hanxiao Tan,"AI Group, TU Dortmund",100,Germany,0,,"So far, few researchers have targeted the explainability of point cloud neural networks. Part of the explainability methods are not directly applicable to those networks due to the structural specifics. In this work, we show that Activation Maximization (AM) with traditional pixel-wise regularizations fails to generate human-perceptible global explanations for point cloud networks. We propose new generative model-based AM approaches to clearly outline the global explanations and enhance their comprehensibility. Additionally, we propose a composite evaluation metric to address the limitations of existing evaluating methods, which simultaneously takes into account activation value, diversity and perceptibility. Extensive experiments demonstrate that our generative-based AM approaches outperform regularization-based ones both qualitatively and quantitatively. To the best of our knowledge, this is the first work investigating global explainability of point cloud networks. Our code is available at: https://github.com/Explain3D/PointCloudAM.",https://openaccess.thecvf.com/content/WACV2023/html/Tan_Visualizing_Global_Explanations_of_Point_Cloud_DNNs_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Tan_Visualizing_Global_Explanations_of_Point_Cloud_DNNs_WACV_2023_paper.pdf,,https://github.com/Explain3D/PointCloudAM,2203.09505,main,Poster,https://ieeexplore.ieee.org/document/10030393/,"['Point cloud compression', 'Measurement', 'Knowledge engineering', 'Visualization', 'Computer vision', 'Codes', 'Neurons']","['Deep Neural Network', 'Point Cloud', 'Global Explanation', 'Model-based Approach', 'Part Of Method', 'General Method', 'Convolutional Layers', 'Output Layer', 'Gaussian Noise', 'Backpropagation', 'Local Optimum', 'Autoencoder', 'Latent Space', 'Gaussian Blur', 'Specific Instances', 'Random Initialization', 'Latent Vector', 'Encoder Layer', 'Discriminator Loss', 'Object Contour', 'Fréchet Inception Distance', 'Distance Loss', 'Point Cloud Model', 'Earth Mover’s Distance', 'Input Instance', 'Total Variance', 'Quantitative Evaluation', 'Decision Rules', 'Alternative Models']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', '3D computer vision']",3,"So far, few researchers have targeted the explainability of point cloud neural networks. Part of the explainability methods are not directly applicable to those networks due to the structural specifics. In this work, we show that Activation Maximization (AM) with traditional pixel-wise regularizations fails to generate human-perceptible global explanations for point cloud networks. We propose new generative model-based AM approaches to clearly outline the global explanations and enhance their comprehensibility. Additionally, we propose a composite evaluation metric to address the limitations of existing evaluating methods, which simultaneously takes into account activation value, diversity and perceptibility. Extensive experiments demonstrate that our generative-based AM approaches outperform regularization-based ones both qualitatively and quantitatively. To the best of our knowledge, this is the first work investigating global explainability of point cloud networks. Our code is available at: https://github.com/Explain3D/PointCloudAM."
Visually Explaining 3D-CNN Predictions for Video Classification With an Adaptive Occlusion Sensitivity Analysis,"Tomoki Uchiyama, Naoya Sogi, Koichiro Niinuma, Kazuhiro Fukui",University of Tsukuba; Fujitsu Research of America,50,Japan,50,USA,"This paper proposes a method for visually explaining the decision-making process of 3D convolutional neural networks (CNN) with a temporal extension of occlusion sensitivity analysis. The key idea here is to occlude a specific volume of data by a 3D mask in an input 3D temporal-spatial data space and then measure the change degree in the output score. The occluded volume data that produces a larger change degree is regarded as a more critical element for classification. However, while the occlusion sensitivity analysis is commonly used to analyze single image classification, it is not so straightforward to apply this idea to video classification as a simple fixed cuboid cannot deal with the motions. To this end, we adapt the shape of a 3D occlusion mask to complicated motions of target objects. Our flexible mask adaptation is performed by considering the temporal continuity and spatial co-occurrence of the optical flows extracted from the input video data. We further propose to approximate our method by using the first-order partial derivative of the score with respect to an input image to reduce its computational cost. We demonstrate the effectiveness of our method through various and extensive comparisons with the conventional methods in terms of the deletion/insertion metric and the pointing metric on the UCF-101. The code is available at: https://github.com/uchiyama33/AOSA.",https://openaccess.thecvf.com/content/WACV2023/html/Uchiyama_Visually_Explaining_3D-CNN_Predictions_for_Video_Classification_With_an_Adaptive_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Uchiyama_Visually_Explaining_3D-CNN_Predictions_for_Video_Classification_With_an_Adaptive_WACV_2023_paper.pdf,,https://github.com/uchiyama33/AOSA,2207.12859,main,Poster,https://ieeexplore.ieee.org/document/10030359/,"['Three-dimensional displays', 'Sensitivity analysis', 'Shape', 'Volume measurement', 'Decision making', 'Extraterrestrial measurements', 'Computational efficiency']","['Sensitivity Analysis', 'Video Analysis', 'Occlusion Sensitivity', 'Neural Network', 'Computational Cost', 'Convolutional Neural Network', 'Partial Differential', 'Single Image', 'Optical Flow', 'Output Score', 'Temporal Continuity', 'Time Series', 'Random Sampling', 'Deep Neural Network', 'Feature Maps', 'First Approximation', 'Bounding Box', 'Taylor Expansion', 'Action Recognition', 'Network Inference', 'Saliency Map', 'Occluded Regions', 'Automatic Differentiation', 'Original Score', 'Image X', 'Naive Approach', 'Skateboarding', 'Temporal Direction', 'Convolutional Neural Network Prediction', 'Time Series Variables']","['Algorithms: Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)']",8,"This paper proposes a method for visually explaining the decision-making process of 3D convolutional neural networks (CNN) with a temporal extension of occlusion sensitivity analysis. The key idea here is to occlude a specific volume of data by a 3D mask in an input 3D temporalspatial data space and then measure the change degree in the output score. The occluded volume data that produces a larger change degree is regarded as a more critical element for classification. However, while the occlusion sensitivity analysis is commonly used to analyze single image classification, it is not so straightforward to apply this idea to video classification as a simple fixed cuboid cannot deal with the motions. To this end, we adapt the shape of a 3D occlusion mask to complicated motions of target objects. Our flexible mask adaptation is performed by considering the temporal continuity and spatial co-occurrence of the optical flows extracted from the input video data. We further propose to approximate our method by using the first-order partial derivative of the score with respect to an input image to reduce its computational cost. We demonstrate the effectiveness of our method through various and extensive comparisons with the conventional methods in terms of the deletion/insertion metric and the pointing metric on the UCF101. The code is available at: https://github.com/uchiyama33/AOSA."
WHFL: Wavelet-Domain High Frequency Loss for Sketch-to-Image Translation,"Min Woo Kim, Nam Ik Cho","Department of ECE, INMC, Seoul National University, Seoul, Korea",100,South Korea,0,,"Even a rough sketch can effectively convey the descriptions of objects, as humans can imagine the original shape from the sketch. The sketch-to-photo translation is a computer vision task that enables a machine to do this imagination, taking a binary sketch image and generating plausible RGB images corresponding to the sketch. Hence, deep neural networks for this task should learn to generate a wide range of frequencies because most parts of the input (binary sketch image) are composed of DC signals. In this paper, we propose a new loss function named Wavelet-domain High-Frequency Loss (WHFL) to overcome the limitations of previous methods that tend to have a bias toward low frequencies. The proposed method emphasizes the loss on the high frequencies by designing a new weight matrix imposing larger weights on the high bands. Unlike existing hand-craft methods that control frequency weights using binary masks, we use the matrix with finely controlled elements according to frequency scales. The WHFL is designed in a multi-scale form, which lets the loss function focus more on the high frequency according to decomposition levels. We use the WHFL as a complementary loss in addition to conventional ones defined in the spatial domain. Experiments show we can improve the qualitative and quantitative results in both spatial and frequency domains. Additionally, we attempt to verify the WHFL's high-frequency generation capability by defining a new evaluation metric named Unsigned Euclidean Distance Field Error (UEDFE).",https://openaccess.thecvf.com/content/WACV2023/html/Kim_WHFL_Wavelet-Domain_High_Frequency_Loss_for_Sketch-to-Image_Translation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Kim_WHFL_Wavelet-Domain_High_Frequency_Loss_for_Sketch-to-Image_Translation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030776/,"['Measurement', 'Wavelet transforms', 'Computer vision', 'Shape', 'Image synthesis', 'Neural networks', 'High frequency']","['Loss Function', 'Frequency Domain', 'Weight Matrix', 'Spatial Domain', 'Part Of The Input', 'Decomposition Level', 'Convolutional Neural Network', 'Frequency Band', 'Matrix Elements', 'Distance Matrix', 'Wavelet Transform', 'Generative Adversarial Networks', 'Discrete Fourier Transform', 'Low-frequency Band', 'Image Retrieval', 'Frequent Interruptions', 'Focal Loss', 'Spectral Position', 'Differences In Bands', 'Canny Edge Detection', 'L2 Loss', 'Generative Adversarial Network Framework', 'Diagonal Direction', 'Spectral Energy', 'Source Domain', 'Euclidean Space', 'Image Generation', 'Natural Images']","['Algorithms: Computational photography', 'image and video synthesis', 'Adversarial learning', 'adversarial attack and defense methods']",5,"Even a rough sketch can effectively convey the descriptions of objects, as humans can imagine the original shape from the sketch. The sketch-to-photo translation is a computer vision task that enables a machine to do this imagination, taking a binary sketch image and generating plausible RGB images corresponding to the sketch. Hence, deep neural networks for this task should learn to generate a wide range of frequencies because most parts of the input (binary sketch image) are composed of DC signals. In this paper, we propose a new loss function named Wavelet-domain High-Frequency Loss (WHFL) to overcome the limitations of previous methods that tend to have a bias toward low frequencies. The proposed method emphasizes the loss on the high frequencies by designing a new weight matrix imposing larger weights on the high bands. Unlike existing handcraft methods that control frequency weights using binary masks, we use the matrix with finely controlled elements according to frequency scales. The WHFL is designed in a multi-scale form, which lets the loss function focus more on the high frequency according to decomposition levels. We use the WHFL as a complementary loss in addition to conventional ones defined in the spatial domain. Experiments show we can improve the qualitative and quantitative results in both spatial and frequency domains. Additionally, we attempt to verify the WHFL’s high-frequency generation capability by defining a new evaluation metric named Unsigned Euclidean Distance Field Error (UEDFE)."
WSNet: Towards an Effective Method for Wound Image Segmentation,"Subba Reddy Oota, Vijay Rowtula, Shahid Mohammed, Minghsun Liu, Manish Gupta","Woundtech Innovative Healthcare Solutions; IIIT-Hyderabad, India; Woundtech Innovative Healthcare Solutions, IIIT-Hyderabad",66.66666667,India,33.33333333,USA,"Medical image segmentation is critical for effective computer-aided diagnosis and localization of ailments. Automated segmentation of wound regions from patient images can aid clinicians in measuring and managing chronic wounds and monitoring the wound healing trajectory. While there exists a plethora of work on general medical image segmentation, there is hardly any work on wound image analysis and segmentation. Existing methods are limited to segmenting a smaller subset of ulcers, such as foot ulcers, with no special processing for wound images. In this paper, we build segmentation models for eight different types of wound images. Wound image analysis is a challenging problem due to the lack of availability of extensive data (labeled or unlabeled) and annotation challenges due to the shortage of well-trained wound care clinicians. To handle these challenges, we contribute WoundSeg, a large and diverse dataset of segmented wound images. Generic wound image segmentation is complex due to the heterogeneous appearance of wound area across images of similar wound types. We propose a novel image segmentation framework, WSNet, which leverages (a) wound-domain adaptive pretraining on a large unlabeled wound image collection and (b) a global-local architecture that utilizes full image and its patches to learn fine-grained details of heterogeneous wounds. On WoundSeg, we achieve a decent Dice score of 0.847. On existing AZH Woundcare and Medetec datasets, we establish a new state-of-the-art. Further, we show the impact of using segmentation for improving the accuracy of downstream tasks like wound area and volume prediction.",https://openaccess.thecvf.com/content/WACV2023/html/Oota_WSNet_Towards_an_Effective_Method_for_Wound_Image_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Oota_WSNet_Towards_an_Effective_Method_for_Wound_Image_Segmentation_WACV_2023_paper.pdf,,https://github.com/subbareddy248/WSNETr,,main,Poster,https://ieeexplore.ieee.org/document/10030591/,"['Image segmentation', 'Wireless sensor networks', 'Adaptation models', 'Image analysis', 'Computer architecture', 'Wounds', 'Trajectory']","['Image Segmentation', 'Wound Images', 'Large Datasets', 'Type Of Injury', 'Segmentation Model', 'Wound Area', 'Foot Ulcers', 'Medical Image Segmentation', 'Dice Score', 'Wound Region', 'Convolutional Neural Network', 'K-means', 'Localization Signal', 'Local Network', 'Logarithmic Scale', 'Adam Optimizer', 'Data Augmentation', 'Intersection Over Union', 'Independent Component Analysis', 'Intersection Over Union Score', 'Traditional Machine Learning Methods', 'Accuracy Trade-off', 'ImageNet Pretraining', 'Dice Loss', 'Traditional Machine Learning', 'Architecture For Segmentation', 'Masked Images', 'Accuracy Of Image Segmentation', 'Image Segmentation Tasks']","['Applications: Biomedical/healthcare/medicine', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",3,"Medical image segmentation is critical for effective computer-aided diagnosis and localization of ailments. Automated segmentation of wound regions from patient images can aid clinicians in measuring and managing chronic wounds and monitoring the wound healing trajectory. While there exists a plethora of work on general medical image segmentation, there is hardly any work on wound image analysis and segmentation. Existing methods are limited to segmenting a smaller subset of ulcers, such as foot ulcers, with no special processing for wound images. In this paper, we build segmentation models for eight different types of wound images. Wound image analysis is a challenging problem due to the lack of availability of extensive data (labeled or unlabeled), and annotation is also challenging due to the shortage of well-trained wound care clinicians. To handle these challenges, we contribute WoundSeg
<sup>1</sup>
, a large and diverse dataset of segmented wound images. Generic wound image segmentation is complex due to the heterogeneous appearance of wound area across images of similar wound types. We propose a novel image segmentation framework, WSNet, which leverages (a) wound-domain adaptive pretraining on a large unlabeled wound image collection and (b) a global-local architecture that utilizes full image and its patches to learn fine-grained details of heterogeneous wounds. On WoundSeg, we achieve a decent Dice score of 0.847. On existing AZH Woundcare and Medetec datasets, we establish a new state-of-the-art. Further, we show the impact of using segmentation for improving the accuracy of downstream tasks like wound area and volume prediction."
Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion,"Shruti Agarwal, Liwen Hu, Evonne Ng, Trevor Darrell, Hao Li, Anna Rohrbach","Pinscreen, Inc.; University of California, Berkeley",50,USA,50,USA,"In today's era of digital misinformation, we are increasingly faced with new threats posed by video falsification techniques. Such falsifications range from cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g., sophisticated AI media synthesis methods), which are becoming perceptually indistinguishable from real videos. To tackle this challenge, we propose a multi-modal semantic forensic approach to discover clues that go beyond detecting discrepancies in visual quality, thereby handling both simpler cheapfakes and visually persuasive deepfakes. In this work, our goal is to verify that the purported person seen in the video is indeed themselves by detecting anomalous facial movements corresponding to the spoken words. We leverage the idea of attribution to learn person-specific biometric patterns that distinguish a given speaker from others. We use interpretable Action Units (AUs) to capture a persons' face and head movement as opposed to deep CNN features, and we are the first to use word-conditioned facial motion analysis. We further demonstrate our method's effectiveness on a range of fakes not seen in training including those without video manipulation, that were not addressed in prior work.",https://openaccess.thecvf.com/content/WACV2023/html/Agarwal_Watch_Those_Words_Video_Falsification_Detection_Using_Word-Conditioned_Facial_Motion_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Watch_Those_Words_Video_Falsification_Detection_Using_Word-Conditioned_Facial_Motion_WACV_2023_paper.pdf,,,2112.10936,main,Poster,https://ieeexplore.ieee.org/document/10030899/,"['Training', 'Deepfakes', 'Visualization', 'Computer vision', 'Biometrics (access control)', 'Forensics', 'Semantics']","['Forensic', 'Spoken Language', 'Head Movements', 'Visual Quality', 'Use Of Video', 'Facial Movements', 'Action Units', 'Semantic Approach', 'Real Videos', 'Deepfake', 'Training Data', 'Training Dataset', 'Gestures', 'Facial Expressions', 'Visual Features', 'Video Clips', 'Facial Features', 'Linear Classifier', 'Specific Words', 'Total Hours', 'Hours Of Video', 'Fake Data', 'Multimodal Techniques', 'Real Training', 'World Leaders', 'Biometric Characteristics', 'Target Person', 'Test Videos']","['Applications: Social good', 'Biometrics', 'face', 'gesture', 'body pose']",12,"In today’s era of digital misinformation, we are increasingly faced with new threats posed by video falsification techniques. Such falsifications range from cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g., sophisticated AI media synthesis methods), which are becoming perceptually indistinguishable from real videos. To tackle this challenge, we propose a multi-modal semantic forensic approach to discover clues that go beyond detecting discrepancies in visual quality, thereby handling both simpler cheapfakes and visually persuasive deepfakes. In this work, our goal is to verify that the purported person seen in the video is indeed themselves by detecting anomalous facial movements corresponding to the spoken words. We leverage the idea of attribution to learn person-specific biometric patterns that distinguish a given speaker from others. We use interpretable Action Units (AUs) to capture a person’s face and head movement as opposed to deep CNN features, and we are the first to use word-conditioned facial motion analysis. We further demonstrate our method’s effectiveness on a range of fakes not seen in training including those without video manipulation, that were not addressed in prior work."
Watching the News: Towards VideoQA Models That Can Read,"Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar","Computer Vision Center, UAB, Spain; CVIT, IIIT Hyderabad, India",100,"India, Spain",0,,"Video Question Answering methods focus on common-sense reasoning and visual cognition of objects or persons and their interactions over time. Current VideoQA approaches ignore the textual information present in the video. Instead, we argue that textual information is complementary to the action and provides essential contextualisation cues to the reasoning process. To this end, we propose a novel VideoQA task that requires reading and understanding the text in the video. To explore this direction, we focus on news videos and require QA systems to comprehend and answer questions about the topics presented by combining visual and textual cues in the video. We introduce the ""NewsVideoQA"" dataset that comprises more than 8,600 QA pairs on 3, 000+ news videos obtained from diverse news channels from around the world. We demonstrate the limitations of current Scene Text VQA and VideoQA methods and propose ways to incorporate scene text information into VideoQA methods.",https://openaccess.thecvf.com/content/WACV2023/html/Jahagirdar_Watching_the_News_Towards_VideoQA_Models_That_Can_Read_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Jahagirdar_Watching_the_News_Towards_VideoQA_Models_That_Can_Read_WACV_2023_paper.pdf,,,2211.05588,main,Poster,https://ieeexplore.ieee.org/document/10030929/,"['Visualization', 'Computer vision', 'Computational modeling', 'Optical character recognition', 'Question answering (information retrieval)', 'Task analysis', 'Commonsense reasoning']","['Question Answering', 'Textual Information', 'Reasoning Process', 'Television News', 'Visual Question Answering', 'Fine-tuned', 'Decoding', 'Sampling Frame', 'Object Detection', 'Visual Features', 'Bounding Box', 'Video Clips', 'Latent Space', 'Video Frames', 'Appearance Features', 'Video Content', 'Visual Content', 'Test Split', 'Common Words', 'Automatic Generation', 'Pre-training Stage', 'Masked Language Model', 'Fine-tuning Stage', 'Official Implementation', 'Common Answer', 'Annotation Process', 'Single Image', 'Vocabulary', 'Cloud Data', 'Baseline Methods']","['Algorithms: Vision + language and/or other modalities', 'Video recognition and understanding (tracking', 'action recognition', 'etc.)', 'Arts/games/social media']",4,"Video Question Answering methods focus on common-sense reasoning and visual cognition of objects or persons and their interactions over time. Current VideoQA approaches ignore the textual information present in the video. Instead, we argue that textual information is complementary to the action and provides essential contextualisation cues to the reasoning process. To this end, we propose a novel VideoQA task that requires reading and understanding the text in the video. To explore this direction, we focus on news videos and require QA systems to comprehend and answer questions about the topics presented by combining visual and textual cues in the video. We introduce the ""NewsVideoQA"" dataset that comprises more than 8, 600 QA pairs on 3, 000+ news videos obtained from diverse news channels from around the world. We demonstrate the limitations of current Scene Text VQA and VideoQA methods and propose ways to incorporate scene text information into VideoQA methods."
Wavelength-Aware 2D Convolutions for Hyperspectral Imaging,"Leon Amadeus Varga, Martin Messmer, Nuri Benbarka, Andreas Zell","Cognitive Systems Group, University of Tuebingen, Tuebingen, Germany",100,Germany,0,,"Deep Learning could drastically boost the classification accuracy for Hyperspectral Imaging (HSI). Still, the training on the mostly small hyperspectral data sets is not trivial. Two key challenges are the large channel dimension of the recordings and the incompatibility between cameras of different manufacturers. By introducing a suitable model bias and continuously defining the channel dimension, we propose a 2D convolution optimized for these challenges of Hyperspectral Imaging. We evaluate the method based on two different hyperspectral applications (inline inspection and remote sensing). Besides the shown superiority of the model, the modification adds additional explanatory power. In addition, the model learns the necessary camera filters in a data-driven manner. Based on these camera filters, an optimal camera can be designed.",https://openaccess.thecvf.com/content/WACV2023/html/Varga_Wavelength-Aware_2D_Convolutions_for_Hyperspectral_Imaging_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Varga_Wavelength-Aware_2D_Convolutions_for_Hyperspectral_Imaging_WACV_2023_paper.pdf,,https://github.com/cogsys-tuebingen/hyve_conv,2209.03136,main,Poster,https://ieeexplore.ieee.org/document/10030284/,"['Training', 'Deep learning', 'Convolution', 'Image color analysis', 'Inspection', 'Cameras', 'Recording']","['Small Datasets', 'Remote Sensing', 'Channel Dimension', 'Normal Distribution', 'Convolutional Neural Network', 'Dimensionality Reduction', 'Convolutional Layers', 'Wavelength Range', 'First Category', 'Linear Interpolation', 'Trainable Parameters', 'Input Channels', 'Avocado', '3D Convolution', 'Indian Pines', 'Vision Transformer', 'Similar Wavelength', 'Multispectral Camera', 'Kernel Similarity', 'Near-infrared Camera', 'Wavelength Channels', 'Pavia University', 'Hyperspectral Cube', 'Color Images', 'Fruit Type', 'Average Accuracy', 'Ripening']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Food science and nutrition', 'Remote Sensing']",,"Deep Learning could drastically boost the classification accuracy for Hyperspectral Imaging (HSI). Still, the training on the mostly small hyperspectral data sets is not trivial. Two key challenges are the large channel dimension of the recordings and the incompatibility between cameras of different manufacturers.By introducing a suitable model bias and continuously defining the channel dimension, we propose a 2D convolution optimized for these challenges of Hyperspectral Imaging. We evaluate the method based on two different hyperspectral applications (inline inspection and remote sensing). Besides the shown superiority of the model, the modification adds additional explanatory power.In addition, the model learns the necessary camera filters in a data-driven manner. Based on these camera filters, an optimal camera can be designed."
Weakly Supervised Cell-Instance Segmentation With Two Types of Weak Labels by Single Instance Pasting,"Kazuya Nishimura, Ryoma Bise","Kyushu University, Fukuoka, Japan",100,Japan,0,,"Cell instance segmentation that recognizes each cell boundary is an important task in cell image analysis. While deep learning-based methods have shown promising performances with a certain amount of training data, most of them require full annotations that show the boundary of each cell. Generating the annotation for cell segmentation is time-consuming and human labor. To reduce the annotation cost, we propose a weakly supervised segmentation method using two types of weak labels (one for cell type and one for nuclei position). Unlike general images, these two labels are easily obtained in phase-contrast images. The intercellular boundary, which is necessary for cell instance segmentation, cannot be directly obtained from these two weak labels, so to generate the boundary information, we propose a single instance pasting based on the copy-and-paste technique. First, we locate single-cell regions by counting cells and store them in a pool. Then, we generate the intercellular boundary by pasting the stored single-cell regions to the original image. Finally, we train a boundary estimation network with the generated labels and perform instance segmentation with the network. Our evaluation on a public dataset demonstrated that the proposed method achieves the best performance among the several weakly supervised methods we compared.",https://openaccess.thecvf.com/content/WACV2023/html/Nishimura_Weakly_Supervised_Cell-Instance_Segmentation_With_Two_Types_of_Weak_Labels_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Nishimura_Weakly_Supervised_Cell-Instance_Segmentation_With_Two_Types_of_Weak_Labels_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030481/,"['Learning systems', 'Image segmentation', 'Computer vision', 'Costs', 'Image recognition', 'Image analysis', 'Annotations']","['Single Instance', 'Type Labels', 'Weak Labels', 'Cell Types', 'Training Data', 'Cell Binding', 'Segmentation Method', 'Amount Of Training Data', 'Deep Learning-based Methods', 'Estimation Network', 'Cell Segmentation', 'Instance Segmentation', 'Human Labor', 'Position Of Nucleus', 'Boundary Information', 'Boundary Estimation', 'Additional Costs', 'Multiple Regions', 'Class Labels', 'Color Images', 'Foreground Pixels', 'Typical Microscope', 'Instance Segmentation Methods', 'Bounding Box', 'Unknown Regions', 'White Pixels', 'Foreground Regions', 'Color Information', 'Image Pairs', 'Class Instances']","['Applications: Biomedical/healthcare/medicine', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",2,"Cell instance segmentation that recognizes each cell boundary is an important task in cell image analysis. While deep learning-based methods have shown promising performances with a certain amount of training data, most of them require full annotations that show the boundary of each cell. Generating the annotation for cell segmentation is time-consuming and human labor. To reduce the annotation cost, we propose a weakly supervised segmentation method using two types of weak labels (one for cell type and one for nuclei position). Unlike general images, these two labels are easily obtained in phase-contrast images. The intercellular boundary, which is necessary for cell instance segmentation, cannot be directly obtained from these two weak labels, so to generate the boundary information, we propose a single instance pasting based on the copy-and-paste technique. First, we locate single-cell regions by counting cells and store them in a pool. Then, we generate the intercel-lular boundary by pasting the stored single-cell regions to the original image. Finally, we train a boundary estimation network with the generated labels and perform instance segmentation with the network. Our evaluation on a public dataset demonstrated that the proposed method achieves the best performance among the several weakly supervised methods we compared."
Weakly Supervised Face Naming With Symmetry-Enhanced Contrastive Loss,"Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens","Department of Computer Science, KU Leuven; Department of Electrical Engineering, KU Leuven",100,Belgium,0,,"We revisit the weakly supervised cross-modal face-name alignment task; that is, given an image and a caption, we label the faces in the image with the names occurring in the caption. Whereas past approaches have learned the latent alignment between names and faces by uncertainty reasoning over a set of images and their respective captions, in this paper, we rely on appropriate loss functions to learn the alignments in a neural network setting and propose SECLA and SECLA-B. SECLA is a Symmetry-Enhanced Contrastive Learning-based Alignment model that can effectively maximize the similarity scores between corresponding faces and names in a weakly supervised fashion. A variation of the model, SECLA-B, learns to align names and faces as humans do, that is, learning from easy to hard cases to further increase the performance of SECLA. More specifically, SECLA-B applies a two-stage learning framework: (1) Training the model on an easy subset with a few names and faces in each image-caption pair. (2) Leveraging the known pairs of names and faces from the easy cases using a bootstrapping strategy with additional loss to prevent forgetting and learning new alignments at the same time. We achieve state-of-the-art results for both the augmented Labeled Faces in the Wild dataset and the Celebrity Together dataset. In addition, we believe that our methods can be adapted to other multimodal news understanding tasks.",https://openaccess.thecvf.com/content/WACV2023/html/Qu_Weakly_Supervised_Face_Naming_With_Symmetry-Enhanced_Contrastive_Loss_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Qu_Weakly_Supervised_Face_Naming_With_Symmetry-Enhanced_Contrastive_Loss_WACV_2023_paper.pdf,,,2210.08957,main,Poster,https://ieeexplore.ieee.org/document/10030289/,"['Training', 'Computer vision', 'Adaptation models', 'Uncertainty', 'Computational modeling', 'Neural networks', 'Cognition']","['Contrastive Loss', 'Face Name', 'Loss Function', 'Similarity Score', 'Face Images', 'F1 Score', 'Multilayer Perceptron', 'Projector', 'Bounding Box', 'Face Recognition', 'Latent Space', 'External Data', 'Self-supervised Learning', 'Two-stage Strategy', 'Correct Alignment', 'Alignment Strategy', 'Image Captioning', 'Early Stage Of Training', 'Hinge Loss', 'Face Matching', 'Face Pairs', 'Average Face', 'Simple Heuristics', 'Perfect Results', 'Disambiguation', 'Semantic', 'Highest Similarity Score', 'Face Type']","['Algorithms: Vision + language and/or other modalities', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",1,"We revisit the weakly supervised cross-modal face-name alignment task; that is, given an image and a caption, we label the faces in the image with the names occurring in the caption. Whereas past approaches have learned the latent alignment between names and faces by uncertainty reasoning over a set of images and their respective captions, in this paper, we rely on appropriate loss functions to learn the alignments in a neural network setting and propose SECLA and SECLA-B.SECLA is a Symmetry-Enhanced Contrastive Learning-based Alignment model that can effectively maximize the similarity scores between corresponding faces and names in a weakly supervised fashion. A variation of the model, SECLA-B, learns to align names and faces as humans do, that is, learning from easy to hard cases to further increase the performance of SECLA. More specifically, SECLA-B applies a two-stage learning framework: (1) Training the model on an easy subset with a few names and faces in each image-caption pair. (2) Leveraging the known pairs of names and faces from the easy cases using a bootstrapping strategy with additional loss to prevent forgetting and learning new alignments at the same time. We achieve state-of-the-art results for both the augmented Labeled Faces in the Wild dataset and the Celebrity Together dataset. In addition, we believe that our methods can be adapted to other multimodal news understanding tasks."
Weakly-Supervised Optical Flow Estimation for Time-of-Flight,"Michael Schelling, Pedro Hermosilla, Timo Ropinski","Ulm University, Germany",100,Germany,0,,"Indirect Time-of-Flight (iToF) cameras are a widespread type of 3D sensor, which perform multiple captures to obtain depth values of the captured scene. While recent approaches to correct iToF depths achieve high performance when removing multi-path-interference and sensor noise, little research has been done to tackle motion artifacts. In this work we propose a training algorithm, which allows to supervise Optical Flow (OF) networks directly on the reconstructed depth, without the need of having ground truth flows. We demonstrate that this approach enables the training of OF networks to align raw iToF measurements and compensate motion artifacts in the iToF depth images. The approach is evaluated for both single- and multi-frequency sensors as well as multi-tap sensors, and is able to outperform other motion compensation techniques.",https://openaccess.thecvf.com/content/WACV2023/html/Schelling_Weakly-Supervised_Optical_Flow_Estimation_for_Time-of-Flight_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Schelling_Weakly-Supervised_Optical_Flow_Estimation_for_Time-of-Flight_WACV_2023_paper.pdf,,https://github.com/schellmi42/WFlowToF,2210.05298,main,Poster,https://ieeexplore.ieee.org/document/10030933/,"['Training', 'Optical losses', 'Computer vision', 'Three-dimensional displays', 'Optical variables measurement', 'Motion compensation', 'Motion measurement']","['Optical Flow', 'High Performance', 'Motion Artifacts', 'Depth Images', 'Network Flow', 'Raw Measurements', 'Sensor Noise', 'Motion Compensation', 'Time Step', 'Similarity Measure', 'Phase Shift', 'Lookup Table', 'Similar Loss', 'Numerical Stability', 'Loss Of Components', 'Latent Vector', 'Flow Prediction', 'Phase Unwrapping', 'Regularization Loss', 'Set Loss', 'Cost Volume', 'Time-of-flight Sensors', 'Smoothness Loss', 'Subsequent Time Steps']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', '3D computer vision']",3,"Indirect Time-of-Flight (iToF) cameras are a widespread type of 3D sensor, which perform multiple captures to obtain depth values of the captured scene. While recent approaches to correct iToF depths achieve high performance when removing multi-path-interference and sensor noise, little research has been done to tackle motion artifacts. In this work we propose a training algorithm, which allows to supervise Optical Flow (OF) networks directly on the reconstructed depth, without the need of having ground truth flows. We demonstrate that this approach enables the training of OF networks to align raw iToF measurements and compensate motion artifacts in the iToF depth images. The approach is evaluated for both single- and multi-frequency sensors as well as multi-tap sensors, and is able to outperform other motion compensation techniques."
Weakly-Supervised Point Cloud Instance Segmentation With Geometric Priors,"Heming Du, Xin Yu, Farookh Hussain, Mohammad Ali Armin, Lars Petersson, Weihao Li","Australian National University; Data61, CSIRO; University of Technology Sydney",100,Australia,0,,"This paper investigates how to leverage more readily acquired annotations, i.e., 3D bounding boxes instead of dense point-wise labels, for instance segmentation. We propose a Weakly-supervised point cloud Instance Segmentation framework with Geometric Priors (WISGP) that allows segmentation models to be trained with 3D bounding boxes of instances. Considering intersections among bounding boxes in a scene would result in ambiguous labels, we first group points into two sets, i.e., univocal and equivocal sets, indicating the certainty of a 3D point belonging to an instance, respectively. Specifically, 3D points with clear labels belong to the univocal set while the rest are grouped into the equivocal set. To assign reliable labels to points in the equivocal set, we design a Geometry-guided Label Propagation (GLP) scheme that progressively propagates labels to linked points based on geometric structure, e.g., polygon meshes and superpoints. Afterwards, we train an instance segmentation model with the univocal points and equivocal points labeled by GLP, and then employ it to assign pseudo labels for the remainder of the unlabeled points. Lastly, we retrain the model with all the labeled points to achieve better instance segmentation performance. Experiments on large-scale datasets ScanNet-v2 and S3DIS demonstrate that WISGP is superior to competing weakly-supervised algorithms and even on par with a few fully-supervised ones.",https://openaccess.thecvf.com/content/WACV2023/html/Du_Weakly-Supervised_Point_Cloud_Instance_Segmentation_With_Geometric_Priors_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Du_Weakly-Supervised_Point_Cloud_Instance_Segmentation_With_Geometric_Priors_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030410/,"['Point cloud compression', 'Solid modeling', 'Computer vision', 'Three-dimensional displays', 'Annotations', 'Computational modeling', 'Semantics']","['Point Cloud', 'Instance Segmentation', 'Point Cloud Instance Segmentation', 'Bounding Box', 'Segmentation Model', 'Set Point', '3D Point', '3D Mesh', 'Segmentation Performance', 'Pseudo Labels', 'Label Propagation', '3D Bounding Box', 'Semantic Similarity', 'Semantic Segmentation', 'Clusters Of Points', 'Neighboring Points', 'Semantic Network', 'Final Segmentation', 'Semantic Labels', '3D Segmentation', 'Point Labels', 'Instance Labels', 'Bounding Box Annotations', 'Spatial Similarity', 'Marching Cubes Algorithm', 'Intersection Area', 'Inclusion Relation', 'Background Points', 'Multiple Boxes']",['Algorithms: 3D computer vision'],7,"This paper investigates how to leverage more readily acquired annotations, i.e., 3D bounding boxes instead of dense point-wise labels, for instance segmentation. We propose a Weakly-supervised point cloud Instance Segmentation framework with Geometric Priors (WISGP) that allows segmentation models to be trained with 3D bounding boxes of instances. Considering intersections among bounding boxes in a scene would result in ambiguous la- bels, we first group points into two sets, i.e., univocal and equivocal sets, indicating the certainty of a 3D point belonging to an instance, respectively. Specifically, 3D points with clear labels belong to the univocal set while the rest are grouped into the equivocal set. To assign reliable labels to points in the equivocal set, we design a Geometry-guided Label Propagation (GLP) scheme that progressively propagates labels to linked points based on geometric structure, e.g., polygon meshes and superpoints. Afterwards, we train an instance segmentation model with the univocal points and equivocal points labeled by GLP, and then employ it to assign pseudo labels for the remainder of the unlabeled points. Lastly, we retrain the model with all the labeled points to achieve better instance segmentation performance. Experiments on large-scale datasets ScanNet-v2 and S3DIS demonstrate that WISGP is superior to competing weakly-supervised algorithms and even on par with a few fully-supervised ones."
What Can We Learn by Predicting Accuracy?,"Olivier Risser-Maroix, Benjamin Chamand","LIPADE, Universit ´e Paris Cit ´e, France; IRIT, Universite de Toulouse, CNRS, Toulouse INP, UT3, Toulouse France",100,France,0,,"This paper seeks to answer the following question: ""What can we learn by predicting accuracy?"". Indeed, classification is one of the most popular tasks in machine learning, and many loss functions have been developed to maximize this non-differentiable objective function. Unlike past work on loss function design, which was guided mainly by intuition and theory before being validated by experimentation, here we propose to approach this problem in the opposite way: we seek to extract knowledge by experimentation. This data-driven approach is similar to that used in physics to discover general laws from data. We used a symbolic regression method to automatically find a mathematical expression highly correlated with a linear classifier's accuracy. The formula discovered on more than 260 datasets of embeddings has a Pearson's correlation of 0.96 and a r2 of 0.93. More interestingly, this formula is highly explainable and confirms insights from various previous papers on loss design. We hope this work will open new perspectives in the search for new heuristics leading to a deeper understanding of machine learning theory.",https://openaccess.thecvf.com/content/WACV2023/html/Risser-Maroix_What_Can_We_Learn_by_Predicting_Accuracy_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Risser-Maroix_What_Can_We_Learn_by_Predicting_Accuracy_WACV_2023_paper.pdf,,,2208.01358,main,Poster,https://ieeexplore.ieee.org/document/10030209/,"['Computer vision', 'Correlation', 'Pipelines', 'Machine learning', 'Feature extraction', 'Linear programming', 'Task analysis']","['Prediction Accuracy', 'Pearson Correlation', 'Machine Learning', 'Linear Classifier', 'Machine Learning Tasks', 'Linear Model', 'Logarithm', 'Training Set', 'Deep Learning', 'Random Forest', 'Decision Tree', 'Gene Regulatory Networks', 'Linear Discriminant Analysis', 'Fitness Function', 'Decades Of Research', 'Representation Learning', 'Statistical Set', 'Decorrelation', 'Neural Architecture Search', 'Hidden Relationships', 'R2 Score', 'SqueezeNet', 'Text Classification Tasks']","['Algorithms: Machine learning architectures', 'formulations', 'and algorithms (including transfer)', 'Explainable', 'fair', 'accountable', 'privacy-preserving', 'ethical computer vision']",2,"This paper seeks to answer the following question: ""What can we learn by predicting accuracy?"". Indeed, classification is one of the most popular tasks in machine learning, and many loss functions have been developed to maximize this non-differentiable objective function. Unlike past work on loss function design, which was guided mainly by intuition and theory before being validated by experimentation, here we propose to approach this problem in the opposite way: we seek to extract knowledge by experimentation. This data-driven approach is similar to that used in physics to discover general laws from data. We used a symbolic regression method to automatically find a mathematical expression highly correlated with a linear classifier’s accuracy. The formula discovered on more than 260 datasets of embeddings has a Pearson’s correlation of 0.96 and a r
<sup>2</sup>
 of 0.93. More interestingly, this formula is highly explainable and confirms insights from various previous papers on loss design. We hope this work will open new perspectives in the search for new heuristics leading to a deeper understanding of machine learning theory."
Wiener Guided DIP for Unsupervised Blind Image Deconvolution,"Gustav Bredell, Ertunc Erdil, Bruno Weber, Ender Konukoglu","University of Zürich; Computer Vision Laboratory, ETH Zürich",100,Switzerland,0,,"Blind deconvolution is an ill-posed problem arising in various fields ranging from microscopy to astronomy. Its ill-posed nature demands adequate priors and initialization to arrive at a desirable solution. Recently, it has been shown that deep networks can serve as an image generation prior (DIP) during unsupervised blind deconvolution optimization, however, DIP's high frequency artifact suppression ability is not explicitly exploited. We propose to use Wiener-deconvolution to guide DIP during optimization in order to better leverage DIP's ability for blind image deconvolution. Wiener-deconvolution sharpens an image while introducing high-frequency artifacts, which are reproduced by DIP with a delay compared to low-frequency features and sharp edges, similar to what has been observed for noise. We embed the computational process in a constrained optimization problem together with an automatic kernel initialization method and show that the proposed method yields higher performance and stability across multiple datasets.",https://openaccess.thecvf.com/content/WACV2023/html/Bredell_Wiener_Guided_DIP_for_Unsupervised_Blind_Image_Deconvolution_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Bredell_Wiener_Guided_DIP_for_Unsupervised_Blind_Image_Deconvolution_WACV_2023_paper.pdf,,,2112.10271,main,Poster,https://ieeexplore.ieee.org/document/10030357/,"['Deconvolution', 'Image synthesis', 'Microscopy', 'Image edge detection', 'Distance measurement', 'Delays', 'High frequency']","['Blind Deconvolution', 'Blind Image Deconvolution', 'Image Generation', 'Constrained Optimization Problem', 'Microscopy Images', 'Learning Rate', 'Convolutional Neural Network', 'Color Images', 'Power Spectral Density', 'Bit Error Rate', 'Optimization Step', 'Peak Signal-to-noise Ratio', 'Kernel Estimation', 'Image Sharpness', 'Image Categories', 'Real-world Images', 'Fourier Domain', 'Blurred Images', 'Fully-connected Network', 'Fluctuations In Performance', 'Structural Similarity Index Measure', 'Blurry Images', 'Microscopy Datasets', 'Blur Kernel', 'Large Image']",['Algorithms: Low-level and physics-based vision'],4,"Blind deconvolution is an ill-posed problem arising in various fields ranging from microscopy to astronomy. Its ill-posed nature demands adequate priors and initialization to arrive at a desirable solution. Recently, it has been shown that deep networks can serve as an image generation prior (DIP) during unsupervised blind deconvolution optimization, however, DIP&#x2019;s high frequency artifact suppression ability is not explicitly exploited. We propose to use Wiener-deconvolution to guide DIP during optimization in order to better leverage DIP&#x2019;s ability for blind image deconvolution. Wiener-deconvolution sharpens an image while introducing high-frequency artifacts, which are reproduced by DIP with a delay compared to low-frequency features and sharp edges, similar to what has been observed for noise. We embed the computational process in a constrained optimization problem together with an automatic kernel initialization method and show that the proposed method yields higher performance and stability across multiple datasets."
X-Align: Cross-Modal Cross-View Alignment for Bird's-Eye-View Segmentation,"Shubhankar Borse, Marvin Klingner, Varun Ravi Kumar, Hong Cai, Abdulaziz Almuzairee, Senthil Yogamani, Fatih Porikli","Automated Driving, QT Technologies Ireland Limited; Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.; Automated Driving, Qualcomm Technologies International GmbH; University of California, San Diego. Work done at Qualcomm.; Automated Driving, Qualcomm Technologies, Inc.",60,"Germany, USA",40,USA,"Bird's-eye-view (BEV) grid is a common representation for the perception of road components, e.g., drivable area, in autonomous driving. Most existing approaches rely on cameras only to perform segmentation in BEV space, which is fundamentally constrained by the absence of reliable depth information. Latest works leverage both camera and LiDAR modalities, but sub-optimally fuse their features using simple, concatenation-based mechanisms. In this paper, we address these problems by enhancing the alignment of the unimodal features in order to aid feature fusion, as well as enhancing the alignment between the cameras' perspective view (PV) and BEV representations. We propose X-Align, a novel end-to-end cross-modal and cross-view learning framework for BEV segmentation consisting of the following components: (i) a novel Cross-Modal Feature Alignment (X-FA) loss, (ii) an attention-based Cross-Modal Feature Fusion (X-FF) module to align multi-modal BEV features implicitly, and (iii) an auxiliary PV segmentation branch with Cross-View Segmentation Alignment (X-SA) losses to improve the PV-to-BEV transformation. We evaluate our proposed method across two commonly used benchmark datasets, i.e., nuScenes and KITTI-360. Notably, X-Align significantly outperforms the state-of-the-art by 3 absolute mIoU points on nuScenes. We also provide extensive ablation studies to demonstrate the effectiveness of the individual components.",https://openaccess.thecvf.com/content/WACV2023/html/Borse_X-Align_Cross-Modal_Cross-View_Alignment_for_Birds-Eye-View_Segmentation_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Borse_X-Align_Cross-Modal_Cross-View_Alignment_for_Birds-Eye-View_Segmentation_WACV_2023_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/10030842/,"['Computer vision', 'Laser radar', 'Fuses', 'Roads', 'Benchmark testing', 'Cameras', 'Reliability']","['Feature Fusion', 'Visual Perspective', 'Feature Alignment', 'Multimodal Features', 'Feature Fusion Module', 'Alignment Loss', 'Attention Mechanism', 'Bounding Box', 'Camera Images', 'Baseline Methods', 'Fusion Method', 'Semantic Features', 'Computational Overhead', 'Feature Aggregation', 'Segmentation Map', 'Focal Loss', 'Depth Estimation', 'Fusion Strategy', 'Intermediate Features', 'Feature Pyramid Network', 'Camera Features', 'Simple Concatenation', '3D Object Detection', 'LiDAR Point Clouds', 'Lack Of Reliable Information', 'Deformable Convolution']","['Applications: Robotics', 'Image recognition and understanding (object detection', 'categorization', 'segmentation', 'scene modeling', 'visual reasoning)', 'Virtual/augmented reality']",12,"Bird’s-eye-view (BEV) grid is a typical representation of the perception of road components, e.g., drivable area, in autonomous driving. Most existing approaches rely on cameras only to perform segmentation in BEV space, which is fundamentally constrained by the absence of reliable depth information. The latest works leverage both camera and LiDAR modalities but suboptimally fuse their features using simple, concatenation-based mechanisms.In this paper, we address these problems by enhancing the alignment of the unimodal features in order to aid feature fusion, as well as enhancing the alignment between the cameras’ perspective view (PV) and BEV representations. We propose X-Align, a novel end-to-end cross-modal and cross-view learning framework for BEV segmentation consisting of the following components: (i) a novel CrossModal Feature Alignment (X-FA) loss, (ii) an attentionbased Cross-Modal Feature Fusion (X-FF) module to align multi-modal BEV features implicitly, and (iii) an auxiliary PV segmentation branch with Cross-View Segmentation Alignment (X-SA) losses to improve the PV-to-BEV transformation. We evaluate our proposed method across two commonly used benchmark datasets, i.e., nuScenes and KITTI-360. Notably, X-Align significantly outperforms the state-of-the-art by 3 absolute mIoU points on nuScenes. We also provide extensive ablation studies to demonstrate the effectiveness of the individual components."
X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360deg Insufficient RGB-D Views,Haoyi Zhu,"Shanghai Jiao Tong University, Shanghai, China",100,China,0,,"Neural Radiance Fields (NeRFs), despite their outstanding performance on novel view synthesis, often need dense input views. Many papers train one model for each scene respectively and few of them explore incorporating multi-modal data into this problem. In this paper, we focus on a rarely discussed but important setting: can we train one model that can represent multiple scenes, with 360deg insufficient views and RGB-D images? We refer insufficient views to few extremely sparse and almost non-overlapping views. To deal with it, X-NeRF, a fully explicit approach which learns a general scene completion process instead of a coordinate-based mapping, is proposed. Given a few insufficient RGB-D input views, X-NeRF first transforms them to a sparse point cloud tensor and then applies a 3D sparse generative Convolutional Neural Network (CNN) to complete it to an explicit radiance field whose volumetric rendering can be conducted fast without running networks during inference. To avoid overfitting, besides common rendering loss, we apply perceptual loss as well as view augmentation through random rotation on point clouds. The proposed methodology significantly out-performs previous implicit methods in our setting, indicating the great potential of proposed problem and approach. Codes and data are available at https://github.com/HaoyiZhu/XNeRF.",https://openaccess.thecvf.com/content/WACV2023/html/Zhu_X-NeRF_Explicit_Neural_Radiance_Field_for_Multi-Scene_360deg_Insufficient_RGB-D_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Zhu_X-NeRF_Explicit_Neural_Radiance_Field_for_Multi-Scene_360deg_Insufficient_RGB-D_WACV_2023_paper.pdf,,https://github.com/HaoyiZhu/XNeRF,,main,Poster,,,,,,
Zero-Shot Versus Many-Shot: Unsupervised Texture Anomaly Detection,"Toshimichi Aota, Lloyd Teh Tzer Tong, Takayuki Okatani","Sanoh Industrial Co., Ltd.; Tohoku University / RIKEN AIP",50,Japan,50,Japan,"Research on unsupervised anomaly detection (AD) has recently progressed, significantly increasing detection accuracy. This paper focuses on texture images and considers how few normal samples are needed for accurate AD. We first highlight the critical nature of the problem that previous studies have overlooked: accurate detection gets harder for anisotropic textures when image orientations are not aligned between inputs and normal samples. We then propose a zero-shot method, which detects anomalies without using a normal sample. The method is free from the issue of unaligned orientation between input and normal images. It assumes the input texture to be homogeneous, detecting image regions that break the homogeneity as anomalies. We present a quantitative criterion to judge whether this assumption holds for an input texture. Experimental results show the broad applicability of the proposed zero-shot method and its good performance comparable to or even higher than the state-of-the-art methods using hundreds of normal samples. The code and data are available from https://drive.google.com/drive/folders/10OyPzvI3H6llCZBxKxFlKWt1Pw1tkMK1.",https://openaccess.thecvf.com/content/WACV2023/html/Aota_Zero-Shot_Versus_Many-Shot_Unsupervised_Texture_Anomaly_Detection_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Aota_Zero-Shot_Versus_Many-Shot_Unsupervised_Texture_Anomaly_Detection_WACV_2023_paper.pdf,https://drive.google.com/drive/folders/10OyPzvI3H6llCZBxKxFlKWt1Pw1tkMK1,,,main,Poster,https://ieeexplore.ieee.org/document/10030870/,"['Computer vision', 'Codes', 'Anomaly detection']","['Anomaly Detection', 'Detection Accuracy', 'Normal Samples', 'Input Image', 'Normal Images', 'Image Texture', 'Feature Space', 'Single Image', 'K-nearest Neighbor', 'Training Images', 'Sample Space', 'Image Point', 'Random Orientation', 'Repeat Structure', 'Pre-defined Threshold', 'Image X', 'Mild Assumptions', 'Anomalous Patterns', 'Anomaly Detection Methods', 'Anomaly Score']","['Algorithms: Image recognition and understanding (object detection', 'categorization', 'segmentation)', 'Machine learning architectures', 'formulations', 'and algorithms (including transfer', 'low-shot', 'semi-', 'self-', 'and un-supervised learning)']",9,"Research on unsupervised anomaly detection (AD) has recently progressed, significantly increasing detection accuracy. This paper focuses on texture images and considers how few normal samples are needed for accurate AD. We first highlight the critical nature of the problem that previous studies have overlooked: accurate detection gets harder for anisotropic textures when image orientations are not aligned between inputs and normal samples. We then propose a zero-shot method, which detects anomalies without using a normal sample. The method is free from the issue of unaligned orientation between input and normal images. It assumes the input texture to be homogeneous, detecting image regions that break the homogeneity as anomalies. We present a quantitative criterion to judge whether this assumption holds for an input texture. Experimental results show the broad applicability of the proposed zero-shot method and its good performance comparable to or even higher than the state-of-the-art methods using hundreds of normal samples. The code and data are available from https://drive.google.com/drive/folders/10OyPzvI3H6llCZBxKxFlKWt1Pw1tkMK1."
iColoriT: Towards Propagating Local Hints to the Right Region in Interactive Colorization by Leveraging Vision Transformer,"Jooyeol Yun, Sanghyeon Lee, Minho Park, Jaegul Choo",Korea Advanced Institute of Science and Technology (KAIST),100,South Korea,0,,"Point-interactive image colorization aims to colorize grayscale images when a user provides the colors for specific locations. It is essential for point-interactive colorization methods to appropriately propagate user-provided colors (i.e., user hints) in the entire image to obtain a reasonably colorized image with minimal user effort. However, existing approaches often produce partially colorized results due to the inefficient design of stacking convolutional layers to propagate hints to distant relevant regions. To address this problem, we present iColoriT, a novel point-interactive colorization Vision Transformer capable of propagating user hints to relevant regions, leveraging the global receptive field of Transformers. The self-attention mechanism of Transformers enables iColoriT to selectively colorize relevant regions with only a few local hints. Our approach colorizes images in real-time by utilizing pixel shuffling, an efficient upsampling technique that replaces the decoder architecture. Also, in order to mitigate the artifacts caused by pixel shuffling with large upsampling ratios, we present the local stabilizing layer. Extensive quantitative and qualitative results demonstrate that our approach highly outperforms existing methods for point-interactive colorization, producing accurately colorized images with a user's minimal effort. Official codes are available at https://pmh9960.github.io/research/iColoriT/.",https://openaccess.thecvf.com/content/WACV2023/html/Yun_iColoriT_Towards_Propagating_Local_Hints_to_the_Right_Region_in_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yun_iColoriT_Towards_Propagating_Local_Hints_to_the_Right_Region_in_WACV_2023_paper.pdf,,https://pmh9960.github.io/research/iColoriT/,2207.06831,main,Poster,https://ieeexplore.ieee.org/document/10030997/,"['Convolutional codes', 'Computer vision', 'Image color analysis', 'Stacking', 'Computer architecture', 'Gray-scale', 'Transformers']","['Interactive', 'Vision Transformer', 'Convolutional Layers', 'Qualitative Results', 'Color Images', 'Receptive Field', 'Grayscale Images', 'Entire Image', 'Relevant Regions', 'Global Field', 'Self-attention Mechanism', 'Decoder Architecture', 'User Effort', 'Official Code', 'Feature Maps', 'Single Image', 'Large-scale Datasets', 'ImageNet', 'Reference Image', 'Patch Size', 'Minimal Interaction', 'Peak Signal-to-noise Ratio', 'Image Patches', 'Sequence Of Tokens', 'Transformer Encoder', 'Color Model', 'Input Tokens', 'Inference Speed', 'Linear Layer', 'Self-supervised Manner']","['Algorithms: Computational photography', 'image and video synthesis']",6,"Point-interactive image colorization aims to colorize grayscale images when a user provides the colors for specific locations. It is essential for point-interactive colorization methods to appropriately propagate user-provided colors (i.e., user hints) in the entire image to obtain a reasonably colorized image with minimal user effort. However, existing approaches often produce partially colorized results due to the inefficient design of stacking convolutional layers to propagate hints to distant relevant regions. To address this problem, we present iColoriT, a novel point-interactive colorization Vision Transformer capable of propagating user hints to relevant regions, leveraging the global receptive field of Transformers. The self-attention mechanism of Transformers enables iColoriT to selectively colorize relevant regions with only a few local hints. Our approach colorizes images in real-time by utilizing pixel shuffling, an efficient upsampling technique that replaces the decoder architecture. Also, in order to mitigate the artifacts caused by pixel shuffling with large upsampling ratios, we present the local stabilizing layer. Extensive quantitative and qualitative results demonstrate that our approach highly out-performs existing methods for point-interactive colorization, producing accurately colorized images with a user's minimal effort. Official codes are available at https://pmh9960.github.io/research/iColoriT/."
nLMVS-Net: Deep Non-Lambertian Multi-View Stereo,"Kohei Yamashita, Yuto Enyo, Shohei Nobuhara, Ko Nishino","Graduate School of Informatics, Kyoto University, Kyoto, Japan",100,Japan,0,,"We introduce a novel multi-view stereo (MVS) method that can simultaneously recover not just per-pixel depth but also surface normals, together with the reflectance of textureless, complex non-Lambertian surfaces captured under known but natural illumination. Our key idea is to formulate MVS as an end-to-end learnable network, which we refer to as nLMVS-Net, that seamlessly integrates radiometric cues to leverage surface normals as view-independent surface features for learned cost volume construction and filtering. It first estimates surface normals as pixel-wise probability densities for each view with a novel shape-from-shading network. These per-pixel surface normal densities and the input multi-view images are then input to a novel cost volume filtering network that learns to recover per-pixel depth and surface normal. The reflectance is also explicitly estimated by alternating with geometry reconstruction. Extensive quantitative evaluations on newly established synthetic and real-world datasets show that nLMVS-Net can robustly and accurately recover the shape and reflectance of complex objects in natural settings.",https://openaccess.thecvf.com/content/WACV2023/html/Yamashita_nLMVS-Net_Deep_Non-Lambertian_Multi-View_Stereo_WACV_2023_paper.html,,https://openaccess.thecvf.com/content/WACV2023/papers/Yamashita_nLMVS-Net_Deep_Non-Lambertian_Multi-View_Stereo_WACV_2023_paper.pdf,https://vision.ist.i.kyoto-u.ac.jp/,,,main,Poster,https://ieeexplore.ieee.org/document/10030798/,"['Reflectivity', 'Geometry', 'Surface reconstruction', 'Costs', 'Uncertainty', 'Filtering', 'Shape']","['Multi-view Stereo', 'Probability Density', 'Key Idea', 'Normal Density', 'Multi-view Images', 'Reflective Objects', 'Filter Network', 'Surface Normals', 'Natural Illumination', 'Cost Volume', 'Neural Network', 'Deep Neural Network', 'Input Image', '3D Printing', 'Image Object', 'Surface Reflectance', 'Supplementary Materials For Details', 'Depth Estimation', 'Stereopsis', 'Surface Geometry', 'Sparse Input', 'Depth Error', 'Latent Vector', 'Accurate Capture', 'Surface Depth', 'Laplace Distribution', 'View Synthesis', 'High Dynamic Range Image', 'Joint Estimation', 'Probability Density Distribution']","['Algorithms: Low-level and physics-based vision', '3D computer vision']",8,"We introduce a novel multi-view stereo (MVS) method that can simultaneously recover not just per-pixel depth but also surface normals, together with the reflectance of textureless, complex non-Lambertian surfaces captured under known but natural illumination. Our key idea is to formulate MVS as an end-to-end learnable network, which we refer to as nLMVS-Net, that seamlessly integrates radiometric cues to leverage surface normals as view-independent surface features for learned cost volume construction and filtering. It first estimates surface normals as pixel-wise probability densities for each view with a novel shape-from-shading network. These per-pixel surface normal densities and the input multi-view images are then input to a novel cost volume filtering network that learns to recover per-pixel depth and surface normal. The reflectance is also explicitly estimated by alternating with geometry reconstruction. Extensive quantitative evaluations on newly established synthetic and real-world datasets show that nLMVS-Net can robustly and accurately recover the shape and reflectance of complex objects in natural settings."
