title,author,aff,university_affiliation,university_country,company_affiliation,company_country,abstract,site,oa,pdf,project,github,arxiv,track,status,ieee_link,ieee_keywords,ieee_index_terms,ieee_author_keywords,ieee_citations,ieee_abstract
3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization,"Sanath Narayan, Hisham Cholakkal, Fahad Shahbaz Khan, Ling Shao","Inception Institute of Artiﬁcial Intelligence, UAE",100.0,uae,0.0,,"Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art. Source code is available at https://github.com/naraysa/3c-net.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Narayan_3C-Net_Category_Count_and_Center_Loss_for_Weakly-Supervised_Action_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Narayan_3C-Net_Category_Count_and_Center_Loss_for_Weakly-Supervised_Action_Localization_ICCV_2019_paper.pdf,,https://github.com/naraysa/3c-net,,main,Poster,https://ieeexplore.ieee.org/document/9008791/,"['Videos', 'Training', 'Feature extraction', 'Motion segmentation', 'Computational modeling', 'Task analysis', 'Motion pictures']","['Central Loss', 'Action Localization', 'Activity Characteristics', 'Challenging Problem', 'Action Classes', 'Loss Term', 'Mean Average Precision', 'Separate Features', 'Temporal Localization', 'Weak Supervision', 'Feature Representation', 'Additional Term', 'Fully-connected Layer', 'Optical Flow', 'Single Instance', 'Backbone Network', 'Motion Features', 'Classification Loss', 'Feature Aggregation', 'Video Segments', 'Video Length', 'Action Instances', 'Temporal Boundaries', 'Discriminative Feature Representation', 'Temporal Attention', 'IoU Threshold', 'Strong Supervision', 'Parallel Bars', 'Test Videos', 'Meaning Maps']",,128,"Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art. Source code is available at https://github.com/naraysa/3c-net."
3D Face Modeling From Diverse Raw Scan Data,"Feng Liu, Luan Tran, Xiaoming Liu","Department of Computer Science and Engineering, Michigan State University, East Lansing MI 48824",100.0,usa,0.0,,"Traditional 3D face models learn a latent representation of faces using linear subspaces from limited scans of a single database. The main roadblock of building a large-scale face model from diverse 3D databases lies in the lack of dense correspondence among raw scans. To address these problems, this paper proposes an innovative framework to jointly learn a nonlinear face model from a diverse set of raw 3D scan databases and establish dense point-to-point correspondence among their scans. Specifically, by treating input scans as unorganized point clouds, we explore the use of PointNet architectures for converting point clouds to identity and expression feature representations, from which the decoder networks recover their 3D face shapes. Further, we propose a weakly supervised learning approach that does not require correspondence label for the scans. We demonstrate the superior dense correspondence and representation power of our proposed method, and its contribution to single-image 3D face reconstruction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009018/,"['Three-dimensional displays', 'Face', 'Solid modeling', 'Databases', 'Shape', 'Computational modeling', 'Decoding']","['Face Model', '3D Face', 'Raw Scans', '3D Face Model', 'Nonlinear Model', 'Point Cloud', 'Latent Representation', 'Representation Of Identity', 'Oral And Maxillofacial Surgery', 'Linear Subspace', 'Facial Shape', 'Face Representation', 'Dense Correspondence', 'Linear Model', '3D Reconstruction', 'Fully-connected Layer', '3D Scanning', 'Edge Length', 'Multiple Databases', 'Single Vector', '3D Shape', 'Real Scans', 'Shape Representation', 'Neutral Expressions', 'Identical Shape', '3D Landmarks', 'Surface Normals', 'Semantic Errors', 'Chamfer Distance', 'Unit Sphere']",,38,"Traditional 3D face models learn a latent representation of faces using linear subspaces from limited scans of a single database. The main roadblock of building a large-scale face model from diverse 3D databases lies in the lack of dense correspondence among raw scans. To address these problems, this paper proposes an innovative framework to jointly learn a nonlinear face model from a diverse set of raw 3D scan databases and establish dense point-to-point correspondence among their scans. Specifically, by treating input scans as unorganized point clouds, we explore the use of PointNet architectures for converting point clouds to identity and expression feature representations, from which the decoder networks recover their 3D face shapes. Further, we propose a weakly supervised learning approach that does not require correspondence label for the scans. We demonstrate the superior dense correspondence and representation power of our proposed method, and its contribution to single-image 3D face reconstruction."
3D Instance Segmentation via Multi-Task Metric Learning,"Jean Lahoud, Bernard Ghanem, Marc Pollefeys, Martin R. Oswald",KAUST; ETH Zurich,100.0,"saudi arabia, switzerland",0.0,,"We propose a novel method for instance label segmentation of dense 3D voxel grids. We target volumetric scene representations, which have been acquired with depth sensors or multi-view stereo methods and which have been processed with semantic 3D reconstruction or scene completion methods. The main task is to learn shape information about individual object instances in order to accurately separate them, including connected and incompletely scanned objects. We solve the 3D instance-labeling problem with a multi-task learning strategy. The first goal is to learn an abstract feature embedding, which groups voxels with the same instance label close to each other while separating clusters with different instance labels from each other. The second goal is to learn instance information by densely estimating directional information of the instance's center of mass for each voxel. This is particularly useful to find instance boundaries in the clustering post-processing step, as well as, for scoring the segmentation quality for the first goal. Both synthetic and real-world experiments demonstrate the viability and merits of our approach. In fact, it achieves state-of-the-art performance on the ScanNet 3D instance segmentation benchmark.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.pdf,https://sites.google.com/view/3d-instance-mtml,,,main,Oral,https://ieeexplore.ieee.org/document/9008793/,"['Three-dimensional displays', 'Semantics', 'Two dimensional displays', 'Task analysis', 'Proposals', 'Labeling', 'Measurement']","['Instance Segmentation', 'Metric Learning', '3D Segmentation', '3D Instance', '3D Instance Segmentation', 'Benchmark', '3D Reconstruction', 'Direct Information', 'Depth Camera', 'Post-processing Step', '3D Scene', 'Object Instances', 'Voxel Grid', 'Instance Labels', 'Feature Space', 'Object Detection', 'Point Cloud', 'Receptive Field', '3D Space', 'Bounding Box', 'Semantic Labels', 'Segmentation Labels', 'Object Proposals', 'Semantic Segmentation', '3D Data', '3D Grid', '3D Sets', 'Multi-task Learning', 'Direct Losses', 'Cluster Centers']",,113,"We propose a novel method for instance label segmentation of dense 3D voxel grids. We target volumetric scene representations, which have been acquired with depth sensors or multi-view stereo methods and which have been processed with semantic 3D reconstruction or scene completion methods. The main task is to learn shape information about individual object instances in order to accurately separate them, including connected and incompletely scanned objects. We solve the 3D instance-labeling problem with a multi-task learning strategy. The first goal is to learn an abstract feature embedding, which groups voxels with the same instance label close to each other while separating clusters with different instance labels from each other. The second goal is to learn instance information by densely estimating directional information of the instance's center of mass for each voxel. This is particularly useful to find instance boundaries in the clustering post-processing step, as well as, for scoring the segmentation quality for the first goal. Both synthetic and real-world experiments demonstrate the viability and merits of our approach. In fact, it achieves state-of-the-art performance on the ScanNet 3D instance segmentation benchmark."
3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions,"Dong Wook Shu, Sung Woo Park, Junseok Kwon","School of Computer Science and Engineering, Chung-Ang University, Seoul, Korea",100.0,south korea,0.0,,"In this paper, we propose a novel generative adversarial network (GAN) for 3D point clouds generation, which is called tree-GAN. To achieve state-of-the-art performance for multi-class 3D point cloud generation, a tree-structured graph convolution network (TreeGCN) is introduced as a generator for tree-GAN. Because TreeGCN performs graph convolutions within a tree, it can use ancestor information to boost the representation power for features. To evaluate GANs for 3D point clouds accurately, we develop a novel evaluation metric called Frechet point cloud distance (FPD). Experimental results demonstrate that the proposed tree-GAN outperforms state-of-the-art GANs in terms of both conventional metrics and FPD, and can generate point clouds for different semantic parts without prior knowledge.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shu_3D_Point_Cloud_Generative_Adversarial_Network_Based_on_Tree_Structured_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shu_3D_Point_Cloud_Generative_Adversarial_Network_Based_on_Tree_Structured_ICCV_2019_paper.pdf,,https://github.com/seowok/TreeGAN,,main,Poster,https://ieeexplore.ieee.org/document/9009495/,"['Three-dimensional displays', 'Gallium nitride', 'Generators', 'Convolutional codes', 'Generative adversarial networks', 'Semantics', 'Computer vision']","['Point Cloud', 'Generative Adversarial Networks', 'Tree Structure', '3D Point Cloud', 'Graph Convolution', 'Graph Convolutional Network', 'Point Cloud Generation', 'Computational Efficiency', 'Deep Neural Network', 'Feature Space', 'Latent Space', 'Final Layer', 'Mean Vector', '3D Coordinates', 'Weak Connections', 'Current Point', 'Object Parts', 'Unsupervised Manner', 'Memory Resources', 'Connectivity Information', 'Point Layer', 'Latent Code', 'Fr√©chet Inception Distance', 'Real Point', 'Real Cloud']",,163,"In this paper, we propose a novel generative adversarial network (GAN) for 3D point clouds generation, which is called tree-GAN. To achieve state-of-the-art performance for multi-class 3D point cloud generation, a tree-structured graph convolution network (TreeGCN) is introduced as a generator for tree-GAN. Because TreeGCN performs graph convolutions within a tree, it can use ancestor information to boost the representation power for features. To evaluate GANs for 3D point clouds accurately, we develop a novel evaluation metric called Fr\'echet point cloud distance (FPD). Experimental results demonstrate that the proposed tree-GAN outperforms state-of-the-art GANs in terms of both conventional metrics and FPD, and can generate point clouds for different semantic parts without prior knowledge."
"3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera","Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R. Zamir, Martin Fischer, Jitendra Malik, Silvio Savarese","Stanford University; Stanford University, University of California, Berkeley; University of California, Berkeley",100.0,usa,0.0,,"A comprehensive semantic understanding of a scene is important for many applications - but in what space should diverse semantic information (e.g., objects, scene categories, material types, 3D shapes, etc.) be grounded and what should be its structure? Aspiring to have one unified structure that hosts diverse types of semantics, we follow the Scene Graph paradigm in 3D, generating a 3D Scene Graph. Given a 3D mesh and registered panoramic images, we construct a graph that spans the entire building and includes semantics on objects (e.g., class, material, shape and other attributes), rooms (e.g., function, illumination type, etc.) and cameras (e.g., location, etc.), as well as the relationships among these entities. However, this process is prohibitively labor heavy if done manually. To alleviate this we devise a semi-automatic framework that employs existing detection methods and enhances them using two main constraints: I. framing of query images sampled on panoramas to maximize the performance of 2D detectors, and II. multi-view consistency enforcement across 2D detections that originate in different camera locations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.pdf,http://3dscenegraph.stanford.edu,,,main,Poster,https://ieeexplore.ieee.org/document/9008302/,"['Three-dimensional displays', 'Semantics', 'Cameras', 'Buildings', 'Visualization', 'Shape', 'Two dimensional displays']","['3D Space', '3D Graph', 'Scene Graph', '3D Scene Graph', 'Semantic Information', '3D Mesh', '3D Shape', 'Camera Position', 'Scene Classification', 'Semantic Types', 'Unified Structure', 'Automatic Method', 'Affordances', 'Confidence Score', 'Object Classification', 'Imaging Center', 'RGB Images', 'Majority Voting', 'Graph Structure', 'Mask R-CNN', 'Conditional Random Field', 'Objects In The Scene', 'COCO Dataset', 'Camera Pose', 'Semantic Annotation', 'Object Instances', 'Spatial Order', 'Feature Pyramid Network', 'Human Labor']",,163,"A comprehensive semantic understanding of a scene is important for many applications - but in what space should diverse semantic information (e.g., objects, scene categories, material types, 3D shapes, etc.) be grounded and what should be its structure? Aspiring to have one unified structure that hosts diverse types of semantics, we follow the Scene Graph paradigm in 3D, generating a 3D Scene Graph. Given a 3D mesh and registered panoramic images, we construct a graph that spans the entire building and includes semantics on objects (e.g., class, material, shape and other attributes), rooms (e.g., function, illumination type, etc.) and cameras (e.g., location, etc.), as well as the relationships among these entities. However, this process is prohibitively labor heavy if done manually. To alleviate this we devise a semi-automatic framework that employs existing detection methods and enhances them using two main constraints: I. framing of query images sampled on panoramas to maximize the performance of 2D detectors, and II. multi-view consistency enforcement across 2D detections that originate in different camera locations."
3D Scene Reconstruction With Multi-Layer Depth and Epipolar Transformers,"Daeyun Shin, Zhile Ren, Erik B. Sudderth, Charless C. Fowlkes","University of California, Irvine; Georgia Institute of Technology",100.0,"USA, usa",0.0,,"We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel ""Epipolar Feature Transformer"" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shin_3D_Scene_Reconstruction_With_Multi-Layer_Depth_and_Epipolar_Transformers_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shin_3D_Scene_Reconstruction_With_Multi-Layer_Depth_and_Epipolar_Transformers_ICCV_2019_paper.pdf,https://research.dshin.org/iccv19/multi-layer-depth,,,main,Poster,https://ieeexplore.ieee.org/document/9009500/,"['Three-dimensional displays', 'Shape', 'Solid modeling', 'Geometry', 'Cameras', 'Image reconstruction', 'Surface reconstruction']","['3D Reconstruction', '3D Scene', 'RGB Images', 'Object Shape', 'Surface Shape', '3D Geometry', 'Visible Surface', 'Complete 3D', 'Single Shape', 'Scene Geometry', 'Virtual Camera', 'Camera Viewpoint', 'Convolutional Neural Network', 'Input Image', 'Feature Maps', 'Precision And Recall', 'Point Cloud', 'Semantic Segmentation', 'Depth Map', 'Depth Images', '3D Shape', 'Scene Representation', 'Overhead Camera', 'Frontal Networks', 'Gating Function', 'Huber Loss', 'Height Map', 'Convolutional Neural Network Features', 'Ground Truth Depth', 'Semantic Segmentation Network']",,42,"We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel ""Epipolar Feature Transformer"" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes."
3D-LaneNet: End-to-End 3D Multiple Lane Detection,"Noa Garnett, Rafi Cohen, Tomer Pe'er, Roee Lahav, Dan Levi","General Motors Israel, HaMada St. 7, Herzlya, Israel",0.0,,100.0,Israel,"We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Garnett_3D-LaneNet_End-to-End_3D_Multiple_Lane_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Garnett_3D-LaneNet_End-to-End_3D_Multiple_Lane_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008811/,"['Three-dimensional displays', 'Cameras', 'Roads', 'Task analysis', 'Feature extraction', 'Estimation', 'Pipelines']","['Multiple Lanes', 'Lane Detection', 'Object Detection', 'Street Scenes', 'Outlier Rejection', 'Dual Representation', 'Terrain', 'Convolutional Neural Network', 'Feature Maps', 'Network Output', 'Real-world Datasets', 'Top View', '3D Point', 'Image Domain', 'Translation Invariance', 'Local Position', 'Homography', 'Top-view Images', '3D Detection', 'Dual Pathway', '3D Curves', 'Camera Height', 'Camera Coordinate', 'Lane Markings', 'Pitch Height', 'Secondary Roads', 'Local Roads', 'Learning Rate', 'Typical Output', 'Validation Set']",,109,"We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art."
3D-RelNet: Joint Object and Relational Network for 3D Prediction,"Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, Abhinav Gupta","Carnegie Mellon University; University of California, Berkeley",100.0,usa,0.0,,"We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not leverage the relationships amongst them. We argue that reasoning about these relationships is crucial, and present an approach to incorporate these in a 3D prediction framework. In addition to independent per-object predictions, we predict pairwise relations in the form of relative 3D pose, and demonstrate that these can be easily incorporated to improve object level estimates. We report performance across different datasets (SUNCG, NYUv2), and show that our approach significantly improves over independent prediction approaches while also outperforming alternate implicit reasoning methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kulkarni_3D-RelNet_Joint_Object_and_Relational_Network_for_3D_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kulkarni_3D-RelNet_Joint_Object_and_Relational_Network_for_3D_Prediction_ICCV_2019_paper.pdf,https://nileshkulkarni.github.io/relative3d/,https://github.com/nileshkulkarni/relative3d,,main,Poster,https://ieeexplore.ieee.org/document/9008257/,"['Three-dimensional displays', 'Shape', 'Cognition', 'Solid modeling', 'Task analysis', 'Layout', 'Two dimensional displays']","['3D Prediction', 'Independent Predictor', '3D Shape', 'Pairwise Relationships', 'Relative Pose', '3D Pose', 'Computer Vision', 'Classification Task', 'Training Time', 'End Of The Spectrum', 'Bounding Box', 'Final Prediction', 'Spatial Coordinates', 'Linear Constraints', 'Pose Estimation', 'Graph Convolutional Network', 'Appendix For Details', 'Objects In The Scene', 'Camera Frame', '3D Scene', 'Object In Frame', 'Ground-truth Box', 'Binary Terms', 'Pose Prediction', 'Higher-order Relationships', 'Related Modules']",,23,"We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not leverage the relationships amongst them. We argue that reasoning about these relationships is crucial, and present an approach to incorporate these in a 3D prediction framework. In addition to independent per-object predictions, we predict pairwise relations in the form of relative 3D pose, and demonstrate that these can be easily incorporated to improve object level estimates. We report performance across different datasets (SUNCG, NYUv2), and show that our approach significantly improves over independent prediction approaches while also outperforming alternate implicit reasoning methods."
3DPeople: Modeling the Geometry of Dressed Humans,"Albert Pumarola, Jordi Sanchez-Riera, Gary P. T. Choi, Alberto Sanfeliu, Francesc Moreno-Noguer","Institut de Rob `otica i Inform `atica Industrial, CSIC-UPC, Barcelona, Spain; John A. Paulson School of Engineering and Applied Sciences, Harvard University, USA",100.0,"spain, usa",0.0,,"Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape. First, we present 3DPeople, a large-scale synthetic dataset with 2 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body we annotated the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3DPeople suitable for a plethora of tasks. We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete. Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pumarola_3DPeople_Modeling_the_Geometry_of_Dressed_Humans_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pumarola_3DPeople_Modeling_the_Geometry_of_Dressed_Humans_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008281/,"['Shape', 'Geometry', 'Three-dimensional displays', 'Clothing', 'Cameras', 'Two dimensional displays', 'Optical imaging']","['Deep Network', 'Input Image', 'Single Image', 'Large-scale Datasets', 'Body Shape', 'Optical Flow', '3D Mesh', '3D Shape', 'Optimal Transport', 'Imaging Geometry', 'Photo-realistic Images', 'Normal Map', 'Naked Body', 'Body Pose', 'Deep Generative Network', 'Semantic', 'Supplemental Material', 'Quantitative Results', 'Qualitative Results', 'Human Body Shape', 'Synthetic Images', 'Layer Of The Discriminator', 'Depth Map', 'Person Image', 'Image X', 'Regular Grid', 'Mesh Resolution', 'Skin Color', '3D Reconstruction']",,84,"Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape. First, we present 3D-People, a large-scale synthetic dataset with 2 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body we annotated the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3D-People suitable for a plethora of tasks. We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete. Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images."
6-DOF GraspNet: Variational Grasp Generation for Object Manipulation,"Arsalan Mousavian, Clemens Eppner, Dieter Fox",NVIDIA,0.0,,100.0,USA,"Generating grasp poses is a crucial component for any robot object manipulation task. In this work, we formulate the problem of grasp generation as sampling a set of grasps using a variational autoencoder and assess and refine the sampled grasps using a grasp evaluator model. Both Grasp Sampler and Grasp Refinement networks take 3D point clouds observed by a depth camera as input. We evaluate our approach in simulation and real-world robot experiments. Our approach achieves 88% success rate on various commonly used objects with diverse appearances, scales, and weights. Our model is trained purely in simulation and works in the real-world without any extra steps.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010919/,"['Three-dimensional displays', 'Grippers', 'Cameras', 'Task analysis', 'Robot vision systems', 'Geometry']","['Grasp Generation', '6-DOF GraspNet', 'Point Cloud', 'Depth Camera', 'Variational Autoencoder', '3D Point Cloud', 'Robot Experiments', 'Collision', 'Training Data', 'Iterative Process', 'Latent Variables', 'Coverage Rate', 'Latent Space', '3D Data', 'Feature Points', 'Robot Manipulator', 'Objects In The Scene', 'Iterative Refinement', 'Point Cloud Data', 'Object Point Cloud', 'Robotic Gripper', 'Object Geometry', 'Physical Simulation', 'Surface Normals', 'Surface Friction', 'Human Pose Estimation', 'Partial Differential', 'Thin Structures', 'RGB Images']",,349,"Generating grasp poses is a crucial component for any robot object manipulation task. In this work, we formulate the problem of grasp generation as sampling a set of grasps using a variational autoencoder and assess and refine the sampled grasps using a grasp evaluator model. Both Grasp Sampler and Grasp Refinement networks take 3D point clouds observed by a depth camera as input. We evaluate our approach in simulation and real-world robot experiments. Our approach achieves 88% success rate on various commonly used objects with diverse appearances, scales, and weights. Our model is trained purely in simulation and works in the real-world without any extra steps."
A Bayesian Optimization Framework for Neural Network Compression,"Xingchen Ma, Amal Rannen Triki, Maxim Berman, Christos Sagonas, Jacques Cali, Matthew B. Blaschko","Onﬁdo; KU Leuven† (Currently afﬁliated with Deepmind); Blue Prism‡ (Contribution while at Onﬁdo, UK); KU Leuven",50.0,belgium,50.0,UK,"Neural network compression is an important step for deploying neural networks where speed is of high importance, or on devices with limited memory. It is necessary to tune compression parameters in order to achieve the desired trade-off between size and performance. This is often done by optimizing the loss on a validation set of data, which should be large enough to approximate the true risk and therefore yield sufficient generalization ability. However, using a full validation set can be computationally expensive. In this work, we develop a general Bayesian optimization framework for optimizing functions that are computed based on U-statistics. We propagate Gaussian uncertainties from the statistics through the Bayesian optimization framework yielding a method that gives a probabilistic approximation certificate of the result. We then apply this to parameter selection in neural network compression. Compression objectives that can be written as U-statistics are typically based on empirical risk and knowledge distillation for deep discriminative models. We demonstrate our method on VGG and ResNet models, and the resulting system can find optimal compression parameters for relatively high-dimensional parametrizations in a matter of minutes on a standard desktop machine, orders of magnitude faster than competing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ma_A_Bayesian_Optimization_Framework_for_Neural_Network_Compression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_A_Bayesian_Optimization_Framework_for_Neural_Network_Compression_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009557/,"['Neural networks', 'Optimization', 'Computational modeling', 'Bayes methods', 'Probabilistic logic', 'Adaptation models', 'Training']","['Neural Network', 'Optimization Framework', 'Bayesian Optimization', 'Network Compression', 'Neural Compression', 'Bayesian Optimization Framework', 'Neural Network Compression', 'Validation Set', 'True Risk', 'Empirical Risk', 'Matter Of Minutes', 'Compression Parameters', 'Optimal Compression', 'Objective Function', 'Gaussian Noise', 'Stochastic Gradient Descent', 'Singular Value Decomposition', 'Marginal Distribution', 'Number Of Weights', 'Acquisition Function', 'Gaussian Process Model', 'Low-rank Decomposition', 'Compression Ratio', 'Top-5 Accuracy', 'Compression Algorithm', 'Tensor Decomposition', 'Low-rank Approximation', 'Probability Of Improvement', 'Fidelity Measures', 'Compression Rate']",,14,"Neural network compression is an important step for deploying neural networks where speed is of high importance, or on devices with limited memory. It is necessary to tune compression parameters in order to achieve the desired trade-off between size and performance. This is often done by optimizing the loss on a validation set of data, which should be large enough to approximate the true risk and therefore yield sufficient generalization ability. However, using a full validation set can be computationally expensive. In this work, we develop a general Bayesian optimization framework for optimizing functions that are computed based on U-statistics. We propagate Gaussian uncertainties from the statistics through the Bayesian optimization framework yielding a method that gives a probabilistic approximation certificate of the result. We then apply this to parameter selection in neural network compression. Compression objectives that can be written as U-statistics are typically based on empirical risk and knowledge distillation for deep discriminative models. We demonstrate our method on VGG and ResNet models, and the resulting system can find optimal compression parameters for relatively high-dimensional parametrizations in a matter of minutes on a standard desktop machine, orders of magnitude faster than competing methods."
A Camera That CNNs: Towards Embedded Neural Networks on Pixel Processor Arrays,"Laurie Bose, Jianing Chen, Stephen J. Carey, Piotr Dudek, Walterio Mayol-Cuevas","University of Bristol, Bristol, United Kingdom; University of Manchester, Manchester, United Kingdom",100.0,"United Kingdom, uk",0.0,,"We present a convolutional neural network implementation for pixel processor array (PPA) sensors. PPA hardware consists of a fine-grained array of general-purpose processing elements, each capable of light capture, data storage, program execution, and communication with neighboring elements. This allows images to be stored and manipulated directly at the point of light capture, rather than having to transfer images to external processing hardware. Our CNN approach divides this array up into 4x4 blocks of processing elements, essentially trading-off image resolution for increased local memory capacity per 4x4 ""pixel"". We implement parallel operations for image addition, subtraction and bit-shifting images in this 4x4 block format. Using these components we formulate how to perform ternary weight convolutions upon these images, compactly store results of such convolutions, perform max-pooling, and transfer the resulting sub-sampled data to an attached micro-controller. We train ternary weight filter CNNs for digit recognition and a simple tracking task, and demonstrate inference of these networks upon the SCAMP5 PPA system. This work represents a first step towards embedding neural network processing capability directly onto the focal plane of a sensor.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bose_A_Camera_That_CNNs_Towards_Embedded_Neural_Networks_on_Pixel_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bose_A_Camera_That_CNNs_Towards_Embedded_Neural_Networks_on_Pixel_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010705/,"['Registers', 'Parallel processing', 'Hardware', 'Neural networks', 'Sensor arrays']","['Neural Network', 'Convolutional Neural Network', 'Processor Array', 'Focal Plane', 'Local Memory', 'Optical Character Recognition', 'Subtraction Images', 'Convolutional Neural Network Implementation', 'Real-valued', 'Efficient Way', 'Network Training', 'Data Augmentation', 'Training Images', 'Bounding Box', 'Grayscale Images', 'Analog-to-digital Converter', 'Simple Operation', 'Convolution Kernel', 'Image Sensor', 'Network Weights', 'Image Convolution', 'Pixel Array', 'Forward Pass', 'Source Images', 'Digital Format', 'Image Storage', 'Single Convolutional Layer', 'Class Of Devices', 'Binary Image']",,23,"We present a convolutional neural network implementation for pixel processor array (PPA) sensors. PPA hardware consists of a fine-grained array of general-purpose processing elements, each capable of light capture, data storage, program execution, and communication with neighboring elements. This allows images to be stored and manipulated directly at the point of light capture, rather than having to transfer images to external processing hardware. Our CNN approach divides this array up into 4x4 blocks of processing elements, essentially trading-off image resolution for increased local memory capacity per 4x4 ”pixel”. We implement parallel operations for image addition, subtraction and bit-shifting images in this 4x4 block format. Using these components we formulate how to perform ternary weight convolutions upon these images, compactly store results of such convolutions, perform max-pooling, and transfer the resulting sub-sampled data to an attached micro-controller. We train ternary weight filter CNNs for digit recognition and a simple tracking task, and demonstrate inference of these networks upon the SCAMP5 PPA system. This work represents a first step towards embedding neural network processing capability directly onto the focal plane of a sensor."
A Closed-Form Solution to Universal Style Transfer,"Ming Lu, Hao Zhao, Anbang Yao, Yurong Chen, Feng Xu, Li Zhang","BNRist and School of Software, Tsinghua University; Intel Labs China; Department of Electronic Engineering, Tsinghua University",66.66666666666666,China,33.33333333333334,China,"Universal style transfer tries to explicitly minimize the losses in feature space, thus it does not require training on any pre-defined styles. It usually uses different layers of VGG network as the encoders and trains several decoders to invert the features into images. Therefore, the effect of style transfer is achieved by feature transform. Although plenty of methods have been proposed, a theoretical analysis of feature transform is still missing. In this paper, we first propose a novel interpretation by treating it as the optimal transport problem. Then, we demonstrate the relations of our formulation with former works like Adaptive Instance Normalization (AdaIN) and Whitening and Coloring Transform (WCT). Finally, we derive a closed-form solution named Optimal Style Transfer (OST) under our formulation by additionally considering the content loss of Gatys. Comparatively, our solution can preserve better structure and achieve visually pleasing results. It is simple yet effective and we demonstrate its advantages both quantitatively and qualitatively. Besides, we hope our theoretical analysis can inspire future works in neural style transfer.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lu_A_Closed-Form_Solution_to_Universal_Style_Transfer_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_A_Closed-Form_Solution_to_Universal_Style_Transfer_ICCV_2019_paper.pdf,,https://github.com/lu-m13/OptimalStyleTransfer,,main,Poster,https://ieeexplore.ieee.org/document/9010325/,"['Decoding', 'Transforms', 'Optimization', 'Covariance matrices', 'Closed-form solutions', 'Training', 'Feature extraction']","['Style Transfer', 'Universal Style Transfer', 'Optimization Problem', 'Transfer Effects', 'Feature Space', 'Network Layer', 'Loss Of Content', 'Optimal Transport', 'Normal Distribution', 'Neural Network', 'Convolutional Neural Network', 'Covariance Matrix', 'Deep Neural Network', 'Optimal Model', 'User Study', 'Dimensional Vector', 'Original Features', 'Transformation Matrix', 'Average Loss', 'Orthogonal Matrix', 'Style Image', 'Gram Matrix', 'Markov Random Field', 'Content Features', 'Patch Matching', 'Maximum Mean Discrepancy', 'Result Of Transfer', 'Input Resolution', 'Feature Channels', 'Feature Maps']",,47,"Universal style transfer tries to explicitly minimize the losses in feature space, thus it does not require training on any pre-defined styles. It usually uses different layers of VGG network as the encoders and trains several decoders to invert the features into images. Therefore, the effect of style transfer is achieved by feature transform. Although plenty of methods have been proposed, a theoretical analysis of feature transform is still missing. In this paper, we first propose a novel interpretation by treating it as the optimal transport problem. Then, we demonstrate the relations of our formulation with former works like Adaptive Instance Normalization (AdaIN) and Whitening and Coloring Transform (WCT). Finally, we derive a closed-form solution named Optimal Style Transfer (OST) under our formulation by additionally considering the content loss of Gatys. Comparatively, our solution can preserve better structure and achieve visually pleasing results. It is simple yet effective and we demonstrate its advantages both quantitatively and qualitatively. Besides, we hope our theoretical analysis can inspire future works in neural style transfer."
A Comprehensive Overhaul of Feature Distillation,"Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, Jin Young Choi","GSCST, Seoul National University, Korea; Clova AI Research, NAVER Corp, Korea; Department of ECE, ASRI, Seoul National University, Korea",66.66666666666666,south korea,33.33333333333334,South Korea,"We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.pdf,,https://bhheo.github.io/overhaul,,main,Poster,https://ieeexplore.ieee.org/document/9009585/,"['Transforms', 'Task analysis', 'Training', 'Jacobian matrices', 'Image coding', 'Neurons', 'Artificial intelligence']","['Feature Distillation', 'Distance Function', 'Object Detection', 'Semantic Segmentation', 'Teacher Network', 'Distillation Method', 'Network Compression', 'Distillation Loss', 'Loss Function', 'Negative Responses', 'Convolutional Layers', 'Kullback-Leibler', 'Batch Normalization', 'Educational Value', 'Residual Block', 'Batch Normalization Layer', 'Typical Architecture', 'Spatial Size', 'Training Modalities', 'Student Network', 'End Of Block', 'Single Shot Detector', 'Beneficial Information', 'Rectified Linear Unit Activation', 'Rectified Linear Unit Function', 'ImageNet Dataset', 'Marginal Value', 'Aspects Of Loss']",,345,"We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at bhheo.github.io/overhaul."
A Dataset of Multi-Illumination Images in the Wild,"Lukas Murmann, MichaÃ«l Gharbi, Miika Aittala, FrÃ©do Durand","MIT CSAIL; MIT CSAIL, Adobe Research",100.0,usa,0.0,,"Collections of images under a single, uncontrolled illumination have enabled the rapid advancement of core computer vision tasks like classification, detection, and segmentation. But even with modern learning techniques, many inverse problems involving lighting and material understanding remain too severely ill-posed to be solved with single-illumination datasets. The data simply does not contain the necessary supervisory signals. Multi-illumination datasets are notoriously hard to capture, so the data is typically collected at small scale, in controlled environments, either using multiple light sources, or robotic gantries. This leads to image collections that are not representative of the variety and complexity of real world scenes. We introduce a new multi-illumination dataset of more than 1000 real scenes, each captured in high dynamic range and high resolution, under 25 lighting conditions. We demonstrate the richness of this dataset by training state-of-the-art models for three challenging applications: single-image illumination estimation, image relighting, and mixed-illuminant white balance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Murmann_A_Dataset_of_Multi-Illumination_Images_in_the_Wild_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Murmann_A_Dataset_of_Multi-Illumination_Images_in_the_Wild_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008252/,"['Lighting', 'Cameras', 'Light sources', 'Dynamic range', 'Estimation', 'Probes', 'Computer vision']","['Light Source', 'Dynamic Range', 'Inverse Problem', 'High Dynamic Range', 'White Balance', 'Real-world Scenes', 'Multiple Light', 'Error Of The Mean', 'Convolutional Network', 'Convolutional Neural Network', 'Linear Combination', 'Input Image', 'Single Image', 'Metal Surface', 'Image Pairs', 'Indoor Environments', 'Direct Light', 'Variable Light', 'Environment Map', 'Object Appearance', 'Probe Light', 'High Dynamic Range Image', 'L2 Loss']",,37,"Collections of images under a single, uncontrolled illumination have enabled the rapid advancement of core computer vision tasks like classification, detection, and segmentation. But even with modern learning techniques, many inverse problems involving lighting and material understanding remain too severely ill-posed to be solved with single-illumination datasets. The data simply does not contain the necessary supervisory signals. Multi-illumination datasets are notoriously hard to capture, so the data is typically collected at small scale, in controlled environments, either using multiple light sources, or robotic gantries. This leads to image collections that are not representative of the variety and complexity of real world scenes. We introduce a new multi-illumination dataset of more than 1000 real scenes, each captured in high dynamic range and high resolution, under 25 lighting conditions. We demonstrate the richness of this dataset by training state-of-the-art models for three challenging applications: single-image illumination estimation, image relighting, and mixed-illuminant white balance."
A Decoupled 3D Facial Shape Model by Adversarial Training,"Victoria FernÃ¡ndez Abrevaya, Adnane Boukhayma, Stefanie Wuhrer, Edmond Boyer","Inria - Univ. Grenoble Alpes - CNRS - LJK, France; University of Oxford, UK",100.0,"France, uk",0.0,,"Data-driven generative 3D face models are used to compactly encode facial shape data into meaningful parametric representations. A desirable property of these models is their ability to effectively decouple natural sources of variation, in particular identity and expression. While factorized representations have been proposed for that purpose, they are still limited in the variability they can capture and may present modeling artifacts when applied to tasks such as expression transfer. In this work, we explore a new direction with Generative Adversarial Networks and show that they contribute to better face modeling performances, especially in decoupling natural factors, while also achieving more diverse samples. To train the model we introduce a novel architecture that combines a 3D generator with a 2D discriminator that leverages conventional CNNs, where the two components are bridged by a geometry mapping layer. We further present a training scheme, based on auxiliary classifiers, to explicitly disentangle identity and expression attributes. Through quantitative and qualitative results on standard face datasets, we illustrate the benefits of our model and demonstrate that it outperforms competing state of the art methods in terms of decoupling and diversity.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Abrevaya_A_Decoupled_3D_Facial_Shape_Model_by_Adversarial_Training_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Abrevaya_A_Decoupled_3D_Facial_Shape_Model_by_Adversarial_Training_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010877/,"['Three-dimensional displays', 'Solid modeling', 'Face', 'Shape', 'Gallium nitride', 'Geometry', 'Two dimensional displays']","['Shape Model', 'Adversarial Training', 'Facial Shape', '3D Face Shape', 'Factorization', 'Quantitative Results', 'Generative Adversarial Networks', 'Face Model', '3D Face', 'Auxiliary Classifier', 'Large Values', 'Variety Of Factors', 'Supplemental Material', 'Partial Data', 'Multilayer Perceptron', 'Latent Space', 'Identification Code', 'Use Of Classes', '3D Mesh', 'Image Domain', '3D Shape', 'Imaging Geometry', 'Identity Vector', 'Latent Code', 'Classification Loss', 'Unit Square', 'Qualitative Examples', 'Autoencoder Architecture', 'Identity Space', 'Multilinear']",,18,"Data-driven generative 3D face models are used to compactly encode facial shape data into meaningful parametric representations. A desirable property of these models is their ability to effectively decouple natural sources of variation, in particular identity and expression. While factorized representations have been proposed for that purpose, they are still limited in the variability they can capture and may present modeling artifacts when applied to tasks such as expression transfer. In this work, we explore a new direction with Generative Adversarial Networks and show that they contribute to better face modeling performances, especially in decoupling natural factors, while also achieving more diverse samples. To train the model we introduce a novel architecture that combines a 3D generator with a 2D discriminator that leverages conventional CNNs, where the two components are bridged by a geometry mapping layer. We further present a training scheme, based on auxiliary classifiers, to explicitly disentangle identity and expression attributes. Through quantitative and qualitative results on standard face datasets, we illustrate the benefits of our model and demonstrate that it outperforms competing state of the art methods in terms of decoupling and diversity."
A Deep Cybersickness Predictor Based on Brain Signal Analysis for Virtual Reality Contents,"Jinwoo Kim, Woojae Kim, Heeseok Oh, Seongmin Lee, Sanghoon Lee",Electronics & Telecommunications Research Institute; Yonsei University,100.0,"Korea, south korea",0.0,,"What if we could interpret the cognitive state of a user while experiencing a virtual reality (VR) and estimate the cognitive state from a visual stimulus? In this paper, we address the above question by developing an electroencephalography (EEG) driven VR cybersickness prediction model. The EEG data has been widely utilized to learn the cognitive representation of brain activity. In the first stage, to fully exploit the advantages of the EEG data, it is transformed into the multi-channel spectrogram which enables to account for the correlation of spectral and temporal coefficient. Then, a convolutional neural network (CNN) is applied to encode the cognitive representation of the EEG spectrogram. In the second stage, we train a cybersickness prediction model on the VR video sequence by designing a Recurrent Neural Network (RNN). Here, the encoded cognitive representation is transferred to the model to train the visual and cognitive features for cybersickness prediction. Through the proposed framework, it is possible to predict the cybersickness level that reflects brain activity automatically. We use 8-channels EEG data to record brain activity while more than 200 subjects experience 44 different VR contents. After rigorous training, we demonstrate that the proposed framework reliably estimates cognitive states without the EEG data. Furthermore, it achieves state-of-the-art performance comparing to existing VR cybersickness prediction models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_A_Deep_Cybersickness_Predictor_Based_on_Brain_Signal_Analysis_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_A_Deep_Cybersickness_Predictor_Based_on_Brain_Signal_Analysis_for_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010856/,"['Electroencephalography', 'Brain modeling', 'Spectrogram', 'Visualization', 'Predictive models', 'Solid modeling', 'Feature extraction']","['Brain Signals', 'Virtual Reality Content', 'Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Brain Activity', 'Visual Stimuli', 'Cognitive Status', 'Visual Features', 'Recurrent Neural Network', 'Cognitive Characteristics', 'EEG Data', 'Cognitive Representations', 'Individual Differences', 'Stage 2', 'Feature Space', 'Long Short-term Memory', 'Temporal Features', 'EEG Signals', 'Fully-connected Layer', 'Spectral Network', 'Spectral Correlation', 'Cognitive Predictors', 'Temporal Dependencies', 'Mean Opinion Score', 'Validation Accuracy', 'Spectral Domain', 'Spectral Dependence', 'Temporal Domain', 'Sequence Network']",,58,"What if we could interpret the cognitive state of a user while experiencing a virtual reality (VR) and estimate the cognitive state from a visual stimulus? In this paper, we address the above question by developing an electroencephalography (EEG) driven VR cybersickness prediction model. The EEG data has been widely utilized to learn the cognitive representation of brain activity. In the first stage, to fully exploit the advantages of the EEG data, it is transformed into the multi-channel spectrogram which enables to account for the correlation of spectral and temporal coefficient. Then, a convolutional neural network (CNN) is applied to encode the cognitive representation of the EEG spectrogram. In the second stage, we train a cybersickness prediction model on the VR video sequence by designing a Recurrent Neural Network (RNN). Here, the encoded cognitive representation is transferred to the model to train the visual and cognitive features for cybersickness prediction. Through the proposed framework, it is possible to predict the cybersickness level that reflects brain activity automatically. We use 8-channels EEG data to record brain activity while more than 200 subjects experience 44 different VR contents. After rigorous training, we demonstrate that the proposed framework reliably estimates cognitive states without the EEG data. Furthermore, it achieves state-of-the-art performance comparing to existing VR cybersickness prediction models."
A Deep Step Pattern Representation for Multimodal Retinal Image Registration,"Jimmy Addison Lee, Peng Liu, Jun Cheng, Huazhu Fu","Inception Institute of Artiﬁcial Intelligence, UAE; Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, China; Big Data Research Center at University of Electronic Science and Technology of China, China; UBTech Research, China",75.0,"china, uae",25.0,China,"This paper presents a novel feature-based method that is built upon a convolutional neural network (CNN) to learn the deep representation for multimodal retinal image registration. We coined the algorithm deep step patterns, in short DeepSPa. Most existing deep learning based methods require a set of manually labeled training data with known corresponding spatial transformations, which limits the size of training datasets. By contrast, our method is fully automatic and scale well to different image modalities with no human intervention. We generate feature classes from simple step patterns within patches of connecting edges formed by vascular junctions in multiple retinal imaging modalities. We leverage CNN to learn and optimize the input patches to be used for image registration. Spatial transformations are estimated based on the output possibility of the fully connected layer of CNN for a pair of images. One of the key advantages of the proposed algorithm is its robustness to non-linear intensity changes, which widely exist on retinal images due to the difference of acquisition modalities. We validate our algorithm on extensive challenging datasets comprising poor quality multimodal retinal images which are adversely affected by pathologies (diseases), speckle noise and low resolutions. The experimental results demonstrate the robustness and accuracy over state-of-the-art multimodal image registration algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_A_Deep_Step_Pattern_Representation_for_Multimodal_Retinal_Image_Registration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_A_Deep_Step_Pattern_Representation_for_Multimodal_Retinal_Image_Registration_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008309/,"['Retina', 'Image edge detection', 'Image registration', 'Feature extraction', 'Diseases', 'Biomedical imaging']","['Image Registration', 'Multimodal Imaging', 'Retinal Images', 'Multi-modal Registration', 'Multimodal Image Registration', 'Volume Change', 'Low Resolution', 'Convolutional Neural Network', 'Imaging Modalities', 'Multiple Modalities', 'Image Pairs', 'Speckle Noise', 'Size Range', 'Classification Accuracy', 'Intersection Point', 'Scale Changes', 'Retinal Diseases', 'Increase In Accuracy', 'Fundus Photography', 'Transformation Function', 'Fluorescein Angiography Images', 'Color Fundus Images', 'Fundus Images', 'Scale-invariant Feature Transform', 'Classification Confidence', 'Rotation Invariance', 'Difference Of Gaussian', 'Random Sample Consensus', 'Second-order Polynomial Model', 'Multimodal Model']",,29,"This paper presents a novel feature-based method that is built upon a convolutional neural network (CNN) to learn the deep representation for multimodal retinal image registration. We coined the algorithm deep step patterns, in short DeepSPa. Most existing deep learning based methods require a set of manually labeled training data with known corresponding spatial transformations, which limits the size of training datasets. By contrast, our method is fully automatic and scale well to different image modalities with no human intervention. We generate feature classes from simple step patterns within patches of connecting edges formed by vascular junctions in multiple retinal imaging modalities. We leverage CNN to learn and optimize the input patches to be used for image registration. Spatial transformations are estimated based on the output possibility of the fully connected layer of CNN for a pair of images. One of the key advantages of the proposed algorithm is its robustness to non-linear intensity changes, which widely exist on retinal images due to the difference of acquisition modalities. We validate our algorithm on extensive challenging datasets comprising poor quality multimodal retinal images which are adversely affected by pathologies (diseases), speckle noise and low resolutions. The experimental results demonstrate the robustness and accuracy over state-of-the-art multimodal image registration algorithms."
A Delay Metric for Video Object Detection: What Average Precision Fails to Tell,"Huizi Mao, Xiaodong Yang, William J. Dally",Stanford University; NVIDIA; Stanford University & NVIDIA,66.66666666666666,usa,33.33333333333334,USA,"Average precision (AP) is a widely used metric to evaluate detection accuracy of image and video object detectors. In this paper, we analyze the object detection from video and point out that mAP alone is not sufficient to capture the temporal nature of video object detection. To tackle this problem, we propose a comprehensive metric, Average Delay (AD), to measure and compare detection delay. To facilitate delay evaluation, we carefully select a subset of ImageNet VID, which we name as ImageNet VIDT with an emphasis on complex trajectories. By extensively evaluating a wide range of detectors on VIDT, we show that most methods drastically increase the detection delay but still preserve mAP well. In other words, mAP is not sensitive enough to reflect the temporal characteristics of a video object detector. Our results suggest that video object detection methods should be evaluated with a delay metric, particularly for latency-critical applications such as autonomous vehicle perception.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mao_A_Delay_Metric_for_Video_Object_Detection_What_Average_Precision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_A_Delay_Metric_for_Video_Object_Detection_What_Average_Precision_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009003/,"['Delays', 'Object detection', 'Detectors', 'Streaming media', 'Autonomous vehicles', 'Task analysis']","['Object Detection', 'Average Precision', 'Video Object Detection', 'Detection Accuracy', 'Autonomous Vehicles', 'Average Delay', 'Detection Delay', 'False Positive', 'Maximum Likelihood Estimation', 'False Alarm', 'Exponential Distribution', 'Bounding Box', 'False Alarm Rate', 'Optical Flow', 'Anomaly Detection', 'Heavy-tailed', 'Identical Distribution', 'Feature Aggregation', 'Mean Average Precision', 'Faster R-CNN', 'False Positive Ratio', 'Key Frames', 'Image Object Detection', 'Delay Distribution', 'Bounding Box Coordinates', 'Cascade System', 'Detection Window', 'Minimum Delay', 'Video Sequences', 'Faster R-CNN Model']",,22,"Average precision (AP) is a widely used metric to evaluate detection accuracy of image and video object detectors. In this paper, we analyze the object detection from video and point out that mAP alone is not sufficient to capture the temporal nature of video object detection. To tackle this problem, we propose a comprehensive metric, Average Delay (AD), to measure and compare detection delay. To facilitate delay evaluation, we carefully select a subset of ImageNet VID, which we name as ImageNet VIDT with an emphasis on complex trajectories. By extensively evaluating a wide range of detectors on VIDT, we show that most methods drastically increase the detection delay but still preserve mAP well. In other words, mAP is not sensitive enough to reflect the temporal characteristics of a video object detector. Our results suggest that video object detection methods should be evaluated with a delay metric, particularly for latency-critical applications such as autonomous vehicle perception."
A Differential Volumetric Approach to Multi-View Photometric Stereo,"Fotios Logothetis, Roberto Mecca, Roberto Cipolla","Toshiba Research, Cambridge, United Kingdom; Department of Engineering, University of Cambridge, United Kingdom",50.0,uk,50.0,United Kingdom,"Highly accurate 3D volumetric reconstruction is still an open research topic where the main difficulty is usually related to merging some rough estimations with high frequency details. One of the most promising methods is the fusion between multi-view stereo and photometric stereo images. Beside the intrinsic difficulties that multi-view stereo and photometric stereo in order to work reliably, supplementary problems arise when considered together. In this work, we present a volumetric approach to the multi-view photometric stereo problem. The key point of our method is the signed distance field parameterisation and its relation to the surface normal. This is exploited in order to obtain a linear partial differential equation which is solved in a variational framework, that combines multiple images from multiple points of view in a single system. In addition, the volumetric approach is naturally implemented on an octree, which allows for fast ray-tracing that reliably alleviates occlusions and cast shadows. Our approach is evaluated on synthetic and real data-sets and achieves state-of-the-art results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Logothetis_A_Differential_Volumetric_Approach_to_Multi-View_Photometric_Stereo_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Logothetis_A_Differential_Volumetric_Approach_to_Multi-View_Photometric_Stereo_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009832/,"['Three-dimensional displays', 'Mathematical model', 'Image reconstruction', 'Light sources', 'Lighting', 'Geometry', 'Surface reconstruction']","['Multi-view Stereo', 'Volumetric Approach', 'Photometric Stereo', 'Differential Equations', 'Multiple Viewpoints', 'Cast Shadows', 'Signed Distance Function', 'Linear Partial Differential Equations', 'Volumetric Reconstruction', 'Light Source', 'Computer Vision', 'Multiple Objects', 'Broad-leaved', 'Optical Flow', '3D Mesh', '3D Geometry', 'Least Squares Problem', 'Structure From Motion', 'Global Coordinate System', 'Object Reconstruction', 'Point Light Source', 'Multi-view Images']",,25,"Highly accurate 3D volumetric reconstruction is still an open research topic where the main difficulty is usually related to merging some rough estimations with high frequency details. One of the most promising methods is the fusion between multi-view stereo and photometric stereo images. Beside the intrinsic difficulties that multi-view stereo and photometric stereo in order to work reliably, supplementary problems arise when considered together. In this work, we present a volumetric approach to the multi-view photometric stereo problem. The key point of our method is the signed distance field parameterisation and its relation to the surface normal. This is exploited in order to obtain a linear partial differential equation which is solved in a variational framework, that combines multiple images from multiple points of view in a single system. In addition, the volumetric approach is naturally implemented on an octree, which allows for fast ray-tracing that reliably alleviates occlusions and cast shadows. Our approach is evaluated on synthetic and real data-sets and achieves state-of-the-art results."
A Dual-Path Model With Adaptive Attention for Vehicle Re-Identification,"Pirazh Khorramshahi, Amit Kumar, Neehar Peri, Sai Saketh Rambhatla, Jun-Cheng Chen, Rama Chellappa","Research Center for Information Technology Innovation, Academia Sinica; Center for Automation Research, UMIACS, University of Maryland, College Park",50.0,usa,50.0,Unknown,"In recent years, attention models have been extensively used for person and vehicle re-identification. Most re-identification methods are designed to focus attention on key-point locations. However, depending on the orientation, the contribution of each key-point varies. In this paper, we present a novel dual-path adaptive attention model for vehicle re-identification (AAVER). The global appearance path captures macroscopic vehicle features while the orientation conditioned part appearance path learns to capture localized discriminative features by focusing attention on the most informative key-points. Through extensive experimentation, we show that the proposed AAVER method is able to accurately re-identify vehicles in unconstrained scenarios, yielding state of the art results on the challenging dataset VeRi-776. As a byproduct, the proposed system is also able to accurately predict vehicle key-points and shows an improvement of more than 7% over state of the art. The code for key-point estimation model is available at https://github.com/Pirazh/Vehicle_Key_ Point_Orientation_Estimation",,http://openaccess.thecvf.com/content_ICCV_2019/html/Khorramshahi_A_Dual-Path_Model_With_Adaptive_Attention_for_Vehicle_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Khorramshahi_A_Dual-Path_Model_With_Adaptive_Attention_for_Vehicle_Re-Identification_ICCV_2019_paper.pdf,,https://github.com/Pirazh/Vehicle_Key_Point_Orientation_Estimation,,main,Oral,https://ieeexplore.ieee.org/document/9010356/,"['Feature extraction', 'Adaptation models', 'Heating systems', 'Estimation', 'Automobiles', 'Training', 'Task analysis']","['Vehicle Re-identification', 'Adaptive Attention', 'Dual-path Model', 'Local Features', 'State Of The Art', 'Discriminative Features', 'Attention Model', 'Macroscopic Features', 'Keypoint Locations', 'Global Features', 'Large-scale Datasets', 'Multilayer Perceptron', 'Deep Convolutional Neural Network', 'Imaging Probes', 'Two-stage Model', 'Post-processing Step', 'Appearance Features', 'Adaptive Selection', 'Markov Random Field', 'Orientation Of Groups', 'Vehicle Images', 'Extract Discriminative Features', 'Human Pose Estimation', 'Orientation Estimation', 'Keypoint Detection', 'Feature Pooling', 'Global Feature Extraction', 'Vehicle Identification', 'Vehicle Parts', 'Single Branch']",,133,"In recent years, attention models have been extensively used for person and vehicle re-identification. Most re-identification methods are designed to focus attention on key-point locations. However, depending on the orientation, the contribution of each key-point varies. In this paper, we present a novel dual-path adaptive attention model for vehicle re-identification (AAVER). The global appearance path captures macroscopic vehicle features while the orientation conditioned part appearance path learns to capture localized discriminative features by focusing attention on the most informative key-points. Through extensive experimentation, we show that the proposed AAVER method is able to accurately re-identify vehicles in unconstrained scenarios, yielding state of the art results on the challenging dataset VeRi-776. As a byproduct, the proposed system is also able to accurately predict vehicle key-points and shows an improvement of more than 7% over state of the art. The code for key-point estimation model is available at https://github.com/Pirazh/Vehicle_Key_ Point_Orientation_Estimation."
A Fast and Accurate One-Stage Approach to Visual Grounding,"Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo",University of Rochester; Tencent AI Lab,50.0,usa,50.0,China,"We propose a simple, fast, and accurate one-stage approach to visual grounding, inspired by the following insight. The performances of existing propose-and-rank two-stage methods are capped by the quality of the region candidates they propose in the first stage --- if none of the candidates could cover the ground truth region, there is no hope in the second stage to rank the right region to the top. To avoid this caveat, we propose a one-stage model that enables end-to-end joint optimization. The main idea is as straightforward as fusing a text query's embedding into the YOLOv3 object detector, augmented by spatial features so as to account for spatial mentions in the query. Despite being simple, this one-stage approach shows great potential in terms of both accuracy and speed for both phrase localization and referring expression comprehension, according to our experiments. Given these results along with careful investigations into some popular region proposals, we advocate for visual grounding a paradigm shift from the conventional two-stage methods to the one-stage framework.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_A_Fast_and_Accurate_One-Stage_Approach_to_Visual_Grounding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_A_Fast_and_Accurate_One-Stage_Approach_to_Visual_Grounding_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010627/,"['Visualization', 'Grounding', 'Feature extraction', 'Proposals', 'Spatial resolution', 'Optimization', 'Encoding']","['One-stage Approach', 'Visual Grounding', 'Spatial Features', 'Object Detection', 'Two-stage Method', 'Region Proposal', 'Text Query', 'One-stage Model', 'Spatial Resolution', 'Training Set', 'Input Image', 'Feature Maps', 'Visual Features', 'Image Regions', 'Bounding Box', 'Softmax Function', 'Similarity Network', 'Candidate Regions', 'Visual Problems', 'Textual Features', 'One-stage Methods', 'Query Language', 'Kinds Of Errors', 'Anchor Boxes', 'Darknet', 'Two-stage Framework', 'Mask R-CNN', 'Tiny Objects', 'Spatial Configuration', 'Feature Encoder']",,204,"We propose a simple, fast, and accurate one-stage approach to visual grounding, inspired by the following insight. The performances of existing propose-and-rank two-stage methods are capped by the quality of the region candidates they propose in the first stage --- if none of the candidates could cover the ground truth region, there is no hope in the second stage to rank the right region to the top. To avoid this caveat, we propose a one-stage model that enables end-to-end joint optimization. The main idea is as straightforward as fusing a text query's embedding into the YOLOv3 object detector, augmented by spatial features so as to account for spatial mentions in the query. Despite being simple, this one-stage approach shows great potential in terms of both accuracy and speed for both phrase localization and referring expression comprehension, according to our experiments. Given these results along with careful investigations into some popular region proposals, we advocate for visual grounding a paradigm shift from the conventional two-stage methods to the one-stage framework."
A Geometry-Inspired Decision-Based Attack,"Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard","École Polytechnique Fédérale de Lausanne, Switzerland; University of Science and Technology of China",100.0,"Switzerland, china",0.0,,"Deep neural networks have recently achieved tremendous success in image classification. Recent studies have however shown that they are easily misled into incorrect classification decisions by adversarial examples. Adversaries can even craft attacks by querying the model in black-box settings, where no information about the model is released except its final decision. Such decision-based attacks usually require lots of queries, while real-world image recognition systems might actually restrict the number of queries. In this paper, we propose qFool, a novel decision-based attack algorithm that can generate adversarial examples using a small number of queries. The qFool method can drastically reduce the number of queries compared to previous decision-based attacks while reaching the same quality of adversarial examples. We also enhance our method by constraining adversarial perturbations in low-frequency subspace, which can make qFool even more computationally efficient. Altogether, we manage to fool commercial image recognition systems with a small number of queries, which demonstrates the actual effectiveness of our new algorithm in practice.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_A_Geometry-Inspired_Decision-Based_Attack_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_A_Geometry-Inspired_Decision-Based_Attack_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010924/,"['Perturbation methods', 'Computational modeling', 'Neural networks', 'Image recognition', 'Estimation', 'Predictive models', 'Gaussian noise']","['Decision-based Attacks', 'Deep Network', 'Deep Neural Network', 'Image Recognition', 'Adversarial Examples', 'Previous Attack', 'Adversarial Perturbations', 'Gaussian Noise', 'Random Noise', 'Target Image', 'Gradient Approximation', 'Gradient Direction', 'Decision Boundary', 'Types Of Attacks', 'Target Model', 'Imperceptible', 'Neighboring Points', 'Noise Vector', 'Estimation Step', 'Target Label', 'Black-box Attacks', 'Fast Gradient Sign Method', 'White-box Attack', 'Discrete Cosine Transform', 'Saliency Map', 'Gradient Of The Loss Function', 'Binary Search']",,27,"Deep neural networks have recently achieved tremendous success in image classification. Recent studies have however shown that they are easily misled into incorrect classification decisions by adversarial examples. Adversaries can even craft attacks by querying the model in black-box settings, where no information about the model is released except its final decision. Such decision-based attacks usually require lots of queries, while real-world image recognition systems might actually restrict the number of queries. In this paper, we propose qFool, a novel decision-based attack algorithm that can generate adversarial examples using a small number of queries. The qFool method can drastically reduce the number of queries compared to previous decision-based attacks while reaching the same quality of adversarial examples. We also enhance our method by constraining adversarial perturbations in low-frequency subspace, which can make qFool even more computationally efficient. Altogether, we manage to fool commercial image recognition systems with a small number of queries, which demonstrates the actual effectiveness of our new algorithm in practice."
A Graph-Based Framework to Bridge Movies and Synopses,"Yu Xiong, Qingqiu Huang, Lingfeng Guo, Hang Zhou, Bolei Zhou, Dahua Lin","University of California, Berkeley; CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong",100.0,"Hong Kong, china, usa",0.0,,"Inspired by the remarkable advances in video analytics, research teams are stepping towards a greater ambition - movie understanding. However, compared to those activity videos in conventional datasets, movies are significantly different. Generally, movies are much longer and consist of much richer temporal structures. More importantly, the interactions among characters play a central role in expressing the underlying story. To facilitate the efforts along this direction, we construct a dataset called Movie Synopses Associations (MSA) over 327 movies, which provides a synopsis for each movie, together with annotated associations between synopsis paragraphs and movie segments. On top of this dataset, we develop a framework to perform matching between movie segments and synopsis paragraphs. This framework integrates different aspects of a movie, including event dynamics and character interactions, and allows them to be matched with parsed paragraphs, based on a graph-based formulation. Our study shows that the proposed framework remarkably improves the matching accuracy over conventional feature-based methods. It also reveals the importance of narrative structures and character interactions in movie understanding. Dataset and code are available at: https://ycxioooong.github.io/projects/moviesyn",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_A_Graph-Based_Framework_to_Bridge_Movies_and_Synopses_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_A_Graph-Based_Framework_to_Bridge_Movies_and_Synopses_ICCV_2019_paper.pdf,https://ycxioooong.github.io/projects/moviesyn,https://github.com/ycxioooong/moviesyn,,main,Oral,https://ieeexplore.ieee.org/document/9009791/,"['Motion pictures', 'Semantics', 'Task analysis', 'Visualization', 'Optimization', 'Training', 'Computer vision']","['Temporal Structure', 'Narrative Structure', 'Visual Features', 'Manual Annotation', 'Matching Score', 'Word2vec', 'Graph-based Methods', 'Query Language', 'Node Activity', 'Short Clips', 'Network Embedding', 'Pool Of Candidates', 'Temporal Interactions', 'Graph Matching', 'High-level Semantics', 'Flow Of Events', 'Shot Sequence', 'Video Retrieval']",,37,"Inspired by the remarkable advances in video analytics, research teams are stepping towards a greater ambition - movie understanding. However, compared to those activity videos in conventional datasets, movies are significantly different. Generally, movies are much longer and consist of much richer temporal structures. More importantly, the interactions among characters play a central role in expressing the underlying story. To facilitate the efforts along this direction, we construct a dataset called Movie Synopses Associations (MSA) over 327 movies, which provides a synopsis for each movie, together with annotated associations between synopsis paragraphs and movie segments. On top of this dataset, we develop a framework to perform matching between movie segments and synopsis paragraphs. This framework integrates different aspects of a movie, including event dynamics and character interactions, and allows them to be matched with parsed paragraphs, based on a graph-based formulation. Our study shows that the proposed framework remarkably improves the matching accuracy over conventional feature-based methods. It also reveals the importance of narrative structures and character interactions in movie understanding. Dataset and code are available at: https://ycxioooong.github.io/projects/moviesyn."
A Learned Representation for Scalable Vector Graphics,"Raphael Gontijo Lopes, David Ha, Douglas Eck, Jonathon Shlens",Google Brain,0.0,,100.0,USA,"Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts crawled from the web and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lopes_A_Learned_Representation_for_Scalable_Vector_Graphics_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lopes_A_Learned_Representation_for_Scalable_Vector_Graphics_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008800/,"['Decoding', 'Computational modeling', 'Visualization', 'Training', 'Probabilistic logic', 'Task analysis']","['Representation Learning', 'Filtering Effect', 'Vector Graphics', 'Imagery', 'Sequential Model', 'Latent Representation', 'Training Set', 'Individual Characteristics', 'Sequence Length', 'Log-likelihood', 'Probabilistic Model', 'High Loss', 'Model Quality', 'Autoregressive Model', 'Class Labels', 'Generative Adversarial Networks', 'Latent Space', 'Appendix For Details', 'Variational Autoencoder', 'Directions In Space', 'Image Synthesis', 'Label Noise', 'Latent Code']",,53,"Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts crawled from the web and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design."
A Neural Network for Detailed Human Depth Estimation From a Single Image,"Sicong Tang, Feitong Tan, Kelvin Cheng, Zhaoyang Li, Siyu Zhu, Ping Tan",Alibaba A.I Labs; Simon Fraser University,50.0,canada,50.0,China,"This paper presents a neural network to estimate a detailed depth map of the foreground human in a single RGB image. The result captures geometry details such as cloth wrinkles, which are important in visualization applications. To achieve this goal, we separate the depth map into a smooth base shape and a residual detail shape and design a network with two branches to regress them respectively. We design a training strategy to ensure both base and detail shapes can be faithfully learned by the corresponding network branches. Furthermore, we introduce a novel network layer to fuse a rough depth map and surface normals to further improve the final result. Quantitative comparison with fused `ground truth' captured by real depth cameras and qualitative examples on unconstrained Internet images demonstrate the strength of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_A_Neural_Network_for_Detailed_Human_Depth_Estimation_From_a_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_A_Neural_Network_for_Detailed_Human_Depth_Estimation_From_a_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008269/,"['Shape', 'Three-dimensional displays', 'Heating systems', 'Estimation', 'Image segmentation', 'Training', 'Skeleton']","['Neural Network', 'Single Image', 'Depth Estimation', 'Wrinkles', 'RGB Images', 'Detailed Mapping', 'Depth Map', 'Depth Camera', 'Vision Applications', 'Detailed Shape', 'Qualitative Examples', 'Smooth Shape', 'Surface Normals', 'Heatmap', 'Convolutional Neural Network', 'Stage 2', 'Body Shape', 'Depth Images', '3D Shape', 'Human Pose Estimation', '3D Joint', '3D Pose', 'Input RGB Image', 'Ground Truth Depth', 'Huber Loss', 'Pose Estimation', 'Bilateral Filter', 'Normal Map', 'Human Shape']",,39,"This paper presents a neural network to estimate a detailed depth map of the foreground human in a single RGB image. The result captures geometry details such as cloth wrinkles, which are important in visualization applications. To achieve this goal, we separate the depth map into a smooth base shape and a residual detail shape and design a network with two branches to regress them respectively. We design a training strategy to ensure both base and detail shapes can be faithfully learned by the corresponding network branches. Furthermore, we introduce a novel network layer to fuse a rough depth map and surface normals to further improve the final result. Quantitative comparison with fused `ground truth' captured by real depth cameras and qualitative examples on unconstrained Internet images demonstrate the strength of the proposed method."
A Novel Unsupervised Camera-Aware Domain Adaptation Framework for Person Re-Identification,"Lei Qi, Lei Wang, Jing Huo, Luping Zhou, Yinghuan Shi, Yang Gao","State Key Laboratory for Novel Software Technology, Nanjing University; School of Electrical and Information Engineering, The University of Sydney; School of Computing and Information Technology, University of Wollongong",100.0,"australia, china",0.0,,"Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of discriminative information in target domain. From the perspective of representation learning, this paper proposes a novel end-to-end deep domain adaptation framework to address them. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic in person Re-ID, and develop a ""camera-aware"" domain adaptation method via adversarial learning. With this method, the learned representation reduces distribution discrepancy not only between source and target domains but also across all cameras. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved representation in training process. Together, the above two methods give rise to a new unsupervised domain adaptation framework for person Re-ID. Extensive experiments and ablation studies conducted on benchmark datasets demonstrate its superiority and interesting properties.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qi_A_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_A_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008837/,"['Cameras', 'Training', 'Task analysis', 'Optimization', 'Generators', 'Benchmark testing', 'Image resolution']","['Domain Adaptation', 'Domain Adaptation Framework', 'Unsupervised Domain Adaptation Framework', 'Feature Representation', 'Representation Learning', 'Target Domain', 'Source Domain', 'Learning Perspective', 'Positive Samples', 'Temporal Information', 'Generative Adversarial Networks', 'Target Image', 'Positive Image', 'Viewing Angle', 'Negative Images', 'Matrix M', 'Backbone Network', 'Dirac Delta', 'Triplet Loss', 'Camera Level', 'Unlabeled Target Domain', 'Domain Adaptation Methods', 'Discriminator Loss', 'Feature Representation Learning', 'Gallery Images', 'Unsupervised Domain Adaptation Methods', 'Query Image', 'Maximum Mean Discrepancy', 'Training Batch']",,106,"Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of discriminative information in target domain. From the perspective of representation learning, this paper proposes a novel end-to-end deep domain adaptation framework to address them. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic in person Re-ID, and develop a “camera-aware” domain adaptation method via adversarial learning. With this method, the learned representation reduces distribution discrepancy not only between source and target domains but also across all cameras. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved representation in training process. Together, the above two methods give rise to a new unsupervised domain adaptation framework for person Re-ID. Extensive experiments and ablation studies conducted on benchmark datasets demonstrate its superiority and interesting properties."
A Quaternion-Based Certifiably Optimal Solution to the Wahba Problem With Outliers,"Heng Yang, Luca Carlone","Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology",100.0,usa,0.0,,"The Wahba problem, also known as rotation search, seeks to find the best rotation to align two sets of vector observations given putative correspondences, and is a fundamental routine in many computer vision and robotics applications. This work proposes the first polynomial-time certifiably optimal approach for solving the Wahba problem when a large number of vector observations are outliers. Our first contribution is to formulate the Wahba problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. The second contribution is to rewrite the problem using unit quaternions and show that the TLS cost can be framed as a Quadratically-Constrained Quadratic Program (QCQP). Since the resulting optimization is still highly non-convex and hard to solve globally, our third contribution is to develop a convex Semidefinite Programming (SDP) relaxation. We show that while a naive relaxation performs poorly in general, our relaxation is tight even in the presence of large noise and outliers. We validate the proposed algorithm, named QUASAR (QUAternion-based Semidefinite relAxation for Robust alignment), in both synthetic and real datasets showing that the algorithm outperforms RANSAC, robust local optimization techniques, global outlier-removal procedures, and Branch-and-Bound methods. QUASAR is able to compute certifiably optimal solutions (i.e. the relaxation is exact) even in the case when 95% of the correspondences are outliers.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_A_Quaternion-Based_Certifiably_Optimal_Solution_to_the_Wahba_Problem_With_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_A_Quaternion-Based_Certifiably_Optimal_Solution_to_the_Wahba_Problem_With_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009001/,"['Robustness', 'Three-dimensional displays', 'Search problems', 'Quaternions', 'Cost function', 'Computer vision']","['Least-squares', 'Computer Vision', 'Presence Of Noise', 'Presence Of Outliers', 'Robust Optimization', 'Large Noise', 'Computer Vision Applications', 'Semidefinite Programming', 'Semidefinite Relaxation', 'Unit Quaternion', 'Branch-and-bound Method', 'Maximum Likelihood Estimation', 'Cost Function', 'Unit Vector', '3D Reconstruction', 'Global Optimization', 'Point Cloud', 'Local Method', 'Mixed-integer Programming', 'Extreme Outliers', 'Point Cloud Registration', 'Robust Formulation', 'High Outliers', '3D Rotation', 'Motion Estimation', 'Outlier Removal', 'Convex Relaxation', 'Large Residuals', 'Cloud Images', 'Camera Frame']",,38,"The Wahba problem, also known as rotation search, seeks to find the best rotation to align two sets of vector observations given putative correspondences, and is a fundamental routine in many computer vision and robotics applications. This work proposes the first polynomial-time certifiably optimal approach for solving the Wahba problem when a large number of vector observations are outliers. Our first contribution is to formulate the Wahba problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. The second contribution is to rewrite the problem using unit quaternions and show that the TLS cost can be framed as a Quadratically-Constrained Quadratic Program (QCQP). Since the resulting optimization is still highly non-convex and hard to solve globally, our third contribution is to develop a convex Semidefinite Programming (SDP) relaxation. We show that while a naive relaxation performs poorly in general, our relaxation is tight even in the presence of large noise and outliers. We validate the proposed algorithm, named QUASAR (QUAternion-based Semidefinite relAxation for Robust alignment), in both synthetic and real datasets showing that the algorithm outperforms RANSAC, robust local optimization techniques, global outlier-removal procedures, and Branch-and-Bound methods. QUASAR is able to compute certifiably optimal solutions (i.e. the relaxation is exact) even in the case when 95% of the correspondences are outliers."
A Robust Learning Approach to Domain Adaptive Object Detection,"Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, William G. Macready","Simon Fraser University, Burnaby, Canada; Quadrant, Burnaby, Canada; NVIDIA, California, USA",33.33333333333333,canada,66.66666666666667,Canada,"Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Khodabandeh_A_Robust_Learning_Approach_to_Domain_Adaptive_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Khodabandeh_A_Robust_Learning_Approach_to_Domain_Adaptive_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008383/,"['Object detection', 'Robustness', 'Training', 'Adaptation models', 'Noise measurement', 'Feature extraction', 'Computational modeling']","['Object Detection', 'Domain Adaptation', 'Robust Learning', 'Training Data', 'Class Labels', 'Detection Model', 'Bounding Box', 'Domain Shift', 'Target Domain', 'Robust Detection', 'Self-driving', 'Source Domain', 'Object Bounding Boxes', 'Noisy Labels', 'Label Noise', 'Bounding Box Size', 'Bounding Box Location', 'Training Set', 'Convolutional Neural Network', 'Image Classification', 'Faster R-CNN', 'Region Proposal Network', 'Robust Training', 'Object Proposals', 'Truth Labels', 'Multiple Kernel Learning', 'Object Labels', 'Bounding Box Annotations', 'Image X', 'Object Location']",,165,"Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets."
A Tour of Convolutional Networks Guided by Linear Interpreters,"Pablo Navarrete Michelini, Hanwen Liu, Yunhua Lu, Xingqun Jiang","BOE Technology Co., Ltd.",0.0,,100.0,China,"Convolutional networks are large linear systems divided into layers and connected by non-linear units. These units are the ""articulations"" that allow the network to adapt to the input. To understand how a network manages to solve a problem we must look at the articulated decisions in entirety. If we could capture the actions of non-linear units for a particular input, we would be able to replay the whole system back and forth as if it was always linear. It would also reveal the actions of non-linearities because the resulting linear system, a Linear Interpreter, depends on the input image. We introduce a hooking layer, called a LinearScope, which allows us to run the network and the linear interpreter in parallel. Its implementation is simple, flexible and efficient. From here we can make many curious inquiries: how do these linear systems look like? When the rows and columns of the transformation matrix are images, how do they look like? What type of basis do these linear transformations rely on? The answers depend on the problems presented, through which we take a tour to some popular architectures used for classification, super-resolution (SR) and image-to-image translation (I2I). For classification we observe that popular networks use a pixel-wise vote per class strategy and heavily rely on bias parameters. For SR and I2I we find that CNNs use wavelet-type basis similar to the human visual system. For I2I we reveal copy-move and template-creation strategies to generate outputs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Michelini_A_Tour_of_Convolutional_Networks_Guided_by_Linear_Interpreters_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Michelini_A_Tour_of_Convolutional_Networks_Guided_by_Linear_Interpreters_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010656/,"['Linear systems', 'Visualization', 'Visual systems', 'Task analysis', 'Benchmark testing', 'Shape', 'Switches']","['Convolutional Network', 'Input Image', 'Column Vector', 'Wavelet', 'Linear System', 'Super-resolution', 'Nonlinear Unit', 'Deep Learning', 'Transposable', 'Deep Neural Network', 'Singular Value', 'Batch Normalization', 'Singular Value Decomposition', 'Taylor Expansion', 'Fully-connected Layer', 'Similar Interpretation', 'Harmonic Functions', 'Left Eigenvectors', 'Input Pixels', 'Sequence Network', 'Filter Matrix', 'Output Domain', 'Forward Projection', 'Output Pixel']",,4,"Convolutional networks are large linear systems divided into layers and connected by non-linear units. These units are the ""articulations"" that allow the network to adapt to the input. To understand how a network manages to solve a problem we must look at the articulated decisions in entirety. If we could capture the actions of non-linear units for a particular input, we would be able to replay the whole system back and forth as if it was always linear. It would also reveal the actions of non-linearities because the resulting linear system, a Linear Interpreter, depends on the input image. We introduce a hooking layer, called a LinearScope, which allows us to run the network and the linear interpreter in parallel. Its implementation is simple, flexible and efficient. From here we can make many curious inquiries: how do these linear systems look like? When the rows and columns of the transformation matrix are images, how do they look like? What type of basis do these linear transformations rely on? The answers depend on the problems presented, through which we take a tour to some popular architectures used for classification, super-resolution (SR) and image-to-image translation (I2I). For classification we observe that popular networks use a pixel-wise vote per class strategy and heavily rely on bias parameters. For SR and I2I we find that CNNs use wavelet-type basis similar to the human visual system. For I2I we reveal copy-move and template-creation strategies to generate outputs."
A Weakly Supervised Fine Label Classifier Enhanced by Coarse Supervision,"Fariborz Taherkhani, Hadi Kazemi, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi","Lane Department of Computer Science and Electrical Engineering, West Virginia University",100.0,usa,0.0,,"Objects are usually organized in a hierarchical structure in which each coarse category (e.g., big cat) corresponds to a superclass of several fine categories (e.g., cheetah, leopard). The objects grouped within the same coarse category, but in different fine categories, usually share a set of global visual features; however, these objects have distinctive local properties that characterize them at a fine level. This paper addresses the challenge of fine image classification in a weakly supervised fashion, whereby a subset of images is tagged by fine labels, while the remaining are tagged by coarse labels. We propose a new deep model that leverages coarse images to improve the classification performance of fine images within the coarse category. Our model is an end to end framework consisting of a Convolutional Neural Network (CNN) which uses both fine and coarse images to tune its parameters. The CNN outputs are then fanned out into two separate branches such that the first branch uses a supervised low rank self expressive layer to project the CNN outputs to the low rank subspaces to capture the global structures for the coarse classification, while the other branch uses a supervised sparse self expressive layer to project them to the sparse subspaces to capture the local structures for the fine classification. Our deep model uses coarse images in conjunction with fine images to jointly explore the low rank and sparse subspaces by sharing the parameters during the training which causes the data points obtained by the CNN to be well-projected to both sparse and low rank subspaces for classification.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010411/,"['Data models', 'Cats', 'Training', 'Visualization', 'Data mining', 'Computer vision']","['Fine Labels', 'Convolutional Neural Network', 'Hierarchical Structure', 'Local Structure', 'Deep Models', 'Image Classification', 'Final Classification', 'Global Structure', 'Final Image', 'Final Level', 'Separate Branch', 'Subset Of Images', 'Cheetah', 'Convolutional Neural Networks Output', 'Big Cats', 'Coarse Classification', 'Training Set', 'Validation Set', 'Recurrent Neural Network', 'Low-rank Representation', 'Lower Rank', 'Affinity Matrix', 'Number Of Data Points', 'Contrastive Loss', 'Case Of Model', 'Sparse Representation', 'Sparsity Constraint', 'Fine-tuning Step', 'Convolutional Neural Network Features']",,9,"Objects are usually organized in a hierarchical structure in which each coarse category (e.g., big cat) corresponds to a superclass of several fine categories (e.g., cheetah, leopard). The objects grouped within the same coarse category, but in different fine categories, usually share a set of global visual features; however, these objects have distinctive local properties that characterize them at a fine level. This paper addresses the challenge of fine image classification in a weakly supervised fashion, whereby a subset of images is tagged by fine labels, while the remaining are tagged by coarse labels. We propose a new deep model that leverages coarse images to improve the classification performance of fine images within the coarse category. Our model is an end to end framework consisting of a Convolutional Neural Network (CNN) which uses both fine and coarse images to tune its parameters. The CNN outputs are then fanned out into two separate branches such that the first branch uses a supervised low rank self expressive layer to project the CNN outputs to the low rank subspaces to capture the global structures for the coarse classification, while the other branch uses a supervised sparse self expressive layer to project them to the sparse subspaces to capture the local structures for the fine classification. Our deep model uses coarse images in conjunction with fine images to jointly explore the low rank and sparse subspaces by sharing the parameters during the training which causes the data points obtained by the CNN to be well-projected to both sparse and low rank subspaces for classification."
A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation From a Single Depth Image,"Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong Yu, Joey Tianyi Zhou, Junsong Yuan","IHPC, A*STAR, Singapore; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artiﬁcial Intelligence and Automation, Huazhong University of Science and Technology; CSE Department, State University of New York at Buffalo",100.0,"china, singapore, usa",0.0,,"For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet- 50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_A2J_Anchor-to-Joint_Regression_Network_for_3D_Articulated_Pose_Estimation_From_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_A2J_Anchor-to-Joint_Regression_Network_for_3D_Articulated_Pose_Estimation_From_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010871/,"['Three-dimensional displays', 'Two dimensional displays', 'Pose estimation', 'Proposals', 'Machine learning', 'Task analysis']","['Depth Images', 'Pose Estimation', 'Regression Network', 'Single Depth Image', 'High Speed', 'Convolutional Layers', 'Spatial Information', 'Backbone Network', 'Joint Position', 'Single GPU', 'Deconvolutional Layers', '3D Body', 'Spatial Contextual Information', 'Body Pose', 'Deep Learning', 'Feature Maps', 'Large-scale Datasets', 'ImageNet', 'Convolution Operation', 'Fully-connected Layer', 'Human Pose Estimation', 'Running Efficiency', '3D Pose', 'Global Regression', 'Offset Estimation', 'Learning Rate Decay', 'Depth Estimation', 'Spatial Layout', 'Depth Values']",,113,"For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet- 50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU."
ABD-Net: Attentive but Diverse Person Re-Identification,"Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou Ren, Zhangyang Wang",Texas A&M University; Wormpex AI Research; University of Science and Technology of China; Walmart Technology,50.0,"china, usa",50.0,China,"Attention mechanisms have been found effective for person re-identification (Re-ID). However, the learned ""attentive"" features are often not naturally uncorrelated or ""diverse"", which compromises the retrieval performance based on the Euclidean distance. We advocate the complementary powers of attention and diversity for Re-ID, by proposing an Attentive but Diverse Network (ABD-Net). ABD-Net seamlessly integrates attention modules and diversity regularizations throughout the entire network to learn features that are representative, robust, and more discriminative. Specifically, we introduce a pair of complementary attention modules, focusing on channel aggregation and position awareness, respectively. Then, we plug in a novel orthogonality constraint that efficiently enforces diversity on both hidden activations and weights. Through an extensive set of ablation study, we verify that the attentive and diverse terms each contributes to the performance boosts of ABD-Net. It consistently outperforms existing state-of-the-art methods on there popular person Re-ID benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.pdf,,https://github.com/TAMU-VITA/ABD-Net,,main,Poster,https://ieeexplore.ieee.org/document/9008377/,"['Correlation', 'Feature extraction', 'Robustness', 'Tensile stress', 'Eigenvalues and eigenfunctions', 'Computer vision', 'Euclidean distance']","['Attention Mechanism', 'Attention Network', 'Attention Module', 'Diverse Networks', 'Popular Benchmark', 'Orthogonality Constraint', 'Convolutional Neural Network', 'Body Parts', 'Convolutional Layers', 'Feature Maps', 'Singular Value', 'Cross-entropy Loss', 'Diverse Characteristics', 'Batch Normalization', 'Singular Value Decomposition', 'Final Feature', 'Correlated Features', 'Frobenius Norm', 'Multi-task Learning', 'Hard Constraints', 'Channel Attention Module', 'Gram Matrix', 'Triplet Loss', 'Dimensional Feature Vector', 'ResNet-50 Backbone', 'Final Embedding', 'Person Image', 'Global Average Pooling Layer', 'Off-diagonal']",,361,"Attention mechanisms have been found effective for person re-identification (Re-ID). However, the learned ``attentive'' features are often not naturally uncorrelated or ``diverse'', which compromises the retrieval performance based on the Euclidean distance. We advocate the complementary powers of attention and diversity for Re-ID, by proposing an Attentive but Diverse Network (ABD-Net). ABD-Net seamlessly integrates attention modules and diversity regularizations throughout the entire network to learn features that are representative, robust, and more discriminative. Specifically, we introduce a pair of complementary attention modules, focusing on channel aggregation and position awareness, respectively. Then, we plug in a novel orthogonality constraint that efficiently enforces diversity on both hidden activations and weights. Through an extensive set of ablation study, we verify that the attentive and diverse terms each contributes to the performance boosts of ABD-Net. It consistently outperforms existing state-of-the-art methods on there popular person Re-ID benchmarks."
ACE: Adapting to Changing Environments for Semantic Segmentation,"Zuxuan Wu, Xin Wang, Joseph E. Gonzalez, Tom Goldstein, Larry S. Davis",UC Berkeley; University of Maryland,100.0,usa,0.0,,"Deep neural networks exhibit exceptional accuracy when they are trained and tested on the same data distributions. However, neural classifiers are often extremely brittle when confronted with domain shift---changes in the input distribution that occur over time. We present ACE, a framework for semantic segmentation that dynamically adapts to changing environments over time. By aligning the distribution of labeled training data from the original source domain with the distribution of incoming data in a shifted domain, ACE synthesizes labeled training data for environments as it sees them. This stylized data is then used to update a segmentation model so that it performs well in new environments. To avoid forgetting knowledge from past environments, we introduce a memory that stores feature statistics from previously seen domains. These statistics can be used to replay images in any of the previously observed domains, thus preventing catastrophic forgetting. In addition to standard batch training using stochastic gradient decent (SGD), we also experiment with fast adaptation methods based on adaptive meta-learning. Extensive experiments are conducted on two datasets from SYNTHIA, the results demonstrate the effectiveness of the proposed approach when adapting to a number of tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_ACE_Adapting_to_Changing_Environments_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_ACE_Adapting_to_Changing_Environments_for_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009823/,"['Task analysis', 'Image segmentation', 'Adaptation models', 'Generators', 'Semantics', 'Training data', 'Lighting']","['Semantic Segmentation', 'Training Data', 'Deep Network', 'Deep Neural Network', 'Domain Shift', 'Segmentation Model', 'Original Source', 'Source Domain', 'Input Distribution', 'Training Batch', 'Fast Adaptation', 'Catastrophic Forgetting', 'Light Conditions', 'Feature Maps', 'Generative Adversarial Networks', 'Target Image', 'Image Generation', 'Lifelong Learning', 'Source Images', 'Target Domain', 'Source Task', 'Target Task', 'Style Image', 'Current Task', 'Style Transfer', 'Gram Matrix', 'Memory Unit', 'Previous Tasks', 'Domain Adaptation', 'Sequential Manner']",,65,"Deep neural networks exhibit exceptional accuracy when they are trained and tested on the same data distributions. However, neural classifiers are often extremely brittle when confronted with domain shift---changes in the input distribution that occur over time. We present ACE, a framework for semantic segmentation that dynamically adapts to changing environments over time. By aligning the distribution of labeled training data from the original source domain with the distribution of incoming data in a shifted domain, ACE synthesizes labeled training data for environments as it sees them. This stylized data is then used to update a segmentation model so that it performs well in new environments. To avoid forgetting knowledge from past environments, we introduce a memory that stores feature statistics from previously seen domains. These statistics can be used to replay images in any of the previously observed domains, thus preventing catastrophic forgetting. In addition to standard batch training using stochastic gradient decent (SGD), we also experiment with fast adaptation methods based on adaptive meta-learning. Extensive experiments are conducted on two datasets from SYNTHIA, the results demonstrate the effectiveness of the proposed approach when adapting to a number of tasks."
ACFNet: Attentional Class Feature Network for Semantic Segmentation,"Fan Zhang, Yanqin Chen, Zhihang Li, Zhibin Hong, Jingtuo Liu, Feifei Ma, Junyu Han, Errui Ding","Laboratory of Parallel Software and Computational Science, Institute of Software, Chinese Academy of Sciences and University of Chinese Academy of Sciences; Baidu Inc.; Laboratory of Parallel Software and Computational Science, Institute of Software, Chinese Academy of Sciences and University of Chinese Academy of Sciences and Baidu Inc.; Institute of Software, Chinese Academy of Sciences and University of Chinese Academy of Sciences",75.0,china,25.0,China,"Recent works have made great progress in semantic segmentation by exploiting richer context, most of which are designed from a spatial perspective. In contrast to previous works, we present the concept of class center which extracts the global context from a categorical perspective. This class-level context describes the overall representation of each class in an image. We further propose a novel module, named Attentional Class Feature (ACF) module, to calculate and adaptively combine different class centers according to each pixel. Based on the ACF module, we introduce a coarse-to-fine segmentation network, called Attentional Class Feature Network (ACFNet), which can be composed of an ACF module and any off-the-shell segmentation network (base network). In this paper, we use two types of base networks to evaluate the effectiveness of ACFNet. We achieve new state-of-the-art performance of 81.85% mIoU on Cityscapes dataset with only finely annotated data used for training.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_ACFNet_Attentional_Class_Feature_Network_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ACFNet_Attentional_Class_Feature_Network_for_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010415/,"['Semantics', 'Image segmentation', 'Computer vision', 'Task analysis', 'Fans', 'Software', 'Training']","['Attention Network', 'Semantic Segmentation', 'Image Classification', 'Central Concept', 'Feature Module', 'Class Center', 'Final Results', 'Convolutional Neural Network', 'Feature Maps', 'Recurrent Neural Network', 'Attention Mechanism', 'Class I', 'Class Distribution', 'Segmentation Results', 'Distribution Of Categories', 'Segmentation Of Structures', 'Global Average Pooling', 'Markov Random Field', 'Attention Map', 'Final Segmentation', 'Atrous Spatial Pyramid Pooling', 'Baseline Network', 'Pixel Features', 'Conditional Random Field', 'Dilation Rate', 'Fully Convolutional Network', 'Yellow Square', 'Attention Module', 'Image Pixels', 'Stochastic Gradient Descent']",,193,"Recent works have made great progress in semantic segmentation by exploiting richer context, most of which are designed from a spatial perspective. In contrast to previous works, we present the concept of class center which extracts the global context from a categorical perspective. This class-level context describes the overall representation of each class in an image. We further propose a novel module, named Attentional Class Feature (ACF) module, to calculate and adaptively combine different class centers according to each pixel. Based on the ACF module, we introduce a coarse-to-fine segmentation network, called Attentional Class Feature Network (ACFNet), which can be composed of an ACF module and any off-the-shell segmentation network (base network). In this paper, we use two types of base networks to evaluate the effectiveness of ACFNet. We achieve new state-of-the-art performance of 81.85% mIoU on Cityscapes dataset with only finely annotated data used for training."
ACMM: Aligned Cross-Modal Memory for Few-Shot Image and Sentence Matching,"Yan Huang, Liang Wang","Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA), University of Chinese Academy of Sciences (UCAS), Chinese Academy of Sciences Artiﬁcial Intelligence Research (CAS-AIR)",100.0,china,0.0,,"Image and sentence matching has drawn much attention recently, but due to the lack of sufficient pairwise data for training, most previous methods still cannot well associate those challenging pairs of images and sentences containing rarely appeared regions and words, i.e., few-shot content. In this work, we study this challenging scenario as few-shot image and sentence matching, and accordingly propose an Aligned Cross-Modal Memory (ACMM) model to memorize the rarely appeared content. Given a pair of image and sentence, the model first includes an aligned memory controller network to produce two sets of semantically-comparable interface vectors through cross-modal alignment. Then the interface vectors are used by modality-specific read and update operations to alternatively interact with shared memory items. The memory items persistently memorize cross-modal shared semantic representations, which can be addressed out to better enhance the representation of few-shot content. We apply the proposed model to both conventional and few-shot image and sentence matching tasks, and demonstrate its effectiveness by achieving the state-of-the-art performance on two benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_ACMM_Aligned_Cross-Modal_Memory_for_Few-Shot_Image_and_Sentence_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ACMM_Aligned_Cross-Modal_Memory_for_Few-Shot_Image_and_Sentence_Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009838/,"['Semantics', 'Task analysis', 'Training', 'Visualization', 'Radio frequency', 'Pattern matching', 'Micromechanical devices']","['Image Registration', 'Sentence Matching', 'Cross-modal Memory', 'Image Pairs', 'Conventional Imaging', 'Memory Model', 'Semantic Representations', 'Memory Control', 'Memory Items', 'Imaging Tasks', 'Read Operation', 'Sentence Pairs', 'Network Alignment', 'Update Operation', 'Shared Memory', 'Large Improvement', 'Vector Representation', 'External Resources', 'Memory Network', 'General Representation', 'Gated Recurrent Unit', 'MS COCO Dataset', 'Representative Regions', 'Graph Convolution', 'Graph Convolutional Network', 'Multiple Words', 'Shared Representation', 'Image Retrieval', 'Word Representations']",,46,"Image and sentence matching has drawn much attention recently, but due to the lack of sufficient pairwise data for training, most previous methods still cannot well associate those challenging pairs of images and sentences containing rarely appeared regions and words, i.e., few-shot content. In this work, we study this challenging scenario as few-shot image and sentence matching, and accordingly propose an Aligned Cross-Modal Memory (ACMM) model to memorize the rarely appeared content. Given a pair of image and sentence, the model first includes an aligned memory controller network to produce two sets of semantically-comparable interface vectors through cross-modal alignment. Then the interface vectors are used by modality-specific read and update operations to alternatively interact with shared memory items. The memory items persistently memorize cross-modal shared semantic representations, which can be addressed out to better enhance the representation of few-shot content. We apply the proposed model to both conventional and few-shot image and sentence matching tasks, and demonstrate its effectiveness by achieving the state-of-the-art performance on two benchmark datasets."
ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks,"Xiaohan Ding, Yuchen Guo, Guiguang Ding, Jungong Han","WMG Data Science, University of Warwick, Coventry, United Kingdom; Department of Automation, Tsinghua University; Institute for Brain and Cognitive Sciences, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist); School of Software, Tsinghua University, Beijing, China",100.0,"China, china, uk",0.0,,"As designing appropriate Convolutional Neural Network (CNN) architecture in the context of a given application usually involves heavy human works or numerous GPU hours, the research community is soliciting the architecture-neutral CNN structures, which can be easily plugged into multiple mature architectures to improve the performance on our real-world applications. We propose Asymmetric Convolution Block (ACB), an architecture-neutral structure as a CNN building block, which uses 1D asymmetric convolutions to strengthen the square convolution kernels. For an off-the-shelf architecture, we replace the standard square-kernel convolutional layers with ACBs to construct an Asymmetric Convolutional Network (ACNet), which can be trained to reach a higher level of accuracy. After training, we equivalently convert the ACNet into the same original architecture, thus requiring no extra computations anymore. We have observed that ACNet can improve the performance of various models on CIFAR and ImageNet by a clear margin. Through further experiments, we attribute the effectiveness of ACB to its capability of enhancing the model's robustness to rotational distortions and strengthening the central skeleton parts of square convolution kernels.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008405/,"['Kernel', 'Convolution', 'Computer architecture', 'Skeleton', 'Two dimensional displays', 'Standards', 'Computational modeling']","['Convolutional Neural Network', 'Asymmetric Block', 'Asymmetric Convolution', 'Asymmetric Convolution Block', 'Research Community', 'Convolutional Layers', 'Real-world Applications', 'ImageNet', 'Convolution Kernel', 'Surgical Margins', 'Convolutional Neural Network Architecture', 'Convolutional Neural Network Structure', 'Original Architecture', 'Extra Computation', 'Performance Of Various Models', 'Asymmetric Network', 'Important Parameter', 'Numeric', 'Common Practice', 'Feature Maps', '3D Kernel', 'Batch Normalization', 'Kernel Weight', 'Flipped Images', 'Batch Normalization Layer', 'Training Configurations', 'Data Augmentation', 'AlexNet', 'Parallel Layers', 'Benchmark Model']",,493,"As designing appropriate Convolutional Neural Network (CNN) architecture in the context of a given application usually involves heavy human works or numerous GPU hours, the research community is soliciting the architecture-neutral CNN structures, which can be easily plugged into multiple mature architectures to improve the performance on our real-world applications. We propose Asymmetric Convolution Block (ACB), an architecture-neutral structure as a CNN building block, which uses 1D asymmetric convolutions to strengthen the square convolution kernels. For an off-the-shelf architecture, we replace the standard square-kernel convolutional layers with ACBs to construct an Asymmetric Convolutional Network (ACNet), which can be trained to reach a higher level of accuracy. After training, we equivalently convert the ACNet into the same original architecture, thus requiring no extra computations anymore. We have observed that ACNet can improve the performance of various models on CIFAR and ImageNet by a clear margin. Through further experiments, we attribute the effectiveness of ACB to its capability of enhancing the model's robustness to rotational distortions and strengthening the central skeleton parts of square convolution kernels."
AFD-Net: Aggregated Feature Difference Learning for Cross-Spectral Image Patch Matching,"Dou Quan, Xuefeng Liang, Shuang Wang, Shaowei Wei, Yanfeng Li, Ning Huyan, Licheng Jiao","School of Artiﬁcial Intelligence, Xidian University, Shaanxi, China; School of Artiﬁcial Intelligence, Xidian University, Shaanxi, China and Kyoto University, Kyoto, Japan",100.0,"China, japan",0.0,,"Image patch matching across different spectral domains is more challenging than in a single spectral domain. We consider the reason is twofold: 1. the weaker discriminative feature learned by conventional methods; 2. the significant appearance difference between two images domains. To tackle these problems, we propose an aggregated feature difference learning network (AFD-Net). Unlike other methods that merely rely on the high-level features, we find the feature differences in other levels also provide useful learning information. Thus, the multi-level feature differences are aggregated to enhance the discrimination. To make features invariant across different domains, we introduce a domain invariant feature extraction network based on instance normalization (IN). In order to optimize the AFD-Net, we borrow the large margin cosine loss which can minimize intra-class distance and maximize inter-class distance between matching and non-matching samples. Extensive experiments show that AFD-Net largely outperforms the state-of-the-arts on the cross-spectral dataset, meanwhile, demonstrates a considerable generalizability on a single spectral dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Quan_AFD-Net_Aggregated_Feature_Difference_Learning_for_Cross-Spectral_Image_Patch_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Quan_AFD-Net_Aggregated_Feature_Difference_Learning_for_Cross-Spectral_Image_Patch_Matching_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009506/,"['Feature extraction', 'Measurement', 'Task analysis', 'Image matching', 'Convolution', 'Aggregates', 'Training']","['Image Patches', 'Patch Matching', 'Image Patch Matching', 'Cross-spectral Image', 'Feature Learning', 'Large Loss', 'Discriminative Features', 'High-level Features', 'Spectral Domain', 'Matched Samples', 'Invariant Features', 'Feature Extraction Network', 'Spectral Dataset', 'Instance Normalization', 'Domain-invariant Features', 'Intra-class Distance', 'Loss Function', 'Rich Information', 'Image Registration', 'Stochastic Gradient Descent', 'Metric Learning Methods', 'Softmax Loss', 'Siamese Network', 'Metric Learning', 'Deep Learning-based Methods', 'Matching Performance', 'Batch Normalization', 'Matching Problem', 'Multi-view Stereo', 'Difference Of Gaussian']",,32,"Image patch matching across different spectral domains is more challenging than in a single spectral domain. We consider the reason is twofold: 1. the weaker discriminative feature learned by conventional methods; 2. the significant appearance difference between two images domains. To tackle these problems, we propose an aggregated feature difference learning network (AFD-Net). Unlike other methods that merely rely on the high-level features, we find the feature differences in other levels also provide useful learning information. Thus, the multi-level feature differences are aggregated to enhance the discrimination. To make features invariant across different domains, we introduce a domain invariant feature extraction network based on instance normalization (IN). In order to optimize the AFD-Net, we borrow the large margin cosine loss which can minimize intra-class distance and maximize inter-class distance between matching and non-matching samples. Extensive experiments show that AFD-Net largely outperforms the state-of-the-arts on the cross-spectral dataset, meanwhile, demonstrates a considerable generalizability on a single spectral dataset."
AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation,"Huaijia Lin, Xiaojuan Qi, Jiaya Jia",The Chinese University of Hong Kong; University of Oxford; The Chinese University of Hong Kong and Tencent YouTu Lab,100.0,"Hong Kong, china, uk",0.0,,"Most video object segmentation approaches process objects separately. This incurs high computational cost when multiple objects exist. In this paper, we propose AGSS-VOS to segment multiple objects in one feed-forward path via instance-agnostic and instance-specific modules. Information from the two modules is fused via an attention-guided decoder to simultaneously segment all object instances in one path. The whole framework is end-to-end trainable with instance IoU loss. Experimental results on Youtube- VOS and DAVIS-2017 dataset demonstrate that AGSS-VOS achieves competitive results in terms of both accuracy and efficiency.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_AGSS-VOS_Attention_Guided_Single-Shot_Video_Object_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_AGSS-VOS_Attention_Guided_Single-Shot_Video_Object_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010958/,"['Feature extraction', 'Visualization', 'Object segmentation', 'Proposals', 'Decoding', 'Image segmentation', 'Training']","['Object Segmentation', 'Video Object Segmentation', 'Multiple Objects', 'Object Instances', 'Training Set', 'Decoding', 'Validation Set', 'Convolutional Layers', 'Reference Frame', 'Feature Maps', 'Visual Features', 'Online Learning', 'Latent Space', 'Segmentation Results', 'Optical Flow', 'Attention Map', 'Region Proposal', 'Previous Frame', 'Challenging Scenarios', 'Attention Feature', 'Target Frame', 'Segmentation Framework', 'Object Proposals', 'Inference Stage', 'Agnostic', 'Region Proposal Network', 'Data Augmentation', 'Mask R-CNN']",,56,"Most video object segmentation approaches process objects separately. This incurs high computational cost when multiple objects exist. In this paper, we propose AGSS-VOS to segment multiple objects in one feed-forward path via instance-agnostic and instance-specific modules. Information from the two modules is fused via an attention-guided decoder to simultaneously segment all object instances in one path. The whole framework is end-to-end trainable with instance IoU loss. Experimental results on Youtube- VOS and DAVIS-2017 dataset demonstrate that AGSS-VOS achieves competitive results in terms of both accuracy and efficiency."
AM-LFS: AutoML for Loss Function Search,"Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, Wanli Ouyang","SenseTime Group Limited; The University of Sydney, SenseTime Computer Vision Research Group, Australia",100.0,"australia, usa",0.0,,"Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_AM-LFS_AutoML_for_Loss_Function_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_AM-LFS_AutoML_for_Loss_Function_Search_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009555/,"['Training', 'Task analysis', 'Transforms', 'Optimization', 'Computer vision', 'Gold', 'Face']","['Loss Function', 'Computer Vision', 'Search Space', 'Design Space', 'Vision Tasks', 'Unique Form', 'Optimization Framework', 'Efficient Framework', 'Loss Function Design', 'Training Set', 'Training Dataset', 'Validation Set', 'Distribution Of Parameters', 'Discriminatory Power', 'Network Parameters', 'Object Detection', 'Difficulty Level', 'Face Recognition', 'Impact Of Loss', 'Softmax Loss', 'Intra-class Distance', 'Bilevel Optimization', 'Focal Loss', 'Query Image', 'Hard Examples', 'Verification Task', 'Gradient Norm', 'Noisy Labels', 'Hyperparameter Tuning']",,33,"Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks."
AMASS: Archive of Motion Capture As Surface Shapes,"Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, Michael J. Black",MPI for Informatics; Meshcapade; MPI for Intelligent Systems; York University,75.0,"canada, germany",25.0,Germany,"Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [Loper et al., 2015], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with markerbased mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and will be publicly available to the research community.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mahmood_AMASS_Archive_of_Motion_Capture_As_Surface_Shapes_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mahmood_AMASS_Archive_of_Motion_Capture_As_Surface_Shapes_ICCV_2019_paper.pdf,https://amass.is.tue.mpg.de/,,,main,Poster,https://ieeexplore.ieee.org/document/9009460/,"['Shape', 'Three-dimensional displays', 'Joints', 'Machine learning', 'Computational modeling', 'Computer vision']","['Surface Shape', 'Deep Learning', 'Computer Vision', '3D Mesh', 'Human Motion', 'Body Model', 'Hand Motion', 'Advances In Computer Vision', 'Training Set', 'Number Of Markers', 'Shape Parameter', 'Body Shape', 'Final Weight', '3D Scanning', 'Mahalanobis Distance', 'Joint Position', '3D Shape', 'Marker Data', 'Pose Estimation', 'Line Search', 'Pose Parameters', '3D Body', 'Shape Space', 'Body Pose', 'Shape Estimation', 'Pose Changes', 'Hyperparameter Search', 'Dynamic Coefficient', 'Skeletal Structure', 'Trained Subjects']",,648,"Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [Loper et al., 2015], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and will be publicly available to the research community."
AMP: Adaptive Masked Proxies for Few-Shot Segmentation,"Mennatullah Siam, Boris N. Oreshkin, Martin Jagersand",University of Alberta; Element AI,50.0,Canada,50.0,Canada,"Deep learning has thrived by training on large-scale datasets. However, in robotics applications sample efficiency is critical. We propose a novel adaptive masked proxies method that constructs the final segmentation layer weights from few labelled samples. It utilizes multi-resolution average pooling on base embeddings masked with the label to act as a positive proxy for the new class, while fusing it with the previously learned class signatures. Our method is evaluated on PASCAL-5^i dataset and outperforms the state-of-the-art in the few-shot semantic segmentation. Unlike previous methods, our approach does not require a second branch to estimate parameters or prototypes, which enables it to be used with 2-stream motion and appearance based segmentation networks. We further propose a novel setup for evaluating continual learning of object segmentation which we name incremental PASCAL (iPASCAL) where our method outperforms the baseline method. Our code is publicly available at https://github.com/MSiam/AdaptiveMaskedProxies.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf,,https://github.com/MSiam/AdaptiveMaskedProxies,,main,Poster,https://ieeexplore.ieee.org/document/9009561/,"['Image segmentation', 'Semantics', 'Training', 'Object segmentation', 'Task analysis', 'Printing', 'Feature extraction']","['Few-shot Segmentation', 'Final Weight', 'Semantic Segmentation', 'Average Pooling', 'Final Layer', 'Sampling Efficiency', 'Incremental Learning', 'Learning Rate', 'Pre-trained Network', 'Adaptive Scheme', 'Continuous Stream', 'Class Weights', 'Semantic Labels', 'Dilated Convolution', 'Support Set', 'RMSprop', 'Update Rate', 'Query Image', 'Few-shot Learning', 'Additional Branch', 'Few-shot Classification', 'Continuous Segments', 'Continuous Stream Of Data', 'Correlation Filter', 'Background Class', 'Large-scale Training', 'Unseen Classes', 'Segmentation Accuracy', 'Fully Convolutional Network']",,128,"Deep learning has thrived by training on large-scale datasets. However, in robotics applications sample efficiency is critical. We propose a novel adaptive masked proxies method that constructs the final segmentation layer weights from few labelled samples. It utilizes multiresolution average pooling on base embeddings masked with the label to act as a positive proxy for the new class, while fusing it with the previously learned class signatures. Our method is evaluated on PASCAL-5
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sup>
 dataset and outperforms the state-of-the-art in the few-shot semantic segmentation. Unlike previous methods, our approach does not require a second branch to estimate parameters or prototypes, which enables it to be used with 2-stream motion and appearance based segmentation networks. We further propose a novel setup for evaluating continual learning of object segmentation which we name incremental PASCAL (iPASCAL) where our method outperforms the baseline method. Our code is publicly available at https://github. com/MSiam/AdaptiveMaskedProxies."
ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal,"Bin Ding, Chengjiang Long, Ling Zhang, Chunxia Xiao","School of Computer Science, Wuhan University, Wuhan, Hubei, China; Wuhan University of Science and Technology, Wuhan, Hubei, China; Kitware Inc., Clifton Park, NY, USA",66.66666666666666,china,33.33333333333334,USA,"In this paper we propose an attentive recurrent generative adversarial network (ARGAN) to detect and remove shadows in an image. The generator consists of multiple progressive steps. At each step a shadow attention detector is firstly exploited to generate an attention map which specifies shadow regions in the input image. Given the attention map, a negative residual by a shadow remover encoder will recover a shadow-lighter or even a shadow-free image. The discriminator is designed to classify whether the output image in the last progressive step is real or fake. Moreover, ARGAN is suitable to be trained with a semi-supervised strategy to make full use of sufficient unsupervised data. The experiments on four public datasets have demonstrated that our ARGAN is robust to detect both simple and complex shadows and to produce more realistic shadow removal results. It outperforms the state-of-the-art methods, especially in detail of recovering shadow areas.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ding_ARGAN_Attentive_Recurrent_Generative_Adversarial_Network_for_Shadow_Detection_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ARGAN_Attentive_Recurrent_Generative_Adversarial_Network_for_Shadow_Detection_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010062/,"['Generators', 'Detectors', 'Feature extraction', 'Generative adversarial networks', 'Robustness', 'Machine learning', 'Computer vision']","['Generative Adversarial Networks', 'Shadow Detection', 'Input Image', 'Recurrent Network', 'Attention Network', 'Output Image', 'Attention Map', 'Steps Of Progression', 'Unsupervised Data', 'Shadow Regions', 'Loss Function', 'Root Mean Square Error', 'Mean Square Error', 'Convolutional Layers', 'Supervised Learning', 'Short-term Memory', 'Super-resolution', 'Attention Mechanism', 'Batch Normalization', 'Semi-supervised Learning', 'Shadow Images', 'Ground Truth Image', 'LeakyReLU Activation Function', 'Deconvolutional Layers', 'Discriminator Network', 'Visual Question Answering', 'Recurrent Unit', 'Attention Values', 'Sufficient Image', 'Perceptual Loss']",,113,"In this paper we propose an attentive recurrent generative adversarial network (ARGAN) to detect and remove shadows in an image. The generator consists of multiple progressive steps. At each step a shadow attention detector is firstly exploited to generate an attention map which specifies shadow regions in the input image. Given the attention map, a negative residual by a shadow remover encoder will recover a shadow-lighter or even a shadow-free image. The discriminator is designed to classify whether the output image in the last progressive step is real or fake. Moreover, ARGAN is suitable to be trained with a semi-supervised strategy to make full use of sufficient unsupervised data. The experiments on four public datasets have demonstrated that our ARGAN is robust to detect both simple and complex shadows and to produce more realistic shadow removal results. It outperforms the state-of-the-art methods, especially in detail of recovering shadow areas."
AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations,"Guo-Jun Qi, Liheng Zhang, Chang Wen Chen, Qi Tian","Huawei Noah’s Ark Lab; Laboratory for MAchine Perception and LEarning (MAPLE), Huawei Cloud; Laboratory for MAchine Perception and LEarning (MAPLE), Huawei Cloud, Huawei Noah’s Ark Lab; The Chinese University of Hong Kong at Shenzhen and Peng Cheng Laboratory",75.0,"Hong Kong, china, usa",25.0,China,"The learning of Transformation-Equivariant Representations (TERs), which is introduced by Hinton et al. [??], has been considered as a principle to reveal visual structures under various transformations. It contains the celebrated Convolutional Neural Networks (CNNs) as a special case that only equivary to the translations. In contrast, we seek to train TERs for a generic class of transformations and train them in an   unsupervised  fashion. To this end, we present a novel principled method by Autoencoding Variational Transformations (AVT), compared with the conventional approach to autoencoding data. Formally, given transformed images, the AVT seeks to train the networks by maximizing the mutual information between the transformations and representations. This ensures the resultant TERs of individual images contain the   intrinsic  information about their visual structures that would equivary   extricably  under various transformations in a generalized   nonlinear  case. Technically, we show that the resultant optimization problem can be efficiently solved by maximizing a variational lower-bound of the mutual information. This variational approach introduces a transformation decoder to approximate the intractable posterior of transformations, resulting in an autoencoding architecture with a pair of the representation encoder and the transformation decoder. Experiments demonstrate the proposed AVT model sets a new record for the performances on unsupervised tasks, greatly closing the performance gap to the supervised models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qi_AVT_Unsupervised_Learning_of_Transformation_Equivariant_Representations_by_Autoencoding_Variational_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_AVT_Unsupervised_Learning_of_Transformation_Equivariant_Representations_by_Autoencoding_Variational_ICCV_2019_paper.pdf,http://maple-lab.net/projects/AVT.htm,,,main,Poster,https://ieeexplore.ieee.org/document/9009446/,"['Mutual information', 'Task analysis', 'Visualization', 'Training', 'Image reconstruction', 'Decoding', 'Gallium nitride']","['Unsupervised Learning', 'Representation Learning', 'Transformation Equivariance', 'Lower Bound', 'Convolutional Neural Network', 'Mutual Information', 'Variable Approach', 'Intrinsic Information', 'Unsupervised Tasks', 'Resulting Optimization Problem', 'Transformer Decoder', 'Random Sampling', 'Decoding', 'Convolutional Layers', 'Feature Maps', 'Unsupervised Methods', 'Fully-connected Layer', 'Image Representation', 'Linear Classifier', 'AlexNet', 'Unsupervised Representation', 'Unsupervised Model', 'Variational Autoencoder', 'Projective Transformation', 'Corner Of The Image', 'Convolutional Block', 'Unsupervised Feature', 'Probabilistic Representation', 'Information-theoretic Perspective', 'Likelihood Of The Data']",,28,"The learning of Transformation-Equivariant Representations (TERs), which is introduced by Hinton et al. \cite{hinton2011transforming}, has been considered as a principle to reveal visual structures under various transformations. It contains the celebrated Convolutional Neural Networks (CNNs) as a special case that only equivary to the translations. In contrast, we seek to train TERs for a generic class of transformations and train them in an {\em unsupervised} fashion. To this end, we present a novel principled method by Autoencoding Variational Transformations (AVT), compared with the conventional approach to autoencoding data. Formally, given transformed images, the AVT seeks to train the networks by maximizing the mutual information between the transformations and representations. This ensures the resultant TERs of individual images contain the {\em intrinsic} information about their visual structures that would equivary {\em extricably} under various transformations in a generalized {\em nonlinear} case. Technically, we show that the resultant optimization problem can be efficiently solved by maximizing a variational lower-bound of the mutual information. This variational approach introduces a transformation decoder to approximate the intractable posterior of transformations, resulting in an autoencoding architecture with a pair of the representation encoder and the transformation decoder. Experiments demonstrate the proposed AVT model sets a new record for the performances on unsupervised tasks, greatly closing the performance gap to the supervised models."
AWSD: Adaptive Weighted Spatiotemporal Distillation for Video Representation,"Mohammad Tavakolian, Hamed R. Tavakoli, Abdenour Hadid",University of Oulu; Aalto University & Nokia Technologies,100.0,finland,0.0,,"We propose an Adaptive Weighted Spatiotemporal Distillation (AWSD) technique for video representation by encoding the appearance and dynamics of the videos into a single RGB image map. This is obtained by adaptively dividing the videos into small segments and comparing two consecutive segments. This allows using pre-trained models on still images for video classification while successfully capturing the spatiotemporal variations in the videos. The adaptive segment selection enables effective encoding of the essential discriminative information of untrimmed videos. Based on Gaussian Scale Mixture, we compute the weights by extracting the mutual information between two consecutive segments. Unlike pooling-based methods, our AWSD gives more importance to the frames that characterize actions or events thanks to its adaptive segment length selection. We conducted extensive experimental analysis to evaluate the effectiveness of our proposed method and compared our results against those of recent state-of-the-art methods on four benchmark datatsets, including UCF101, HMDB51, ActivityNet v1.3, and Maryland. The obtained results on these benchmark datatsets showed that our method significantly outperforms earlier works and sets the new state-of-the-art performance in video classification. Code is available at the project webpage: https://mohammadt68.github.io/AWSD/",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tavakolian_AWSD_Adaptive_Weighted_Spatiotemporal_Distillation_for_Video_Representation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tavakolian_AWSD_Adaptive_Weighted_Spatiotemporal_Distillation_for_Video_Representation_ICCV_2019_paper.pdf,https://mohammadt68.github.io/AWSD/,https://github.com/mohammadt68/AWSD,,main,Poster,https://ieeexplore.ieee.org/document/9010972/,"['Spatiotemporal phenomena', 'Image segmentation', 'Three-dimensional displays', 'Strain', 'Mutual information', 'Task analysis', 'Covariance matrices']","['Single Image', 'Mutual Information', 'Segment Length', 'RGB Images', 'Video Analysis', 'Still Images', 'Adaptive Selection', 'Video Information', 'Consecutive Segments', 'Neural Network', 'Learning Process', 'Convolutional Neural Network', 'Deep Neural Network', 'Deep Models', 'Gaussian Noise', 'Convolutional Neural Network Model', 'Deep Architecture', 'Dynamic Imaging', 'Optical Flow', 'Intensity Pattern', 'Point In Frame', 'Dynamic Video', 'Video Segments', 'Gain Factor', 'Spatiotemporal Information', 'Video Understanding', 'Intermediate Representation', 'Dynamic Scenes', 'ImageNet Dataset', 'Window Size']",,4,"We propose an Adaptive Weighted Spatiotemporal Distillation (AWSD) technique for video representation by encoding the appearance and dynamics of the videos into a single RGB image map. This is obtained by adaptively dividing the videos into small segments and comparing two consecutive segments. This allows using pre-trained models on still images for video classification while successfully capturing the spatiotemporal variations in the videos. The adaptive segment selection enables effective encoding of the essential discriminative information of untrimmed videos. Based on Gaussian Scale Mixture, we compute the weights by extracting the mutual information between two consecutive segments. Unlike pooling-based methods, our AWSD gives more importance to the frames that characterize actions or events thanks to its adaptive segment length selection. We conducted extensive experimental analysis to evaluate the effectiveness of our proposed method and compared our results against those of recent state-of-the-art methods on four benchmark datatsets, including UCF101, HMDB51, ActivityNet v1.3, and Maryland. The obtained results on these benchmark datatsets showed that our method significantly outperforms earlier works and sets the new state-of-the-art performance in video classification. Code is available at the project webpage: https://mohammadt68.github.io/AWSD/."
Accelerate CNN via Recursive Bayesian Pruning,"Yuefu Zhou, Ya Zhang, Yanfeng Wang, Qi Tian","Huawei Noah’s Ark Lab; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University and MediaSmart Technology; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University",66.66666666666666,China,33.33333333333334,China,"Channel Pruning, widely used for accelerating Convolutional Neural Networks, is an NP-hard problem due to the inter-layer dependency of channel redundancy. Existing methods generally ignored the above dependency for computation simplicity. To solve the problem, under the Bayesian framework, we here propose a layer-wise Recursive Bayesian Pruning method (RBP). A new dropout-based measurement of redundancy, which facilitate the computation of posterior assuming inter-layer dependency, is introduced. Specifically, we model the noise across layers as a Markov chain and target its posterior to reflect the inter-layer dependency. Considering the closed form solution for posterior is intractable, we derive a sparsity-inducing Dirac-like prior which regularizes the distribution of the designed noise to automatically approximate the posterior. Compared with the existing methods, no additional overhead is required when the inter-layer dependency assumed. The redundant channels can be simply identified by tiny dropout noise and directly pruned layer by layer. Experiments on popular CNN architectures have shown that the proposed method outperforms several state-of-the-arts. Particularly, we achieve up to 5.0x, 2.2x and 1.7x FLOPs reduction with little accuracy loss on the large scale dataset ILSVRC2012 for VGG16, ResNet50 and MobileNetV2, respectively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009006/,"['Redundancy', 'Bayes methods', 'Acceleration', 'Markov processes', 'Training', 'Computer vision', 'Computational modeling']","['Convolutional Neural Network', 'Bayesian Inference', 'Markov Chain', 'Layer-by-layer', 'Large-scale Datasets', 'Bayesian Framework', 'Accuracy Loss', 'Pruning Method', 'Convolutional Layers', 'Output Layer', 'Gaussian Noise', 'Dropout Rate', 'Data Fitting', 'Data Augmentation', 'Residual Block', 'Residual Network', 'Dirac Delta', 'Blocking Layer', 'Compression Rate', 'Acceleration Effect', 'FC Layer', 'Downsampling Layer', 'Pruning Strategy', 'Lth Layer', 'Competitive Accuracy', 'Popular Solution', 'Half Of The Channel']",,35,"Channel Pruning, widely used for accelerating Convolutional Neural Networks, is an NP-hard problem due to the inter-layer dependency of channel redundancy. Existing methods generally ignored the above dependency for computation simplicity. To solve the problem, under the Bayesian framework, we here propose a layer-wise Recursive Bayesian Pruning method (RBP). A new dropout-based measurement of redundancy, which facilitate the computation of posterior assuming inter-layer dependency, is introduced. Specifically, we model the noise across layers as a Markov chain and target its posterior to reflect the inter-layer dependency. Considering the closed form solution for posterior is intractable, we derive a sparsity-inducing Dirac-like prior which regularizes the distribution of the designed noise to automatically approximate the posterior. Compared with the existing methods, no additional overhead is required when the inter-layer dependency assumed. The redundant channels can be simply identified by tiny dropout noise and directly pruned layer by layer. Experiments on popular CNN architectures have shown that the proposed method outperforms several state-of-the-arts. Particularly, we achieve up to 5.0x, 2.2x and 1.7x FLOPs reduction with little accuracy loss on the large scale dataset ILSVRC2012 for VGG16, ResNet50 and MobileNetV2, respectively."
Accelerate Learning of Deep Hashing With Gradient Attention,"Long-Kai Huang, Jianda Chen, Sinno Jialin Pan","Nanyang Technological University, Singapore",100.0,Singapore,0.0,,"Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Accelerate_Learning_of_Deep_Hashing_With_Gradient_Attention_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Accelerate_Learning_of_Deep_Hashing_With_Gradient_Attention_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008802/,"['Training', 'Binary codes', 'Computational modeling', 'Optimization', 'Machine learning', 'Switches', 'Acceleration']","['Deep Learning', 'Learning Process', 'Gradient Descent', 'Deep Models', 'Worst Case', 'Benchmark Datasets', 'Hash Function', 'Image Retrieval', 'Large-scale Image', 'Instance Pairs', 'Loss Function', 'Training Set', 'Objective Function', 'Learning Rate', 'ImageNet', 'Stochastic Gradient Descent', 'Similar Form', 'Similar Information', 'Pairwise Similarity', 'Binary Code', 'Decrease In Loss', 'Mean Average Precision', 'Current Code', 'Training Pairs', 'Code Length', 'Query Set', 'Gradient Generator', 'Sign Function', 'Hamming Distance', 'Attention Weights']",,14,"Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models."
Accelerated Gravitational Point Set Alignment With Altered Physical Laws,"Vladislav Golyanik, Christian Theobalt, Didier Stricker","MPI for Informatics; University of Kaiserslautern, DFKI",100.0,"Germany, germany",0.0,,"This work describes Barnes-Hut Rigid Gravitational Approach (BH-RGA) -- a new rigid point set registration method relying on principles of particle dynamics. Interpreting the inputs as two interacting particle swarms, we directly minimise the gravitational potential energy of the system using non-linear least squares. Compared to solutions obtained by solving systems of second-order ordinary differential equations, our approach is more robust and less dependent on the parameter choice. We accelerate otherwise exhaustive particle interactions with a Barnes-Hut tree and efficiently handle massive point sets in quasilinear time while preserving the globally multiply-linked character of interactions. Among the advantages of BH-RGA is the possibility to define boundary conditions or additional alignment cues through varying point masses. Systematic experiments demonstrate that BH-RGA surpasses performances of baseline methods in terms of the convergence basin and accuracy when handling incomplete, noisy and perturbed data. The proposed approach also positively compares to the competing method for the alignment with prior matches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009498/,"['Acceleration', 'Transforms', 'Convergence', 'Three-dimensional displays', 'Optimization', 'Iterative closest point algorithm', 'Potential energy']","['Ordinary Differential Equations', 'Nonlinear Least Squares', 'Particle Dynamics', 'Additional Cues', 'Point Cloud Registration', 'Gravitational Energy', 'Cultural Heritage', 'Reference Frame', 'Energy Function', 'Point Cloud', 'Gene Trees', 'Gaussian Mixture Model', 'Amount Of Noise', 'Rigid Transformation', 'Gravitational Constant', 'Physical Simulation', 'Optimal Alignment', 'Iterative Closest Point', 'Quadratic Complexity', 'Graph Metrics', 'Simulation Interval', 'External Nodes', 'Acceleration Techniques', 'Misalignment Angle']",,9,"This work describes Barnes-Hut Rigid Gravitational Approach (BH-RGA) - a new rigid point set registration method relying on principles of particle dynamics. Interpreting the inputs as two interacting particle swarms, we directly minimise the gravitational potential energy of the system using non-linear least squares. Compared to solutions obtained by solving systems of second-order ordinary differential equations, our approach is more robust and less dependent on the parameter choice. We accelerate otherwise exhaustive particle interactions with a Barnes-Hut tree and efficiently handle massive point sets in quasilinear time while preserving the globally multiply-linked character of interactions. Among the advantages of BH-RGA is the possibility to define boundary conditions or additional alignment cues through varying point masses. Systematic experiments demonstrate that BH-RGA surpasses performances of baseline methods in terms of the convergence basin and accuracy when handling incomplete, noisy and perturbed data. The proposed approach also positively compares to the competing method for the alignment with prior matches."
Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving,"Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli Ouyang, Xin Fan","Dalian University of Technology, China; The University of Sydney, SenseTime Computer Vision Research Group, Australia; Dalian University of Technology, China and Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, China",100.0,"australia, china",0.0,,"In this paper, we propose a monocular 3D object detection framework in the domain of autonomous driving. Unlike previous image-based methods which focus on RGB feature extracted from 2D images, our method solves this problem in the reconstructed 3D space in order to exploit 3D contexts explicitly. To this end, we first leverage a stand-alone module to transform the input data from 2D image plane to 3D point clouds space for a better input representation, then we perform the 3D detection using PointNet backbone net to obtain objects' 3D locations, dimensions and orientations. To enhance the discriminative capability of point clouds, we propose a multi-modal feature fusion module to embed the complementary RGB cue into the generated point clouds representation. We argue that it is more effective to infer the 3D bounding boxes from the generated 3D scene space (i.e., X,Y, Z space) compared to the image plane (i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows that our approach boosts the performance of state-of-the-art monocular approach by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Accurate_Monocular_3D_Object_Detection_via_Color-Embedded_3D_Reconstruction_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Accurate_Monocular_3D_Object_Detection_via_Color-Embedded_3D_Reconstruction_for_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009489/,"['Three-dimensional displays', 'Two dimensional displays', 'Task analysis', 'Laser radar', 'Feature extraction', 'Object detection', 'Transforms']","['Object Detection', 'Autonomous Vehicles', 'Accurate Object Detection', '3D Object Detection', 'Monocular 3D Object Detection', 'Accurate 3D Object Detection', 'Image Plane', '2D Images', 'Point Cloud', '3D Space', 'Bounding Box', 'Feature Fusion', '3D Position', 'Detection Framework', 'Space In Order', 'Input Representation', 'Image-based Methods', 'Domains Framework', '3D Detection', 'Feature Fusion Module', 'RGB Information', 'Detection Task', 'Localization Task', 'Depth Map', 'Monocular Images', 'Depth Estimation', 'Background Points', 'Training Set', '3D Coordinates', 'Image-based Detection']",,214,"In this paper, we propose a monocular 3D object detection framework in the domain of autonomous driving. Unlike previous image-based methods which focus on RGB feature extracted from 2D images, our method solves this problem in the reconstructed 3D space in order to exploit 3D contexts explicitly. To this end, we first leverage a stand-alone module to transform the input data from 2D image plane to 3D point clouds space for a better input representation, then we perform the 3D detection using PointNet backbone net to obtain objects' 3D locations, dimensions and orientations. To enhance the discriminative capability of point clouds, we propose a multi-modal feature fusion module to embed the complementary RGB cue into the generated point clouds representation. We argue that it is more effective to infer the 3D bounding boxes from the generated 3D scene space (i.e., X,Y, Z space) compared to the image plane (i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows that our approach boosts the performance of state-of-the-art monocular approach by a large margin."
Action Assessment by Joint Relation Graphs,"Jia-Hui Pan, Jibin Gao, Wei-Shi Zheng","School of Data and Computer Science, Sun Yat-sen University, China; School of Data and Computer Science, Sun Yat-sen University, China; Peng Cheng Laboratory, Shenzhen 518005, China; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",100.0,"China, china",0.0,,"We present a new model to assess the performance of actions from videos, through graph-based joint relation modelling. Previous works mainly focused on the whole scene including the performer's body and background, yet they ignored the detailed joint interactions. This is insufficient for fine-grained, accurate action assessment, because the action quality of each joint is dependent of its neighbouring joints. Therefore, we propose to learn the detailed joint motion based on the joint relations. We build trainable Joint Relation Graphs, and analyze joint motion on them. We propose two novel modules, the Joint Commonality Module and the Joint Difference Module, for joint motion learning. The Joint Commonality Module models the general motion for certain body parts, and the Joint Difference Module models the motion differences within body parts. We evaluate our method on six public Olympic actions for performance assessment. Our method outperforms previous approaches (+0.0912) and the whole-scene analysis (+0.0623) in the Spearman's Rank Correlation. We also demonstrate our model's ability to interpret the action assessment process.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Action_Assessment_by_Joint_Relation_Graphs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Action_Assessment_by_Joint_Relation_Graphs_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009513/,"['Skeleton', 'Videos', 'Computational modeling', 'Task analysis', 'Feature extraction', 'Convolution', 'Kinetic theory']","['Relation Graph', 'Spearman Correlation', 'Rank Correlation', 'Body Parts', 'Joint Motion', 'Graph-based Models', 'Time Step', 'Athletes', 'Human Bone', 'Spatial Relationship', 'Temporal Relationship', 'Motor Coordination', 'Pair Of Nodes', 'Hidden State', 'Action Recognition', 'Motion Features', 'Pose Estimation', 'Feature Encoder', 'Graph Convolution', 'Skeleton Structure', 'Temporal Graph', 'Pose Estimation Methods', 'Joint Feature', 'Start Of Training', 'Computer Vision Community']",,77,"We present a new model to assess the performance of actions from videos, through graph-based joint relation modelling. Previous works mainly focused on the whole scene including the performer's body and background, yet they ignored the detailed joint interactions. This is insufficient for fine-grained, accurate action assessment, because the action quality of each joint is dependent of its neighbouring joints. Therefore, we propose to learn the detailed joint motion based on the joint relations. We build trainable Joint Relation Graphs, and analyze joint motion on them. We propose two novel modules, the Joint Commonality Module and the Joint Difference Module, for joint motion learning. The Joint Commonality Module models the general motion for certain body parts, and the Joint Difference Module models the motion differences within body parts. We evaluate our method on six public Olympic actions for performance assessment. Our method outperforms previous approaches (+0.0912) and the whole-scene analysis (+0.0623) in the Spearman's Rank Correlation. We also demonstrate our model's ability to interpret the action assessment process."
Action Recognition With Spatial-Temporal Discriminative Filter Banks,"Brais MartÃ­nez, Davide Modolo, Yuanjun Xiong, Joseph Tighe",Amazon,0.0,,100.0,USA,"Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or exploring different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we hypothesize that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Martinez_Action_Recognition_With_Spatial-Temporal_Discriminative_Filter_Banks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Martinez_Action_Recognition_With_Spatial-Temporal_Discriminative_Filter_Banks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010714/,"['Three-dimensional displays', 'Solid modeling', 'Two dimensional displays', 'Benchmark testing', 'Visualization', 'Task analysis', 'Aggregates']","['Action Recognition', 'Discriminative Filter', 'Network Layer', 'Fine Details', 'Average Pooling', 'Global Pooling', 'Global Average Pooling', 'Advances In Literature', 'Human Activities', 'Confusion Matrix', 'Max-pooling', 'Final Prediction', '3D Network', 'Classism', 'Linear Classifier', 'Action Classes', 'Temporal Model', 'Local Cues', 'Fine-grained Information', 'Car Model', 'Feature Volume', 'Top-5 Accuracy', 'Action Recognition Model', 'Fine-grained Approach', 'Video Action Recognition']",,54,"Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or exploring different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we hypothesize that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks."
Active Learning for Deep Detection Neural Networks,"Hamed H. Aghdam, Abel Gonzalez-Garcia, Joost van de Weijer, Antonio M. LÃ³pez","Computer Vision Center (CVC), Univ. Aut `onoma de Barcelona (UAB); Computer Vision Center (CVC), Univ. Aut `onoma de Barcelona (UAB); Computer Science Dpt., Univ. Aut `onoma de Barcelona (UAB)",100.0,Spain,0.0,,"The cost of drawing object bounding boxes (i.e. labeling) for millions of images is prohibitively high. For instance, labeling pedestrians in a regular urban image could take 35 seconds on average. Active learning aims to reduce the cost of labeling by selecting only those images that are informative to improve the detection network accuracy. In this paper, we propose a method to perform active learning of object detectors based on convolutional neural networks. We propose a new image-level scoring process to rank unlabeled images for their automatic selection, which clearly outperforms classical scores. The proposed method can be applied to videos and sets of still images. In the former case, temporal selection rules can complement our scoring process. As a relevant use case, we extensively study the performance of our method on the task of pedestrian detection. Overall, the experiments show that the proposed method performs better than random selection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Aghdam_Active_Learning_for_Deep_Detection_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Aghdam_Active_Learning_for_Deep_Detection_Neural_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009535,"['Labeling', 'Task analysis', 'Detectors', 'Videos', 'Neural networks', 'Training', 'Object detection']","['Neural Network', 'Active Learning', 'Convolutional Neural Network', 'Random Selection', 'Object Detection', 'Bounding Box', 'Detection Task', 'Selection Rules', 'Still Images', 'Automatic Selection', 'Unlabeled Images', 'Pedestrian Detection', 'Labeling Cost', 'Millions Of Images', 'Posterior Probability', 'Convolutional Layers', 'Domain Shift', 'False-positive And False-negative', 'End Of Cycle', 'Active Learning Methods', 'Current Cycle', 'Frame Selection', 'Prediction Probability', 'Visual Patterns', 'Temporal Smoothing', 'Video Sequences', 'Probability Matrix', 'False Negative Predictions', 'Temporal Distance']",,82,"The cost of drawing object bounding boxes (i.e. labeling) for millions of images is prohibitively high. For instance, labeling pedestrians in a regular urban image could take 35 seconds on average. Active learning aims to reduce the cost of labeling by selecting only those images that are informative to improve the detection network accuracy. In this paper, we propose a method to perform active learning of object detectors based on convolutional neural networks. We propose a new image-level scoring process to rank unlabeled images for their automatic selection, which clearly outperforms classical scores. The proposed method can be applied to videos and sets of still images. In the former case, temporal selection rules can complement our scoring process. As a relevant use case, we extensively study the performance of our method on the task of pedestrian detection. Overall, the experiments show that the proposed method performs better than random selection."
AdaTransform: Adaptive Data Transformation,"Zhiqiang Tang, Xi Peng, Tingfeng Li, Yizhe Zhu, Dimitris N. Metaxas",Rutgers University; University of Delaware,100.0,usa,0.0,,"Data augmentation is widely used to increase data variance in training deep neural networks. However, previous methods require either comprehensive domain knowledge or high computational cost. Can we learn data transformation automatically and efficiently with limited domain knowledge? Furthermore, can we leverage data transformation to improve not only network training but also network testing? In this work, we propose adaptive data transformation to achieve the two goals. The AdaTransform can increase data variance in training and decrease data variance in testing. Experiments on different tasks prove that it can improve generalization performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_AdaTransform_Adaptive_Data_Transformation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_AdaTransform_Adaptive_Data_Transformation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010842/,"['Training', 'Task analysis', 'Testing', 'Training data', 'Learning (artificial intelligence)', 'Neural networks', 'Transforms']","['Data Transformation', 'Neural Network', 'Training Data', 'Variance In The Data', 'Deep Neural Network', 'Cognitive Domains', 'Network Training', 'Data Augmentation', 'Image Classification', 'Test Accuracy', 'Generative Adversarial Networks', 'Training Stage', 'Testing Stage', 'Pose Estimation', 'Joint Optimization', 'Target Network', 'Space Transformation', 'Human Pose Estimation', 'Human Pose', 'Competitive Mode', 'Face Alignment', 'Competitive Task', 'Hard Examples', 'Final Policy', 'Cooperative Mode', 'Transformation Operations', 'Gradient Ascent']",,13,"Data augmentation is widely used to increase data variance in training deep neural networks. However, previous methods require either comprehensive domain knowledge or high computational cost. Can we learn data transformation automatically and efficiently with limited domain knowledge? Furthermore, can we leverage data transformation to improve not only network training but also network testing? In this work, we propose adaptive data transformation to achieve the two goals. The AdaTransform can increase data variance in training and decrease data variance in testing. Experiments on different tasks prove that it can improve generalization performance."
AdaptIS: Adaptive Instance Selection Network,"Konstantin Sofiiuk, Olga Barinova, Anton Konushin",Samsung AI Center,0.0,,100.0,South Korea,"We present Adaptive Instance Selection network architecture for class-agnostic instance segmentation. Given an input image and a point (x, y), it generates a mask for the object located at (x, y). The network adapts to the input point with a help of AdaIN layers [??], thus producing different masks for different objects on the same image. AdaptIS generates pixel-accurate object masks, therefore it accurately segments objects of complex shape or severely occluded ones. AdaptIS can be easily combined with standard semantic segmentation pipeline to perform panoptic segmentation. To illustrate the idea, we perform experiments on a challenging toy problem with difficult occlusions. Then we extensively evaluate the method on panoptic segmentation benchmarks. We obtain state-of-the-art results on Cityscapes and Mapillary even without pretraining on COCO, and show competitive results on a challenging COCO dataset. The source code of the method and the trained models are available at  https://github.com/saic-vul/adaptis  https://github.com/saic-vul/adaptis .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sofiiuk_AdaptIS_Adaptive_Instance_Selection_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sofiiuk_AdaptIS_Adaptive_Instance_Selection_Network_ICCV_2019_paper.pdf,,https://github.com/saic-vul/adaptis,,main,Poster,https://ieeexplore.ieee.org/document/9009779/,"['Image segmentation', 'Proposals', 'Semantics', 'Standards', 'Pipelines', 'Benchmark testing', 'Aerospace electronics']","['Input Image', 'Semantic Segmentation', 'Object Segmentation', 'Standard Pipeline', 'Instance Segmentation', 'COCO Dataset', 'Architecture For Segmentation', 'Standard Segmentation', 'Learning Rate', 'Batch Size', 'Network Layer', 'Image Object', 'Control Network', 'Bounding Box', 'Latent Space', 'Simple Random Sampling', 'Standard Benchmark', 'Highest Confidence', 'Semantic Labels', 'Binary Segmentation', 'Mask R-CNN', 'Breadth-first Search', 'Instance Segmentation Methods', 'Segmentation Branch']",,132,"We present \emph{Adaptive Instance Selection} network architecture for class-agnostic instance segmentation. Given an input image and a point $(x, y)$, it generates a mask for the object located at $(x, y)$. The network adapts to the input point with a help of AdaIN layers \cite{karras2018style}, thus producing different masks for different objects on the same image. AdaptIS generates pixel-accurate object masks, therefore it accurately segments objects of complex shape or severely occluded ones. AdaptIS can be easily combined with standard semantic segmentation pipeline to perform panoptic segmentation. To illustrate the idea, we perform experiments on a challenging toy problem with difficult occlusions. Then we extensively evaluate the method on panoptic segmentation benchmarks. We obtain state-of-the-art results on Cityscapes and Mapillary even without pretraining on COCO, and show competitive results on a challenging COCO dataset. The source code of the method and the trained models are available at \href{https://github.com/saic-vul/adaptis}{https://github.com/saic-vul/adaptis}."
Adaptative Inference Cost With Convolutional Neural Mixture Models,"Adria Ruiz, Jakob Verbeek","Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France",100.0,France,0.0,,"Despite the outstanding performance of convolutional neural networks (CNNs) for many vision tasks, the required computational cost during inference is problematic when resources are limited. In this context, we propose Convolutional Neural Mixture Models (CNMMs), a probabilistic model embedding a large number of CNNs that can be jointly trained and evaluated in an efficient manner. Within the proposed framework, we present different mechanisms to prune subsets of CNNs from the mixture, allowing to easily adapt the computational cost required for inference. Image classification and semantic segmentation experiments show that our method achieve excellent accuracy-compute trade-offs. Moreover, unlike most of previous approaches, a single CNMM provides a large range of operating points along this trade-off, without any re-training.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ruiz_Adaptative_Inference_Cost_With_Convolutional_Neural_Mixture_Models_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ruiz_Adaptative_Inference_Cost_With_Convolutional_Neural_Mixture_Models_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010026/,"['Training', 'Computational modeling', 'Mixture models', 'Computational efficiency', 'Task analysis', 'Computer vision', 'Convolutional neural networks']","['Neural Model', 'Convolutional Model', 'Inference Cost', 'Neural Network', 'Computational Cost', 'Convolutional Neural Network', 'Image Classification', 'Semantic Segmentation', 'Spatial Resolution', 'Prediction Accuracy', 'Training Set', 'Validation Set', 'Scaling Factor', 'Feature Maps', 'Second Derivative', 'Network Output', 'Functional Identification', 'Linear Classifier', 'Convolutional Block', 'Efficient Inference', 'Intermediate Class', 'Pruning Strategy', 'Neural Architecture Search', 'Network Pruning', 'Output Tensor', 'Bayesian Neural Network', 'Input Tensor', 'Ensemble Of Neural Networks']",,7,"Despite the outstanding performance of convolutional neural networks (CNNs) for many vision tasks, the required computational cost during inference is problematic when resources are limited. In this context, we propose Convolutional Neural Mixture Models (CNMMs), a probabilistic model embedding a large number of CNNs that can be jointly trained and evaluated in an efficient manner. Within the proposed framework, we present different mechanisms to prune subsets of CNNs from the mixture, allowing to easily adapt the computational cost required for inference. Image classification and semantic segmentation experiments show that our method achieve excellent accuracy-compute trade-offs. Moreover, unlike most of previous approaches, a single CNMM provides a large range of operating points along this trade-off, without any re-training."
Adaptive Activation Thresholding: Dynamic Routing Type Behavior for Interpretability in Convolutional Neural Networks,"Yiyou Sun, Sathya N. Ravi, Vikas Singh",University of Wisconsin-Madison; University of Illinois at Chicago,100.0,usa,0.0,,"There is a growing interest in strategies that can help us understand or interpret neural networks -- that is, not merely provide a prediction, but also offer additional context explaining why and how. While many current methods offer tools to perform this analysis for a given (trained) network post-hoc, recent results (especially on capsule networks) suggest that when classes map to a few high level ""concepts"" in the preceding layers of the network, the behavior of the network is easier to interpret or explain. Such training may be accomplished via dynamic/EM routing where the network ""routes"" for individual classes (or subsets of images) are dynamic and involve few nodes even if the full network may not be sparse. In this paper, we show how a simple modification of the SGD scheme can help provide dynamic/EM routing type behavior in convolutional neural networks. Through extensive experiments, we evaluate the effect of this idea for interpretability where we obtain promising results, while also showing that no compromise in attainable accuracy is involved. Further, we show that the minor modification is seemingly ad-hoc, the new algorithm can be analyzed by an approximate method which provably matches known rates for SGD.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Adaptive_Activation_Thresholding_Dynamic_Routing_Type_Behavior_for_Interpretability_in_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Adaptive_Activation_Thresholding_Dynamic_Routing_Type_Behavior_for_Interpretability_in_ICCV_2019_paper.pdf,,https://github.com/sunyiyou/dynamic-k-activation,,main,Poster,https://ieeexplore.ieee.org/document/9009501/,"['Routing', 'Dogs', 'Heuristic algorithms', 'Training', 'Computational modeling', 'Cats', 'Standards']","['Convolutional Neural Network', 'Adaptive Threshold', 'Dynamic Routing', 'Subset Of Images', 'Capsule Network', 'Training Time', 'Set Of Equations', 'Model Interpretation', 'Feed-forward Network', 'Final Layer', 'Lipschitz Continuous', 'Visual Model', 'Cross-entropy Loss Function', 'Second Set Of Experiments', 'Units In Layer', 'Secret Sharing', 'Sparsity Constraint', 'Routing Algorithm']",,9,"There is a growing interest in strategies that can help us understand or interpret neural networks -- that is, not merely provide a prediction, but also offer additional context explaining why and how. While many current methods offer tools to perform this analysis for a given (trained) network post-hoc, recent results (especially on capsule networks) suggest that when classes map to a few high level ``concepts'' in the preceding layers of the network, the behavior of the network is easier to interpret or explain. Such training may be accomplished via dynamic/EM routing where the network ``routes'' for individual classes (or subsets of images) are dynamic and involve few nodes even if the full network may not be sparse. In this paper, we show how a simple modification of the SGD scheme can help provide dynamic/EM routing type behavior in convolutional neural networks. Through extensive experiments, we evaluate the effect of this idea for interpretability where we obtain promising results, while also showing that no compromise in attainable accuracy is involved. Further, we show that the minor modification is seemingly ad-hoc, the new algorithm can be analyzed by an approximate method which provably matches known rates for SGD."
Adaptive Context Network for Scene Parsing,"Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, Hanqing Lu","Nanjing University of Science and Technology; Business Growth BU, JD.com; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",80.0,china,20.0,China,"Recent works attempt to improve scene parsing performance by exploring different levels of contexts, and typically train a well-designed convolutional network to exploit useful contexts across all pixels equally. However, in this paper, we find that the context demands are varying from different pixels or regions in each image. Based on this observation, we propose an Adaptive Context Network (ACNet) to capture the pixel-aware contexts by a competitive fusion of global context and local context according to different per-pixel demands. Specifically, when given a pixel, the global context demand is measured by the similarity between the global feature and its local feature, whose reverse value can also be used to measure the local context demand. We model the two demanding measurements by the proposed global context module and local context module, respectively, to generate their adaptive contextual features. Furthermore, we import multiple such modules to build several adaptive context blocks in different levels of network to obtain a coarse-to-fine result. Finally, comprehensive experimental evaluations demonstrate the effectiveness of the proposed ACNet, and new state-of-the-arts performances are achieved on all four public datasets, i.e. Cityscapes, ADE20K, PASCAL Context, and COCO Stuff.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Adaptive_Context_Network_for_Scene_Parsing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_Adaptive_Context_Network_for_Scene_Parsing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010927/,"['Logic gates', 'Adaptive systems', 'Fuses', 'Task analysis', 'Image resolution', 'Convolution', 'Semantics']","['Scene Parsing', 'Local Context', 'Local Features', 'Global Features', 'Global Context', 'Local Module', 'Global Module', 'Contextual Demands', 'Contextual Modulation', 'Convolutional Layers', 'Training Phase', 'Attention Mechanism', 'Receptive Field', 'Training Images', 'High-level Features', 'Small Objects', 'Low-level Features', 'Backbone Network', 'Global Pooling', 'Spatial Details', 'ResNet-101 Backbone', 'Fully Convolutional Network', 'Large Objects', 'Mean Intersection Over Union', 'ResNet Block', 'Global Average Pooling', 'Dilation Rate', 'Feature Pooling', 'Data Augmentation']",,99,"Recent works attempt to improve scene parsing performance by exploring different levels of contexts, and typically train a well-designed convolutional network to exploit useful contexts across all pixels equally. However, in this paper, we find that the context demands are varying from different pixels or regions in each image. Based on this observation, we propose an Adaptive Context Network (ACNet) to capture the pixel-aware contexts by a competitive fusion of global context and local context according to different per-pixel demands. Specifically, when given a pixel, the global context demand is measured by the similarity between the global feature and its local feature, whose reverse value can also be used to measure the local context demand. We model the two demanding measurements by the proposed global context module and local context module, respectively, to generate their adaptive contextual features. Furthermore, we import multiple such modules to build several adaptive context blocks in different levels of network to obtain a coarse-to-fine result. Finally, comprehensive experimental evaluations demonstrate the effectiveness of the proposed ACNet, and new state-of-the-arts performances are achieved on all four public datasets, i.e. Cityscapes, ADE20K, PASCAL Context, and COCO Stuff."
Adaptive Density Map Generation for Crowd Counting,"Jia Wan, Antoni Chan","Department of Computer Science, City University of Hong Kong",100.0,Hong Kong,0.0,,"Crowd counting is an important topic in computer vision due to its practical usage in surveillance systems. The typical design of crowd counting algorithms is divided into two steps. First, the ground-truth density maps of crowd images are generated from the ground-truth dot maps (density map generation), e.g., by convolving with a Gaussian kernel. Second, deep learning models are designed to predict a density map from an input image (density map estimation). Most research efforts have concentrated on the density map estimation problem, while the problem of density map generation has not been adequately explored. In particular, the density map could be considered as an intermediate representation used to train a crowd counting network. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used. To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. Then, we propose an adaptive density map generator, which takes the annotation dot map as input, and learns a density map representation for a counter. The counter and generator are trained jointly within an end-to-end framework. The experiment results on popular counting datasets confirm the effectiveness of the proposed learnable density map representations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wan_Adaptive_Density_Map_Generation_for_Crowd_Counting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wan_Adaptive_Density_Map_Generation_for_Crowd_Counting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009065/,"['Kernel', 'Estimation', 'Bandwidth', 'Prediction algorithms', 'Training', 'Generators', 'Feature extraction']","['Density Map', 'Crowd Counting', 'Density Map Generation', 'Gaussian Kernel', 'Input Image', 'Popular Datasets', 'Intermediate Representation', 'Joint Training', 'Counting Algorithm', 'Loss Function', 'Root Mean Square Error', 'Learning Rate', 'Superior Performance', 'General Framework', 'Density Estimation', 'Adam Optimizer', 'Scale Variation', 'Receptive Field', 'Challenging Dataset', 'Attention Map', 'Refiner', 'Adaptive Kernel', 'Traditional Mapping', 'Global Regression', 'Self-attention Module', 'Image Pyramid', 'International Exhibition', 'Multi-scale Feature Extraction', 'Crowd Density']",,107,"Crowd counting is an important topic in computer vision due to its practical usage in surveillance systems. The typical design of crowd counting algorithms is divided into two steps. First, the ground-truth density maps of crowd images are generated from the ground-truth dot maps (density map generation), e.g., by convolving with a Gaussian kernel. Second, deep learning models are designed to predict a density map from an input image (density map estimation). Most research efforts have concentrated on the density map estimation problem, while the problem of density map generation has not been adequately explored. In particular, the density map could be considered as an intermediate representation used to train a crowd counting network. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used. To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. Then, we propose an adaptive density map generator, which takes the annotation dot map as input, and learns a density map representation for a counter. The counter and generator are trained jointly within an end-to-end framework. The experiment results on popular counting datasets confirm the effectiveness of the proposed learnable density map representations."
Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding,"Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Dechao Meng, Qingming Huang","3University of Science and Technology of China, Hefei, China; 1Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China; 2University of Chinese Academy of Sciences, Beijing, China; 1Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China; 2University of Chinese Academy of Sciences, Beijing, China; 4Peng Cheng Laboratory, Shenzhen, China; 1Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China",100.0,china,0.0,,"Weakly supervised referring expression grounding aims at localizing the referential object in an image according to the linguistic query, where the mapping between the referential object and query is unknown in the training stage. To address this problem, we propose a novel end-to-end adaptive reconstruction network (ARN). It builds the correspondence between image region proposal and query in an adaptive manner: adaptive grounding and collaborative reconstruction. Specifically, we first extract the subject, location and context features to represent the proposals and the query respectively. Then, we design the adaptive grounding module to compute the matching score between each proposal and query by a hierarchical attention model. Finally, based on attention score and proposal features, we reconstruct the input query with a collaborative loss of language reconstruction loss, adaptive reconstruction loss, and attribute classification loss. This adaptive mechanism helps our model to alleviate the variance of different referring expressions. Experiments on four large-scale datasets show ARN outperforms existing state-of-the-art methods by a large margin. Qualitative results demonstrate that the proposed ARN can better handle the situation where multiple objects of a particular category situated together.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_Expression_Grounding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_Expression_Grounding_ICCV_2019_paper.pdf,,https://github.com/GingL/ARN,,main,Poster,https://ieeexplore.ieee.org/document/9008126/,"['Proposals', 'Image reconstruction', 'Grounding', 'Feature extraction', 'Collaboration', 'Visualization', 'Adaptive systems']","['Network Reconstruction', 'Referring Expression Grounding', 'Local Features', 'Image Regions', 'Multiple Objects', 'Matching Score', 'Reconstruction Loss', 'Region Proposal', 'Attention Scores', 'Reference Object', 'Adaptive Modulation', 'Adaptive Manner', 'Categorical Attributes', 'Local Information', 'Related Features', 'Visual Features', 'Attention Mechanism', 'Intersection Over Union', 'Training Images', 'Relative Location', 'Proposal Features', 'Object Proposals', 'Hidden Vector', 'Final Representation', 'Image Categories', 'Word Embedding', 'Faster R-CNN', 'Reconstruction Module', 'Bidirectional Long Short-term Memory', 'Set Of Proposals']",,62,"Weakly supervised referring expression grounding aims at localizing the referential object in an image according to the linguistic query, where the mapping between the referential object and query is unknown in the training stage. To address this problem, we propose a novel end-to-end adaptive reconstruction network (ARN). It builds the correspondence between image region proposal and query in an adaptive manner: adaptive grounding and collaborative reconstruction. Specifically, we first extract the subject, location and context features to represent the proposals and the query respectively. Then, we design the adaptive grounding module to compute the matching score between each proposal and query by a hierarchical attention model. Finally, based on attention score and proposal features, we reconstruct the input query with a collaborative loss of language reconstruction loss, adaptive reconstruction loss, and attribute classification loss. This adaptive mechanism helps our model to alleviate the variance of different referring expressions. Experiments on four large-scale datasets show ARN outperforms existing state-of-the-art methods by a large margin. Qualitative results demonstrate that the proposed ARN can better handle the situation where multiple objects of a particular category situated together."
Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression,"Xinyao Wang, Liefeng Bo, Li Fuxin",Oregon State University; JD Digits,50.0,usa,50.0,USA,"Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Adaptive_Wing_Loss_for_Robust_Face_Alignment_via_Heatmap_Regression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Adaptive_Wing_Loss_for_Robust_Face_Alignment_via_Heatmap_Regression_ICCV_2019_paper.pdf,,https://github.com/protossw512/AdaptiveWingLoss,,main,Poster,https://ieeexplore.ieee.org/document/9010657/,"['Heating systems', 'Face', 'Training', 'Adaptation models', 'Predictive models', 'Convolution', 'Robustness']","['Face Alignment', 'Heatmap Regression', 'Wing Loss', 'Adaptive Wing', 'Loss Function', 'Background Pixels', 'Landmark Localization', 'Foreground Pixels', 'Types Of Pixels', 'Boundary Prediction', 'Convolutional Neural Network', 'Localization Accuracy', 'Convolution Operation', 'Bounding Box', 'Area Under Curve', 'Top-down And Bottom-up', 'Pose Estimation', 'Small Influence', 'Additional Channels', 'Coordinate Information', 'Mean Square Error Loss', 'Influence Of Errors', 'Human Pose Estimation', 'Landmark Coordinates', 'Improve Localization Accuracy', 'Ground Truth Pixels', 'Boundary Information', 'Nonlinear Part', 'Interocular Distance', 'Gradient Magnitude']",,194,"Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks."
Addressing Model Vulnerability to Distributional Shifts Over Image Transformation Sets,"Riccardo Volpi, Vittorio Murino","Istituto Italiano di Tecnologia; Istituto Italiano di Tecnologia, Universit`a di Verona, Huawei Technologies (Ireland) Co., Ltd., Dublin",100.0,Italy,0.0,,"We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Volpi_Addressing_Model_Vulnerability_to_Distributional_Shifts_Over_Image_Transformation_Sets_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Volpi_Addressing_Model_Vulnerability_to_Distributional_Shifts_Over_Image_Transformation_Sets_ICCV_2019_paper.pdf,,https://github.com/ricvolpi/domain-shift-robustness,,main,Poster,https://ieeexplore.ieee.org/document/9010347/,"['Optimization', 'Training', 'Robustness', 'Search problems', 'Data models', 'Computational modeling', 'Learning systems']","['Domain Shift', 'Image Transformation', 'Optimization Problem', 'Classification Problem', 'Search Algorithm', 'Data Augmentation', 'Training Procedure', 'Semantic Segmentation', 'Combinatorial Optimization Problem', 'Model Performance', 'Training Set', 'Learning Rate', 'Convolutional Neural Network', 'Mutation Rate', 'Fitness Function', 'Learning System', 'Capability Of Model', 'RGB Images', 'Random Data', 'Visual Conditions', 'Empirical Risk Minimization', 'Random Search', 'Face Detection', 'Domain Generalization', 'Generalization Capability Of The Model', 'Problem Instances', 'Domain Adaptation', 'Important Research Direction', 'Simplest Approach', 'Use Of Inputs']",,52,"We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts."
AdvIT: Adversarial Frames Identifier Based on Temporal Consistency in Videos,"Chaowei Xiao, Ruizhi Deng, Bo Li, Taesung Lee, Benjamin Edwards, Jinfeng Yi, Dawn Song, Mingyan Liu, Ian Molloy","University of Michigan, Ann Arbor; IBM Research AI; JD.com; UIUC; Simon Fraser University; UC Berkeley",66.66666666666666,"canada, usa",33.33333333333334,USA,"Deep neural networks (DNNs) have been widely applied in various applications, including autonomous driving and surveillance systems. However, DNNs are found to be vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead a learner to make incorrect predictions. While several defense and detection approaches are proposed for static image classification, many security-critical tasks use videos as their input and require efficient processing. In this paper, we propose an efficient and effective method advIT to detect adversarial frames within videos against different types of attacks based on temporal consistency property of videos. In particular, we apply optical flow estimation to the target and previous frames to generate pseudo frames and evaluate the consistency of the learner output between these pseudo frames and target. High inconsistency indicates that the target frame is adversarial. We conduct extensive experiments on various learning tasks including video semantic segmentation, human pose estimation, object detection, and action recognition, and demonstrate that we can achieve above 95% adversarial frame detection rate. To consider adaptive attackers, we show that even if an adversary has access to the detector and performs a strong adaptive attack based on the state of the art expectation of transformation method, the detection rate stays almost the same. We also tested the transferability among different optical flow estimators and show that it is hard for attackers to attack one and transfer the perturbation to others. In addition, as efficiency is important in video analysis, we show that advIT can achieve real-time detection in about 0.03--0.4 seconds.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiao_AdvIT_Adversarial_Frames_Identifier_Based_on_Temporal_Consistency_in_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiao_AdvIT_Adversarial_Frames_Identifier_Based_on_Temporal_Consistency_in_Videos_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010733/,"['Videos', 'Task analysis', 'Perturbation methods', 'Object detection', 'Pose estimation', 'Semantics', 'Adaptive optics']","['Temporal Consistency', 'Consistency In Videos', 'Adversarial Framing', 'Deep Neural Network', 'Learning Task', 'Object Detection', 'Semantic Segmentation', 'Static Images', 'Action Recognition', 'Optical Flow', 'Flow Estimation', 'Previous Frame', 'Human Pose Estimation', 'Human Activity Recognition', 'Adversarial Examples', 'Optical Flow Estimation', 'Target Frame', 'Different Types Of Attacks', 'Defensive Approach', 'Detection Methods', 'Adversarial Perturbations', 'Adversarial Attacks', 'Object Detection Task', 'Defense Methods', 'Current Frame', 'Temporal Continuity', 'Video Frames', 'Video Action Recognition', 'Action Recognition Model', 'Bounding Box']",,29,"Deep neural networks (DNNs) have been widely applied in various applications, including autonomous driving and surveillance systems. However, DNNs are found to be vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead a learner to make incorrect predictions. While several defense and detection approaches are proposed for static image classification, many security-critical tasks use videos as their input and require efficient processing. In this paper, we propose an efficient and effective method advIT to detect adversarial frames within videos against different types of attacks based on temporal consistency property of videos. In particular, we apply optical flow estimation to the target and previous frames to generate pseudo frames and evaluate the consistency of the learner output between these pseudo frames and target. High inconsistency indicates that the target frame is adversarial. We conduct extensive experiments on various learning tasks including video semantic segmentation, human pose estimation, object detection, and action recognition, and demonstrate that we can achieve above 95% adversarial frame detection rate. To consider adaptive attackers, we show that even if an adversary has access to the detector and performs a strong adaptive attack based on the state of the art expectation of transformation method, the detection rate stays almost the same. We also tested the transferability among different optical flow estimators and show that it is hard for attackers to attack one and transfer the perturbation to others. In addition, as efficiency is important in video analysis, we show that advIT can achieve real-time detection in about 0.03--0.4 seconds."
Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks,"Aamir Mustafa, Salman Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, Ling Shao",Australian National University; Beijing Institute of Technology; Inception Institute of Artiﬁcial Intelligence; University of Canberra,100.0,"Australia, China, uae",0.0,,"Deep neural networks are vulnerable to adversarial attacks which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mustafa_Adversarial_Defense_by_Restricting_the_Hidden_Space_of_Deep_Neural_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mustafa_Adversarial_Defense_by_Restricting_the_Hidden_Space_of_Deep_Neural_ICCV_2019_paper.pdf,,https://github.com/aamir-mustafa/pcl-adversarial-defense,,main,Poster,https://ieeexplore.ieee.org/document/9010713/,"['Perturbation methods', 'Robustness', 'Training', 'Optimization', 'Neural networks', 'Marine vehicles', 'Iterative methods']","['Deep Network', 'Deep Neural Network', 'Adversarial Defense', 'Feature Space', 'Feature Representation', 'Classification Of Samples', 'Clear Image', 'Extensive Evaluation', 'Region Classification', 'Intermediate Features', 'Intermediate Representation', 'Adversarial Attacks', 'Polytope', 'Maximum Separation', 'Simple Constraints', 'White-box Attack', 'Loss Function', 'Objective Function', 'Cognitive Model', 'Cross-entropy Loss', 'Fast Gradient Sign Method', 'Adversarial Examples', 'Projected Gradient Descent', 'MNIST Dataset', 'Defense Methods', 'Adversarial Training', 'Black-box Attacks', 'Adversarial Perturbations', 'Gradient Loss', 'Output Space']",,89,"Deep neural networks are vulnerable to adversarial attacks which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses."
Adversarial Defense via Learning to Generate Diverse Attacks,"Yunseok Jang, Tianchen Zhao, Seunghoon Hong, Honglak Lee","University of Michigan; University of Michigan, Korea Advanced Institute of Science and Technology",100.0,"south korea, usa",0.0,,"With the remarkable success of deep learning, Deep Neural Networks (DNNs) have been applied as dominant tools to various machine learning domains. Despite this success, however, it has been found that DNNs are surprisingly vulnerable to malicious attacks; adding a small, perceptually indistinguishable perturbations to the data can easily degrade classification performance. Adversarial training is an effective defense strategy to train a robust classifier. In this work, we propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbation by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classifier. Our experiment results on MNIST and CIFAR-10 datasets show that the classifier adversarially trained with our method yields more robust performance over various white-box and black-box attacks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008544/,"['Generators', 'Perturbation methods', 'Training', 'Robustness', 'Optimization', 'Neural networks', 'Machine learning']","['Adversarial Defense', 'Diverse Attacks', 'Neural Network', 'Deep Learning', 'Deep Neural Network', 'Target Class', 'Robust Classification', 'Adversarial Training', 'MNIST Dataset', 'Adversarial Examples', 'Machine Learning Domain', 'Black-box Attacks', 'Defense Mechanisms', 'Alternative Models', 'Gradient Descent', 'Latent Variables', 'Loss Of Diversity', 'Classification Loss', 'Optimal Path', 'Fourth Row', 'Projected Gradient Descent', 'Fast Gradient Sign Method', 'Adversarial Attacks', 'Adversarial Perturbations', 'First-order Method', 'Gradient-based Methods', 'White-box Attack', 'Attack Methods', 'LeNet-5']",,51,"With the remarkable success of deep learning, Deep Neural Networks (DNNs) have been applied as dominant tools to various machine learning domains. Despite this success, however, it has been found that DNNs are surprisingly vulnerable to malicious attacks; adding a small, perceptually indistinguishable perturbations to the data can easily degrade classification performance. Adversarial training is an effective defense strategy to train a robust classifier. In this work, we propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbation by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classifier. Our experiment results on MNIST and CIFAR-10 datasets show that the classifier adversarially trained with our method yields more robust performance over various white-box and black-box attacks."
Adversarial Feedback Loop,"Firas Shama, Roey Mechrez, Alon Shoshan, Lihi Zelnik-Manor","Technion - Israel Institute of Technology; Technion - Israel Institute of Technology, Alibaba Group",100.0,israel,0.0,,"Thanks to their remarkable generative capabilities, GANs have gained great popularity, and are used abundantly in state-of-the-art methods and applications. In a GAN based model, a discriminator is trained to learn the real data distribution. To date, it has been used only for training purposes, where it's utilized to train the generator to provide real-looking outputs. In this paper we propose a novel method that makes an explicit use of the discriminator in test-time, in a feedback manner in order to improve the generator results. To the best of our knowledge it is the first time a discriminator is involved in test-time. We claim that the discriminator holds significant information on the real data distribution, that could be useful for test-time as well, a potential that has not been explored before. The approach we propose does not alter the conventional training stage. At test-time, however, it transfers the output from the generator into the discriminator, and uses feedback modules (convolutional blocks) to translate the features of the discriminator layers into corrections to the features of the generator layers, which are used eventually to get a better generator result. Our method can contribute to both conditional and unconditional GANs. As demonstrated by our experiments, it can improve the results of state-of-the-art networks for super-resolution, and image generation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shama_Adversarial_Feedback_Loop_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shama_Adversarial_Feedback_Loop_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010247/,"['Generators', 'Training', 'Gallium nitride', 'Feedback loop', 'Image resolution', 'Image generation', 'Feeds']","['Feedback Loop', 'Image Generation', 'Real Distribution', 'Conditional Generative Adversarial Network', 'Feedback Modalities', 'Real Data Distribution', 'Convolutional Layers', 'Training Phase', 'Discriminative Features', 'Reference Image', 'Activation Maps', 'Visual Quality', 'Output Image', 'Pose Estimation', 'General Architecture', 'Adversarial Training', 'Perceptual Loss', 'Sanity Check', 'Baseline Network', 'Fr√©chet Inception Distance', 'Generation Layer', 'Wasserstein Distance', 'Inception Distance', 'Layer Of The Discriminator', 'Concept Of Feedback']",,14,"Thanks to their remarkable generative capabilities, GANs have gained great popularity, and are used abundantly in state-of-the-art methods and applications. In a GAN based model, a discriminator is trained to learn the real data distribution. To date, it has been used only for training purposes, where it's utilized to train the generator to provide real-looking outputs. In this paper we propose a novel method that makes an explicit use of the discriminator in test-time, in a feedback manner in order to improve the generator results. To the best of our knowledge it is the first time a discriminator is involved in test-time. We claim that the discriminator holds significant information on the real data distribution, that could be useful for test-time as well, a potential that has not been explored before. The approach we propose does not alter the conventional training stage. At test-time, however, it transfers the output from the generator into the discriminator, and uses feedback modules (convolutional blocks) to translate the features of the discriminator layers into corrections to the features of the generator layers, which are used eventually to get a better generator result. Our method can contribute to both conditional and unconditional GANs. As demonstrated by our experiments, it can improve the results of state-of-the-art networks for super-resolution, and image generation."
Adversarial Fine-Grained Composition Learning for Unseen Attribute-Object Recognition,"Kun Wei, Muli Yang, Hao Wang, Cheng Deng, Xianglong Liu","State Key Lab of Software Development Environment, Beihang University, Beijing 100191, China; School of Electronic Engineering, Xidian University, Xi’an 710071, China; Tencent AI Lab, Shenzhen 518057, China; School of Electronic Engineering, Xidian University, Xi’an 710071, China",75.0,"China, china",25.0,China,"Recognizing unseen attribute-object pairs never appearing in the training data is a challenging task, since an object often refers to a specific entity while an attribute is an abstract semantic description. Besides, attributes are highly correlated to objects, i.e., an attribute tends to describe different visual features of various objects. Existing methods mainly employ two classifiers to recognize attribute and object separately, or simply simulate the composition of attribute and object, which ignore the inherent discrepancy and correlation between them. In this paper, we propose a novel adversarial fine-grained composition learning model for unseen attribute-object pair recognition. Considering their inherent discrepancy, we leverage multi-scale feature integration to capture discriminative fine-grained features from a given image. Besides, we devise a quintuplet loss to depict more accurate correlations between attributes and objects. Adversarial learning is employed to model the discrepancy and correlations among attributes and objects. Extensive experiments on two challenging benchmarks indicate that our method consistently outperforms state-of-the-art competitors by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wei_Adversarial_Fine-Grained_Composition_Learning_for_Unseen_Attribute-Object_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wei_Adversarial_Fine-Grained_Composition_Learning_for_Unseen_Attribute-Object_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010732/,"['Generators', 'Task analysis', 'Visualization', 'Training', 'Gallium nitride', 'Feature extraction', 'Cats']","['Generative Adversarial Networks', 'Visual Features', 'Discriminative Features', 'Object Features', 'Large Margin', 'Inherent Variability', 'Fine-grained Features', 'Challenging Benchmark', 'Fine-grained Model', 'Objective Function', 'Classification Accuracy', 'Positive Samples', 'Negative Samples', 'Recognition Task', 'Harmonic Mean', 'Word Embedding', 'Common Space', 'Relational Space', 'Linear Support Vector Machine', 'ImageNet Dataset', 'Zero-shot', 'Triplet Loss', 'Closed And Open', 'Unseen Images', 'Training Pairs', 'Candidate Pairs', 'Trade-off Parameter']",,59,"Recognizing unseen attribute-object pairs never appearing in the training data is a challenging task, since an object often refers to a specific entity while an attribute is an abstract semantic description. Besides, attributes are highly correlated to objects, i.e., an attribute tends to describe different visual features of various objects. Existing methods mainly employ two classifiers to recognize attribute and object separately, or simply simulate the composition of attribute and object, which ignore the inherent discrepancy and correlation between them. In this paper, we propose a novel adversarial fine-grained composition learning model for unseen attribute-object pair recognition. Considering their inherent discrepancy, we leverage multi-scale feature integration to capture discriminative fine-grained features from a given image. Besides, we devise a quintuplet loss to depict more accurate correlations between attributes and objects. Adversarial learning is employed to model the discrepancy and correlations among attributes and objects. Extensive experiments on two challenging benchmarks indicate that our method consistently outperforms state-of-the-art competitors by a large margin."
Adversarial Learning With Margin-Based Triplet Embedding Regularization,"Yaoyao Zhong, Weihong Deng",Beijing University of Posts and Telecommunications,100.0,China,0.0,,"The Deep neural networks (DNNs) have achieved great success on a variety of computer vision tasks, however, they are highly vulnerable to adversarial attacks. To address this problem, we propose to improve the local smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification objective, so that the obtained models learn to resist adversarial examples. The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way. Experimental results on MNIST, CASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the robustness of the network against both feature and label adversarial attacks in simple object classification and deep face recognition.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhong_Adversarial_Learning_With_Margin-Based_Triplet_Embedding_Regularization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhong_Adversarial_Learning_With_Margin-Based_Triplet_Embedding_Regularization_ICCV_2019_paper.pdf,,https://github.com/zhongyy/Adversarial_MTER,,main,Poster,https://ieeexplore.ieee.org/document/9010739/,"['Training', 'Robustness', 'Perturbation methods', 'Iterative methods', 'Face recognition', 'Optimization', 'Entropy']","['Generative Adversarial Networks', 'Embedding Regularization', 'Deep Neural Network', 'Face Recognition', 'Object Classification', 'Large Margin', 'Regularization Term', 'Simple Classification', 'Adversarial Attacks', 'Iterative Way', 'Adversarial Examples', 'Training Dataset', 'Specific Tasks', 'Softmax', 'Cross-entropy', 'Data Augmentation', 'Fast Method', 'Large-scale Datasets', 'Target Image', 'Latent Space', 'Fast Gradient Sign Method', 'Black-box Attacks', 'Adversarial Training', 'Source Model', 'Defense Methods', 'Face Model', 'Attack Methods', 'Clear Image', 'Source Images', 'Recognition Performance']",,24,"The Deep neural networks (DNNs) have achieved great success on a variety of computer vision tasks, however, they are highly vulnerable to adversarial attacks. To address this problem, we propose to improve the local smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification objective, so that the obtained models learn to resist adversarial examples. The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way. Experimental results on MNIST, CASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the robustness of the network against both feature and label adversarial attacks in simple object classification and deep face recognition."
Adversarial Representation Learning for Text-to-Image Matching,"Nikolaos Sarafianos, Xiang Xu, Ioannis A. Kakadiaris","Computational Biomedicine Lab, University of Houston",100.0,USA,0.0,,"For many computer vision applications such as image captioning, visual question answering, and person search, learning discriminative feature representations at both image and text level is an essential yet challenging problem. Its challenges originate from the large word variance in the text domain as well as the difficulty of accurately measuring the distance between the features of the two modalities. Most prior work focuses on the latter challenge, by introducing loss functions that help the network learn better feature representations but fail to account for the complexity of the textual input. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial Matching approach that learns modality-invariant feature representations using adversarial and cross-modal matching objectives. In addition, we demonstrate that BERT, a publicly-available language model that extracts word embeddings, can successfully be applied in the text-to-image matching domain. The proposed approach achieves state-of-the-art cross-modal matching performance on four widely-used publicly-available datasets resulting in absolute improvements ranging from 2% to 5% in terms of rank-1 accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sarafianos_Adversarial_Representation_Learning_for_Text-to-Image_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sarafianos_Adversarial_Representation_Learning_for_Text-to-Image_Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009775/,"['Visualization', 'Feature extraction', 'Bit error rate', 'Task analysis', 'Training', 'Computer vision', 'Distance measurement']","['Generative Adversarial Networks', 'Representation Learning', 'Adversarial Representation Learning', 'Loss Function', 'Computer Vision', 'Feature Representation', 'Discriminative Features', 'Language Model', 'Word Embedding', 'Matching Approach', 'Matching Performance', 'Input Text', 'Visual Question Answering', 'Discriminative Feature Representation', 'Discriminative Feature Learning', 'Image Features', 'Visual Representation', 'Visual Features', 'Pedestrian', 'Textual Features', 'Textual Descriptions', 'Text Representation', 'Text Modality', 'Matching Loss', 'Best-performing Method', 'Fully-connected Layer', 'Kullback-Leibler', 'Attention Mechanism', 'Backbone Architecture']",,126,"For many computer vision applications such as image captioning, visual question answering, and person search, learning discriminative feature representations at both image and text level is an essential yet challenging problem. Its challenges originate from the large word variance in the text domain as well as the difficulty of accurately measuring the distance between the features of the two modalities. Most prior work focuses on the latter challenge, by introducing loss functions that help the network learn better feature representations but fail to account for the complexity of the textual input. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial Matching approach that learns modality-invariant feature representations using adversarial and cross-modal matching objectives. In addition, we demonstrate that BERT, a publicly-available language model that extracts word embeddings, can successfully be applied in the text-to-image matching domain. The proposed approach achieves state-of-the-art cross-modal matching performance on four widely-used publicly-available datasets resulting in absolute improvements ranging from 2% to 5% in terms of rank-1 accuracy."
"Adversarial Robustness vs. Model Compression, or Both?","Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin","University of California, Los Angeles, USA; Xi’an Jiaotong University, China; IIIS, Tsinghua University & IIISCT, China; SenseTime Research, China; Northeastern University, USA; MIT-IBM Watson AI Lab, IBM Research",83.33333333333334,"China, china, usa",16.666666666666657,China,"It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting; training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robustness nor high standard accuracy. Code is available at https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.pdf,,https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM,,main,Poster,https://ieeexplore.ieee.org/document/9009036/,['Hafnium'],"['Adversarial Robustness', 'Neural Network', 'Deep Neural Network', 'Model Size', 'Network Capacity', 'Adversarial Training', 'Conventional Settings', 'Adversarial Attacks', 'Notion Of Security', 'Minimax Optimization', 'Concurrent Training', 'Deep Learning', 'Learning Rate', 'Step Size', 'Test Accuracy', 'Lagrange Multiplier', 'Network Size', 'Saddle Point', 'Nonzero Elements', 'Maximization Problem', 'Pruning Strategy', 'Adversarial Examples', 'Sparse Weight', 'Projected Gradient Descent', 'Filters In Layer', 'Deep Neural Network Model', 'Target Model', 'Smaller Model Size', 'Adversarial Perturbations', 'Strong Robustness']",,76,"It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting; training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robustness nor high standard accuracy. Code is available at https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM."
Aggregation via Separation: Boosting Facial Landmark Detector With Semi-Supervised Style Translation,"Shengju Qian, Keqiang Sun, Wayne Wu, Chen Qian, Jiaya Jia","Tsinghua University; The Chinese University of Hong Kong; YouTu Lab, Tencent; SenseTime Research",75.0,"China, Hong Kong, china",25.0,China,"Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement. Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW, 300W, COFW, and AFLW datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qian_Aggregation_via_Separation_Boosting_Facial_Landmark_Detector_With_Semi-Supervised_Style_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_Aggregation_via_Separation_Boosting_Facial_Landmark_Detector_With_Semi-Supervised_Style_ICCV_2019_paper.pdf,,https://github.com/thesouthfrog/stylealign,,main,Poster,https://ieeexplore.ieee.org/document/9010025/,"['Face', 'Detectors', 'Training', 'Geometry', 'Visualization', 'Lighting', 'Image reconstruction']","['Landmark Detection', 'Facial Landmark Detection', 'Illumination', 'New Perspective', 'Large Margin', 'Face Images', 'Face Alignment', 'Structural Information', 'Training Images', 'Representation Learning', 'Latent Space', 'Visual Quality', 'Skip Connections', 'Synthetic Images', 'Self-supervised Learning', 'Landmark Localization', 'Facial Structure', 'Style Transfer', 'Baseline Network', 'Latent Code', 'Conditional Variational Autoencoder', 'Disentangled Representation', 'Style Image', 'Representation Of Geometry']",,61,"Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement. Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW, 300W, COFW, and AFLW datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign."
Agile Depth Sensing Using Triangulation Light Curtains,"Joseph R. Bartels, Jian Wang, William ""Red"" Whittaker, Srinivasa G. Narasimhan","The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University and Snap Research",100.0,usa,0.0,,"Depth sensors like LIDARs and Kinect use a fixed depth acquisition strategy that is independent of the scene of interest. Due to the low spatial and temporal resolution of these sensors, this strategy can undersample parts of the scene that are important (small or fast moving objects), or oversample areas that are not informative for the task at hand (a fixed planar wall). In this paper, we present an approach and system to dynamically and adaptively sample the depths of a scene using the principle of triangulation light curtains. The approach directly detects the presence or absence of objects at specified 3D lines. These 3D lines can be sampled sparsely, non-uniformly, or densely only at specified regions. The depth sampling can be varied in real-time, enabling quick object discovery or detailed exploration of areas of interest. These results are achieved using a novel prototype light curtain system that is based on a 2D rolling shutter camera with higher light efficiency, working range, and faster adaptation than previous work, making it useful broadly for autonomous navigation and exploration.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bartels_Agile_Depth_Sensing_Using_Triangulation_Light_Curtains_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bartels_Agile_Depth_Sensing_Using_Triangulation_Light_Curtains_ICCV_2019_paper.pdf,http://www.cs.cmu.edu/~ILIM/agile_depth_sensing,,,main,Oral,https://ieeexplore.ieee.org/document/9009554/,"['Cameras', 'Three-dimensional displays', 'Two dimensional displays', 'Robot sensing systems']","['Depth Camera', 'Light Curtain', 'Spatial Resolution', 'Low Resolution', 'Adaptive Sampling', 'Part Of The Scene', 'Absence Of Objects', '3D Line', 'Rolling Shutter', 'Field Of View', 'Image Plane', 'Point Cloud', 'Laser Light', 'Rotation Axis', 'Vision Research', 'Line Segment', 'Ambient Light', 'Laser Line', 'Depth Images', 'Design Points', 'Light Plane', 'Light Projection', 'Set Of Planes', 'Light Sheet', 'Objects In The Scene', 'Line Source', 'Hardware Prototype', 'Obstacle Avoidance', 'Low Distortion', 'Light Profiles']",,15,"Depth sensors like LIDARs and Kinect use a fixed depth acquisition strategy that is independent of the scene of interest. Due to the low spatial and temporal resolution of these sensors, this strategy can undersample parts of the scene that are important (small or fast moving objects), or oversample areas that are not informative for the task at hand (a fixed planar wall). In this paper, we present an approach and system to dynamically and adaptively sample the depths of a scene using the principle of triangulation light curtains. The approach directly detects the presence or absence of objects at specified 3D lines. These 3D lines can be sampled sparsely, non-uniformly, or densely only at specified regions. The depth sampling can be varied in real-time, enabling quick object discovery or detailed exploration of areas of interest. These results are achieved using a novel prototype light curtain system that is based on a 2D rolling shutter camera with higher light efficiency, working range, and faster adaptation than previous work, making it useful broadly for autonomous navigation and exploration."
Algebraic Characterization of Essential Matrices and Their Averaging in Multiview Settings,"Yoni Kasten, Amnon Geifman, Meirav Galun, Ronen Basri",Weizmann Institute of Science,100.0,israel,0.0,,"Essential matrix averaging, i.e., the task of recovering camera locations and orientations in calibrated, multiview settings, is a first step in global approaches to Euclidean structure from motion. A common approach to essential matrix averaging is to separately solve for camera orientations and subsequently for camera positions. This paper presents a novel approach that solves simultaneously for both camera orientations and positions. We offer a complete characterization of the algebraic conditions that enable a unique Euclidean reconstruction of n cameras from a collection of (^n_2) essential matrices. We next use these conditions to formulate essential matrix averaging as a constrained optimization problem, allowing us to recover a consistent set of essential matrices given a (possibly partial) set of measured essential matrices computed independently for pairs of images. We finally use the recovered essential matrices to determine the global positions and orientations of the n cameras. We test our method on common SfM datasets, demonstrating high accuracy while maintaining efficiency and robustness, compared to existing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kasten_Algebraic_Characterization_of_Essential_Matrices_and_Their_Averaging_in_Multiview_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kasten_Algebraic_Characterization_of_Essential_Matrices_and_Their_Averaging_in_Multiview_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009564/,"['Cameras', 'Symmetric matrices', 'Matrix decomposition', 'Image reconstruction', 'Robustness', 'Optimization', 'Eigenvalues and eigenfunctions']","['Essential Matrix', 'Multi-view Setting', 'Optimization Problem', 'Constrained Optimization', 'Constrained Optimization Problem', 'Camera Position', 'Structure From Motion', 'Camera Orientation', 'Collinearity', 'Eigenvectors', 'Diagonal Matrix', 'Set Of Conditions', 'Symmetric Matrix', 'Pairwise Matrix', 'Consistency Score', 'Absolute Position', 'Robust Optimization', 'Similarity Transformation', 'Spectral Decomposition', 'Skew-symmetric', 'Absolute Orientation', 'Fundamental Matrix', 'Bundle Adjustment', 'Positive Eigenvalues', 'Global Coordinate System', 'Distinct Eigenvalues', 'Calibration Matrix']",,14,"Essential matrix averaging, i.e., the task of recovering camera locations and orientations in calibrated, multiview settings, is a first step in global approaches to Euclidean structure from motion. A common approach to essential matrix averaging is to separately solve for camera orientations and subsequently for camera positions. This paper presents a novel approach that solves simultaneously for both camera orientations and positions. We offer a complete characterization of the algebraic conditions that enable a unique Euclidean reconstruction of n cameras from a collection of (
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">n</sup>
) essential matrices. We next use these conditions to formulate essential matrix averaging as a constrained optimization problem, allowing us to recover a consistent set of essential matrices given a (possibly partial) set of measured essential matrices computed independently for pairs of images. We finally use the recovered essential matrices to determine the global positions and orientations of the n cameras. We test our method on common SfM datasets, demonstrating high accuracy while maintaining efficiency and robustness, compared to existing methods."
"Align, Attend and Locate: Chest X-Ray Diagnosis via Contrast Induced Attention Network With Limited Supervision","Jingyu Liu,  Gangming Zhao,  Yu Fei,  Ming Zhang,  Yizhou Wang,  Yizhou Yu","Deepwise AI Lab; Department of Computer Science, Peking University; Department of Computer Science, Peking University; Deepwise AI Lab; Peng Cheng Laboratory",60.0,china,40.0,China,"Obstacles facing accurate identification and localization of diseases in chest X-ray images lie in the lack of high-quality images and annotations. In this paper, we propose a Contrast Induced Attention Network (CIA-Net), which exploits the highly structured property of chest X-ray images and localizes diseases via contrastive learning on the aligned positive and negative samples. To force the attention module to focus only on sites of abnormalities, we also introduce a learnable alignment module to adjust all the input images, which eliminates variations of scales, angles, and displacements of X-ray images generated under bad scan conditions. We show that the use of contrastive attention and alignment module allows the model to learn rich identification and localization information using only a small amount of location annotations, resulting in state-of-the-art performance in NIH chest X-ray dataset.",http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Align_Attend_and_Locate_Chest_X-Ray_Diagnosis_via_Contrast_Induced_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Align_Attend_and_Locate_Chest_X-Ray_Diagnosis_via_Contrast_Induced_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9010639/,"['X-ray imaging', 'Diseases', 'Feature extraction', 'Task analysis', 'Object detection', 'Detectors', 'Visualization']","['Chest X-ray', 'Chest X-ray Diagnosis', 'Positive Samples', 'Local Information', 'Input Image', 'Negative Samples', 'Localization Accuracy', 'Self-supervised Learning', 'Annotated Loci', 'Chest X-ray Images', 'Lack Of Annotation', 'Scanning Conditions', 'Alignment Module', 'Convolutional Neural Network', 'Feature Maps', 'Object Detection', 'Attention Mechanism', 'Mean Accuracy', 'Target Image', 'Positive Image', 'Image Annotation', 'Multiple Instance Learning', 'Negative Images', 'Limited Annotation', 'Network Alignment', 'Class Activation Maps', 'Attention Map', 'One-stage Detectors', 'Cardiomegaly', 'Visual Contrast']",,55,"Obstacles facing accurate identification and localization of diseases in chest X-ray images lie in the lack of high-quality images and annotations. In this paper, we propose a Contrast Induced Attention Network (CIA-Net), which exploits the highly structured property of chest X-ray images and localizes diseases via contrastive learning on the aligned positive and negative samples. To force the attention module to focus only on sites of abnormalities, we also introduce a learnable alignment module to adjust all the input images, which eliminates variations of scales, angles, and displacements of X-ray images generated under bad scan conditions. We show that the use of contrastive attention and alignment module allows the model to learn rich identification and localization information using only a small amount of location annotations, resulting in state-of-the-art performance in NIH chest X-ray dataset."
Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment,"Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran","Facebook AI Research; Georgia Institute of Technology; SRI International, Princeton, NJ",66.66666666666666,usa,33.33333333333334,USA,"We address the problem of grounding free-form textual phrases by using weak supervision from image-caption pairs. We propose a novel end-to-end model that uses caption-to-image retrieval as a downstream task to guide the process of phrase localization. Our method, as a first step, infers the latent correspondences between regions-of-interest (RoIs) and phrases in the caption and creates a discriminative image representation using these matched RoIs. In the subsequent step, this learned representation is aligned with the caption. Our key contribution lies in building this ""caption-conditioned"" image encoding, which tightly couples both the tasks and allows the weak supervision to effectively guide visual grounding. We provide extensive empirical and qualitative analysis to investigate the different components of our proposed model and compare it with competitive baselines. For phrase localization, we report an improvement of 4.9% and 1.3% (absolute) over the prior state-of-the-art on the VisualGenome and Flickr30k Entities datasets. We also report results that are at par with the state-of-the-art on the downstream caption-to-image retrieval task on COCO and Flickr30k datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Datta_Align2Ground_Weakly_Supervised_Phrase_Grounding_Guided_by_Image-Caption_Alignment_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Datta_Align2Ground_Weakly_Supervised_Phrase_Grounding_Guided_by_Image-Caption_Alignment_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010970/,"['Task analysis', 'Grounding', 'Visualization', 'Image representation', 'Computational modeling', 'Training', 'Image coding']","['Phrase Grounding', 'Image Representation', 'Tight Coupling', 'COCO Dataset', 'Weak Supervision', 'Image Encoder', 'Validation Set', 'Recurrent Neural Network', 'Image Regions', 'Multilayer Perceptron', 'Bounding Box', 'Latent Space', 'Semantic Similarity', 'Correct Location', 'Word Embedding', 'Local Module', 'Matching Score', 'Matching Task', 'Region Proposal', 'Noun Phrase', 'Matching Module', 'Verb Phrase', 'Ranking Loss', 'Hidden Layer Size']",,59,"We address the problem of grounding free-form textual phrases by using weak supervision from image-caption pairs. We propose a novel end-to-end model that uses caption-to-image retrieval as a downstream task to guide the process of phrase localization. Our method, as a first step, infers the latent correspondences between regions-of-interest (RoIs) and phrases in the caption and creates a discriminative image representation using these matched RoIs. In the subsequent step, this learned representation is aligned with the caption. Our key contribution lies in building this ""caption-conditioned"" image encoding, which tightly couples both the tasks and allows the weak supervision to effectively guide visual grounding. We provide extensive empirical and qualitative analysis to investigate the different components of our proposed model and compare it with competitive baselines. For phrase localization, we report an improvement of 4.9% and 1.3% (absolute) over the prior state-of-the-art on the VisualGenome and Flickr30k Entities datasets. We also report results that are at par with the state-of-the-art on the downstream caption-to-image retrieval task on COCO and Flickr30k datasets."
Aligning Latent Spaces for 3D Hand Pose Estimation,"Linlin Yang, Shile Li, Dongheui Lee, Angela Yao","Technical University of Munich, Germany; German Aerospace Center, Germany; National University of Singapore, Singapore; University of Bonn, Germany",75.0,"germany, singapore",25.0,Germany,"Hand pose estimation from monocular RGB inputs is a highly challenging task. Many previous works for monocular settings only used RGB information for training despite the availability of corresponding data in other modalities such as depth maps. In this work, we propose to learn a joint latent representation that leverages other modalities as weak labels to boost the RGB-based hand pose estimator. By design, our architecture is highly flexible in embedding various diverse modalities such as heat maps, depth maps and point clouds. In particular, we find that encoding and decoding the point cloud of the hand surface can improve the quality of the joint latent representation. Experiments show that with the aid of other modalities during training, our proposed method boosts the accuracy of RGB-based hand pose estimation systems and significantly outperforms state-of-the-art on two public benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Aligning_Latent_Spaces_for_3D_Hand_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Aligning_Latent_Spaces_for_3D_Hand_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009486/,"['Three-dimensional displays', 'Pose estimation', 'Training', 'Decoding', 'Space heating', 'Task analysis']","['Latent Space', 'Pose Estimation', 'Hand Pose', 'Hand Pose Estimation', '3D Hand Pose Estimation', 'Heatmap', 'Point Cloud', 'Depth Map', 'Latent Representation', 'Input RGB', 'Weak Labels', 'Latent Variables', 'Multiple Modalities', 'Conditional Independence', 'Kullback-Leibler', 'RGB Images', 'Depth Images', 'Cross-modal', 'Unlabeled Data', 'Depth Data', '3D Pose', 'Earth Moverâ€™s Distance', 'Point Cloud Reconstruction', '3D Point Cloud', 'Variational Autoencoder', 'Evidence Lower Bound', 'Multimodal Learning', 'Input Modalities', 'Chamfer Distance', 'Target Modality']",,64,"Hand pose estimation from monocular RGB inputs is a highly challenging task. Many previous works for monocular settings only used RGB information for training despite the availability of corresponding data in other modalities such as depth maps. In this work, we propose to learn a joint latent representation that leverages other modalities as weak labels to boost the RGB-based hand pose estimator. By design, our architecture is highly flexible in embedding various diverse modalities such as heat maps, depth maps and point clouds. In particular, we find that encoding and decoding the point cloud of the hand surface can improve the quality of the joint latent representation. Experiments show that with the aid of other modalities during training, our proposed method boosts the accuracy of RGB-based hand pose estimation systems and significantly outperforms state-of-the-art on two public benchmarks."
An Alarm System for Segmentation Algorithm Based on Shape Model,"Fengze Liu,  Yingda Xia,  Dong Yang,  Alan L. Yuille,  Daguang Xu",NVIDIA Corporation; Johns Hopkins University,50.0,usa,50.0,USA,"It is usually hard for a learning system to predict correctly on rare events that never occur in the training data, and there is no exception for segmentation algorithms. Meanwhile, manual inspection of each case to locate the failures becomes infeasible due to the trend of large data scale and limited human resource. Therefore, we build an alarm system that will set off alerts when the segmentation result is possibly unsatisfactory, assuming no corresponding ground truth mask is provided. One plausible solution is to project the segmentation results into a low dimensional feature space; then learn classifiers/regressors to predict their qualities. Motivated by this, in this paper, we learn a feature space using the shape information which is a strong prior shared among different datasets and robust to the appearance variation of input data. The shape feature is captured using a Variational Auto-Encoder (VAE) network that trained with only the ground truth masks. During testing, the segmentation results with bad shapes shall not fit the shape prior well, resulting in large loss values. Thus, the VAE is able to evaluate the quality of segmentation result on unseen data, without using ground truth. Finally, we learn a regressor in the one-dimensional feature space to predict the qualities of segmentation results. Our alarm system is evaluated on several recent state-of-art segmentation algorithms for 3D medical segmentation tasks. Compared with other standard quality assessment methods, our system consistently provides more reliable prediction on the qualities of segmentation results.",http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_An_Alarm_System_for_Segmentation_Algorithm_Based_on_Shape_Model_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_An_Alarm_System_for_Segmentation_Algorithm_Based_on_Shape_Model_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9010615/,"['Shape', 'Prediction algorithms', 'Image segmentation', 'Alarm systems', 'Uncertainty', 'Task analysis', 'Quality assessment']","['Segmentation Algorithm', 'Alarm System', 'Training Data', 'Quality Assessment', 'Feature Space', 'Rare Events', 'Shape Features', 'Segmentation Results', 'Segmentation Task', 'Variational Autoencoder', 'Limited Human Resources', 'Strong Prior', 'Low-dimensional Feature Space', 'Computed Tomography', '3D Images', 'Great Power', 'Bounding Box', 'Latent Space', 'Anomaly Detection', 'Dice Similarity Coefficient', 'Bayesian Neural Network', 'Tumor Segmentation', 'Number Of Training Data', 'Epistemic Uncertainty', 'Dice Score', 'Direct Regression', 'Dice Loss', 'Results Of The Quality Assessment', 'Aleatory', 'Identity Mapping']",,12,"It is usually hard for a learning system to predict correctly on rare events that never occur in the training data, and there is no exception for segmentation algorithms. Meanwhile, manual inspection of each case to locate the failures becomes infeasible due to the trend of large data scale and limited human resource. Therefore, we build an alarm system that will set off alerts when the segmentation result is possibly unsatisfactory, assuming no corresponding ground truth mask is provided. One plausible solution is to project the segmentation results into a low dimensional feature space; then learn classifiers/regressors to predict their qualities. Motivated by this, in this paper, we learn a feature space using the shape information which is a strong prior shared among different datasets and robust to the appearance variation of input data. The shape feature is captured using a Variational Auto-Encoder (VAE) network that trained with only the ground truth masks. During testing, the segmentation results with bad shapes shall not fit the shape prior well, resulting in large loss values. Thus, the VAE is able to evaluate the quality of segmentation result on unseen data, without using ground truth. Finally, we learn a regressor in the one-dimensional feature space to predict the qualities of segmentation results. Our alarm system is evaluated on several recent state-of-art segmentation algorithms for 3D medical segmentation tasks. Compared with other standard quality assessment methods, our system consistently provides more reliable prediction on the qualities of segmentation results."
An Efficient Solution to the Homography-Based Relative Pose Problem With a Common Reference Direction,"Yaqing Ding, Jian Yang, Jean Ponce, Hui Kong","INRIA, Paris, France; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China and IAAI Nanjing, Horizon Robotics",100.0,"France, china",0.0,,"In this paper, we propose a novel approach to two-view minimal-case relative pose problems based on homography with a common reference direction. We explore the rank-1 constraint on the difference between the Euclidean homography matrix and the corresponding rotation, and propose an efficient two-step solution for solving both the calibrated and partially calibrated (unknown focal length) problems. We derive new 3.5-point, 3.5-point, 4-point solvers for two cameras such that the two focal lengths are unknown but equal, one of them is unknown, and both are unknown and possibly different, respectively. We present detailed analyses and comparisons with existing 6 and 7-point solvers, including results with smart phone images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ding_An_Efficient_Solution_to_the_Homography-Based_Relative_Pose_Problem_With_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_An_Efficient_Solution_to_the_Homography-Based_Relative_Pose_Problem_With_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009580/,"['Cameras', 'Mathematical model', 'Transmission line matrix methods', 'Gravity', 'Matrix decomposition', 'Smart phones', 'Pose estimation']","['Efficient Solution', 'Relative Pose', 'Relative Pose Problem', 'Smartphone', 'Focal Length', 'Homography Matrix', 'Results Of System', 'Standard Algorithm', 'Corresponding Points', 'Image Point', 'Multiple Solutions', 'Null Space', 'Multiple Devices', 'Polynomial Equation', 'Monomial', '2-dimensional Space', 'Gravity Vector', 'Translation Error', 'Translation Vector', 'Extra Points', 'Rotation Error', 'Camera Focal Length', 'Standard Solver', 'Pure Rotation', 'Template Size', 'Unknown Problem', 'Equal Length', 'Scaling Factor', 'Numerical Stability']",,23,"In this paper, we propose a novel approach to two-view minimal-case relative pose problems based on homography with a common reference direction. We explore the rank-1 constraint on the difference between the Euclidean homography matrix and the corresponding rotation, and propose an efficient two-step solution for solving both the calibrated and partially calibrated (unknown focal length) problems. We derive new 3.5-point, 3.5-point, 4-point solvers for two cameras such that the two focal lengths are unknown but equal, one of them is unknown, and both are unknown and possibly different, respectively. We present detailed analyses and comparisons with existing 6 and 7-point solvers, including results with smart phone images."
An Empirical Study of Spatial Attention Mechanisms in Deep Networks,"Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, Jifeng Dai",Microsoft Research Asia; University of Science and Technology of China,50.0,china,50.0,USA,"Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_An_Empirical_Study_of_Spatial_Attention_Mechanisms_in_Deep_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_An_Empirical_Study_of_Spatial_Attention_Mechanisms_in_Deep_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009578/,"['Convolution', 'Task analysis', 'Image recognition', 'Computer vision', 'Semantics', 'Computational modeling', 'Natural language processing']","['Deep Network', 'Attention Mechanism', 'Spatial Attention', 'Spatial Attention Mechanism', 'Transformer', 'Deep Neural Network', 'Room For Improvement', 'Improvements In Design', 'Key Content', 'Deformable Convolution', 'Object Detection', 'Set Of Elements', 'Image Recognition', 'Semantic Segmentation', 'Attention Module', 'Computational Overhead', 'Words In Sentences', 'Attention Weights', 'Natural Language Processing Tasks', 'Feature Pyramid Network', 'Neural Machine Translation', 'Image Recognition Tasks', 'Subset Of Factors', 'Input Sentence', 'Query Terms', 'Input Word', 'Attention Heads', 'Non-zero Weights', 'Depthwise Separable Convolution', 'Depthwise Convolution']",,279,"Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms."
An Internal Learning Approach to Video Inpainting,"Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John Collomosse, Hailin Jin","Adobe Research; Stanford University; Adobe Research, University of Surrey",66.66666666666666,"uk, usa",33.33333333333334,USA,"We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_An_Internal_Learning_Approach_to_Video_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_An_Internal_Learning_Approach_to_Video_Inpainting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008384/,"['Electronics packaging', 'Training', 'Visualization', 'Optical imaging', 'Image generation', 'Feature extraction', 'Task analysis']","['Internal Learning', 'Video Inpainting', 'Convolutional Network', 'Static Images', 'Optical Flow', 'Motion Information', 'Prior Imaging', 'Convolutional Neural Network', 'Deep Neural Network', 'Loss Of Generality', 'Image Regions', 'Image Generation', 'Window Length', 'Video Frames', 'External Data', 'Peak Signal-to-noise Ratio', 'Consistency Loss', 'Complex Motion', 'Perceptual Loss', 'Temporal Consistency', 'Patch-based Methods', 'Hole Region', 'Image Inpainting', 'Fr√©chet Inception Distance', 'Flow Map', 'Noise Map', 'Middle Frame', '3D Convolution', 'Average Peak Signal-to-noise Ratio', 'Adjacent Frames']",,47,"We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency."
Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction,"Luca Anthony Thiede, Pratik Prabhanjan Brahma","Georg-August-Universität Göttingen; Innovation and Engineering Center California, Volkswagen Group of America",50.0,Germany,50.0,USA,"Trajectory or behavior prediction of traffic agents is an important component of autonomous driving and robot planning in general. It can be framed as a probabilistic future sequence generation problem and recent literature has studied the applicability of generative models in this context. The variety or Minimum over N (MoN) loss, which tries to minimize the error between the ground truth and the closest of N output predictions, has been used in these recent learning models to improve the diversity of predictions. In this work, we present a proof to show that the MoN loss does not lead to the ground truth probability density function, but approximately to its square root instead. We validate this finding with extensive experiments on both simulated toy as well as real world datasets. We also propose multiple solutions to compensate for the dilation to show improvement of log likelihood of the ground truth samples in the corrected probability density function.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010632/,"['Trajectory', 'Probability density function', 'Measurement', 'Predictive models', 'Data models', 'Histograms', 'Probabilistic logic']","['Trajectory Prediction', 'Square Root', 'Log-likelihood', 'Probability Density Function', 'Real-world Datasets', 'Ground Truth Samples', 'Likelihood Of Sampling', 'Time Step', 'Expected Value', 'Low Probability', 'Cost Function', 'Mixture Model', 'Kullback-Leibler', 'Generative Adversarial Networks', 'Marginal Distribution', 'Path Planning', 'Inference Time', 'Variational Autoencoder', 'Image Synthesis', 'Bin Width', 'Jensen-Shannon Divergence', 'Certain Point In Time', 'Pedestrian Trajectory', 'Average Likelihood']",,33,"Trajectory or behavior prediction of traffic agents is an important component of autonomous driving and robot planning in general. It can be framed as a probabilistic future sequence generation problem and recent literature has studied the applicability of generative models in this context. The variety or Minimum over N (MoN) loss, which tries to minimize the error between the ground truth and the closest of N output predictions, has been used in these recent learning models to improve the diversity of predictions. In this work, we present a proof to show that the MoN loss does not lead to the ground truth probability density function, but approximately to its square root instead. We validate this finding with extensive experiments on both simulated toy as well as real world datasets. We also propose multiple solutions to compensate for the dilation to show improvement of log likelihood of the ground truth samples in the corrected probability density function."
Anchor Diffusion for Unsupervised Video Object Segmentation,"Zhao Yang, Qiang Wang, Luca Bertinetto, Weiming Hu, Song Bai, Philip H. S. Torr",CASIA; University of Oxford; Five AI,66.66666666666666,"china, uk",33.33333333333334,UK,"Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approach tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators, we introduce a technique to establish dense correspondences between pixel embeddings of a reference ""anchor"" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of 81.7%, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the video saliency dataset ViSal, showing results competitive with the state of the art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Anchor_Diffusion_for_Unsupervised_Video_Object_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Anchor_Diffusion_for_Unsupervised_Video_Object_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009054/,"['Object segmentation', 'Optical network units', 'Adaptive optics', 'Adaptation models', 'Pipelines', 'Recurrent neural networks', 'Task analysis']","['Video Object Segmentation', 'Unsupervised Video Object Segmentation', 'State Of The Art', 'Recurrent Neural Network', 'Unsupervised Methods', 'Static Images', 'Optical Flow', 'Temporal Dependencies', 'Foreground Objects', 'Non-local Operation', 'Benchmark', 'Object Detection', 'Bounding Box', 'Vector Field', 'Semantic Segmentation', 'Dot Product', 'Changes In Appearance', 'Segmentation Task', 'Salient Object Detection', 'Semi-supervised Methods', 'Absolute Point', 'Current Frame', 'Gating Mechanism', 'Video Sequences', 'Pixel-level Annotations', 'Inference Time', 'Binary Cross-entropy Loss', 'Feature Encoder']",,69,"Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approach tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators, we introduce a technique to establish dense correspondences between pixel embeddings of a reference ""anchor"" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of 81.7%, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the video saliency dataset ViSal, showing results competitive with the state of the art."
Anchor Loss: Modulating Loss Scale Based on Prediction Difficulty,"Serim Ryou, Seong-Gyun Jeong, Pietro Perona",CODE42.ai; California Institute of Technology,50.0,usa,50.0,China,"We propose a novel loss function that dynamically re-scales the cross entropy based on prediction difficulty regarding a sample. Deep neural network architectures in image classification tasks struggle to disambiguate visually similar objects. Likewise, in human pose estimation symmetric body parts often confuse the network with assigning indiscriminative scores to them. This is due to the output prediction, in which only the highest confidence label is selected without taking into consideration a measure of uncertainty. In this work, we define the prediction difficulty as a relative property coming from the confidence score gap between positive and negative labels. More precisely, the proposed loss function penalizes the network to avoid the score of a false prediction being significant. To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ryou_Anchor_Loss_Modulating_Loss_Scale_Based_on_Prediction_Difficulty_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ryou_Anchor_Loss_Modulating_Loss_Scale_Based_on_Prediction_Difficulty_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009013/,"['Task analysis', 'Entropy', 'Pose estimation', 'Robustness', 'Heating systems', 'Object detection', 'Modulation']","['Loss Function', 'Body Parts', 'Deep Neural Network', 'Classification Task', 'Image Classification', 'Cross-entropy', 'Prediction Score', 'Confidence Score', 'Related Properties', 'Deep Architecture', 'Pose Estimation', 'False Predictions', 'Image Classification Tasks', 'Deep Neural Network Architecture', 'Parts Of The Human Body', 'Human Pose Estimation', 'Symmetric Part', 'True Positive', 'Object Detection', 'Hard Examples', 'Focal Loss', 'Target Class', 'Binary Cross Entropy', 'Loss Value', 'Cross-entropy Loss', 'Binary Entropy', 'Standard Cross', 'Pixel Location', 'Network Output']",,32,"We propose a novel loss function that dynamically re-scales the cross entropy based on prediction difficulty regarding a sample. Deep neural network architectures in image classification tasks struggle to disambiguate visually similar objects. Likewise, in human pose estimation symmetric body parts often confuse the network with assigning indiscriminative scores to them. This is due to the output prediction, in which only the highest confidence label is selected without taking into consideration a measure of uncertainty. In this work, we define the prediction difficulty as a relative property coming from the confidence score gap between positive and negative labels. More precisely, the proposed loss function penalizes the network to avoid the score of a false prediction being significant. To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods."
Anomaly Detection in Video Sequence With Appearance-Motion Correspondence,"Trong-Nguyen Nguyen, Jean Meunier","DIRO, University of Montreal",100.0,Canada,0.0,,"Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_Anomaly_Detection_in_Video_Sequence_With_Appearance-Motion_Correspondence_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_Anomaly_Detection_in_Video_Sequence_With_Appearance-Motion_Correspondence_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009067/,"['Decoding', 'Anomaly detection', 'Image reconstruction', 'Adaptive optics', 'Training', 'Trajectory', 'Surveillance']","['Anomaly Detection', 'Video Sequences', 'Convolutional Neural Network', 'Pedestrian', 'Benchmark Datasets', 'Training Stage', 'Video Frames', 'Common Objects', 'Normal Events', 'Video Surveillance', 'Object Appearance', 'Detection In Videos', 'Training Set', 'Training Data', 'Convolutional Layers', 'Feature Maps', 'Object Detection', 'Generative Adversarial Networks', 'Small Patches', 'Optical Flow', 'Anomalous Events', 'Inception Module', 'Optical Flow Estimation', 'Object Detection Results', 'Entry Gate', 'Loss Of Intensity', 'Input Frames', 'Inference Stage', 'Skip Connections', 'Temporal Axis']",,253,"Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-of-the-art methods."
Approximated Bilinear Modules for Temporal Modeling,"Xinqi Zhu, Chang Xu, Langwen Hui, Cewu Lu, Dacheng Tao","UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia; Shanghai Jiao Tong University, Shanghai, China",100.0,"China, australia",0.0,,"We consider two less-emphasized temporal properties of video: 1. Temporal cues are fine-grained; 2. Temporal modeling needs reasoning. To tackle both problems at once, we exploit approximated bilinear modules (ABMs) for temporal modeling. There are two main points making the modules effective: two-layer MLPs can be seen as a constraint approximation of bilinear operations, thus can be used to construct deep ABMs in existing CNNs while reusing pretrained parameters; frame features can be divided into static and dynamic parts because of visual repetition in adjacent frames, which enables temporal modeling to be more efficient. Multiple ABM variants and implementations are investigated, from high performance to high efficiency. Specifically, we show how two-layer subnets in CNNs can be converted to temporal bilinear modules by adding an auxiliary-branch. Besides, we introduce snippet sampling and shifting inference to boost sparse-frame video classification performance. Extensive ablation studies are conducted to show the effectiveness of proposed techniques. Our models can outperform most state-of-the-art methods on Something-Something v1 and v2 datasets without Kinetics pretraining, and are also competitive on other YouTube-like action recognition datasets. Our code is available on https://github.com/zhuxinqimac/abm-pytorch.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Approximated_Bilinear_Modules_for_Temporal_Modeling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Approximated_Bilinear_Modules_for_Temporal_Modeling_ICCV_2019_paper.pdf,,https://github.com/zhuxinqimac/abm-pytorch,,main,Poster,https://ieeexplore.ieee.org/document/9010406/,"['Computational modeling', 'Computer architecture', 'Three-dimensional displays', 'Cognition', 'Visualization', 'Feature extraction', 'Image recognition']","['Temporal Model', 'Action Recognition', 'Dynamic Part', 'Bilinear Map', 'Adjacent Frames', 'Temporal Cues', 'Frame Features', 'Static Part', 'Pre-trained Parameters', 'V2 Dataset', 'Model Performance', 'Validation Set', 'Convolutional Layers', 'Image Classification', 'Deep Convolutional Neural Network', 'Temporal Information', 'Deep Architecture', 'Single Frame', 'Optical Flow', 'Video Action Recognition', 'Bilinear Model', '3D Architecture', 'Top Model', 'Input RGB', 'Input Frames', 'Inference Speed', 'Sparse Sampling']",,20,"We consider two less-emphasized temporal properties of video: 1. Temporal cues are fine-grained; 2. Temporal modeling needs reasoning. To tackle both problems at once, we exploit approximated bilinear modules (ABMs) for temporal modeling. There are two main points making the modules effective: two-layer MLPs can be seen as a constraint approximation of bilinear operations, thus can be used to construct deep ABMs in existing CNNs while reusing pretrained parameters; frame features can be divided into static and dynamic parts because of visual repetition in adjacent frames, which enables temporal modeling to be more efficient. Multiple ABM variants and implementations are investigated, from high performance to high efficiency. Specifically, we show how two-layer subnets in CNNs can be converted to temporal bilinear modules by adding an auxiliary-branch. Besides, we introduce snippet sampling and shifting inference to boost sparse-frame video classification performance. Extensive ablation studies are conducted to show the effectiveness of proposed techniques. Our models can outperform most state-of-the-art methods on Something-Something v1 and v2 datasets without Kinetics pretraining, and are also competitive on other YouTube-like action recognition datasets. Our code is available on https://github.com/zhuxinqimac/abm-pytorch."
Asymmetric Cross-Guided Attention Network for Actor and Action Video Segmentation From Natural Language Query,"Hao Wang, Cheng Deng, Junchi Yan, Dacheng Tao","School of Electronic Engineering, Xidian University, Xi'an 710071, China; School of Electronic Engineering, Xidian University, Xi'an 710071, China; Tencent AI Lab, Shenzhen, China; Department of CSE, and MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University; UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Australia",80.0,"China, australia",20.0,China,"Actor and action video segmentation from natural language query aims to selectively segment the actor and its action in a video based on an input textual description. Previous works mostly focus on learning simple correlation between two heterogeneous features of vision and language via dynamic convolution or fully convolutional classification. However, they ignore the linguistic variation of natural language query and have difficulty in modeling global visual context, which leads to unsatisfactory segmentation performance. To address these issues, we propose an asymmetric cross-guided attention network for actor and action video segmentation from natural language query. Specifically, we frame an asymmetric cross-guided attention network, which consists of vision guided language attention to reduce the linguistic variation of input query and language guided vision attention to incorporate query-focused global visual context simultaneously. Moreover, we adopt multi-resolution fusion scheme and weighted loss for foreground and background pixels to obtain further performance improvement. Extensive experiments on Actor-Action Dataset Sentences and J-HMDB Sentences show that our proposed approach notably outperforms state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Asymmetric_Cross-Guided_Attention_Network_for_Actor_and_Action_Video_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Asymmetric_Cross-Guided_Attention_Network_for_Actor_and_Action_Video_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008106/,"['Feature extraction', 'Visualization', 'Linguistics', 'Convolution', 'Dogs', 'Task analysis']","['Natural Language', 'Asymmetric Network', 'Natural Language Query', 'Global Context', 'Heterogeneous Characteristics', 'Textual Descriptions', 'Linguistic Diversity', 'Segmentation Performance', 'Unsatisfactory Performance', 'Visual Context', 'Foreground Pixels', 'Convolutional Neural Network', 'Feature Maps', 'Spatial Features', 'Visual Features', 'Attention Mechanism', 'Video Clips', 'Semantic Segmentation', 'Video Analysis', 'Attention Module', 'Visual Question Answering', 'Video Features', 'Query Features', 'Video Encoding', 'Word Embedding', 'Text Encoder', 'Mean Intersection Over Union', 'Segmentation Results', 'Decoder Features', 'Language Mode']",,40,"Actor and action video segmentation from natural language query aims to selectively segment the actor and its action in a video based on an input textual description. Previous works mostly focus on learning simple correlation between two heterogeneous features of vision and language via dynamic convolution or fully convolutional classification. However, they ignore the linguistic variation of natural language query and have difficulty in modeling global visual context, which leads to unsatisfactory segmentation performance. To address these issues, we propose an asymmetric cross-guided attention network for actor and action video segmentation from natural language query. Specifically, we frame an asymmetric cross-guided attention network, which consists of vision guided language attention to reduce the linguistic variation of input query and language guided vision attention to incorporate query-focused global visual context simultaneously. Moreover, we adopt multi-resolution fusion scheme and weighted loss for foreground and background pixels to obtain further performance improvement. Extensive experiments on Actor-Action Dataset Sentences and J-HMDB Sentences show that our proposed approach notably outperforms state-of-the-art methods."
Asymmetric Non-Local Neural Networks for Semantic Segmentation,"Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, Xiang Bai",Huazhong University of Science and Technology; University of Oxford,100.0,"china, uk",0.0,,"The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf,,https://github.com/MendelXu/ANN.git,,main,Poster,https://ieeexplore.ieee.org/document/9008832/,"['Computer vision', 'Neural networks', 'Semantics']","['Neural Network', 'Semantic Segmentation', 'Asymmetric Network', 'Considerable Improvement', 'GPU Memory', 'Long-range Dependencies', 'Prominent Component', 'Different Levels Of Features', 'Non-local Block', 'Training Set', 'Deep Network', 'Deep Neural Network', 'Feature Maps', 'Intersection Over Union', 'Receptive Field', 'Pooling Layer', 'Matrix Multiplication', 'Similarity Matrix', 'Backbone Network', 'Validation Images', 'Spatial Pyramid Pooling', 'Low-level Feature Maps', 'Standard Block', 'Conditional Random Field', 'High-level Feature Maps', 'Pyramid Pooling', 'Scene Parsing', 'Output Size', 'Key Branch', 'Spatial Pyramid Pooling Module']",,466,"The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git."
Asynchronous Single-Photon 3D Imaging,"Anant Gupta, Atul Ingle, Mohit Gupta",University of Wisconsin-Madison,100.0,usa,0.0,,"Single-photon avalanche diodes (SPADs) are becoming popular in time-of-flight depth-ranging due to their unique ability to capture individual photons with picosecond timing resolution. However, ambient light (e.g., sunlight) incident on a SPAD-based 3D camera leads to severe non-linear distortions (pileup) in the measured waveform, resulting in large depth errors. We propose asynchronous single-photon 3D imaging, a family of acquisition schemes to mitigate pileup during data acquisition itself. Asynchronous acquisition temporally misaligns SPAD measurement windows and the laser cycles through deterministically predefined or randomized offsets. Our key insight is that pileup distortions can be ""averaged out"" by choosing a sequence of offsets that span the entire depth range. We develop a generalized image formation model and perform theoretical analysis to explore the space of asynchronous acquisition schemes and design high-performance schemes. Our simulations and experiments demonstrate an improvement in depth accuracy of up to an order of magnitude as compared to the state-of-the-art, across a wide range of imaging scenarios, including those with high ambient flux.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_Asynchronous_Single-Photon_3D_Imaging_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_Asynchronous_Single-Photon_3D_Imaging_ICCV_2019_paper.pdf,www.SinglePhoton3DImaging.com,,,main,Oral,https://ieeexplore.ieee.org/document/9009520/,"['Photonics', 'Cameras', 'Three-dimensional displays', 'Histograms', 'Laser modes']","['High Flux', 'Ambient Light', 'Depth Range', 'Depth Camera', 'Acquisition Strategies', 'Accurate Depth', 'Depth Error', 'Single-photon Avalanche Diode', 'Activity Time', 'Laser Pulse', 'Photon Flux', 'Dead Time', 'Time Bins', 'Depth Estimation', 'Wide Range Of Levels', 'Time-correlated Single-photon Counting', 'Photon Detection', 'Start Of Cycle', 'Histogram Bins', 'Incident Flux', 'Signal Photons', 'Optimal Flux', 'Conventional Camera', 'Scene Depth', 'Sequential Shift', 'Incident Photon Flux', 'Conventional Acquisition', 'Scene Point', 'High Light', 'Maximum Likelihood Estimation']",,50,"Single-photon avalanche diodes (SPADs) are becoming popular in time-of-flight depth-ranging due to their unique ability to capture individual photons with picosecond timing resolution. However, ambient light (e.g., sunlight) incident on a SPAD-based 3D camera leads to severe non-linear distortions (pileup) in the measured waveform, resulting in large depth errors. We propose asynchronous single-photon 3D imaging, a family of acquisition schemes to mitigate pileup during data acquisition itself. Asynchronous acquisition temporally misaligns SPAD measurement windows and the laser cycles through deterministically predefined or randomized offsets. Our key insight is that pileup distortions can be “averaged out” by choosing a sequence of offsets that span the entire depth range. We develop a generalized image formation model and perform theoretical analysis to explore the space of asynchronous acquisition schemes and design high-performance schemes. Our simulations and experiments demonstrate an improvement in depth accuracy of up to an order of magnitude as compared to the state-of-the-art, across a wide range of imaging scenarios, including those with high ambient flux."
AttPool: Towards Hierarchical Feature Representation in Graph Convolutional Networks via Attention Mechanism,"Jingjia Huang, Zhangheng Li, Nannan Li, Shan Liu, Ge Li","School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; Tencent Media Lab, Palo Alto, CA94301, USA",66.66666666666666,china,33.33333333333334,China,"Graph convolutional networks (GCNs) are potentially short of the ability to learn hierarchical representation for graph embedding, which holds them back in the graph classification task. Here, we propose AttPool, which is a novel graph pooling module based on attention mechanism, to remedy the problem. It is able to select nodes that are significant for graph representation adaptively, and generate hierarchical features via aggregating the attention-weighted information in nodes. Additionally, we devise a hierarchical prediction architecture to sufficiently leverage the hierarchical representation and facilitate the model learning. The AttPool module together with the entire training structure can be integrated into existing GCNs, and is trained in an end-to-end fashion conveniently. The experimental results on several graph-classification benchmark datasets with various scales demonstrate the effectiveness of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_AttPool_Towards_Hierarchical_Feature_Representation_in_Graph_Convolutional_Networks_via_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_AttPool_Towards_Hierarchical_Feature_Representation_in_Graph_Convolutional_Networks_via_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009471/,"['Task analysis', 'Convolution', 'Training', 'Benchmark testing', 'Neural networks', 'Symmetric matrices', 'Adaptation models']","['Attention Mechanism', 'Graph Convolutional Network', 'Hierarchical Features', 'Graph Convolution', 'Hierarchical Representation', 'Hierarchical Feature Representation', 'Graphical Representation', 'Hierarchical Architecture', 'Graph Classification', 'Convolutional Neural Network', 'Hierarchical Structure', 'Large-scale Datasets', 'Pooling Layer', 'Representation Learning', 'Nodes In The Graph', 'Global Attention', 'Graph Neural Networks', 'Information Aggregation', 'Node Representations', 'Link Prediction', 'Local Attention', 'Graph-structured Data', 'Part Of The Graph', 'Graph Convolutional Network Model', 'Node Embeddings', 'Pooling Mechanism', 'Hierarchical Graph', 'Small-scale Datasets', 'Current Graph', 'Graph Structure']",,40,"Graph convolutional networks (GCNs) are potentially short of the ability to learn hierarchical representation for graph embedding, which holds them back in the graph classification task. Here, we propose AttPool, which is a novel graph pooling module based on attention mechanism, to remedy the problem. It is able to select nodes that are significant for graph representation adaptively, and generate hierarchical features via aggregating the attention-weighted information in nodes. Additionally, we devise a hierarchical prediction architecture to sufficiently leverage the hierarchical representation and facilitate the model learning. The AttPool module together with the entire training structure can be integrated into existing GCNs, and is trained in an end-to-end fashion conveniently. The experimental results on several graph-classification benchmark datasets with various scales demonstrate the effectiveness of our method."
Attacking Optical Flow,"Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J. Black",Max Planck Institute for Intelligent Systems,100.0,Germany,0.0,,"Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ranjan_Attacking_Optical_Flow_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ranjan_Attacking_Optical_Flow_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010809/,"['Optical imaging', 'Adaptive optics', 'Optical fiber networks', 'Optical computing', 'Optical variables control', 'Robustness', 'Biomedical optical imaging']","['Optical Flow', 'Neural Network', 'Deep Network', 'Deep Neural Network', 'Feature Maps', 'Image Size', 'Small Patches', 'Robust Network', 'Self-driving', 'Network Flow', 'Objects In The Scene', 'Adversarial Attacks', 'Spatial Pyramid', 'Optical Networks', 'Optical Flow Estimation', 'Image Area', 'Unsupervised Methods', 'Real-world Scenarios', 'Scale Changes', 'Patch Size', 'Adversarial Examples', 'White-box Attack', 'Black-box Attacks', 'Optical Flow Method', 'Camera Motion', 'Patch Region', 'Ground Truth Labels', 'Local Image', 'Image Pyramid', 'Patch Area']",,46,"Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes."
Attention Augmented Convolutional Networks,"Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V. Le",Google Brain,0.0,,100.0,USA,"Convolutional networks have enjoyed much success in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighbourhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we propose to augment convolutional networks with self-attention by concatenating convolutional feature maps with a set of feature maps produced via a novel relative self-attention mechanism. In particular, we extend previous work on relative self-attention over sequences to images and discuss a memory efficient implementation. Unlike Squeeze-and-Excitation, which performs attention over the channels and ignores spatial information, our self-attention mechanism attends jointly to both features and spatial locations while preserving translation equivariance. We find that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 AP in COCO Object Detection on top of a RetinaNet baseline.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010285/,"['Convolution', 'Convolutional codes', 'Task analysis', 'Head', 'Computer architecture', 'Object detection', 'Encoding']","['Convolutional Network', 'Computer Vision', 'Image Classification', 'Feature Maps', 'Object Detection', 'Related Mechanisms', 'Attention Mechanism', 'Visual Task', 'Convolution Operation', 'Discrimination Task', 'Self-attention Mechanism', 'Computer Vision Applications', 'Convolutional Features', 'ImageNet Classification', 'Convolutional Feature Maps', 'Convolutional Neural Network', 'Spatial Dimensions', 'Residual Block', 'Input Dimension', 'Number Of Filters', 'Positional Encoding', 'Featurization', 'Image Classification Tasks', 'Input Filter', 'COCO Dataset', 'Relative Width', 'Fraction Of Channels', 'Pointwise Convolution', 'Two-dimensional Position', 'Output Filter']",,622,"Convolutional networks have enjoyed much success in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighbourhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we propose to augment convolutional networks with self-attention by concatenating convolutional feature maps with a set of feature maps produced via a novel relative self-attention mechanism. In particular, we extend previous work on relative self-attention over sequences to images and discuss a memory efficient implementation. Unlike Squeeze-and-Excitation, which performs attention over the channels and ignores spatial information, our self-attention mechanism attends jointly to both features and spatial locations while preserving translation equivariance. We find that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 AP in COCO Object Detection on top of a RetinaNet baseline."
Attention Bridging Network for Knowledge Transfer,"Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, Yun Fu","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA; Khoury College of Computer Science, Northeastern University, Boston, MA",100.0,china,0.0,,"The attention of a deep neural network obtained by back-propagating gradients can effectively explain the decision of the network. They can further be used to explicitly access to the network response to a specific pattern. Considering objects of the same category but from different domains share similar visual patterns, we propose to treat the network attention as a bridge to connect objects across domains. In this paper, we use knowledge from the source domain to guide the network's response to categories shared with the target domain. With weights sharing and domain adversary training, this knowledge can be successfully transferred by regularizing the network's response to the same category in the target domain. Specifically, we transfer the foreground prior from a simple single-label dataset to another complex multi-label dataset, leading to improvement of attention maps. Experiments about the weakly-supervised semantic segmentation task show the effectiveness of our method. Besides, we further explore and validate that the proposed method is able to improve the generalization ability of a classification network in domain adaptation and domain generalization settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008571/,"['Visualization', 'Task analysis', 'Knowledge engineering', 'Semantics', 'Training', 'Bridges', 'Knowledge transfer']","['Knowledge Transfer', 'Deep Neural Network', 'Classification Network', 'Semantic Segmentation', 'Target Domain', 'Visual Patterns', 'Domain Adaptation', 'Ability Of The Network', 'Source Domain', 'Domain Generalization', 'Attention Map', 'Adversarial Training', 'Convolutional Neural Network', 'Performance Of Method', 'Convolutional Layers', 'Feature Maps', 'Attention Mechanism', 'Visual Attention', 'Segmentation Results', 'Saliency Map', 'Classification Loss', 'Local Cues', 'Source Domain Data', 'Semantic Segmentation Results', 'Different Amounts Of Data', 'Joint Training', 'Object Regions', 'Bridging Effect', 'Task Of Interest']",,14,"The attention of a deep neural network obtained by back-propagating gradients can effectively explain the decision of the network. They can further be used to explicitly access to the network response to a specific pattern. Considering objects of the same category but from different domains share similar visual patterns, we propose to treat the network attention as a bridge to connect objects across domains. In this paper, we use knowledge from the source domain to guide the network's response to categories shared with the target domain. With weights sharing and domain adversary training, this knowledge can be successfully transferred by regularizing the network's response to the same category in the target domain. Specifically, we transfer the foreground prior from a simple single-label dataset to another complex multi-label dataset, leading to improvement of attention maps. Experiments about the weakly-supervised semantic segmentation task show the effectiveness of our method. Besides, we further explore and validate that the proposed method is able to improve the generalization ability of a classification network in domain adaptation and domain generalization settings."
Attention on Attention for Image Captioning,"Lun Huang, Wenmin Wang, Jie Chen, Xiao-Yong Wei","School of Electronic and Computer Engineering, Peking University; Peng Cheng Laboratory; School of Electronic and Computer Engineering, Peking University; Macau University of Science and Technology",100.0,china,0.0,,"Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf,,https://github.com/husthuaan/AoANet,,main,Oral,https://ieeexplore.ieee.org/document/9008770,"['Decoding', 'Logic gates', 'Feature extraction', 'Visualization', 'Task analysis', 'Computer vision', 'Testing']","['Image Captioning', 'Time Step', 'Attention Mechanism', 'Current Context', 'Element-wise Multiplication', 'Conventional Mechanism', 'Attention Gate', 'Decoding', 'Cross-entropy Loss', 'Image Object', 'Dimensional Vector', 'Linear Transformation', 'Visual Attention', 'Attention Module', 'Hidden State', 'Sequential Manner', 'Extracted Feature Vectors', 'MS COCO Dataset', 'Log Loss', 'Feed-forward Layer', 'Sequence Learning Task']",,582,"Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet."
Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval,"Xingxu Yao, Dongyu She, Sicheng Zhao, Jie Liang, Yu-Kun Lai, Jufeng Yang","Cardiff University; University of California, Berkeley; Nankai University",100.0,"China, uk, usa",0.0,,"Images play a crucial role for people to express their opinions online due to the increasing popularity of social networks. While an affective image retrieval system is useful for obtaining visual contents with desired emotions from a massive repository, the abstract and subjective characteristics make the task challenging. To address the problem, this paper introduces an Attention-aware Polarity Sensitive Embedding (APSE) network to learn affective representations in an end-to-end manner. First, to automatically discover and model the informative regions of interest, we develop a hierarchical attention mechanism, in which both polarity- and emotion-specific attended representations are aggregated for discriminative feature embedding. Second, we present a weighted emotion-pair loss to take the inter- and intra-polarity relationships of the emotional labels into consideration. Guided by attention module, we weight the sample pairs adaptively which further improves the performance of feature embedding. Extensive experiments on four popular benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yao_Attention-Aware_Polarity_Sensitive_Embedding_for_Affective_Image_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Attention-Aware_Polarity_Sensitive_Embedding_for_Affective_Image_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008797/,"['Image retrieval', 'Head', 'Visualization', 'Task analysis', 'Semantics', 'Feature extraction', 'Psychology']","['Image Retrieval', 'Affective Image', 'Extensive Experiments', 'Attention Mechanism', 'Attention Module', 'Popular Datasets', 'Network Embedding', 'Popular Social Networking', 'Deep Learning', 'Lower Layer', 'Confidence Score', 'High-level Features', 'Low-level Features', 'Higher Layers', 'Emotion Categories', 'Attention Map', 'Embedding Learning', 'Examples Of Categories', 'International Affective Picture System', 'Triplet Loss', 'Sentiment Polarity', 'Query Image', 'Co-occurrence Features', 'Retrieval Results', 'Attention Loss', 'Content-based Image Retrieval', 'Different Levels Of Features', 'Softmax Loss', 'Opposite Polarity', 'Latent Space']",,23,"Images play a crucial role for people to express their opinions online due to the increasing popularity of social networks. While an affective image retrieval system is useful for obtaining visual contents with desired emotions from a massive repository, the abstract and subjective characteristics make the task challenging. To address the problem, this paper introduces an Attention-aware Polarity Sensitive Embedding (APSE) network to learn affective representations in an end-to-end manner. First, to automatically discover and model the informative regions of interest, we develop a hierarchical attention mechanism, in which both polarity- and emotion-specific attended representations are aggregated for discriminative feature embedding. Second, we present a weighted emotion-pair loss to take the inter- and intra-polarity relationships of the emotional labels into consideration. Guided by attention module, we weight the sample pairs adaptively which further improves the performance of feature embedding. Extensive experiments on four popular benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches."
Attention-Based Autism Spectrum Disorder Screening With Privileged Modality,"Shi Chen, Qi Zhao","Department of Computer Science and Engineering, University of Minnesota",100.0,usa,0.0,,"This paper presents a novel framework for automatic and quantitative screening of autism spectrum disorder (ASD). It is motivated to address two issues in the current clinical settings: 1) short of clinical resources with the prevalence of ASD (1.7% in the United States), and 2) subjectivity of ASD screening. This work differentiates itself with three unique features: first, it proposes an ASD screening with privileged modality framework that integrates information from two behavioral modalities during training and improves the performance on each single modality at testing. The proposed framework does not require overlap in subjects between the modalities. Second, it develops the first computational model to classify people with ASD using a photo-taking task where subjects freely explore their environment in a more ecological setting. Photo-taking reveals attentional preference of subjects, differentiating people with ASD from healthy people, and is also easy to implement in real-world clinical settings without requiring advanced diagnostic instruments. Third, this study for the first time takes advantage of the temporal information in eye movements while viewing images, encoding more detailed behavioral differences between ASD people and healthy controls. Experiments show that our ASD screening models can achieve superior performance, outperforming the previous state-of-the-art methods by a considerable margin. Moreover, our framework using diverse modalities demonstrates performance improvement on both the photo-taking and image-viewing tasks, providing a general paradigm that takes in multiple sources of behavioral data for a more accurate ASD screening. The framework is also applicable to various scenarios where one-to-one pairwise relationship is difficult to obtain across different modalities.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Attention-Based_Autism_Spectrum_Disorder_Screening_With_Privileged_Modality_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Attention-Based_Autism_Spectrum_Disorder_Screening_With_Privileged_Modality_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010066/,"['Task analysis', 'Variable speed drives', 'Visualization', 'Computational modeling', 'Training', 'Testing', 'Biological system modeling']","['Autism Spectrum Disorder Screening', 'Subjectivity', 'Eye Movements', 'Temporal Information', 'Behavior Modification', 'Attentional Orienting', 'Screening Model', 'Pairwise Relationships', 'Accurate Screening', 'Real-world Clinical Setting', 'Considerable Margin', 'Visual Features', 'Long Short-term Memory', 'Distinct Modes', 'Visual Attention', 'Scarcity Of Data', 'Hidden State', 'Abundant Information', 'Deep Neural Network Model', 'Shared Space', 'Multimodal Methods', 'Sequence Of Photographs', 'Visual Fixation', 'Movement Information', 'Privileged Information', 'Label Consistency', 'Feature Alignment', 'Joint Training', 'Independent Training', 'Non-human Objects']",,36,"This paper presents a novel framework for automatic and quantitative screening of autism spectrum disorder (ASD). It is motivated to address two issues in the current clinical settings: 1) short of clinical resources with the prevalence of ASD (1.7% in the United States), and 2) subjectivity of ASD screening. This work differentiates itself with three unique features: first, it proposes an ASD screening with privileged modality framework that integrates information from two behavioral modalities during training and improves the performance on each single modality at testing. The proposed framework does not require overlap in subjects between the modalities. Second, it develops the first computational model to classify people with ASD using a photo-taking task where subjects freely explore their environment in a more ecological setting. Photo-taking reveals attentional preference of subjects, differentiating people with ASD from healthy people, and is also easy to implement in real-world clinical settings without requiring advanced diagnostic instruments. Third, this study for the first time takes advantage of the temporal information in eye movements while viewing images, encoding more detailed behavioral differences between ASD people and healthy controls. Experiments show that our ASD screening models can achieve superior performance, outperforming the previous state-of-the-art methods by a considerable margin. Moreover, our framework using diverse modalities demonstrates performance improvement on both the photo-taking and image-viewing tasks, providing a general paradigm that takes in multiple sources of behavioral data for a more accurate ASD screening. The framework is also applicable to various scenarios where one-to-one pairwise relationship is difficult to obtain across different modalities."
AttentionRNN: A Structured Spatial Attention Mechanism,"Siddhesh Khandelwal, Leonid Sigal","University of British Columbia, Vector Institute for AI; University of British Columbia, Vector Institute for AI, Canada CIFAR AI Chair",100.0,canada,0.0,,"Visual attention mechanisms have proven to be integrally important constituent components of many modern deep neural architectures. They provide an efficient and effective way to utilize visual information selectively, which has shown to be especially valuable in multi-modal learning tasks. However, all prior attention frameworks lack the ability to explicitly model structural dependencies among attention variables, making it difficult to predict consistent attention masks. In this paper we develop a novel structured spatial attention mechanism which is end-to-end trainable and can be integrated with any feed-forward convolutional neural network. This proposed AttentionRNN layer explicitly enforces structure over the spatial attention variables by sequentially predicting attention values in the spatial mask in a bi-directional raster-scan and inverse raster-scan order. As a result, each attention value depends not only on local image or contextual information, but also on the previously predicted attention values. Our experiments show consistent quantitative and qualitative improvements on a variety of recognition tasks and datasets; including image categorization, question answering and image generation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Khandelwal_AttentionRNN_A_Structured_Spatial_Attention_Mechanism_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Khandelwal_AttentionRNN_A_Structured_Spatial_Attention_Mechanism_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008506/,"['Visualization', 'Task analysis', 'Computer architecture', 'Image generation', 'Computer vision', 'Predictive models', 'Computational modeling']","['Attention Mechanism', 'Spatial Attention', 'Neural Network', 'Convolutional Neural Network', 'Image Classification', 'Variety Of Tasks', 'Visual Attention', 'Feed-forward Network', 'Image Generation', 'Local Image', 'Qualitative Improvement', 'Attention Values', 'Visual Attention Mechanism', 'Image Features', 'Validation Set', 'Convolutional Layers', 'Local Context', 'Computer Vision', 'Work In This Area', 'Feature Maps', 'Visual Question Answering', 'Hidden State', 'Hair Color', 'Local Mechanisms', 'Convolution Kernel', 'Blue Region', 'Sharp Discontinuity', 'Target Region', 'Pooling Layer', 'Attention Layer']",,6,"Visual attention mechanisms have proven to be integrally important constituent components of many modern deep neural architectures. They provide an efficient and effective way to utilize visual information selectively, which has shown to be especially valuable in multi-modal learning tasks. However, all prior attention frameworks lack the ability to explicitly model structural dependencies among attention variables, making it difficult to predict consistent attention masks. In this paper we develop a novel structured spatial attention mechanism which is end-to-end trainable and can be integrated with any feed-forward convolutional neural network. This proposed AttentionRNN layer explicitly enforces structure over the spatial attention variables by sequentially predicting attention values in the spatial mask in a bi-directional raster-scan and inverse raster-scan order. As a result, each attention value depends not only on local image or contextual information, but also on the previously predicted attention values. Our experiments show consistent quantitative and qualitative improvements on a variety of recognition tasks and datasets; including image categorization, question answering and image generation."
Attentional Feature-Pair Relation Networks for Accurate Face Recognition,"Bong-Nam Kang, Yonghyun Kim, Bongjin Jun, Daijin Kim","POSTECH; StradVision, Inc.; Kakao Corp.",33.33333333333333,south korea,66.66666666666667,USA,"Human face recognition is one of the most important research areas in biometrics. However, the robust face recognition under a drastic change of the facial pose, expression, and illumination is a big challenging problem for its practical application. Such variations make face recognition more difficult. In this paper, we propose a novel face recognition method, called Attentional Feature-pair Relation Network (AFRN), which represents the face by the relevant pairs of local appearance block features with their attention scores. The AFRN represents the face by all possible pairs of the 9x9 local appearance block features, the importance of each pair is considered by the attention map that is obtained from the low-rank bilinear pooling, and each pair is weighted by its corresponding attention score. To increase the accuracy, we select top-K pairs of local appearance block features as relevant facial information and drop the remaining irrelevant. The weighted top-K pairs are propagated to extract the joint feature-pair relation by using bilinear attention network. In experiments, we show the effectiveness of the proposed AFRN and achieve the outstanding performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Attentional_Feature-Pair_Relation_Networks_for_Accurate_Face_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kang_Attentional_Feature-Pair_Relation_Networks_for_Accurate_Face_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008238/,"['Face', 'Feature extraction', 'Face recognition', 'Facial features', 'Convolution', 'Encoding', 'Benchmark testing']","['Face Recognition', 'Attention Network', 'Local Features', 'Biometric', 'Important Area Of Research', 'Attention Map', 'Feature Pairs', 'Face Identity', 'Attention Scores', 'Important Pairs', 'Neural Network', 'Training Set', 'Validation Set', 'Feature Maps', 'Attention Mechanism', 'Facial Features', 'Face Images', 'Attention Allocation', 'Appearance Features', 'Video Capture', 'False Acceptance Rate', 'Verification Task', 'Unconstrained Environment', 'Selection Of Pairs', 'Open Task', 'Facial Points', 'Bilinear Map', 'Part Features', 'Face Parts', 'Selective Layer']",,34,"Human face recognition is one of the most important research areas in biometrics. However, the robust face recognition under a drastic change of the facial pose, expression, and illumination is a big challenging problem for its practical application. Such variations make face recognition more difficult. In this paper, we propose a novel face recognition method, called Attentional Feature-pair Relation Network (AFRN), which represents the face by the relevant pairs of local appearance block features with their attention scores. The AFRN represents the face by all possible pairs of the 9x9 local appearance block features, the importance of each pair is considered by the attention map that is obtained from the low-rank bilinear pooling, and each pair is weighted by its corresponding attention score. To increase the accuracy, we select top-K pairs of local appearance block features as relevant facial information and drop the remaining irrelevant. The weighted top-K pairs are propagated to extract the joint feature-pair relation by using bilinear attention network. In experiments, we show the effectiveness of the proposed AFRN and achieve the outstanding performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C datasets."
Attentional Neural Fields for Crowd Counting,"Anran Zhang, Lei Yue, Jiayi Shen, Fan Zhu, Xiantong Zhen, Xianbin Cao, Ling Shao","School of Electronic and Information Engineering, Beihang University, Beijing, China; Key Laboratory of Advanced Technology of Near Space Information System (Beihang University), Ministry of Industry and Information Technology of China, Beijing, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beijing, China; Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE; School of Electronic and Information Engineering, Beihang University, Beijing, China",80.0,"china, uae",20.0,China,"Crowd counting has recently generated huge popularity in computer vision, and is extremely challenging due to the huge scale variations of objects. In this paper, we propose the Attentional Neural Field (ANF) for crowd counting via density estimation. Within the encoder-decoder network, we introduce conditional random fields (CRFs) to aggregate multi-scale features, which can build more informative representations. To better model pair-wise potentials in CRFs, we incorperate non-local attention mechanism implemented as inter- and intra-layer attentions to expand the receptive field to the entire image respectively within the same layer and across different layers, which captures long-range dependencies to conquer huge scale variations. The CRFs coupled with the attention mechanism are seamlessly integrated into the encoder-decoder network, establishing an ANF that can be optimized end-to-end by back propagation. We conduct extensive experiments on four public datasets, including ShanghaiTech, WorldEXPO 10, UCF-CC-50 and UCF-QNRF. The results show that our ANF achieves high counting performance, surpassing most previous methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Attentional_Neural_Fields_for_Crowd_Counting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Attentional_Neural_Fields_for_Crowd_Counting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009565/,"['Estimation', 'Task analysis', 'Feature extraction', 'Computational modeling', 'Adaptation models', 'Aggregates', 'Machine learning']","['Crowd Counting', 'Random Fields', 'Public Datasets', 'Density Estimation', 'Attention Mechanism', 'Scale Variation', 'Receptive Field', 'Huge Variety', 'Multi-scale Features', 'Conditional Random Field', 'Long-range Dependencies', 'Encoder-decoder Network', 'International Exhibition', 'Pairwise Potential', 'Convolutional Network', 'Convolutional Neural Network', 'Gaussian Kernel', 'Convolutional Layers', 'Feature Maps', 'Object Detection', 'Density Map', 'Message Passing', 'Latent Features', 'Matrix Multiplication Operation', 'Images Of People', 'Non-local Operation', 'Crowd Density', 'Hidden Variables', 'Attention Model', 'Depth Estimation']",,80,"Crowd counting has recently generated huge popularity in computer vision, and is extremely challenging due to the huge scale variations of objects. In this paper, we propose the Attentional Neural Field (ANF) for crowd counting via density estimation. Within the encoder-decoder network, we introduce conditional random fields (CRFs) to aggregate multi-scale features, which can build more informative representations. To better model pair-wise potentials in CRFs, we incorperate non-local attention mechanism implemented as inter- and intra-layer attentions to expand the receptive field to the entire image respectively within the same layer and across different layers, which captures long-range dependencies to conquer huge scale variations. The CRFs coupled with the attention mechanism are seamlessly integrated into the encoder-decoder network, establishing an ANF that can be optimized end-to-end by back propagation. We conduct extensive experiments on four public datasets, including ShanghaiTech, WorldEXPO 10, UCF-CC-50 and UCF-QNRF. The results show that our ANF achieves high counting performance, surpassing most previous methods."
Attract or Distract: Exploit the Margin of Open Set,"Qianyu Feng, Guoliang Kang, Hehe Fan, Yi Yang","Baidu Research; ReLER, University of Technology Sydney; School of Computer Science, Carnegie Mellon University",66.66666666666666,"australia, usa",33.33333333333334,China,"Open set domain adaptation aims to diminish the domain shift across domains, with partially shared classes. There exist unknown target samples out of the knowledge of source domain. Compared to the close set setting, how to separate the unknown (unshared) class from the known (shared) ones plays the key role. Whereas, previous methods did not emphasize the semantic structure of the open set data, which may introduce bias into the domain alignment and confuse the classifier around the decision boundary. In this paper, we exploit the semantic structure of open set data from two aspects: 1) Semantic Categorical Alignment, which aims to achieve good separability of target known classes by categorically aligning the centroid of target with the source. 2) Semantic Contrastive Mapping, which aims to push the unknown class away from the decision boundary. Empirically, we demonstrate that our method performs favourably against the state-of-the-art methods on representative benchmarks, e.g. Digits and Office-31 datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Feng_Attract_or_Distract_Exploit_the_Margin_of_Open_Set_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_Attract_or_Distract_Exploit_the_Margin_of_Open_Set_ICCV_2019_paper.pdf,,https://github.com/qy-feng/margin-openset.git,,main,Poster,https://ieeexplore.ieee.org/document/9010858/,"['Semantics', 'Task analysis', 'Data models', 'Training', 'Generators', 'Computer vision', 'Benchmark testing']","['Open Set', 'Fluidic', 'Open Data', 'Target Sample', 'Domain Shift', 'Unknown Samples', 'Decision Boundary', 'Domain Adaptation', 'Closed Set', 'Source Domain', 'Semantic Map', 'Digital Dataset', 'Cross-entropy Loss', 'Generative Adversarial Networks', 'Target Domain', 'AlexNet', 'Reliability In Sample', 'Contrastive Loss', 'Open Domain', 'Pseudo Labels', 'Adversarial Domain Adaptation', 'Maximum Mean Discrepancy', 'Target Domain Data', 'Discriminative Representations']",,38,"Open set domain adaptation aims to diminish the domain shift across domains, with partially shared classes. There exist unknown target samples out of the knowledge of source domain. Compared to the close set setting, how to separate the unknown (unshared) class from the known (shared) ones plays the key role. Whereas, previous methods did not emphasize the semantic structure of the open set data, which may introduce bias into the domain alignment and confuse the classifier around the decision boundary. In this paper, we exploit the semantic structure of open set data from two aspects: 1) Semantic Categorical Alignment, which aims to achieve good separability of target known classes by categorically aligning the centroid of target with the source. 2) Semantic Contrastive Mapping, which aims to push the unknown class away from the decision boundary. Empirically, we demonstrate that our method performs favourably against the state-of-the-art methods on representative benchmarks, e.g. Digits and Office-31 datasets."
Attribute Attention for Semantic Disambiguation in Zero-Shot Learning,"Yang Liu, Jishun Guo, Deng Cai, Xiaofei He","GAC R&D Center, Guangzhou, China; Fabu Inc., Hangzhou, China; State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, China",33.33333333333333,China,66.66666666666667,China,"Zero-shot learning (ZSL) aims to accurately recognize unseen objects by learning mapping matrices that bridge the gap between visual information and semantic attributes. Previous works implicitly treat attributes equally in compatibility score while ignoring that they have different importance for discrimination, which leads to severe semantic ambiguity. Considering both low-level visual information and global class-level features that relate to this ambiguity, we propose a practical Latent Feature Guided Attribute Attention (LFGAA) framework to perform object-based attribute attention for semantic disambiguation. By distracting semantic activation in dimensions that cause ambiguity, our method outperforms existing state-of-the-art methods on AwA2, CUB and SUN datasets in both inductive and transductive settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Attribute_Attention_for_Semantic_Disambiguation_in_Zero-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Attribute_Attention_for_Semantic_Disambiguation_in_Zero-Shot_Learning_ICCV_2019_paper.pdf,,https://github.com/ZJULearning/AttentionZSL,,main,Poster,https://ieeexplore.ieee.org/document/9008400/,"['Semantics', 'Visualization', 'Prototypes', 'Training', 'Search problems', 'Feature extraction', 'Aggregates']","['Disambiguation', 'Zero-shot', 'Visual Information', 'Global Features', 'Function Matrix', 'Latent Features', 'Semantic Properties', 'Unseen Objects', 'Semantic Ambiguity', 'Binary Classification', 'Feature Space', 'Feature Maps', 'General Case', 'Visual Representation', 'Latent Space', 'Semantic Features', 'Ridge Regression', 'Feature Categories', 'Specific Layer', 'Selective Properties', 'Semantic Space', 'Unseen Classes', 'Semantic Prediction', 'Visual Space', 'Pseudo Labels', 'Transduction Methods', 'Conventional Settings', 'Distraction Effects', 'Domain Shift Problem', 'Benefit In Settings']",,96,"Zero-shot learning (ZSL) aims to accurately recognize unseen objects by learning mapping matrices that bridge the gap between visual information and semantic attributes. Previous works implicitly treat attributes equally in compatibility score while ignoring that they have different importance for discrimination, which leads to severe semantic ambiguity. Considering both low-level visual information and global class-level features that relate to this ambiguity, we propose a practical Latent Feature Guided Attribute Attention (LFGAA) framework to perform object-based attribute attention for semantic disambiguation. By distracting semantic activation in dimensions that cause ambiguity, our method outperforms existing state-of-the-art methods on AwA2, CUB and SUN datasets in both inductive and transductive settings."
Attribute Manipulation Generative Adversarial Networks for Fashion Images,"Kenan E. Ak, Joo Hwee Lim, Jo Yew Tham, Ashraf A. Kassim","Institute for Infocomm Research, A*STAR, Singapore; National University of Singapore, Singapore; ESP xMedia Pte. Ltd., Singapore",66.66666666666666,singapore,33.33333333333334,Singapore,"Recent advances in Generative Adversarial Networks (GANs) have made it possible to conduct multi-domain image-to-image translation using a single generative network. While recent methods such as Ganimation and SaGAN are able to conduct translations on attribute-relevant regions using attention, they do not perform well when the number of attributes increases as the training of attention masks mostly rely on classification losses. To address this and other limitations, we introduce Attribute Manipulation Generative Adversarial Networks (AMGAN) for fashion images. While AMGAN's generator network uses class activation maps (CAMs) to empower its attention mechanism, it also exploits perceptual losses by assigning reference (target) images based on attribute similarities. AMGAN incorporates an additional discriminator network that focuses on attribute-relevant regions to detect unrealistic translations. Additionally, AMGAN can be controlled to perform attribute manipulations on specific regions such as the sleeve or torso regions. Experiments show that AMGAN outperforms state-of-the-art methods using traditional evaluation metrics as well as an alternative one that is based on image retrieval.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ak_Attribute_Manipulation_Generative_Adversarial_Networks_for_Fashion_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ak_Attribute_Manipulation_Generative_Adversarial_Networks_for_Fashion_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008395/,"['Gallium nitride', 'Cams', 'Generators', 'Image color analysis', 'Task analysis', 'Generative adversarial networks', 'Image retrieval']","['Generative Adversarial Networks', 'Fashion Images', 'Attribute Manipulation', 'Attention Mechanism', 'Reference Image', 'Single Network', 'Classification Loss', 'Image Retrieval', 'Perceptual Loss', 'Traditional Metrics', 'Discriminator Network', 'Class Activation Maps', 'Loss Function', 'Convolutional Neural Network', 'Classification Accuracy', 'Red Color', 'Input Image', 'Average Accuracy', 'User Study', 'Bounding Box', 'Attention Loss', 'Cycle Consistency Loss', 'Image Inpainting', 'Quantitative Experiments', 'Retrieval Accuracy', 'Orange Color', 'Local Ability', 'Realistic Images', 'Output Image', 'Ablation Experiments']",,44,"Recent advances in Generative Adversarial Networks (GANs) have made it possible to conduct multi-domain image-to-image translation using a single generative network. While recent methods such as Ganimation and SaGAN are able to conduct translations on attribute-relevant regions using attention, they do not perform well when the number of attributes increases as the training of attention masks mostly rely on classification losses. To address this and other limitations, we introduce Attribute Manipulation Generative Adversarial Networks (AMGAN) for fashion images. While AMGAN's generator network uses class activation maps (CAMs) to empower its attention mechanism, it also exploits perceptual losses by assigning reference (target) images based on attribute similarities. AMGAN incorporates an additional discriminator network that focuses on attribute-relevant regions to detect unrealistic translations. Additionally, AMGAN can be controlled to perform attribute manipulations on specific regions such as the sleeve or torso regions. Experiments show that AMGAN outperforms state-of-the-art methods using traditional evaluation metrics as well as an alternative one that is based on image retrieval."
Attribute-Driven Spontaneous Motion in Unpaired Image Translation,"Ruizheng Wu, Xin Tao, Xiaodong Gu, Xiaoyong Shen, Jiaya Jia","Tencent YouTu Lab; Harbin Institute of Technology, Shenzhen; The Chinese University of Hong Kong; The Chinese University of Hong Kong, Tencent YouTu Lab",100.0,"Hong Kong, china",0.0,,"Current image translation methods, albeit effective to produce high-quality results in various applications, still do not consider much geometric transform. We in this paper propose the spontaneous motion estimation module, along with a refinement part, to learn attribute-driven deformation between source and target domains. Extensive experiments and visualization demonstrate effectiveness of these modules. We achieve promising results in unpaired-image translation tasks, and enable interesting applications based on spontaneous motion.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Attribute-Driven_Spontaneous_Motion_in_Unpaired_Image_Translation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Attribute-Driven_Spontaneous_Motion_in_Unpaired_Image_Translation_ICCV_2019_paper.pdf,,https://github.com/mikirui/ADSPM,,main,Poster,https://ieeexplore.ieee.org/document/9008762/,"['Face', 'Transforms', 'Strain', 'Motion estimation', 'Visualization', 'Task analysis', 'Training']","['Spontaneous Motion', 'Unpaired Image Translation', 'Target Domain', 'Source Domain', 'Motion Estimation', 'Geometric Transformation', 'High-resolution Images', 'Positive Samples', 'Convolutional Layers', 'Input Image', 'Facial Expressions', 'Paired Data', 'Generative Adversarial Networks', 'Latent Space', 'Final Layer', 'Source Images', 'Residual Block', 'Happy Faces', 'Domain Classifier', 'Bilinear Interpolation', 'Motion Field', 'Geometric Deformation', 'Style Transfer', 'Forward Motion', 'Reconstruction Loss', 'Image Coordinates', 'Image Space']",,14,"Current image translation methods, albeit effective to produce high-quality results in various applications, still do not consider much geometric transform. We in this paper propose the spontaneous motion estimation module, along with a refinement part, to learn attribute-driven deformation between source and target domains. Extensive experiments and visualization demonstrate effectiveness of these modules. We achieve promising results in unpaired-image translation tasks, and enable interesting applications based on spontaneous motion."
Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints,"Ning Yu, Larry S. Davis, Mario Fritz","University of Maryland, College Park; University of Maryland, College Park and Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; CISPA Helmholtz Center for Information Security, Saarland Informatics Campus, Germany",100.0,"germany, usa",0.0,,"Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Attributing_Fake_Images_to_GANs_Learning_and_Analyzing_GAN_Fingerprints_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Attributing_Fake_Images_to_GANs_Learning_and_Analyzing_GAN_Fingerprints_ICCV_2019_paper.pdf,,https://github.com/,,main,Poster,https://ieeexplore.ieee.org/document/9010964/,"['Gallium nitride', 'Generative adversarial networks', 'Visualization', 'Forensics', 'Training', 'Watermarking', 'Intellectual property']","['Generative Adversarial Networks', 'Forensic', 'Authentication', 'Generative Adversarial Networks Training', 'Neural Network', 'Training Set', 'Deep Neural Network', 'Frequency Band', 'Feature Representation', 'Latent Space', 'High-quality Images', 'RGB Images', 'Patch Size', 'Types Of Attacks', 'Low-frequency Components', 'Component Of Image', 'Real Ones', 'Variety Of Experimental Conditions', 'Generative Adversarial Networks Model', 'Generative Adversarial Network Architecture', 'Digital Watermarking', 'JPEG Compression', 'Downsampling Factor', 'Image Domain', 'Correlation Index']",,232,"Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups."
Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification,"Hang Xu, Lewei Yao, Wei Zhang, Xiaodan Liang, Zhenguo Li",Sun Yat-sen University; Huawei Noah’s Ark Lab,50.0,China,50.0,China,"Abstract Neural architecture search (NAS) has shown great potential in automating the manual process of designing a good CNN architecture for image classification. In this paper, we study NAS for object detection, a core computer vision task that classifies and localizes object instances in an image. Existing works focus on transferring the searched architecture from classification task (ImageNet) to the detector backbone, while the rest of the architecture of the detector remains unchanged. However, this pipeline is not task-specific or data-oriented network search which cannot guarantee optimal adaptation to any dataset. Therefore, we propose an architecture search framework named Auto-FPN specifically designed for detection beyond simply searching a classification backbone. Specifically, we propose two auto search modules for detection: Auto-fusion to search a better fusion of the multi-level features; Auto-head to search a better structure for classification and bounding-box(bbox) regression. Instead of searching for one repeatable cell structure, we relax the constraint and allow different cells. The search space of both modules covers many popular designs of detectors and allows efficient gradient-based architecture search with resource constraint (2 days for COCO on 8 GPU cards). Extensive experiments on Pascal VOC, COCO, BDD, VisualGenome and ADE demonstrate the effectiveness of the proposed method, e.g. achieving around 5% improvement than FPN in terms of mAP while requiring around 50% fewer parameters on the searched modules.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Auto-FPN_Automatic_Network_Architecture_Adaptation_for_Object_Detection_Beyond_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Auto-FPN_Automatic_Network_Architecture_Adaptation_for_Object_Detection_Beyond_Classification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009812/,"['Computer architecture', 'Neck', 'Object detection', 'Feature extraction', 'Head', 'Task analysis', 'Training']","['Object Detection', 'Image Classification', 'Resource Constraints', 'Search Space', 'Class Structure', 'Feature Fusion', 'Fewer Parameters', 'Multi-level Features', 'Neural Architecture Search', 'Image Instance', 'CD4 T Cells', 'High-resolution Images', 'Evolutionary Algorithms', 'Receptive Field', 'Training Strategy', 'Weight Decay', 'Stochastic Gradient Descent', 'Cell Yield', 'Spatial Size', 'Feature Pyramid', 'Architecture Parameters', 'Region Proposal Network', 'Floating-point Operations', 'Continuous Relaxation', 'Dilated Convolution', 'Longer Training Time', 'ImageNet Pretraining', 'One-stage Detectors', 'Flipped Images', 'Detection Benchmark']",,144,"Neural architecture search (NAS) has shown great potential in automating the manual process of designing a good CNN architecture for image classification. In this paper, we study NAS for object detection, a core computer vision task that classifies and localizes object instances in an image. Existing works focus on transferring the searched architecture from classification task (ImageNet) to the detector backbone, while the rest of the architecture of the detector remains unchanged. However, this pipeline is not task-specific or data-oriented network search which cannot guarantee optimal adaptation to any dataset. Therefore, we propose an architecture search framework named Auto-FPN specifically designed for detection beyond simply searching a classification backbone. Specifically, we propose two auto search modules for detection: Auto-fusion to search a better fusion of the multi-level features; Auto-head to search a better structure for classification and bounding-box(bbox) regression. Instead of searching for one repeatable cell structure, we relax the constraint and allow different cells. The search space of both modules covers many popular designs of detectors and allows efficient gradient-based architecture search with resource constraint (2 days for COCO on 8 GPU cards). Extensive experiments on Pascal VOC, COCO, BDD, VisualGenome and ADE demonstrate the effectiveness of the proposed method, e.g. achieving around 5% improvement than FPN in terms of mAP while requiring around 50% fewer parameters on the searched modules."
Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification,"Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, Yi Yang","ReLER, University of Technology Sydney; Baidu Research, ReLER, University of Technology Sydney",100.0,australia,0.0,,"Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrieval-based search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-the-art performance while reducing 50% parameters and 53% FLOPs compared to others.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Quan_Auto-ReID_Searching_for_a_Part-Aware_ConvNet_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Quan_Auto-ReID_Searching_for_a_Part-Aware_ConvNet_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008366/,"['Computer architecture', 'Task analysis', 'Tensile stress', 'Microprocessors', 'Feature extraction', 'Training', 'Search problems']","['Convolutional Neural Network', 'Search Algorithm', 'Search Space', 'Deep Convolutional Neural Network', 'Architectural Design', 'Convolutional Neural Network Architecture', 'Body Structure', 'Neural Architecture Search', 'Re-identification Task', 'Neurons', 'Training Set', 'Convolutional Layers', 'Cross-entropy Loss', 'Bounding Box', 'Convolutional Neural Network Model', 'Standard Space', 'High Mapping', 'Identical Images', 'Identity Mapping', 'Dilated Convolution', 'Kinds Of Layers', 'Output Tensor', 'Triplet Loss', 'Convolutional Neural Networks Backbone', 'Standard Search', 'Manual Design', 'Re-identification Methods', 'Query Image', 'Gallery Images']",,169,"Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrieval-based search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-the-art performance while reducing 50% parameters and 53% FLOPs compared to others."
AutoDispNet: Improving Disparity Estimation With AutoML,"Tonmoy Saikia, Yassine Marrakchi, Arber Zela, Frank Hutter, Thomas Brox","University of Freiburg, Germany",100.0,germany,0.0,,"Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Saikia_AutoDispNet_Improving_Disparity_Estimation_With_AutoML_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Saikia_AutoDispNet_Improving_Disparity_Estimation_With_AutoML_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009795/,"['Computer architecture', 'Optimization', 'Task analysis', 'Training', 'Estimation', 'Computer vision', 'Bayes methods']","['Disparity Estimation', 'Computer Vision', 'Optimal Efficiency', 'Bayesian Optimization', 'Neural Architecture Search', 'Hyperparameter Search', 'Manually Optimized', 'Learning Rate', 'Feature Maps', 'Part Of Network', 'Search Space', 'First Approximation', 'Weight Decay', 'Semantic Segmentation', 'Single Network', 'Training Loss', 'Skip Connections', 'Flow Estimation', 'Output Node', 'Learned Weights', 'Encoder-decoder Network', 'Baseline Architecture', 'Single GPU', 'Architecture Parameters', 'Learning Rate Decay', 'Mixed Operator', 'Refinement Network', 'Dense Prediction', 'Spatial Resolution', 'Separable Convolution']",,46,"Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance."
AutoFocus: Efficient Multi-Scale Inference,"Mahyar Najibi, Bharat Singh, Larry S. Davis","University of Maryland, College Park",100.0,usa,0.0,,"This paper describes AutoFocus, an efficient multi-scale inference algorithm for deep-learning based object detectors. Instead of processing an entire image pyramid, AutoFocus adopts a coarse to fine approach and only processes regions which are likely to contain small objects at finer scales. This is achieved by predicting category agnostic segmentation maps for small objects at coarser scales, called FocusPixels. FocusPixels can be predicted with high recall, and in many cases, they only cover a small fraction of the entire image. To make efficient use of FocusPixels, an algorithm is proposed which generates compact rectangular FocusChips which enclose FocusPixels. The detector is only applied inside FocusChips, which reduces computation while processing finer scales. Different types of error can arise when detections from FocusChips of multiple scales are combined, hence techniques to correct them are proposed. AutoFocus obtains an mAP of 47.9% (68.3% at 50% overlap) on the COCO test-dev set while processing 6.4 images per second on a Titan X (Pascal) GPU. This is 2.5X faster than our multi-scale baseline detector and matches its mAP. The number of pixels processed in the pyramid can be reduced by 5X with a 1% drop in mAP. AutoFocus obtains more than 10% mAP gain compared to RetinaNet but runs at the same speed with the same ResNet-101 backbone.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Najibi_AutoFocus_Efficient_Multi-Scale_Inference_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Najibi_AutoFocus_Efficient_Multi-Scale_Inference_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008785/,"['Detectors', 'Training', 'Image resolution', 'Object detection', 'Acceleration', 'Prediction algorithms', 'Inference algorithms']","['Efficient Inference', 'Multi-scale Inference', 'Multiple Scales', 'Object Detection', 'Types Of Errors', 'Entire Image', 'Small Objects', 'Coarse Scale', 'Different Types Of Errors', 'Image Pyramid', 'ResNet-101 Backbone', 'Convolutional Neural Network', 'High-resolution Images', 'Minimum Size', 'Feature Maps', 'Image Regions', 'Detection Efficiency', 'Bounding Box', 'Saccade', 'Ground-truth Bounding Box', 'COCO Dataset', 'Objects Of Different Sizes', 'PASCAL VOC Dataset', 'Difference Of Gaussian', 'Peripheral Vision', 'Horseback Riding', 'Feature Pyramid', 'Scale Resolution', 'Laplacian Of Gaussian']",,82,"This paper describes AutoFocus, an efficient multi-scale inference algorithm for deep-learning based object detectors. Instead of processing an entire image pyramid, AutoFocus adopts a coarse to fine approach and only processes regions which are likely to contain small objects at finer scales. This is achieved by predicting category agnostic segmentation maps for small objects at coarser scales, called FocusPixels. FocusPixels can be predicted with high recall, and in many cases, they only cover a small fraction of the entire image. To make efficient use of FocusPixels, an algorithm is proposed which generates compact rectangular FocusChips which enclose FocusPixels. The detector is only applied inside FocusChips, which reduces computation while processing finer scales. Different types of error can arise when detections from FocusChips of multiple scales are combined, hence techniques to correct them are proposed. AutoFocus obtains an mAP of 47.9% (68.3% at 50% overlap) on the COCO test-dev set while processing 6.4 images per second on a Titan X (Pascal) GPU. This is 2.5× faster than our multi-scale baseline detector and matches its mAP. The number ofpixels processed in the pyramid can be reduced by 5× with a 1% drop in mAP. AutoFocus obtains more than 10% mAP gain compared to RetinaNet but runs at the same speed with the same ResNet-101 backbone."
AutoGAN: Neural Architecture Search for Generative Adversarial Networks,"Xinyu Gong, Shiyu Chang, Yifan Jiang, Zhangyang Wang","Department of Computer Science & Engineering, Texas A&M University; MIT-IBM Watson AI Lab",100.0,usa,0.0,,"Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gong_AutoGAN_Neural_Architecture_Search_for_Generative_Adversarial_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_AutoGAN_Neural_Architecture_Search_for_Generative_Adversarial_Networks_ICCV_2019_paper.pdf,,https://github.com/TAMU-VITA/AutoGAN,,main,Poster,https://ieeexplore.ieee.org/document/9010885/,"['Gallium nitride', 'Training', 'Computer architecture', 'Generators', 'Task analysis', 'Generative adversarial networks', 'Prediction algorithms']","['Generative Adversarial Networks', 'Neural Architecture Search', 'Search Strategy', 'Image Classification', 'Image Segmentation', 'Search Space', 'Recurrent Neural Network', 'Image Generation', 'Image Classification Tasks', 'Neural Network', 'Training Set', 'Optimization Algorithm', 'Search Algorithm', 'Adam Optimizer', 'Data Augmentation', 'Binary Data', 'Training Loss', 'Skip Connections', 'Convolutional Block', 'General Architecture', 'Generative Adversarial Networks Training', 'Beam Search', 'Hinge Loss', 'Progressive Training', 'Random Search', 'Spectral Normalization', 'Hidden Vector', 'Round Of Search']",,167,"Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN"
Automatic and Robust Skull Registration Based on Discrete Uniformization,"Junli Zhao, Xin Qi, Chengfeng Wen, Na Lei, Xianfeng Gu","Department of Computer Science, Stony Brook University, Stony Brook, USA; International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; School of Data Science and Software Engineering, Qingdao University, Qingdao, China",100.0,"china, usa",0.0,,"Skull registration plays a fundamental role in forensic science and is crucial for craniofacial reconstruction. The complicated topology, lack of anatomical features, and low quality reconstructed mesh make skull registration challenging. In this work, we propose an automatic skull registration method based on the discrete uniformization theory, which can handle complicated topologies and is robust to low quality meshes. We apply dynamic Yamabe flow to realize discrete uniformization, which modifies the mesh combinatorial structure during the flow and conformally maps the multiply connected skull surface onto a planar disk with circular holes. The 3D surfaces can be registered by matching their planar images using harmonic maps. This method is rigorous with theoretic guarantee, automatic without user intervention, and robust to low mesh quality. Our experimental results demonstrate the efficiency and efficacy of the method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Automatic_and_Robust_Skull_Registration_Based_on_Discrete_Uniformization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Automatic_and_Robust_Skull_Registration_Based_on_Discrete_Uniformization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008291/,"['Three-dimensional displays', 'Measurement', 'Topology', 'Robustness', 'Iterative closest point algorithm', 'Surface morphology', 'Face']","['Automatic Registration', 'Low Quality', 'Flow Dynamics', '3D Surface', 'Forensic Science', 'Registration Method', 'Skull Surface', 'User Intervention', 'Harmonic Functions', 'Circular Hole', 'Discrete Theory', 'Triangular', 'Finite Element Method', 'Unique Solution', 'Edge Weights', 'Geodesic', 'Feature Points', 'Computational Pipeline', 'Bicontinuous', 'Registration Accuracy', 'Iterative Closest Point', 'Surface Registration', 'Arbitrary Topology', 'Registration Results', 'Hyperbolic Geometry', 'Theoretical Guarantees', 'Non-rigid Registration', 'Elliptic Partial Differential Equations', 'Iterative Framework', 'Manual Input']",,5,"Skull registration plays a fundamental role in forensic science and is crucial for craniofacial reconstruction. The complicated topology, lack of anatomical features, and low quality reconstructed mesh make skull registration challenging. In this work, we propose an automatic skull registration method based on the discrete uniformization theory, which can handle complicated topologies and is robust to low quality meshes. We apply dynamic Yamabe flow to realize discrete uniformization, which modifies the mesh combinatorial structure during the flow and conformally maps the multiply connected skull surface onto a planar disk with circular holes. The 3D surfaces can be registered by matching their planar images using harmonic maps. This method is rigorous with theoretic guarantee, automatic without user intervention, and robust to low mesh quality. Our experimental results demonstrate the efficiency and efficacy of the method."
BMN: Boundary-Matching Network for Temporal Action Proposal Generation,"Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen","Department of Computer Vision Technology (VIS), Baidu Inc.",100.0,china,0.0,,"Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009808/,"['Proposals', 'Videos', 'Task analysis', 'Feature extraction', 'Visualization', 'Computer vision', 'Reliability']","['Proposal Generation', 'Detection Performance', 'Confidence Score', 'Reliability Scores', 'Efficient Generation', 'Action Classes', 'Temporal Boundaries', 'Remarkable Efficiency', 'Precise Boundaries', 'Confidence Map', 'Action Detection', 'End Boundary', 'Training Set', 'Validation Set', 'Convolutional Layers', 'Temporal Dimension', 'Sequence Features', 'Dot Product', 'Temporal Sequence', 'Action Recognition', 'Series Of Convolutional Layers', 'Action Instances', 'Cost Volume', 'IoU Threshold', 'Observation Window', 'Stereo Matching', 'Maximum Duration', 'Two-stream Network', 'Raw Video', 'Evaluation Mechanism']",,371,"Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance."
Bae-Net: Branched Autoencoder for Shape Co-Segmentation,"Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, Hao Zhang","Adobe Research; Adobe Research, IIT Bombay; Simon Fraser University",66.66666666666666,"India, canada",33.33333333333334,USA,"We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with a collection of un-segmented shapes, using a shape reconstruction loss, without any ground-truth labels. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning. We show unsupervised, weakly supervised, and one-shot learning results by BAE-NET, demonstrating that using only a couple of exemplars, our network can generally outperform state-of-the-art supervised methods trained on hundreds of segmented shapes. Code is available at https://github.com/czq142857/BAE-NET.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_BAE-NET_Branched_Autoencoder_for_Shape_Co-Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_BAE-NET_Branched_Autoencoder_for_Shape_Co-Segmentation_ICCV_2019_paper.pdf,,https://github.com/czq142857/BAE-NET,,main,Poster,https://ieeexplore.ieee.org/document/9009017/,"['Shape', 'Decoding', 'Image reconstruction', 'Three-dimensional displays', 'Image segmentation', 'Neurons', 'Training']","['Autoencoder', 'Neural Network', 'Convolutional Neural Network', 'Unsupervised Learning', 'Representation Learning', 'Coordinates Of Points', 'Ground Truth Labels', 'Reconstruction Loss', 'Feature Coding', 'Input Shape', 'Shape Reconstruction', 'Shape Of Segment', 'Deep Neural Network', 'Output Value', 'Intersection Over Union', 'Point Cloud', 'Shape Of Distribution', 'Max-pooling', 'Convolutional Neural Network Model', 'Shape Categories', 'Middle Image', 'Weak Supervision', 'Jet Engine', 'Reconstruction Quality', 'Voxel Grid', 'Middle Line', 'Shape Properties', 'Strong Supervision', 'Input Point']",,74,"We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with a collection of un-segmented shapes, using a shape reconstruction loss, without any ground-truth labels. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning. We show unsupervised, weakly supervised, and one-shot learning results by BAE-NET, demonstrating that using only a couple of exemplars, our network can generally outperform state-of-the-art supervised methods trained on hundreds of segmented shapes. Code is available at https://github.com/czq142857/BAE-NET."
Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations,"Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, Vicente Ordonez",University of California Los Angeles; University of Virginia; Allen Institute for Artiﬁcial Intelligence,100.0,"USA, usa",0.0,,"In this work, we present a framework to measure and mitigate intrinsic biases with respect to protected variables -such as gender- in visual recognition tasks. We show that trained models significantly amplify the association of target labels with gender beyond what one would expect from biased datasets. Surprisingly, we show that even when datasets are balanced such that each label co-occurs equally with each gender, learned models amplify the association between labels and gender, as much as if data had not been balanced! To mitigate this, we adopt an adversarial approach to remove unwanted features corresponding to protected variables from intermediate representations in a deep neural network - and provide a detailed analysis of its effectiveness. Experiments on two datasets: the COCO dataset (objects), and the imSitu dataset (actions), show reductions in gender bias amplification while maintaining most of the accuracy of the original models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Balanced_Datasets_Are_Not_Enough_Estimating_and_Mitigating_Gender_Bias_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Balanced_Datasets_Are_Not_Enough_Estimating_and_Mitigating_Gender_Bias_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008527/,"['Predictive models', 'Task analysis', 'Data models', 'Computational modeling', 'Computer vision', 'Visualization', 'Neural networks']","['Balanced Dataset', 'Visual Recognition', 'Amplification Bias', 'Target Label', 'Intermediate Representation', 'COCO Dataset', 'Adversarial Approach', 'Prediction Model', 'Training Data', 'Convolutional Neural Network', 'Deep Neural Network', 'Convolutional Layers', 'Computer Vision', 'F1 Score', 'Image Information', 'Multilayer Perceptron', 'Target Variable', 'Protective Properties', 'Information Leakage', 'Images Of People', 'Ground Truth Segmentation', 'Ground Truth Labels', 'Conditional Random Field', 'Adversarial Training', 'Strong Baseline', 'Action Recognition', 'Linear Layer', 'Multi-label', 'Level Of Accuracy']",,169,"In this work, we present a framework to measure and mitigate intrinsic biases with respect to protected variables -such as gender- in visual recognition tasks. We show that trained models significantly amplify the association of target labels with gender beyond what one would expect from biased datasets. Surprisingly, we show that even when datasets are balanced such that each label co-occurs equally with each gender, learned models amplify the association between labels and gender, as much as if data had not been balanced! To mitigate this, we adopt an adversarial approach to remove unwanted features corresponding to protected variables from intermediate representations in a deep neural network - and provide a detailed analysis of its effectiveness. Experiments on two datasets: the COCO dataset (objects), and the imSitu dataset (actions), show reductions in gender bias amplification while maintaining most of the accuracy of the original models."
Batch DropBlock Network for Person Re-Identification and Beyond,"Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu, Ping Tan",Alibaba A.I. Labs; Simon Fraser University,50.0,canada,50.0,China,"Since the person re-identification task often suffers from the problem of pose changes and occlusions, some attentive local features are often suppressed when training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network which is a two branch network composed of a conventional ResNet-50 as the global branch and a feature dropping branch.The global branch encodes the global salient representations.Meanwhile, the feature dropping branch consists of an attentive feature learning module called Batch DropBlock, which randomly drops the same region of all input feature maps in a batch to reinforce the attentive feature learning of local regions.The network then concatenates features from both branches and provides a more comprehensive and spatially distributed feature representation. Albeit simple, our method achieves state-of-the-art on person re-identification and it is also applicable to general metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existed works by a large margin (more than 6%).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dai_Batch_DropBlock_Network_for_Person_Re-Identification_and_Beyond_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_Batch_DropBlock_Network_for_Person_Re-Identification_and_Beyond_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009470/,"['Task analysis', 'Measurement', 'Training', 'Semantics', 'Convolution', 'Visualization', 'Cameras']","['Feature Maps', 'Learning Task', 'Feature Representation', 'Feature Learning', 'Attention Module', 'Metric Learning', 'Global Representation', 'Pose Changes', 'Loss Function', 'Hyperparameters', 'Body Parts', 'Training Images', 'Viewing Angle', 'Backbone Network', 'Global Pooling', 'Pose Estimation', 'Feature Aggregation', 'Global Average Pooling', 'Image Retrieval', 'Weak Features', 'Triplet Loss', 'Global Max Pooling', 'Query Image', 'Data Augmentation Methods', 'Softmax Loss', 'Retrieval Datasets', 'Gallery Images', 'Class Activation Maps', 'Training Data', 'Batch Of Images']",,167,"Since the person re-identification task often suffers from the problem of pose changes and occlusions, some attentive local features are often suppressed when training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network which is a two branch network composed of a conventional ResNet-50 as the global branch and a feature dropping branch.The global branch encodes the global salient representations.Meanwhile, the feature dropping branch consists of an attentive feature learning module called Batch DropBlock, which randomly drops the same region of all input feature maps in a batch to reinforce the attentive feature learning of local regions.The network then concatenates features from both branches and provides a more comprehensive and spatially distributed feature representation. Albeit simple, our method achieves state-of-the-art on person re-identification and it is also applicable to general metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existed works by a large margin (more than 6%)."
Batch Weight for Domain Adaptation With Mass Shift,"Mikolaj BiÅkowski, Devon Hjelm, Aaron Courville","Microsoft Research and Mila, Université de Montréal; CIFAR Fellow and Mila, Université de Montréal; Mila, Université de Montréal and Imperial College London",100.0,"canada, uk",0.0,,"Unsupervised domain transfer is the task of transferring or translating samples from a source distribution to a different target distribution. Current solutions unsupervised domain transfer often operate on data on which the modes of the distribution are well-matched, for instance have the same frequencies of classes between source and target distributions. However, these models do not perform well when the modes are not well-matched, as would be the case when samples are drawn independently from two different, but related, domains. This mode imbalance is problematic as generative adversarial networks (GANs), a successful approach in this setting, are sensitive to mode frequency, which results in a mismatch of semantics between source samples and generated samples of the target distribution. We propose a principled method of re-weighting training samples to correct for such mass shift between the transferred distributions, which we call batch weight. We also provide rigorous probabilistic setting for domain transfer and new simplified objective for training transfer networks, an alternative to complex, multi-component loss functions used in the current state-of-the art image-to-image translation models. The new objective stems from the discrimination of joint distributions and enforces cycle-consistency in an abstract, high-level, rather than pixel-wise, sense. Lastly, we experimentally show the effectiveness of the proposed methods in several image-to-image translation tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Binkowski_Batch_Weight_for_Domain_Adaptation_With_Mass_Shift_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Binkowski_Batch_Weight_for_Domain_Adaptation_With_Mass_Shift_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010339/,"['Gallium nitride', 'Generators', 'Training', 'Semantics', 'Transfer functions', 'Decoding', 'Task analysis']","['Domain Adaptation', 'Mass Shift', 'Batch Weight', 'Discrimination', 'Loss Function', 'Modulation Frequency', 'Generative Adversarial Networks', 'Source Distribution', 'Translational Model', 'Target Distribution', 'Transfer Network', 'Domain Transfer', 'Latent Variables', 'Supervised Learning', 'Transfer Function', 'Latent Space', 'Residual Block', 'Target Domain', 'Implicit Bias', 'Training Examples', 'MNIST Dataset', 'Style Transfer', 'Presence Of Transfer', 'Presence Of Imbalance', 'Spectral Normalization', 'Source Domain', 'Quantitative Metrics', 'Optimal Transport', 'Noise Vector', 'Number Of Buildings']",,5,"Unsupervised domain transfer is the task of transferring or translating samples from a source distribution to a different target distribution. Current solutions unsupervised domain transfer often operate on data on which the modes of the distribution are well-matched, for instance have the same frequencies of classes between source and target distributions. However, these models do not perform well when the modes are not well-matched, as would be the case when samples are drawn independently from two different, but related, domains. This mode imbalance is problematic as generative adversarial networks (GANs), a successful approach in this setting, are sensitive to mode frequency, which results in a mismatch of semantics between source samples and generated samples of the target distribution. We propose a principled method of re-weighting training samples to correct for such mass shift between the transferred distributions, which we call batch weight. We also provide rigorous probabilistic setting for domain transfer and new simplified objective for training transfer networks, an alternative to complex, multi-component loss functions used in the current state-of-the art image-to-image translation models. The new objective stems from the discrimination of joint distributions and enforces cycle-consistency in an abstract, high-level, rather than pixel-wise, sense. Lastly, we experimentally show the effectiveness of the proposed methods in several image-to-image translation tasks."
Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement,"Minyoung Kim, Yuting Wang, Pritish Sahu, Vladimir Pavlovic","Samsung AI Center, Cambridge, UK and Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA; Samsung AI Center, Cambridge, UK",66.66666666666666,usa,33.33333333333334,UK,"We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Bayes-Factor-VAE_Hierarchical_Bayesian_Deep_Auto-Encoder_Models_for_Factor_Disentanglement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Bayes-Factor-VAE_Hierarchical_Bayesian_Deep_Auto-Encoder_Models_for_Factor_Disentanglement_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010048,"['Bayes methods', 'Computational modeling', 'Data models', 'Lighting', 'Mathematical model', 'Task analysis', 'Maximum likelihood estimation']","['Bayesian Model', 'Deep Models', 'Variance In The Data', 'Latent Variables', 'Latent Factors', 'Learning Objectives', 'Recent Attempts', 'Variational Autoencoder', 'Latent Dimensions', 'Long-tailed Distribution', 'Hyperprior', 'Upper Bound', 'Factorization', 'Mixture Model', 'Kullback-Leibler', 'Generative Adversarial Networks', 'Independent Component Analysis', 'Representation Learning', 'Density Ratio', 'Reconstruction Loss', 'Nuisance Variables', 'Supplement For Details', 'Latent Vector', 'Trade-off Parameter']",,11,"We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks."
Bayesian Adaptive Superpixel Segmentation,"Roy Uziel, Meitar Ronen, Oren Freifeld","Computer Science, Ben-Gurion University, Email: meitarr@post.bgu.ac.il; Email: uzielr@post.bgu.ac.il; Email: orenfr@cs.bgu.ac.il",33.33333333333333,israel,66.66666666666667,USA,"Superpixels provide a useful intermediate image representation. Existing superpixel methods, however, suffer from at least some of the following drawbacks: 1) topology is handled heuristically; 2) the number of superpixels is either predefined or estimated at a prohibitive cost; 3) lack of adaptiveness. As a remedy, we propose a novel probabilistic model, self-coined Bayesian Adaptive Superpixel Segmentation (BASS), together with an efficient inference. BASS is a Bayesian nonparametric mixture model that also respects topology and favors spatial coherence. The optimizationbased and topology-aware inference is parallelizable and implemented in GPU. Quantitatively, BASS achieves results that are either better than the state-of-the-art or close to it, depending on the performance index and/or dataset. Qualitatively, we argue it achieves the best results; we demonstrate this by not only subjective visual inspection but also objective quantitative performance evaluation of the downstream application of face detection. Our code is available at https://github.com/uzielroy/BASS.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Uziel_Bayesian_Adaptive_Superpixel_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Uziel_Bayesian_Adaptive_Superpixel_Segmentation_ICCV_2019_paper.pdf,,https://github.com/uzielroy/BASS,,main,Poster,https://ieeexplore.ieee.org/document/9008124/,"['Bayes methods', 'Image segmentation', 'Adaptation models', 'Spatial coherence', 'Topology', 'Shape', 'Image color analysis']","['Adaptive Segmentation', 'Quantitative Evaluation', 'Parallelization', 'Mixture Model', 'Face Detection', 'Efficient Inference', 'Conditional Independence', 'Top-down Approach', 'Regular Grid', 'Subclusters', 'Color Space', 'Spatial Coverage', 'Top-down And Bottom-up', 'Gaussian Mixture Model', 'Gibbs Sampling', 'Simple Point', 'Sufficient Statistics', 'Breadth-first Search', 'Topological Constraints']",,26,"Superpixels provide a useful intermediate image representation. Existing superpixel methods, however, suffer from at least some of the following drawbacks: 1) topology is handled heuristically; 2) the number of superpixels is either predefined or estimated at a prohibitive cost; 3) lack of adaptiveness. As a remedy, we propose a novel probabilistic model, self-coined Bayesian Adaptive Superpixel Segmentation (BASS), together with an efficient inference. BASS is a Bayesian nonparametric mixture model that also respects topology and favors spatial coherence. The optimizationbased and topology-aware inference is parallelizable and implemented in GPU. Quantitatively, BASS achieves results that are either better than the state-of-the-art or close to it, depending on the performance index and/or dataset. Qualitatively, we argue it achieves the best results; we demonstrate this by not only subjective visual inspection but also objective quantitative performance evaluation of the downstream application of face detection. Our code is available at https://github.com/uzielroy/BASS."
Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition,"Rui Zhao, Kang Wang, Hui Su, Qiang Ji","RPI; RPI, IBM Research",100.0,usa,0.0,,"We propose a framework for recognizing human actions from skeleton data by modeling the underlying dynamic process that generates the motion pattern. We capture three major factors that contribute to the complexity of the motion pattern including spatial dependencies among body joints, temporal dependencies of body poses, and variation among subjects in action execution. We utilize graph convolution to extract structure-aware feature representation from pose data by exploiting the skeleton anatomy. Long short-term memory (LSTM) network is then used to capture the temporal dynamics of the data. Finally, the whole model is extended under the Bayesian framework to a probabilistic model in order to better capture the stochasticity and variation in the data. An adversarial prior is developed to regularize the model parameters to improve the generalization of the model. A Bayesian inference problem is formulated to solve the classification task. We demonstrate the benefit of this framework in several benchmark datasets with recognition under various generalization conditions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010712/,"['Convolution', 'Skeleton', 'Bayes methods', 'Hidden Markov models', 'Data models', 'Probabilistic logic', 'Kernel']","['Long Short-term Memory', 'Action Recognition', 'Graph Convolution', 'Convolutional Long Short-term Memory', 'Skeleton-based Action Recognition', 'Bayesian Long Short-Term Memory', 'Variance In The Data', 'Bayesian Inference', 'Short-term Memory', 'Probabilistic Model', 'Feature Representation', 'Bayesian Framework', 'Body Joints', 'Skeleton Data', 'Neural Network', 'Training Data', 'Test Data', 'Posterior Probability', 'Bayesian Model', 'Artificial Neural Network', 'Recurrent Neural Network', 'Graph Kernel', 'Local Graph', 'Long Short-term Memory Cell', 'Probabilistic Graphical Models', 'Spectral Graph Theory', 'Global Graph', 'Generative Adversarial Networks', 'Bayesian Neural Network', 'Next Set Of Experiments']",,81,"We propose a framework for recognizing human actions from skeleton data by modeling the underlying dynamic process that generates the motion pattern. We capture three major factors that contribute to the complexity of the motion pattern including spatial dependencies among body joints, temporal dependencies of body poses, and variation among subjects in action execution. We utilize graph convolution to extract structure-aware feature representation from pose data by exploiting the skeleton anatomy. Long short-term memory (LSTM) network is then used to capture the temporal dynamics of the data. Finally, the whole model is extended under the Bayesian framework to a probabilistic model in order to better capture the stochasticity and variation in the data. An adversarial prior is developed to regularize the model parameters to improve the generalization of the model. A Bayesian inference problem is formulated to solve the classification task. We demonstrate the benefit of this framework in several benchmark datasets with recognition under various generalization conditions."
Bayesian Loss for Crowd Count Estimation With Point Supervision,"Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong","Faculty of Electronic and Information Engineering, Xi’an Jiaotong University",100.0,china,0.0,,"In crowd counting datasets, each person is annotated by a point, which is usually the center of the head. And the task is to estimate the total count in a crowd scene. Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a ""ground truth"" density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. However, such a ""ground-truth"" density map is imperfect due to occlusions, perspective effects, variations in object shapes, etc. On the contrary, we propose Bayesian loss, a novel loss function which constructs a density contribution probability model from the point annotations. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point. Without bells and whistles, the loss function makes substantial improvements over the baseline loss on all tested datasets. Moreover, our proposed loss function equipped with a standard backbone network, without using any external detectors or multi-scale architectures, plays favourably against the state of the arts. Our method outperforms previous best approaches by a large margin on the latest and largest UCF-QNRF dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Bayesian_Loss_for_Crowd_Count_Estimation_With_Point_Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Bayesian_Loss_for_Crowd_Count_Estimation_With_Point_Supervision_ICCV_2019_paper.pdf,,https://github.com/ZhihengCV/Baysian-Crowd-Counting,,main,Oral,https://ieeexplore.ieee.org/document/9009503/,"['Training', 'Estimation', 'Kernel', 'Head', 'Bayes methods', 'Shape', 'Feature extraction']","['Count Estimates', 'Crowd Counting', 'Bayesian Loss', 'Point Supervision', 'Loss Function', 'Gaussian Kernel', 'Total Count', 'Density Map', 'Training Loss', 'Object Shape', 'Whistle', 'Center Of Head', 'Learning Targets', 'Multi-scale Architecture', 'External Detector', 'Crowded Scenes', 'Convolutional Neural Network', 'Posterior Probability', 'Image Resolution', 'Convolutional Layers', 'Baseline Methods', 'Head Points', 'Crowd Density', 'Deep Convolutional Neural Network', 'Long Short-term Memory', 'Convolutional Neural Network Model', 'Benchmark Datasets', 'Sparse Areas', 'Background Counts', 'AlexNet']",,338,"In crowd counting datasets, each person is annotated by a point, which is usually the center of the head. And the task is to estimate the total count in a crowd scene. Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a “ground truth” density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. However, such a ""ground-truth"" density map is imperfect due to occlusions, perspective effects, variations in object shapes, etc. On the contrary, we propose Bayesian loss, a novel loss function which constructs a density contribution probability model from the point annotations. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point. Without bells and whistles, the loss function makes substantial improvements over the baseline loss on all tested datasets. Moreover, our proposed loss function equipped with a standard backbone network, without using any external detectors or multi-scale architectures, plays favourably against the state of the arts. Our method outperforms previous best approaches by a large margin on the latest and largest UCF-QNRF dataset."
Bayesian Optimized 1-Bit CNNs,"Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Jianzhuang Liu, Guodong Guo, Rongrong Ji","School of Information Science and Engineering, Xiamen University, Fujian, China; Peng Cheng Lab, Shenzhen, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Institute of Deep Learning, Baidu Research, Beijing China; National Engineering Laboratory for Deep Learning Technology and Application; Institute of Deep Learning, Baidu Research, Beijing China; National Engineering Laboratory for Deep Learning Technology and Application; Huawei Noah’s Ark Lab, China",77.77777777777779,"China, china",22.222222222222214,China,"Deep convolutional neural networks (DCNNs) have dominated the recent developments in computer vision through making various record-breaking models. However, it is still a great challenge to achieve powerful DCNNs in resource-limited environments, such as on embedded devices and smart phones. Researchers have realized that 1-bit CNNs can be one feasible solution to resolve the issue; however, they are baffled by the inferior performance compared to the full-precision DCNNs. In this paper, we propose a novel approach, called Bayesian optimized 1-bit CNNs (denoted as BONNs), taking the advantage of Bayesian learning, a well-established strategy for hard problems, to significantly improve the performance of extreme 1-bit CNNs. We incorporate the prior distributions of full-precision kernels and features into the Bayesian framework to construct 1-bit CNNs in an end-to-end manner, which have not been considered in any previous related methods. The Bayesian losses are achieved with a theoretical support to optimize the network simultaneously in both continuous and discrete spaces, aggregating different losses jointly to improve the model capacity. Extensive experiments on the ImageNet and CIFAR datasets show that BONNs achieve the best classification performance compared to state-of-the-art 1-bit CNNs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Bayesian_Optimized_1-Bit_CNNs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Bayesian_Optimized_1-Bit_CNNs_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008128/,"['Bayes methods', 'Kernel', 'Quantization (signal)', 'Convolution', 'Training', 'Frequency modulation', 'Indexes']","['Convolutional Neural Network', 'Bayesian Inference', 'Distribution Characteristics', 'Deep Convolutional Neural Network', 'Discrete Space', 'Kernel Distribution', 'Learning Rate', 'Convolutional Layers', 'Mixture Model', 'Data Augmentation', 'Stochastic Gradient Descent', 'Convolution Operation', 'Convolution Kernel', 'Quantization Error', 'Gaussian Mixture Model', 'Values Of Quantities', 'Sign Function', 'Maximum A Posteriori', 'ImageNet Dataset', 'Kernel Weight', 'Fine-tuning Process', 'Central Loss', 'Forward Pass']",,37,"Deep convolutional neural networks (DCNNs) have dominated the recent developments in computer vision through making various record-breaking models. However, it is still a great challenge to achieve powerful DCNNs in resource-limited environments, such as on embedded devices and smart phones. Researchers have realized that 1-bit CNNs can be one feasible solution to resolve the issue; however, they are baffled by the inferior performance compared to the full-precision DCNNs. In this paper, we propose a novel approach, called Bayesian optimized 1-bit CNNs (denoted as BONNs), taking the advantage of Bayesian learning, a well-established strategy for hard problems, to significantly improve the performance of extreme 1-bit CNNs. We incorporate the prior distributions of full-precision kernels and features into the Bayesian framework to construct 1-bit CNNs in an end-to-end manner, which have not been considered in any previous related methods. The Bayesian losses are achieved with a theoretical support to optimize the network simultaneously in both continuous and discrete spaces, aggregating different losses jointly to improve the model capacity. Extensive experiments on the ImageNet and CIFAR datasets show that BONNs achieve the best classification performance compared to state-of-the-art 1-bit CNNs."
Bayesian Relational Memory for Semantic Visual Navigation,"Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian",Facebook AI Research; Technion; UC Berkeley,66.66666666666666,"israel, usa",33.33333333333334,USA,"We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Bayesian_Relational_Memory_for_Semantic_Visual_Navigation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Bayesian_Relational_Memory_for_Semantic_Visual_Navigation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009539/,"['Semantics', 'Navigation', 'Visualization', 'Planning', 'Probabilistic logic', 'Bayes methods', 'Layout']","['Machine Vision', 'Test Environment', 'Training Environment', 'Efficient Planning', 'Relation Graph', 'Room Type', 'Memory Updating', 'Probabilistic Graph', 'Unseen Environments', 'Horizon', 'Deep Learning', 'Time Step', 'Spatial Memory', 'Semantic Information', 'High Success Rate', 'Baseline Methods', 'Visual Input', 'Current Environment', 'Ground Truth Labels', 'Semantic Knowledge', 'Deep Reinforcement Learning', 'Short Trajectories', 'Performance Of Different Approaches', 'Deep Reinforcement Learning Agent', 'Noisy Observations', 'Replay Buffer', 'Navigation Task', 'Partial Observation']",,65,"We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure."
Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation,"Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, Kaisheng Ma","Institute for Interdisciplinary Information Sciences, Tsinghua University; Yau Mathematical Sciences Center, Tsinghua University; HiSilicon; Institute for Interdisciplinary Information Core Technology",75.0,"China, china",25.0,China,"Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.pdf,,https://github.com/ArchipLab-LinfengZhang/pytorch-self-distillation,,main,Poster,https://ieeexplore.ieee.org/document/9008829/,"['Training', 'Convolutional neural networks', 'Knowledge engineering', 'Time factors', 'Numerical models', 'Computational modeling']","['Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Performance Of Neural Networks', 'Softmax Layer', 'Accuracy Enhancement', 'Training Set', 'Feature Maps', 'Knowledge Transfer', 'Feature Classification', 'Cross-entropy Loss', 'Teacher Model', 'ImageNet', 'Discriminative Features', 'Classification Output', 'Entropy Loss', 'Student Model', 'Extra Layer', 'Vanishing Gradient Problem', 'L2 Loss', 'Distillation Method', 'Divergence Loss', 'Bottleneck Layer', 'Deep Supervision', 'Shallow Neural Network', 'Kinds Of Datasets', 'Learning Rate Decay', 'Extra Education', 'Object Detection', 'Convolutional Layers']",,431,"Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."
Better and Faster: Exponential Loss for Image Patch Matching,"Shuang Wang, Yanfeng Li, Xuefeng Liang, Dou Quan, Bowu Yang, Shaowei Wei, Licheng Jiao","School of Artiﬁcial Intelligence, Xidian University, Shaanxi, China; School of Artiﬁcial Intelligence, Xidian University, Shaanxi, China and Kyoto University, Kyoto, Japan",100.0,"China, japan",0.0,,"Recent studies on image patch matching are paying more attention on hard sample learning, because easy samples do not contribute much to the network optimization. They have proposed various hard negative sample mining strategies, but very few addressed this problem from the perspective of loss functions. Our research shows that the conventional Siamese and triplet losses treat all samples linearly, thus make the training time consuming. Instead, we propose the exponential Siamese and triplet losses, which can naturally focus more on hard samples and put less emphasis on easy ones, meanwhile, speed up the optimization. To assist the exponential losses, we introduce the hard positive sample mining to further enhance the effectiveness. The extensive experiments demonstrate our proposal improves both metric and descriptor learning on several well accepted benchmarks, and outperforms the state-of-the-arts on the UBC dataset. Moreover, it also shows a better generalizability on cross-spectral image matching and image retrieval tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Better_and_Faster_Exponential_Loss_for_Image_Patch_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Better_and_Faster_Exponential_Loss_for_Image_Patch_Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008529/,"['Training', 'Measurement', 'Optimization', 'Task analysis', 'Learning systems', 'Training data', 'Convergence']","['Image Patches', 'Patch Matching', 'Exponential Loss', 'Image Patch Matching', 'Loss Function', 'Benchmark', 'Positive Samples', 'Negative Samples', 'Image Registration', 'Image Retrieval', 'Matching Task', 'Triplet Loss', 'Easy Samples', 'Mining Strategy', 'Image Retrieval Task', 'Linear Function', 'Large Distances', 'Exponential Function', 'Descriptive Characteristics', 'Pairwise Distances', 'Metric Learning', 'Linear Loss', 'Matching Probability', 'Siamese Network', 'Small Distance', 'Matched Pairs', 'Feature Distance', 'Patch Pairs', 'Difference Of Gaussian', 'L2 Loss']",,16,"Recent studies on image patch matching are paying more attention on hard sample learning, because easy samples do not contribute much to the network optimization. They have proposed various hard negative sample mining strategies, but very few addressed this problem from the perspective of loss functions. Our research shows that the conventional Siamese and triplet losses treat all samples linearly, thus make the training time consuming. Instead, we propose the exponential Siamese and triplet losses, which can naturally focus more on hard samples and put less emphasis on easy ones, meanwhile, speed up the optimization. To assist the exponential losses, we introduce the hard positive sample mining to further enhance the effectiveness. The extensive experiments demonstrate our proposal improves both metric and descriptor learning on several well accepted benchmarks, and outperforms the state-of-the-arts on the UBC dataset. Moreover, it also shows a better generalizability on cross-spectral image matching and image retrieval tasks."
"Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution for Small Object Detection","Junhyug Noh, Wonho Bae, Wonhee Lee, Jinhwan Seo, Gunhee Kim",University of Massachusetts Amherst; Seoul National University,100.0,"south korea, usa",0.0,,"In spite of recent success of proposal-based CNN models for object detection, it is still difficult to detect small objects due to the limited and distorted information that small region of interests (RoI) contain. One way to alleviate this issue is to enhance the features of small RoIs using a super-resolution (SR) technique. We investigate how to improve feature-level super-resolution especially for small object detection, and discover its performance can be significantly improved by (i) utilizing proper high-resolution target features as supervision signals for training of a SR model and (ii) matching the relative receptive fields of training pairs of input low-resolution features and target high-resolution features. We propose a novel feature-level super-resolution approach that not only correctly addresses these two desiderata but also is integrable with any proposal-based detectors with feature pooling. In our experiments, our approach significantly improves the performance of Faster R-CNN on three benchmarks of Tsinghua-Tencent 100K, PASCAL VOC and MS COCO. The improvement for small objects is remarkably large, and encouragingly, those for medium and large objects are nontrivial too. As a result, we achieve new state-of-the-art performance on Tsinghua-Tencent 100K and highly competitive results on both PASCAL VOC and MS COCO.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Noh_Better_to_Follow_Follow_to_Be_Better_Towards_Precise_Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Noh_Better_to_Follow_Follow_to_Be_Better_Towards_Precise_Supervision_ICCV_2019_paper.pdf,http://vision.snu.ac.kr/projects/better-to-follow,,,main,Poster,https://ieeexplore.ieee.org/document/9010998/,"['Feature extraction', 'Image resolution', 'Signal resolution', 'Object detection', 'Proposals', 'Convolution', 'Detectors']","['Object Detection', 'Small Objects', 'Small Object Detection', 'Convolutional Neural Network', 'Receptive Field', 'Related Fields', 'Detection Model', 'Target Features', 'Large Objects', 'Faster R-CNN', 'Feature Pairs', 'Proper Characterization', 'High-resolution Features', 'Object Detection Model', 'Proper Targeting', 'Feature Pooling', 'Super-resolution Approaches', 'Super-resolution Model', 'Convolutional Layers', 'Contextual Information', 'Low-resolution Feature', 'Convolutional Neural Networks Backbone', 'Atrous Convolution', 'Proposal Features', 'Generative Adversarial Networks', 'Image Retrieval Task', 'Prediction Box', 'Direct Supervision', 'Input Image', 'Region Proposal Network']",,144,"In spite of recent success of proposal-based CNN models for object detection, it is still difficult to detect small objects due to the limited and distorted information that small region of interests (RoI) contain. One way to alleviate this issue is to enhance the features of small RoIs using a super-resolution (SR) technique. We investigate how to improve feature-level super-resolution especially for small object detection, and discover its performance can be significantly improved by (i) utilizing proper high-resolution target features as supervision signals for training of a SR model and (ii) matching the relative receptive fields of training pairs of input low-resolution features and target high-resolution features. We propose a novel feature-level super-resolution approach that not only correctly addresses these two desiderata but also is integrable with any proposal-based detectors with feature pooling. In our experiments, our approach significantly improves the performance of Faster R-CNN on three benchmarks of Tsinghua-Tencent 100K, PASCAL VOC and MS COCO. The improvement for small objects is remarkably large, and encouragingly, those for medium and large objects are nontrivial too. As a result, we achieve new state-of-the-art performance on Tsinghua-Tencent 100K and highly competitive results on both PASCAL VOC and MS COCO."
Beyond Cartesian Representations for Local Descriptors,"Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, Eduard Trulls","Computer Vision Lab, Ecole Polytechnique Federale de Lausanne; Visual Computing Group, University of Victoria; Google Switzerland",66.66666666666666,"canada, france",33.33333333333334,USA,"The dominant approach for learning local patch descriptors relies on small image regions whose scale must be properly estimated a priori by a keypoint detector. In other words, if two patches are not in correspondence, their descriptors will not match. A strategy often used to alleviate this problem is to ""pool"" the pixel-wise features over log-polar regions, rather than regularly spaced ones. By contrast, we propose to extract the ""support region"" directly with a log-polar sampling scheme. We show that this provides us with a better representation by simultaneously oversampling the immediate neighbourhood of the point and undersampling regions far away from it. We demonstrate that this representation is particularly amenable to learning descriptors with deep networks. Our models can match descriptors across a much wider range of scales than was possible before, and also leverage much larger support regions without suffering from occlusions. We report state-of-the-art results on three different datasets",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ebel_Beyond_Cartesian_Representations_for_Local_Descriptors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ebel_Beyond_Cartesian_Representations_for_Local_Descriptors_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009098/,"['Feature extraction', 'Aggregates', 'Detectors', 'Computational modeling', 'Solid modeling', 'Principal component analysis', 'Measurement']","['Local Descriptors', 'Deep Network', 'Small Region', 'Image Regions', 'Neighboring Points', 'Wide Range Of Scales', 'Keypoint Detection', 'Convolutional Layers', 'Computer Vision', 'Local Features', 'State Of The Art', 'Dense Network', 'Stochastic Gradient Descent', 'Batch Normalization', 'Image Pairs', 'Imaging Measurements', 'Scale Changes', 'Depth Map', 'Scale-invariant', 'Cartesian Space', 'Patch Extraction', 'Difference Of Gaussian', 'Patch Matching', 'Scale Mismatch', 'Sparse Feature', 'Illumination Changes']",,52,"The dominant approach for learning local patch descriptors relies on small image regions whose scale must be properly estimated a priori by a keypoint detector. In other words, if two patches are not in correspondence, their descriptors will not match. A strategy often used to alleviate this problem is to “pool” the pixel-wise features over log-polar regions, rather than regularly spaced ones. By contrast, we propose to extract the “support region” directly with a log-polar sampling scheme. We show that this provides us with a better representation by simultaneously oversampling the immediate neighbourhood of the point and undersampling regions far away from it. We demonstrate that this representation is particularly amenable to learning descriptors with deep networks. Our models can match descriptors across a much wider range of scales than was possible before, and also leverage much larger support regions without suffering from occlusions. We report state-of-the-art results on three different datasets"
Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification,"Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jin-Ge Yao, Kai Han","Key Laboratory of Machine Perception (MOE), Peking University; Microsoft Research Asia; Noah’s Ark Lab",33.33333333333333,china,66.66666666666667,USA,"Person re-identification is a challenging task due to various complex factors. Recent studies have attempted to integrate human parsing results or externally defined attributes to help capture human parts or important object regions. On the other hand, there still exist many useful contextual cues that do not fall into the scope of predefined human parts or attributes. In this paper, we address the missed contextual cues by exploiting both the accurate human parts and the coarse non-human parts. In our implementation, we apply a human parsing model to extract the binary human part masks and a self-attention mechanism to capture the soft latent (non-human) part masks. We verify the effectiveness of our approach with new state-of-the-art performance on three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our implementation is available at https://github.com/ggjy/P2Net.pytorch.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Guo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Guo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_Person_Re-Identification_ICCV_2019_paper.pdf,,https://github.com/ggjy/P2Net.pytorch,,main,Poster,https://ieeexplore.ieee.org/document/9010823/,"['Feature extraction', 'Semantics', 'Task analysis', 'Benchmark testing', 'Cameras', 'Computational modeling', 'Visualization']","['Dual Representation', 'Human Model', 'Contextual Cues', 'Self-attention Mechanism', 'Human Attributes', 'Challenging Benchmark', 'Contextual Information', 'Feature Maps', 'Attention Mechanism', 'ImageNet', 'Bounding Box', 'Semantic Segmentation', 'Pose Estimation', 'Important Cues', 'Latent Representation', 'Influence Of Information', 'Attention Map', 'Output Feature Map', 'Partial Representation', 'Triplet Loss', 'Confidence Map', 'Pixel Representation', 'Human Pose', 'Softmax Loss', 'Misalignment Problem']",,141,"Person re-identification is a challenging task due to various complex factors. Recent studies have attempted to integrate human parsing results or externally defined attributes to help capture human parts or important object regions. On the other hand, there still exist many useful contextual cues that do not fall into the scope of predefined human parts or attributes. In this paper, we address the missed contextual cues by exploiting both the accurate human parts and the coarse non-human parts. In our implementation, we apply a human parsing model to extract the binary human part masks and a self-attention mechanism to capture the soft latent (non-human) part masks. We verify the effectiveness of our approach with new state-of-the-art performance on three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our implementation is available at https://github.com/ggjy/P2Net.pytorch."
Bidirectional One-Shot Unsupervised Domain Mapping,"Tomer Cohen, Lior Wolf",Tel Aviv University; Facebook AI Research and Tel Aviv University,100.0,"israel, usa, israel",0.0,,"We study the problem of mapping between a domain A, in which there is a single training sample and a domain B, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain B. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cohen_Bidirectional_One-Shot_Unsupervised_Domain_Mapping_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cohen_Bidirectional_One-Shot_Unsupervised_Domain_Mapping_ICCV_2019_paper.pdf,,https://github.com/tomercohen11/BiOST,,main,Poster,https://ieeexplore.ieee.org/document/9009515/,"['Training', 'Decoding', 'Gallium nitride', 'Visualization', 'Task analysis', 'Encoding', 'Unsupervised learning']","['Domain Mapping', 'Training Set', 'Single Image', 'Autoencoder', 'Latent Space', 'Target Domain', 'Image Domain', 'Visual Domain', 'Ecosystem', 'Feature Space', 'Target Image', 'Baseline Methods', 'Source Images', 'Residual Block', 'Loss Of Content', 'Output Image', 'Loss Term', 'Hair Color', 'Source Domain', 'Reconstruction Loss', 'Style Transfer', 'Cycle Loss', 'Style Image', 'Black Hair', 'Adversarial Training', 'Ablation Analysis']",,22,"We study the problem of mapping between a domain A, in which there is a single training sample and a domain B, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain B. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST."
Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks,"Jianyu Wang, Haichao Zhang",Baidu Research USA,0.0,,100.0,China,"In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training [??], we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we first show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7% versus 47.2%; on SVHN, we achieve 59.1% versus 42.1%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009088/,"['Training', 'Robustness', 'Computational modeling', 'Perturbation methods', 'Machine learning', 'Analytical models', 'Data models']","['Adversarial Training', 'Fast Training', 'Adversarial Attacks', 'Fast Method', 'Cross-entropy Loss', 'ImageNet Dataset', 'Target Label', 'Attack Target', 'Gradient Magnitude', 'Heuristic Solution', 'Random Start', 'Leakage Problem', 'Defense Methods', 'White-box Attack', 'Adversarial Robustness', 'Deep Learning', 'Step Size', 'Number Of Steps', 'Random Noise', 'Adversarial Examples', 'Projected Gradient Descent', 'Fast Gradient Sign Method', 'Adversarial Perturbations', 'Black-box Attacks', 'Feasible Set', 'Learning Rate Schedule', 'Saddle Point', 'Training Images']",,54,"In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training [34], we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we first show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7% versus 47.2%; on SVHN, we achieve 59.1% versus 42.1%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method."
Bilinear Attention Networks for Person Retrieval,"Pengfei Fang, Jieming Zhou, Soumava Kumar Roy, Lars Petersson, Mehrtash Harandi","DATA61-CSIRO, Australia; Monash University, DATA61-CSIRO, Australia; The Australian National University; The Australian National University, DATA61-CSIRO, Australia",75.0,"Australia, australia",25.0,Australia,"This paper investigates a novel Bilinear attention (Bi-attention) block, which discovers and uses second order statistical information in an input feature map, for the purpose of person retrieval. The Bi-attention block uses bilinear pooling to model the local pairwise feature interactions along each channel, while preserving the spatial structural information. We propose an Attention in Attention (AiA) mechanism to build inter-dependency among the second order local and global features with the intent to make better use of, or pay more attention to, such higher order statistical relationships. The proposed network, equipped with the proposed Bi-attention is referred to as Bilinear ATtention network (BAT-net). Our approach outperforms current state-of-the-art by a considerable margin across the standard benchmark datasets (e.g., CUHK03, Market-1501, DukeMTMC-reID and MSMT17).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008814/,"['Feature extraction', 'Task analysis', 'Visualization', 'Computer vision', 'Benchmark testing', 'Training', 'Computer architecture']","['Person Retrieval', 'Bilinear Attention', 'Spatial Information', 'Local Features', 'Feature Maps', 'Attention Mechanism', 'Global Features', 'Order Statistics', 'Attention Block', 'Higher-order Statistics', 'Considerable Margin', 'Spatial Structure Information', 'Standard Benchmark Datasets', 'Training Set', 'Convolutional Neural Network', 'Network Performance', 'Training Images', 'Bounding Box', 'Representation Learning', 'Latent Space', 'Appearance Features', 'Triplet Loss', 'Bilinear Model', 'Query Image', 'Multi-task Training', 'Gallery Images', 'Task Loss', 'Person Image', 'Ranking Task', 'Discriminative Parts']",,102,"This paper investigates a novel Bilinear attention (Bi-attention) block, which discovers and uses second order statistical information in an input feature map, for the purpose of person retrieval. The Bi-attention block uses bilinear pooling to model the local pairwise feature interactions along each channel, while preserving the spatial structural information. We propose an Attention in Attention (AiA) mechanism to build inter-dependency among the second order local and global features with the intent to make better use of, or pay more attention to, such higher order statistical relationships. The proposed network, equipped with the proposed Bi-attention is referred to as Bilinear ATtention network (BAT-net). Our approach outperforms current state-of-the-art by a considerable margin across the standard benchmark datasets (e.g., CUHK03, Market-1501, DukeMTMC-reID and MSMT17)."
Bit-Flip Attack: Crushing Neural Network With Progressive Bit Search,"Adnan Siraj Rakin, Zhezhi He, Deliang Fan","Dept. of Electrical and Computer Engineering, Arizona State University, Tempe, AZ 85287",100.0,usa,0.0,,"Several important security issues of Deep Neural Network (DNN) have been raised recently associated with different applications and components. The most widely investigated security concern of DNN is from its malicious input, a.k.a adversarial example. Nevertheless, the security challenge of DNN's parameters is not well explored yet. In this work, we are the first to propose a novel DNN weight attack methodology called Bit-Flip Attack (BFA) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., DRAM). The bit-flip operations could be conducted through well-known Row-Hammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of DNN weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bit-flips. Our proposed BFA utilizes a Progressive Bit Search (PBS) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. With the aid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e., top-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93 million bits, while randomly flipping 100 bits merely degrades the accuracy by less than 1%. Code is released at: https://github.com/elliothe/Neural_Network_Weight_Attack",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rakin_Bit-Flip_Attack_Crushing_Neural_Network_With_Progressive_Bit_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rakin_Bit-Flip_Attack_Crushing_Neural_Network_With_Progressive_Bit_Search_ICCV_2019_paper.pdf,,https://github.com/elliothe/Neural_Network_Weight_Attack,,main,Poster,https://ieeexplore.ieee.org/document/9009040/,"['Security', 'Random access memory', 'Quantization (signal)', 'Robustness', 'Biological neural networks', 'Degradation']","['Neural Network', 'Deep Neural Network', 'Weight Parameters', 'Memory System', 'Random Flipping', 'Security Concern', 'Adversarial Examples', 'Quantum', 'Efficient Algorithm', 'Network Parameters', 'Computer System', 'Robust Network', 'Types Of Attacks', 'Deep Neural Network Model', 'AlexNet', 'ImageNet Dataset', 'Forward Error Correction', 'Adversarial Training', 'Neural Network Parameters', 'Adversarial Attacks', 'Output Of The Deep Neural Network', 'Attack Methods', 'Random Bits', 'White-box Attack', 'Previous Attack', 'ResNet Architecture', 'Defense Methods', 'Hamming Distance', 'Robustness Of Neural Networks', 'Deep Neural Network Parameters']",,135,"Several important security issues of Deep Neural Network (DNN) have been raised recently associated with different applications and components. The most widely investigated security concern of DNN is from its malicious input, a.k.a adversarial example. Nevertheless, the security challenge of DNN's parameters is not well explored yet. In this work, we are the first to propose a novel DNN weight attack methodology called Bit-Flip Attack (BFA) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., DRAM). The bit-flip operations could be conducted through well-known Row-Hammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of DNN weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bit-flips. Our proposed BFA utilizes a Progressive Bit Search (PBS) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. With the aid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e., top-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93 million bits, while randomly flipping 100 bits merely degrades the accuracy by less than 1%. Code is released at: https://github.com/elliothe/Neural_Network_Weight_Attack."
Block Annotation: Better Image Annotation With Sub-Image Decomposition,"Hubert Lin, Paul Upchurch, Kavita Bala",Cornell University,100.0,usa,0.0,,"Image datasets with high-quality pixel-level annotations are valuable for semantic segmentation: labelling every pixel in an image ensures that rare classes and small objects are annotated. However, full-image annotations are expensive, with experts spending up to 90 minutes per image. We propose block sub-image annotation as a replacement for full-image annotation. Despite the attention cost of frequent task switching, we find that block annotations can be crowdsourced at higher quality compared to full-image annotation with equal monetary cost using existing annotation tools developed for full-image annotation. Surprisingly, we find that 50% pixels annotated with blocks allows semantic segmentation to achieve equivalent performance to 100% pixels annotated. Furthermore, as little as 12% of pixels annotated allows performance as high as 98% of the performance with dense annotation. In weakly-supervised settings, block annotation outperforms existing methods by 3-4% (absolute) given equivalent annotation time. To recover the necessary global structure for applications such as characterizing spatial context and affordance relationships, we propose an effective method to inpaint block-annotated images with high-quality labels without additional human effort. As such, fewer annotations can also be used for these applications compared to full-image annotation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_Block_Annotation_Better_Image_Annotation_With_Sub-Image_Decomposition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Block_Annotation_Better_Image_Annotation_With_Sub-Image_Decomposition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010240/,"['Image segmentation', 'Task analysis', 'Semantics', 'Tools', 'Image annotation', 'Labeling', 'Quality control']","['Image Annotation', 'Image Pixels', 'Crowdsourcing', 'Semantic Segmentation', 'Monetary Cost', 'Rare Classes', 'Pixel-level Annotations', 'Large-scale Datasets', 'Competitive Performance', 'Real-world Datasets', 'Automatic Segmentation', 'Segmentation Performance', 'Checkers', 'Annotation Quality', 'Scribble', 'Image Block', 'Inpainting', 'Weak Supervision', 'Complex Boundary', 'Median Wage', 'Pixel-level Labels', 'Annotation Task', 'Semantic Regions', 'Polygon Boundaries', 'Block Boundaries', 'Interactive Segmentation', 'Expert Annotations', 'Annotation Efforts', 'Ground Truth Labels']",,13,"Image datasets with high-quality pixel-level annotations are valuable for semantic segmentation: labelling every pixel in an image ensures that rare classes and small objects are annotated. However, full-image annotations are expensive, with experts spending up to 90 minutes per image. We propose block sub-image annotation as a replacement for full-image annotation. Despite the attention cost of frequent task switching, we find that block annotations can be crowdsourced at higher quality compared to full-image annotation with equal monetary cost using existing annotation tools developed for full-image annotation. Surprisingly, we find that 50% pixels annotated with blocks allows semantic segmentation to achieve equivalent performance to 100% pixels annotated. Furthermore, as little as 12% of pixels annotated allows performance as high as 98% of the performance with dense annotation. In weakly-supervised settings, block annotation outperforms existing methods by 3-4% (absolute) given equivalent annotation time. To recover the necessary global structure for applications such as characterizing spatial context and affordance relationships, we propose an effective method to inpaint block-annotated images with high-quality labels without additional human effort. As such, fewer annotations can also be used for these applications compared to full-image annotation."
Boosting Few-Shot Visual Learning With Self-Supervision,"Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick PÃ©rez, Matthieu Cord","LIGM, Ecole des Pont ParisTech; valeo.ai; valeo.ai3Sorbonne Université",66.66666666666666,"France, france",33.33333333333334,Germany,"Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques. We provide the implementation code at: https://github.com/valeoai/BF3S",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.pdf,,https://github.com/valeoai/BF3S,,main,Poster,https://ieeexplore.ieee.org/document/9010654/,"['Task analysis', 'Visualization', 'Training', 'Data models', 'Image representation', 'Biological system modeling', 'Feature extraction']","['Unlabeled Data', 'Self-supervised Learning', 'Few-shot Learning', 'Auxiliary Task', 'Training Data', 'Convolutional Neural Network', 'Image Features', 'Classification Performance', 'Object Detection', 'Training Images', 'Semantic Segmentation', 'Relative Location', 'Set Of Classes', 'Training Examples', 'Learning Stage', 'Base Classes', 'Image X', 'Unlabeled Images', 'Auxiliary Loss', 'Pretext Task', 'Self-supervised Task', 'Few-shot Classification', 'Prototypical Network', 'Self-supervised Representation', 'Weight Vector', 'Class Labels', 'Strong Baseline', 'Image Rotation', 'Image Classification', 'Stochastic Gradient Descent']",,246,"Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques. We provide the implementation code at: https://github.com/valeoai/BF3S."
Bottleneck Potentials in Markov Random Fields,"Ahmed Abbas, Paul Swoboda","Max Planck Institute for Informatics, ZEISS; Max Planck Institute for Informatics",100.0,germany,0.0,,"We consider general discrete Markov Random Fields(MRFs) with additional bottleneck potentials which penalize the maximum (instead of the sum) over local potential value taken by the MRF-assignment. Bottleneck potentials or analogous constructions have been considered in (i) combinatorial optimization (e.g. bottleneck shortest path problem, the minimum bottleneck spanning tree problem, bottleneck function minimization in greedoids), (ii) inverse problems with L_ infinity -norm regularization and (iii) valued constraint satisfaction on the (min,max)-pre-semirings. Bottleneck potentials for general discrete MRFs are a natural generalization of the above direction of modeling work to Maximum-A-Posteriori (MAP) inference in MRFs. To this end we propose MRFs whose objective consists of two parts: terms that factorize according to (i) (min,+), i.e. potentials as in plain MRFs, and (ii) (min,max), i.e. bottleneck potentials. To solve the ensuing inference problem, we propose high-quality relaxations and efficient algorithms for solving them. We empirically show efficacy of our approach on large scale seismic horizon tracking problems.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Abbas_Bottleneck_Potentials_in_Markov_Random_Fields_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Abbas_Bottleneck_Potentials_in_Markov_Random_Fields_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010010/,"['Labeling', 'Optimization', 'Markov random fields', 'Inference algorithms', 'Heuristic algorithms', 'Inverse problems', 'Image segmentation']","['Markov Random Field', 'Efficient Algorithm', 'Optimal Combination', 'Shortest Path', 'Inverse Problem', 'Inference Problem', 'Constraint Satisfaction', 'Shortest Path Problem', 'Optimization Problem', 'Graphical Model', 'Optical Flow', 'Tracking Algorithm', 'Adjacent Nodes', 'Dual Problem', 'Local Terms', 'Breadth-first Search', 'Node Labels', 'Rock Strata', 'Label Space', 'Graph Algorithms', 'Pairwise Potential', 'Dual Decomposition', 'Optimal Labeling', 'Linear Programming Relaxation', 'Seismic Interpretation', 'Lack Of Ground Truth']",,2,"We consider general discrete Markov Random Fields (MRFs) with additional bottleneck potentials which penalize the maximum (instead of the sum) over local potential value taken by the MRF-assignment. Bottleneck potentials or analogous constructions have been considered in (i) combinatorial optimization (e.g. bottleneck shortest path problem, the minimum bottleneck spanning tree problem, bottleneck function minimization in greedoids), (ii) inverse problems with L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">∞</sub>
-norm regularization, and (iii) valued constraint satisfaction on the (min, max)-pre-semirings. Bottleneck potentials for general discrete MRFs are a natural generalization of the above direction of modeling work to Maximum-A-Posteriori (MAP) inference in MRFs. To this end, we propose MRFs whose objective consists of two parts: terms that factorize according to (i) (min, +), i.e. potentials as in plain MRFs, and (ii) (min, max), i.e. bottleneck potentials. To solve the ensuing inference problem, we propose high-quality relaxations and efficient algorithms for solving them. We empirically show efficacy of our approach on large scale seismic horizon tracking problems."
Boundary-Aware Feature Propagation for Scene Segmentation,"Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, Gang Wang","University of Geneva, Switzerland; Alibaba Group, China; Nanyang Technological University, Singapore",66.66666666666666,"Singapore, Switzerland",33.33333333333334,China,"In this work, we address the challenging issue of scene segmentation. To increase the feature similarity of the same object while keeping the feature discrimination of different objects, we explore to propagate information throughout the image under the control of objects' boundaries. To this end, we first propose to learn the boundary as an additional semantic class to enable the network to be aware of the boundary layout. Then, we propose unidirectional acyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs), which structurize the image via building graphic pixel-by-pixel connections, in an efficient and effective way. Furthermore, we propose a boundary-aware feature propagation (BFP) module to harvest and propagate the local features within their regions isolated by the learned boundaries in the UAG-structured image. The proposed BFP is capable of splitting the feature propagation into a set of semantic groups via building strong connections among the same segment region but weak connections between different segment regions. Without bells and whistles, our approach achieves new state-of-the-art segmentation performance on three challenging semantic segmentation datasets, i.e., PASCAL-Context, CamVid, and Cityscapes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009573/,"['Image segmentation', 'Semantics', 'Layout', 'Task analysis', 'Image edge detection', 'Convolution']","['Propagation Characteristics', 'Scene Segmentation', 'Efficient Way', 'Local Features', 'Undirected', 'Additional Categories', 'Semantic Segmentation', 'Acyclic Graph', 'Control Objective', 'Segmented Regions', 'Weak Connections', 'Segmentation Performance', 'Generation Module', 'Segmentation Dataset', 'Object Discrimination', 'Semantic Segmentation Datasets', 'Convolutional Neural Network', 'Multiple Processes', 'Feature Maps', 'Graphical Model', 'Directed Acyclic Graph', 'Boundary Detection', 'Boundary Information', 'Confidence Map', 'Number Of Loops', 'Conditional Random Field', 'Fully Convolutional Network', 'Network Propagation', 'Markov Random Field', 'Object Boundaries']",,161,"In this work, we address the challenging issue of scene segmentation. To increase the feature similarity of the same object while keeping the feature discrimination of different objects, we explore to propagate information throughout the image under the control of objects' boundaries. To this end, we first propose to learn the boundary as an additional semantic class to enable the network to be aware of the boundary layout. Then, we propose unidirectional acyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs), which structurize the image via building graphic pixel-by-pixel connections, in an efficient and effective way. Furthermore, we propose a boundary-aware feature propagation (BFP) module to harvest and propagate the local features within their regions isolated by the learned boundaries in the UAG-structured image. The proposed BFP is capable of splitting the feature propagation into a set of semantic groups via building strong connections among the same segment region but weak connections between different segment regions. Without bells and whistles, our approach achieves new state-of-the-art segmentation performance on three challenging semantic segmentation datasets, i.e., PASCAL-Context, CamVid, and Cityscapes."
Boundless: Generative Adversarial Networks for Image Extension,"Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David Belanger, Ce Liu, William T. Freeman",Google Research,0.0,,100.0,USA,"Image extension models have broad applications in image editing, computational photography and computer graphics. While image inpainting has been extensively studied in the literature, it is challenging to directly apply the state-of-the-art inpainting methods to image extension as they tend to generate blurry or repetitive pixels with inconsistent semantics. We introduce semantic conditioning to the discriminator of a generative adversarial network (GAN), and achieve strong results on image extension with coherent semantics and visually pleasing colors and textures. We also show promising results in extreme extensions, such as panorama generation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Teterwak_Boundless_Generative_Adversarial_Networks_for_Image_Extension_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Teterwak_Boundless_Generative_Adversarial_Networks_for_Image_Extension_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008290/,"['Gallium nitride', 'Semantics', 'Generators', 'Training', 'Generative adversarial networks', 'Image edge detection', 'Context modeling']","['Generative Adversarial Networks', 'Image Inpainting', 'Training Set', 'Deep Network', 'Deep Neural Network', 'Convolutional Layers', 'Super-resolution', 'Skip Connections', 'Output Image', 'Feature Matching', 'Pre-trained Network', 'Extensive Regions', 'Central Square', 'Perceptual Loss', 'Strong Baseline', 'Large Extension', 'Semantic Change', 'Unknown Regions', 'Fr√©chet Inception Distance', 'Wasserstein Generative Adversarial Networks', 'Semantic Coherence']",,22,"Image extension models have broad applications in image editing, computational photography and computer graphics. While image inpainting has been extensively studied in the literature, it is challenging to directly apply the state-of-the-art inpainting methods to image extension as they tend to generate blurry or repetitive pixels with inconsistent semantics. We introduce semantic conditioning to the discriminator of a generative adversarial network (GAN), and achieve strong results on image extension with coherent semantics and visually pleasing colors and textures. We also show promising results in extreme extensions, such as panorama generation."
Bridging the Domain Gap for Ground-to-Aerial Image Matching,"Krishna Regmi, Mubarak Shah","Center for Research in Computer Vision, University of Central Florida",100.0,usa,0.0,,"The visual entities in cross-view (e.g. ground and aerial) images exhibit drastic domain changes due to the differences in viewpoints each set of images is captured from. Existing state-of-the-art methods address the problem by learning view-invariant images descriptors. We propose a novel method for solving this task by exploiting the gener- ative powers of conditional GANs to synthesize an aerial representation of a ground-level panorama query and use it to minimize the domain gap between the two views. The synthesized image being from the same view as the ref- erence (target) image, helps the network to preserve im- portant cues in aerial images following our Joint Feature Learning approach. We fuse the complementary features from a synthesized aerial image with the original ground- level panorama features to obtain a robust query represen- tation. In addition, we employ multi-scale feature aggre- gation in order to preserve image representations at dif- ferent scales useful for solving this complex task. Experi- mental results show that our proposed approach performs significantly better than the state-of-the-art methods on the challenging CVUSA dataset in terms of top-1 and top-1% retrieval accuracies. Furthermore, we evaluate the gen- eralization of the proposed method for urban landscapes on our newly collected cross-view localization dataset with geo-reference information.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Regmi_Bridging_the_Domain_Gap_for_Ground-to-Aerial_Image_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Regmi_Bridging_the_Domain_Gap_for_Ground-to-Aerial_Image_Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010757/,"['Gallium nitride', 'Task analysis', 'Image edge detection', 'Training', 'Image matching', 'Semantics', 'Image segmentation']","['Image Registration', 'Domain Gap', 'Generative Adversarial Networks', 'Aerial Images', 'Reference Image', 'Image Representation', 'Multi-scale Features', 'Robust Representation', 'Complementary Features', 'Retrieval Accuracy', 'Order Aggregates', 'Deep Network', 'Image Features', 'Convolutional Layers', 'Feature Representation', 'Learning Network', 'Image Pairs', 'Representation Learning', 'Fully-connected Layer', 'Feature Fusion', 'Street View Images', 'Image Ground', 'Query Image', 'Top-1 Accuracy', 'Fourth Row', 'Feature Fusion Network', 'Triplet Loss', 'Global Average Pooling', 'Matching Task', 'Image Retrieval']",,104,"The visual entities in cross-view (e.g. ground and aerial) images exhibit drastic domain changes due to the differences in viewpoints each set of images is captured from. Existing state-of-the-art methods address the problem by learning view-invariant images descriptors. We propose a novel method for solving this task by exploiting the generative powers of conditional GANs to synthesize an aerial representation of a ground-level panorama query and use it to minimize the domain gap between the two views. The synthesized image being from the same view as the reference (target) image, helps the network to preserve important cues in aerial images following our Joint Feature Learning approach. We fuse the complementary features from a synthesized aerial image with the original ground-level panorama features to obtain a robust query representation. In addition, we employ multi-scale feature aggregation in order to preserve image representations at different scales useful for solving this complex task. Experimental results show that our proposed approach performs significantly better than the state-of-the-art methods on the challenging CVUSA dataset in terms of top-1 and top-1% retrieval accuracies. Furthermore, we evaluate the generalization of the proposed method for urban landscapes on our newly collected cross-view localization dataset with geo-reference information."
Bridging the Gap Between Detection and Tracking: A Unified Approach,"Lianghua Huang, Xin Zhao, Kaiqi Huang","CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China",100.0,china,0.0,,"Object detection models have been a source of inspiration for many tracking-by-detection algorithms over the past decade. Recent deep trackers borrow designs or modules from the latest object detection methods, such as bounding box regression, RPN and ROI pooling, and can deliver impressive performance. In this paper, instead of redesigning a new tracking-by-detection algorithm, we aim to explore a general framework for building trackers directly upon almost any advanced object detector. To achieve this, three key gaps must be bridged: (1) Object detectors are class-specific, while trackers are class-agnostic. (2) Object detectors do not differentiate intra-class instances, while this is a critical capability of a tracker. (3) Temporal cues are important for stable long-term tracking while they are not considered in still-image detectors. To address the above issues, we first present a simple target-guidance module for guiding the detector to locate target-relevant objects. Then a meta-learner is adopted for the detector to fast learn and adapt a target-distractor classifier online. We further introduce an anchored updating strategy to alleviate the problem of overfitting. The framework is instantiated on SSD and FasterRCNN, the typical one- and two-stage detectors, respectively. Experiments on OTB, UAV123 and NfS have verified our framework and show that our trackers can benefit from deeper backbone networks, as opposed to many recent trackers.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008262/,"['Detectors', 'Object detection', 'Target tracking', 'Task analysis', 'Feature extraction', 'Optimization', 'Training']","['Object Detection', 'Detection Model', 'Bounding Box', 'Backbone Network', 'Region Proposal Network', 'Object Detection Model', 'Temporal Cues', 'Bounding Box Regression', 'Two-stage Detectors', 'Fine-tuned', 'Learning Algorithms', 'Multiple Layers', 'Target Features', 'Iterative Optimization', 'Detection Parameters', 'Challenging Dataset', 'Support Set', 'Query Set', 'Object Detection Task', 'Basic Detection', 'Detection Head', 'Exemplary Images', 'Few-shot Learning', 'Change In Aspect Ratio', 'Large Training Data', 'Query Image', 'Classification Head', 'Meta Learning', 'Correlation Filter', 'Scale Changes']",,38,"Object detection models have been a source of inspiration for many tracking-by-detection algorithms over the past decade. Recent deep trackers borrow designs or modules from the latest object detection methods, such as bounding box regression, RPN and ROI pooling, and can deliver impressive performance. In this paper, instead of redesigning a new tracking-by-detection algorithm, we aim to explore a general framework for building trackers directly upon almost any advanced object detector. To achieve this, three key gaps must be bridged: (1) Object detectors are class-specific, while trackers are class-agnostic. (2) Object detectors do not differentiate intra-class instances, while this is a critical capability of a tracker. (3) Temporal cues are important for stable long-term tracking while they are not considered in still-image detectors. To address the above issues, we first present a simple target-guidance module for guiding the detector to locate target-relevant objects. Then a meta-learner is adopted for the detector to fast learn and adapt a target-distractor classifier online. We further introduce an anchored updating strategy to alleviate the problem of overfitting. The framework is instantiated on SSD and FasterRCNN, the typical one- and two-stage detectors, respectively. Experiments on OTB, UAV123 and NfS have verified our framework and show that our trackers can benefit from deeper backbone networks, as opposed to many recent trackers."
Budget-Aware Adapters for Multi-Domain Learning,"Rodrigo Berriel, StÃ©phane LathuillÃ¨re, Moin Nabi, Tassilo Klein, Thiago Oliveira-Santos, Nicu Sebe, Elisa Ricci","DISI, University of Trento; LCAD, UFES; DISI, University of Trento and Fondazione Bruno Kessler; SAP ML Research",50.0,italy,50.0,USA,"Multi-Domain Learning (MDL) refers to the problem of learning a set of models derived from a common deep architecture, each one specialized to perform a task in a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL with a particular interest in obtaining domain-specific models with an adjustable budget in terms of the number of network parameters and computational complexity. Our intuition is that, as in real applications the number of domains and tasks can be very large, an effective MDL approach should not only focus on accuracy but also on having as few parameters as possible. To implement this idea we derive specialized deep models for each domain by adapting a pre-trained architecture but, differently from other methods, we propose a novel strategy to automatically adjust the computational complexity of the network. To this aim, we introduce Budget-Aware Adapters that select the most relevant feature channels to better handle data from a novel domain. Some constraints on the number of active switches are imposed in order to obtain a network respecting the desired complexity budget. Experimentally, we show that our approach leads to recognition accuracy competitive with state-of-the-art approaches but with much lighter networks both in terms of storage and computation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Berriel_Budget-Aware_Adapters_for_Multi-Domain_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Berriel_Budget-Aware_Adapters_for_Multi-Domain_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009060/,"['Task analysis', 'Adaptation models', 'Computer architecture', 'Computational modeling', 'Convolution', 'Computational complexity']","['Multi-domain Learning', 'Computational Complexity', 'Network Parameters', 'Deep Architecture', 'Number Of Network Parameters', 'Convolutional Neural Network', 'Convolutional Layers', 'Resource Constraints', 'Training Procedure', 'Binary Data', 'ImageNet', 'Stochastic Gradient Descent', 'Lifelong Learning', 'Residual Block', 'Pre-trained Network', 'Low Computational Complexity', 'Constrained Optimization Problem', 'Floating-point Operations', 'Memory Footprint', 'Catastrophic Forgetting', 'Fewer Operations', 'Computational Graph', 'Tight Budget']",,22,"Multi-Domain Learning (MDL) refers to the problem of learning a set of models derived from a common deep architecture, each one specialized to perform a task in a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL with a particular interest in obtaining domain-specific models with an adjustable budget in terms of the number of network parameters and computational complexity. Our intuition is that, as in real applications the number of domains and tasks can be very large, an effective MDL approach should not only focus on accuracy but also on having as few parameters as possible. To implement this idea we derive specialized deep models for each domain by adapting a pre-trained architecture but, differently from other methods, we propose a novel strategy to automatically adjust the computational complexity of the network. To this aim, we introduce Budget-Aware Adapters that select the most relevant feature channels to better handle data from a novel domain. Some constraints on the number of active switches are imposed in order to obtain a network respecting the desired complexity budget. Experimentally, we show that our approach leads to recognition accuracy competitive with state-of-the-art approaches but with much lighter networks both in terms of storage and computation."
C-MIDN: Coupled Multiple Instance Detection Network With Segmentation Guidance for Weakly Supervised Object Detection,"Yan Gao, Boxiao Liu, Nan Guo, Xiaochun Ye, Fang Wan, Haihang You, Dongrui Fan","University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China",100.0,china,0.0,,"Weakly supervised object detection (WSOD) that only needs image-level annotations has obtained much attention recently. By combining convolutional neural network with multiple instance learning method, Multiple Instance Detection Network (MIDN) has become the most popular method to address the WSOD problem and been adopted as the initial model in many works. We argue that MIDN inclines to converge to the most discriminative object parts, which limits the performance of methods based on it. In this paper, we propose a novel Coupled Multiple Instance Detection Network (C-MIDN) to address this problem. Specifically, we use a pair of MIDNs, which work in a complementary manner with proposal removal. The localization information of the MIDNs is further coupled to obtain tighter bounding boxes and localize multiple objects. We also introduce a Segmentation Guided Proposal Removal (SGPR) algorithm to guarantee the MIL constraint after the removal and ensure the robustness of C-MIDN. Through a simple implementation of the C-MIDN with online detector refinement, we obtain 53.6% and 50.3% mAP on the challenging PASCAL VOC 2007 and 2012 benchmarks respectively, which significantly outperform the previous state-of-the-arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_C-MIDN_Coupled_Multiple_Instance_Detection_Network_With_Segmentation_Guidance_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_C-MIDN_Coupled_Multiple_Instance_Detection_Network_With_Segmentation_Guidance_for_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009008/,"['Proposals', 'Detectors', 'Image segmentation', 'Semantics', 'Couplings', 'Training', 'Feature extraction']","['Object Detection', 'Convolutional Neural Network', 'Bounding Box', 'Object Parts', 'Coupling Network', 'Multiple Instance Learning', 'Detection Results', 'Image Object', 'Semantic Segmentation', 'Segmentation Map', 'Faster R-CNN', 'Class Instances', 'Complementary Way', 'Object Regions', 'Coverage Threshold', 'Bounding Box Annotations', 'Entire Object', 'MS COCO Dataset', 'Object Proposals', 'Set Of Proposals', 'IoU Threshold']",,33,"Weakly supervised object detection (WSOD) that only needs image-level annotations has obtained much attention recently. By combining convolutional neural network with multiple instance learning method, Multiple Instance Detection Network (MIDN) has become the most popular method to address the WSOD problem and been adopted as the initial model in many works. We argue that MIDN inclines to converge to the most discriminative object parts, which limits the performance of methods based on it. In this paper, we propose a novel Coupled Multiple Instance Detection Network (C-MIDN) to address this problem. Specifically, we use a pair of MIDNs, which work in a complementary manner with proposal removal. The localization information of the MIDNs is further coupled to obtain tighter bounding boxes and localize multiple objects. We also introduce a Segmentation Guided Proposal Removal (SGPR) algorithm to guarantee the MIL constraint after the removal and ensure the robustness of C-MIDN. Through a simple implementation of the C-MIDN with online detector refinement, we obtain 53.6% and 50.3% mAP on the challenging PASCAL VOC 2007 and 2012 benchmarks respectively, which significantly outperform the previous state-of-the-arts."
C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion,"David Novotny, Nikhila Ravi, Benjamin Graham, Natalia Neverova, Andrea Vedaldi",Facebook AI Research,0.0,,100.0,USA,"We propose C3DPO, a method for extracting 3D models of deformable objects from 2D keypoint annotations in unconstrained images. We do so by learning a deep network that reconstructs a 3D object from a single view at a time, accounting for partial occlusions, and explicitly factoring the effects of viewpoint changes and object deformations. In order to achieve this factorization, we introduce a novel regularization technique. We first show that the factorization is successful if, and only if, there exists a certain canonicalization function of the reconstructed shapes. Then, we learn the canonicalization function together with the reconstruction one, which constrains the result to be consistent. We demonstrate state-of-the-art reconstruction results for methods that do not use ground-truth 3D supervision for a number of benchmarks, including Up3D and PASCAL3D+.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009035/,"['Three-dimensional displays', 'Shape', 'Two dimensional displays', 'Image reconstruction', 'Cameras', 'Solid modeling', 'Deformable models']","['Structure From Motion', '3D Pose', 'Non-rigid Structure', 'Single View', 'Viewpoint Changes', 'Deformable Objects', 'Ground Truth 3D', '2D Keypoints', 'Ambiguity', 'Deep Learning', 'Observed Values', '3D Reconstruction', 'Matrix Factorization', 'Row Vector', 'Point Cloud', 'Multiple Images', '3D Shape', 'Pose Estimation', 'Rigid Transformation', 'Network Reconstruction', 'Human Pose Estimation', 'Intra-class Variance', 'Pose Parameters', 'Public Code', 'Objective View', 'CAD Model', 'Strong Baseline', 'Random Rotation', 'Camera Intrinsic Parameters']",,69,"We propose C3DPO, a method for extracting 3D models of deformable objects from 2D keypoint annotations in unconstrained images. We do so by learning a deep network that reconstructs a 3D object from a single view at a time, accounting for partial occlusions, and explicitly factoring the effects of viewpoint changes and object deformations. In order to achieve this factorization, we introduce a novel regularization technique. We first show that the factorization is successful if, and only if, there exists a certain canonicalization function of the reconstructed shapes. Then, we learn the canonicalization function together with the reconstruction one, which constrains the result to be consistent. We demonstrate state-of-the-art reconstruction results for methods that do not use ground-truth 3D supervision for a number of benchmarks, including Up3D and PASCAL3D+."
CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation,"Gang Xu,  Zhigang Song,  Zhuo Sun,  Calvin Ku,  Zhe Yang,  Cancheng Liu,  Shuhao Wang,  Jianpeng Ma,  Wei Xu",Thorough Images; Tsinghua University; The Chinese PLA General Hospital; Fudan University; Tsinghua University/Thorough Images,60.0,China,40.0,USA,"Histopathology image analysis plays a critical role in cancer diagnosis and treatment. To automatically segment the cancerous regions, fully supervised segmentation algorithms require labor-intensive and time-consuming labeling at the pixel level. In this research, we propose CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels. Using multiple instance learning (MIL)-based label enrichment, CAMEL splits the image into latticed instances and automatically generates instance-level labels. After label enrichment, the instance-level labels are further assigned to the corresponding pixels, producing the approximate pixel-level labels and making fully supervised training of segmentation models possible. CAMEL achieves comparable performance with the fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset. Moreover, the generality of the automatic labeling methodology may benefit future weakly supervised learning studies for histopathology image analysis.",http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_CAMEL_A_Weakly_Supervised_Learning_Framework_for_Histopathology_Image_Segmentation_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_CAMEL_A_Weakly_Supervised_Learning_Framework_for_Histopathology_Image_Segmentation_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9008367/,"['Image segmentation', 'Supervised learning', 'Training', 'Image analysis', 'Cancer', 'Labeling', 'Feature extraction']","['Supervised Learning', 'Image Segmentation', 'Histopathological Images', 'Segmentation Framework', 'Histopathology Image Segmentation', 'Cancer Diagnosis', 'Segmentation Model', 'Pixel Level', 'Multiple Instance Learning', 'Image-level Labels', 'Pixel-level Labels', 'Training Set', 'High Specificity', 'Deep Neural Network', 'Scaling Factor', 'Computer Vision', 'Low Specificity', 'Adam Optimizer', 'Intersection Over Union', 'Segmentation Performance', 'Supervision Information', 'Digital Pathology', 'Instance Selection', 'Finer Granularity', 'Lack Of Sufficient Information', 'Morphological Appearance', 'Large-scale Factors']",,80,"Histopathology image analysis plays a critical role in cancer diagnosis and treatment. To automatically segment the cancerous regions, fully supervised segmentation algorithms require labor-intensive and time-consuming labeling at the pixel level. In this research, we propose CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels. Using multiple instance learning (MIL)-based label enrichment, CAMEL splits the image into latticed instances and automatically generates instance-level labels. After label enrichment, the instance-level labels are further assigned to the corresponding pixels, producing the approximate pixel-level labels and making fully supervised training of segmentation models possible. CAMEL achieves comparable performance with the fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset. Moreover, the generality of the automatic labeling methodology may benefit future weakly supervised learning studies for histopathology image analysis."
CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval,"Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, Jing Shao","CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong; Beihang University; SenseTime Research",66.66666666666666,"Hong Kong, china",33.33333333333334,China,"Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.pdf,,https://github.com/ZihaoWang-CV/CAMP_iccv19,,main,Poster,https://ieeexplore.ieee.org/document/9010827/,"['Message passing', 'Visualization', 'Logic gates', 'Adaptation models', 'Feature extraction', 'Aggregates', 'Task analysis']","['Message Passing', 'Image-text Retrieval', 'Image Regions', 'Cross-entropy Loss', 'Latent Space', 'Joint Space', 'Words In Sentences', 'Matching Score', 'Salient Information', 'Field Task', 'Cross-modal Interactions', 'Field Of Language', 'Joint Embedding', 'Image Features', 'Image Registration', 'Regional Characteristics', 'Visual Features', 'Original Features', 'Visual Modality', 'Textual Features', 'Word Features', 'Text Modality', 'Ranking Loss', 'Visual Question Answering', 'Matching Loss', 'N Words', 'Fusion Operation', 'Feature Distance', 'Supervision Training', 'Alignment Information']",,176,"Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach."
CARAFE: Content-Aware ReAssembly of FEatures,"Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, Dahua Lin","Nanyang Technological University; CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong",100.0,"Hong Kong, Singapore, china",0.0,,"Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2% AP, 1.3% AP, 1.8% mIoU, 1.1dB respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. Code and models are available at https://github.com/open-mmlab/mmdetection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.pdf,,https://github.com/open-mmlab/mmdetection,,main,Oral,https://ieeexplore.ieee.org/document/9010830/,"['Kernel', 'Task analysis', 'Convolution', 'Semantics', 'Interpolation', 'Deconvolution', 'Image segmentation']","['Object Detection', 'Large Field', 'Receptive Field', 'Computational Overhead', 'Bilinear Interpolation', 'Feature Pyramid', 'Standard Benchmark', 'Large Receptive Field', 'Key Operations', 'Convolutional Layers', 'Feature Maps', 'Input Features', 'Multiple Scales', 'Super-resolution', 'Semantic Segmentation', 'Spatial Attention', 'Content Features', 'Faster R-CNN', 'Instance Segmentation', 'Feature Map Size', 'Image Inpainting', 'Feature Pyramid Network', 'Mask R-CNN', 'Dynamic Filter', 'Pyramid Pooling Module', 'Nearest Neighbor Interpolation', 'Deconvolutional Layers', 'High-resolution Feature Maps', 'Larger Kernel Size', 'Submodule']",,376,"Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2% AP, 1.3% AP, 1.8% mIoU, 1.1dB respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. Code and models are available at https://github.com/open-mmlab/mmdetection."
CCNet: Criss-Cross Attention for Semantic Segmentation,"Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu","ReLER, UTS; School of EIC, Huazhong University of Science and Technology; Horizon Robotics",66.66666666666666,"australia, china",33.33333333333334,China,"Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.pdf,,https://github.com/speedinghzl/CCNet,,main,Poster,https://ieeexplore.ieee.org/document/9133304/,"['Semantics', 'Task analysis', 'Aggregates', 'Benchmark testing', 'Lips', 'Image segmentation', 'Context modeling']","['Semantic Segmentation', 'Criss-cross Network', 'Validation Set', 'Efficient Way', 'Contextual Information', 'Extensive Experiments', 'Discriminative Features', 'Attention Module', 'Instance Segmentation', 'GPU Memory', 'Segmentation Benchmark', 'Non-local Block', 'Training Set', 'Convolutional Network', 'Convolutional Neural Network', 'Convolutional Layers', 'Local Features', 'Feature Maps', 'Vertical Direction', 'Segmentation Results', 'Attention Map', 'Graph Neural Networks', 'Piecewise Function', 'COCO Dataset', 'Fully Convolutional Network', 'Dilated Convolution', 'Conditional Random Field', 'Output Feature Map', 'Atrous Spatial Pyramid Pooling', 'Markov Random Field']","['Semantic segmentation', 'graph attention', 'criss-cross network', 'context modeling']",231,"Contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a criss-cross network (CCNet) for obtaining full-image contextual information in a very effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 
<inline-formula><tex-math notation=""LaTeX"">$11\times$</tex-math></inline-formula>
 less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85 percent of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9, 45.76 and 55.47 percent on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at 
<uri>https://github.com/speedinghzl/CCNethttps://github.com/speedinghzl/CCNet</uri>
."
CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation,"Zhigang Li, Gu Wang, Xiangyang Ji","Tsinghua University, Beijing, China",100.0,China,0.0,,"6-DoF object pose estimation from a single RGB image is a fundamental and long-standing problem in computer vision. Current leading approaches solve it by training deep networks to either regress both rotation and translation from image directly or to construct 2D-3D correspondences and further solve them via PnP indirectly. We argue that rotation and translation should be treated differently for their significant difference. In this work, we propose a novel 6-DoF pose estimation approach: Coordinates-based Disentangled Pose Network (CDPN), which disentangles the pose to predict rotation and translation separately to achieve highly accurate and robust pose estimation. Our method is flexible, efficient, highly accurate and can deal with texture-less and occluded objects. Extensive experiments on LINEMOD and Occlusion datasets are conducted and demonstrate the superiority of our approach. Concretely, our approach significantly exceeds the state-of-the- art RGB-based methods on commonly used metrics.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009519/,"['Pose estimation', 'Three-dimensional displays', 'Detectors', 'Robustness', 'Real-time systems', 'Two dimensional displays']","['Pose Estimation', 'Human Pose Estimation', 'Object Pose', 'Pose Network', '6-DoF Object Pose', '6DoF Object Pose Estimation', 'Deep Network', 'Single Image', 'RGB Images', 'Computer Vision Problems', 'Occluded Objects', 'Training Set', 'Image Object', 'Bounding Box', 'Direct Approach', 'Semantic Segmentation', 'Object Location', 'Object Size', 'Error Detection', '3D Coordinates', 'Iterative Closest Point', 'Object Coordinates', 'Confidence Map', 'Unique Network', 'Translation Accuracy', 'Indirect Estimates', 'Ground Truth Pose', '2D Projection', 'RGB-D Images', 'Central Objective']",,303,"6-DoF object pose estimation from a single RGB image is a fundamental and long-standing problem in computer vision. Current leading approaches solve it by training deep networks to either regress both rotation and translation from image directly or to construct 2D-3D correspondences and further solve them via PnP indirectly. We argue that rotation and translation should be treated differently for their significant difference. In this work, we propose a novel 6-DoF pose estimation approach: Coordinates-based Disentangled Pose Network (CDPN), which disentangles the pose to predict rotation and translation separately to achieve highly accurate and robust pose estimation. Our method is flexible, efficient, highly accurate and can deal with texture-less and occluded objects. Extensive experiments on LINEMOD and Occlusion datasets are conducted and demonstrate the superiority of our approach. Concretely, our approach significantly exceeds the state-of-the- art RGB-based methods on commonly used metrics."
CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark,"Alan LukeÅ¾iÄ, Ugur Kart, Jani KÃ¤pylÃ¤, Ahmed Durmush, Joni-Kristian KÃ¤mÃ¤rÃ¤inen, JiÅÃ­ Matas, Matej Kristan","Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic; Computing Sciences, Tampere University, Finland; Faculty of Computer and Information Science, University of Ljubljana, Slovenia",100.0,"Czech Republic, Slovenia, finland",0.0,,"We propose a new color-and-depth general visual object tracking benchmark (CDTB). CDTB is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The CDTB dataset is the largest and most diverse dataset in RGB-D tracking, with an order of magnitude larger number of frames than related datasets. The sequences have been carefully recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. Experiments with RGB and RGB-D trackers show that CDTB is more challenging than previous datasets. State-of-the-art RGB trackers outperform the recent RGB-D trackers, indicating a large gap between the two fields, which has not been previously detected by the prior benchmarks. Based on the results of the analysis we point out opportunities for future research in RGB-D tracker design.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lukezic_CDTB_A_Color_and_Depth_Visual_Object_Tracking_Dataset_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lukezic_CDTB_A_Color_and_Depth_Visual_Object_Tracking_Dataset_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010284/,"['Target tracking', 'Cameras', 'Benchmark testing', 'Image color analysis', 'Three-dimensional displays', 'Visualization', 'Robot sensing systems']","['Object Tracking', 'Tracking Dataset', 'Visual Object Tracking', 'Tracking Benchmark', 'Object Tracking Benchmark', 'General Objective', 'Direct Sunlight', 'Pose Changes', 'Field Of View', 'Precision And Recall', 'Bounding Box', 'RGB Images', 'Deep Features', 'Depth Images', 'Tracking Performance', 'Changes In Appearance', 'Depth Information', 'Particle Filter', 'Partial Occlusion', 'Focal Length Of Lens', 'Long-term Scenarios', 'Time-of-flight Sensors', 'Precise Tracking', 'RGB-D Sensor', 'Correlation Filter', 'Long-term Tracking', 'Change In Aspect Ratio', 'RGB-D Dataset', 'Fast Motion', 'Hand Tracking']",,45,"We propose a new color-and-depth general visual object tracking benchmark (CDTB). CDTB is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The CDTB dataset is the largest and most diverse dataset in RGB-D tracking, with an order of magnitude larger number of frames than related datasets. The sequences have been carefully recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. Experiments with RGB and RGB-D trackers show that CDTB is more challenging than previous datasets. State-of-the-art RGB trackers outperform the recent RGB-D trackers, indicating a large gap between the two fields, which has not been previously detected by the prior benchmarks. Based on the results of the analysis we point out opportunities for future research in RGB-D tracker design."
CFSNet: Toward a Controllable Feature Space for Image Restoration,"Wei Wang, Ruiming Guo, Yapeng Tian, Wenming Yang",University of Rochester; Tsinghua University; Chinese University of Hong Kong,100.0,"China, Hong Kong, usa",0.0,,"Deep learning methods have witnessed the great progress in image restoration with specific metrics (e.g., PSNR, SSIM). However, the perceptual quality of the restored image is relatively subjective, and it is necessary for users to control the reconstruction result according to personal preferences or image characteristics, which cannot be done using existing deterministic networks. This motivates us to exquisitely design a unified interactive framework for general image restoration tasks. Under this framework, users can control continuous transition of different objectives, e.g., the perception-distortion trade-off of image super-resolution, the trade-off between noise reduction and detail preservation. We achieve this goal by controlling the latent features of the designed network. To be specific, our proposed framework, named Controllable Feature Space Network (CFSNet), is entangled by two branches based on different objectives. Our framework can adaptively learn the coupling coefficients of different layers and channels, which provides finer control of the restored image quality. Experiments on several typical image restoration tasks fully validate the effective benefits of the proposed method. Code is available at https://github.com/qibao77/CFSNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_CFSNet_Toward_a_Controllable_Feature_Space_for_Image_Restoration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CFSNet_Toward_a_Controllable_Feature_Space_for_Image_Restoration_ICCV_2019_paper.pdf,,https://github.com/qibao77/CFSNet,,main,Poster,https://ieeexplore.ieee.org/document/9009077/,"['Couplings', 'Image restoration', 'Task analysis', 'Image resolution', 'Tuning', 'Aerospace electronics', 'Distortion']","['Feature Space', 'Super-resolution', 'Perception Of Quality', 'Coupling Coefficient', 'Adaptive Learning', 'Fine Control', 'Reconstruction Results', 'Continuous Transition', 'Detail Preservation', 'High-quality', 'Control Variables', 'Learning Rate', 'Manifold', 'Optimization Process', 'Convolutional Layers', 'Parameter Space', 'Latent Space', 'Visual Quality', 'Vision Tasks', 'Low Distortion', 'JPEG Images', 'Level Of Degradation', 'Combination Of Coefficients', 'Perceptual Distortions', 'Control Coefficients', 'Image Transformation', 'Linear Network']",,46,"Deep learning methods have witnessed the great progress in image restoration with specific metrics (e.g., PSNR, SSIM). However, the perceptual quality of the restored image is relatively subjective, and it is necessary for users to control the reconstruction result according to personal preferences or image characteristics, which cannot be done using existing deterministic networks. This motivates us to exquisitely design a unified interactive framework for general image restoration tasks. Under this framework, users can control continuous transition of different objectives, e.g., the perception-distortion trade-off of image super-resolution, the trade-off between noise reduction and detail preservation. We achieve this goal by controlling the latent features of the designed network. To be specific, our proposed framework, named Controllable Feature Space Network (CFSNet), is entangled by two branches based on different objectives. Our framework can adaptively learn the coupling coefficients of different layers and channels, which provides finer control of the restored image quality. Experiments on several typical image restoration tasks fully validate the effective benefits of the proposed method. Code is available at https://github.com/qibao77/CFSNet."
CIIDefence: Defeating Adversarial Attacks by Fusing Class-Specific Image Inpainting and Image Denoising,"Puneet Gupta, Esa Rahtu","IIT Indore, Indore, India; Tampere University, Finland",100.0,"India, finland",0.0,,"This paper presents a novel approach for protecting deep neural networks from adversarial attacks, i.e., methods that add well-crafted imperceptible modifications to the original inputs such that they are incorrectly classified with high confidence. The proposed defence mechanism is inspired by the recent works mitigating the adversarial disturbances by the means of image reconstruction and denoising. However, unlike the previous works, we apply the reconstruction only for small and carefully selected image areas that are most influential to the current classification outcome. The selection process is guided by the class activation map responses obtained for multiple top-ranking class labels. The same regions are also the most prominent for the adversarial perturbations and hence most important to purify. The resulting inpainting task is substantially more tractable than the full image reconstruction, while still being able to prevent the adversarial attacks. Furthermore, we combine the selective image inpainting with wavelet based image denoising to produce a non differentiable layer that prevents attacker from using gradient backpropagation. Moreover, the proposed nonlinearity cannot be easily approximated with simple differentiable alternative as demonstrated in the experiments with Backward Pass Differentiable Approximation (BPDA) attack. Finally, we experimentally show that the proposed Class-specific Image Inpainting Defence (CIIDefence) is able to withstand several powerful adversarial attacks including the BPDA. The obtained results are consistently better compared to the other recent defence approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class-Specific_Image_Inpainting_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class-Specific_Image_Inpainting_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010982/,"['Image reconstruction', 'Perturbation methods', 'Generative adversarial networks', 'Gallium nitride', 'Image denoising', 'Noise reduction', 'Neural networks']","['Adversarial Attacks', 'Image Inpainting', 'Class-specific Image', 'Deep Neural Network', 'Backpropagation', 'Image Reconstruction', 'Image Area', 'Class Labels', 'Original Input', 'Class Activation Maps', 'Adversarial Perturbations', 'Training Set', 'Hyperparameters', 'Classification Accuracy', 'Classification Performance', 'Input Image', 'Image Classification', 'Generative Adversarial Networks', 'Target Class', 'Clear Image', 'Fast Gradient Sign Method', 'Adversarial Examples', 'Projected Gradient Descent', 'JPEG Compression', 'Global Max Pooling', 'Local Image Features', 'CNN Layers', 'Gradient Technique', 'Image Patch Size', 'Failure Cases']",,27,"This paper presents a novel approach for protecting deep neural networks from adversarial attacks, i.e., methods that add well-crafted imperceptible modifications to the original inputs such that they are incorrectly classified with high confidence. The proposed defence mechanism is inspired by the recent works mitigating the adversarial disturbances by the means of image reconstruction and denoising. However, unlike the previous works, we apply the reconstruction only for small and carefully selected image areas that are most influential to the current classification outcome. The selection process is guided by the class activation map responses obtained for multiple top-ranking class labels. The same regions are also the most prominent for the adversarial perturbations and hence most important to purify. The resulting inpainting task is substantially more tractable than the full image reconstruction, while still being able to prevent the adversarial attacks. Furthermore, we combine the selective image inpainting with wavelet based image denoising to produce a non differentiable layer that prevents attacker from using gradient backpropagation. Moreover, the proposed nonlinearity cannot be easily approximated with simple differentiable alternative as demonstrated in the experiments with Backward Pass Differentiable Approximation (BPDA) attack. Finally, we experimentally show that the proposed Class-specific Image Inpainting Defence (CIIDefence) is able to withstand several powerful adversarial attacks including the BPDA. The obtained results are consistently better compared to the other recent defence approaches."
COCO-GAN: Generation by Parts via Conditional Coordinating,"Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen",National Taiwan University; Google AI; National Tsing Hua University,66.66666666666666,taiwan,33.33333333333334,USA,"Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose COnditional COordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never manipulated during training, we show that COCO-GAN can produce state-of-the-art-quality full images during inference. We further demonstrate a variety of novel applications enabled by our coordinate-aware framework. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called ""beyond-boundary generation"". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_COCO-GAN_Generation_by_Parts_via_Conditional_Coordinating_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_COCO-GAN_Generation_by_Parts_via_Conditional_Coordinating_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010947/,"['Generators', 'Training', 'Gallium nitride', 'Manifolds', 'Image edge detection', 'Computational modeling', 'Task analysis']","['Coordinate System', 'Generative Adversarial Networks', 'Spatial Coordinates', 'Cylindrical Coordinates', 'Conditional Generative Adversarial Network', 'Cylindrical Coordinate System', 'Divide-and-conquer Approach', 'Global Coherence', 'Interpolation', 'High-resolution Images', 'Horizontal Plane', 'Convergence Rate', 'Image Generation', 'High-quality Images', 'Patch Size', 'Discrete Set', 'Latent Vector', 'Image X', 'Adjacent Patches', 'Multiple Patches', 'Local View', 'Coordination State', 'Wasserstein Distance', 'Optimal Discrimination', 'Mercator Projection', 'Merge Function']",,71,"Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose COnditional COordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never manipulated during training, we show that COCO-GAN can produce state-of-the-art-quality full images during inference. We further demonstrate a variety of novel applications enabled by our coordinate-aware framework. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called ""beyond-boundary generation"". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand."
Calibration Wizard: A Guidance System for Camera Calibration Based on Modelling Geometric and Corner Uncertainty,"Songyou Peng, Peter Sturm",ETH Zurich; INRIA Grenoble – Rhône-Alpes,100.0,"France, switzerland",0.0,,"It is well known that the accuracy of a calibration depends strongly on the choice of camera poses from which images of a calibration object are acquired. We present a system -- Calibration Wizard -- that interactively guides a user towards taking optimal calibration images. For each new image to be taken, the system computes, from all previously acquired images, the pose that leads to the globally maximum reduction of expected uncertainty on intrinsic parameters and then guides the user towards that pose. We also show how to incorporate uncertainty in corner point position in a novel principled manner, for both, calibration and computation of the next best pose. Synthetic and real-world experiments are performed to demonstrate the effectiveness of Calibration Wizard.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Calibration_Wizard_A_Guidance_System_for_Camera_Calibration_Based_on_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Calibration_Wizard_A_Guidance_System_for_Camera_Calibration_Based_on_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009540/,"['Calibration', 'Cameras', 'Uncertainty', 'Covariance matrices', 'Three-dimensional displays', 'Optimization', 'Computational modeling']","['Camera Calibration', 'Real-world Experiments', 'Intrinsic Parameters', 'Camera Pose', 'Corner Points', 'Image Calibration', 'Synthetic Experiments', 'Field Of View', 'Covariance Matrix', 'Gaussian Noise', '3D Reconstruction', 'Focal Length', 'Nonlinear Programming', 'Image Distortion', '3D Point', 'Fisher Information', 'Pose Estimation', 'Open Angle', 'Depth Of Field', 'Calibration Results', 'Autocorrelation Matrix', 'Calibration Quality', 'Random Images', 'Calibration Target', 'Extrinsic Parameters', 'Bundle Adjustment', 'Monocular Camera', 'Calibration Process', 'Partial Differential', 'Camera Model']",,24,"It is well known that the accuracy of a calibration depends strongly on the choice of camera poses from which images of a calibration object are acquired. We present a system - Calibration Wizard - that interactively guides a user towards taking optimal calibration images. For each new image to be taken, the system computes, from all previously acquired images, the pose that leads to the globally maximum reduction of expected uncertainty on intrinsic parameters and then guides the user towards that pose. We also show how to incorporate uncertainty in corner point position in a novel principled manner, for both, calibration and computation of the next best pose. Synthetic and real-world experiments are performed to demonstrate the effectiveness of Calibration Wizard."
Calibration of Axial Fisheye Cameras Through Generic Virtual Central Models,"Pierre-AndrÃ© Brousseau, SÃ©bastien Roy","Département d’informatique et de recherche opérationnelle, Université de Montréal",100.0,canada,0.0,,"Fisheye cameras are notoriously hard to calibrate using traditional plane-based methods. This paper proposes a new calibration method for large field of view cameras. Similarly to planar calibration, it relies on multiple images of a planar calibration grid with dense correspondences, typically obtained using structured light. By relying on the grids themselves instead of the distorted image plane, we can build a rectilinear Generic Virtual Central (GVC) camera. Instead of relying on a single GVC camera, our method proposes a selection of multiple GVC cameras which can cover any field of view and be trivially aligned to provide a very accurate generic central model. We demonstrate that this approach can directly model axial cameras, assuming the distortion center is located on the camera axis. Experimental validation is provided on both synthetic and real fisheye cameras featuring up to a 280deg field of view. To our knowledge, this is one of the only practical methods to calibrate axial cameras.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Brousseau_Calibration_of_Axial_Fisheye_Cameras_Through_Generic_Virtual_Central_Models_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Brousseau_Calibration_of_Axial_Fisheye_Cameras_Through_Generic_Virtual_Central_Models_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010986/,"['Cameras', 'Calibration', 'Lenses', 'Optical imaging', 'Three-dimensional displays', 'Distortion', 'Optical distortion']","['Camera Calibration', 'Fisheye Lens', 'Field Of View', 'Calibration Method', 'Structured Illumination', 'Large Field Of View', 'Multiple Cameras', 'Dense Correspondence', 'Linear System', 'Lookup Table', 'Neighboring Nodes', 'Image Distortion', 'Image Point', 'Model Projections', '3D Point', 'Radially Symmetric', 'LCD Monitor', 'Principal Plane', 'Camera Model', 'Axial Displacement', 'Virtual Camera', 'Reprojection Error', 'Camera Features', 'Common Axis', 'Displacement Of The Center', 'Homography', 'Single Grid', 'Reference Plane']",,11,"Fisheye cameras are notoriously hard to calibrate using traditional plane-based methods. This paper proposes a new calibration method for large field of view cameras. Similarly to planar calibration, it relies on multiple images of a planar calibration grid with dense correspondences, typically obtained using structured light. By relying on the grids themselves instead of the distorted image plane, we can build a rectilinear Generic Virtual Central (GVC) camera. Instead of relying on a single GVC camera, our method proposes a selection of multiple GVC cameras which can cover any field of view and be trivially aligned to provide a very accurate generic central model. We demonstrate that this approach can directly model axial cameras, assuming the distortion center is located on the camera axis. Experimental validation is provided on both synthetic and real fisheye cameras featuring up to a 280deg field of view. To our knowledge, this is one of the only practical methods to calibrate axial cameras."
CamNet: Coarse-to-Fine Retrieval for Camera Re-Localization,"Mingyu Ding, Zhe Wang, Jiankai Sun, Jianping Shi, Ping Luo",The Chinese University of Hong Kong; The University of Hong Kong; SenseTime Research,66.66666666666666,Hong Kong,33.33333333333334,China,"Camera re-localization is an important but challenging task in applications like robotics and autonomous driving. Recently, retrieval-based methods have been considered as a promising direction as they can be easily generalized to novel scenes. Despite significant progress has been made, we observe that the performance bottleneck of previous methods actually lies in the retrieval module. These methods use the same features for both retrieval and relative pose regression tasks which have potential conflicts in learning. To this end, here we present a coarse-to-fine retrieval-based deep learning framework, which includes three steps, i.e., image-based coarse retrieval, pose-based fine retrieval and precise relative pose regression. With our carefully designed retrieval module, the relative pose regression task can be surprisingly simpler. We design novel retrieval losses with batch hard sampling criterion and two-stage retrieval to locate samples that adapt to the relative pose regression task. Extensive experiments show that our model (CamNet) outperforms the state-of-the-art methods by a large margin on both indoor and outdoor datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.pdf,,,,main,Poster,http://ieeexplore.ieee.org/document/9008579/,"['Cameras', 'Three-dimensional displays', 'Solid modeling', 'Task analysis', 'Predictive models', 'Training', 'Artificial neural networks']","['Camera Relocalization', 'Deep Learning', 'Large Margin', 'Related Tasks', 'Relative Pose', 'Loss Function', 'Convolutional Neural Network', 'Deep Models', 'Benchmark Datasets', 'Image Pairs', 'RGB Images', 'Depth Camera', 'Pose Estimation', 'Image Retrieval', 'Nearest Neighbor Search', 'Image Descriptors', '3D Scene', 'Structure From Motion', 'Camera Pose', 'Image-based Methods', 'Retrieval Results', 'Query Image', 'FC Layer', 'Inliers', 'Camera Orientation', 'Random Forest', 'Descriptive Characteristics', 'Encoder Network', 'Indoor Environments', 'Bilateral Loss']",,79,"Camera re-localization is an important but challenging task in applications like robotics and autonomous driving. Recently, retrieval-based methods have been considered as a promising direction as they can be easily generalized to novel scenes. Despite significant progress has been made, we observe that the performance bottleneck of previous methods actually lies in the retrieval module. These methods use the same features for both retrieval and relative pose regression tasks which have potential conflicts in learning. To this end, here we present a coarse-to-fine retrieval-based deep learning framework, which includes three steps, i.e., image-based coarse retrieval, pose-based fine retrieval and precise relative pose regression. With our carefully designed retrieval module, the relative pose regression task can be surprisingly simpler. We design novel retrieval losses with batch hard sampling criterion and two-stage retrieval to locate samples that adapt to the relative pose regression task. Extensive experiments show that our model (CamNet) outperforms the state-of-the-art methods by a large margin on both indoor and outdoor datasets."
Camera Distance-Aware Top-Down Approach for 3D Multi-Person Pose Estimation From a Single RGB Image,"Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee","ECE & ASRI, Seoul National University, Korea; ECE, Kwangwoon University, Korea",100.0,"South Korea, south korea",0.0,,"Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in (https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE) , (https://github.com/mks0601/3DMPPE_POSENET_RELEASE).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Moon_Camera_Distance-Aware_Top-Down_Approach_for_3D_Multi-Person_Pose_Estimation_From_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Moon_Camera_Distance-Aware_Top-Down_Approach_for_3D_Multi-Person_Pose_Estimation_From_ICCV_2019_paper.pdf,,https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE,,main,Poster,https://ieeexplore.ieee.org/document/9010999/,"['Three-dimensional displays', 'Pose estimation', 'Two dimensional displays', 'Cameras', 'Pipelines', 'Feature extraction', 'Heating systems']","['Single Image', 'Top-down Approach', 'Pose Estimation', 'Human Pose Estimation', '3D Pose', 'Multi-person Pose Estimation', 'Previous Calculations', '3D Position', 'Ground Truth Information', 'Pose Estimation Methods', 'Convolutional Neural Network', 'Image Features', 'Input Image', 'Feature Maps', 'Object Detection', 'Bounding Box', 'Image Space', 'Inference Time', 'Depth Values', 'Batch Normalization Layer', '2D Pose', 'Cropped Images', 'Mask R-CNN', 'Joint Learning', 'Ground-truth Box', 'Final 3D', 'COCO Dataset', '3D Method', 'ReLU Activation Function', 'Accuracy Of Pose Estimation']",,206,"Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any ground truth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1,2</sup>
."
Canonical Surface Mapping via Geometric Cycle Consistency,"Nilesh Kulkarni, Abhinav Gupta, Shubham Tulsiani",Carnegie Mellon University; Facebook AI Research,50.0,usa,50.0,USA,"We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kulkarni_Canonical_Surface_Mapping_via_Geometric_Cycle_Consistency_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kulkarni_Canonical_Surface_Mapping_via_Geometric_Cycle_Consistency_ICCV_2019_paper.pdf,,https://nileshkulkarni.github.io/csm/,,main,Poster,https://ieeexplore.ieee.org/document/9010377/,"['Three-dimensional displays', 'Task analysis', 'Shape', 'Cameras', 'Training', 'Semantics', 'Solid modeling']","['Surface Mapping', 'Cycle Consistency', 'Canonical Map', 'Geometric Consistency', 'Key Insights', 'Manual Labeling', 'Consistency Loss', '3D Projection', 'Map Tasks', 'Dense Correspondence', 'Geometric Loss', 'General Characteristics', 'Input Image', 'Image Pixels', 'General Class', 'Parametrized', 'Image Pairs', 'Target Image', '3D Surface', 'Source Images', '3D Shape', 'Spherical Surface', 'Reprojection', 'Form Of Supervision', 'Z Coordinates', 'Training Objective', 'Appendix For Details', 'Instances Of Categories', 'Training Setup']",,57,"We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision."
Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection,"Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, Jesse Berent","Google Research, Zurich, Switzerland; Department of Computer Science, University of Pittsburgh, Pittsburgh PA, USA",50.0,usa,50.0,USA,"Learning to localize and name object instances is a fundamental problem in vision, but state-of-the-art approaches rely on expensive bounding box supervision. While weakly supervised detection (WSOD) methods relax the need for boxes to that of image-level annotations, even cheaper supervision is naturally available in the form of unstructured textual descriptions that users may freely provide when uploading image content. However, straightforward approaches to using such data for WSOD wastefully discard captions that do not exactly match object names. Instead, we show how to squeeze the most information out of these captions by training a text-only classifier that generalizes beyond dataset boundaries. Our discovery provides an opportunity for learning detection models from noisy but more abundant and freely-available caption data. We also validate our model on three classic object detection benchmarks and achieve state-of-the-art WSOD performance. Our code is available at https://github.com/yekeren/Cap2Det.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Cap2Det_Learning_to_Amplify_Weak_Caption_Supervision_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Cap2Det_Learning_to_Amplify_Weak_Caption_Supervision_for_Object_Detection_ICCV_2019_paper.pdf,,https://github.com/yekeren/Cap2Det,,main,Poster,https://ieeexplore.ieee.org/document/9008808/,"['Proposals', 'Object detection', 'Detectors', 'Training', 'Bicycles', 'Ovens', 'Natural languages']","['Object Detection', 'Detection Methods', 'Detection Model', 'Bounding Box', 'Formal Description', 'Visual Problems', 'Natural Language', 'Learning Task', 'Precision And Recall', 'ImageNet', 'Max-pooling', 'Exact Match', 'Classification Score', 'Ground Truth Labels', 'Word Embedding', 'Text Classification', 'Image X', 'Pseudo Labels', 'Object Labels', 'Weak Supervision', 'Multiple Instance Learning', 'Image-level Labels', 'Form Of Supervision', 'Free-form Text', 'Qualitative Examples', 'Natural Language Descriptions', 'Final Detection Results']",,31,"Learning to localize and name object instances is a fundamental problem in vision, but state-of-the-art approaches rely on expensive bounding box supervision. While weakly supervised detection (WSOD) methods relax the need for boxes to that of image-level annotations, even cheaper supervision is naturally available in the form of unstructured textual descriptions that users may freely provide when uploading image content. However, straightforward approaches to using such data for WSOD wastefully discard captions that do not exactly match object names. Instead, we show how to squeeze the most information out of these captions by training a text-only classifier that generalizes beyond dataset boundaries. Our discovery provides an opportunity for learning detection models from noisy but more abundant and freely-available caption data. We also validate our model on three classic object detection benchmarks and achieve state-of-the-art WSOD performance. Our code is available at https://github.com/yekeren/Cap2Det."
CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing,"Kevin Duarte, Yogesh S. Rawat, Mubarak Shah","Center for Research in Computer Vision, University of Central Florida",100.0,usa,0.0,,"In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Duarte_CapsuleVOS_Semi-Supervised_Video_Object_Segmentation_Using_Capsule_Routing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Duarte_CapsuleVOS_Semi-Supervised_Video_Object_Segmentation_Using_Capsule_Routing_ICCV_2019_paper.pdf,,https://github.com/KevinDuarte/CapsuleVOS,,main,Poster,https://ieeexplore.ieee.org/document/9010040/,"['Object segmentation', 'Routing', 'Task analysis', 'Memory modules', 'Motion segmentation', 'Optical imaging', 'Computer architecture']","['Object Segmentation', 'Video Object Segmentation', 'Semi-supervised Video Object Segmentation', 'Capsule Routing', 'Reference Frame', 'Small Objects', 'Optical Flow', 'Routing Algorithm', 'Memory Module', 'Occluded Objects', 'Capsule Network', 'Reference Segmentation', 'Ablation', 'Objective Function', 'Convolutional Layers', 'Online Learning', 'Convolution Operation', 'Bounding Box', 'Video Clips', 'Object Of Interest', 'Training Videos', 'Video Object', 'Frame Region', 'Object Parts', 'Capsule Type', 'Foreground Objects', 'Groups Of Neurons', 'Segmentation Task', 'Object In Frame']",,47,"In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS."
Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion,"Pingping Zhang, Wei Liu, Yinjie Lei, Huchuan Lu, Xiaoyun Yang",Dalian University of Technology; University of Adelaide; China Science IntelliCloud Technology Co.Ltd; Sichuan University,75.0,"australia, china",25.0,China,"Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. It helps intelligent devices to understand and interact with the surrounding scenes. Due to the high-memory requirement, current methods only produce low-resolution completion predictions, and generally lose the object details. Furthermore, they also ignore the multi-scale spatial contexts, which play a vital role for the 3D inference. To address these issues, in this work we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly infer the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The proposed CCPNet improves the labeling coherence with a cascaded context pyramid. Meanwhile, based on the low-level features, it progressively restores the fine-structures of objects with Guided Residual Refinement (GRR) modules. Our proposed framework has three outstanding advantages: (1) it explicitly models the 3D spatial context for performance improvement; (2) full-resolution 3D volumes are produced with structure-preserving details; (3) light-weight models with low-memory requirements are captured with a good extensibility. Extensive experiments demonstrate that in spite of taking a single-view depth map, our proposed framework can generate high-quality SSC results, and outperforms state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Cascaded_Context_Pyramid_for_Full-Resolution_3D_Semantic_Scene_Completion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Cascaded_Context_Pyramid_for_Full-Resolution_3D_Semantic_Scene_Completion_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008381/,"['Three-dimensional displays', 'Semantics', 'Task analysis', 'Convolution', 'Two dimensional displays', 'Labeling', 'Image segmentation']","['Scene Completion', 'Semantic Scene Completion', 'Context Pyramid', 'Depth Map', 'Low-level Features', 'Depth Images', 'Deep Learning Framework', 'Semantic Labels', '3D Scene', 'Single Depth', 'Convolutional Neural Network', 'Feature Maps', 'Intersection Over Union', 'Receptive Field', 'Semantic Segmentation', '3D Data', 'Residual Block', 'Handcrafted Features', 'Local Details', 'Shallow Layers', '3D Convolution', 'Multi-scale Context', '3D Features', 'Objects In The Scene', 'Semantic Gap', 'Dilated Convolution', 'Dilation Rate', 'Considerable Margin', 'Geometric Details', 'Complete 3D']",,47,"Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. It helps intelligent devices to understand and interact with the surrounding scenes. Due to the high-memory requirement, current methods only produce low-resolution completion predictions, and generally lose the object details. Furthermore, they also ignore the multi-scale spatial contexts, which play a vital role for the 3D inference. To address these issues, in this work we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly infer the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The proposed CCPNet improves the labeling coherence with a cascaded context pyramid. Meanwhile, based on the low-level features, it progressively restores the fine-structures of objects with Guided Residual Refinement (GRR) modules. Our proposed framework has three outstanding advantages: (1) it explicitly models the 3D spatial context for performance improvement; (2) full-resolution 3D volumes are produced with structure-preserving details; (3) light-weight models with low-memory requirements are captured with a good extensibility. Extensive experiments demonstrate that in spite of taking a single-view depth map, our proposed framework can generate high-quality SSC results, and outperforms state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets."
Cascaded Parallel Filtering for Memory-Efficient Image-Based Localization,"Wentao Cheng, Weisi Lin, Kan Chen, Xinfeng Zhang","Fraunhofer Singapore, Singapore; University of Chinese Academy of Sciences, China; Nanyang Technological University, Singapore",66.66666666666666,"Singapore, china",33.33333333333334,Singapore,"Image-based localization (IBL) aims to estimate the 6DOF camera pose for a given query image. The camera pose can be computed from 2D-3D matches between a query image and Structure-from-Motion (SfM) models. Despite recent advances in IBL, it remains difficult to simultaneously resolve the memory consumption and match ambiguity problems of large SfM models. In this work, we propose a cascaded parallel filtering method that leverages the feature, visibility and geometry information to filter wrong matches under binary feature representation. The core idea is that we divide the challenging filtering task into two parallel tasks before deriving an auxiliary camera pose for final filtering. One task focuses on preserving potentially correct matches, while another focuses on obtaining high quality matches to facilitate subsequent more powerful filtering. Moreover, our proposed method improves the localization accuracy by introducing a quality-aware spatial reconfiguration method and a principal focal length enhanced pose estimation method. Experimental results on real-world datasets demonstrate that our method achieves very competitive localization performances in a memory-efficient manner.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Cascaded_Parallel_Filtering_for_Memory-Efficient_Image-Based_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Cascaded_Parallel_Filtering_for_Memory-Efficient_Image-Based_Localization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009817/,"['Cameras', 'Three-dimensional displays', 'Visualization', 'Task analysis', 'Databases', 'Hamming distance', 'Memory management']","['Image-based Localization', 'Visual Information', 'Feature Representation', 'Localization Accuracy', 'Focal Length', 'Pose Estimation', 'Memory Consumption', 'Camera Pose', 'Correct Matches', 'Query Image', 'Geometry Information', 'Semantic', 'Ratio Test', 'Feature Space', 'Point Cloud', 'Filtering Step', 'Autonomous Vehicles', '3D Point', 'Image Database', 'Visibility Graph', 'Camera Pose Estimation', 'Matched Filter', 'Matching Score', 'Inliers', 'Hamming Distance', 'Reprojection Error', 'Bilateral Test', 'Final Pose', 'Two-step Selection', 'Uniform Spatial Distribution']",,18,"Image-based localization (IBL) aims to estimate the 6DOF camera pose for a given query image. The camera pose can be computed from 2D-3D matches between a query image and Structure-from-Motion (SfM) models. Despite recent advances in IBL, it remains difficult to simultaneously resolve the memory consumption and match ambiguity problems of large SfM models. In this work, we propose a cascaded parallel filtering method that leverages the feature, visibility and geometry information to filter wrong matches under binary feature representation. The core idea is that we divide the challenging filtering task into two parallel tasks before deriving an auxiliary camera pose for final filtering. One task focuses on preserving potentially correct matches, while another focuses on obtaining high quality matches to facilitate subsequent more powerful filtering. Moreover, our proposed method improves the localization accuracy by introducing a quality-aware spatial reconfiguration method and a principal focal length enhanced pose estimation method. Experimental results on real-world datasets demonstrate that our method achieves very competitive localization performances in a memory-efficient manner."
CenterNet: Keypoint Triplets for Object Detection,"Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian","Peng Cheng Laboratory; Huawei Noah’s Ark Lab; Huazhong University of Science and Technology; University of Chinese Academy of Sciences; Key Laboratory of Big Data Mining and Knowledge Management, UCAS",80.0,"China, china",20.0,China,"In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.pdf,,https://github.com/Duankaiwen/CenterNet,,main,Poster,https://ieeexplore.ieee.org/document/9010985/,"['Object detection', 'Heating systems', 'Visualization', 'Detectors', 'Proposals', 'Feature extraction', 'Task analysis']","['Object Detection', 'Central Region', 'Minimum Cost', 'Precision And Recall', 'Bounding Box', 'Visual Patterns', 'Top Left Corner', 'Crops In Regions', 'Inference Speed', 'Object Bounding Boxes', 'Two-stage Detectors', 'One-stage Detectors', 'MS COCO Dataset', 'Bottom-right Corner', 'False Discovery Rate', 'Heatmap', 'Convolutional Neural Network', 'Horizontal Plane', 'Input Image', 'Feature Maps', 'One-stage Approach', 'Anchor Boxes', 'Intersection Over Union', 'Small Objects', 'Intersection Over Union Threshold', 'Input Image Resolution', 'Two-stage Approach', 'Large Objects', 'Keypoint Detection', 'Large Box']",,2011,"In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet."
Chinese Street View Text: Large-Scale Chinese Text Reading With Partially Supervised Learning,"Yipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding, Jingtuo Liu","Department of Computer Science, The University of Hong Kong; Department of Computer Vision Technology (VIS), Baidu Inc.",100.0,"Hong Kong, china",0.0,,"Most existing text reading benchmarks make it difficult to evaluate the performance of more advanced deep learning models in large vocabularies due to the limited amount of training data. To address this issue, we introduce a new large-scale text reading benchmark dataset named Chinese Street View Text (C-SVT) with 430,000 street view images, which is at least 14 times as large as the existing Chinese text reading benchmarks. To recognize Chinese text in the wild while keeping large-scale datasets labeling cost-effective, we propose to annotate one part of the C-SVT dataset (30,000 images) in locations and text labels as full annotations and add 400,000 more images, where only the corresponding text-of-interest in the regions is given as weak annotations. To exploit the rich information from the weakly annotated data, we design a text reading network in a partially supervised learning framework, which enables to localize and recognize text, learn from fully and weakly annotated data simultaneously. To localize the best matched text proposals from weakly labeled images, we propose an online proposal matching module incorporated in the whole model, spotting the keyword regions by sharing parameters for end-to-end training. Compared with fully supervised training algorithms, this model can improve the end-to-end recognition performance remarkably by 4.03% in F-score at the same labeling cost. The proposed model can also achieve state-of-the-art results on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of the proposed partially supervised learning framework.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Chinese_Street_View_Text_Large-Scale_Chinese_Text_Reading_With_Partially_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Chinese_Street_View_Text_Large-Scale_Chinese_Text_Reading_With_Partially_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010975/,"['Benchmark testing', 'Supervised learning', 'Text recognition', 'Labeling', 'Training', 'Proposals', 'Task analysis']","['Supervised Learning', 'Text Reading', 'Street View', 'Chinese Text', 'Large-scale Text', 'Chinese Text Reading', 'Model Performance', 'Deep Learning', 'Learning Models', 'Annotation Data', 'Recognition Performance', 'Weak Labels', 'Text Labels', 'Labeling Cost', 'Training Set', 'Positive Samples', 'Validation Set', 'Feature Maps', 'Vertical Line', 'Precise Location', 'Optical Character Recognition', 'Width Of The Feature Map', 'Previous Benchmark', 'Image Annotation', 'Gated Recurrent Unit', 'Bounding Box', 'Latent Space', 'Text Lines', 'Sequence Features', 'Natural Images']",,37,"Most existing text reading benchmarks make it difficult to evaluate the performance of more advanced deep learning models in large vocabularies due to the limited amount of training data. To address this issue, we introduce a new large-scale text reading benchmark dataset named Chinese Street View Text (C-SVT) with 430,000 street view images, which is at least 14 times as large as the existing Chinese text reading benchmarks. To recognize Chinese text in the wild while keeping large-scale datasets labeling cost-effective, we propose to annotate one part of the C-SVT dataset (30,000 images) in locations and text labels as full annotations and add 400,000 more images, where only the corresponding text-of-interest in the regions is given as weak annotations. To exploit the rich information from the weakly annotated data, we design a text reading network in a partially supervised learning framework, which enables to localize and recognize text, learn from fully and weakly annotated data simultaneously. To localize the best matched text proposals from weakly labeled images, we propose an online proposal matching module incorporated in the whole model, spotting the keyword regions by sharing parameters for end-to-end training. Compared with fully supervised training algorithms, this model can improve the end-to-end recognition performance remarkably by 4.03% in F-score at the same labeling cost. The proposed model can also achieve state-of-the-art results on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of the proposed partially supervised learning framework."
Closed-Form Optimal Two-View Triangulation Based on Angular Errors,"Seong Hun Lee, Javier Civera","I3A, University of Zaragoza, Spain",100.0,spain,0.0,,"In this paper, we study closed-form optimal solutions to two-view triangulation with known internal calibration and pose. By formulating the triangulation problem as L-1 and L-infinity minimization of angular reprojection errors, we derive the exact closed-form solutions that guarantee global optimality under respective cost functions. To the best of our knowledge, we are the first to present such solutions. Since the angular error is rotationally invariant, our solutions can be applied for any type of central cameras, be it perspective, fisheye or omnidirectional. Our methods also require significantly less computation than the existing optimal methods. Experimental results on synthetic and real datasets validate our theoretical derivations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Closed-Form_Optimal_Two-View_Triangulation_Based_on_Angular_Errors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Closed-Form_Optimal_Two-View_Triangulation_Based_on_Angular_Errors_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010386/,"['Cameras', 'Three-dimensional displays', 'Radio frequency', 'Minimization', 'Cost function', 'Closed-form solutions']","['Angular Error', 'Cost Function', 'Rotation Invariance', 'Reprojection Error', 'Maximum Likelihood Estimation', 'Gaussian Noise', 'Large Errors', 'Point Cloud', 'Reasonable Choice', 'Image Point', '3D Point', 'L1-norm', 'Simultaneous Localization And Mapping', 'Error Criterion']",,13,"In this paper, we study closed-form optimal solutions to two-view triangulation with known internal calibration and pose. By formulating the triangulation problem as L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
 and L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">∞</sub>
 minimization of angular reprojection errors, we derive the exact closed-form solutions that guarantee global optimality under respective cost functions. To the best of our knowledge, we are the first to present such solutions. Since the angular error is rotationally invariant, our solutions can be applied for any type of central cameras, be it perspective, fisheye or omnidirectional. Our methods also require significantly less computation than the existing optimal methods. Experimental results on synthetic and real datasets validate our theoretical derivations."
ClothFlow: A Flow-Based Model for Clothed Person Generation,"Xintong Han, Xiaojun Hu, Weilin Huang, Matthew R. Scott","Malong Technologies, Shenzhen, China; Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China",0.0,,100.0,China,"We present ClothFlow, an appearance-flow-based generative model to synthesize clothed person for posed-guided person image generation and virtual try-on. By estimating a dense flow between source and target clothing regions, ClothFlow effectively models the geometric changes and naturally transfers the appearance to synthesize novel images as shown in Figure 1. We achieve this with a three-stage framework: 1) Conditioned on a target pose, we first estimate a person semantic layout to provide richer guidance to the generation process. 2) Built on two feature pyramid networks, a cascaded flow estimation network then accurately estimates the appearance matching between corresponding clothing regions. The resulting dense flow warps the source image to flexibly account for deformations. 3) Finally, a generative network takes the warped clothing regions as inputs and renders the target view. We conduct extensive experiments on the DeepFashion dataset for pose-guided person image generation and on the VITON dataset for the virtual try-on task. Strong qualitative and quantitative results validate the effectiveness of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010778/,"['Clothing', 'Strain', 'Estimation', 'Layout', 'Two dimensional displays', 'Image generation', 'Three-dimensional displays']","['Qualitative Results', 'Image Generation', 'Source Images', 'Flow Estimation', 'Feature Pyramid', 'Person Image', 'Geometric Changes', 'Feature Pyramid Network', 'Virtual Task', 'Target Pose', 'Target View', 'Structural Similarity', 'Stage 2', 'Feature Maps', 'Target Image', 'Semantic Segmentation', 'Large Deformation', 'Optical Flow', 'Original Protocol', 'Semantic Map', 'Feature Maps Of Images', 'Target Person', 'Optical Flow Estimation', 'Texture Details', 'Image Synthesis', 'Warped Image', '2D Keypoints', 'Geometric Transformation', 'Segmentation Map']",,146,"We present ClothFlow, an appearance-flow-based generative model to synthesize clothed person for posed-guided person image generation and virtual try-on. By estimating a dense flow between source and target clothing regions, ClothFlow effectively models the geometric changes and naturally transfers the appearance to synthesize novel images as shown in Figure 1. We achieve this with a three-stage framework: 1) Conditioned on a target pose, we first estimate a person semantic layout to provide richer guidance to the generation process. 2) Built on two feature pyramid networks, a cascaded flow estimation network then accurately estimates the appearance matching between corresponding clothing regions. The resulting dense flow warps the source image to flexibly account for deformations. 3) Finally, a generative network takes the warped clothing regions as inputs and renders the target view. We conduct extensive experiments on the DeepFashion dataset for pose-guided person image generation and on the VITON dataset for the virtual try-on task. Strong qualitative and quantitative results validate the effectiveness of our method."
Cluster Alignment With a Teacher for Unsupervised Domain Adaptation,"Zhijie Deng, Yucen Luo, Jun Zhu","Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Lab, THBI Lab, Tsinghua University",100.0,China,0.0,,"Deep learning methods have shown promise in unsupervised domain adaptation, which aims to leverage a labeled source domain to learn a classifier for the unlabeled target domain with a different distribution. However, such methods typically learn a domain-invariant representation space to match the marginal distributions of the source and target domains, while ignoring their fine-level structures. In this paper, we propose Cluster Alignment with a Teacher (CAT) for unsupervised domain adaptation, which can effectively incorporate the discriminative clustering structures in both domains for better adaptation. Technically, CAT leverages an implicit ensembling teacher model to reliably discover the class-conditional structure in the feature space for the unlabeled target domain. Then CAT forces the features of both the source and the target domains to form discriminative class-conditional clusters and aligns the corresponding clusters across domains. Empirical results demonstrate that CAT achieves state-of-the-art results in several unsupervised domain adaptation scenarios.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Cluster_Alignment_With_a_Teacher_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Cluster_Alignment_With_a_Teacher_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008112/,"['Cats', 'Task analysis', 'Training', 'Feature extraction', 'Adaptation models', 'Machine learning', 'Labeling']","['Domain Adaptation', 'Cluster Alignment', 'Deep Learning', 'Feature Space', 'Representation Of Space', 'Structure Of Space', 'Marginal Distribution', 'Target Domain', 'Source Domain', 'Unlabeled Target Domain', 'Challenging Task', 'Deep Models', 'Test Accuracy', 'Target Sample', 'Generative Adversarial Networks', 'Labeled Data', 'Ground Truth Labels', 'Target Data', 'Decision Boundary', 'Feature Matching', 'Pseudo Labels', 'Maximum Mean Discrepancy', 'Alignment Procedure', 'Semi-supervised Learning', 'Clustering Loss', 'Adversarial Training', 'Alignment Loss', 'Unsupervised Domain Adaptation Methods', 'Discriminative Learning', 'Tight Cluster']",,142,"Deep learning methods have shown promise in unsupervised domain adaptation, which aims to leverage a labeled source domain to learn a classifier for the unlabeled target domain with a different distribution. However, such methods typically learn a domain-invariant representation space to match the marginal distributions of the source and target domains, while ignoring their fine-level structures. In this paper, we propose Cluster Alignment with a Teacher (CAT) for unsupervised domain adaptation, which can effectively incorporate the discriminative clustering structures in both domains for better adaptation. Technically, CAT leverages an implicit ensembling teacher model to reliably discover the class-conditional structure in the feature space for the unlabeled target domain. Then CAT forces the features of both the source and the target domains to form discriminative class-conditional clusters and aligns the corresponding clusters across domains. Empirical results demonstrate that CAT achieves state-of-the-art results in several unsupervised domain adaptation scenarios."
ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation,"Jiahui Huang, Sheng Yang, Zishuo Zhao, Yu-Kun Lai, Shi-Min Hu","BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing; BNRist, Department of Computer Science and Technology, Tsinghua University, Beijing; Alibaba A.I. Labs, China; Cardiff University, UK",75.0,"China, uk",25.0,China,"We present a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. While recent factor graph based state optimization algorithms have shown their ability to robustly solve SLAM problems by treating dynamic objects as outliers, the dynamic motions are rarely considered. In this paper, we exploit the consensus of 3D motions among the landmarks extracted from the same rigid body for clustering and estimating static and dynamic objects in a unified manner. Specifically, our algorithm builds a noise-aware motion affinity matrix upon landmarks, and uses agglomerative clustering for distinguishing those rigid bodies. Accompanied by a decoupled factor graph optimization for revising their shape and trajectory, we obtain an iterative scheme to update both cluster assignments and motion estimation reciprocally. Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneous tracking of ego-motion and multiple objects.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_ClusterSLAM_A_SLAM_Backend_for_Simultaneous_Rigid_Body_Clustering_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ClusterSLAM_A_SLAM_Backend_for_Simultaneous_Rigid_Body_Clustering_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008108/,"['Simultaneous localization and mapping', 'Motion segmentation', 'Dynamics', 'Three-dimensional displays', 'Tracking', 'Cameras', 'Optimization']","['Rigid Body', 'Motion Estimation', 'Simultaneous Localization And Mapping', 'Cluster Assignment', 'Agglomerative Clustering', '3D Motion', 'Dynamic Objects', 'Factor Graph', 'Hierarchical Clustering', 'Coordinate System', 'Running Time', 'Clustering Method', 'Sparse Matrix', 'Local Coordinate', 'Clustering Approach', 'Semantic Segmentation', 'Landmark Localization', 'Consensus Clustering', 'Pair Of Clusters', 'Camera Pose', 'Stereo Camera', 'Outdoor Scenes', 'Scalar Form', 'Reprojection Error', 'Chunk Size', 'Dynamic Scenes', 'Clustering Module', 'Input Frames', 'Image Space', 'Self-driving']",,48,"We present a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. While recent factor graph based state optimization algorithms have shown their ability to robustly solve SLAM problems by treating dynamic objects as outliers, the dynamic motions are rarely considered. In this paper, we exploit the consensus of 3D motions among the landmarks extracted from the same rigid body for clustering and estimating static and dynamic objects in a unified manner. Specifically, our algorithm builds a noise-aware motion affinity matrix upon landmarks, and uses agglomerative clustering for distinguishing those rigid bodies. Accompanied by a decoupled factor graph optimization for revising their shape and trajectory, we obtain an iterative scheme to update both cluster assignments and motion estimation reciprocally. Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneous tracking of ego-motion and multiple objects."
Clustered Object Detection in Aerial Images,"Fan Yang, Heng Fan, Peng Chu, Erik Blasch, Haibin Ling","Air Force Research Lab, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department Computer Science, Stony Brook University, Stony Brook, NY, USA",66.66666666666666,usa,33.33333333333334,USA,"Detecting objects in aerial images is challenging for at least two reasons: (1) target objects like pedestrians are very small in pixels, making them hardly distinguished from surrounding background; and (2) targets are in general sparsely and non-uniformly distributed, making the detection very inefficient. In this paper, we address both issues inspired by observing that these targets are often clustered. In particular, we propose a Clustered Detection (ClusDet) network that unifies object clustering and detection in an end-to-end framework. The key components in ClusDet include a cluster proposal sub-network (CPNet), a scale estimation sub-network (ScaleNet), and a dedicated detection network (DetecNet). Given an input image, CPNet produces object cluster regions and ScaleNet estimates object scales for these regions. Then, each scale-normalized cluster region is fed into DetecNet for object detection. ClusDet has several advantages over previous solutions: (1) it greatly reduces the number of chips for final object detection and hence achieves high running time efficiency, (2) the cluster-based scale estimation is more accurate than previously used single-object based ones, hence effectively improves the detection for small objects, and (3) the final DetecNet is dedicated for clustered regions and implicitly models the prior context information so as to boost detection accuracy. The proposed method is tested on three popular aerial image datasets including VisDrone, UAVDT and DOTA. In all experiments, ClusDet achieves promising performance in comparison with state-of-the-art detectors.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010045/,"['Detectors', 'Proposals', 'Object detection', 'Merging', 'Feature extraction', 'Estimation', 'Image resolution']","['Object Detection', 'Aerial Images', 'Detection In Aerial Images', 'Regional Clusters', 'Small Objects', 'Final Detection', 'Object Scale', 'Aerial Image Dataset', 'Number Of Chips', 'Training Data', 'Convolutional Neural Network', 'Deep Neural Network', 'Feature Maps', 'Detection Performance', 'Image Regions', 'Number Of Images', 'Detection Results', 'Backbone Network', 'Large Objects', 'Object Regions', 'Feature Pyramid Network', 'Region Proposal Network', 'Global Image', 'COCO Dataset', 'Faster R-CNN', 'Merging Clusters', 'Vehicle Detection', 'Non-maximum Suppression', 'Natural Images']",,205,"Detecting objects in aerial images is challenging for at least two reasons: (1) target objects like pedestrians are very small in pixels, making them hardly distinguished from surrounding background; and (2) targets are in general sparsely and non-uniformly distributed, making the detection very inefficient. In this paper, we address both issues inspired by observing that these targets are often clustered. In particular, we propose a Clustered Detection (ClusDet) network that unifies object clustering and detection in an end-to-end framework. The key components in ClusDet include a cluster proposal sub-network (CPNet), a scale estimation sub-network (ScaleNet), and a dedicated detection network (DetecNet). Given an input image, CPNet produces object cluster regions and ScaleNet estimates object scales for these regions. Then, each scale-normalized cluster region is fed into DetecNet for object detection. ClusDet has several advantages over previous solutions: (1) it greatly reduces the number of chips for final object detection and hence achieves high running time efficiency, (2) the cluster-based scale estimation is more accurate than previously used single-object based ones, hence effectively improves the detection for small objects, and (3) the final DetecNet is dedicated for clustered regions and implicitly models the prior context information so as to boost detection accuracy. The proposed method is tested on three popular aerial image datasets including VisDrone, UAVDT and DOTA. In all experiments, ClusDet achieves promising performance in comparison with state-of-the-art detectors."
Co-Evolutionary Compression for Unpaired Image Translation,"Han Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi Tian, Chang Xu","Key Lab of Machine Perception (MOE), CMIC, School of EECS, Peking University, China; School of Computer Science, Faculty of Engineering, The University of Sydney, Australia; Huawei Noah’s Ark Lab",66.66666666666666,"australia, china",33.33333333333334,China,"Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, generators in these networks are of complicated architectures with large number of parameters and huge computational complexities. Existing methods are mainly designed for compressing and speeding-up deep neural networks in the classification task, and cannot be directly applied on GANs for image translation, due to their different objectives and training procedures. To this end, we develop a novel co-evolutionary approach for reducing their memory usage and FLOPs simultaneously. In practice, generators for two image domains are encoded as two populations and synergistically optimized for investigating the most important convolution filters iteratively. Fitness of each individual is calculated using the number of parameters, a discriminator-aware regularization, and the cycle consistency. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method for obtaining compact and effective generators.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shu_Co-Evolutionary_Compression_for_Unpaired_Image_Translation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shu_Co-Evolutionary_Compression_for_Unpaired_Image_Translation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010692/,"['Generators', 'Image coding', 'Task analysis', 'Gallium nitride', 'Training', 'Neural networks', 'Convolution']","['Unpaired Image Translation', 'Neural Network', 'Deep Neural Network', 'Extensive Experiments', 'Benchmark Datasets', 'Generative Adversarial Networks', 'Individual Fitness', 'Memory Usage', 'Cycle Consistency', 'Considerable Computation', 'Convolutional Layers', 'Population Of Individuals', 'Model Size', 'Image Generation', 'Efficient Generation', 'Network Efficiency', 'Number Of Filters', 'Benchmark Model', 'Compression Ratio', 'Neural Network Parameters', 'Pre-trained Neural Network', 'Network Compression', 'Filters In Layer', 'Fréchet Inception Distance', 'Pruning Method', 'Style Transfer', 'Translation Task', 'Discriminator Network', 'Coding Tree', 'Semantic Map']",,51,"Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, generators in these networks are of complicated architectures with large number of parameters and huge computational complexities. Existing methods are mainly designed for compressing and speeding-up deep neural networks in the classification task, and cannot be directly applied on GANs for image translation, due to their different objectives and training procedures. To this end, we develop a novel co-evolutionary approach for reducing their memory usage and FLOPs simultaneously. In practice, generators for two image domains are encoded as two populations and synergistically optimized for investigating the most important convolution filters iteratively. Fitness of each individual is calculated using the number of parameters, a discriminator-aware regularization, and the cycle consistency. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method for obtaining compact and effective generators."
Co-Mining: Deep Face Recognition With Noisy Labels,"Xiaobo Wang, Shuo Wang, Jun Wang, Hailin Shi, Tao Mei","JD AI Research, Beijing, China",0.0,,100.0,China,"Face recognition has achieved significant progress with the growing scale of collected datasets, which empowers us to train strong convolutional neural networks (CNNs). While a variety of CNN architectures and loss functions have been devised recently, we still have a limited understanding of how to train the CNN models with the label noise inherent in existing face recognition datasets. To address this issue, this paper develops a novel co-mining strategy to effectively train on the datasets with noisy labels. Specifically, we simultaneously use the loss values as the cue to detect noisy labels, exchange the high-confidence clean faces to alleviate the errors accumulated issue caused by the sample-selection bias, and re-weight the predicted clean faces to make them dominate the discriminative model training in a mini-batch fashion. Extensive experiments by training on three popular datasets (i.e., CASIA-WebFace, MS-Celeb-1M and VggFace2) and testing on several benchmarks, including LFW, AgeDB, CFP, CALFW, CPLFW, RFW, and MegaFace, have demonstrated the effectiveness of our new approach over the state-of-the-art alternatives.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf,,http://www.cbsr.ia.ac.cn/users/xiaobowang/,,main,Oral,https://ieeexplore.ieee.org/document/9008131/,"['Face', 'Noise measurement', 'Face recognition', 'Training', 'Artificial neural networks', 'Training data', 'Robustness']","['Face Recognition', 'Noisy Labels', 'Deep Face Recognition', 'Loss Function', 'Neural Network', 'Convolutional Neural Network', 'Sampling Bias', 'Loss Value', 'Convolutional Neural Network Model', 'Model Discrimination', 'Convolutional Neural Network Architecture', 'Variety Of Architectures', 'Popular Datasets', 'Training Data', 'Training Dataset', 'Deep Neural Network', 'Deep Models', 'Average Accuracy', 'Large-scale Datasets', 'Stochastic Gradient Descent', 'Noise Rate', 'Peer Networks', 'Discriminative Features', 'Synthetic Noise', 'Student Network', 'Development Of New Techniques', 'Experiments In This Paper', 'Deep Convolutional Neural Network', 'Image Retrieval', 'Clean Samples']",,77,"Face recognition has achieved significant progress with the growing scale of collected datasets, which empowers us to train strong convolutional neural networks (CNNs). While a variety of CNN architectures and loss functions have been devised recently, we still have a limited understanding of how to train the CNN models with the label noise inherent in existing face recognition datasets. To address this issue, this paper develops a novel co-mining strategy to effectively train on the datasets with noisy labels. Specifically, we simultaneously use the loss values as the cue to detect noisy labels, exchange the high-confidence clean faces to alleviate the errors accumulated issue caused by the sample-selection bias, and re-weight the predicted clean faces to make them dominate the discriminative model training in a mini-batch fashion. Extensive experiments by training on three popular datasets (\textit{i.e.}, CASIA-WebFace, MS-Celeb-1M and VggFace2) and testing on several benchmarks, including LFW, AgeDB, CFP, CALFW, CPLFW, RFW, and MegaFace, have demonstrated the effectiveness of our new approach over the state-of-the-art alternatives."
Co-Segmentation Inspired Attention Networks for Video-Based Person Re-Identification,"Arulkumar Subramaniam, Athira Nambiar, Anurag Mittal","Department of Computer Science and Engineering, Indian Institute of Technology Madras",100.0,"India, india",0.0,,"Person re-identification (Re-ID) is an important real-world surveillance problem that entails associating a person's identity over a network of cameras. Video-based Re-ID approaches have gained significant attention recently since a video, and not just an image, is often available. In this work, we propose a novel Co-segmentation inspired video Re-ID deep architecture and formulate a Co-segmentation based Attention Module (COSAM) that activates a common set of salient features across multiple frames of a video via mutual consensus in an unsupervised manner. As opposed to most of the prior work, our approach is able to attend to person accessories along with the person. Our plug-and-play and interpretable COSAM module applied on two deep architectures (ResNet50, SE-ResNet50) outperform the state-of-the-art methods on three benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010901/,"['Feature extraction', 'Image segmentation', 'Task analysis', 'Surveillance', 'Pose estimation', 'Computer architecture', 'Convolution']","['Video-based Person Re-identification', 'Video Frames', 'Deep Architecture', 'Camera Network', 'Training Set', 'Feature Maps', 'Multilayer Perceptron', 'Background Characteristics', 'Spatial Attention', 'Common Objects', 'Pose Estimation', 'Feature Aggregation', 'Convolutional Block', 'Local Descriptors', 'Channel Attention', 'Triplet Loss', 'Background Clutter', 'Normalized Cross-correlation', 'Recurrent Layers', 'Viewpoint Changes', 'Temporal Aggregation', 'Cost Volume', 'mAP Improvement', 'Aggregation Layer', 'Channel Activity', 'Deep Neural Network', 'Video Sequences', 'Fully Convolutional Network', 'Spatial Step', 'Backbone Network']",,90,"Person re-identification (Re-ID) is an important real-world surveillance problem that entails associating a person's identity over a network of cameras. Video-based Re-ID approaches have gained significant attention recently since a video, and not just an image, is often available. In this work, we propose a novel Co-segmentation inspired video Re-ID deep architecture and formulate a Co-segmentation based Attention Module (COSAM) that activates a common set of salient features across multiple frames of a video via mutual consensus in an unsupervised manner. As opposed to most of the prior work, our approach is able to attend to person accessories along with the person. Our plug-and-play and interpretable COSAM module applied on two deep architectures (ResNet50, SE-ResNet50) outperform the state-of-the-art methods on three benchmark datasets."
Co-Separating Sounds of Visual Objects,"Ruohan Gao, Kristen Grauman",UT Austin and Facebook AI Research; UT Austin,100.0,usa,0.0,,"Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of ""true"" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Co-Separating_Sounds_of_Visual_Objects_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Co-Separating_Sounds_of_Visual_Objects_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009045/,"['Training', 'Source separation', 'Visualization', 'Spectrogram', 'Feature extraction', 'Particle separators', 'Object detection']","['Visual Object', 'Sound Objects', 'Deep Network', 'Deep Neural Network', 'Video Clips', 'Source Separation', 'Training Objective', 'Training Paradigm', 'Test Videos', 'Qualitative Results', 'Object Detection', 'Visual Features', 'Learning Objectives', 'Video Frames', 'Musical Instruments', 'Signal Separation', 'Sound Source', 'Mixed Signals', 'Short-time Fourier Transform', 'Training Videos', 'Video Object', 'Single Video', 'Signal-to-interference Ratio', 'Object Regions', 'Saxophone', 'Object In Frame', 'Ambient Noise Levels', 'Individual Video', 'Soundtrack', 'Separate Networks']",,128,"Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of ""true"" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets."
Coherent Semantic Attention for Image Inpainting,"Hongyu Liu, Bin Jiang, Yi Xiao, Chao Yang","College of Computer Science and Electronic Engineering, Hunan University",100.0,china,0.0,,"The latest deep learning-based approaches have shown promising results for the challenging task of inpainting missing regions of an image. However, the existing methods often generate contents with blurry textures and distorted structures due to the discontinuity of the local pixels. From a semantic-level perspective, the local pixel discontinuity is mainly because these methods ignore the semantic relevance and feature continuity of hole regions. To handle this problem, we investigate the human behavior in repairing pictures and propose a fined deep generative model-based approach with a novel coherent semantic attention (CSA) layer, which can not only preserve contextual structure but also make more effective predictions of missing parts by modeling the semantic relevance between the holes features. The task is divided into rough, refinement as two steps and we model each step with a neural network under the U-Net architecture, where the CSA layer is embedded into the encoder of refinement step. Meanwhile, we further propose consistency loss and feature patch discriminator to stabilize the network training process and improve the details. The experiments on CelebA, Places2, and Paris StreetView datasets have validated the effectiveness of our proposed methods in image inpainting tasks and can obtain images with a higher quality as compared with the existing state-of-the-art approaches. The codes and pre-trained models will be available at https://github.com/KumapowerLIU/CSA-inpainting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Coherent_Semantic_Attention_for_Image_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Coherent_Semantic_Attention_for_Image_Inpainting_ICCV_2019_paper.pdf,,https://github.com/KumapowerLIU/CSA-inpainting,,main,Poster,https://ieeexplore.ieee.org/document/9009473/,"['Semantics', 'Convolution', 'Task analysis', 'Training', 'Decoding', 'Correlation', 'Painting']","['Image Inpainting', 'Neural Network', 'Pixel Location', 'Attention Layer', 'Consistency Loss', 'Missing Regions', 'Patch Features', 'Hole Region', 'Deep Neural Network', 'Feature Maps', 'Deep Convolutional Neural Network', 'Fine Details', 'Third Position', 'Adjacent Pixels', 'Attention Map', 'Reconstruction Loss', 'Encoder Layer', 'Dilated Convolution', 'Perceptual Loss', 'Plausible Results', 'Similar Patches', 'Refinement Network', 'Local Coherence', '4th Position', '2nd Position', 'Context Encoder', 'Contextual Attention', 'VGG Network', 'Unknown Regions', 'Adversarial Training']",,235,"The latest deep learning-based approaches have shown promising results for the challenging task of inpainting missing regions of an image. However, the existing methods often generate contents with blurry textures and distorted structures due to the discontinuity of the local pixels. From a semantic-level perspective, the local pixel discontinuity is mainly because these methods ignore the semantic relevance and feature continuity of hole regions. To handle this problem, we investigate the human behavior in repairing pictures and propose a fined deep generative model-based approach with a novel coherent semantic attention (CSA) layer, which can not only preserve contextual structure but also make more effective predictions of missing parts by modeling the semantic relevance between the holes features. The task is divided into rough, refinement as two steps and we model each step with a neural network under the U-Net architecture, where the CSA layer is embedded into the encoder of refinement step. Meanwhile, we further propose consistency loss and feature patch discriminator to stabilize the network training process and improve the details. The experiments on CelebA, Places2, and Paris StreetView datasets have validated the effectiveness of our proposed methods in image inpainting tasks and can obtain images with a higher quality as compared with the existing state-of-the-art approaches. The codes and pre-trained models will be available at https://github.com/KumapowerLIU/CSA-inpainting."
Collect and Select: Semantic Alignment Metric Learning for Few-Shot Learning,"Fusheng Hao, Fengxiang He, Jun Cheng, Lei Wang, Jianzhong Cao, Dacheng Tao","UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia; Xi’an Institute of Optics and Precision Mechanics, CAS, China; CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, CAS, China and The Chinese University of Hong Kong, Hong Kong, China",100.0,"China, Hong Kong, australia, china",0.0,,"Few-shot learning aims to learn latent patterns from few training examples and has shown promises in practice. However, directly calculating the distances between the query image and support image in existing methods may cause ambiguity because dominant objects can locate anywhere on images. To address this issue, this paper proposes a Semantic Alignment Metric Learning (SAML) method for few-shot learning that aligns the semantically relevant dominant objects through a ""collect-and-select"" strategy. Specifically, we first calculate a relation matrix (RM) to ""collect"" the distances of each local region pairs of the 3D tensor extracted from a query image and the mean tensor of the support images. Then, the attention technique is adapted to ""select"" the semantically relevant pairs and put more weights on them. Afterwards, a multi-layer perceptron (MLP) is utilized to map the reweighted RMs to their corresponding similarity scores. Theoretical analysis demonstrates the generalization ability of SAML and gives a theoretical guarantee. Empirical results demonstrate that semantic alignment is achieved. Extensive experiments on benchmark datasets validate the strengths of the proposed approach and demonstrate that SAML significantly outperforms the current state-of-the-art methods. The source code is available at https://github.com/haofusheng/SAML.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hao_Collect_and_Select_Semantic_Alignment_Metric_Learning_for_Few-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hao_Collect_and_Select_Semantic_Alignment_Metric_Learning_for_Few-Shot_Learning_ICCV_2019_paper.pdf,,https://github.com/haofusheng/SAML,,main,Poster,https://ieeexplore.ieee.org/document/9010298/,"['Measurement', 'Tensile stress', 'Three-dimensional displays', 'Semantics', 'Training', 'Feature extraction', 'Task analysis']","['Fluidic', 'Metric Learning', 'Few-shot Learning', 'Theoretical Analysis', 'Similarity Score', 'Extensive Experiments', 'Multilayer Perceptron', 'Relationship Matrix', 'Training Examples', 'Pair Distance', 'Theoretical Guarantees', 'Query Image', '3D Tensor', 'Relevant Pairs', 'Metric Learning Methods', 'Neural Network', 'Convolutional Neural Network', 'Image Size', 'Time Complexity', 'Network Embedding', 'Average Pooling Layer', 'Relevant Regions', 'Max-pooling Layer', 'Query Set', 'Support Set', 'Representative Class', 'Representation Of Space', 'Training Sample Set', 'Image Embedding']",,85,"Few-shot learning aims to learn latent patterns from few training examples and has shown promises in practice. However, directly calculating the distances between the query image and support image in existing methods may cause ambiguity because dominant objects can locate anywhere on images. To address this issue, this paper proposes a Semantic Alignment Metric Learning (SAML) method for few-shot learning that aligns the semantically relevant dominant objects through a ``collect-and-select'' strategy. Specifically, we first calculate a relation matrix (RM) to ``collect"" the distances of each local region pairs of the $3$D tensor extracted from a query image and the mean tensor of the support images. Then, the attention technique is adapted to ``select"" the semantically relevant pairs and put more weights on them. Afterwards, a multi-layer perceptron (MLP) is utilized to map the reweighted RMs to their corresponding similarity scores. Theoretical analysis demonstrates the generalization ability of SAML and gives a theoretical guarantee. Empirical results demonstrate that semantic alignment is achieved. Extensive experiments on benchmark datasets validate the strengths of the proposed approach and demonstrate that SAML significantly outperforms the current state-of-the-art methods. The source code is available at https://github.com/haofusheng/SAML."
Compact Trilinear Interaction for Visual Question Answering,"Tuong Do, Thanh-Toan Do, Huy Tran, Erman Tjiputra, Quang D. Tran","AIOZ Pte Ltd, Singapore; University of Liverpool",50.0,uk,50.0,Singapore,"In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear teraction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.pdf,,https://github.com/aioz-ai/ICCV19_VQA-CTI,,main,Poster,https://ieeexplore.ieee.org/document/9010363/,"['Tensile stress', 'Computational modeling', 'Visualization', 'Computational efficiency', 'Testing', 'Knowledge discovery', 'Correlation']","['Visual Question Answering', 'Trilinear Interaction', 'Computational Cost', 'Compact Model', 'Bilinear Model', 'Factorization', 'Binary Classification', 'State Of The Art', 'Training Phase', 'Cross-entropy Loss', 'Teacher Model', 'Hidden State', 'Decomposition Rate', 'Image Representation', 'Dimensional Representation', 'Good Trade-off', 'Student Model', 'Attention Map', 'Question Categories', 'Outer Product', 'Joint Representation', 'Joint Embedding', 'Scalar Weights', 'Dimensionality Issue']",,31,"In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear teraction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets."
CompenNet++: End-to-End Full Projector Compensation,"Bingyao Huang, Haibin Ling",Temple University; Stony Brook University,100.0,usa,0.0,,"Full projector compensation aims to modify a projector input image such that it can compensate for both geometric and photometric disturbance of the projection surface. Traditional methods usually solve the two parts separately, although they are known to correlate with each other. In this paper, we propose the first end-to-end solution, named CompenNet++, to solve the two problems jointly. Our work non-trivially extends CompenNet, which was recently proposed for photometric compensation with promising performance. First, we propose a novel geometric correction subnet, which is designed with a cascaded coarse-to-fine structure to learn the sampling grid directly from photometric sampling images. Second, by concatenating the geometric correction subset with CompenNet, CompenNet++ accomplishes full projector compensation and is end-to-end trainable. Third, after training, we significantly simplify both geometric and photometric compensation parts, and hence largely improves the running time efficiency. Moreover, we construct the first setup-independent full compensation benchmark to facilitate the study on this topic. In our thorough experiments, our method shows clear advantages over previous arts with promising compensation quality and meanwhile being practically convenient.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_CompenNet_End-to-End_Full_Projector_Compensation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CompenNet_End-to-End_Full_Projector_Compensation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010866/,"['Surface texture', 'Measurement', 'Geometry', 'Image color analysis', 'Cameras', 'Face', 'Training']","['Images Of Samples', 'Input Image', 'Geometric Correction', 'Surface Projection', 'Runtime Efficiency', 'Training Set', 'Convolutional Neural Network', 'Deep Neural Network', 'Surface Images', 'Two-step Method', 'Simple Network', 'Output Image', 'Bilinear Interpolation', 'Frontal View', 'Specular Reflection', 'Compensation Method', 'Geometric Transformation', 'Weight Initialization', 'Traditional Solutions', 'Blue Part', 'Non-planar Surfaces', 'Warped Image', 'Partial Compensation', 'Grid Refinement', 'Full Problem', 'Mapping Project', 'Affine Transformation', 'Surface Texture', 'Network Effects', 'Image Pairs']",,18,"Full projector compensation aims to modify a projector input image such that it can compensate for both geometric and photometric disturbance of the projection surface. Traditional methods usually solve the two parts separately, although they are known to correlate with each other. In this paper, we propose the first end-to-end solution, named CompenNet++, to solve the two problems jointly. Our work non-trivially extends CompenNet, which was recently proposed for photometric compensation with promising performance. First, we propose a novel geometric correction subnet, which is designed with a cascaded coarse-to-fine structure to learn the sampling grid directly from photometric sampling images. Second, by concatenating the geometric correction subset with CompenNet, CompenNet++ accomplishes full projector compensation and is end-to-end trainable. Third, after training, we significantly simplify both geometric and photometric compensation parts, and hence largely improves the running time efficiency. Moreover, we construct the first setup-independent full compensation benchmark to facilitate the study on this topic. In our thorough experiments, our method shows clear advantages over previous arts with promising compensation quality and meanwhile being practically convenient."
CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition,"Nadav Schor, Oren Katzir, Hao Zhang, Daniel Cohen-Or",Tel Aviv University; Simon Fraser University,100.0,"canada, israel",0.0,,"Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to enable a model to generate a rich variety of samples from the entire target distribution, rather than only from a distribution confined to the training data. In other words, we would like the generative model to go beyond the observed samples and learn to generate ""unseen"", yet still plausible, data. In our work, we present CompoNet, a generative neural network for 2D or 3D shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions. Treating a shape not as an unstructured whole, but as a (re-)composable set of deformable parts, adds a combinatorial dimension to the generative process to enrich the diversity of the output, encouraging the generator to venture more into the ""unseen"". We show that our part-based model generates richer variety of plausible shapes compared with baseline generative models. To this end, we introduce two quantitative metrics to evaluate the diversity of a generative model and assess how well the generated data covers both the training data and unseen data from the same target distribution.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Schor_CompoNet_Learning_to_Generate_the_Unseen_by_Part_Synthesis_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Schor_CompoNet_Learning_to_Generate_the_Unseen_by_Part_Synthesis_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010059/,"['Shape', 'Three-dimensional displays', 'Training', 'Data models', 'Training data', 'Generators', 'Neural networks']","['Neural Network', 'Training Data', 'Deep Neural Network', 'Part Of Set', '3D Shape', 'Unseen Data', 'Quantitative Metrics', 'Target Distribution', 'General Neural Network', '2D Shape', 'Training Set', 'Convolutional Layers', 'Adam Optimizer', 'Autoencoder', 'Point Cloud', 'Generative Adversarial Networks', 'Fully-connected Layer', 'Affine Transformation', 'Batch Normalization Layer', 'Variational Autoencoder', 'Transposed Convolution Layers', 'Network Composition', 'ReLU Activation Function', 'Chamfer Distance', 'Decoder Output', '2D Case', 'Unit Square', 'Latent Vector', '3D Case']",,34,"Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to enable a model to generate a rich variety of samples from the entire target distribution, rather than only from a distribution confined to the training data. In other words, we would like the generative model to go beyond the observed samples and learn to generate ``unseen'', yet still plausible, data. In our work, we present CompoNet, a generative neural network for 2D or 3D shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions. Treating a shape not as an unstructured whole, but as a (re-)composable set of deformable parts, adds a combinatorial dimension to the generative process to enrich the diversity of the output, encouraging the generator to venture more into the ``unseen''. We show that our part-based model generates richer variety of plausible shapes compared with baseline generative models. To this end, we introduce two quantitative metrics to evaluate the diversity of a generative model and assess how well the generated data covers both the training data and unseen data from the same target distribution."
Composite Shape Modeling via Latent Space Factorization,"Anastasia Dubrovina, Fei Xia, Panos Achlioptas, Mira Shalah, RaphaÃ«l Groscot, Leonidas J. Guibas",Stanford University; PSL Research University,100.0,"France, usa",0.0,,"We present a novel neural network architecture, termed Decomposer-Composer, for semantic structure-aware 3D shape modeling. Our method utilizes an auto-encoder-based pipeline, and produces a novel factorized shape embedding space, where the semantic structure of the shape collection translates into a data-dependent sub-space factorization, and where shape composition and decomposition become simple linear operations on the embedding coordinates. We further propose to model shape assembly using an explicit learned part deformation module, which utilizes a 3D spatial transformer network to perform an in-network volumetric grid deformation, and which allows us to train the whole system end-to-end. The resulting network allows us to perform part-level shape manipulation, unattainable by existing approaches. Our extensive ablation study, comparison to baseline methods and qualitative analysis demonstrate the improved performance of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dubrovina_Composite_Shape_Modeling_via_Latent_Space_Factorization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dubrovina_Composite_Shape_Modeling_via_Latent_Space_Factorization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009813/,"['Shape', 'Semantics', 'Three-dimensional displays', 'Decoding', 'Solid modeling', 'Strain', 'Image reconstruction']","['Latent Factors', 'Latent Space', 'Shape Model', 'Linear Operator', '3D Shape', 'Spatial Network', 'Semantic Structure', 'Use Of Shapes', 'Spatial Transformer Network', 'Point Cloud', 'Bounding Box', 'Projection Matrix', 'Latent Representation', 'Variational Autoencoder', 'Composers', 'Transformation Parameters', 'Semantic Labels', 'Part In Shaping', 'Plausible Results', 'Set Of Transformations', 'Random Part', 'Cycle Consistency Loss', 'Precise Segmentation', 'Occupancy Grid', 'Input Shape', 'Cycle Consistency', 'Projection Layer', 'Cycle Loss', 'Part Geometry', 'Shape Reconstruction']",,28,"We present a novel neural network architecture, termed Decomposer-Composer, for semantic structure-aware 3D shape modeling. Our method utilizes an auto-encoder-based pipeline, and produces a novel factorized shape embedding space, where the semantic structure of the shape collection translates into a data-dependent sub-space factorization, and where shape composition and decomposition become simple linear operations on the embedding coordinates. We further propose to model shape assembly using an explicit learned part deformation module, which utilizes a 3D spatial transformer network to perform an in-network volumetric grid deformation, and which allows us to train the whole system end-to-end. The resulting network allows us to perform part-level shape manipulation, unattainable by existing approaches. Our extensive ablation study, comparison to baseline methods and qualitative analysis demonstrate the improved performance of the proposed method."
Compositional Video Prediction,"Yufei Ye, Maneesh Singh, Abhinav Gupta, Shubham Tulsiani",Verisk Analytics; Carnegie Mellon University; Facebook AI Research,33.33333333333333,usa,66.66666666666667,USA,"We present an approach for pixel-level future prediction given an input image of a scene. We observe that a scene is comprised of distinct entities that undergo motion and present an approach that operationalizes this insight. We implicitly predict future states of independent entities while reasoning about their interactions, and compose future video frames using these predicted states. We overcome the inherent multi-modality of the task using a global trajectory-level latent random variable, and show that this allows us to sample diverse and plausible futures. We empirically validate our approach against alternate representations and ways of incorporating multi-modality. We examine two datasets, one comprising of stacked objects that may fall, and the other containing videos of humans performing activities in a gym, and show that our approach allows realistic stochastic video prediction across these diverse settings. See project website (https://judyye.github.io/CVP/) for video predictions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Compositional_Video_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Compositional_Video_Prediction_ICCV_2019_paper.pdf,https://judyye.github.io/CVP/,,,main,Poster,https://ieeexplore.ieee.org/document/9009053/,"['Decoding', 'Task analysis', 'Predictive models', 'Training', 'Cognition', 'Stochastic processes', 'Encoding']","['Video Prediction', 'Random Variables', 'Scene Images', 'Realistic Predictions', 'Future Frames', 'Time Step', 'Factorization', 'Interaction Network', 'Latent Variables', 'Qualitative Results', 'Bounding Box', 'Feed-forward Network', 'Line Of Work', 'Sequence Of Frames', 'Pixel Level', 'Graph Convolutional Network', 'Appendix For Details', 'Latent Representation', 'Training Objective', 'Pixel Spacing', 'Intermediate Representation', 'Human Pose', 'Late Fusion', 'Representation Of Entities', 'Features Of Entities', 'Image Coordinates', 'End Goal', 'Appearance Features', 'Ambiguity']",,46,"We present an approach for pixel-level future prediction given an input image of a scene. We observe that a scene is comprised of distinct entities that undergo motion and present an approach that operationalizes this insight. We implicitly predict future states of independent entities while reasoning about their interactions, and compose future video frames using these predicted states. We overcome the inherent multi-modality of the task using a global trajectory-level latent random variable, and show that this allows us to sample diverse and plausible futures. We empirically validate our approach against alternate representations and ways of incorporating multi-modality. We examine two datasets, one comprising of stacked objects that may fall, and the other containing videos of humans performing activities in a gym, and show that our approach allows realistic stochastic video prediction across these diverse settings. See project website (https://judyye.github.io/CVP/) for video predictions."
Computational Hyperspectral Imaging Based on Dimension-Discriminative Low-Rank Tensor Recovery,"Shipeng Zhang, Lizhi Wang, Ying Fu, Xiaoming Zhong, Hua Huang",Xi’an Jiaotong University; Beijing Institute of Space Mechanics and Electricity; Beijing Institute of Technology,100.0,"China, china",0.0,,"Exploiting the prior information is fundamental for the image reconstruction in computational hyperspectral imaging. Existing methods usually unfold the 3D signal as a 1D vector and treat the prior information within different dimensions in an indiscriminative manner, which ignores the high-dimensionality nature of hyperspectral image (HSI) and thus results in poor quality reconstruction. In this paper, we propose to make full use of the high-dimensionality structure of the desired HSI to boost the reconstruction quality. We first build a high-order tensor by exploiting the nonlocal similarity in HSI. Then, we propose a dimension-discriminative low-rank tensor recovery (DLTR) model to characterize the structure prior adaptively in each dimension. By integrating the structure prior in DLTR with the system imaging process, we develop an optimization framework for HSI reconstruction, which is finally solved via the alternating minimization algorithm. Extensive experiments implemented with both synthetic and real data demonstrate that our method outperforms state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Computational_Hyperspectral_Imaging_Based_on_Dimension-Discriminative_Low-Rank_Tensor_Recovery_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Computational_Hyperspectral_Imaging_Based_on_Dimension-Discriminative_Low-Rank_Tensor_Recovery_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008805/,,"['Computer Image', 'Low-rank Tensor', 'Tensor Recovery', 'Prior Information', 'Image Reconstruction', 'Reconstruction Framework', '1D Vector', 'Imaging System', 'Spatial Dimensions', 'Singular Value', 'Spectral Properties', 'Taylor Expansion', 'Sparse Representation', 'Spatial Size', 'Recovery Model', 'Penalty Factor', 'Nuclear Norm', 'Spectral Correlation', 'Total Variation Regularization', 'Similar Patches', 'Compressive Measurements', '3D Tensor']",,56,"Exploiting the prior information is fundamental for the image reconstruction in computational hyperspectral imaging. Existing methods usually unfold the 3D signal as a 1D vector and treat the prior information within different dimensions in an indiscriminative manner, which ignores the high-dimensionality nature of hyperspectral image (HSI) and thus results in poor quality reconstruction. In this paper, we propose to make full use of the high-dimensionality structure of the desired HSI to boost the reconstruction quality. We first build a high-order tensor by exploiting the nonlocal similarity in HSI. Then, we propose a dimension-discriminative low-rank tensor recovery (DLTR) model to characterize the structure prior adaptively in each dimension. By integrating the structure prior in DLTR with the system imaging process, we develop an optimization framework for HSI reconstruction, which is finally solved via the alternating minimization algorithm. Extensive experiments implemented with both synthetic and real data demonstrate that our method outperforms state-of-the-art methods."
Conditional Coupled Generative Adversarial Networks for Zero-Shot Domain Adaptation,"Jinghua Wang, Jianmin Jiang","Research Institute for Future Media Computing, College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China",100.0,china,0.0,,"Machine learning models trained in one domain perform poorly in the other domains due to the existence of domain shift. Domain adaptation techniques solve this problem by training transferable models from the label-rich source domain to the label-scarce target domain. Unfortunately, a majority of the existing domain adaptation techniques rely on the availability of the target-domain data, and thus limit their applications to a small community across few computer vision problems. In this paper, we tackle the challenging zero-shot domain adaptation (ZSDA) problem, where the target-domain data is non-available in the training stage. For this purpose, we propose conditional coupled generative adversarial networks (CoCoGAN) by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. Compared with the existing state of the arts, our proposed CoCoGAN is able to capture the joint distribution of dual-domain samples in two different tasks, i.e. the relevant task (RT) and an irrelevant task (IRT). We train the CoCoGAN with both source-domain samples in RT and the dual-domain samples in IRT to complete the domain adaptation. While the former provide the high-level concepts of the non-available target-domain data, the latter carry the sharing correlation between the two domains in RT and IRT. To train the CoCoGAN in the absence of the target-domain data for RT, we propose a new supervisory signal, i.e. the alignment between representations across tasks. Extensive experiments carried out demonstrate that our proposed CoCoGAN outperforms existing state of the arts in image classifications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Conditional_Coupled_Generative_Adversarial_Networks_for_Zero-Shot_Domain_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Conditional_Coupled_Generative_Adversarial_Networks_for_Zero-Shot_Domain_Adaptation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010897/,"['Task analysis', 'Generative adversarial networks', 'Gallium nitride', 'Training', 'Adaptation models', 'Image analysis', 'Computer vision']","['Generative Adversarial Networks', 'Domain Adaptation', 'Coupled Generative Adversarial Networks', 'Sample Distribution', 'Computer Vision', 'Image Classification', 'Training Stage', 'Domain Shift', 'Target Domain', 'Source Domain', 'Relevant Tasks', 'Target Domain Data', 'Supervisory Signal', 'Domain Adaptation Techniques', 'Source Domain Samples', 'Objective Function', 'Sample Set', 'Convolutional Neural Network', 'Convolutional Layers', 'Image Size', 'Unseen Domains', 'Target Domain Samples', 'Grayscale Images', 'Color Images', 'Distribution Of Sample Data', 'Maximum Mean Discrepancy', 'Training Procedure', 'Distribution Of Images', 'Marginal Distribution']",,29,"Machine learning models trained in one domain perform poorly in the other domains due to the existence of domain shift. Domain adaptation techniques solve this problem by training transferable models from the label-rich source domain to the label-scarce target domain. Unfortunately, a majority of the existing domain adaptation techniques rely on the availability of the target-domain data, and thus limit their applications to a small community across few computer vision problems. In this paper, we tackle the challenging zero-shot domain adaptation (ZSDA) problem, where the target-domain data is non-available in the training stage. For this purpose, we propose conditional coupled generative adversarial networks (CoCoGAN) by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. Compared with the existing state of the arts, our proposed CoCoGAN is able to capture the joint distribution of dual-domain samples in two different tasks, i.e. the relevant task (RT) and an irrelevant task (IRT). We train the CoCoGAN with both source-domain samples in RT and the dual-domain samples in IRT to complete the domain adaptation. While the former provide the high-level concepts of the non-available target-domain data, the latter carry the sharing correlation between the two domains in RT and IRT. To train the CoCoGAN in the absence of the target-domain data for RT, we propose a new supervisory signal, i.e. the alignment between representations across tasks. Extensive experiments carried out demonstrate that our proposed CoCoGAN outperforms existing state of the arts in image classifications."
Conditional Recurrent Flow: Conditional Generation of Longitudinal Samples With Applications to Neuroimaging,"Seong Jae Hwang,  Zirui Tao,  Won Hwa Kim,  Vikas Singh",Univ. of Texas at Arlington; Univ. of Wisconsin-Madison; Univ. of Pittsburgh,100.0,USA,0.0,,"We develop a conditional generative model for longitudinal image datasets based on sequential invertible neural networks. Longitudinal image acquisitions are common in various scientific and biomedical studies where often each image sequence sample may also come together with various secondary (fixed or temporally dependent) measurements. The key goal is not only to estimate the parameters of a deep generative model for the given longitudinal data, but also to enable evaluation of how the temporal course of the generated longitudinal samples are influenced as a function of induced changes in the (secondary) temporal measurements (or events). Our proposed formulation incorporates recurrent subnetworks and temporal context gating, which provides a smooth transition in a temporal sequence of generated data that can be easily informed or modulated by secondary temporal conditioning variables. We show that the formulation works well despite the smaller sample sizes common in these applications. Our model is validated on two video datasets and a longitudinal Alzheimer's disease (AD) dataset for both quantitative and qualitative evaluations of the generated samples. Further, using our generated longitudinal image samples, we show that we can capture the pathological progressions in the brain that turn out to be consistent with the existing literature, and could facilitate various types of downstream statistical analysis.",http://openaccess.thecvf.com/content_ICCV_2019/html/Hwang_Conditional_Recurrent_Flow_Conditional_Generation_of_Longitudinal_Samples_With_Applications_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hwang_Conditional_Recurrent_Flow_Conditional_Generation_of_Longitudinal_Samples_With_Applications_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9008303/,"['Adaptation models', 'Brain modeling', 'Computational modeling', 'Cognition', 'Couplings', 'Biological neural networks', 'Diseases']","['Longitudinal Study', 'Neural Network', 'Alzheimer’s Disease', 'Longitudinal Dataset', 'Longitudinal Imaging', 'Sequencing Data', 'Latent Variables', 'Sequencing Of Samples', 'Mild Cognitive Impairment', 'Density Estimation', 'Recurrent Network', 'Long Short-term Memory', 'Sequential Model', 'Latent Space', 'Inverse Problem', 'Hidden State', 'Jacobian Matrix', 'Apparel', 'Gated Recurrent Unit', 'Normal Flow', 'Alzheimer’s Disease Neuroimaging Initiative', 'Forward Mapping', 'Standardized Uptake Value Ratio', 'Inverse Mapping', 'Previous Time Point', 'Recurrent Neural Network', 'Jacobian Determinant', 'Latent Information', 'Training Data', 'Appendix For List']",,5,"We develop a conditional generative model for longitudinal image datasets based on sequential invertible neural networks. Longitudinal image acquisitions are common in various scientific and biomedical studies where often each image sequence sample may also come together with various secondary (fixed or temporally dependent) measurements. The key goal is not only to estimate the parameters of a deep generative model for the given longitudinal data, but also to enable evaluation of how the temporal course of the generated longitudinal samples are influenced as a function of induced changes in the (secondary) temporal measurements (or events). Our proposed formulation incorporates recurrent subnetworks and temporal context gating, which provides a smooth transition in a temporal sequence of generated data that can be easily informed or modulated by secondary temporal conditioning variables. We show that the formulation works well despite the smaller sample sizes common in these applications. Our model is validated on two video datasets and a longitudinal Alzheimer's disease (AD) dataset for both quantitative and qualitative evaluations of the generated samples. Further, using our generated longitudinal image samples, we show that we can capture the pathological progressions in the brain that turn out to be consistent with the existing literature, and could facilitate various types of downstream statistical analysis."
Confidence Regularized Self-Training,"Yang Zou, Zhiding Yu, Xiaofeng Liu, B.V.K. Vijaya Kumar, Jinsong Wang",General Motors R&D; Carnegie Mellon University; NVIDIA,33.33333333333333,usa,66.66666666666667,USA,"Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Confidence_Regularized_Self-Training_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Confidence_Regularized_Self-Training_ICCV_2019_paper.pdf,,https://github.com/yzou2/CRST,,main,Oral,https://ieeexplore.ieee.org/document/9010413/,"['Training', 'Entropy', 'Minimization', 'Noise measurement', 'Optimization', 'Adaptation models', 'Semantics']","['Smoothing', 'Image Classification', 'Error Propagation', 'Semantic Segmentation', 'Target Domain', 'Domain Adaptation', 'Learning Rate', 'Deep Network', 'Deep Neural Network', 'Validation Set', 'Lagrange Multiplier', 'Weight Decay', 'ImageNet', 'Generative Adversarial Networks', 'Global Minimum', 'Target Data', 'Semi-supervised Learning', 'Source Domain', 'Minimum Entropy', 'Negative Entropy', 'Softmax Probability', 'Maximum Mean Discrepancy', 'Noisy Labels']",,498,"Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST."
Consensus Maximization Tree Search Revisited,"Zhipeng Cai, Tat-Jun Chin, Vladlen Koltun",Intel Labs; The University of Adelaide,50.0,australia,50.0,USA,"Consensus maximization is widely used for robust fitting in computer vision. However, solving it exactly, i.e., finding the globally optimal solution, is intractable. A* tree search, which has been shown to be fixed-parameter tractable, is one of the most efficient exact methods, though it is still limited to small inputs. We make two key contributions towards improving A* tree search. First, we show that the consensus maximization tree structure used previously actually contains paths that connect nodes at both adjacent and non-adjacent levels. Crucially, paths connecting non-adjacent levels are redundant for tree search, but they were not avoided previously. We propose a new acceleration strategy that avoids such redundant paths. In the second contribution, we show that the existing branch pruning technique also deteriorates quickly with the problem dimension. We then propose a new branch pruning technique that is less dimension-sensitive to address this issue. Experiments show that both new techniques can significantly accelerate A* tree search, making it reasonably efficient on inputs that were previously out of reach. Demo code is available at https://github.com/ZhipengCai/MaxConTreeSearch.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Consensus_Maximization_Tree_Search_Revisited_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Consensus_Maximization_Tree_Search_Revisited_ICCV_2019_paper.pdf,,https://github.com/ZhipengCai/MaxConTreeSearch,,main,Oral,https://ieeexplore.ieee.org/document/9010731/,"['Acceleration', 'Data models', 'Robustness', 'Heuristic algorithms', 'Runtime', 'Computer vision', 'Computational modeling']","['Tree Search', 'Maximum Consensus', 'Global Optimization', 'Acceleration Scheme', 'Nonlinear Problem', 'Image Pairs', 'Root Node', 'Number Of Outliers', 'Breadth-first Search', 'Nonlinear Case', 'Adjacent Base', 'Standard Solver', 'Acceleration Techniques']",,17,"Consensus maximization is widely used for robust fitting in computer vision. However, solving it exactly, i.e., finding the globally optimal solution, is intractable. A* tree search, which has been shown to be fixed-parameter tractable, is one of the most efficient exact methods, though it is still limited to small inputs. We make two key contributions towards improving A* tree search. First, we show that the consensus maximization tree structure used previously actually contains paths that connect nodes at both adjacent and non-adjacent levels. Crucially, paths connecting non-adjacent levels are redundant for tree search, but they were not avoided previously. We propose a new acceleration strategy that avoids such redundant paths. In the second contribution, we show that the existing branch pruning technique also deteriorates quickly with the problem dimension. We then propose a new branch pruning technique that is less dimension-sensitive to address this issue. Experiments show that both new techniques can significantly accelerate A* tree search, making it reasonably efficient on inputs that were previously out of reach. Demo code is available at https://github.com/ZhipengCai/MaxConTreeSearch."
Conservative Wasserstein Training for Pose Estimation,"Xiaofeng Liu, Yang Zou, Tong Che, Peng Ding, Ping Jia, Jane You, B.V.K. Vijaya Kumar","Carnegie Mellon University; Harvard University; CIOMP, Chinese Academy of Sciences; The Hong Kong Polytechnic University; Carnegie Mellon University; MILA",83.33333333333334,"Hong Kong, china, usa",16.666666666666657,Canada,"This paper targets the task with discrete and periodic class labels (e.g., pose/orientation estimation) in the context of deep learning. The commonly used cross-entropy or regression loss is not well matched to this problem as they ignore the periodic nature of the labels and the class similarity, or assume labels are continuous value. We propose to incorporate inter-class correlations in a Wasserstein training framework by pre-defining (i.e., using arc length of a circle) or adaptively learning the ground metric. We extend the ground metric as a linear, convex or concave increasing function w.r.t. arc length from an optimization perspective. We also propose to construct the conservative target labels which model the inlier and outlier noises using a wrapped unimodal-uniform mixture distribution. Unlike the one-hot setting, the conservative label makes the computation of Wasserstein distance more challenging. We systematically conclude the practical closed-form solution of Wasserstein distance for pose data with either one-hot or conservative target label. We evaluate our method on head, body, vehicle and 3D object pose benchmarks with exhaustive ablation studies. The Wasserstein loss obtaining superior performance over the current methods, especially using convex mapping function for ground metric, conservative label, and closed-form solution.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Conservative_Wasserstein_Training_for_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Conservative_Wasserstein_Training_for_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009022/,"['Pose estimation', 'Training', 'Computational modeling', 'Uncertainty', 'Loss measurement', 'Task analysis']","['Pose Estimation', 'Cross-entropy Loss', 'Continuous Values', 'Adaptive Learning', 'Arc Length', 'Mixture Distribution', 'Target Label', 'Regression Loss', 'Object Pose', 'Optimistic Perspective', 'Head Pose', 'Discrete Labels', 'Uniform Distribution', 'Poisson Distribution', 'Probability Density Function', 'Network Parameters', 'Multi-label', 'Bounding Box', 'Convex Function', 'Unimodal Distribution', 'Metric Learning', 'Discrete Distribution', 'Circle Points', 'Empirical Risk Minimization', 'Alternating Optimization', 'Fast Computation', 'Ground-truth Class', 'One-hot Label', 'Concave Function']",,18,"This paper targets the task with discrete and periodic class labels (e.g., pose/orientation estimation) in the context of deep learning. The commonly used cross-entropy or regression loss is not well matched to this problem as they ignore the periodic nature of the labels and the class similarity, or assume labels are continuous value. We propose to incorporate inter-class correlations in a Wasserstein training framework by pre-defining (i.e., using arc length of a circle) or adaptively learning the ground metric. We extend the ground metric as a linear, convex or concave increasing function w.r.t. arc length from an optimization perspective. We also propose to construct the conservative target labels which model the inlier and outlier noises using a wrapped unimodal-uniform mixture distribution. Unlike the one-hot setting, the conservative label makes the computation of Wasserstein distance more challenging. We systematically conclude the practical closed-form solution of Wasserstein distance for pose data with either one-hot or conservative target label. We evaluate our method on head, body, vehicle and 3D object pose benchmarks with exhaustive ablation studies. The Wasserstein loss obtaining superior performance over the current methods, especially using convex mapping function for ground metric, conservative label, and closed-form solution."
Constructing Self-Motivated Pyramid Curriculums for Cross-Domain Semantic Segmentation: A Non-Adversarial Approach,"Qing Lian, Fengmao Lv, Lixin Duan, Boqing Gong",University of Electronic Science and Technology of China; Google,50.0,china,50.0,USA,"We propose a new approach, called self-motivated pyramid curriculum domain adaptation (PyCDA), to facilitate the adaptation of semantic segmentation neural networks from synthetic source domains to real target domains. Our approach draws on an insight connecting two existing works: curriculum domain adaptation and self-training. Inspired by the former, PyCDA constructs a pyramid curriculum which contains various properties about the target domain. Those properties are mainly about the desired label distributions over the target domain images, image regions, and pixels. By enforcing the segmentation neural network to observe those properties, we can improve the network's generalization capability to the target domain. Motivated by the self-training, we infer this pyramid of properties by resorting to the semantic segmentation network itself. Unlike prior work, we do not need to maintain any additional models (e.g., logistic regression or discriminator networks) or to solve minmax problems which are often difficult to optimize. We report state-of-the-art results for the adaptation from both GTAV and SYNTHIA to Cityscapes, two popular settings in unsupervised domain adaptation for semantic segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lian_Constructing_Self-Motivated_Pyramid_Curriculums_for_Cross-Domain_Semantic_Segmentation_A_Non-Adversarial_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lian_Constructing_Self-Motivated_Pyramid_Curriculums_for_Cross-Domain_Semantic_Segmentation_A_Non-Adversarial_ICCV_2019_paper.pdf,,https://github.com/lianqing11/pycda,,main,Poster,https://ieeexplore.ieee.org/document/9010374/,"['Image segmentation', 'Semantics', 'Training', 'Adaptation models', 'Task analysis', 'Neural networks', 'Logistics']","['Semantic Segmentation', 'Neural Network', 'Image Regions', 'Target Domain', 'Domain Adaptation', 'Semantic Network', 'Source Domain', 'Label Distribution', 'Semantic Segmentation Network', 'Synthetic Sources', 'Target Domain Images', 'Training Data', 'Deep Neural Network', 'Validation Set', 'Small Region', 'Cross-entropy Loss', 'Target Image', 'Source Images', 'Synthetic Images', 'Pixel Level', 'Pseudo Labels', 'Square Pixels', 'Style Transfer', 'Adversarial Training', 'Urban Scenes', 'Global Image', 'Curriculum Adaptation', 'Source Labels', 'Domain Adaptation Methods', 'Spatial Layout']",,142,"We propose a new approach, called self-motivated pyramid curriculum domain adaptation (PyCDA), to facilitate the adaptation of semantic segmentation neural networks from synthetic source domains to real target domains. Our approach draws on an insight connecting two existing works: curriculum domain adaptation and self-training. Inspired by the former, PyCDA constructs a pyramid curriculum which contains various properties about the target domain. Those properties are mainly about the desired label distributions over the target domain images, image regions, and pixels. By enforcing the segmentation neural network to observe those properties, we can improve the network's generalization capability to the target domain. Motivated by the self-training, we infer this pyramid of properties by resorting to the semantic segmentation network itself. Unlike prior work, we do not need to maintain any additional models (e.g., logistic regression or discriminator networks) or to solve minmax problems which are often difficult to optimize. We report state-of-the-art results for the adaptation from both GTAV and SYNTHIA to Cityscapes, two popular settings in unsupervised domain adaptation for semantic segmentation."
Content and Style Disentanglement for Artistic Style Transfer,"Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, BjÃ¶rn Ommer","Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University",100.0,germany,0.0,,"Artists rarely paint in a single style throughout their career. More often they change styles or develop variations of it. In addition, artworks in different styles and even within one style depict real content differently: while Picasso's Blue Period displays a vase in a blueish tone but as a whole, his Cubist works deconstruct the object. To produce artistically convincing stylizations, style transfer models must be able to reflect these changes and variations. Recently many works have aimed to improve the style transfer task, but neglected to address the described observations. We present a novel approach which captures particularities of style and the variations within and separates style and content. This is achieved by introducing two novel losses: a fixpoint triplet style loss to learn subtle variations within one style or between different styles and a disentanglement loss to ensure that the stylization is not conditioned on the real input photo. In addition the paper proposes various evaluation methods to measure the importance of both losses on the validity, quality and variability of final stylizations. We provide qualitative results to demonstrate the performance of our approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kotovenko_Content_and_Style_Disentanglement_for_Artistic_Style_Transfer_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kotovenko_Content_and_Style_Disentanglement_for_Artistic_Style_Transfer_ICCV_2019_paper.pdf,,https://compvis.github.io/content-style-disentangled-ST/,,main,Poster,https://ieeexplore.ieee.org/document/9008547/,"['Task analysis', 'Painting', 'Feature extraction', 'Aerospace electronics', 'Image generation', 'Decoding', 'Loss measurement']","['Style Transfer', 'Artistic Style Transfer', 'Disentanglement Of Style', 'Fixed Point', 'Subtle Variations', 'Neural Network', 'Changes In Content', 'Convolutional Neural Network', 'Subtle Changes', 'Probability Density Function', 'ImageNet', 'Latent Space', 'Encoder-decoder', 'Image Collection', 'Pre-trained Network', 'Image Synthesis', 'Original Content', 'Variety Of Styles', 'Image X', 'Triplet Loss', 'Style Image', 'Multiple Styles', 'Preservation Of Content']",,92,"Artists rarely paint in a single style throughout their career. More often they change styles or develop variations of it. In addition, artworks in different styles and even within one style depict real content differently: while Picasso's Blue Period displays a vase in a blueish tone but as a whole, his Cubist works deconstruct the object. To produce artistically convincing stylizations, style transfer models must be able to reflect these changes and variations. Recently many works have aimed to improve the style transfer task, but neglected to address the described observations. We present a novel approach which captures particularities of style and the variations within and separates style and content. This is achieved by introducing two novel losses: a fixpoint triplet style loss to learn subtle variations within one style or between different styles and a disentanglement loss to ensure that the stylization is not conditioned on the real input photo. In addition the paper proposes various evaluation methods to measure the importance of both losses on the validity, quality and variability of final stylizations. We provide qualitative results to demonstrate the performance of our approach."
Context-Aware Emotion Recognition Networks,"Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, Kwanghoon Sohn",´Ecole Polytechnique F ´ed´erale de Lausanne (EPFL); Yonsei University,100.0,"france, south korea, switzerland",0.0,,"Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to separately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at http://caer-dataset.github.io.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Context-Aware_Emotion_Recognition_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Context-Aware_Emotion_Recognition_Networks_ICCV_2019_paper.pdf,http://caer-dataset.github.io,,,main,Poster,https://ieeexplore.ieee.org/document/9008268/,"['Emotion recognition', 'Encoding', 'Videos', 'Feature extraction', 'Visualization', 'Convolution', 'Adaptive systems']","['Emotion Recognition', 'Benchmark', 'Deep Network', 'Contextual Information', 'Facial Expressions', 'Attention Mechanism', 'Human Faces', 'Human Facial Expressions', 'Dynamic Model', 'Convolutional Neural Network', 'Convolutional Layers', 'Recurrent Neural Network', 'Deep Convolutional Neural Network', 'Video Clips', 'Image Recognition', 'Face Images', 'Attention Module', 'Static Model', 'Max-pooling Layer', 'Emotion Categories', 'Emotion Recognition Performance', 'Discriminative Parts', 'Discriminative Regions', 'Emotion Recognition Accuracy', 'Sentiment Analysis', 'Emotional Signals', 'Attention Weights', 'Average Pooling Layer', 'Visual Content']",,151,"Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to separately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at http://caer-dataset.github.io."
Context-Aware Feature and Label Fusion for Facial Action Unit Intensity Estimation With Partially Labeled Data,"Yong Zhang, Haiyong Jiang, Baoyuan Wu, Yanbo Fan, Qiang Ji","Tencent AI Lab; Rensselaer Polytechnic Institute; Nanyang Technological University, Singapore",66.66666666666666,"Singapore, usa",33.33333333333334,China,"Facial action unit (AU) intensity estimation is a fundamental task for facial behaviour analysis. Most previous methods use a whole face image as input for intensity prediction. Considering that AUs are defined according to their corresponding local appearance, a few patch-based methods utilize image features of local patches. However, fusion of local features is always performed via straightforward feature concatenation or summation. Besides, these methods require fully annotated databases for model learning, which is expensive to acquire. In this paper, we propose a novel weakly supervised patch-based deep model on basis of two types of attention mechanisms for joint intensity estimation of multiple AUs. The model consists of a feature fusion module and a label fusion module. And we augment attention mechanisms of these two modules with a learnable task-related context, as one patch may play different roles in analyzing different AUs and each AU has its own temporal evolution rule. The context-aware feature fusion module is used to capture spatial relationships among local patches while the context-aware label fusion module is used to capture the temporal dynamics of AUs. The latter enables the model to be trained on a partially annotated database. Experimental evaluations on two benchmark expression databases demonstrate the superior performance of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Context-Aware_Feature_and_Label_Fusion_for_Facial_Action_Unit_Intensity_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Context-Aware_Feature_and_Label_Fusion_for_Facial_Action_Unit_Intensity_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010955/,"['Gold', 'Feature extraction', 'Task analysis', 'Estimation', 'Databases', 'Adaptation models', 'Face']","['Volume Estimation', 'Feature Fusion', 'Action Units', 'Facial Action Units', 'Label Fusion', 'Action Unit Intensity', 'Facial Action Unit Intensity', 'Contralateral', 'Local Features', 'Deep Models', 'Temporal Dynamics', 'Attention Mechanism', 'Types Of Mechanisms', 'Face Images', 'Patch Features', 'Feature Fusion Module', 'Patch-based Methods', 'Deep Learning', 'Objective Function', 'Supervised Learning', 'Key Frames', 'Semi-supervised Methods', 'Long Short-term Memory', 'Facial Action Coding System', 'Supervised Learning Methods', 'Facial Appearance', 'Multilayer Perceptron', 'Spatial Relationship', 'Training Frames', 'Global Features']",,20,"Facial action unit (AU) intensity estimation is a fundamental task for facial behaviour analysis. Most previous methods use a whole face image as input for intensity prediction. Considering that AUs are defined according to their corresponding local appearance, a few patch-based methods utilize image features of local patches. However, fusion of local features is always performed via straightforward feature concatenation or summation. Besides, these methods require fully annotated databases for model learning, which is expensive to acquire. In this paper, we propose a novel weakly supervised patch-based deep model on basis of two types of attention mechanisms for joint intensity estimation of multiple AUs. The model consists of a feature fusion module and a label fusion module. And we augment attention mechanisms of these two modules with a learnable task-related context, as one patch may play different roles in analyzing different AUs and each AU has its own temporal evolution rule. The context-aware feature fusion module is used to capture spatial relationships among local patches while the context-aware label fusion module is used to capture the temporal dynamics of AUs. The latter enables the model to be trained on a partially annotated database. Experimental evaluations on two benchmark expression databases demonstrate the superior performance of the proposed method."
Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation,"Qiqi Hou, Feng Liu",Portland State University,100.0,usa,0.0,,"Natural image matting is an important problem in computer vision and graphics. It is an ill-posed problem when only an input image is available without any external information. While the recent deep learning approaches have shown promising results, they only estimate the alpha matte. This paper presents a context-aware natural image matting method for simultaneous foreground and alpha matte estimation. Our method employs two encoder networks to extract essential information for matting. Particularly, we use a matting encoder to learn local features and a context encoder to obtain more global context information. We concatenate the outputs from these two encoders and feed them into decoder networks to simultaneously estimate the foreground and alpha matte. To train this whole deep neural network, we employ both the standard Laplacian loss and the feature loss: the former helps to achieve high numerical performance while the latter leads to more perceptually plausible results. We also report several data augmentation strategies that greatly improve the network's generalization performance. Our qualitative and quantitative experiments show that our method enables high-quality matting for a single natural image.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Context-Aware_Image_Matting_for_Simultaneous_Foreground_and_Alpha_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Context-Aware_Image_Matting_for_Simultaneous_Foreground_and_Alpha_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009079/,"['Decoding', 'Neural networks', 'Feature extraction', 'Laplace equations', 'Loss measurement', 'Estimation', 'Machine learning']","['Alpha Estimates', 'Image Matting', 'Neural Network', 'Deep Learning', 'Deep Neural Network', 'Local Features', 'Input Image', 'Data Augmentation', 'Global Information', 'External Information', 'Computer Vision Problems', 'Plausible Results', 'Global Context Information', 'Alpha Matte', 'Context Encoder', 'Loss Function', 'Convolutional Neural Network', 'Convolutional Layers', 'Fine Structure', 'User Study', 'Real-world Images', 'Background Image', 'Gaussian Blur', 'Image Synthesis', 'High-quality Images', 'Downsampling Factor', 'Real-world Examples', 'Global Sample', 'Image Patches', 'Short Side']",,102,"Natural image matting is an important problem in computer vision and graphics. It is an ill-posed problem when only an input image is available without any external information. While the recent deep learning approaches have shown promising results, they only estimate the alpha matte. This paper presents a context-aware natural image matting method for simultaneous foreground and alpha matte estimation. Our method employs two encoder networks to extract essential information for matting. Particularly, we use a matting encoder to learn local features and a context encoder to obtain more global context information. We concatenate the outputs from these two encoders and feed them into decoder networks to simultaneously estimate the foreground and alpha matte. To train this whole deep neural network, we employ both the standard Laplacian loss and the feature loss: the former helps to achieve high numerical performance while the latter leads to more perceptually plausible results. We also report several data augmentation strategies that greatly improve the network's generalization performance. Our qualitative and quantitative experiments show that our method enables high-quality matting for a single natural image."
Contextual Attention for Hand Detection in the Wild,"Supreeth Narasimhaswamy, Zhengwei Wei, Yang Wang, Justin Zhang, Minh Hoai",Stony Brook University; VinAI Research; Caltech,66.66666666666666,usa,33.33333333333334,Vietnam,"We present Hand-CNN, a novel convolutional network architecture for detecting hand masks and predicting hand orientations in unconstrained images. Hand-CNN extends MaskRCNN with a novel attention mechanism to incorporate contextual cues in the detection process. This attention mechanism can be implemented as an efficient network module that captures non-local dependencies between features. This network module can be inserted at different stages of an object detection network, and the entire detector can be trained end-to-end. We also introduce large-scale annotated hand datasets containing hands in unconstrained images for training and evaluation. We show that Hand-CNN outperforms existing methods on the newly collected datasets and the publicly available PASCAL VOC human layout dataset. Data and code: https://www3.cs.stonybrook.edu/ cvl/projects/hand_det_attention/",,http://openaccess.thecvf.com/content_ICCV_2019/html/Narasimhaswamy_Contextual_Attention_for_Hand_Detection_in_the_Wild_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Narasimhaswamy_Contextual_Attention_for_Hand_Detection_in_the_Wild_ICCV_2019_paper.pdf,https://www3.cs.stonybrook.edu/~cvl/projects/hand_det_attention/,,,main,Poster,https://ieeexplore.ieee.org/document/9008388/,"['Videos', 'Detectors', 'Skin', 'Wrist', 'Shape', 'TV', 'Object detection']","['Contextual Attention', 'Object Detection', 'Attention Mechanism', 'Large-scale Datasets', 'Detection Process', 'Contextual Cues', 'Heuristic', 'Training Data', 'Performance Of Method', 'Body Parts', 'Bounding Box', 'Detection Task', 'Video Frames', 'Attention Module', 'Action Recognition', 'Manual Annotation', 'Parts Of The Human Body', 'Error Threshold', 'Branch Network', 'COCO Dataset', 'TV Series', 'Motion Blur', 'Hand Area', 'Lack Of Training Data', 'Region Proposal Network', 'Feature Maps', 'Test Data']",,38,"We present Hand-CNN, a novel convolutional network architecture for detecting hand masks and predicting hand orientations in unconstrained images. Hand-CNN extends MaskRCNN with a novel attention mechanism to incorporate contextual cues in the detection process. This attention mechanism can be implemented as an efficient network module that captures non-local dependencies between features. This network module can be inserted at different stages of an object detection network, and the entire detector can be trained end-to-end. We also introduce large-scale annotated hand datasets containing hands in unconstrained images for training and evaluation. We show that Hand-CNN outperforms existing methods on the newly collected datasets and the publicly available PASCAL VOC human layout dataset. Data and code: https://www3.cs.stonybrook.edu/~cvl/projects/hand_det_attention/"
Continual Learning by Asymmetric Loss Approximation With Single-Side Overestimation,"Dongmin Park, Seokil Hong, Bohyung Han, Kyoung Mu Lee","ECE & ASRI, Seoul National University, Korea",100.0,south korea,0.0,,"Catastrophic forgetting is a critical challenge in training deep neural networks. Although continual learning has been investigated as a countermeasure to the problem, it often suffers from the requirements of additional network components and the limited scalability to a large number of tasks. We propose a novel approach to continual learning by approximating a true loss function using an asymmetric quadratic function with one of its sides overestimated. Our algorithm is motivated by the empirical observation that the network parameter updates affect the target loss functions asymmetrically. In the proposed continual learning framework, we estimate an asymmetric loss function for the tasks considered in the past through a proper overestimation of its unobserved sides in training new tasks, while deriving the accurate model parameter for the observable sides. In contrast to existing approaches, our method is free from the side effects and achieves the state-of-the-art accuracy that is even close to the upper-bound performance on several challenging benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Park_Continual_Learning_by_Asymmetric_Loss_Approximation_With_Single-Side_Overestimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Continual_Learning_by_Asymmetric_Loss_Approximation_With_Single-Side_Overestimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009502/,"['Task analysis', 'Training', 'Neural networks', 'Approximation algorithms', 'Optimization', 'Batch production systems', 'Scalability']","['Incremental Learning', 'Asymmetric Loss', 'Loss Function', 'Neural Network', 'Model Parameters', 'Deep Neural Network', 'Quadratic Function', 'Benchmark Datasets', 'Parameter Update', 'Challenging Dataset', 'True Function', 'Asymmetric Function', 'Catastrophic Forgetting', 'Accurate Measurement', 'Hyperparameters', 'Convolutional Layers', 'Mapping Approach', 'Episodic Memory', 'Stochastic Gradient Descent', 'Max-pooling', 'Architecture Approach', 'Quadratic Loss', 'Previous Tasks', 'Symmetric Function', 'Standard Datasets', 'Surrogate Function', 'CIFAR-100 Dataset', 'Taylor Series', 'Regular Structure', 'Current Task']",,19,"Catastrophic forgetting is a critical challenge in training deep neural networks. Although continual learning has been investigated as a countermeasure to the problem, it often suffers from the requirements of additional network components and the limited scalability to a large number of tasks. We propose a novel approach to continual learning by approximating a true loss function using an asymmetric quadratic function with one of its sides overestimated. Our algorithm is motivated by the empirical observation that the network parameter updates affect the target loss functions asymmetrically. In the proposed continual learning framework, we estimate an asymmetric loss function for the tasks considered in the past through a proper overestimation of its unobserved sides in training new tasks, while deriving the accurate model parameter for the observable sides. In contrast to existing approaches, our method is free from the side effects and achieves the state-of-the-art accuracy that is even close to the upper-bound performance on several challenging benchmark datasets."
Controllable Artistic Text Style Transfer via Shape-Matching GAN,"Shuai Yang, Zhangyang Wang, Zhaowen Wang, Ning Xu, Jiaying Liu, Zongming Guo","Adobe Research; Texas A&M University; Institute of Computer Science and Technology, Peking University",66.66666666666666,"china, usa",33.33333333333334,USA,"Artistic text style transfer is the task of migrating the style from a source image to the target text to create artistic typography. Recent style transfer methods have considered texture control to enhance usability. However, controlling the stylistic degree in terms of shape deformation remains an important open challenge. In this paper, we present the first text style transfer network that allows for real-time control of the crucial stylistic degree of the glyph through an adjustable parameter. Our key contribution is a novel bidirectional shape matching framework to establish an effective glyph-style mapping at various deformation levels without paired ground truth. Based on this idea, we propose a scale-controllable module to empower a single network to continuously characterize the multi-scale shape features of the style image and transfer these features to the target text. The proposed method demonstrates its superiority over previous state-of-the-arts in generating diverse, controllable and high-quality stylized text.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Controllable_Artistic_Text_Style_Transfer_via_Shape-Matching_GAN_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Controllable_Artistic_Text_Style_Transfer_via_Shape-Matching_GAN_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008123/,"['Shape', 'Strain', 'Real-time systems', 'Gallium nitride', 'Task analysis', 'Training', 'Deformable models']","['Generative Adversarial Networks', 'Style Transfer', 'Text Style', 'Text Style Transfer', 'Style Image', 'Target Text', 'Neural Network', 'Feed-forward Network', 'Reference Image', 'Image Patches', 'Image Texture', 'Fast Transfer', 'Robust Function', 'Degree Of Deformation', 'Transfer Problem', 'Continuous Transformation', 'Forward Transfer', 'Transformer Block', 'Reference Style']",,69,"Artistic text style transfer is the task of migrating the style from a source image to the target text to create artistic typography. Recent style transfer methods have considered texture control to enhance usability. However, controlling the stylistic degree in terms of shape deformation remains an important open challenge. In this paper, we present the first text style transfer network that allows for real-time control of the crucial stylistic degree of the glyph through an adjustable parameter. Our key contribution is a novel bidirectional shape matching framework to establish an effective glyph-style mapping at various deformation levels without paired ground truth. Based on this idea, we propose a scale-controllable module to empower a single network to continuously characterize the multi-scale shape features of the style image and transfer these features to the target text. The proposed method demonstrates its superiority over previous state-of-the-arts in generating diverse, controllable and high-quality stylized text."
Controllable Attention for Structured Layered Video Decomposition,"Jean-Baptiste Alayrac, JoÃ£o Carreira, Relja ArandjeloviÄ, Andrew Zisserman","DeepMind; VGG, Dept. of Engineering Science, University of Oxford",50.0,uk,50.0,UK,"The objective of this paper is to be able to separate a video into its natural layers, and to control which of the separated layers to attend to. For example, to be able to separate reflections, transparency or object motion. We make the following three contributions: (i) we introduce a new structured neural network architecture that explicitly incorporates layers (as spatial masks) into its design. This improves separation performance over previous general purpose networks for this task; (ii) we demonstrate that we can augment the architecture to leverage external cues such as audio for controllability and to help disambiguation; and (iii) we experimentally demonstrate the effectiveness of our approach and training procedure with controlled experiments while also showing that the proposed model can be successfully applied to real-word applications such as reflection removal and action recognition in cluttered scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Alayrac_Controllable_Attention_for_Structured_Layered_Video_Decomposition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Alayrac_Controllable_Attention_for_Structured_Layered_Video_Decomposition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010833/,"['Visualization', 'Task analysis', 'Computer architecture', 'Mirrors', 'Decoding', 'Generators', 'Controllability']","['Decomposition Layers', 'Training Procedure', 'Disambiguation', 'Action Recognition', 'External Cues', 'Control Signal', 'Visual Features', 'Symmetry Breaking', 'Training Loss', 'Composite Model', 'Channel Dimension', 'Reconstruction Loss', 'U-Net Architecture', 'Gradient Operator', 'Decomposition Model', 'Audio Stream', 'Covert Attention', 'Internal Prediction', 'Action Recognition Task', 'Deterministic Control', 'Video Output']",,8,"The objective of this paper is to be able to separate a video into its natural layers, and to control which of the separated layers to attend to. For example, to be able to separate reflections, transparency or object motion. We make the following three contributions: (i) we introduce a new structured neural network architecture that explicitly incorporates layers (as spatial masks) into its design. This improves separation performance over previous general purpose networks for this task; (ii) we demonstrate that we can augment the architecture to leverage external cues such as audio for controllability and to help disambiguation; and (iii) we experimentally demonstrate the effectiveness of our approach and training procedure with controlled experiments while also showing that the proposed model can be successfully applied to real-word applications such as reflection removal and action recognition in cluttered scenes."
Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network,"Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Jingwen Wang, Wei Liu","Tencent AI Lab; School of Control Science and Engineering, Shandong University",50.0,china,50.0,China,"In this paper, we propose to guide the video caption generation with Part-of-Speech (POS) information, based on a gated fusion of multiple representations of input videos. We construct a novel gated fusion network, with one particularly designed cross-gating (CG) block, to effectively encode and fuse different types of representations, e.g., the motion and content features of an input video. One POS sequence generator relies on this fused representation to predict the global syntactic structure, which is thereafter leveraged to guide the video captioning generation and control the syntax of the generated sentence. Specifically, a gating strategy is proposed to dynamically and adaptively incorporate the global syntactic POS information into the decoder for generating each word. Experimental results on two benchmark datasets, namely MSR-VTT and MSVD, demonstrate that the proposed model can well exploit complementary information from multiple representations, resulting in improved performances. Moreover, the generated global POS information can well capture the global syntactic structure of the sentence, and thus be exploited to control the syntactic structure of the description. Such POS information not only boosts the video captioning performance but also improves the diversity of the generated captions. Our code is at: https://github.com/vsislab/Controllable_XGating.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.pdf,,https://github.com/vsislab/Controllable_XGating,,main,Poster,https://ieeexplore.ieee.org/document/9009800/,"['Logic gates', 'Generators', 'Syntactics', 'Semantics', 'Fuses', 'Decoding', 'Encoding']","['Video Captioning', 'Gated Fusion Network', 'Part-of-speech Sequence', 'Benchmark Datasets', 'Global Information', 'Global Structure', 'Motion Features', 'Content Features', 'Syntactic Structure', 'Multiple Representations', 'Video Features', 'Syntactic Information', 'Different Types Of Representations', 'Information Content', 'Previous Step', 'Diverse Characteristics', 'Video Clips', 'Learnable Parameters', 'Optical Flow', 'Current Step', 'Postage', 'Video Encoding', 'Words In Sentences', 'Image Captioning', 'Motion Information', 'Description Language', 'Video Content', 'Text Generation', 'Simple Concatenation', 'Word Embedding']",,122,"In this paper, we propose to guide the video caption generation with Part-of-Speech (POS) information, based on a gated fusion of multiple representations of input videos. We construct a novel gated fusion network, with one particularly designed cross-gating (CG) block, to effectively encode and fuse different types of representations, e.g., the motion and content features of an input video. One POS sequence generator relies on this fused representation to predict the global syntactic structure, which is thereafter leveraged to guide the video captioning generation and control the syntax of the generated sentence. Specifically, a gating strategy is proposed to dynamically and adaptively incorporate the global syntactic POS information into the decoder for generating each word. Experimental results on two benchmark datasets, namely MSR-VTT and MSVD, demonstrate that the proposed model can well exploit complementary information from multiple representations, resulting in improved performances. Moreover, the generated global POS information can well capture the global syntactic structure of the sentence, and thus be exploited to control the syntactic structure of the description. Such POS information not only boosts the video captioning performance but also improves the diversity of the generated captions. Our code is at: https://github.com/vsislab/Controllable_XGating."
Controlling Neural Networks via Energy Dissipation,"Michael Moeller, Thomas MÃ¶llenhoff, Daniel Cremers",University of Siegen; TU Munich,100.0,"Germany, germany",0.0,,"The last decade has shown a tremendous success in solving various computer vision problems with the help of deep learning techniques. Lately, many works have demonstrated that learning-based approaches with suitable network architectures even exhibit superior performance for the solution of (ill-posed) image reconstruction problems such as deblurring, super-resolution, or medical image reconstruction. The drawback of purely learning-based methods, however, is that they cannot provide provable guarantees for the trained network to follow a given data formation process during inference. In this work we propose energy dissipating networks that iteratively compute a descent direction with respect to a given cost function or energy at the currently estimated reconstruction. Therefore, an adaptive step size rule such as a line-search, along with a suitable number of iterations can guarantee the reconstruction to follow a given data formation model encoded in the energy to arbitrary precision, and hence control the model's behavior even during test time. We prove that under standard assumptions, descent using the direction predicted by the network converges (linearly) to the global minimum of the energy. We illustrate the effectiveness of the proposed approach in experiments on single image super resolution and computed tomography (CT) reconstruction, and further illustrate extensions to convex feasibility problems.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Moeller_Controlling_Neural_Networks_via_Energy_Dissipation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Moeller_Controlling_Neural_Networks_via_Energy_Dissipation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009810/,"['Image reconstruction', 'Computed tomography', 'Inverse problems', 'Training', 'Neural networks', 'Image resolution', 'Iterative methods']","['Neural Network', 'Computed Tomography', 'Deep Learning', 'Medical Imaging', 'Energy Minimization', 'Super-resolution', 'Feasibility Problem', 'Descent Direction', 'Single Image Super-resolution', 'Adaptive Step Size', 'Magnetic Resonance Imaging', 'Training Data', 'Convolutional Neural Network', 'Gradient Descent', 'Convergence Rate', 'Energy Function', 'Network Output', 'Vector Field', 'Inverse Problem', 'Linear Operator', 'Linear Inverse Problem', 'Data Fidelity Term', 'Iterative Gradient Descent', 'Deblurring', 'Toy Example', 'Accurate Solution', 'Regularization Method', 'Training Distribution', 'Lyapunov Function', 'Total Variation Regularization']",,8,"The last decade has shown a tremendous success in solving various computer vision problems with the help of deep learning techniques. Lately, many works have demonstrated that learning-based approaches with suitable network architectures even exhibit superior performance for the solution of (ill-posed) image reconstruction problems such as deblurring, super-resolution, or medical image reconstruction. The drawback of purely learning-based methods, however, is that they cannot provide provable guarantees for the trained network to follow a given data formation process during inference. In this work we propose energy dissipating networks that iteratively compute a descent direction with respect to a given cost function or energy at the currently estimated reconstruction. Therefore, an adaptive step size rule such as a line-search, along with a suitable number of iterations can guarantee the reconstruction to follow a given data formation model encoded in the energy to arbitrary precision, and hence control the model's behavior even during test time. We prove that under standard assumptions, descent using the direction predicted by the network converges (linearly) to the global minimum of the energy. We illustrate the effectiveness of the proposed approach in experiments on single image super resolution and computed tomography (CT) reconstruction, and further illustrate extensions to convex feasibility problems."
Convex Relaxations for Consensus and Non-Minimal Problems in 3D Vision,"Thomas Probst, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool","Computer Vision Laboratory, ETH Zurich, Switzerland",100.0,switzerland,0.0,,"In this paper, we formulate a generic non-minimal solver using the existing tools of Polynomials Optimization Problems (POP) from computational algebraic geometry. The proposed method exploits the well known Shor's or Lasserre's relaxations, whose theoretical aspects are also discussed. Notably, we further exploit the POP formulation of non-minimal solver also for the generic consensus maximization problems in 3D vision. Our framework is simple and straightforward to implement, which is also supported by three diverse applications in 3D vision, namely rigid body transformation estimation, Non-Rigid Structure-from-Motion (NRSfM), and camera autocalibration. In all three cases, both non-minimal and consensus maximization are tested, which are also compared against the state-of-the-art methods. Our results are competitive to the compared methods, and are also coherent with our theoretical analysis. The main contribution of this paper is the claim that a good approximate solution for many polynomial problems involved in 3D vision can be obtained using the existing theory of numerical computational algebra. This claim leads us to reason about why many relaxed methods in 3D vision behave so well? And also allows us to offer a generic relaxed solver in a rather straightforward way. We further show that the convex relaxation of these polynomials can easily be used for maximizing consensus in a deterministic manner. We support our claim using several experiments for aforementioned three diverse problems in 3D vision.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Probst_Convex_Relaxations_for_Consensus_and_Non-Minimal_Problems_in_3D_Vision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Probst_Convex_Relaxations_for_Consensus_and_Non-Minimal_Problems_in_3D_Vision_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010046/,"['Three-dimensional displays', 'Symmetric matrices', 'Optimization', 'Cameras', 'Minimization', 'Computer vision', 'Estimation']","['3D Visualization', 'Visual Problems', 'Maximization Problem', 'Rigid Transformation', 'Relaxation Method', 'Rigid Body Transformation', 'Computer Algebra', 'Deterministic Manner', 'Body-worn Cameras', 'Quadratic Function', '3D Reconstruction', 'Parametrized', 'Singular Value Decomposition', 'Outlier Detection', 'Presence Of Outliers', 'Iterative Refinement', 'Biquadratic', 'Semidefinite Programming', '3D Problem', 'Fundamental Matrix', 'General Polynomial', 'Christoffel Symbols', 'Moment Matrix', 'Search Paradigm', 'High-degree Polynomial']",,7,"In this paper, we formulate a generic non-minimal solver using the existing tools of Polynomials Optimization Problems (POP) from computational algebraic geometry. The proposed method exploits the well known Shor's or Lasserre's relaxations, whose theoretical aspects are also discussed. Notably, we further exploit the POP formulation of non-minimal solver also for the generic consensus maximization problems in 3D vision. Our framework is simple and straightforward to implement, which is also supported by three diverse applications in 3D vision, namely rigid body transformation estimation, Non-Rigid Structure-from-Motion (NRSfM), and camera autocalibration. In all three cases, both non-minimal and consensus maximization are tested, which are also compared against the state-of-the-art methods. Our results are competitive to the compared methods, and are also coherent with our theoretical analysis. The main contribution of this paper is the claim that a good approximate solution for many polynomial problems involved in 3D vision can be obtained using the existing theory of numerical computational algebra. This claim leads us to reason about why many relaxed methods in 3D vision behave so well? And also allows us to offer a generic relaxed solver in a rather straightforward way. We further show that the convex relaxation of these polynomials can easily be used for maximizing consensus in a deterministic manner. We support our claim using several experiments for aforementioned three diverse problems in 3D vision."
Convex Shape Prior for Multi-Object Segmentation Using a Single Level Set Function,"Shousheng Luo, Xue-Cheng Tai, Limei Huo, Yang Wang, Roland Glowinski","Department of Mathematics, University of Houston, Houston, TX 77204, USA; Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Hong Kong; School of Mathematics and Statistics, Henan University, Kaifeng, China.; Beijing Computational Science Research Center, Building 9, East Zone, ZPark II, Haidian District, Beijing, China; Department of Mathematics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong",80.0,"China, Hong Kong, USA, china",20.0,Unknown,"Many objects in real world have convex shapes. It is a difficult task to have representations for convex shapes with good and fast numerical solutions. This paper proposes a method to incorporate convex shape prior for multi-object segmentation using level set method. The relationship between the convexity of the segmented objects and the signed distance function corresponding to their union is analyzed theoretically. This result is combined with Gaussian mixture method for the multiple objects segmentation with convexity shape prior. Alternating direction method of multiplier (ADMM) is adopted to solve the proposed model. Special boundary conditions are also imposed to obtain efficient algorithms for 4th order partial differential equations in one step of ADMM algorithm. In addition, our method only needs one level set function regardless of the number of objects. So the increase in the number of objects does not result in the increase of model and algorithm complexity. Various numerical experiments are illustrated to show the performance and advantages of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Convex_Shape_Prior_for_Multi-Object_Segmentation_Using_a_Single_Level_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Convex_Shape_Prior_for_Multi-Object_Segmentation_Using_a_Single_Level_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008107/,"['Level set', 'Shape', 'Image segmentation', 'Object segmentation', 'Computational modeling', 'Computational efficiency']","['Single Function', 'Convex Shape', 'Shape Priors', 'Multi-object Segmentation', 'Efficient Algorithm', 'Partial Differential Equations', 'Segmentation Method', 'Multiple Objects', 'Number Of Objects', 'Object Segmentation', 'Multiple Segments', 'Real-world Objects', 'Level Set Method', 'Signed Distance Function', 'Optimization Problem', 'Computational Cost', 'Image Segmentation', 'Hedgehog', 'Single Object', 'Segmentation Results', 'Convex Objective', 'Object Regions', 'Discrete Cosine Transform', 'Object Boundaries', 'Image Domain', 'Shipwreck', 'Label Information', 'Proper Boundary Conditions', 'Poor Image']",,12,"Many objects in real world have convex shapes. It is a difficult task to have representations for convex shapes with good and fast numerical solutions. This paper proposes a method to incorporate convex shape prior for multi-object segmentation using level set method. The relationship between the convexity of the segmented objects and the signed distance function corresponding to their union is analyzed theoretically. This result is combined with Gaussian mixture method for the multiple objects segmentation with convexity shape prior. Alternating direction method of multiplier (ADMM) is adopted to solve the proposed model. Special boundary conditions are also imposed to obtain efficient algorithms for 4th order partial differential equations in one step of ADMM algorithm. In addition, our method only needs one level set function regardless of the number of objects. So the increase in the number of objects does not result in the increase of model and algorithm complexity. Various numerical experiments are illustrated to show the performance and advantages of the proposed method."
Convolutional Approximations to the General Non-Line-of-Sight Imaging Operator,"Byeongjoo Ahn, Akshat Dave, Ashok Veeraraghavan, Ioannis Gkioulekas, Aswin C. Sankaranarayanan",Carnegie Mellon University; Rice University,100.0,usa,0.0,,"Non-line-of-sight (NLOS) imaging aims to reconstruct scenes outside the field of view of an imaging system. A common approach is to measure the so-called light transients, which facilitates reconstructions through ellipsoidal tomography that involves solving a linear least-squares. Unfortunately, the corresponding linear operator is very high-dimensional and lacks structures that facilitate fast solvers, and so, the ensuing optimization is a computationally daunting task. We introduce a computationally tractable framework for solving the ellipsoidal tomography problem. Our main observation is that the Gram of the ellipsoidal tomography operator is convolutional, either exactly under certain idealized imaging conditions, or approximately in practice. This, in turn, allows us to obtain the ellipsoidal tomography solution by using efficient deconvolution procedures to solve a linear least-squares problem involving the Gram operator. The computational tractability of our approach also facilitates the use of various regularizers during the deconvolution procedure. We demonstrate the advantages of our framework in a variety of simulated and real experiments.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ahn_Convolutional_Approximations_to_the_General_Non-Line-of-Sight_Imaging_Operator_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ahn_Convolutional_Approximations_to_the_General_Non-Line-of-Sight_Imaging_Operator_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009533/,"['Nonlinear optics', 'Transient analysis', 'Image reconstruction', 'Volume measurement', 'Computed tomography']","['Imaging System', 'Least Squares Regression', 'Regular Use', 'Linear Problem', 'Least Squares Problem', 'Computational Tractability', 'Variety Of Frameworks', 'Linear Least Squares Problem', 'Ultrasound Imaging', 'Convolution Operation', 'Inverse Problem', 'Lack Of Structure', 'Prior Use', 'Lateral Resolution', 'Measurement Matrix', 'Reconstruction Procedure', 'Error Metrics', 'Scan Resolution', 'Scan Pattern', 'Gram Matrix', 'Single-photon Avalanche Diode', 'Symmetric Kernel', 'Linear Inverse Problem', 'Scene Point', 'Confocal Measurements', 'Imaging Geometry', 'Least-squares Sense', 'Filtered Back-projection Algorithm', 'Discretion', 'Confocal Scanning']",,38,"Non-line-of-sight (NLOS) imaging aims to reconstruct scenes outside the field of view of an imaging system. A common approach is to measure the so-called light transients, which facilitates reconstructions through ellipsoidal tomography that involves solving a linear least-squares. Unfortunately, the corresponding linear operator is very high-dimensional and lacks structures that facilitate fast solvers, and so, the ensuing optimization is a computationally daunting task. We introduce a computationally tractable framework for solving the ellipsoidal tomography problem. Our main observation is that the Gram of the ellipsoidal tomography operator is convolutional, either exactly under certain idealized imaging conditions, or approximately in practice. This, in turn, allows us to obtain the ellipsoidal tomography solution by using efficient deconvolution procedures to solve a linear least-squares problem involving the Gram operator. The computational tractability of our approach also facilitates the use of various regularizers during the deconvolution procedure. We demonstrate the advantages of our framework in a variety of simulated and real experiments."
Convolutional Character Networks,"Linjie Xing, Zhi Tian, Weilin Huang, Matthew R. Scott","Malong Technologies, Shenzhen, China; Shenzhen Malong Artificial Intelligence Research Center, Shenzhen, China; University of Adelaide, Australia",33.33333333333333,australia,66.66666666666667,China,"Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful one-stage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-the-art approaches [25, 24] by a large margin, e.g., with improvements of 65.33%->71.08% (with generic lexicon) on ICDAR 2015, and 54.0%->69.23% on Total-Text, on end-to-end text recognition. Code is available at: https://github.com/MalongTech/research-charnet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xing_Convolutional_Character_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xing_Convolutional_Character_Networks_ICCV_2019_paper.pdf,,https://github.com/MalongTech/research-charnet,,main,Poster,https://ieeexplore.ieee.org/document/9010699,"['Text recognition', 'Character recognition', 'Task analysis', 'Training', 'Image recognition', 'Iterative methods', 'Neural networks']","['Recognition Task', 'Bounding Box', 'Natural Images', 'Large Margin', 'Detection Framework', 'Standard Benchmark', 'Optical Character Recognition', 'Real-world Images', 'Recognition Framework', 'Two-stage Framework', 'One-stage Model', 'Neural Network', 'Convolutional Layers', 'Feature Maps', 'Real-world Data', 'Recurrent Neural Network', 'Attention Mechanism', 'Field Direction', 'Iteration Step', 'Word Recognition', 'Layer In Stage', 'Synthetic Images', 'Direct Recognition', 'Recognition Model', 'ResNet-50 Backbone', 'RNN-based Models', 'Convolutional Features', 'Amount Of Training Samples', 'Text Characters', 'Orientation Angle']",,112,"Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful one-stage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-the-art approaches [25, 24] by a large margin, e.g., with improvements of 65.33%→71.08% (with generic lexicon) on ICDAR 2015, and 54.0%→69.23% on Total-Text, on end-to-end text recognition. Code is available at: https://github.com/MalongTech/research-charnet."
Convolutional Sequence Generation for Skeleton-Based Action Synthesis,"Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, Dahua Lin","Department of Information Engineering, The Chinese University of Hong Kong",100.0,Hong Kong,0.0,,"In this work, we aim to generate long actions represented as sequences of skeletons. The generated sequences must demonstrate continuous, meaningful human actions, while maintaining coherence among body parts. Instead of generating skeletons sequentially following an autoregressive model, we propose a framework that generates the entire sequence altogether by transforming from a sequence of latent vectors sampled from a Gaussian process (GP). This framework, named Convolutional Sequence Generation Network (CSGN), jointly models structures in temporal and spatial dimensions. It captures the temporal structure at multiple scales through the GP prior and the temporal convolutions; and establishes the spatial connection between the latent vectors and the skeleton graphs via a novel graph refining scheme. It is noteworthy that CSGN allows bidirectional transforms between the latent and the observed spaces, thus enabling semantic manipulation of the action sequences in various forms. We conducted empirical studies on multiple datasets, including a set of high-quality dancing sequences collected by us. The results show that our framework can produce long action sequences that are coherent across time steps and among body parts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Convolutional_Sequence_Generation_for_Skeleton-Based_Action_Synthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Convolutional_Sequence_Generation_for_Skeleton-Based_Action_Synthesis_ICCV_2019_paper.pdf,,https://github.com/yysijie/CSGN,,main,Poster,https://ieeexplore.ieee.org/document/9008261/,"['Skeleton', 'Convolution', 'Gaussian processes', 'Spatial resolution', 'Transforms', 'Generators', 'Generative adversarial networks']","['Generation Sequencing', 'Human Activities', 'Time Step', 'Convolutional Network', 'Temporal Dimension', 'Sequence Of Actions', 'Multiple Scales', 'Spatial Dimensions', 'Autoregressive Model', 'Gaussian Process', 'Latent Vector', 'Temporal Convolution', 'Mean Time', 'Short Sequences', 'Quantitative Evaluation', 'Length Scale', 'Generative Adversarial Networks', 'Latent Space', 'Action Recognition', 'Quantitative Metrics', 'Graph Convolution', 'Fr√©chet Inception Distance', 'Series Of Blocks', 'Ensemble Mean', 'Graph Operations', 'Encoder Network', 'Inverse Mapping', 'Actual Dataset', 'Basic Metrics', 'Partial Observation']",,79,"In this work, we aim to generate long actions represented as sequences of skeletons. The generated sequences must demonstrate continuous, meaningful human actions, while maintaining coherence among body parts. Instead of generating skeletons sequentially following an autoregressive model, we propose a framework that generates the entire sequence altogether by transforming from a sequence of latent vectors sampled from a Gaussian process (GP). This framework, named Convolutional Sequence Generation Network (CSGN), jointly models structures in temporal and spatial dimensions. It captures the temporal structure at multiple scales through the GP prior and the temporal convolutions; and establishes the spatial connection between the latent vectors and the skeleton graphs via a novel graph refining scheme. It is noteworthy that CSGN allows bidirectional transforms between the latent and the observed spaces, thus enabling semantic manipulation of the action sequences in various forms. We conducted empirical studies on multiple datasets, including a set of high-quality dancing sequences collected by us. The results show that our framework can produce long action sequences that are coherent across time steps and among body parts."
Copy-and-Paste Networks for Deep Video Inpainting,"Sungho Lee, Seoung Wug Oh, DaeYeun Won, Seon Joo Kim",Hyundai MNSOFT; Yonsei University,50.0,south korea,50.0,South Korea,"We present a novel deep learning based algorithm for video inpainting. Video inpainting is a process of completing corrupted or missing regions in videos. Video inpainting has additional challenges compared to image inpainting due to the extra temporal information as well as the need for maintaining the temporal coherency. We propose a novel DNN-based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames of the video. The network is trained to copy corresponding contents in reference frames and paste them to fill the holes in the target frame. Our network also includes an alignment network that computes homographies between frames for the alignment, enabling the network to take information from more distant frames for robustness. Our method produces visually pleasing and temporally coherent results while running faster than the state-of-the-art optimization-based method. In addition, we extend our framework for enhancing over/under exposed frames in videos. Using this enhancement technique, we were able to significantly improve the lane detection accuracy on road videos.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Copy-and-Paste_Networks_for_Deep_Video_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Copy-and-Paste_Networks_for_Deep_Video_Inpainting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010669/,"['Optical imaging', 'Machine learning', 'Image restoration', 'Decoding', 'Task analysis', 'Convolution', 'Optical feedback']","['Video Inpainting', 'Reference Frame', 'Video Frames', 'Affinity Matrix', 'Temporal Coherence', 'Missing Regions', 'Target Frame', 'Image Inpainting', 'Network Alignment', 'Convolution', 'Deep Neural Network', 'Feature Space', 'User Study', 'Correction Process', 'Optical Flow', 'Video Sequences', 'Visual Map', 'Saliency Map', 'Temporal Consistency', 'Hole Region', 'NVIDIA Titan Xp GPU', 'Valid Pixels', 'Patch Matching', 'Homography', 'Deep Generative Models']",,87,"We present a novel deep learning based algorithm for video inpainting. Video inpainting is a process of completing corrupted or missing regions in videos. Video inpainting has additional challenges compared to image inpainting due to the extra temporal information as well as the need for maintaining the temporal coherency. We propose a novel DNN-based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames of the video. The network is trained to copy corresponding contents in reference frames and paste them to fill the holes in the target frame. Our network also includes an alignment network that computes homographies between frames for the alignment, enabling the network to take information from more distant frames for robustness. Our method produces visually pleasing and temporally coherent results while running faster than the state-of-the-art optimization-based method. In addition, we extend our framework for enhancing over/under exposed frames in videos. Using this enhancement technique, we were able to significantly improve the lane detection accuracy on road videos."
Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential Fixations,"Hanxiao Wang, Venkatesh Saligrama, Stan Sclaroff, Vitaly Ablavsky",Boston University,100.0,usa,0.0,,"We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images. Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, we train a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching(DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Cost-Aware_Fine-Grained_Recognition_for_IoTs_Based_on_Sequential_Fixations_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Cost-Aware_Fine-Grained_Recognition_for_IoTs_Based_on_Sequential_Fixations_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010623/,"['Image edge detection', 'Servers', 'Training', 'Visualization', 'Standards', 'Computational modeling']","['Fixation Sequences', 'Class Labels', 'Coaching', 'Actor Network', 'Edge Devices', 'Classification Accuracy', 'Input Image', 'Fixed Point', 'Image Regions', 'Standard Classification', 'Bounding Box', 'Entire Image', 'Input Size', 'Reward Function', 'Attention Model', 'Target Network', 'Policy Learning', 'Policy Improvement', 'Critic Network', 'End Of Episode', 'Standard Input', 'State-value Function', 'Replay Buffer', 'Actor Training', 'Discriminative Regions', 'Fixation Location', 'Real-world Deployment', 'Brute-force Approach', 'Classification Results', 'Transmission Efficiency']",,1,"We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images. Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, we train a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching(DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels."
Counterfactual Critic Multi-Agent Training for Scene Graph Generation,"Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, Shih-Fu Chang","University of Science and Technology of China; DVMM Lab, Columbia University; MReal Lab, Nanyang Technological University; DCD Lab, College of Computer Science and Technology, Zhejiang University; Hikvision Research Institute",100.0,"China, Singapore, china, usa",0.0,,"Scene graphs --- objects as nodes and visual relationships as edges --- describe the whereabouts and interactions of objects in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects. For example, ""person"" on ""bike"" can help to determine the relationship ""ride"", which in turn contributes to the confidence of the two objects. However, we argue that the visual context is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes should not be penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach. CMAT is a multi-agent policy gradient method that frames objects into cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the predictions of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art performance by significant gains under various settings and metrics.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Counterfactual_Critic_Multi-Agent_Training_for_Scene_Graph_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Counterfactual_Critic_Multi-Agent_Training_for_Scene_Graph_Generation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010810/,"['Visualization', 'Training', 'Context modeling', 'Measurement', 'Proposals', 'Image edge detection', 'Task analysis']","['Scene Graph', 'Policy Gradient', 'Scene Understanding', 'Visual Context', 'Policy Gradient Method', 'Visual Relationship', 'Value Function', 'Object Detection', 'Bounding Box', 'Training Stage', 'Object Classification', 'Global Pooling', 'Consistent Improvement', 'Training Objective', 'Extraction Module', 'Training Paradigm', 'Object Pairs', 'Object Labels', 'Image Captioning', 'Communication Rounds', 'Agent Communication', 'Log Loss', 'Multi-agent Reinforcement Learning']",,89,"Scene graphs --- objects as nodes and visual relationships as edges --- describe the whereabouts and interactions of objects in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects. For example, ``person'' on ``bike'' can help to determine the relationship ``ride'', which in turn contributes to the confidence of the two objects. However, we argue that the visual context is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes should not be penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach. CMAT is a multi-agent policy gradient method that frames objects into cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the predictions of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art performance by significant gains under various settings and metrics."
Counting With Focus for Free,"Zenglin Shi, Pascal Mettes, Cees G. M. Snoek",University of Amsterdam,100.0,Netherlands,0.0,,"This paper aims to count arbitrary objects in images. The leading counting approaches start from point annotations per object from which they construct density maps. Then, their training objective transforms input images to density maps through deep convolutional networks. We posit that the point annotations serve more supervision purposes than just constructing density maps. We introduce ways to repurpose the points for free. First, we propose supervised focus from segmentation, where points are converted into binary maps. The binary maps are combined with a network branch and accompanying loss function to focus on areas of interest. Second, we propose supervised focus from global density, where the ratio of point annotations to image pixels is used in another branch to regularize the overall density estimation. To assist both the density estimation and the focus from segmentation, we also introduce an improved kernel size estimator for the point annotations. Experiments on six datasets show that all our contributions reduce the counting error, regardless of the base network, resulting in state-of-the-art accuracy using only a single network. Finally, we are the first to count on WIDER FACE, allowing us to show the benefits of our approach in handling varying object scales and crowding levels. Code is available at https://github.com/shizenglin/Counting-with-Focus-for-Free",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Counting_With_Focus_for_Free_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Counting_With_Focus_for_Free_ICCV_2019_paper.pdf,,https://github.com/shizenglin/Counting-with-Focus-for-Free,,main,Poster,https://ieeexplore.ieee.org/document/9010851/,"['Image segmentation', 'Kernel', 'Training', 'Estimation', 'Task analysis', 'Optimization', 'Convolution']","['Loss Function', 'Deep Network', 'Repurposing', 'Input Image', 'Density Estimation', 'Image Object', 'Density Map', 'Deep Convolutional Network', 'Single Network', 'Binary Map', 'Branch Network', 'Object Scale', 'Counting Error', 'Global Density', 'Level Of Crowding', 'Root Mean Square Error', 'Convolutional Neural Network', 'Convolutional Layers', 'K-nearest Neighbor', 'Mean Absolute Error', 'Spatial Attention', 'Segmentation Map', 'Kernel Estimation', 'L2 Loss', 'Dense Objects', 'Peak Signal-to-noise Ratio', 'Image X', 'Perspective Distortion', 'Lower Error', 'Box Annotations']",,77,"This paper aims to count arbitrary objects in images. The leading counting approaches start from point annotations per object from which they construct density maps. Then, their training objective transforms input images to density maps through deep convolutional networks. We posit that the point annotations serve more supervision purposes than just constructing density maps. We introduce ways to repurpose the points for free. First, we propose supervised focus from segmentation, where points are converted into binary maps. The binary maps are combined with a network branch and accompanying loss function to focus on areas of interest. Second, we propose supervised focus from global density, where the ratio of point annotations to image pixels is used in another branch to regularize the overall density estimation. To assist both the density estimation and the focus from segmentation, we also introduce an improved kernel size estimator for the point annotations. Experiments on six datasets show that all our contributions reduce the counting error, regardless of the base network, resulting in state-of-the-art accuracy using only a single network. Finally, we are the first to count on WIDER FACE, allowing us to show the benefits of our approach in handling varying object scales and crowding levels. Code is available at https://github.com/shizenglin/Counting-with-Focus-for-Free."
Creativity Inspired Zero-Shot Learning,"Mohamed Elhoseiny, Mohamed Elfeki","Facebook AI Research (FAIR), King Abdullah University of Science and Technology (KAUST); University of Central Florida",100.0,"saudi arabia, usa",0.0,,"Zero-shot learning (ZSL) aims at understanding unseen categories with no training examples from class-level descriptions. To improve the discriminative power of zero-shot learning, we model the visual learning process of unseen categories with an inspiration from the psychology of human creativity for producing novel art. We relate ZSL to human creativity by observing that zero-shot learning is about recognizing the unseen and creativity is about creating a likable unseen. We introduce a learning signal inspired by creativity literature that explores the unseen space with hallucinated class-descriptions and encourages careful deviation of their visual feature generations from seen classes while allowing knowledge transfer from seen to unseen classes. Empirically, we show consistent improvement over the state of the art of several percents on the largest available benchmarks on the challenging task or generalized ZSL from a noisy text that we focus on, using the CUB and NABirds datasets. We also show the advantage of our loss on Attribute-based ZSL on three additional datasets (AwA2, aPY, and SUN). Code is available at https://github.com/mhelhoseiny/CIZSL.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.pdf,,https://github.com/mhelhoseiny/CIZSL,,main,Poster,https://ieeexplore.ieee.org/document/9009042/,"['Creativity', 'Visualization', 'Semantics', 'Computational modeling', 'Art', 'Encyclopedias']","['Zero-shot', 'Benchmark', 'Knowledge Transfer', 'Visual Features', 'Hallucinations', 'Human Creativity', 'Unseen Classes', 'Training Data', 'Visual Representation', 'Class Labels', 'Training Images', 'Kullback-Leibler', 'Generative Adversarial Networks', 'Textual Descriptions', 'Semantic Representations', 'Textual Features', 'Category Labels', 'Visual Space', 'Adversarial Training', 'Real Features', 'Wikipedia Articles', 'Text Representation', 'Term Frequency-inverse Document Frequency', 'Description Of Classes', 'Supplement For Details', 'Semantic Space']",,34,"Zero-shot learning (ZSL) aims at understanding unseen categories with no training examples from class-level descriptions. To improve the discriminative power of zero-shot learning, we model the visual learning process of unseen categories with an inspiration from the psychology of human creativity for producing novel art. We relate ZSL to human creativity by observing that zero-shot learning is about recognizing the unseen and creativity is about creating a likable unseen. We introduce a learning signal inspired by creativity literature that explores the unseen space with hallucinated class-descriptions and encourages careful deviation of their visual feature generations from seen classes while allowing knowledge transfer from seen to unseen classes. Empirically, we show consistent improvement over the state of the art of several percents on the largest available benchmarks on the challenging task or generalized ZSL from a noisy text that we focus on, using the CUB and NABirds datasets. We also show the advantage of our loss on Attribute-based ZSL on three additional datasets (AwA2, aPY, and SUN). Code is available at https://github.com/mhelhoseiny/CIZSL."
Cross View Fusion for 3D Human Pose Estimation,"Haibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, Wenjun Zeng",TuSimple; Microsoft Research Asia; University of Science and Technology of China,66.66666666666666,"china, usa",33.33333333333334,USA,"We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qiu_Cross_View_Fusion_for_3D_Human_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Cross_View_Fusion_for_3D_Human_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008809/,"['Three-dimensional displays', 'Two dimensional displays', 'Heating systems', 'Pose estimation', 'Cameras', 'Solid modeling', 'Asia']","['Pose Estimation', 'Human Pose Estimation', 'Cross-view', '3D Human Pose', 'Joint Position', 'Joint Estimation', '3D Pose', 'Multi-view Images', '2D Pose', 'Training Dataset', 'Large Errors', '3D Space', 'Quantization Error', 'Feature Fusion', 'Total Dataset', 'Discrete Space', 'Camera View', 'Fusion Approach', 'Body Joints', 'World Coordinate System', 'Pairwise Potential', 'Bundle Adjustment', 'Discrete State Space', 'Leg Joints', 'Pseudo Labels', 'Accuracy Of Pose Estimation', 'Hand Joints', '3D Error', 'Monocular Images']",,147,"We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm)."
Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation,"Yu-Jhe Li, Ci-Siang Lin, Yan-Bo Lin, Yu-Chiang Frank Wang","MOST Joint Research Center for AI Technology and All Vista Healthcare, Taiwan; ASUS Intelligent Cloud Services, Taiwan; National Taiwan University, Taiwan",33.33333333333333,taiwan,66.66666666666667,Taiwan,"Person re-identification (re-ID) aims at recognizing the same person from images taken across different cameras. To address this challenging task, existing re-ID models typically rely on a large amount of labeled training data, which is not practical for real-world applications. To alleviate this limitation, researchers now targets at cross-dataset re-ID which focuses on generalizing the discriminative ability to the unlabeled target domain when given a labeled source domain dataset. To achieve this goal, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) aims at learning deep image representation with pose and domain information properly disentangled. With the learned cross-domain pose invariant feature space, our proposed PDA-Net is able to perform pose disentanglement across domains without supervision in identities, and the resulting features can be applied to cross-dataset re-ID. Both of our qualitative and quantitative results on two benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art cross-dataset Re-ID approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Cross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_Disentanglement_and_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Cross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_Disentanglement_and_Adaptation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010744/,"['Generators', 'Cameras', 'Adaptation models', 'Training data', 'Image representation', 'Visualization', 'Training']","['Qualitative Results', 'Benchmark Datasets', 'Target Domain', 'Label Information', 'Image Recovery', 'Deep Representation', 'Pose Information', 'Input Image', 'Source Images', 'Content Features', 'Source Domain', 'Mean Average Precision', 'Reconstruction Loss', 'Conditional Random Field', 'Image X', 'Identity Labels', 'Triplet Loss', 'Satisfactory Quality', 'Query Image', 'Background Clutter', 'Maximum Mean Discrepancy', 'Domain-invariant Representations', 'Re-identification Task', 'Domain Discriminator', 'Re-identification Methods', 'Dataset Of Interest', 'Target Domain Data', 'Latent Space', 'Output Image', 'Network Modules']",,134,"Person re-identification (re-ID) aims at recognizing the same person from images taken across different cameras. To address this challenging task, existing re-ID models typically rely on a large amount of labeled training data, which is not practical for real-world applications. To alleviate this limitation, researchers now targets at cross-dataset re-ID which focuses on generalizing the discriminative ability to the unlabeled target domain when given a labeled source domain dataset. To achieve this goal, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) aims at learning deep image representation with pose and domain information properly disentangled. With the learned cross-domain pose invariant feature space, our proposed PDA-Net is able to perform pose disentanglement across domains without supervision in identities, and the resulting features can be applied to cross-dataset re-ID. Both of our qualitative and quantitative results on two benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art cross-dataset Re-ID approaches."
Cross-Domain Adaptation for Animal Pose Estimation,"Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, Yu-Wing Tai",Tencent; Shanghai Jiao Tong University,50.0,China,50.0,China,"In this paper, we are interested in pose estimation of animals. Animals usually exhibit a wide range of variations on poses and there is no available animal pose dataset for training and testing. To address this problem, we build an animal pose dataset to facilitate training and evaluation. Considering the heavy labor needed to label dataset and it is impossible to label data for all concerned animal species, we, therefore, proposed a novel cross-domain adaptation method to transform the animal pose knowledge from labeled animal classes to unlabeled animal classes. We use the modest animal pose dataset to adapt learned knowledge to multiple animals species. Moreover, humans also share skeleton similarities with some animals (especially four-footed mammals). Therefore, the easily available human pose dataset, which is of a much larger scale than our labeled animal dataset, provides important prior knowledge to boost up the performance on animal pose estimation. Experiments show that our proposed method leverages these pieces of prior knowledge well and achieves convincing results on animal pose estimation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cao_Cross-Domain_Adaptation_for_Animal_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Cross-Domain_Adaptation_for_Animal_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009505/,"['Animals', 'Pose estimation', 'Feature extraction', 'Adaptation models', 'Task analysis', 'Training', 'Data models']","['Pose Estimation', 'Cross-domain Adaptation', 'Animal Pose Estimation', 'Animal Species', 'Human Pose', 'Human Data', 'Feature Maps', 'Object Detection', 'Human Animal', 'Large-scale Datasets', 'Domain Shift', 'Similar Tasks', 'Animal Samples', 'Ground Truth Labels', 'Target Domain', 'Domain Adaptation', 'Source Domain', 'Training Labels', 'Differences In Texture', 'Human Pose Estimation', 'Pseudo Labels', 'Domain Discriminator', 'Source Target Domain', 'Style Transfer', 'Weak Supervision', 'Weighting Factor', 'COCO Dataset', 'Unseen Classes', 'Learning Rate', 'Training Data']",,90,"In this paper, we are interested in pose estimation of animals. Animals usually exhibit a wide range of variations on poses and there is no available animal pose dataset for training and testing. To address this problem, we build an animal pose dataset to facilitate training and evaluation. Considering the heavy labor needed to label dataset and it is impossible to label data for all concerned animal species, we, therefore, proposed a novel cross-domain adaptation method to transform the animal pose knowledge from labeled animal classes to unlabeled animal classes. We use the modest animal pose dataset to adapt learned knowledge to multiple animals species. Moreover, humans also share skeleton similarities with some animals (especially four-footed mammals). Therefore, the easily available human pose dataset, which is of a much larger scale than our labeled animal dataset, provides important prior knowledge to boost up the performance on animal pose estimation. Experiments show that our proposed method leverages these pieces of prior knowledge well and achieves convincing results on animal pose estimation."
Cross-View Policy Learning for Street Navigation,"Ang Li, Huiyi Hu, Piotr Mirowski, Mehrdad Farajtabar","DeepMind, Mountain View, CA; DeepMind, London, UK",0.0,,100.0,UK,"The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to transfer a ground view policy, from training areas to unseen (target) parts of the city, by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Our experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to train a multi-modal policy on ground and aerial views, then transfer the ground view policy to unseen (target) parts of the city by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Cross-View_Policy_Learning_for_Street_Navigation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Cross-View_Policy_Learning_for_Street_Navigation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008306/,"['Navigation', 'Task analysis', 'Training', 'Visualization', 'Urban areas', 'Google', 'Trajectory']","['Policy Learning', 'Deep Learning', 'Transfer Learning', 'Ground Level', 'Visual Observation', 'Aerial Images', 'Latent Space', 'Deep Reinforcement Learning', 'Street View', 'Navigation Task', 'Unfamiliar Environment', 'Reinforcement Learning Agent', 'Convolutional Network', 'Convolutional Neural Network', 'Target Region', 'Knowledge Transfer', 'Long Short-term Memory', 'Recurrent Neural Network', 'Challenging Problem', 'Indoor Environments', 'Adaptation Stage', 'Performance Of Agents', 'Machine Vision', 'Distillation Loss', 'Visual Domain', 'Multimodal Learning', 'Action Detection', 'L2 Loss', 'Auxiliary Task', 'Policy Network']",,14,"The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to transfer a ground view policy, from training areas to unseen (target) parts of the city, by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Our experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to train a multi-modal policy on ground and aerial views, then transfer the ground view policy to unseen (target) parts of the city by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments."
Cross-X Learning for Fine-Grained Visual Categorization,"Wei Luo, Xitong Yang, Xianjie Mo, Yuheng Lu, Larry S. Davis, Jun Li, Jian Yang, Ser-Nam Lim","MIT; University of Maryland, College Park; Nanjing University of Science and Technology; South China Agricultural University; Facebook AI",80.0,"China, china, usa",20.0,USA,"Recognizing objects from subcategories with very subtle differences remains a challenging task due to the large intra-class and small inter-class variation. Recent work tackles this problem in a weakly-supervised manner: object parts are first detected and the corresponding part-specific features are extracted for fine-grained classification. However, these methods typically treat the part-specific features of each image in isolation while neglecting their relationships between different images. In this paper, we propose Cross-X learning, a simple yet effective approach that exploits the relationships between different images and between different network layers for robust multi-scale feature learning. Our approach involves two novel components: (i) a cross-category cross-semantic regularizer that guides the extracted features to represent semantic parts and, (ii) a cross-layer regularizer that improves the robustness of multi-scale features by matching the prediction distribution across multiple layers. Our approach can be easily trained end-to-end and is scalable to large datasets like NABirds. We empirically analyze the contributions of different components of our approach and demonstrate its robustness, effectiveness and state-of-the-art performance on five benchmark datasets. Code is available at https://github.com/cswluo/CrossX.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Cross-X_Learning_for_Fine-Grained_Visual_Categorization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Cross-X_Learning_for_Fine-Grained_Visual_Categorization_ICCV_2019_paper.pdf,,https://github.com/cswluo/CrossX,,main,Poster,https://ieeexplore.ieee.org/document/9009047/,"['Feature extraction', 'Semantics', 'Robustness', 'Correlation', 'Measurement', 'Task analysis', 'Visualization']","['Fine-grained Visual Classification', 'Multiple Layers', 'Feature Learning', 'Benchmark Datasets', 'Robust Features', 'Multi-scale Features', 'Object Parts', 'Intra-class Variance', 'Contribution Of Different Components', 'Convolutional Neural Network', 'Feature Maps', 'Feature Classification', 'Multiple Regions', 'Stochastic Gradient Descent', 'High-level Features', 'Activation Maps', 'Semantic Features', 'Modulation Of Excitability', 'Global Average Pooling', 'Global Max Pooling', 'Advanced Architectures', 'Metric Learning', 'Feature Pyramid Network', 'Learning Loss', 'Multi-task Learning', 'Feature Merging', 'Fine-grained Features', 'Attention Feature', 'Feature Pooling']",,159,"Recognizing objects from subcategories with very subtle differences remains a challenging task due to the large intra-class and small inter-class variation. Recent work tackles this problem in a weakly-supervised manner: object parts are first detected and the corresponding part-specific features are extracted for fine-grained classification. However, these methods typically treat the part-specific features of each image in isolation while neglecting their relationships between different images. In this paper, we propose Cross-X learning, a simple yet effective approach that exploits the relationships between different images and between different network layers for robust multi-scale feature learning. Our approach involves two novel components: (i) a cross-category cross-semantic regularizer that guides the extracted features to represent semantic parts and, (ii) a cross-layer regularizer that improves the robustness of multi-scale features by matching the prediction distribution across multiple layers. Our approach can be easily trained end-to-end and is scalable to large datasets like NABirds. We empirically analyze the contributions of different components of our approach and demonstrate its robustness, effectiveness and state-of-the-art performance on five benchmark datasets. Code is available at \url{https://github.com/cswluo/CrossX}."
Crowd Counting With Deep Structured Scale Integration Network,"Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, Liang Lin","Sun Yat-sen University; Sun Yat-sen University, DarkMatter AI Research; The University of Sydney, SenseTime Computer Vision Research Group, Australia",100.0,"China, australia",0.0,,"Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. Specifically, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. In particular, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Crowd_Counting_With_Deep_Structured_Scale_Integration_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Crowd_Counting_With_Deep_Structured_Scale_Integration_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010246/,"['Correlation', 'Task analysis', 'Robustness', 'Optimization', 'Fuses', 'Message passing', 'Head']","['Crowd Counting', 'Loss Function', 'Contralateral', 'Structural Features', 'Density Map', 'Scale Variation', 'Size Of Region', 'Continuous Distribution', 'Huge Variety', 'Loss Of Structure', 'Multi-scale Features', 'Diverse People', 'Similar Loss', 'Local Correlation', 'Conditional Random Field', 'Challenging Benchmark', 'Convolutional Neural Network', 'Gaussian Kernel', 'Convolutional Layers', 'Small Region', 'Layers Of VGG16', 'Conditional Random Field Model', 'Image Pyramid', 'Regional Correlation', 'Receptive Field', 'Shared Parameters', 'Shallow Layers', 'International Exhibition', 'Mean Absolute Error', 'High-level Tasks']",,184,"Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. Specifically, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. In particular, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods."
Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation,"Chengchao Shen, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, Mingli Song","Stevens Institute of Technology; Zhejiang University, Alibaba-Zhejiang University Joint Institute of Frontier Technologies; Zhejiang University",100.0,"China, usa",0.0,,"A massive number of well-trained deep networks have been released by developers online. These networks may focus on different tasks and in many cases are optimized for different datasets. In this paper, we study how to exploit such heterogeneous pre-trained networks, known as teachers, so as to train a customized student network that tackles a set of selective tasks defined by the user. We assume no human annotations are available, and each teacher may be either single- or multi-task. To this end, we introduce a dual-step strategy that first extracts the task-specific knowledge from the heterogeneous teachers sharing the same sub-task, and then amalgamates the extracted knowledge to build the student network. To facilitate the training, we employ a selective learning scheme where, for each unlabelled sample, the student learns adaptively from only the teacher with the least prediction ambiguity. We evaluate the proposed approach on several datasets and the experimental results demonstrate that the student, learned by such adaptive knowledge amalgamation, achieves performances even better than those of the teachers.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010941/,,"['Student Network', 'Knowledge Amalgamation', 'Deep Network', 'Selection Strategy', 'Pre-trained Network', 'Image Classification', 'Feature Maps', 'Feature Learning', 'Transfer Learning', 'Convolution Operation', 'Multiple Tasks', 'Student Characteristics', 'Teacher Characteristics', 'Target Domain', 'Unlabeled Data', 'Dog Breeds', 'Source Domain', 'Student Model', 'Teacher Network', 'Feature Alignment', 'Multiple Teachers', 'Transfer Loss', 'Facial Attributes', 'Category Recognition', 'Problem Setup', 'Knowledge Transfer', 'Compact Size']",,27,"A massive number of well-trained deep networks have been released by developers online. These networks may focus on different tasks and in many cases are optimized for different datasets. In this paper, we study how to exploit such heterogeneous pre-trained networks, known as teachers, so as to train a customized student network that tackles a set of selective tasks defined by the user. We assume no human annotations are available, and each teacher may be either single- or multi-task. To this end, we introduce a dual-step strategy that first extracts the task-specific knowledge from the heterogeneous teachers sharing the same sub-task, and then amalgamates the extracted knowledge to build the student network. To facilitate the training, we employ a selective learning scheme where, for each unlabelled sample, the student learns adaptively from only the teacher with the least prediction ambiguity. We evaluate the proposed approach on several datasets and the experimental results demonstrate that the student, learned by such adaptive knowledge amalgamation, achieves performances even better than those of the teachers."
CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features,"Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo","Clova AI Research, LINE Plus Corp.; Clova AI Research, NAVER Corp.",0.0,,100.0,South Korea,"Regional dropout strategies have been proposed to enhance performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it suffers from information loss causing inefficiency in training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gain in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix can improve the model robustness against input corruptions and its out-of distribution detection performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.pdf,,https://github.com/clovaai/CutMix-PyTorch,,main,Oral,https://ieeexplore.ieee.org/document/9008296/,"['Training', 'Task analysis', 'Object detection', 'Computer vision', 'Computational modeling', 'Dogs', 'Robustness']","['Convolutional Neural Network', 'Classification Performance', 'Detection Performance', 'Random Noise', 'Training Images', 'Object Location', 'Ground Truth Labels', 'Augmentation Methods', 'Localization Task', 'Augmentation Strategy', 'Cut-and-paste', 'Image Captioning', 'ImageNet Classification', 'Deep Network', 'Computer Vision', 'Input Image', 'Image Classification', 'Localization Accuracy', 'Object Detection', 'Data Augmentation', 'ImageNet Pre-trained Model', 'ImageNet Pretraining', 'Validation Error', 'Fast Gradient Sign Method', 'Transfer Learning', 'Local Ability', 'Robust Improvement', 'Localization Performance', 'PASCAL VOC', 'Bounding Box']",,2479,"Regional dropout strategies have been proposed to enhance performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it suffers from information loss causing inefficiency in training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gain in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix can improve the model robustness against input corruptions and its out-of distribution detection performance."
DADA: Depth-Aware Domain Adaptation in Semantic Segmentation,"Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick PÃ©rez","valeo.ai, Paris, France; Sorbonne University, Paris, France; valeo.ai, Paris, France",33.33333333333333,France,66.66666666666667,Germany,"Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real ""target domain"" data models that are trained on annotated images from a different ""source domain"", notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Vu_DADA_Depth-Aware_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Vu_DADA_Depth-Aware_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2019_paper.pdf,,https://github.com/valeoai/DADA,,main,Poster,https://ieeexplore.ieee.org/document/9011010/,"['Semantics', 'Training', 'Task analysis', 'Image segmentation', 'Image color analysis', 'Adaptation models', 'Benchmark testing']","['Semantic Segmentation', 'Domain Adaptation', 'Benchmark', 'Target Domain', 'Source Domain', 'Privileged Information', 'Convolutional Neural Network', 'Convolutional Layers', 'Feature Space', 'Generative Adversarial Networks', 'Depth Map', 'Feature Fusion', 'Source Images', 'Residual Block', 'Depth Information', 'Appearance Features', 'Target Distribution', 'Adversarial Training', 'Image X', 'Domain Gap', 'Auxiliary Task', 'Segmentation Annotations', 'Vulnerable Road Users', 'Depth Prediction', 'Unsupervised Domain Adaptation Methods', 'Convolutional Neural Networks Backbone', 'Learning Rate', 'General Approach']",,126,"Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real “target domain” data models that are trained on annotated images from a different “source domain”, notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks."
DAGMapper: Learning to Map by Discovering Lane Topology,"Namdar Homayounfar, Wei-Chiu Ma, Justin Liang, Xinyu Wu, Jack Fan, Raquel Urtasun","Uber Advanced Technologies Group, University of Toronto; Uber Advanced Technologies Group; Uber Advanced Technologies Group, MIT",66.66666666666666,"Canada, usa",33.33333333333334,USA,"One of the fundamental challenges to scale self-driving is being able to create accurate high definition maps (HD maps) with low cost. Current attempts to automate this pro- cess typically focus on simple scenarios, estimate independent maps per frame or do not have the level of precision required by modern self driving vehicles. In contrast, in this paper we focus on drawing the lane boundaries of complex highways with many lanes that contain topology changes due to forks and merges. Towards this goal, we formulate the problem as inference in a directed acyclic graphical model (DAG), where the nodes of the graph encode geo- metric and topological properties of the local regions of the lane boundaries. Since we do not know a priori the topology of the lanes, we also infer the DAG topology (i.e., nodes and edges) for each region. We demonstrate the effectiveness of our approach on two major North American Highways in two different states and show high precision and recall as well as 89% correct topology.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Homayounfar_DAGMapper_Learning_to_Map_by_Discovering_Lane_Topology_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Homayounfar_DAGMapper_Learning_to_Map_by_Discovering_Lane_Topology_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010023/,"['Topology', 'Roads', 'Laser radar', 'Network topology', 'Feature extraction', 'Buildings']","['Lane Topology', 'Highway', 'Precision And Recall', 'Geometric Properties', 'Graphical Model', 'Topological Changes', 'Properties Of Regions', 'Correct Topology', 'Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Recurrent Neural Network', 'Point Cloud', 'Road Network', 'Aerial Images', 'Autonomous Vehicles', 'Distance Map', 'Topological States', 'Maximum A Posteriori', 'Convolutional Recurrent Network', 'Lane Markings', 'Convolutional Recurrent Neural Network', 'Representation Of The World', 'Vertex Position', 'Notion Of State', 'Birdâ€™s Eye', 'Position Xi', 'Conditional Distribution']",,66,"One of the fundamental challenges to scale self-driving is being able to create accurate high definition maps (HD maps) with low cost. Current attempts to automate this process typically focus on simple scenarios, estimate independent maps per frame or do not have the level of precision required by modern self driving vehicles. In contrast, in this paper we focus on drawing the lane boundaries of complex highways with many lanes that contain topology changes due to forks and merges. Towards this goal, we formulate the problem as inference in a directed acyclic graphical model (DAG), where the nodes of the graph encode geometric and topological properties of the local regions of the lane boundaries. Since we do not know a priori the topology of the lanes, we also infer the DAG topology (i.e., nodes and edges) for each region. We demonstrate the effectiveness of our approach on two major North American Highways in two different states and show high precision and recall as well as 89% correct topology."
DANet: Divergent Activation for Weakly Supervised Object Localization,"Haolan Xue, Chang Liu, Fang Wan, Jianbin Jiao, Xiangyang Ji, Qixiang Ye","University of Chinese Academy of Sciences, Beijing, China and Peng Cheng Laboratory, Shenzhen, China; University of Chinese Academy of Sciences, Beijing, China; Tsinghua University, Beijing, China",100.0,"China, china",0.0,,"Weakly supervised object localization remains a challenge when learning object localization models from image category labels. Optimizing image classification tends to activate object parts and ignore the full object extent, while expanding object parts into full object extent could deteriorate the performance of image classification. In this paper, we propose a divergent activation (DA) approach, and target at learning complementary and discriminative visual patterns for image classification and weakly supervised object localization from the perspective of discrepancy. To this end, we design hierarchical divergent activation (HDA), which leverages the semantic discrepancy to spread feature activation, implicitly. We also propose discrepant divergent activation (DDA), which pursues object extent by learning mutually exclusive visual patterns, explicitly. Deep networks implemented with HDA and DDA, referred to as DANets, diverge and fuse discrepant yet discriminative features for image classification and object localization in an end-to-end manner. Experiments validate that DANets advance the performance of object localization while maintaining high performance of image classification on CUB-200 and ILSVRC datasets",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xue_DANet_Divergent_Activation_for_Weakly_Supervised_Object_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_DANet_Divergent_Activation_for_Weakly_Supervised_Object_Localization_ICCV_2019_paper.pdf,,https://github.com/xuehaolan/DANet,,main,Poster,https://ieeexplore.ieee.org/document/9010422/,"['Visualization', 'Training', 'Proposals', 'Optimization', 'Semantics', 'Image coding', 'Data models']","['Object Location', 'Weakly Supervised Object Localization', 'Classification Performance', 'Image Classification', 'Visual Patterns', 'Object Parts', 'Category Labels', 'Network Implementation', 'Image Categories', 'Discriminative Patterns', 'Complementary Patterns', 'Image Classification Performance', 'Loss Function', 'Objective Function', 'Convolutional Neural Network', 'Feature Maps', 'Pooling Layer', 'Semantic Similarity', 'Activation Maps', 'Levels Of Hierarchy', 'Class Activation Maps', 'Multiple Instance Learning', 'Global Average Pooling Layer', 'Base Classes', 'Discriminative Regions', 'Joint Optimization', 'Global Average Pooling', 'Network For Image Classification', 'Region Proposal', 'WordNet']",,113,"Weakly supervised object localization remains a challenge when learning object localization models from image category labels. Optimizing image classification tends to activate object parts and ignore the full object extent, while expanding object parts into full object extent could deteriorate the performance of image classification. In this paper, we propose a divergent activation (DA) approach, and target at learning complementary and discriminative visual patterns for image classification and weakly supervised object localization from the perspective of discrepancy. To this end, we design hierarchical divergent activation (HDA), which leverages the semantic discrepancy to spread feature activation, implicitly. We also propose discrepant divergent activation (DDA), which pursues object extent by learning mutually exclusive visual patterns, explicitly. Deep networks implemented with HDA and DDA, referred to as DANets, diverge and fuse discrepant yet discriminative features for image classification and object localization in an end-to-end manner. Experiments validate that DANets advance the performance of object localization while maintaining high performance of image classification on CUB-200 and ILSVRC datasets."
DDSL: Deep Differentiable Simplex Layer for Learning Geometric Signals,"Chiyu ""Max"" Jiang, Dana Lansigan, Philip Marcus, Matthias NieÃner",Technical University of Munich; UC Berkeley,100.0,"germany, usa",0.0,,"We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for geometric deep learning. The DDSL is a differentiable layer compatible with deep neural networks for bridging simplex mesh-based geometry representations (point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images (e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to perform differentiable, efficient, anti- aliased rasterization of simplex-based signals. We present a complete theoretical framework for the process as well as an efficient backpropagation algorithm. Compared to previous differentiable renderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees and dimensions. In particular, we explore its applications to 2D shapes and illustrate two applications of this method: (1) mesh editing and optimization guided by neural network outputs, and (2) using DDSL for a differentiable rasterization loss to facilitate end-to-end training of polygon generators. We are able to validate the effectiveness of gradient-based shape optimization with the example of airfoil optimization, and using the differentiable rasterization loss to facilitate end-to-end training, we surpass state of the art for polygonal image segmentation given ground-truth bounding boxes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_DDSL_Deep_Differentiable_Simplex_Layer_for_Learning_Geometric_Signals_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_DDSL_Deep_Differentiable_Simplex_Layer_for_Learning_Geometric_Signals_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010730/,"['Shape', 'Optimization', 'Neural networks', 'Machine learning', 'Three-dimensional displays', 'Image segmentation', 'Distortion']","['Simplex', 'Neural Network', 'Fourier Transform', 'Deep Learning', 'Deep Neural Network', 'Image Segmentation', 'Point Cloud', 'Gradient-based Optimization', 'Triangular Mesh', 'Generator Training', 'Tetrahedral Mesh', 'Shape Optimization', 'Ground-truth Bounding Box', 'Degrees Of Freedom', 'Learning Algorithms', 'Convolutional Neural Network', 'Alternative Models', 'Finite Difference', 'Mesh Size', 'Simplicial Complex', 'Backward Pass', 'Angle Of Attack', 'Literary Works', 'Element In Row', 'Finite Difference Method', 'Reynolds Number', 'MNIST Dataset', 'Geometric Representation', 'Physical Simulation']",,5,"We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for geometric deep learning. The DDSL is a differentiable layer compatible with deep neural networks for bridging simplex mesh-based geometry representations (point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images (e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to perform differentiable, efficient, anti- aliased rasterization of simplex-based signals. We present a complete theoretical framework for the process as well as an efficient backpropagation algorithm. Compared to previous differentiable renderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees and dimensions. In particular, we explore its applications to 2D shapes and illustrate two applications of this method: (1) mesh editing and optimization guided by neural network outputs, and (2) using DDSL for a differentiable rasterization loss to facilitate end-to-end training of polygon generators. We are able to validate the effectiveness of gradient-based shape optimization with the example of airfoil optimization, and using the differentiable rasterization loss to facilitate end-to-end training, we surpass state of the art for polygonal image segmentation given ground-truth bounding boxes."
DF2Net: A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction,"Xiaoxing Zeng, Xiaojiang Peng, Yu Qiao","ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China",100.0,china,0.0,,"Reconstructing the detailed geometric structure from a single face image is a challenging problem due to its ill-posed nature and the fine 3D structures to be recovered. This paper proposes a deep Dense-Fine-Finer Network (DF2Net) to address this challenging problem. DF2Net decomposes the reconstruction process into three stages, each of which is processed by an elaborately-designed network, namely D-Net, F-Net, and Fr-Net. D-Net exploits a U-net architecture to map the input image to a dense depth image. F-Net refines the output of D-Net by integrating features from depth and RGB domains, whose output is further enhanced by Fr-Net with a novel multi-resolution hypercolumn architecture. In addition, we introduce three types of data to train these networks, including 3D model synthetic data, 2D image reconstructed data, and fine facial images. We elaborately exploit different datasets (or combination) together with well-designed losses to train different networks. Qualitative evaluation indicates that our DF2Net can effectively reconstruct subtle facial details such as small crow's feet and wrinkles. Our DF2Net achieves performance superior or comparable to state-of-the-art algorithms in qualitative and quantitative analyses on real-world images and the BU-3DFE dataset. Code and the collected 70K image-depth data will be publicly available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_DF2Net_A_Dense-Fine-Finer_Network_for_Detailed_3D_Face_Reconstruction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_DF2Net_A_Dense-Fine-Finer_Network_for_Detailed_3D_Face_Reconstruction_ICCV_2019_paper.pdf,,https://github.com/xiaoxingzeng/DF2Net,,main,Poster,https://ieeexplore.ieee.org/document/9008121/,"['Three-dimensional displays', 'Face', 'Image reconstruction', 'Shape', 'Solid modeling', 'Two dimensional displays', 'Training data']","['Oral And Maxillofacial Surgery', 'Detailed Reconstruction', '3D Face', '3D Face Reconstruction', '3D Structure', 'Fine Structure', 'Input Image', 'Single Image', '2D Images', 'Face Images', 'Depth Images', 'Real-world Images', 'Subtle Details', 'Dense Depth', 'Facial Details', 'Training Data', 'Convolutional Neural Network', 'Deep Network', 'Dense Network', '3D Reconstruction', 'Dense Reconstruction', '3D Shape', 'Fine Network', 'Depth Map', 'Low-dimensional Representation', 'Fine Details', 'Subtle Structure', 'Coarse Model', 'Rich Details', 'Impressive Performance']",,61,"Reconstructing the detailed geometric structure from a single face image is a challenging problem due to its ill-posed nature and the fine 3D structures to be recovered. This paper proposes a deep Dense-Fine-Finer Network (DF2Net) to address this challenging problem. DF2Net decomposes the reconstruction process into three stages, each of which is processed by an elaborately-designed network, namely D-Net, F-Net, and Fr-Net. D-Net exploits a U-net architecture to map the input image to a dense depth image. F-Net refines the output of D-Net by integrating features from depth and RGB domains, whose output is further enhanced by Fr-Net with a novel multi-resolution hypercolumn architecture. In addition, we introduce three types of data to train these networks, including 3D model synthetic data, 2D image reconstructed data, and fine facial images. We elaborately exploit different datasets (or combination) together with well-designed losses to train different networks. Qualitative evaluation indicates that our DF2Net can effectively reconstruct subtle facial details such as small crow's feet and wrinkles. Our DF2Net achieves performance superior or comparable to state-of-the-art algorithms in qualitative and quantitative analyses on real-world images and the BU-3DFE dataset. Code and the collected 70K image-depth data will be publicly available."
DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation,"Xiaohui Zeng, Renjie Liao, Li Gu, Yuwen Xiong, Sanja Fidler, Raquel Urtasun","University of Toronto, Vector Institute, Uber ATG Toronto, NVIDIA; University of Toronto, Vector Institute, Uber ATG Toronto, Canadian Institute for Advanced Research; University of Toronto, Vector Institute, Uber ATG Toronto; University of Toronto, NVIDIA, Canadian Institute for Advanced Research; University of Toronto",100.0,Canada,0.0,,"In this paper, we propose the differentiable mask-matching network (DMM-Net) for solving the video object segmentation problem where the initial object masks are provided. Relying on the Mask R-CNN backbone, we extract mask proposals per frame and formulate the matching between object templates and proposals as a linear assignment problem where thA heading inside a blocke cost matrix is predicted by a deep convolutional neural network. We propose a differentiable matching layer which unrolls a projected gradient descent algorithm in which the projection step exploits the Dykstra's algorithm. We prove that under mild conditions, the matching is guaranteed to converge to the optimal one. In practice, it achieves similar performance compared to the Hungarian algorithm during inference. Meanwhile, we can back-propagate through it to learn the cost matrix. After matching, a U-Net style architecture is exploited to refine the matched mask per time step. On DAVIS 2017 dataset, DMM-Net achieves the best performance without online learning on the first frames and the 2nd best with it. Without any fine-tuning, DMM-Net performs comparably to state-of-the-art methods on SegTrack v2 dataset. At last, our differentiable matching layer is very simple to implement; we attach the PyTorch code in the supplementary material which is less than 50 lines long.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_DMM-Net_Differentiable_Mask-Matching_Network_for_Video_Object_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_DMM-Net_Differentiable_Mask-Matching_Network_for_Video_Object_Segmentation_ICCV_2019_paper.pdf,,https://github.com/ZENGXH/DMM_Net,,main,Poster,https://ieeexplore.ieee.org/document/9008825/,"['Proposals', 'Feature extraction', 'Prediction algorithms', 'Strain', 'Object segmentation', 'Inference algorithms', 'Task analysis']","['Differential Network', 'Object Segmentation', 'Video Object Segmentation', 'Time Step', 'Convolutional Neural Network', 'Deep Neural Network', 'Deep Convolutional Neural Network', 'Online Learning', 'U-Net Architecture', 'Dijkstraâ€™s Algorithm', 'Mask R-CNN', 'Cost Matrix', 'Matching Layer', 'Projected Gradient Descent', 'Training Set', 'Learning Rate', 'Input Image', 'Reference Frame', 'Feature Maps', 'Video Frames', 'Current Frame', 'Optical Flow', 'Matching Cost', 'Refinement Network', 'Segmentation Quality', 'Metric Learning', 'COCO Dataset', 'Matching Problem', 'Template Matching', 'Changes In Appearance']",,48,"In this paper, we propose the differentiable mask-matching network (DMM-Net) for solving the video object segmentation problem where the initial object masks are provided. Relying on the Mask R-CNN backbone, we extract mask proposals per frame and formulate the matching between object templates and proposals as a linear assignment problem where thA heading inside a blocke cost matrix is predicted by a deep convolutional neural network. We propose a differentiable matching layer which unrolls a projected gradient descent algorithm in which the projection step exploits the Dykstra's algorithm. We prove that under mild conditions, the matching is guaranteed to converge to the optimal one. In practice, it achieves similar performance compared to the Hungarian algorithm during inference. Meanwhile, we can back-propagate through it to learn the cost matrix. After matching, a U-Net style architecture is exploited to refine the matched mask per time step. On DAVIS 2017 dataset, DMM-Net achieves the best performance without online learning on the first frames and the 2nd best with it. Without any fine-tuning, DMM-Net performs comparably to state-of-the-art methods on SegTrack v2 dataset. At last, our differentiable matching layer is very simple to implement; we attach the PyTorch code in the supplementary material which is less than 50 lines long."
DPOD: 6D Pose Object Detector and Refiner,"Sergey Zakharov, Ivan Shugurov, Slobodan Ilic",Technical University of Munich; Siemens Corporate Technology,50.0,germany,50.0,Germany,"In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010850/,"['Detectors', 'Training', 'Three-dimensional displays', 'Pose estimation', 'Machine learning', 'Solid modeling', 'Training data']","['Object Detection', 'Refiner', '6D Pose', 'Training Data', 'Deep Learning', 'Input Image', 'RGB Images', 'Pose Estimation', 'Pose Estimation Methods', '3D Object Detection', 'Synthetic Training Data', 'Real Training Data', 'Data Augmentation', 'ImageNet', 'Bounding Box', 'Relational Approach', 'Detection In Images', 'Depth Images', 'Iterative Refinement', 'Camera Pose', 'Human Pose Estimation', 'Object Pose', 'Ground Truth Pose', 'Spherical Projection', 'Dense Correspondence', 'Separate Heading', 'Object Bounding Boxes', 'Accuracy Of Pose Estimation', 'Current Pose', 'Object Bounding']",,334,"In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable."
DSConv: Efficient Convolution Operator,"Marcelo Gennari do Nascimento, Roger Fawcett, Victor Adrian Prisacariu","Intel Corporation, https://www.omnitek.tv/about; University of Oxford, Active Vision Lab",50.0,uk,50.0,USA,"Quantization is a popular way of increasing the speed and lowering the memory usage of Convolution Neural Networks (CNNs). When labelled training data is available, network weights and activations have successfully been quantized down to 1-bit. The same cannot be said about the scenario when labelled training data is not available, e.g. when quantizing a pre-trained model, where current approaches show, at best, no loss of accuracy at 8-bit quantizations. We introduce DSConv, a flexible quantized convolution operator that replaces single-precision operations with their far less expensive integer counterparts, while maintaining the probability distributions over both the kernel weights and the outputs. We test our model as a plug-and-play replacement for standard convolution on most popular neural network architectures, ResNet, DenseNet, GoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with less than 1% loss of accuracy, without retraining, using only 4-bit quantization. We also show how a distillation-based adaptation stage with unlabelled data can improve results even further.",,http://openaccess.thecvf.com/content_ICCV_2019/html/do_Nascimento_DSConv_Efficient_Convolution_Operator_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/do_Nascimento_DSConv_Efficient_Convolution_Operator_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008114/,"['Quantization (signal)', 'Tensile stress', 'Convolution', 'Neural networks', 'Data models', 'Kernel', 'Training data']","['Convolution Operation', 'Neural Network', 'Training Data', 'Accuracy Loss', 'Unlabeled Data', 'AlexNet', 'Labeled Training Data', 'Single Precision', 'Exponent', 'Validation Set', 'Scaling Factor', 'Previous Paper', 'Fixed Point', 'Batch Normalization', 'Block Size', 'Low Precision', 'Integer Values', 'Pre-trained Network', 'Channel Size', 'Floating-point Operations', 'Custom Hardware', 'Tensor Of Size', 'Floating-point Numbers', 'Coding Tree', 'Bitwise Operations', 'Variety Of Architectures', 'Precise Network', 'Fast Inference', 'Neural Network Inference', 'ImageNet']",,40,"Quantization is a popular way of increasing the speed and lowering the memory usage of Convolution Neural Networks (CNNs). When labelled training data is available, network weights and activations have successfully been quantized down to 1-bit. The same cannot be said about the scenario when labelled training data is not available, e.g. when quantizing a pre-trained model, where current approaches show, at best, no loss of accuracy at 8-bit quantizations. We introduce DSConv, a flexible quantized convolution operator that replaces single-precision operations with their far less expensive integer counterparts, while maintaining the probability distributions over both the kernel weights and the outputs. We test our model as a plug-and-play replacement for standard convolution on most popular neural network architectures, ResNet, DenseNet, GoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with less than 1% loss of accuracy, without retraining, using only 4-bit quantization. We also show how a distillation-based adaptation stage with unlabelled data can improve results even further."
DSIC: Deep Stereo Image Compression,"Jerry Liu, Shenlong Wang, Raquel Urtasun",Uber ATG,0.0,,100.0,USA,"In this paper we tackle the problem of stereo image compression, and leverage the fact that the two images have overlapping fields of view to further compress the representations. Our approach leverages state-of-the-art single-image compression autoencoders and enhances the compression with novel parametric skip functions to feed fully differentiable, disparity-warped features at all levels to the encoder/decoder of the second image. Moreover, we model the probabilistic dependence between the image codes using a conditional entropy model. Our experiments show an impressive 30 - 50% reduction in the second image bitrate at low bitrates compared to deep single-image compression, and a 10 - 20% reduction at higher bitrates.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_DSIC_Deep_Stereo_Image_Compression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_DSIC_Deep_Stereo_Image_Compression_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010974/,"['Image coding', 'Bit rate', 'Entropy', 'Video compression', 'Decoding', 'Cameras', 'Redundancy']","['Image Compression', 'Stereo Images', 'Least Significant Bit', 'Most Significant Bit', 'Conditional Entropy', 'Entropy Model', 'Image Steganography', 'Convolutional Neural Network', 'Decoding', 'Feature Maps', 'Probability Density Function', 'Gaussian Mixture Model', 'Peak Signal-to-noise Ratio', 'Markov Random Field', 'Cumulative Density Function', 'Motion Compensation', 'Latent Code', 'Stereo Pairs', 'Video Compression', 'Joint Entropy', 'Cost Volume', 'Stereo Matching', 'Decoding Function', 'Disparity Map', 'Feature Maps Of Images', 'Disparity Range', 'Level Of Encoding', 'Bitstream', 'Images In Order', 'Global Context']",,22,"In this paper we tackle the problem of stereo image compression, and leverage the fact that the two images have overlapping fields of view to further compress the representations. Our approach leverages state-of-the-art single-image compression autoencoders and enhances the compression with novel parametric skip functions to feed fully differentiable, disparity-warped features at all levels to the encoder/decoder of the second image. Moreover, we model the probabilistic dependence between the image codes using a conditional entropy model. Our experiments show an impressive 30 - 50% reduction in the second image bitrate at low bitrates compared to deep single-image compression, and a 10 - 20% reduction at higher bitrates."
DUAL-GLOW: Conditional Flow-Based Generative Model for Modality Transfer,"Haoliang Sun,  Ronak Mehta,  Hao H. Zhou,  Zhichun Huang,  Sterling C. Johnson,  Vivek Prabhakaran,  Vikas Singh","1University of Wisconsin-Madison; 1University of Wisconsin-Madison, 2Shandong University, 3Inception Institute of Artiﬁcial Intelligence",100.0,"china, uae, usa",0.0,,"Positron emission tomography (PET) imaging is an imaging modality for diagnosing a number of neurological diseases. In contrast to Magnetic Resonance Imaging (MRI), PET is costly and involves injecting a radioactive substance into the patient. Motivated by developments in modality transfer in vision, we study the generation of certain types of PET images from MRI data. We derive new flow-based generative models which we show perform well in this small sample size regime (much smaller than dataset sizes available in standard vision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks and a relation network that maps the latent spaces to each other. We discuss how given the prior distribution, learning the conditional distribution of PET given the MRI image reduces to obtaining the conditional distribution between the two latent codes w.r.t. the two image types. We also extend our framework to leverage ""side"" information (or attributes) when available. By controlling the PET generation through ""conditioning"" on age, our model is also able to capture brain FDG-PET (hypometabolism) changes, as a function of age. We present experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset with 826 subjects, and obtain good performance in PET image synthesis, qualitatively and quantitatively better than recent works.",http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_DUAL-GLOW_Conditional_Flow-Based_Generative_Model_for_Modality_Transfer_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_DUAL-GLOW_Conditional_Flow-Based_Generative_Model_for_Modality_Transfer_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9010685/,"['Positron emission tomography', 'Magnetic resonance imaging', 'Computational modeling', 'Image generation', 'Diseases', 'Task analysis']","['Flow-based Generative Models', 'Magnetic Resonance Imaging', 'Positron Emission Tomography', 'Network Of Relationships', 'Conditional Distribution', 'Function Of Age', 'Latent Space', 'Positron Emission Tomography Imaging', 'Image Synthesis', 'Latent Code', 'Medical Imaging', 'Support Vector Machine', 'Latent Variables', 'Computer Vision', 'Mild Cognitive Impairment', 'Conditional Probability', 'Extensive Experiments', 'Deep Convolutional Neural Network', 'Network Inference', 'Simple Distribution', 'Generative Adversarial Networks', 'Invertible Function', 'Early Mild Cognitive Impairment', 'Peak Signal-to-noise Ratio', 'Image Generation', 'Latent Representation', 'Unknown Distribution', 'Rule Changes']",,25,"Positron emission tomography (PET) imaging is an imaging modality for diagnosing a number of neurological diseases. In contrast to Magnetic Resonance Imaging (MRI), PET is costly and involves injecting a radioactive substance into the patient. Motivated by developments in modality transfer in vision, we study the generation of certain types of PET images from MRI data. We derive new flow-based generative models which we show perform well in this small sample size regime (much smaller than dataset sizes available in standard vision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks and a relation network that maps the latent spaces to each other. We discuss how given the prior distribution, learning the conditional distribution of PET given the MRI image reduces to obtaining the conditional distribution between the two latent codes w.r.t. the two image types. We also extend our framework to leverage ``side'' information (or attributes) when available. By controlling the PET generation through ``conditioning'' on age, our model is also able to capture brain FDG-PET (hypometabolism) changes, as a function of age. We present experiments on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset with 826 subjects, and obtain good performance in PET image synthesis, qualitatively and quantitatively better than recent works."
DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense,"Hang Zhou, Kejiang Chen, Weiming Zhang, Han Fang, Wenbo Zhou, Nenghai Yu",University of Science and Technology of China,100.0,china,0.0,,"Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classification, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a data-driven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8% of C&W and l2 loss based attack (point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point adding) and 9.0% of saliency map based attack (point dropping) under 200 dropped points on PointNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_DUP-Net_Denoiser_and_Upsampler_Network_for_3D_Adversarial_Point_Clouds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_DUP-Net_Denoiser_and_Upsampler_Network_for_3D_Adversarial_Point_Clouds_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010939/,"['Three-dimensional displays', 'Robustness', 'Perturbation methods', 'Neural networks', 'Training', 'Machine learning', 'Measurement']","['Point Cloud', '3D Point Cloud', 'Adversarial Point Clouds', 'Neural Network', 'Denoising', 'Small Datasets', 'Target Model', 'Outlier Removal', 'Saliency Map', 'Adversarial Attacks', 'Hausdorff Distance', 'Adversarial Examples', 'Point Cloud Dataset', 'Point Cloud Classification', 'Deep Network', 'Classification Accuracy', 'Manifold', 'Deep Neural Network', 'Percentage Points', 'Classification Network', 'Earth Moverâ€™s Distance', 'White-box Attack', 'Object Surface', 'Adversarial Perturbations', 'Adversarial Training', 'Simple Random Sampling', 'Point Cloud Data', 'Defense Methods', 'Attack Success Rate', 'Reconstruction Loss']",,77,"Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classification, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a data-driven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8% of C&W and l2 loss based attack (point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point adding) and 9.0% of saliency map based attack (point dropping) under 200 dropped points on PointNet."
Data-Free Learning of Student Networks,"Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, Qi Tian","Huawei Noah’s Ark Lab; National Engineering Laboratory for Video Technology, Peking University, Peng Cheng Laboratory; School of Computer Science, Faculty of Engineering, The University of Sydney, Australia; Key Lab of Machine Perception (MOE), CMIC, School of EECS, Peking University, China",75.0,"australia, china",25.0,China,"Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (e.g. privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efficient deep neural networks by exploiting generative adversarial networks (GANs). To be specific, the pre-trained teacher networks are regarded as a fixed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efficient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efficient student networks learned using the proposed Data-Free Learning (DFL) method achieve 92.22% and 74.47% accuracies without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56% accuracy on the CelebA benchmark.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Data-Free_Learning_of_Student_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Data-Free_Learning_of_Student_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010308/,"['Training', 'Gallium nitride', 'Biological neural networks', 'Feature extraction', 'Generators', 'Knowledge engineering']","['Student Network', 'Neural Network', 'Training Data', 'Training Dataset', 'Deep Network', 'Deep Neural Network', 'Deep Models', 'Generative Adversarial Networks', 'Network Efficiency', 'Pre-trained Network', 'Teacher Network', 'Edge Devices', 'Network Compression', 'Smaller Model Size', 'Loss Function', 'Convolutional Neural Network', 'Metadata', 'Input Image', 'Real-world Data', 'Cross-entropy Loss', 'MNIST Dataset', 'LeNet-5', 'Neural Network Weights', 'Pre-trained Neural Network', 'Compression Algorithm', 'Accuracy Of Network', 'Discrete Cosine Transform', 'Ablation Experiments', 'Benchmark Datasets', 'Information Entropy']",,199,"Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (\eg privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efficient deep neural networks by exploiting generative adversarial networks (GANs). To be specific, the pre-trained teacher networks are regarded as a fixed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efficient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efficient student networks learned using the proposed Data-Free Learning (DFL) method achieve 92.22% and 74.47% accuracies without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56% accuracy on the CelebA benchmark."
Data-Free Quantization Through Weight Equalization and Bias Correction,"Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling","Qualcomm AI Research†, Qualcomm Technologies Netherlands B.V.",100.0,netherlands,0.0,,"We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008784/,"['Quantization (signal)', 'Computational modeling', 'Computer architecture', 'Computer vision', 'Machine learning', 'Training', 'Adaptation models']","['Bias Correction', 'Data-free Quantization', 'Activation Function', 'Model Performance', 'Deep Learning', 'Computer Vision', 'Quantification Method', 'Object Detection', 'Semantic Segmentation', 'Quantification Model', 'Common Architecture', 'Common Vision', 'Segmentation Detection', 'Output Layer', 'Deep Learning Models', 'Scale Parameter', 'Network Layer', 'Performance Degradation', 'Batch Normalization', 'Bias Error', 'Layer Model', 'Output Channels', 'Quantization Levels', 'Shift Parameter', 'Quantization Error', 'Quantification Approach', 'Bit-shift', 'Weight Range', 'Quantization Noise']",,239,"We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection."
DeCaFA: Deep Convolutional Cascade for Face Alignment in the Wild,"Arnaud Dapogny, Kevin Bailly, Matthieu Cord","Datakalab, 114 boulevard Malesherbes, 75017 Paris; LIP6, Sorbonne Université, CNRS, 4 place Jussieu, 75005 Paris",50.0,france,50.0,USA,"Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce an end-to-end deep convolutional cascade (DeCaFA) architecture for face alignment. Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce DeCaFA, an end-to-end deep convolutional cascade architecture for face alignment. DeCaFA uses fully-convolutional stages to keep full spatial resolution throughout the cascade. Between each cascade stage, DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark-wise attention maps for each of several landmark alignment tasks. Weighted intermediate supervision, as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end-to-end manner. We show experimentally that DeCaFA significantly outperforms existing approaches on 300W, CelebA and WFLW databases. In addition, we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dapogny_DeCaFA_Deep_Convolutional_Cascade_for_Face_Alignment_in_the_Wild_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dapogny_DeCaFA_Deep_Convolutional_Cascade_for_Face_Alignment_in_the_Wild_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010350/,"['Task analysis', 'Face', 'Databases', 'Nose', 'Computer vision', 'Shape', 'Computer architecture']","['Face Alignment', 'Feature Fusion', 'Attention Map', 'Transfer Layer', 'Number Of Landmarks', 'Stage Of The Cascade', 'Alignment Task', 'Convolutional Layers', 'Input Image', 'Facial Expressions', 'Room For Improvement', 'Recent Approaches', 'Training Images', 'Bounding Box', 'Deep Approach', 'Global Information', 'Feature Points', 'Pose Estimation', 'Landmark Localization', 'Image Annotation', 'Head Pose', 'Human Pose Estimation', 'Landmark Coordinates', 'Test Partition', 'Common Subset', 'Tip Of The Nose']",,60,"Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce an end-to-end deep convolutional cascade (DeCaFA) architecture for face alignment. Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce DeCaFA, an end-to-end deep convolutional cascade architecture for face alignment. DeCaFA uses fully-convolutional stages to keep full spatial resolution throughout the cascade. Between each cascade stage, DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark-wise attention maps for each of several landmark alignment tasks. Weighted intermediate supervision, as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end-to-end manner. We show experimentally that DeCaFA significantly outperforms existing approaches on 300W, CelebA and WFLW databases. In addition, we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data."
DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better,"Orest Kupyn, Tetiana Martyniuk, Junru Wu, Zhangyang Wang","Department of Computer Science and Engineering, Texas A&M University; Ukrainian Catholic University, Lviv, Ukraine; SoftServe, Lviv, Ukraine; Ukrainian Catholic University, Lviv, Ukraine",75.0,"Ukraine, usa",25.0,Ukraine,"We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-V2, which considerably boosts state-of-the-art deblurring performance while being much more flexible and efficient. DeblurGAN-V2 is based on a relativistic conditional GAN with a double-scale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-V2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plug-in of sophisticated backbones (e.g. Inception ResNet v2) can lead to solid state-of-the-art performance. Meanwhile, with light-weight backbones (e.g. MobileNet and its variants), DeblurGAN-V2 becomes 10-100 times faster than the nearest competitors, while maintaining close to state-of-the-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-V2 has very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. In addition, we show the architecture to be effective for general image restoration tasks too. Our models and codes will be made available upon acceptance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kupyn_DeblurGAN-v2_Deblurring_Orders-of-Magnitude_Faster_and_Better_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kupyn_DeblurGAN-v2_Deblurring_Orders-of-Magnitude_Faster_and_Better_ICCV_2019_paper.pdf,,https://github.com/KupynOrest/DeblurGANv2,,main,Poster,https://ieeexplore.ieee.org/document/9008540/,"['Image restoration', 'Gallium nitride', 'Training', 'Generative adversarial networks', 'Generators', 'Feature extraction', 'Task analysis']","['Generative Adversarial Networks', 'Real-time Video', 'Feature Pyramid Network', 'Conditional Generative Adversarial Network', 'Block Generation', 'Convolutional Network', 'Convolutional Neural Network', 'Favorable Outcome', 'Feature Maps', 'Quantitative Evaluation', 'Super-resolution', 'Semantic Information', 'Image Enhancement', 'Video Capture', 'Image Sharpness', 'Real-world Images', 'Perceptual Loss', 'Single GPU', 'Stable Training', 'Blurred Images', 'Image Deblurring', 'Blur Kernel', 'GoPro Hero', 'Blurry Images', 'Efficient Inference', 'Visual Quality', 'Image Pairs', 'Perception Of Quality']",,642,"We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-V2, which considerably boosts state-of-the-art deblurring performance while being much more flexible and efficient. DeblurGAN-V2 is based on a relativistic conditional GAN with a double-scale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-V2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plug-in of sophisticated backbones (e.g. Inception ResNet v2) can lead to solid state-of-the-art performance. Meanwhile, with light-weight backbones (e.g. MobileNet and its variants), DeblurGAN-V2 becomes 10-100 times faster than the nearest competitors, while maintaining close to state-of-the-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-V2 has very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. In addition, we show the architecture to be effective for general image restoration tasks too. Our models and codes will be made available upon acceptance."
DeceptionNet: Network-Driven Domain Randomization,"Sergey Zakharov, Wadim Kehl, Slobodan Ilic",Technical University of Munich; Siemens Corporate Technology; Toyota Research Institute,66.66666666666666,"germany, usa",33.33333333333334,Germany,"We present a novel approach to tackle domain adaptation between synthetic and real data. Instead, of employing ""blind"" domain randomization, i.e., augmenting synthetic renderings with random backgrounds or changing illumination and colorization, we leverage the task network as its own adversarial guide toward useful augmentations that maximize the uncertainty of the output. To this end, we design a min-max optimization scheme where a given task competes against a special deception network to minimize the task error subject to the specific constraints enforced by the deceiver. The deception network samples from a family of differentiable pixel-level perturbations and exploits the task architecture to find the most destructive augmentations. Unlike GAN-based approaches that require unlabeled data from the target domain, our method achieves robust mappings that scale well to multiple target distributions from source data alone. We apply our framework to the tasks of digit recognition on enhanced MNIST variants, classification and object pose estimation on the Cropped LineMOD dataset as well as semantic segmentation on the Cityscapes dataset and compare it to a number of domain adaptation approaches, thereby demonstrating similar results with superior generalization capabilities.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DeceptionNet_Network-Driven_Domain_Randomization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DeceptionNet_Network-Driven_Domain_Randomization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009063/,"['Task analysis', 'Decoding', 'Training', 'Gallium nitride', 'Distortion', 'Image recognition', 'Robustness']","['Domain Adaptation', 'Data Sources', 'Semantic Segmentation', 'Generalization Capability', 'Target Domain', 'Pose Estimation', 'Target Distribution', 'Random Changes', 'Human Pose Estimation', 'Task Network', 'Decoding', 'Similar Fashion', 'Unsupervised Learning', 'Random Method', 'Intersection Over Union', 'Target Image', 'Latent Space', 'Source Images', 'Target Data', 'Adaptive Technique', 'Domain Adaptation Methods', 'Background', 'Recognition Network', 'Synthetic Images', 'Domain Gap', 'Decoder Output', 'Angle Error', 'Source Dataset', 'Unseen Domains']",,47,"We present a novel approach to tackle domain adaptation between synthetic and real data. Instead, of employing ""blind"" domain randomization, i.e., augmenting synthetic renderings with random backgrounds or changing illumination and colorization, we leverage the task network as its own adversarial guide toward useful augmentations that maximize the uncertainty of the output. To this end, we design a min-max optimization scheme where a given task competes against a special deception network to minimize the task error subject to the specific constraints enforced by the deceiver. The deception network samples from a family of differentiable pixel-level perturbations and exploits the task architecture to find the most destructive augmentations. Unlike GAN-based approaches that require unlabeled data from the target domain, our method achieves robust mappings that scale well to multiple target distributions from source data alone. We apply our framework to the tasks of digit recognition on enhanced MNIST variants, classification and object pose estimation on the Cropped LineMOD dataset as well as semantic segmentation on the Cityscapes dataset and compare it to a number of domain adaptation approaches, thereby demonstrating similar results with superior generalization capabilities."
Deep Appearance Maps,"Maxim Maximov, Laura Leal-TaixÃ©, Mario Fritz, Tobias Ritschel",CISPA Helmholtz Center for Information Security; Technical University Munich; University College London,100.0,"germany, uk",0.0,,"We propose a deep representation of appearance, i.e. the relation of color, surface orientation, viewer position, material and illumination. Previous approaches have used deep learning to extract classic appearance representations relating to reflectance model parameters (e.g. Phong) or illumination (e.g. HDR environment maps). We suggest to directly represent appearance itself as a network we call a deep appearance map (DAM). This is a 4D generalization over 2D reflectance maps, which held the view direction fixed. First, we show how a DAM can be learned from images or video frames and later be used to synthesize appearance, given new surface orientations and viewer positions. Second, we demonstrate how another network can be used to map from an image or video frames to a DAM network to reproduce this appearance, without using a lengthy optimization such as stochastic gradient descent (learning-to-learn). Finally, we show the example of an appearance estimation-and-segmentation task, mapping from an image showing multiple materials to multiple deep appearance maps.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Maximov_Deep_Appearance_Maps_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Maximov_Deep_Appearance_Maps_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010700/,"['Lighting', 'Two dimensional displays', 'Rendering (computer graphics)', 'Task analysis', 'Geometry', 'Neural networks', 'Image color analysis']","['Illumination', 'Stochastic Gradient Descent', 'Video Frames', 'Multiple Materials', 'Deep Representation', 'Environment Map', 'Surface Orientation', 'View Direction', 'Convolutional Neural Network', 'Deep Neural Network', 'Input Image', 'Parametrized', '2D Images', 'Fully-connected Layer', 'Light Field', 'Number Of Materials', 'Image Synthesis', 'Internal Parameters', 'Explicit Representation', 'Ground Truth Segmentation', 'Explicit Parameters', 'World Space', 'Single 2D']",,21,"We propose a deep representation of appearance, i.e. the relation of color, surface orientation, viewer position, material and illumination. Previous approaches have used deep learning to extract classic appearance representations relating to reflectance model parameters (e.g. Phong) or illumination (e.g. HDR environment maps). We suggest to directly represent appearance itself as a network we call a deep appearance map (DAM). This is a 4D generalization over 2D reflectance maps, which held the view direction fixed. First, we show how a DAM can be learned from images or video frames and later be used to synthesize appearance, given new surface orientations and viewer positions. Second, we demonstrate how another network can be used to map from an image or video frames to a DAM network to reproduce this appearance, without using a lengthy optimization such as stochastic gradient descent (learning-to-learn). Finally, we show the example of an appearance estimation-and-segmentation task, mapping from an image showing multiple materials to multiple deep appearance maps."
Deep Blind Hyperspectral Image Fusion,"Wu Wang, Weihong Zeng, Yue Huang, Xinghao Ding, John Paisley","Department of Electrical Engineering, Columbia University, New York, NY, USA; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, China",100.0,"china, usa",0.0,,"Hyperspectral image fusion (HIF) reconstructs high spatial resolution hyperspectral images from low spatial resolution hyperspectral images and high spatial resolution multispectral images. Previous works usually assume that the linear mapping between the point spread functions of the hyperspectral camera and the spectral response functions of the conventional camera is known. This is unrealistic in many scenarios. We propose a method for blind HIF problem based on deep learning, where the estimation of the observation model and fusion process are optimized iteratively and alternatingly during the super-resolution reconstruction. In addition, the proposed framework enforces simultaneous spatial and spectral accuracy. Using three public datasets, the experimental results demonstrate that the proposed algorithm outperforms existing blind and non-blind methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Blind_Hyperspectral_Image_Fusion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Blind_Hyperspectral_Image_Fusion_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009078/,"['Hyperspectral imaging', 'Spatial resolution', 'Zinc', 'Convolution', 'Training']","['Blind Image', 'Hyperspectral Image Fusion', 'High-resolution', 'Spatial Resolution', 'Deep Learning', 'Low Resolution', 'High Spatial Resolution', 'Super-resolution', 'Multispectral Images', 'Fusion Process', 'Point Spread Function', 'Spectral Response Function', 'Super-resolution Reconstruction', 'High-resolution Multispectral Image', 'Neural Network', 'Training Data', 'Objective Function', 'Convolutional Neural Network', 'Deep Neural Network', 'Spatial Information', 'RGB Images', 'Iterative Refinement', 'Data Fidelity Term', 'Single Image Super-resolution', 'Trainable Parameters', 'Fusion Problem', 'Down-sampling Operation', 'Fusion Function', 'Single Convolution', 'Iterative Framework']",,82,"Hyperspectral image fusion (HIF) reconstructs high spatial resolution hyperspectral images from low spatial resolution hyperspectral images and high spatial resolution multispectral images. Previous works usually assume that the linear mapping between the point spread functions of the hyperspectral camera and the spectral response functions of the conventional camera is known. This is unrealistic in many scenarios. We propose a method for blind HIF problem based on deep learning, where the estimation of the observation model and fusion process are optimized iteratively and alternatingly during the super-resolution reconstruction. In addition, the proposed framework enforces simultaneous spatial and spectral accuracy. Using three public datasets, the experimental results demonstrate that the proposed algorithm outperforms existing blind and non-blind methods."
Deep CG2Real: Synthetic-to-Real Translation via Image Disentanglement,"Sai Bi, Kalyan Sunkavalli, Federico Perazzi, Eli Shechtman, Vladimir G. Kim, Ravi Ramamoorthi",Adobe Research; UC San Diego,50.0,usa,50.0,USA,"We present a method to improve the visual realism of low-quality, synthetic images, e.g. OpenGL renderings. Training an unpaired synthetic-to-real translation network in image space is severely under-constrained and produces visible artifacts. Instead, we propose a semi-supervised approach that operates on the disentangled shading and albedo layers of the image. Our two-stage pipeline first learns to predict accurate shading in a supervised fashion using physically-based renderings as targets, and further increases the realism of the textures and shading with an improved CycleGAN network. Extensive evaluations on the SUNCG indoor scene dataset demonstrate that our approach yields more realistic images compared to other state-of-the-art approaches. Furthermore, networks trained on our generated ""real"" images predict more accurate depth and normals than domain adaptation approaches, suggesting that improving the visual realism of the images can be more effective than imposing task-specific losses.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Deep_CG2Real_Synthetic-to-Real_Translation_via_Image_Disentanglement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Deep_CG2Real_Synthetic-to-Real_Translation_via_Image_Disentanglement_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009005,"['Lighting', 'Task analysis', 'Rendering (computer graphics)', 'Visualization', 'Semantics', 'Generators', 'Training']","['Image Space', 'Synthetic Images', 'Domain Adaptation', 'Images Of Layer', 'Translation Network', 'High-quality', 'Paired Data', 'Receptive Field', 'Visual Quality', 'Residual Block', 'Scene Images', 'Normal Approximation', 'Depth Estimation', 'Separate Networks', 'Unsupervised Manner', 'Forward Translation', 'Percentage Of Pixels', 'Backward Translation', 'Intrinsic Networks', 'Relative Mean Square Error', 'High Visual Quality', 'Domain Gap', 'Cycle Consistency Loss', 'Task Network', 'Illumination', 'Deep Network', 'Visual Comparison', 'Convolutional Block', 'Image Synthesis']",,23,"We present a method to improve the visual realism of low-quality, synthetic images, e.g. OpenGL renderings. Training an unpaired synthetic-to-real translation network in image space is severely under-constrained and produces visible artifacts. Instead, we propose a semi-supervised approach that operates on the disentangled shading and albedo layers of the image. Our two-stage pipeline first learns to predict accurate shading in a supervised fashion using physically-based renderings as targets, and further increases the realism of the textures and shading with an improved CycleGAN network. Extensive evaluations on the SUNCG indoor scene dataset demonstrate that our approach yields more realistic images compared to other state-of-the-art approaches. Furthermore, networks trained on our generated ``real'' images predict more accurate depth and normals than domain adaptation approaches, suggesting that improving the visual realism of the images can be more effective than imposing task-specific losses."
Deep Closest Point: Learning Representations for Point Cloud Registration,"Yue Wang, Justin M. Solomon",Massachusetts Institute of Technology,100.0,usa,0.0,,"Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009466/,"['Three-dimensional displays', 'Iterative closest point algorithm', 'Pipelines', 'Neural networks', 'Natural language processing', 'Optimization', 'Task analysis']","['Point Cloud', 'Representation Learning', 'Closest Point', 'Point Cloud Registration', 'Deep Closest Point', 'Computer Vision', 'Global Features', 'Singular Value Decomposition', 'Rigid Transformation', 'Computer Vision Techniques', 'Iterative Closest Point', 'Neural Network', 'Mean Square Error', 'Deep Learning', 'Transformer', 'Local Features', 'Attention Mechanism', 'Multilayer Perceptron', 'Global Information', 'Attention Module', 'Simultaneous Localization And Mapping', 'Input Point Cloud', 'Graph Neural Networks', 'Feature Matching', 'Graph Attention Network', 'Current Transformer', 'Structure From Motion', 'Pointer Network', 'Embedding Dimension', 'Pair Of Points']",,593,"Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration."
Deep Clustering by Gaussian Mixture Variational Autoencoders With Graph Embedding,"Linxiao Yang, Ngai-Man Cheung, Jiaying Li, Jun Fang",University of Electronic Science and Technology of China; Singapore University of Technology and Design (SUTD),100.0,"china, singapore",0.0,,"We propose DGG:  D eep clustering via a  G aussian-mixture variational autoencoder (VAE) with  G raph embedding. To facilitate clustering, we apply Gaussian mixture model (GMM) as the prior in VAE. To handle data with complex spread, we apply graph embedding. Our idea is that graph information which captures local data structures is an excellent complement to deep GMM. Combining them facilitates the network to learn powerful representations that follow global model and local structural constraints. Therefore, our method unifies model-based and similarity-based approaches for clustering. To combine graph embedding with probabilistic deep GMM, we propose a novel stochastic extension of graph embedding: we treat samples as nodes on a graph and minimize the weighted distance between their posterior distributions. We apply Jenson-Shannon divergence as the distance. We combine the divergence minimization with the log-likelihood maximization of the deep GMM. We derive formulations to obtain an unified objective that enables simultaneous deep representation learning and clustering. Our experimental results show that our proposed DGG outperforms recent deep Gaussian mixture methods (model-based) and deep spectral clustering (similarity-based). Our results highlight advantages of combining model-based and similarity-based clustering as proposed in this work. Our code is published here: https://github.com/dodoyang0929/DGG.git",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Deep_Clustering_by_Gaussian_Mixture_Variational_Autoencoders_With_Graph_Embedding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Deep_Clustering_by_Gaussian_Mixture_Variational_Autoencoders_With_Graph_Embedding_ICCV_2019_paper.pdf,,https://github.com/dodoyang0929/DGG.git,,main,Poster,https://ieeexplore.ieee.org/document/9010011/,"['Training', 'Data models', 'Neural networks', 'Clustering methods', 'Machine learning', 'Gaussian mixture model']","['Mixture Model', 'Variational Autoencoder', 'Deep Clustering', 'Gaussian Mixture Variational Autoencoder', 'Deep Learning', 'Posterior Probability', 'Local Structure', 'Representation Learning', 'Infographic', 'Gaussian Mixture Model', 'Spectral Clustering', 'Model-based Clustering', 'Similarity-based Approach', 'Normal Distribution', 'Neural Network', 'Factorization', 'Characteristics Of Data', 'Deep Neural Network', 'Clustering Method', 'Classification Of Samples', 'Latent Features', 'Similarity-based Methods', 'MNIST Dataset', 'Affinity Matrix', 'Model Inference', 'Cluster Assignment', 'Model-based Methods', 'Gaussian Components', 'Clustering Accuracy', 'Reparameterization Trick']",,61,"We propose DGG: {\textbf D}eep clustering via a {\textbf G}aussian-mixture variational autoencoder (VAE) with {\textbf G}raph embedding. To facilitate clustering, we apply Gaussian mixture model (GMM) as the prior in VAE. To handle data with complex spread, we apply graph embedding. Our idea is that graph information which captures local data structures is an excellent complement to deep GMM. Combining them facilitates the network to learn powerful representations that follow global model and local structural constraints. Therefore, our method unifies model-based and similarity-based approaches for clustering. To combine graph embedding with probabilistic deep GMM, we propose a novel stochastic extension of graph embedding: we treat samples as nodes on a graph and minimize the weighted distance between their posterior distributions. We apply Jenson-Shannon divergence as the distance. We combine the divergence minimization with the log-likelihood maximization of the deep GMM. We derive formulations to obtain an unified objective that enables simultaneous deep representation learning and clustering. Our experimental results show that our proposed DGG outperforms recent deep Gaussian mixture methods (model-based) and deep spectral clustering (similarity-based). Our results highlight advantages of combining model-based and similarity-based clustering as proposed in this work. Our code is published here: https://github.com/dodoyang0929/DGG.git"
Deep Comprehensive Correlation Mining for Image Clustering,"Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, Hongbin Zha","School of Computer Science and Technology, Shandong University; Key Laboratory of Machine Perception (MOE), School of EECS, Peking University; SenseTime Research",66.66666666666666,china,33.33333333333334,China,"Recent developed deep unsupervised methods allow us to jointly learn representation and cluster unlabelled data. These deep clustering methods %like DAC start with mainly focus on the correlation among samples, e.g., selecting high precision pairs to gradually tune the feature representation, which neglects other useful correlations. In this paper, we propose a novel clustering framework, named deep comprehensive correlation mining (DCCM), for exploring and taking full advantage of various kinds of correlations behind the unlabeled data from three aspects: 1) Instead of only using pair-wise information, pseudo-label supervision is proposed to investigate category information and learn discriminative features. 2) The features' robustness to image transformation of input space is fully explored, which benefits the network learning and significantly improves the performance. 3) The triplet mutual information among features is presented for clustering problem to lift the recently discovered instance-level deep mutual information to a triplet-level formation, which further helps to learn more discriminative features. Extensive experiments on several challenging datasets show that our method achieves good performance, e.g., attaining 62.3% clustering accuracy on CIFAR-10, which is 10.1% higher than the state-of-the-art results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Deep_Comprehensive_Correlation_Mining_for_Image_Clustering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Deep_Comprehensive_Correlation_Mining_for_Image_Clustering_ICCV_2019_paper.pdf,,https://github.com/Cory-M/DCCM,,main,Poster,https://ieeexplore.ieee.org/document/9010276/,"['Correlation', 'Mutual information', 'Training', 'Robustness', 'Feature extraction', 'Clustering methods', 'Task analysis']","['Deep Mining', 'Clustering Method', 'Feature Representation', 'Mutual Information', 'Discriminative Features', 'Unlabeled Data', 'Category Information', 'Challenging Dataset', 'Clustering Accuracy', 'Kind Of Correlation', 'Deep Learning', 'Convolutional Neural Network', 'Deep Neural Network', 'Convolutional Layers', 'Deeper Layers', 'Unsupervised Learning', 'Feature Learning', 'Precision And Recall', 'Generative Adversarial Networks', 'Predictive Features', 'Unsupervised Deep Learning', 'Deep Features', 'Largest Probability', 'Deep Learning Features', 'Traditional Clustering Methods', 'Self-supervised Learning', 'Subspace Clustering', 'Unsupervised Feature Learning', 'Progressive Manner', 'Softplus']",,107,"Recent developed deep unsupervised methods allow us to jointly learn representation and cluster unlabelled data. These deep clustering methods %like DAC start with mainly focus on the correlation among samples, e.g., selecting high precision pairs to gradually tune the feature representation, which neglects other useful correlations. In this paper, we propose a novel clustering framework, named deep comprehensive correlation mining~(DCCM), for exploring and taking full advantage of various kinds of correlations behind the unlabeled data from three aspects: 1) Instead of only using pair-wise information, pseudo-label supervision is proposed to investigate category information and learn discriminative features. 2) The features' robustness to image transformation of input space is fully explored, which benefits the network learning and significantly improves the performance. 3) The triplet mutual information among features is presented for clustering problem to lift the recently discovered instance-level deep mutual information to a triplet-level formation, which further helps to learn more discriminative features. Extensive experiments on several challenging datasets show that our method achieves good performance, e.g., attaining 62.3% clustering accuracy on CIFAR-10, which is 10.1% higher than the state-of-the-art results."
Deep Constrained Dominant Sets for Person Re-Identification,"Leulseged Tesfaye Alemu, Marcello Pelillo, Mubarak Shah","CRCV, University of Central Florida; Ca’ Foscari University of Venice, ECLT, Venezia; Ca’ Foscari University of Venice",100.0,"italy, usa",0.0,,"In this work, we propose an end-to-end constrained clustering scheme to tackle the person re-identification (re-id) problem. Deep neural networks (DNN) have recently proven to be effective on person re-identification task. In particular, rather than leveraging solely a probe-gallery similarity, diffusing the similarities among the gallery images in an end-to-end manner has proven to be effective in yielding a robust probe-gallery affinity. However, existing methods do not apply probe image as a constraint, and are prone to noise propagation during the similarity diffusion process. To overcome this, we propose an intriguing scheme which treats person-image retrieval problem as a constrained clustering optimization problem, called deep constrained dominant sets (DCDS). Given a probe and gallery images, we re-formulate person re-id problem as finding a constrained cluster, where the probe image is taken as a constraint (seed) and each cluster corresponds to a set of images corresponding to the same person. By optimizing the constrained clustering in an end-to-end manner, we naturally leverage the contextual knowledge of a set of images corresponding to the given person-images. We further enhance the performance by integrating an auxiliary net alongside DCDS, which employs a multi-scale ResNet. To validate the effectiveness of our method we present experiments on several benchmark datasets and show that the proposed method can outperform state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Alemu_Deep_Constrained_Dominant_Sets_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Alemu_Deep_Constrained_Dominant_Sets_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008999/,"['Probes', 'Robustness', 'Neural networks', 'Optimization', 'Image segmentation', 'Task analysis', 'Computer vision']","['Dominating Set', 'Neural Network', 'Deep Neural Network', 'Benchmark Datasets', 'Imaging Probes', 'Gallery Images', 'Convolutional Network', 'Similarity Score', 'Random Walk', 'Semantic Segmentation', 'Nodes In The Graph', 'Baseline Methods', 'Quadratic Programming', 'Part Of Cluster', 'Deep Neural Network Model', 'Replication Kinetics', 'Graph Neural Networks', 'Contrastive Loss', 'Person Image', 'Affinity Matrix', 'Triplet Loss', 'Graph-based Methods', 'Graph-based Models', 'Membership Scores', 'FC Layer', 'Score Map', 'Parametrized', 'Diagonal Elements', 'Test Phase', 'Image Segmentation']",,15,"In this work, we propose an end-to-end constrained clustering scheme to tackle the person re-identification (re-id) problem. Deep neural networks (DNN) have recently proven to be effective on person re-identification task. In particular, rather than leveraging solely a probe-gallery similarity, diffusing the similarities among the gallery images in an end-to-end manner has proven to be effective in yielding a robust probe-gallery affinity. However, existing methods do not apply probe image as a constraint, and are prone to noise propagation during the similarity diffusion process. To overcome this, we propose an intriguing scheme which treats person-image retrieval problem as a constrained clustering optimization problem, called deep constrained dominant sets (DCDS). Given a probe and gallery images, we re-formulate person re-id problem as finding a constrained cluster, where the probe image is taken as a constraint (seed) and each cluster corresponds to a set of images corresponding to the same person. By optimizing the constrained clustering in an end-to-end manner, we naturally leverage the contextual knowledge of a set of images corresponding to the given person-images. We further enhance the performance by integrating an auxiliary net alongside DCDS, which employs a multi-scale ResNet. To validate the effectiveness of our method we present experiments on several benchmark datasets and show that the proposed method can outperform state-of-the-art methods."
Deep Contextual Attention for Human-Object Interaction Detection,"Tiancai Wang, Rao Muhammad Anwer, Muhammad Haris Khan, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao, Jorma Laaksonen","Inception Institute of Artiﬁcial Intelligence (IIAI), UAE; School of Electrical and Information Engineering, Tianjin University; Department of Computer Science, Aalto University School of Science, Finland",100.0,"china, finland, uae",0.0,,"Human-object interaction detection is an important and relatively new class of visual relationship detection tasks, essential for deeper scene understanding. Most existing approaches decompose the problem into object localization and interaction recognition. Despite showing progress, these approaches only rely on the appearances of humans and objects and overlook the available context information, crucial for capturing subtle interactions between them. We propose a contextual attention framework for human-object interaction detection. Our approach leverages context by learning contextually-aware appearance features for human and object instances. The proposed attention module then adaptively selects relevant instance-centric context information to highlight image regions likely to contain human-object interactions. Experiments are performed on three benchmarks: V-COCO, HICO-DET and HCVRD. Our approach outperforms the state-of-the-art on all datasets. On the V-COCO dataset, our method achieves a relative gain of 4.4% in terms of role mean average precision (mAP role ), compared to the existing best approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Contextual_Attention_for_Human-Object_Interaction_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Contextual_Attention_for_Human-Object_Interaction_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008846/,"['Streaming media', 'Task analysis', 'Detectors', 'Feature extraction', 'Standards', 'Object detection', 'Visualization']","['Contextual Attention', 'Human-object Interaction', 'Human-Object Interaction Detection', 'Contextual Information', 'Object Location', 'Attention Module', 'Appearance Features', 'Mean Average Precision', 'Object Instances', 'Object Interaction', 'Contextual Framework', 'Visual Relationship', 'Convolutional Neural Network', 'Deep Neural Network', 'Feature Maps', 'Input Features', 'Object Detection', 'Global Context', 'Pairwise Interactions', 'Convolutional Neural Network Features', 'Attention Map', 'Feature Module', 'Large Kernel', 'Bottom-up Attention', 'Two-stage Object Detection', 'Region-of-interest Region', 'Two-stage Approach', 'Standard Features']",,95,"Human-object interaction detection is an important and relatively new class of visual relationship detection tasks, essential for deeper scene understanding. Most existing approaches decompose the problem into object localization and interaction recognition. Despite showing progress, these approaches only rely on the appearances of humans and objects and overlook the available context information, crucial for capturing subtle interactions between them. We propose a contextual attention framework for human-object interaction detection. Our approach leverages context by learning contextually-aware appearance features for human and object instances. The proposed attention module then adaptively selects relevant instance-centric context information to highlight image regions likely to contain human-object interactions. Experiments are performed on three benchmarks: V-COCO, HICO-DET and HCVRD. Our approach outperforms the state-of-the-art on all datasets. On the V-COCO dataset, our method achieves a relative gain of 4.4% in terms of role mean average precision (mAP role ), compared to the existing best approach."
Deep Depth From Aberration Map,"Masako Kashiwagi, Nao Mishima, Tatsuo Kozakaya, Shinsaku Hiura",Toshiba Corporate Research & Development Center; University of Hyogo,50.0,Japan,50.0,Japan,"Passive and convenient depth estimation from single-shot image is still an open problem. Existing depth from defocus methods require multiple input images or special hardware customization. Recent deep monocular depth estimation is also limited to an image with sufficient contextual information. In this work, we propose a novel method which realizes a single-shot deep depth measurement based on physical depth cue using only an off-the-shelf camera and lens. When a defocused image is taken by a camera, it contains various types of aberrations corresponding to distances from the image sensor and positions in the image plane. We call these minute and complexly compound aberrations as Aberration Map (A-Map) and we found that A-Map can be utilized as reliable physical depth cue. Additionally, our deep network named A-Map Analysis Network (AMA-Net) is also proposed, which can effectively learn and estimate depth via A-Map. To evaluate validity and robustness of our approach, we have conducted extensive experiments using both real outdoor scenes and simulated images. The qualitative result shows the accuracy and availability of the method in comparison with a state-of-the-art deep context-based method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kashiwagi_Deep_Depth_From_Aberration_Map_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kashiwagi_Deep_Depth_From_Aberration_Map_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009807/,"['Lenses', 'Cameras', 'Estimation', 'Image color analysis', 'Shape', 'Apertures', 'Image sensors']","['Deeper Depths', 'Aberration Maps', 'Deep Network', 'Contextual Information', 'Image Plane', 'Image Sensor', 'Defocus', 'Depth Measurements', 'Simulated Images', 'Depth Estimation', 'Depth Perception', 'Physical Cues', 'Outdoor Scenes', 'Types Of Aberrations', 'Large Errors', 'Quantitative Evaluation', 'Blue Light', 'Red Light', 'F-value', 'Focal Length', 'Point Spread Function', 'Camera Lens', 'Lens Aperture', 'Near-side', 'Accurate Depth', 'Corner Of The Image', 'Positive Dependence', 'Optical Aberrations', 'Field Curves']",,11,"Passive and convenient depth estimation from single-shot image is still an open problem. Existing depth from defocus methods require multiple input images or special hardware customization. Recent deep monocular depth estimation is also limited to an image with sufficient contextual information. In this work, we propose a novel method which realizes a single-shot deep depth measurement based on physical depth cue using only an off-the-shelf camera and lens. When a defocused image is taken by a camera, it contains various types of aberrations corresponding to distances from the image sensor and positions in the image plane. We call these minute and complexly compound aberrations as Aberration Map (A-Map) and we found that A-Map can be utilized as reliable physical depth cue. Additionally, our deep network named A-Map Analysis Network (AMA-Net) is also proposed, which can effectively learn and estimate depth via A-Map. To evaluate validity and robustness of our approach, we have conducted extensive experiments using both real outdoor scenes and simulated images. The qualitative result shows the accuracy and availability of the method in comparison with a state-of-the-art deep context-based method."
Deep Elastic Networks With Model Selection for Multi-Task Learning,"Chanho Ahn, Eunwoo Kim, Songhwai Oh","Dept. of ECE and ASRI, Seoul National University; Department of Engineering Science, University of Oxford",100.0,"south korea, uk",0.0,,"In this work, we consider the problem of instance-wise dynamic network model selection for multi-task learning. To this end, we propose an efficient approach to exploit a compact but accurate model in a backbone architecture for each instance of all tasks. The proposed method consists of an estimator and a selector. The estimator is based on a backbone architecture and structured hierarchically. It can produce multiple different network models of different configurations in a hierarchical structure. The selector chooses a model dynamically from a pool of candidate models given an input instance. The selector is a relatively small-size network consisting of a few layers, which estimates a probability distribution over the candidate models when an input instance of a task is given. Both estimator and selector are jointly trained in a unified learning framework in conjunction with a sampling-based learning strategy, without additional computation steps. We demonstrate the proposed approach for several image classification tasks compared to existing approaches performing model selection or learning multiple tasks. Experimental results show that our approach gives not only outstanding performance compared to other competitors but also the versatility to perform instance-wise model selection for multiple tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ahn_Deep_Elastic_Networks_With_Model_Selection_for_Multi-Task_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ahn_Deep_Elastic_Networks_With_Model_Selection_for_Multi-Task_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010404/,"['Task analysis', 'Computational modeling', 'Convolution', 'Probability distribution', 'Adaptation models', 'Computer vision', 'Computer architecture']","['Model Selection', 'Multi-task Learning', 'Dynamic Model', 'Network Model', 'Hierarchical Structure', 'Learning Framework', 'Multiple Tasks', 'Selection Problem', 'Candidate Models', 'Compact Model', 'Dynamic Selection', 'Input Instance', 'Model Selection Problem', 'Deep Neural Network', 'Convolutional Layers', 'Search Space', 'Weighting Factor', 'Final Classification', 'Recent Approaches', 'Baseline Methods', 'Backbone Network', 'Levels Of Hierarchy', 'Network Compression', 'Multi-task Learning Method', 'Pruning Method', 'Hierarchical Classification', 'Channel Dimension', 'Residual Network', 'Image Scale']",,29,"In this work, we consider the problem of instance-wise dynamic network model selection for multi-task learning. To this end, we propose an efficient approach to exploit a compact but accurate model in a backbone architecture for each instance of all tasks. The proposed method consists of an estimator and a selector. The estimator is based on a backbone architecture and structured hierarchically. It can produce multiple different network models of different configurations in a hierarchical structure. The selector chooses a model dynamically from a pool of candidate models given an input instance. The selector is a relatively small-size network consisting of a few layers, which estimates a probability distribution over the candidate models when an input instance of a task is given. Both estimator and selector are jointly trained in a unified learning framework in conjunction with a sampling-based learning strategy, without additional computation steps. We demonstrate the proposed approach for several image classification tasks compared to existing approaches performing model selection or learning multiple tasks. Experimental results show that our approach gives not only outstanding performance compared to other competitors but also the versatility to perform instance-wise model selection for multiple tasks."
Deep End-to-End Alignment and Refinement for Time-of-Flight RGB-D Module,"Di Qiu, Jiahao Pang, Wenxiu Sun, Chengxi Yang","SenseTime Research, The Chinese University of Hong Kong; SenseTime Research",50.0,Hong Kong,50.0,China,"Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qiu_Deep_End-to-End_Alignment_and_Refinement_for_Time-of-Flight_RGB-D_Module_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Deep_End-to-End_Alignment_and_Refinement_for_Time-of-Flight_RGB-D_Module_ICCV_2019_paper.pdf,,,,main,Poster,http://ieeexplore.ieee.org/document/9008821,"['Cameras', 'Kernel', 'Optical imaging', 'Optical sensors', 'Estimation', 'Adaptive optics']","['Deep Learning', 'RGB Images', 'Optical Flow', 'Computer Graphics', 'RGB Camera', 'Time-of-flight Sensors', 'Loss Function', 'Convolutional Neural Network', 'Deep Neural Network', 'Color Images', 'Thermal Noise', 'Depth Images', 'Depth Measurements', 'Flow Estimation', 'Bias Term', 'Fusion Network', 'Alignment Problem', 'Optical Flow Estimation', 'Synthetic Data Generation', 'Average Mean Absolute Error', 'Alignment Module', 'Multipath Interference', 'Ground Truth Depth', 'Warp Field', 'True Depth', 'Camera Module', 'Depth Camera', 'Depth Values', 'Simple Change']",,18,"Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement."
Deep Floor Plan Recognition Using a Multi-Task Network With Room-Boundary-Guided Attention,"Zhiliang Zeng, Xianzhi Li, Ying Kin Yu, Chi-Wing Fu",; The Chinese University of Hong Kong,100.0,Hong Kong,0.0,,"This paper presents a new approach to recognize elements in floor plan layouts. Besides walls and rooms, we aim to recognize diverse floor plan elements, such as doors, windows and different types of rooms, in the floor layouts. To this end, we model a hierarchy of floor plan elements and design a deep multi-task neural network with two tasks: one to learn to predict room-boundary elements, and the other to predict rooms with types. More importantly, we formulate the room-boundary-guided attention mechanism in our spatial contextual module to carefully take room-boundary features into account to enhance the room-type predictions. Furthermore, we design a cross-and-within-task weighted loss to balance the multi-label tasks and prepare two new datasets for floor plan recognition. Experimental results demonstrate the superiority and effectiveness of our network over the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Deep_Floor_Plan_Recognition_Using_a_Multi-Task_Network_With_Room-Boundary-Guided_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Deep_Floor_Plan_Recognition_Using_a_Multi-Task_Network_With_Room-Boundary-Guided_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009528,"['Layout', 'Task analysis', 'Decoding', 'Neural networks', 'Semantics', 'Image recognition', 'Junctions']","['Ground Plane', 'Multi-task Network', 'Neural Network', 'Weight Loss', 'Deep Neural Network', 'Attention Mechanism', 'Spatial Module', 'Room Type', 'Semantic', 'Convolutional Neural Network', 'Convolutional Layers', 'Spatial Relationship', 'Bounding Box', 'Visual Comparison', 'Shared Features', 'Edge Detection', 'Thin Line', 'Recognition Results', 'Attention Weights', 'Room Layout', 'Network Elements', 'Top Branch', 'Supplementary Material For Results', 'Fully Convolutional Network']",,54,"This paper presents a new approach to recognize elements in floor plan layouts. Besides walls and rooms, we aim to recognize diverse floor plan elements, such as doors, windows and different types of rooms, in the floor layouts. To this end, we model a hierarchy of floor plan elements and design a deep multi-task neural network with two tasks: one to learn to predict room-boundary elements, and the other to predict rooms with types. More importantly, we formulate the room-boundary-guided attention mechanism in our spatial contextual module to carefully take room-boundary features into account to enhance the room-type predictions. Furthermore, we design a cross-and-within-task weighted loss to balance the multi-label tasks and prepare two new datasets for floor plan recognition. Experimental results demonstrate the superiority and effectiveness of our network over the state-of-the-art methods."
Deep Graphical Feature Learning for the Feature Matching Problem,"Zhen Zhang, Wee Sun Lee","Department of Computer Science, National University of Singapore; Australian Institute for Machine Learning, School of Computer Science, The University of Adelaide",100.0,"australia, singapore",0.0,,"The feature matching problem is a fundamental problem in various areas of computer vision including image registration, tracking and motion analysis. Rich local representation is a key part of efficient feature matching methods. However, when the local features are limited to the coordinate of key points, it becomes challenging to extract rich local representations. Traditional approaches use pairwise or higher order handcrafted geometric features to get robust matching; this requires solving NP-hard assignment problems. In this paper, we address this problem by proposing a graph neural network model to transform coordinates of feature points into local features. With our local features, the traditional NP-hard assignment problems are replaced with a simple assignment problem which can be solved efficiently. Promising results on both synthetic and real datasets demonstrate the effectiveness of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010922/,"['Neural networks', 'Kernel', 'Convolution', 'Feature extraction', 'Message passing', 'Inference algorithms', 'Machine learning']","['Deep Learning', 'Feature Matching', 'Matching Problem', 'Feature Matching Problem', 'Neural Network', 'Local Features', 'Geometric Features', 'Coordinates Of Points', 'Feature Points', 'Motion Analysis', 'Graph Neural Networks', 'Assignment Problem', 'Convolutional Neural Network', 'Convolutional Layers', 'Point Cloud', 'Convolution Kernel', 'Graph Structure', 'Scale-invariant', 'Global Pooling', 'Inference Time', 'Types Of Edges', 'Rich Features', 'Branch-and-bound', 'Shallow Model', 'Geometric Problem', 'Graph Matching', 'Linear Layer', 'Delaunay Triangulation', 'Matched Pairs', 'Message Passing']",,36,"The feature matching problem is a fundamental problem in various areas of computer vision including image registration, tracking and motion analysis. Rich local representation is a key part of efficient feature matching methods. However, when the local features are limited to the coordinate of key points, it becomes challenging to extract rich local representations. Traditional approaches use pairwise or higher order handcrafted geometric features to get robust matching; this requires solving NP-hard assignment problems. In this paper, we address this problem by proposing a graph neural network model to transform coordinates of feature points into local features. With our local features, the traditional NP-hard assignment problems are replaced with a simple assignment problem which can be solved efficiently. Promising results on both synthetic and real datasets demonstrate the effectiveness of the proposed method."
Deep Head Pose Estimation Using Synthetic Images and Partial Adversarial Domain Adaption for Continuous Label Spaces,"Felix Kuhnke, JÃ¶rn Ostermann","Institut f¨ur Informationsverarbeitung, Leibniz University Hannover, Germany",100.0,germany,0.0,,"Head pose estimation aims at predicting an accurate pose from an image. Current approaches rely on supervised deep learning, which typically requires large amounts of labeled data. Manual or sensor-based annotations of head poses are prone to errors. A solution is to generate synthetic training data by rendering 3D face models. However, the differences (domain gap) between rendered (source-domain) and real-world (target-domain) images can cause low performance. Advances in visual domain adaptation allow reducing the influence of domain differences using adversarial neural networks, which match the feature spaces between domains by enforcing domain-invariant features. While previous work on visual domain adaptation generally assumes discrete and shared label spaces, these assumptions are both invalid for pose estimation tasks. We are the first to present domain adaptation for head pose estimation with a focus on partially shared and continuous label spaces. More precisely, we adapt the predominant weighting approaches to continuous label spaces by applying a weighted resampling of the source domain during training. To evaluate our approach, we revise and extend existing datasets resulting in a new benchmark for visual domain adaption. Our experiments show that our method improves the accuracy of head pose estimation for real-world images despite using only labels from synthetic images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009467/,"['Pose estimation', 'Task analysis', 'Visualization', 'Handheld computers', 'Feature extraction', 'Face']","['Part Of Domain', 'Pose Estimation', 'Synthetic Images', 'Domain Adaptation', 'Partial Adaptation', 'Label Space', 'Head Pose', 'Adversarial Domain Adaptation', 'Head Pose Estimation', 'Deep Head', 'Training Data', 'Deep Learning', 'Source Domain', 'Real-world Images', 'Domain Gap', 'Discrete Labels', 'Visual Adaptation', 'Synthetic Training Data', 'Adversarial Neural Network', 'Data Sources', 'Target Domain', 'Target Data', 'Source Labels', 'Target Dataset', 'Domain Discriminator', 'Source Distribution', 'FC Layer', 'Bounding Box', 'Target Sample', 'Label Distribution']",,35,"Head pose estimation aims at predicting an accurate pose from an image. Current approaches rely on supervised deep learning, which typically requires large amounts of labeled data. Manual or sensor-based annotations of head poses are prone to errors. A solution is to generate synthetic training data by rendering 3D face models. However, the differences (domain gap) between rendered (source-domain) and real-world (target-domain) images can cause low performance. Advances in visual domain adaptation allow reducing the influence of domain differences using adversarial neural networks, which match the feature spaces between domains by enforcing domain-invariant features. While previous work on visual domain adaptation generally assumes discrete and shared label spaces, these assumptions are both invalid for pose estimation tasks. We are the first to present domain adaptation for head pose estimation with a focus on partially shared and continuous label spaces. More precisely, we adapt the predominant weighting approaches to continuous label spaces by applying a weighted resampling of the source domain during training. To evaluate our approach, we revise and extend existing datasets resulting in a new benchmark for visual domain adaption. Our experiments show that our method improves the accuracy of head pose estimation for real-world images despite using only labels from synthetic images."
Deep Hough Voting for 3D Object Detection in Point Clouds,"Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas","Facebook AI Research, Stanford University; Facebook AI Research",50.0,usa,50.0,USA,"Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qi_Deep_Hough_Voting_for_3D_Object_Detection_in_Point_Clouds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_Deep_Hough_Voting_for_3D_Object_Detection_in_Point_Clouds_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008567/,"['Three-dimensional displays', 'Two dimensional displays', 'Object detection', 'Proposals', 'Detectors', 'Feature extraction', 'Pipelines']","['Object Detection', 'Point Cloud', '3D Object Detection', 'Detection In Point Clouds', 'Hough Voting', 'Deep Hough Voting', 'Deep Network', 'Color Images', '3D Space', 'Bounding Box', 'Regular Grid', '3D Scanning', 'Birdâ€™s Eye', '3D Point Cloud', '3D Detection', 'Scene Point', 'Object Point Cloud', 'Image Segmentation', 'Receptive Field', 'Input Point Cloud', 'Object Proposals', 'Semantic Segmentation', 'Central Objective', 'Object Classification', 'Instance Segmentation', '3D Bounding Box', 'Input Point', 'Object Surface', 'Object Parts']",,756,"Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data - samples from 2D manifolds in 3D space - we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images."
Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval,"Shupeng Su, Zhisheng Zhong, Chao Zhang","Key Laboratory of Machine Perception (MOE), School of EECS, Peking University",100.0,china,0.0,,"Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint-Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Su_Deep_Joint-Semantics_Reconstructing_Hashing_for_Large-Scale_Unsupervised_Cross-Modal_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Su_Deep_Joint-Semantics_Reconstructing_Hashing_for_Large-Scale_Unsupervised_Cross-Modal_Retrieval_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009571/,"['Semantics', 'Binary codes', 'Image reconstruction', 'Training', 'Laplace equations', 'Encoding', 'Correlation']","['Cross-modal Retrieval', 'Unsupervised Cross-modal Retrieval', 'Deep Network', 'Deep Neural Network', 'Hash Function', 'Common Space', 'Binary Code', 'Neighborhood Information', 'Neighborhood Structure', 'Affinity Matrix', 'Retrieval Performance', 'Neighborhood Of The Origin', 'Intrinsic Affinity', 'Time And Space', 'Image Features', 'Network Training', 'Matrix Factorization', 'Multilayer Perceptron', 'Unsupervised Methods', 'Similarity Matrix', 'Latent Dirichlet Allocation', 'Current Batch', 'Input Instance', 'Hamming Distance', 'High Neighborhood', 'Training Objective', 'Random Training', 'Retrieval Results', 'Space Complexity', 'Graph Laplacian']",,184,"Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint-Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks."
Deep Learning for Light Field Saliency Detection,"Tiantian Wang, Yongri Piao, Xiao Li, Lihe Zhang, Huchuan Lu","Dalian University of Technology, China",100.0,china,0.0,,"Recent research in 4D saliency detection is limited by the deficiency of a large-scale 4D light field dataset. To address this, we introduce a new dataset to assist the subsequent research in 4D light field saliency detection. To the best of our knowledge, this is to date the largest light field dataset in which the dataset provides 1465 all-focus images with human-labeled ground truth masks and the corresponding focal stacks for every light field image. To verify the effectiveness of the light field data, we first introduce a fusion framework which includes two CNN streams where the focal stacks and all-focus images serve as the input. The focal stack stream utilizes a recurrent attention mechanism to adaptively learn to integrate every slice in the focal stack, which benefits from the extracted features of the good slices. Then it is incorporated with the output map generated by the all-focus stream to make the saliency prediction. In addition, we introduce adversarial examples by adding noise intentionally into images to help train the deep network, which can improve the robustness of the proposed network. The noise is designed by users, which is imperceptible but can fool the CNNs to make the wrong prediction. Extensive experiments show the effectiveness and superiority of the proposed model on the popular evaluation metrics. The proposed method performs favorably compared with the existing 2D, 3D and 4D saliency detection methods on the proposed dataset and existing LFSD light field dataset. The code and results can be found at https://github.com/OIPLab-DUT/ ICCV2019_Deeplightfield_Saliency. Moreover, to facilitate research in this field, all images we collected are shared in a ready-to-use manner.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Learning_for_Light_Field_Saliency_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Learning_for_Light_Field_Saliency_Detection_ICCV_2019_paper.pdf,,https://github.com/OIPLab-DUT/ICCV2019_Deeplightfield_Saliency,,main,Poster,https://ieeexplore.ieee.org/document/9010926/,"['Saliency detection', 'Streaming media', 'Two dimensional displays', 'Three-dimensional displays', 'Image segmentation', 'Feature extraction', 'Image color analysis']","['Deep Learning', 'Light Field', 'Saliency Detection', 'Detection Methods', 'Convolutional Neural Network', 'Deep Network', 'Attention Mechanism', 'Large-scale Datasets', 'Light Images', 'Light Data', 'Adversarial Examples', 'Recurrent Mechanism', 'Convolutional Layers', 'Feature Maps', 'Feature Representation', 'Recurrent Network', '2D Images', 'Output Feature', 'RGB Images', 'Low-level Features', 'Salient Object', 'Salient Object Detection', 'Depth Perception', 'Camera Array', 'RGB-D Images', 'Output Feature Map', 'Prediction Map', 'RGB Features', 'Saliency Map', 'Hierarchical Features']",,81,"Recent research in 4D saliency detection is limited by the deficiency of a large-scale 4D light field dataset. To address this, we introduce a new dataset to assist the subsequent research in 4D light field saliency detection. To the best of our knowledge, this is to date the largest light field dataset in which the dataset provides 1465 all-focus images with human-labeled ground truth masks and the corresponding focal stacks for every light field image. To verify the effectiveness of the light field data, we first introduce a fusion framework which includes two CNN streams where the focal stacks and all-focus images serve as the input. The focal stack stream utilizes a recurrent attention mechanism to adaptively learn to integrate every slice in the focal stack, which benefits from the extracted features of the good slices. Then it is incorporated with the output map generated by the all-focus stream to make the saliency prediction. In addition, we introduce adversarial examples by adding noise intentionally into images to help train the deep network, which can improve the robustness of the proposed network. The noise is designed by users, which is imperceptible but can fool the CNNs to make the wrong prediction. Extensive experiments show the effectiveness and superiority of the proposed model on the popular evaluation metrics. The proposed method performs favorably compared with the existing 2D, 3D and 4D saliency detection methods on the proposed dataset and existing LFSD light field dataset. The code and results can be found at https://github.com/OIPLab-DUT/ ICCV2019_Deeplightfield_Saliency. Moreover, to facilitate research in this field, all images we collected are shared in a ready-to-use manner."
Deep Learning for Seeing Through Window With Raindrops,"Yuhui Quan, Shijie Deng, Yixin Chen, Hui Ji","Department of Mathematics, National University of Singapore, Singapore 119076; School of Computer Science & Engineering, South China University of Technology, Guangzhou 510006, China",100.0,"china, singapore",0.0,,"When taking pictures through glass window in rainy day, the images are comprised and corrupted by the raindrops adhered to glass surfaces. It is a challenging problem to remove the effect of raindrops from an image. The key task is how to accurately and robustly identify the raindrop regions in an image. This paper develops a convolutional neural network (CNN) for removing the effect of raindrops from an image. In the proposed CNN, we introduce a double attention mechanism that concurrently guides the CNN using shape-driven attention and channel re-calibration. The shape-driven attention exploits physical shape priors of raindrops, i.e. convexness and contour closedness, to accurately locate raindrops, and the channel re-calibration improves the robustness when processing raindrops with varying appearances. The experimental results show that the proposed CNN outperforms the state-of-the-art approaches in terms of both quantitative metrics and visual quality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Quan_Deep_Learning_for_Seeing_Through_Window_With_Raindrops_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Quan_Deep_Learning_for_Seeing_Through_Window_With_Raindrops_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009448/,"['Microsoft Windows', 'Windows', 'Rain', 'Cameras', 'Glass', 'Shape', 'Machine learning']","['Deep Learning', 'Convolutional Neural Network', 'Attention Mechanism', 'Visual Quality', 'Quantitative Metrics', 'Rainy Days', 'Glass Window', 'Convolutional Layers', 'Multilayer Perceptron', 'Multiple Images', 'Generative Adversarial Networks', 'Learnable Parameters', 'Patch Size', 'Attention Module', 'Base -2', 'Attention Map', 'Channel Attention', 'Joint Attention', 'Tangent Vector', 'Adjacent Frames', 'Channel Attention Module', 'Set Of Kernels', 'Downsampling Layer', 'Latent Image', 'Rainy Weather', 'Shallow Convolutional Neural Network', 'Stereo Camera', 'Single Image', 'Visual Hallucinations', 'Output Of Module']",,73,"When taking pictures through glass window in rainy day, the images are comprised and corrupted by the raindrops adhered to glass surfaces. It is a challenging problem to remove the effect of raindrops from an image. The key task is how to accurately and robustly identify the raindrop regions in an image. This paper develops a convolutional neural network (CNN) for removing the effect of raindrops from an image. In the proposed CNN, we introduce a double attention mechanism that concurrently guides the CNN using shape-driven attention and channel re-calibration. The shape-driven attention exploits physical shape priors of raindrops, i.e. convexness and contour closedness, to accurately locate raindrops, and the channel re-calibration improves the robustness when processing raindrops with varying appearances. The experimental results show that the proposed CNN outperforms the state-of-the-art approaches in terms of both quantitative metrics and visual quality."
Deep Mesh Reconstruction From Single RGB Images via Topology Modification Networks,"Junyi Pan, Xiaoguang Han, Weikai Chen, Jiapeng Tang, Kui Jia","Shenzhen Research Institute of Big Data, the Chinese University of Hong Kong (Shenzhen); School of Electronic and Information Engineering, South China University of Technology; USC Institute for Creative Technologies",100.0,"Hong Kong, china, usa",0.0,,"Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difficulty of generating a feasible mesh structure, the state-of-the-art approaches often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difficult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modification. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modification network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary refinement network is designed to refine the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009447/,"['Topology', 'Image reconstruction', 'Three-dimensional displays', 'Network topology', 'Shape', 'Strain', 'Surface reconstruction']","['Single Image', 'Mesh Reconstruction', 'Topological Modification', 'High-quality', 'Visual Quality', 'Reconstruction Accuracy', '3D Mesh', '3D Shape', 'Target Surface', 'Mesh Network', 'Complex Topology', 'Multiple Patches', 'Single Template', 'Mesh Deformation', 'Neural Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Large Errors', 'Input Image', 'Learning Framework', 'Point Cloud', 'Earth Moverâ€™s Distance', 'Multilayer Perceptron', '3D Reconstruction', 'Triangular Mesh', 'Open Boundary', 'Chamfer Distance', 'Directional Distance', 'Template Model', 'Ground Truth Points']",,102,"Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difficulty of generating a feasible mesh structure, the state-of-the-art approaches often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difficult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modification. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modification network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary refinement network is designed to refine the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies."
Deep Meta Functionals for Shape Representation,"Gidi Littwin, Lior Wolf",Tel Aviv University; Tel Aviv University and Facebook AI Research,100.0,israel,0.0,,"We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network parametrized by these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code will be available at: https: //github.com/gidilittwin/Deep-Meta.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Littwin_Deep_Meta_Functionals_for_Shape_Representation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Littwin_Deep_Meta_Functionals_for_Shape_Representation_ICCV_2019_paper.pdf,,https://github.com/gidilittwin/Deep-Meta,,main,Poster,https://ieeexplore.ieee.org/document/9010022/,"['Shape', 'Three-dimensional displays', 'Training', 'Topology', 'Computer architecture', 'Image reconstruction', 'Two dimensional displays']","['Shape Representation', 'Deep Neural Network', 'Single Image', 'Network Parameters', 'Parametrized', 'Network Weights', '3D Shape', '2D Projection', 'Volume Point', 'Decoding', 'Hidden Layer', 'Training Time', 'Input Image', 'Point Cloud', 'Multilayer Perceptron', 'Methods In The Literature', '3D Point', '3D Mesh', 'Decision Boundary', 'Grid Resolution', 'Level Set Method', 'Uniform Random Sampling', 'Smooth Manifold', 'Residual Module', 'Feature Vector Of Size', 'Single View', 'Gradient Information', '3D Representation', 'Loss Term', 'High Capacity']",,37,"We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network parametrized by these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code will be available at: https: //github.com/gidilittwin/Deep-Meta."
Deep Meta Learning for Real-Time Target-Aware Visual Tracking,"Janghoon Choi, Junseok Kwon, Kyoung Mu Lee","ASRI, Department of ECE, Seoul National University; School of CSE, Chung-Ang Univeristy",100.0,"Korea, south korea",0.0,,"In this paper, we propose a novel on-line visual tracking framework based on the Siamese matching network and meta-learner network, which run at real-time speeds. Conventional deep convolutional feature-based discriminative visual tracking algorithms require continuous re-training of classifiers or correlation filters, which involve solving complex optimization tasks to adapt to the new appearance of a target object. To alleviate this complex process, our proposed algorithm incorporates and utilizes a meta-learner network to provide the matching network with new appearance information of the target objects by adding target-aware feature space. The parameters for the target-specific feature space are provided instantly from a single forward-pass of the meta-learner network. By eliminating the necessity of continuously solving complex optimization tasks in the course of tracking, experimental results demonstrate that our algorithm performs at a real-time speed while maintaining competitive performance among other state-of-the-art tracking algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Deep_Meta_Learning_for_Real-Time_Target-Aware_Visual_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Deep_Meta_Learning_for_Real-Time_Target-Aware_Visual_Tracking_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009472/,"['Target tracking', 'Visualization', 'Feature extraction', 'Training', 'Correlation', 'Task analysis', 'Real-time systems']","['Deep Learning', 'Meta Learning', 'Feature Space', 'Target Object', 'Tracking Algorithm', 'Vision Algorithms', 'Matching Network', 'Correlation Filter', 'Convolutional Neural Network', 'Positive Samples', 'Convolutional Layers', 'Feature Maps', 'Target Location', 'Adam Optimizer', 'Intersection Over Union', 'Bounding Box', 'Generalization Capability', 'Challenging Conditions', 'Image X', 'Deep Representation', 'Response Map', 'Tracking Dataset', 'Siamese Network', 'Set Position', 'Bounding Box Annotations', 'Object Trajectory', 'Background Clutter', 'Spatial Regularization', 'Performance Gain', 'Loss Function']",,78,"In this paper, we propose a novel on-line visual tracking framework based on the Siamese matching network and meta-learner network, which run at real-time speeds. Conventional deep convolutional feature-based discriminative visual tracking algorithms require continuous re-training of classifiers or correlation filters, which involve solving complex optimization tasks to adapt to the new appearance of a target object. To alleviate this complex process, our proposed algorithm incorporates and utilizes a meta-learner network to provide the matching network with new appearance information of the target objects by adding target-aware feature space. The parameters for the target-specific feature space are provided instantly from a single forward-pass of the meta-learner network. By eliminating the necessity of continuously solving complex optimization tasks in the course of tracking, experimental results demonstrate that our algorithm performs at a real-time speed while maintaining competitive performance among other state-of-the-art tracking algorithms."
Deep Meta Metric Learning,"Guangyi Chen, Tianren Zhang, Jiwen Lu, Jie Zhou","Beijing National Research Center for Information Science and Technology, China; Department of Automation, Tsinghua University, China",100.0,"China, china",0.0,,"In this paper, we present a deep meta metric learning (DMML) approach for visual recognition. Unlike most existing deep metric learning methods formulating the learning process by an overall objective, our DMML formulates the metric learning in a meta way, and proves that softmax and triplet loss are consistent in the meta space. Specifically, we sample some subsets from the original training set and learn metrics across different subsets. In each sampled sub-task, we split the training data into a support set as well as a query set, and learn the set-based distance, instead of sample-based one, to verify the query cell from multiple support cells. In addition, we introduce hard sample mining for set-based distance to encourage the intra-class compactness. Experimental results on three visual recognition applications including person re-identification, vehicle re-identification and face verification show that the proposed DMML method outperforms most existing approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Deep_Meta_Metric_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Deep_Meta_Metric_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009474/,"['Measurement', 'Task analysis', 'Training', 'Visualization', 'Learning systems', 'Training data', 'Face']","['Deep Learning', 'Metric Learning', 'Metametric Learning', 'Training Set', 'Training Data', 'Learning Process', 'Face Recognition', 'Visual Recognition', 'Support Set', 'Query Set', 'Triplet Loss', 'Personal Face', 'Deep Metric Learning', 'Metric Learning Methods', 'Objective Function', 'Performance Of Method', 'Deep Neural Network', 'Positive Samples', 'Negative Samples', 'Latent Space', 'Query Sample', 'Additional Margin', 'Softmax Loss', 'Inter-class Separability', 'Single Object', 'Contrastive Loss', 'Query Image', 'Conventional Learning Methods', 'Large Margin', 'Conduct Ablation Experiments']",,42,"In this paper, we present a deep meta metric learning (DMML) approach for visual recognition. Unlike most existing deep metric learning methods formulating the learning process by an overall objective, our DMML formulates the metric learning in a meta way, and proves that softmax and triplet loss are consistent in the meta space. Specifically, we sample some subsets from the original training set and learn metrics across different subsets. In each sampled sub-task, we split the training data into a support set as well as a query set, and learn the set-based distance, instead of sample-based one, to verify the query cell from multiple support cells. In addition, we introduce hard sample mining for set-based distance to encourage the intra-class compactness. Experimental results on three visual recognition applications including person re-identification, vehicle re-identification and face verification show that the proposed DMML method outperforms most existing approaches."
Deep Metric Learning With Tuplet Margin Loss,"Baosheng Yu, Dacheng Tao","UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia",100.0,australia,0.0,,"Deep metric learning, in which the loss function plays a key role, has proven to be extremely useful in visual recognition tasks. However, existing deep metric learning loss functions such as contrastive loss and triplet loss usually rely on delicately selected samples (pairs or triplets) for fast convergence. In this paper, we propose a new deep metric learning loss function, tuplet margin loss, using randomly selected samples from each mini-batch. Specifically, the proposed tuplet margin loss implicitly up-weights hard samples and down-weights easy samples, while a slack margin in angular space is introduced to mitigate the problem of overfitting on the hardest sample. Furthermore, we address the problem of intra-pair variation by disentangling class-specific information to improve the generalizability of tuplet margin loss. Experimental results on three widely used deep metric learning datasets, CARS196, CUB200-2011, and Stanford Online Products, demonstrate significant improvements over existing deep metric learning methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010708/,"['Measurement', 'Task analysis', 'Visualization', 'Training', 'Convergence', 'Face recognition', 'Bars']","['Deep Learning', 'Metric Learning', 'Deep Metric Learning', 'Loss Function', 'Random Selection', 'Visual Task', 'Variational Problem', 'Contrastive Loss', 'Triplet Loss', 'Easy Samples', 'Learning Dataset', 'Visual Recognition Tasks', 'Metric Learning Methods', 'Training Set', 'Training Data', 'Learning Models', 'Factorization', 'Scaling Factor', 'Deep Learning Models', 'Backbone Network', 'Types Of Pairs', 'Image Retrieval', 'Random Selection Method', 'Stochastic Gradient Descent', 'Face Recognition', 'Intuitive Example', 'Central Loss', 'Bounding Box', 'Surgical Margins']",,66,"Deep metric learning, in which the loss function plays a key role, has proven to be extremely useful in visual recognition tasks. However, existing deep metric learning loss functions such as contrastive loss and triplet loss usually rely on delicately selected samples (pairs or triplets) for fast convergence. In this paper, we propose a new deep metric learning loss function, tuplet margin loss, using randomly selected samples from each mini-batch. Specifically, the proposed tuplet margin loss implicitly up-weights hard samples and down-weights easy samples, while a slack margin in angular space is introduced to mitigate the problem of overfitting on the hardest sample. Furthermore, we address the problem of intra-pair variation by disentangling class-specific information to improve the generalizability of tuplet margin loss. Experimental results on three widely used deep metric learning datasets, CARS196, CUB200-2011, and Stanford Online Products, demonstrate significant improvements over existing deep metric learning methods."
Deep Multi-Model Fusion for Single-Image Dehazing,"Zijun Deng, Lei Zhu, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Qing Zhang, Jing Qin, Pheng-Ann Heng","Sun Yat-sen University; The Chinese University of Hong Kong; Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, CAS; The Chinese University of Hong Kong, CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, CAS; South China University of Technology; The Hong Kong Polytechnic University",100.0,"China, Hong Kong, china",0.0,,"This paper presents a deep multi-model fusion network to attentively integrate multiple models to separate layers and boost the performance in single-image dehazing. To do so, we first formulate the attentional feature integration module to maximize the integration of the convolutional neural network (CNN) features at different CNN layers and generate the attentional multi-level integrated features (AMLIF). Then, from the AMLIF, we further predict a haze-free result for an atmospheric scattering model, as well as for four haze-layer separation models, and then fuse the results together to produce the final haze-free image. To evaluate the effectiveness of our method, we compare our network with several state-of-the-art methods on two widely-used dehazing benchmark datasets, as well as on two sets of real-world hazy images. Experimental results demonstrate clear quantitative and qualitative improvements of our method over the state-of-the-arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Deep_Multi-Model_Fusion_for_Single-Image_Dehazing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Deep_Multi-Model_Fusion_for_Single-Image_Dehazing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/abstract/document/9009514/,"['Atmospheric modeling', 'Predictive models', 'Scattering', 'Computational modeling', 'Neural networks', 'Computer vision', 'Solid modeling']","['Single Image Dehazing', 'Convolutional Neural Network', 'Deep Network', 'Attention Module', 'Separate Layers', 'Feature Integration', 'Convolutional Neural Network Layers', 'Real-world Images', 'Convolutional Neural Network Features', 'Diffuse Solar Radiation', 'Final Results', 'Superior Performance', 'Linear Combination', 'Convolutional Layers', 'Feature Maps', 'Attention Mechanism', 'Visual Comparison', 'Clear Image', 'Residual Block', 'Pixel Location', 'Transmission Map', 'Atmospheric Light', 'Decomposition Layers', 'Color Distortion', 'Attention Map', 'Synthetic Benchmark', 'Feature Concatenation', 'Detailed Background', 'Shallow Layers', 'Specific Layer']",,103,"This paper presents a deep multi-model fusion network to attentively integrate multiple models to separate layers and boost the performance in single-image dehazing. To do so, we first formulate the attentional feature integration module to maximize the integration of the convolutional neural network (CNN) features at different CNN layers and generate the attentional multi-level integrated features (AMLIF). Then, from the AMLIF, we further predict a haze-free result for an atmospheric scattering model, as well as for four haze-layer separation models, and then fuse the results together to produce the final haze-free image. To evaluate the effectiveness of our method, we compare our network with several state-of-the-art methods on two widely-used dehazing benchmark datasets, as well as on two sets of real-world hazy images. Experimental results demonstrate clear quantitative and qualitative improvements of our method over the state-of-the-arts."
Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition,"Wei Zhai, Yang Cao, Jing Zhang, Zheng-Jun Zha","Department of Automation, University of Science and Technology of China; UBTECH Sydney Artificial Intelligence Centre, The University of Sydney",100.0,"australia, china",0.0,,"Texture recognition is a challenging visual task as multiple perceptual attributes may be perceived from the same texture image when combined with different spatial context. Some recent works building upon Convolutional Neural Network (CNN) incorporate feature encoding with orderless aggregating to provide invariance to spatial layouts. However, these existing methods ignore visual texture attributes, which are important cues for describing the real-world texture images, resulting in incomplete description and inaccurate recognition. To address this problem, we propose a novel deep Multiple-Attribute-Perceived Network (MAP-Net) by progressively learning visual texture attributes in a mutually reinforced manner. Specifically, a multi-branch network architecture is devised, in which cascaded global contexts are learned by introducing similarity constraint at each branch, and leveraged as guidance of spatial feature encoding at next branch through an attribute transfer scheme. To enhance the modeling capability of spatial transformation, a deformable pooling strategy is introduced to augment the spatial sampling with adaptive offsets to the global context, leading to perceive new visual attributes. An attribute fusion module is then introduced to jointly utilize the perceived visual attributes and the abstracted semantic concepts at each branch. Experimental results on the five most challenging texture recognition datasets have demonstrated the superiority of the proposed model against the state-of-the-arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009995,,"['Texture Recognition', 'Real-world Texture', 'Convolutional Neural Network', 'Multiple Dimensions', 'Global Context', 'Textural Properties', 'Spatial Context', 'Semantic Knowledge', 'Image Texture', 'Visual Properties', 'Perceptual Dimensions', 'Challenging Dataset', 'Feature Encoder', 'Similar Constraints', 'Final Results', 'Local Features', 'Feature Maps', 'Feature Representation', 'Deep Convolutional Neural Network', 'Texture Features', 'Triplet Loss', 'Texture Representation', 'Feature Pooling', 'Different Levels Of Features', 'Basic Representation', 'Global Representation', 'Kind Of Loss', 'Global Context Information', 'Softmax Loss']",,25,"Texture recognition is a challenging visual task as multiple perceptual attributes may be perceived from the same texture image when combined with different spatial context. Some recent works building upon Convolutional Neural Network (CNN) incorporate feature encoding with orderless aggregating to provide invariance to spatial layouts. However, these existing methods ignore visual texture attributes, which are important cues for describing the real-world texture images, resulting in incomplete description and inaccurate recognition. To address this problem, we propose a novel deep Multiple-Attribute-Perceived Network (MAP-Net) by progressively learning visual texture attributes in a mutually reinforced manner. Specifically, a multi-branch network architecture is devised, in which cascaded global contexts are learned by introducing similarity constraint at each branch, and leveraged as guidance of spatial feature encoding at next branch through an attribute transfer scheme. To enhance the modeling capability of spatial transformation, a deformable pooling strategy is introduced to augment the spatial sampling with adaptive offsets to the global context, leading to perceive new visual attributes. An attribute fusion module is then introduced to jointly utilize the perceived visual attributes and the abstracted semantic concepts at each branch. Experimental results on the five most challenging texture recognition datasets have demonstrated the superiority of the proposed model against the state-of-the-arts."
Deep Non-Rigid Structure From Motion,"Chen Kong, Simon Lucey",Carnegie Mellon University,100.0,usa,0.0,,"Current non-rigid structure from motion (NRSfM) algorithms are mainly limited with respect to: (i) the number of images, and (ii) the type of shape variability they can handle. This has hampered the practical utility of NRSfM for many applications within vision. In this paper we propose a novel deep neural network to recover camera poses and 3D points solely from an ensemble of 2D image coordinates. The proposed neural network is mathematically interpretable as a multi-layer block sparse dictionary learning problem, and can handle problems of unprecedented scale and shape complexity. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in the order of magnitude. We further propose a quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstruction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009016/,"['Shape', 'Three-dimensional displays', 'Encoding', 'Dictionaries', 'Two dimensional displays', 'Neural networks', 'Sparse matrices']","['Structure From Motion', 'Non-rigid Structure', 'Neural Network', 'Deep Neural Network', 'Impressive Performance', 'Image Coordinates', 'Dictionary Learning', 'Sparse Learning', 'Ground Truth 3D', '3D Structure', 'Singular Value Decomposition', 'Feed-forward Network', 'Frobenius Norm', '3D Shape', 'Sparse Representation', 'Kronecker Product', 'Sparse Model', 'Bias Term', '2D Projection', 'Sparse Coding', '3D Error', 'Soft-thresholding Operator', 'Shape Priors', 'Spectral Norm', '2D Point', 'Object Geometry', 'Multilayer Model', 'Reprojection', 'Step Size']",,41,"Current non-rigid structure from motion (NRSfM) algorithms are mainly limited with respect to: (i) the number of images, and (ii) the type of shape variability they can handle. This has hampered the practical utility of NRSfM for many applications within vision. In this paper we propose a novel deep neural network to recover camera poses and 3D points solely from an ensemble of 2D image coordinates. The proposed neural network is mathematically interpretable as a multi-layer block sparse dictionary learning problem, and can handle problems of unprecedented scale and shape complexity. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in the order of magnitude. We further propose a quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstruction."
Deep Optics for Monocular Depth Estimation and 3D Object Detection,"Julie Chang, Gordon Wetzstein",Stanford University,100.0,usa,0.0,,"Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010976/,"['Lenses', 'Estimation', 'Three-dimensional displays', 'Optical imaging', 'Cameras', 'Object detection']","['Object Detection', 'Depth Estimation', '3D Object Detection', 'Monocular Depth Estimation', 'Monocular 3D Object Detection', 'Neural Network', 'Deep Neural Network', 'Defocus', 'Image Capture', 'Depth Perception', 'Real-world Outcomes', 'Optical Design', 'Object Detection Network', 'KITTI Dataset', 'Lens Design', 'Convolutional Neural Network', 'Super-resolution', 'Point Cloud', 'Bounding Box', 'Focal Plane', 'Point Spread Function', 'Optical Elements', 'Depth Map', 'Optical Processing', 'Optical Layer', 'Single Lens', 'Optical Model', 'Depth Information', 'Focal Length', 'Astigmatism']",,107,"Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance."
Deep Parametric Indoor Lighting Estimation,"Marc-AndrÃ© Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian GagnÃ©, Jean-FranÃ§ois Lalonde",Adobe Research; Université Laval,50.0,canada,50.0,USA,"We present a method to estimate lighting from a single image of an indoor scene. Previous work has used an environment map representation that does not account for the localized nature of indoor lighting. Instead, we represent lighting as a set of discrete 3D lights with geometric and photometric parameters. We train a deep neural network to regress these parameters from a single image, on a dataset of environment maps annotated with depth. We propose a differentiable layer to convert these parameters to an environment map to compute our loss; this bypasses the challenge of establishing correspondences between estimated and ground truth lights. We demonstrate, via quantitative and qualitative evaluations, that our representation and training scheme lead to more accurate results compared to previous work, while allowing for more realistic 3D object compositing with spatially-varying lighting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gardner_Deep_Parametric_Indoor_Lighting_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gardner_Deep_Parametric_Indoor_Lighting_Estimation_ICCV_2019_paper.pdf,https://lvsn.github.io/deepparametric/,https://github.com/lvsn/deepparametric,,main,Poster,https://ieeexplore.ieee.org/document/9010254/,"['Lighting', 'Three-dimensional displays', 'Light sources', 'Training', 'Geometry', 'Estimation', 'Neural networks']","['Neural Network', 'Deep Network', 'Deep Neural Network', 'Single Image', 'Environment Map', 'Light Source', 'Light Intensity', 'Input Image', 'User Study', '3D Reconstruction', 'Parametrized', 'Ability Of Method', 'Training Step', 'Depth Estimation', 'Insertion Point', 'Latent Vector', 'Angular Distance', 'Virtual Objects', 'Angular Size', 'L2 Loss', 'Light Position', 'Low Dynamic Range', 'Scene Geometry', 'Scene Point', 'Direct Light', 'Intensity Of The Light Source', 'Model Parameters']",,84,"We present a method to estimate lighting from a single image of an indoor scene. Previous work has used an environment map representation that does not account for the localized nature of indoor lighting. Instead, we represent lighting as a set of discrete 3D lights with geometric and photometric parameters. We train a deep neural network to regress these parameters from a single image, on a dataset of environment maps annotated with depth. We propose a differentiable layer to convert these parameters to an environment map to compute our loss; this bypasses the challenge of establishing correspondences between estimated and ground truth lights. We demonstrate, via quantitative and qualitative evaluations, that our representation and training scheme lead to more accurate results compared to previous work, while allowing for more realistic 3D object compositing with spatially-varying lighting."
Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification,"Zimo Liu, Jingya Wang, Shaogang Gong, Huchuan Lu, Dacheng Tao","Dalian University of Technology; Queen Mary University of London; UBTECH Sydney AI Center, The University of Sydney",100.0,"australia, china, uk",0.0,,"Most existing person re-identification(Re-ID) approaches achieve superior results based on the assumption that a large amount of pre-labelled data is usually available and can be put into training phrase all at once. However, this assumption is not applicable to most real-world deployment of the Re-ID task. In this work, we propose an alternative reinforcement learning based human-in-the-loop model which releases the restriction of pre-labelling and keeps model upgrading with progressively collected data. The goal is to minimize human annotation efforts while maximizing Re-ID performance. It works in an iteratively updating framework by refining the RL policy and CNN parameters alternately. In particular, we formulate a Deep Reinforcement Active Learning (DRAL) method to guide an agent (a model in a reinforcement learning process) in selecting training samples on-the-fly by a human user/annotator. The reinforcement learning reward is the uncertainty value of each human selected sample. A binary feedback (positive or negative) labelled by the human annotator is used to select the samples of which are used to fine-tune a pre-trained CNN Re-ID model. Extensive experiments demonstrate the superiority of our DRAL method for deep reinforcement learning based human-in-the-loop person Re-ID when compared to existing unsupervised and transfer learning models as well as active learning models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010038/,"['Learning (artificial intelligence)', 'Cameras', 'Training', 'Task analysis', 'Data models', 'Machine learning', 'Computational modeling']","['Deep Learning', 'Active Learning', 'Learning Models', 'Unsupervised Learning', 'Transfer Learning', 'Deep Reinforcement Learning', 'Transfer Learning Model', 'Active Learning Methods', 'Selected Training Samples', 'Reinforcement Learning Process', 'Reinforcement Learning Policy', 'Re-identification Task', 'Training Data', 'Learning Rate', 'Supervised Learning', 'Learning Framework', 'Pedestrian', 'Cross-entropy Loss', 'Annotation Data', 'Mahalanobis Distance', 'Triplet Loss', 'Sparse Graph', 'Pairwise Data', 'Active Learning Approach', 'Camera View', 'Iterative Scheme', 'Query Sample', 'Selective Labeling', 'State St', 'Policy Network']",,56,"Most existing person re-identification(Re-ID) approaches achieve superior results based on the assumption that a large amount of pre-labelled data is usually available and can be put into training phrase all at once. However, this assumption is not applicable to most real-world deployment of the Re-ID task. In this work, we propose an alternative reinforcement learning based human-in-the-loop model which releases the restriction of pre-labelling and keeps model upgrading with progressively collected data. The goal is to minimize human annotation efforts while maximizing Re-ID performance. It works in an iteratively updating framework by refining the RL policy and CNN parameters alternately. In particular, we formulate a Deep Reinforcement Active Learning (DRAL) method to guide an agent (a model in a reinforcement learning process) in selecting training samples on-the-fly by a human user/annotator. The reinforcement learning reward is the uncertainty value of each human selected sample. A binary feedback (positive or negative) labelled by the human annotator is used to select the samples of which are used to fine-tune a pre-trained CNN Re-ID model. Extensive experiments demonstrate the superiority of our DRAL method for deep reinforcement learning based human-in-the-loop person Re-ID when compared to existing unsupervised and transfer learning models as well as active learning models."
Deep Residual Learning in the JPEG Transform Domain,"Max Ehrlich, Larry S. Davis","University of Maryland, College Park, MD, USA",100.0,usa,0.0,,We introduce a general method of performing Residual Network inference and learning in the JPEG transform domain that allows the network to consume compressed images as input. Our formulation leverages the linearity of the JPEG transform to redefine convolution and batch normalization with a tune-able numerical approximation for ReLu. The result is mathematically equivalent to the spatial domain network up to the ReLu approximation accuracy. A formulation for image classification and a model conversion algorithm for spatial domain networks are given as examples of the method. We show that the sparsity of the JPEG format allows for faster processing of images with little to no penalty in the network accuracy.,,http://openaccess.thecvf.com/content_ICCV_2019/html/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010275/,"['Transform coding', 'Image coding', 'Discrete cosine transforms', 'Tensile stress', 'Convolution', 'Machine learning']","['Deep Learning', 'Residual Learning', 'Deep Residual Learning', 'Batch Normalization', 'Spatial Domain', 'Spatial Network', 'Image Compression', 'Machine Learning', 'Deep Network', 'Linear Function', 'Training Time', 'Simple Form', 'Spatial Model', 'Convolution Operation', 'Residual Block', 'Inference Time', 'Global Average Pooling', 'Multilinear', 'Human Visual System', 'Piecewise Linear Function', 'JPEG Compression', 'Batch Of Images', 'Entropy Coding', 'JPEG Images', 'Bilinear Map', 'ReLU Function']",,91,We introduce a general method of performing Residual Network inference and learning in the JPEG transform domain that allows the network to consume compressed images as input. Our formulation leverages the linearity of the JPEG transform to redefine convolution and batch normalization with a tune-able numerical approximation for ReLu. The result is mathematically equivalent to the spatial domain network up to the ReLu approximation accuracy. A formulation for image classification and a model conversion algorithm for spatial domain networks are given as examples of the method. We show that the sparsity of the JPEG format allows for faster processing of images with little to no penalty in the network accuracy.
Deep Restoration of Vintage Photographs From Scanned Halftone Prints,"Qifan Gao, Xiao Shu, Xiaolin Wu",McMaster University; Shanghai Jiao Tong University,100.0,"China, canada",0.0,,"A great number of invaluable historical photographs unfortunately only exist in the form of halftone prints in old publications such as newspapers or books. Their original continuous-tone films have long been lost or irreparably damaged. There have been attempts to digitally restore these vintage halftone prints to the original film quality or higher. However, even using powerful deep convolutional neural networks, it is still difficult to obtain satisfactory results. The main challenge is that the degradation process is complex and compounded while little to no real data is available for properly training a data-driven method. In this research, we adopt a novel strategy of two-stage deep learning, in which the restoration task is divided into two stages: the removal of printing artifacts and the inverse of halftoning. The advantage of our technique is that only the simple first stage requires unsupervised training in order to make the combined network generalize on real halftone prints, while the more complex second stage of inverse halftoning can be easily trained with synthetic data. Extensive experimental results demonstrate the efficacy of the proposed technique for real halftone prints; the new technique significantly outperforms the existing ones in visual quality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Deep_Restoration_of_Vintage_Photographs_From_Scanned_Halftone_Prints_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Deep_Restoration_of_Vintage_Photographs_From_Scanned_Halftone_Prints_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008392/,"['Image restoration', 'Training', 'Machine learning', 'Nickel', 'Task analysis', 'Training data', 'Ink']","['Paired Data', 'Deep Convolutional Neural Network', 'Visual Quality', 'Artifact Removal', 'Synthetic Training Data', 'Historical Photographs', 'Deep Learning', 'Objective Function', 'Supervised Learning', 'Unsupervised Learning', 'Receptive Field', 'Training Images', 'Frequency Components', 'Lookup Table', 'Inverse Method', 'Clear Image', 'Synthetic Images', 'Maximum A Posteriori', 'Component Of Image', 'High Frequency Components', 'Joint Training', 'Radial Basis Function Neural Network', 'Synthetic Counterparts', 'Fiber Paper', 'Smooth Regions', 'Artifacts In Regions', 'High Frequency Information', 'Original Authors']",,7,"A great number of invaluable historical photographs unfortunately only exist in the form of halftone prints in old publications such as newspapers or books. Their original continuous-tone films have long been lost or irreparably damaged. There have been attempts to digitally restore these vintage halftone prints to the original film quality or higher. However, even using powerful deep convolutional neural networks, it is still difficult to obtain satisfactory results. The main challenge is that the degradation process is complex and compounded while little to no real data is available for properly training a data-driven method. In this research, we adopt a novel strategy of two-stage deep learning, in which the restoration task is divided into two stages: the removal of printing artifacts and the inverse of halftoning. The advantage of our technique is that only the simple first stage requires unsupervised training in order to make the combined network generalize on real halftone prints, while the more complex second stage of inverse halftoning can be easily trained with synthetic data. Extensive experimental results demonstrate the efficacy of the proposed technique for real halftone prints; the new technique significantly outperforms the existing ones in visual quality."
Deep SR-ITM: Joint Learning of Super-Resolution and Inverse Tone-Mapping for 4K UHD HDR Applications,"Soo Ye Kim, Jihyong Oh, Munchurl Kim","Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea",100.0,south korea,0.0,,"Recent modern displays are now able to render high dynamic range (HDR), high resolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently, UHD HDR broadcasting and streaming have emerged as high quality premium services. However, due to the lack of original UHD HDR video content, appropriate conversion technologies are urgently needed to transform the legacy low resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions. In this paper, we propose a joint super-resolution (SR) and inverse tone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct mapping from LR SDR video to their HR HDR version. Joint SR and ITM is an intricate task, where high frequency details must be restored for SR, jointly with the local contrast, for ITM. Our network is able to restore fine details by decomposing the input image and focusing on the separate base (low frequency) and detail (high frequency) layers. Moreover, the proposed modulation blocks apply location-variant operations to enhance local contrast. The Deep SR-ITM shows good subjective quality with increased contrast and details, outperforming the previous joint SR-ITM method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Deep_SR-ITM_Joint_Learning_of_Super-Resolution_and_Inverse_Tone-Mapping_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Deep_SR-ITM_Joint_Learning_of_Super-Resolution_and_Inverse_Tone-Mapping_for_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008274/,"['Videos', 'Dynamic range', 'Image restoration', 'UHDTV', 'Broadcasting', 'Multimedia communication']","['High Dynamic Range', 'Ultra-high-definition', 'Dynamic Range', 'Input Image', 'Video Content', 'Base Layer', 'Local Contrast', 'Detail Layer', 'Deep Network', 'Convolutional Layers', 'Feature Maps', 'Real Conditions', 'Convolution Operation', 'Residual Block', 'Skip Connections', 'Element-wise Multiplication', 'Block Type', 'Direct Channel', 'Residual Learning', 'Saturation Region', 'High Dynamic Range Image', 'Guided Filter', 'Pixel Domain', 'High Frequency Information', 'Color Gamut', 'Attention Block', 'Display Format', 'L2 Loss', 'Bicubic', 'Image Signal']",,68,"Recent modern displays are now able to render high dynamic range (HDR), high resolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently, UHD HDR broadcasting and streaming have emerged as high quality premium services. However, due to the lack of original UHD HDR video content, appropriate conversion technologies are urgently needed to transform the legacy low resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions. In this paper, we propose a joint super-resolution (SR) and inverse tone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct mapping from LR SDR video to their HR HDR version. Joint SR and ITM is an intricate task, where high frequency details must be restored for SR, jointly with the local contrast, for ITM. Our network is able to restore fine details by decomposing the input image and focusing on the separate base (low frequency) and detail (high frequency) layers. Moreover, the proposed modulation blocks apply location-variant operations to enhance local contrast. The Deep SR-ITM shows good subjective quality with increased contrast and details, outperforming the previous joint SR-ITM method."
Deep Self-Learning From Noisy Labels,"Jiangfan Han, Ping Luo, Xiaogang Wang","The University of Hong Kong; CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong",100.0,"Hong Kong, china",0.0,,"ConvNets achieve good results when training from clean data, but learning from noisy labels significantly degrades performances and remains challenging. Unlike previous works constrained by many conditions, making them infeasible to real noisy cases, this work presents a novel deep self-learning framework to train a robust network on the real noisy datasets without extra supervision. The proposed approach has several appealing benefits. (1) Different from most existing work, it does not rely on any assumption on the distribution of the noisy labels, making it robust to real noises. (2) It does not need extra clean supervision or accessorial network to help training. (3) A self-learning framework is proposed to train the network in an iterative end-to-end manner, which is effective and efficient. Extensive experiments in challenging benchmarks such as Clothing1M and Food101-N show that our approach outperforms its counterparts in all empirical settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008525/,"['Noise measurement', 'Prototypes', 'Training', 'Robustness', 'Switched mode power supplies', 'Feature extraction', 'Optimization']","['Noisy Labels', 'Convolutional Neural Network', 'Loss Function', 'Training Set', 'Deep Neural Network', 'Training Phase', 'Network Training', 'Cross-entropy Loss', 'Training Procedure', 'Training Images', 'Similarity Matrix', 'Real-world Datasets', 'Classification Datasets', 'Deep Features', 'End Of Training', 'Correct Label', 'Original Label', 'Image X', 'Clean Dataset', 'Class Prototypes', 'Label Noise', 'Neural Network', 'Human Labeling', 'Transition Probabilities', 'Noisy Data', 'Deep Network', 'Iterative Framework', 'Accurate Correction', 'Number Of Images']",,172,"ConvNets achieve good results when training from clean data, but learning from noisy labels significantly degrades performances and remains challenging. Unlike previous works constrained by many conditions, making them infeasible to real noisy cases, this work presents a novel deep self-learning framework to train a robust network on the real noisy datasets without extra supervision. The proposed approach has several appealing benefits. (1) Different from most existing work, it does not rely on any assumption on the distribution of the noisy labels, making it robust to real noises. (2) It does not need extra clean supervision or accessorial network to help training. (3) A self-learning framework is proposed to train the network in an iterative end-to-end manner, which is effective and efficient. Extensive experiments in challenging benchmarks such as Clothing1M and Food101-N show that our approach outperforms its counterparts in all empirical settings."
Deep Single-Image Portrait Relighting,"Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, David W. Jacobs","Adobe Research; University of Maryland, College Park, MD, USA; University of Maryland, College Park, MD, USA / Amazon AWS; Amazon",50.0,usa,50.0,USA,"Conventional physically-based methods for relighting portrait images need to solve an inverse rendering problem, estimating face geometry, reflectance and lighting. However, the inaccurate estimation of face components can cause strong artifacts in relighting, leading to unsatisfactory results. In this work, we apply a physically-based portrait relighting method to generate a large scale, high quality, ""in the wild"" portrait relighting dataset (DPR). A deep Convolutional Neural Network (CNN) is then trained using this dataset to generate a relit portrait image by using a source image and a target lighting as input. The training procedure regularizes the generated results, removing the artifacts caused by physically-based relighting methods. A GAN loss is further applied to improve the quality of the relit portrait image. Our trained network can relight portrait images with resolutions as high as 1024 x 1024. We evaluate the proposed method on the proposed DPR datset, Flickr portrait dataset and Multi-PIE dataset both qualitatively and quantitatively. Our experiments demonstrate that the proposed method achieves state-of-the-art results. Please refer to https://zhhoper.github.io/dpr.html for dataset and code.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Deep_Single-Image_Portrait_Relighting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Deep_Single-Image_Portrait_Relighting_ICCV_2019_paper.pdf,https://zhhoper.github.io/dpr.html,https://github.com/zhhoper/dpr,,main,Poster,https://ieeexplore.ieee.org/document/9010718/,"['Lighting', 'Face', 'Image resolution', 'Geometry', 'Ear', 'Neck', 'Feature extraction']","['Portrait Relighting', 'Convolutional Neural Network', 'Network Training', '3D Reconstruction', 'Deep Convolutional Neural Network', 'Source Images', 'Fine-tuned', 'Light Conditions', 'Image Pixels', 'Image Dataset', 'Target Image', 'Reference Image', 'Face Images', 'Neck Region', 'Skip Connections', 'Normal Approximation', 'Deep Learning-based Methods', 'Ratio Images', 'Spherical Harmonics', 'L1 Loss', 'Landmark Detection', 'Triangular Mesh', 'Style Transfer', 'Bottleneck Layer', 'Decoder Part', 'Optimization-based Methods', 'Scale-invariant', 'Facial Features', 'High-resolution']",,133,"Conventional physically-based methods for relighting portrait images need to solve an inverse rendering problem, estimating face geometry, reflectance and lighting. However, the inaccurate estimation of face components can cause strong artifacts in relighting, leading to unsatisfactory results. In this work, we apply a physically-based portrait relighting method to generate a large scale, high quality, “in the wild” portrait relighting dataset (DPR). A deep Convolutional Neural Network (CNN) is then trained using this dataset to generate a relit portrait image by using a source image and a target lighting as input. The training procedure regularizes the generated results, removing the artifacts caused by physically-based relighting methods. A GAN loss is further applied to improve the quality of the relit portrait image. Our trained network can relight portrait images with resolutions as high as 1024 × 1024. We evaluate the proposed method on the proposed DPR datset, Flickr portrait dataset and Multi-PIE dataset both qualitatively and quantitatively. Our experiments demonstrate that the proposed method achieves state-of-the-art results. Please refer to https://zhhoper.github.io/dpr.html for dataset and code."
Deep Supervised Hashing With Anchor Graph,"Yudong Chen, Zhihui Lai, Yujuan Ding, Kaiyi Lin, Wai Keung Wong","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society, Shenzhen, China; School of Software and Microelectronics, Peking University, Beijing, China; Institute of Textiles and Clothing, The Hong Kong Polytechnic University, Hong Kong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University, Shenzhen, China",100.0,"China, Hong Kong, china",0.0,,"Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware's memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Deep_Supervised_Hashing_With_Anchor_Graph_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Deep_Supervised_Hashing_With_Anchor_Graph_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010953/,"['Binary codes', 'Training', 'Data models', 'Neural networks', 'Linear programming', 'Computational modeling', 'Training data']","['Anchor Graph', 'Deep Supervised Hashing', 'Training Set', 'Computational Cost', 'Training Time', 'Large-scale Datasets', 'Advanced Learning', 'Deep Features', 'Hash Function', 'Efficient Learning', 'Binary Code', 'Coding Efficiency', 'Retrieval Performance', 'Entire Training Set', 'Label Matrix', 'Neural Network', 'Training Data', 'Deep Learning', 'Deep Neural Network', 'Partial Differential', 'Performance Of Different Methods', 'Locality Sensitive Hashing', 'Mean Average Precision', 'Network Output', 'Learning Network', 'Regression Terms', 'Subset Of Samples', 'Software Quality', 'Affinity Matrix', 'Binary Space']",,33,"Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware's memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods."
Deep Tensor ADMM-Net for Snapshot Compressive Imaging,"Jiawei Ma, Xiao-Yang Liu, Zheng Shou, Xin Yuan",Nokia Bell Labs; Columbia University,50.0,usa,50.0,USA,"Snapshot compressive imaging (SCI) systems have been developed to capture high-dimensional (> 3) signals using low-dimensional off-the-shelf sensors, i.e., mapping multiple video frames into a single measurement frame. One key module of a SCI system is an accurate decoder that recovers the original video frames. However, existing model-based decoding algorithms require exhaustive parameter tuning with prior knowledge and cannot support practical applications due to the extremely long running time. In this paper, we propose a deep tensor ADMM-Net for video SCI systems that provides high-quality decoding in seconds. Firstly, we start with a standard tensor ADMM algorithm, unfold its inference iterations into a layer-wise structure, and design a deep neural network based on tensor operations. Secondly, instead of relying on a pre-specified sparse representation domain, the network learns the domain of low-rank tensor through stochastic gradient descent. It is worth noting that the proposed deep tensor ADMM-Net has potentially mathematical interpretations. On public video data, the simulation results show the proposed  method  achieves average 0.8 ~ 2.5 dB improvement in PSNR and 0.07 ~ 0.1 in SSIM, and 1500x~ 3600 xspeedups over the state-of-the-art methods. On real data captured by SCI cameras, the experimental results show comparable visual results with the state-of-the-art methods but in much shorter running time.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Deep_Tensor_ADMM-Net_for_Snapshot_Compressive_Imaging_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Deep_Tensor_ADMM-Net_for_Snapshot_Compressive_Imaging_ICCV_2019_paper.pdf,,https://github.com/Phoenix-V/tensor-admm-net-sci,,main,Poster,https://ieeexplore.ieee.org/document/9009497/,,"['Neural Network', 'Imaging System', 'Deep Network', 'Deep Neural Network', 'Running Time', 'Stochastic Gradient Descent', 'Video Frames', 'Video Data', 'Multiple Frames', 'Decoding Accuracy', 'Decoding Algorithm', 'Low-rank Tensor', 'Tensor Operations', 'Transformer', 'Simulated Data', 'Sparsity', 'Invertible', 'Network Training', 'Road Accidents', 'Transformation Matrix', 'Nuclear Norm', 'Linear Aggregation', 'Nuclear Norm Minimization', 'Gaussian Mixture Model', 'Similar Patches', 'Computer Image', 'Shrinkage Operator', 'Feed-forward Network', 'Norm Minimization', 'Linear Projection']",,110,"Snapshot compressive imaging (SCI) systems have been developed to capture high-dimensional (≥ 3) signals using low-dimensional off-the-shelf sensors, i.e., mapping multiple video frames into a single measurement frame. One key module of a SCI system is an accurate decoder that recovers the original video frames. However, existing model-based decoding algorithms require exhaustive parameter tuning with prior knowledge and cannot support practical applications due to the extremely long running time. In this paper, we propose a deep tensor ADMM-Net for video SCI systems that provides high-quality decoding in seconds. Firstly, we start with a standard tensor ADMM algorithm, unfold its inference iterations into a layer-wise structure, and design a deep neural network based on tensor operations. Secondly, instead of relying on a pre-specified sparse representation domain, the network learns the domain of low-rank tensor through stochastic gradient descent. It is worth noting that the proposed deep tensor ADMM-Net has potentially mathematical interpretations. On public video data, the simulation results show the proposed method achieves average 0.8 ~ 2.5 dB improvement in PSNR and 0.07 ~ 0.1 in SSIM, and 1500× ~ 3600× speedups over the state-of-the-art methods. On real data captured by SCI cameras, the experimental results show comparable visual results with the state-of-the-art methods but in much shorter running time."
DeepGCNs: Can GCNs Go As Deep As CNNs?,"Guohao Li, Matthias MÃ¼ller, Ali Thabet, Bernard Ghanem","Visual Computing Center, KAUST, Thuwal, Saudi Arabia",100.0,saudi arabia,0.0,,"Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNNs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNNs_ICCV_2019_paper.pdf,https://sites.google.com/view/deep-gcns,,,main,Oral,https://ieeexplore.ieee.org/document/9010334/,"['Three-dimensional displays', 'Semantics', 'Training', 'Task analysis', 'Computational modeling', 'Reliability', 'Stacking']","['Convolutional Neural Network', 'Graph Convolutional Network', 'Extensive Experiments', 'Point Cloud', 'Deep Convolutional Neural Network', 'Semantic Segmentation', 'Vanishing Gradient', 'Vanishing Gradient Problem', 'Dilated Convolution', 'Graph Convolution', 'Semantic Segmentation Task', 'Wide Variety Of Fields', 'Graph Convolutional Network Model', 'Graphical Representation', 'K-nearest Neighbor', 'Receptive Field', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Deep Architecture', 'Segmentation Task', 'Residual Connection', 'Dense Connections', 'Input Graph', 'Aggregation Function', 'Scene Graph', 'Weak Connections', 'Fusion Block', 'Loss Of Resolution', 'Point Cloud Segmentation', 'Update Function']",,762,"Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research."
DeepHuman: 3D Human Reconstruction From a Single Image,"Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu",Tsinghua University; Orbbec Company; Beihang University,66.66666666666666,"China, china",33.33333333333334,China,"We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D human reconstruction from a single RGB image. To reduce the ambiguities associated with the reconstruction of invisible areas, our method leverages a dense semantic representation generated from SMPL model as an additional input. One key feature of our network is that it fuses different scales of image features into the 3D space through volumetric feature transformation, which helps to recover accurate surface geometry. The surface details are further refined through a normal refinement network, which can be concatenated with the volume generation network using our proposed volumetric normal projection layer. We also contribute THuman, a 3D real-world human model dataset containing approximately 7000 models. The network is trained using training data generated from the dataset. Overall, due to the specific design of our network and the diversity in our dataset, our method enables 3D human model estimation given only a single image and outperforms state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010852/,"['Three-dimensional displays', 'Image reconstruction', 'Surface reconstruction', 'Two dimensional displays', 'Shape', 'Solid modeling', 'Cameras']","['Single Image', 'Human Reconstruction', '3D Human Reconstruction', 'Volumetric', 'Human Model', '3D Reconstruction', '3D Space', 'Characteristic Scale', 'Real-world Datasets', 'Feature Transformation', 'Surface Geometry', '3D Datasets', 'Detailed Surface', 'Dense Representation', 'Visible Surface', 'Features Of Different Scales', '3D Human Model', 'Projection Layer', 'Input Image', 'Depth Map', 'Multi-scale Features', 'Normal Map', 'Semantic Map', '3D Volume', '3D Skeleton', '3D Voxel', 'Latent Code', 'RGB Camera', 'Geometric Details', 'Person Image']",,247,"We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D human reconstruction from a single RGB image. To reduce the ambiguities associated with the reconstruction of invisible areas, our method leverages a dense semantic representation generated from SMPL model as an additional input. One key feature of our network is that it fuses different scales of image features into the 3D space through volumetric feature transformation, which helps to recover accurate surface geometry. The surface details are further refined through a normal refinement network, which can be concatenated with the volume generation network using our proposed volumetric normal projection layer. We also contribute THuman, a 3D real-world human model dataset containing approximately 7000 models. The network is trained using training data generated from the dataset. Overall, due to the specific design of our network and the diversity in our dataset, our method enables 3D human model estimation given only a single image and outperforms state-of-the-art approaches."
DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch,"Shivam Duggal, Shenlong Wang, Wei-Chiu Ma, Rui Hu, Raquel Urtasun","Uber ATG; Uber ATG, University of Toronto; Uber ATG, Massachusetts Institute of Technology",66.66666666666666,"Canada, usa",33.33333333333334,USA,"Our goal is to significantly speed up the runtime of current state-of-the-art stereo algorithms to enable real-time inference. Towards this goal, we developed a differentiable PatchMatch module that allows us to discard most disparities without requiring full cost volume evaluation. We then exploit this representation to learn which range to prune for each pixel. By progressively reducing the search space and effectively propagating such information, we are able to efficiently compute the cost volume for high likelihood hypotheses and achieve savings in both memory and computation.Finally, an image guided refinement module is exploited to further improve the performance. Since all our components are differentiable, the full network can be trained end-to-end. Our experiments show that our method achieves competitive results on KITTI and SceneFlow datasets while running in real-time at 62ms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Duggal_DeepPruner_Learning_Efficient_Stereo_Matching_via_Differentiable_PatchMatch_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Duggal_DeepPruner_Learning_Efficient_Stereo_Matching_via_Differentiable_PatchMatch_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009458/,"['Real-time systems', 'Feature extraction', 'Estimation', 'Memory management', 'Inference algorithms', 'Robustness', 'Computational modeling']","['Stereo Matching', 'Differentiable PatchMatch', 'Search Space', 'KITTI Dataset', 'Cost Volume', 'Neural Network', 'Convolution', 'Upper Bound', 'Image Pairs', 'Solution Space', 'Neural Network Layers', 'Memory Consumption', 'Adjacent Pixels', 'Real-time Model', 'Stereo Images', 'Confidence Region', 'Stereo Pairs', 'Disparity Values', 'Cost Aggregation', 'Confidence Range', 'Stereo Image Pairs', 'Disparity Estimation', 'Real-time Inference', 'Disparity Map', 'Matching Score', 'Left Image', 'Residual Block', 'Deep Learning', 'Real-time Method']",,182,"Our goal is to significantly speed up the runtime of current state-of-the-art stereo algorithms to enable real-time inference. Towards this goal, we developed a differentiable PatchMatch module that allows us to discard most disparities without requiring full cost volume evaluation. We then exploit this representation to learn which range to prune for each pixel. By progressively reducing the search space and effectively propagating such information, we are able to efficiently compute the cost volume for high likelihood hypotheses and achieve savings in both memory and computation.Finally, an image guided refinement module is exploited to further improve the performance. Since all our components are differentiable, the full network can be trained end-to-end. Our experiments show that our method achieves competitive results on KITTI and SceneFlow datasets while running in real-time at 62ms."
DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration,"Weixin Lu, Guowei Wan, Yao Zhou, Xiangyu Fu, Pengfei Yuan, Shiyu Song",Baidu Autonomous Driving Technology Department (ADT),100.0,China,0.0,,"We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009450/,"['Three-dimensional displays', 'Feature extraction', 'Task analysis', 'Laser radar', 'Iron', 'Estimation', 'Iterative closest point algorithm']","['Deep Neural Network', 'Point Cloud', 'Point Cloud Registration', 'Deep Network', 'Comparable Accuracy', 'Geometric Method', 'Static Objects', 'Registration Accuracy', 'Dynamic Objects', 'Matching Probability', 'KITTI Dataset', 'Keypoint Detection', 'Descriptive Characteristics', 'Similar Position', 'Multilayer Perceptron', 'Singular Value Decomposition', 'Point Source', 'Local Coordinate', 'Light Detection And Ranging', 'Euclidean Space', 'Feature Extraction Layer', 'Angular Error', 'Corresponding Points', 'Translation Error', 'Original Point Cloud', 'Manhattan Distance', 'Ground Truth Pose', 'Tree Trunks', 'Softmax Operation']",,195,"We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task."
Defending Against Universal Perturbations With Shared Adversarial Training,"Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen","University of Freiburg; University of Freiburg, Bosch Center for Artiﬁcial Intelligence, Germany; Bosch Center for Artiﬁcial Intelligence, Germany",66.66666666666666,germany,33.33333333333334,Germany,"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mummadi_Defending_Against_Universal_Perturbations_With_Shared_Adversarial_Training_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mummadi_Defending_Against_Universal_Perturbations_With_Shared_Adversarial_Training_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010642/,"['Perturbation methods', 'Training', 'Robustness', 'Semantics', 'Image segmentation', 'Task analysis', 'Computational modeling']","['Adversarial Training', 'Universal Perturbation', 'Deep Neural Network', 'Image Classification', 'Semantic Segmentation', 'Extensive Training', 'Target Scene', 'Adversarial Perturbations', 'Defense Mechanisms', 'Upper Bound', 'Supplementary Materials For Details', 'Physical World', 'Standard Training', 'Decision Boundary', 'Image Classification Tasks', 'Misclassification Rate', 'Threat Model', 'Binary Search', 'Semantic Segmentation Task', 'Target Segment', 'Projected Gradient Descent', 'Adversarial Examples', 'Empirical Risk Minimization', 'Categorical Cross-entropy', 'Perturbation Magnitude', 'Set Of Perturbations', 'Tight Upper Bound']",,32,"Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene."
Deformable Surface Tracking by Graph Matching,"Tao Wang, Haibin Ling, Congyan Lang, Songhe Feng, Xiaohui Hou","Beijing Jiaotong University, Beijing 100044, China; HiScene Information Technologies, Shanghai 201210, China; Stony Brook University, Stony Brook, NY 11794, USA",66.66666666666666,"china, usa",33.33333333333334,China,"This paper addresses the problem of deformable surface tracking from monocular images. Specifically, we propose a graph-based approach that effectively explores the structure information of the surface to enhance tracking performance. Our approach solves simultaneously for feature correspondence, outlier rejection and shape reconstruction by optimizing a single objective function, which is defined by means of pairwise projection errors between graph structures instead of unary projection errors between matched points. Furthermore, an efficient matching algorithm is developed based on soft matching relaxation. For evaluation, our approach is extensively compared to state-of-the-art algorithms on a standard dataset of occluded surfaces, as well as a newly compiled dataset of different surfaces with rich, weak or repetitive texture. Experimental results reveal that our approach achieves robust tracking results for surfaces with different types of texture, and outperforms other algorithms in both accuracy and efficiency.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deformable_Surface_Tracking_by_Graph_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deformable_Surface_Tracking_by_Graph_Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010840/,"['Shape', 'Three-dimensional displays', 'Image reconstruction', 'Strain', 'Surface reconstruction', 'Two dimensional displays', 'Surface texture']","['Surface Deformation', 'Graph Matching', 'Track Surface', 'Structural Information', 'Graph Structure', 'Corresponding Points', 'Monocular', 'Tracking Performance', 'Matching Algorithm', 'Tracking Results', 'Shape Reconstruction', 'Outlier Rejection', 'Projection Error', 'Computation Time', 'Input Image', 'Reference Image', 'Surface Type', 'Feature Points', 'Tracking Error', '3D Shape', 'Candidate Matches', 'Presence Of Occlusion', 'Accurate Shape', 'Different Types Of Surfaces', 'Baseline Algorithms', 'Affinity Matrix', 'Illumination Changes', 'Deformation Model', 'Geodesic Distance', 'Motion Blur']",,8,"This paper addresses the problem of deformable surface tracking from monocular images. Specifically, we propose a graph-based approach that effectively explores the structure information of the surface to enhance tracking performance. Our approach solves simultaneously for feature correspondence, outlier rejection and shape reconstruction by optimizing a single objective function, which is defined by means of pairwise projection errors between graph structures instead of unary projection errors between matched points. Furthermore, an efficient matching algorithm is developed based on soft matching relaxation. For evaluation, our approach is extensively compared to state-of-the-art algorithms on a standard dataset of occluded surfaces, as well as a newly compiled dataset of different surfaces with rich, weak or repetitive texture. Experimental results reveal that our approach achieves robust tracking results for surfaces with different types of texture, and outperforms other algorithms in both accuracy and efficiency."
Delving Deep Into Hybrid Annotations for 3D Human Recovery in the Wild,"Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, Chen Change Loy","Stanford University; Nanyang Technological University; CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong; SenseTime Research",75.0,"Hong Kong, Singapore, china, usa",25.0,China,"Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available. Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, the model exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rong_Delving_Deep_Into_Hybrid_Annotations_for_3D_Human_Recovery_in_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rong_Delving_Deep_Into_Hybrid_Annotations_for_3D_Human_Recovery_in_ICCV_2019_paper.pdf,https://penincillin.github.io/dct_iccv2019,,,main,Poster,https://ieeexplore.ieee.org/document/9009023/,"['Three-dimensional displays', 'Solid modeling', 'Two dimensional displays', 'Shape', 'Labeling', 'Training', 'Image segmentation']","['Body Parts', 'Paired Data', 'Cost Trade-off', 'Dense Correspondence', '2D Keypoints', 'Model Performance', 'Parameter Estimates', 'Deep Network', 'Training Phase', 'Image Dataset', 'Shape Parameter', 'Deep Convolutional Neural Network', 'RGB Images', 'Pose Estimation', 'Single Branch', 'COCO Dataset', 'Labeling Density', 'Wrong Predictions', 'Optimization-based Methods', 'Annotated Training', 'Pose Parameters', '3D Human Model', '3D Joint', '3D Pose', 'Input Encoding', '3D Mesh', 'Supplemental Material']",,44,"Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available. Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, the model exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research."
Delving Into Robust Object Detection From Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach,"Zhenyu Wu, Karthik Suresh, Priya Narayanan, Hongyu Xu, Heesung Kwon, Zhangyang Wang",U.S. Army Research Laboratory; Texas A&M University; University of Maryland,66.66666666666666,usa,33.33333333333334,USA,"Object detection from images captured by Unmanned Aerial Vehicles (UAVs) is becoming increasingly useful. Despite the great success of the generic object detection methods trained on ground-to-ground images, a huge performance drop is observed when they are directly applied to images captured by UAVs. The unsatisfactory performance is owing to many UAV-specific nuisances, such as varying flying altitudes, adverse weather conditions, dynamically changing viewing angles, etc. Those nuisances constitute a large number of fine-grained domains, across which the detection model has to stay robust. Fortunately, UAVs will record meta-data that depict those varying attributes, which are either freely available along with the UAV images, or can be easily obtained. We propose to utilize those free meta-data in conjunction with associated UAV images to learn domain-robust features via an adversarial training framework dubbed Nuisance Disentangled Feature Transform (NDFT), for the specific challenging problem of object detection in UAV images, achieving a substantial gain in robustness to those nuisances. We demonstrate the effectiveness of our proposed algorithm, by showing state-of-the- art performance (single model) on two existing UAV-based object detection benchmarks. The code is available at https://github.com/TAMU-VITA/UAV-NDFT.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Delving_Into_Robust_Object_Detection_From_Unmanned_Aerial_Vehicles_A_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Delving_Into_Robust_Object_Detection_From_Unmanned_Aerial_Vehicles_A_ICCV_2019_paper.pdf,,https://github.com/TAMU-VITA/UAV-NDFT,,main,Poster,https://ieeexplore.ieee.org/document/9008566/,"['Object detection', 'Task analysis', 'Robustness', 'Meteorology', 'Unmanned aerial vehicles', 'Feature extraction', 'Detectors']","['Object Detection', 'Nuisance', 'Unmanned Aerial Vehicles', 'Robust Detection', 'Robust Object Detection', 'Weather', 'Detection Model', 'Viewing Angle', 'Number Of Domains', 'Adversarial Training', 'Object Detection Problem', 'Publicly Accessible', 'Data Augmentation', 'Transfer Learning', 'Bounding Box', 'Aerial Images', 'Latent Space', 'Target Domain', 'Domain Adaptation', 'Multi-task Learning', 'Source Domain', 'Unseen Domains', 'Object Scale', 'Object Detection Model', 'Camera View', 'Object Detection Task', 'Vehicle Category', 'Public Benchmark', 'Adversarial Domain Adaptation', 'Multi-object Tracking']",,59,"Object detection from images captured by Unmanned Aerial Vehicles (UAVs) is becoming increasingly useful. Despite the great success of the generic object detection methods trained on ground-to-ground images, a huge performance drop is observed when they are directly applied to images captured by UAVs. The unsatisfactory performance is owing to many UAV-specific nuisances, such as varying flying altitudes, adverse weather conditions, dynamically changing viewing angles, etc. Those nuisances constitute a large number of fine-grained domains, across which the detection model has to stay robust. Fortunately, UAVs will record meta-data that depict those varying attributes, which are either freely available along with the UAV images, or can be easily obtained. We propose to utilize those free meta-data in conjunction with associated UAV images to learn domain-robust features via an adversarial training framework dubbed Nuisance Disentangled Feature Transform (NDFT), for the specific challenging problem of object detection in UAV images, achieving a substantial gain in robustness to those nuisances. We demonstrate the effectiveness of our proposed algorithm, by showing state-of-the- art performance (single model) on two existing UAV-based object detection benchmarks. The code is available at https://github.com/TAMU-VITA/UAV-NDFT."
DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing,"Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, Chunhong Pan","Department of Automation, Tsinghua University; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",100.0,"China, china",0.0,,"Point cloud processing is very challenging, as the diverse shapes formed by irregular points are often indistinguishable. A thorough grasp of the elusive shape requires sufficiently contextual semantic information, yet few works devote to this. Here we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing. Technically, it extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and achieves efficient inductive learning of local patterns. Architecturally, it finds inspiration from dense connection mode, to repeatedly aggregate multi-level and multi-scale semantics in a deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, verify DensePoint achieves the state of the arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_DensePoint_Learning_Densely_Contextual_Representation_for_Efficient_Point_Cloud_Processing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_DensePoint_Learning_Densely_Contextual_Representation_for_Efficient_Point_Cloud_Processing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008249/,"['Three-dimensional displays', 'Shape', 'Semantics', 'Convolution', 'Computer architecture', 'Complexity theory', 'Aggregates']","['Point Cloud', 'Point Cloud Processing', 'Convolutional Neural Network', 'Contextual Information', 'State Of The Art', 'Local Patterns', 'Shape Variation', 'Regular Grid', 'Efficient Learning', 'General Architecture', 'Learning Patterns', 'Semantic Context', 'Density Modulation', 'Complex Models', 'Output Layer', 'Deeper Layers', 'Random Noise', 'Max-pooling', '3D Point', '3D Shape', 'Convolutional Neural Network Classifier', 'FC Layer', 'Shape Recognition', 'Input Point Cloud', 'Shape Context', 'Point-based Methods', 'Aggregation Function', 'Symmetric Function', 'Normal Approximation', 'Subset Of Locations']",,177,"Point cloud processing is very challenging, as the diverse shapes formed by irregular points are often indistinguishable. A thorough grasp of the elusive shape requires sufficiently contextual semantic information, yet few works devote to this. Here we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing. Technically, it extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and achieves efficient inductive learning of local patterns. Architecturally, it finds inspiration from dense connection mode, to repeatedly aggregate multi-level and multi-scale semantics in a deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, verify DensePoint achieves the state of the arts."
DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare,"Yuanlu Xu, Song-Chun Zhu, Tony Tung","University of California, Los Angeles, USA; Facebook Reality Labs, Sausalito, USA",50.0,usa,50.0,USA,"We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various human-related tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_DenseRaC_Joint_3D_Pose_and_Shape_Estimation_by_Dense_Render-and-Compare_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_DenseRaC_Joint_3D_Pose_and_Shape_Estimation_by_Dense_Render-and-Compare_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010054/,"['Three-dimensional displays', 'Shape', 'Solid modeling', 'Biological system modeling', 'Estimation', 'Cameras', 'Training']","['3D Shape', 'Pose Estimation', 'Shape Estimation', 'Human Pose Estimation', '3D Pose', '3D Shape Estimation', 'Neural Network', 'Human Activities', 'Body Parts', 'State Of The Art', 'Body Shape', 'RGB Images', '3D Scanning', 'Camera View', 'Joint Learning', 'Human Pose', 'Public Benchmark', '3D Body', 'Human 3D', 'Human Body Shape', '3D Mesh', '3D Reconstruction', '3D Animation', 'Semantic Segmentation', 'Body Model', 'Parametrized', 'Pose Parameters', 'Human Body Model', 'Trained Subjects', 'Body Segments']",,128,"We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various human-related tasks."
Depth Completion From Sparse LiDAR Data With Depth-Normal Constraints,"Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, Hongsheng Li","The Chinese University of Hong Kong; State Key Lab of CAD&CG, Zhejiang University; SenseTime Research",66.66666666666666,"China, Hong Kong",33.33333333333334,China,"Depth completion aims to recover dense depth maps from sparse depth measurements. It is of increasing importance for autonomous driving and draws increasing attention from the vision community. Most of the current competitive methods directly train a network to learn a mapping from sparse depth inputs to dense depth maps, which has difficulties in utilizing the 3D geometric constraints and handling the practical sensor noises. In this paper, to regularize the depth completion and improve the robustness against noise, we propose a unified CNN framework that 1) models the geometric constraints between depth and surface normal in a diffusion module and 2) predicts the confidence of sparse LiDAR measurements to mitigate the impact of noise. Specifically, our encoder-decoder backbone predicts the surface normal, coarse depth and confidence of LiDAR inputs simultaneously, which are subsequently inputted into our diffusion refinement module to obtain the final completion results. Extensive experiments on KITTI depth completion dataset and NYU-Depth-V2 dataset demonstrate that our method achieves state-of-the-art performance. Further ablation study and analysis give more insights into the proposed components and demonstrate the generalization capability and stability of our model.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Depth_Completion_From_Sparse_LiDAR_Data_With_Depth-Normal_Constraints_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Depth_Completion_From_Sparse_LiDAR_Data_With_Depth-Normal_Constraints_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010832/,"['Three-dimensional displays', 'Laser radar', 'Noise measurement', 'Decoding', 'Color', 'Cameras', 'Surface treatment']","['Depth Completion', 'Sparse LiDAR', 'Sparse LiDAR Data', 'Convolutional Neural Network', 'Extensive Experiments', 'Density Map', 'Depth Map', 'Depth Measurements', 'Geometric Constraints', 'Sensor Noise', 'Convolutional Neural Network Framework', 'Sparse Measurements', 'Surface Normals', 'Lidar Measurements', 'Dense Depth', 'Root Mean Square Error', 'Generalization Ability', 'Linear System', 'Mean Absolute Error', 'Step Function', 'Refinement Network', 'Sparse Input', 'Prediction Network', 'Anisotropic Diffusion', 'Confidence Map', 'Color Images', 'Diffusion Model', 'Depth Estimation', 'Prediction Confidence', '3D Point']",,147,"Depth completion aims to recover dense depth maps from sparse depth measurements. It is of increasing importance for autonomous driving and draws increasing attention from the vision community. Most of the current competitive methods directly train a network to learn a mapping from sparse depth inputs to dense depth maps, which has difficulties in utilizing the 3D geometric constraints and handling the practical sensor noises. In this paper, to regularize the depth completion and improve the robustness against noise, we propose a unified CNN framework that 1) models the geometric constraints between depth and surface normal in a diffusion module and 2) predicts the confidence of sparse LiDAR measurements to mitigate the impact of noise. Specifically, our encoder-decoder backbone predicts the surface normal, coarse depth and confidence of LiDAR inputs simultaneously, which are subsequently inputted into our diffusion refinement module to obtain the final completion results. Extensive experiments on KITTI depth completion dataset and NYU-Depth-V2 dataset demonstrate that our method achieves state-of-the-art performance. Further ablation study and analysis give more insights into the proposed components and demonstrate the generalization capability and stability of our model."
Depth From Videos in the Wild: Unsupervised Monocular Depth Learning From Unknown Cameras,"Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova",Google AI; Robotics at Google,0.0,,100.0,USA,"We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos. The code will be open sourced once anonymity is lifted.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.pdf,,https://github.com/google-research/google-research/tree/master/depth%20from%20video%20inthewild,,main,Poster,https://ieeexplore.ieee.org/document/9010905/,"['Cameras', 'Videos', 'Training', 'Silicon carbide', 'Distortion', 'Robot sensing systems', 'Computer vision']","['Unsupervised Learning', 'State Of The Art', 'Video Frames', 'Depth Map', 'Image Distortion', 'Object Motion', 'Unsupervised Manner', 'Intrinsic Parameters', 'KITTI Dataset', 'Depth Prediction', 'Supervision Signal', 'Camera Intrinsic Parameters', 'Micro Air Vehicles', 'Camera Intrinsics', 'Training Set', 'Deep Network', 'Single Image', 'Bounding Box', 'Focal Length', 'Depth Estimation', 'Camera Motion', 'Motion Field', 'Target Frame', 'Instance Segmentation', 'Consistency Loss', 'Optical Flow', 'Raw Video', 'Motion Estimation', 'Depth Camera']",,240,"We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos. The code will be open sourced once anonymity is lifted."
Depth-Induced Multi-Scale Recurrent Attention Network for Saliency Detection,"Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, Huchuan Lu","Dalian University of Technology, China",100.0,china,0.0,,"In this work, we propose a novel depth-induced multi-scale recurrent attention network for saliency detection. It achieves dramatic performance especially in complex scenarios. There are three main contributions of our network that are experimentally demonstrated to have significant practical merits. First, we design an effective depth refinement block using residual connections to fully extract and fuse multi-level paired complementary cues from RGB and depth streams. Second, depth cues with abundant spatial information are innovatively combined with multi-scale context features for accurately locating salient objects. Third, we boost our model's performance by a novel recurrent attention module inspired by Internal Generative Mechanism of human brain. This module can generate more accurate saliency results via comprehensively learning the internal semantic relation of the fused feature and progressively optimizing local details with memory-oriented scene understanding. In addition, we create a large scale RGB-D dataset containing more complex scenarios, which can contribute to comprehensively evaluating saliency models. Extensive experiments on six public datasets and ours demonstrate that our method can accurately identify salient objects and achieve consistently superior performance over 16 state-of-the-art RGB and RGB-D approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Piao_Depth-Induced_Multi-Scale_Recurrent_Attention_Network_for_Saliency_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Piao_Depth-Induced_Multi-Scale_Recurrent_Attention_Network_for_Saliency_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010728/,"['Random access memory', 'Feature extraction', 'Semantics', 'Saliency detection', 'Fuses', 'Frequency modulation', 'Task analysis']","['Multi-scale Network', 'Saliency Detection', 'Recurrent Attention', 'Complex Scenarios', 'Multi-scale Features', 'Local Details', 'Internal Relationships', 'Depth Perception', 'Salient Object', 'Saliency Models', 'Convolutional Layers', 'Feature Information', 'Convolution Operation', 'Pooling Layer', 'Multiple Objects', 'Softmax Function', 'Handcrafted Features', 'Depth Information', 'Precision-recall Curve', 'Salient Object Detection', 'Weight Space', 'Saliency Map', 'RGB Features', 'Fusion Strategy', 'Salient Regions', 'Ablation Analysis', 'Depth Features', 'Attention Block', 'Complex Background']",,312,"In this work, we propose a novel depth-induced multi-scale recurrent attention network for saliency detection. It achieves dramatic performance especially in complex scenarios. There are three main contributions of our network that are experimentally demonstrated to have significant practical merits. First, we design an effective depth refinement block using residual connections to fully extract and fuse multi-level paired complementary cues from RGB and depth streams. Second, depth cues with abundant spatial information are innovatively combined with multi-scale context features for accurately locating salient objects. Third, we boost our model's performance by a novel recurrent attention module inspired by Internal Generative Mechanism of human brain. This module can generate more accurate saliency results via comprehensively learning the internal semantic relation of the fused feature and progressively optimizing local details with memory-oriented scene understanding. In addition, we create a large scale RGB-D dataset containing more complex scenarios, which can contribute to comprehensively evaluating saliency models. Extensive experiments on six public datasets and ours demonstrate that our method can accurately identify salient objects and achieve consistently superior performance over 16 state-of-the-art RGB and RGB-D approaches."
Detecting 11K Classes: Large Scale Object Detection Without Fine-Grained Bounding Boxes,"Hao Yang, Hao Wu, Hao Chen",Amazon Web Services,0.0,,100.0,USA,"Recent advances in deep learning greatly boost the performance of object detection. State-of-the-art methods such as Faster-RCNN, FPN and R-FCN have achieved high accuracy in challenging benchmark datasets. However, these methods require fully annotated object bounding boxes for training, which are incredibly hard to scale up due to the high annotation cost. Weakly-supervised methods, on the other hand, only require image-level labels for training, but the performance is far below their fully-supervised counterparts. In this paper, we propose a semi-supervised large scale fine-grained detection method, which only needs bounding box annotations of a smaller number of coarse-grained classes and image-level labels of large scale fine-grained classes, and can detect all classes at nearly fully-supervised accuracy. We achieve this by utilizing the correlations between coarse-grained and fine-grained classes with shared backbone, soft-attention based proposal re-ranking, and a dual-level memory module. Experiment results show that our methods can achieve close accuracy on object detection to state-of-the-art fully-supervised methods on two large scale datasets, ImageNet and OpenImages, with only a small fraction of fully annotated classes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Detecting_11K_Classes_Large_Scale_Object_Detection_Without_Fine-Grained_Bounding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Detecting_11K_Classes_Large_Scale_Object_Detection_Without_Fine-Grained_Bounding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009848/,"['Correlation', 'Proposals', 'Object detection', 'Visualization', 'Detectors', 'Semantics', 'Training']","['Object Detection', 'Bounding Box', 'Large-scale Object', 'Large-scale Object Detection', 'Deep Learning', 'Detection Methods', 'Large-scale Datasets', 'Semi-supervised Methods', 'Memory Module', 'Bounding Box Annotations', 'Image-level Labels', 'Intersection Over Union', 'Average Pooling', 'Object Relations', 'Semi-supervised Learning', 'ImageNet Dataset', 'Attention Map', 'Region Proposal Network', 'Visual Similarity', 'Chihuahua', 'Fine-grained Data', 'Semantic Correlation', 'Two-stage Detectors', 'Background Class', 'Fine-grained Image', 'Multi-task Training', 'WordNet', 'One-stage Methods', 'Feature Pooling', 'Fine-grained Level']",,12,"Recent advances in deep learning greatly boost the performance of object detection. State-of-the-art methods such as Faster-RCNN, FPN and R-FCN have achieved high accuracy in challenging benchmark datasets. However, these methods require fully annotated object bounding boxes for training, which are incredibly hard to scale up due to the high annotation cost. Weakly-supervised methods, on the other hand, only require image-level labels for training, but the performance is far below their fully-supervised counterparts. In this paper, we propose a semi-supervised large scale fine-grained detection method, which only needs bounding box annotations of a smaller number of coarse-grained classes and image-level labels of large scale fine-grained classes, and can detect all classes at nearly fully-supervised accuracy. We achieve this by utilizing the correlations between coarse-grained and fine-grained classes with shared backbone, soft-attention based proposal re-ranking, and a dual-level memory module. Experiment results show that our methods can achieve close accuracy on object detection to state-of-the-art fully-supervised methods on two large scale datasets, ImageNet and OpenImages, with only a small fraction of fully annotated classes."
Detecting Photoshopped Faces by Scripting Photoshop,"Sheng-Yu Wang, Oliver Wang, Andrew Owens, Richard Zhang, Alexei A. Efros",Adobe Research; UC Berkeley,50.0,usa,50.0,USA,"Most malicious photo manipulations are created using standard image editing tools, such as Adobe Photoshop. We present a method for detecting one very popular Photoshop manipulation -- image warping applied to human faces -- using a model trained entirely using fake images that were automatically generated by scripting Photoshop itself. We show that our model outperforms humans at the task of recognizing manipulated images, can predict the specific location of edits, and in some cases can be used to ""undo"" a manipulation to reconstruct the original, unedited image. We demonstrate that the system can be successfully applied to artist-created image manipulations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Detecting_Photoshopped_Faces_by_Scripting_Photoshop_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Detecting_Photoshopped_Faces_by_Scripting_Photoshop_ICCV_2019_paper.pdf,,https://peterwang512.github.io/FALdetector,,main,Poster,https://ieeexplore.ieee.org/document/9010719/,"['Tools', 'Training', 'Flickr', 'Forensics', 'Task analysis', 'Image reconstruction', 'Visualization']","['Use Of Imaging', 'Image Editing', 'Warped Image', 'Fake Images', 'Semantic', 'Supplemental Material', 'Validation Set', 'Flow Field', 'Average Precision', 'Peak Signal-to-noise Ratio', 'Optical Flow', 'Localization Prediction', 'Low-resolution Images', 'Reconstruction Loss', 'Use Of Face', 'Open Image', 'Professional Artists', 'Real Faces', 'JPEG Compression', 'Deepfake', 'Subtle Manipulation', 'Endpoint Error', 'Warp Field', 'Large Image Datasets', 'Training Data', 'Random Sampling', 'High-resolution Model', 'High-level Semantics', 'Facial Expressions']",,75,"Most malicious photo manipulations are created using standard image editing tools, such as Adobe Photoshop. We present a method for detecting one very popular Photoshop manipulation -- image warping applied to human faces -- using a model trained entirely using fake images that were automatically generated by scripting Photoshop itself. We show that our model outperforms humans at the task of recognizing manipulated images, can predict the specific location of edits, and in some cases can be used to ""undo"" a manipulation to reconstruct the original, unedited image. We demonstrate that the system can be successfully applied to artist-created image manipulations."
Detecting Unseen Visual Relations Using Analogies,"Julia Peyre, Ivan Laptev, Cordelia Schmid, Josef Sivic","D´epartement d’informatique de l’ENS, Ecole normale supérieure, CNRS, PSL Research University, 75005 Paris, France; INRIA; Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague.; INRIA; Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.; D´epartement d’informatique de l’ENS, Ecole normale supérieure, CNRS, PSL Research University, 75005 Paris, France; INRIA",100.0,"Czech Republic, France",0.0,,"We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as ""person riding dog"", where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010418/,"['Visualization', 'Detectors', 'Training', 'Training data', 'Dogs', 'Computational modeling', 'Vocabulary']","['Training Data', 'Training Examples', 'Challenging Dataset', 'Related Images', 'Individual Entities', 'Triplet Formation', 'Visual Representation', 'Object Detection', 'Visual Features', 'Bounding Box', 'Latent Space', 'Spatial Configuration', 'Composite Model', 'Word Embedding', 'Appearance Features', 'Subject And Object', 'Word2vec', 'Language Representation', 'Unigram', 'Candidate Objects', 'Benefit Transfer', 'Candidate Pairs', 'ReLU Nonlinearity', 'Representation Of Composition', 'Object Boxes', 'Objective Scores', 'Visual Detection', 'Parallel Model', 'Evaluation Dataset']",,77,"We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as “person riding dog”, where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset."
Detecting the Unexpected via Image Resynthesis,"Krzysztof Lis, Krishna Nakka, Pascal Fua, Mathieu Salzmann","Computer Vision Laboratory, EPFL",100.0,switzerland,0.0,,"Classical semantic segmentation methods, including the recent deep learning ones, assume that all classes observed at test time have been seen during training. In this paper, we tackle the more realistic scenario where unexpected objects of unknown classes can appear at test time. The main trends in this area either leverage the notion of prediction uncertainty to flag the regions with low confidence as unknown, or rely on autoencoders and highlight poorly-decoded regions. Having observed that, in both cases, the detected regions typically do not correspond to unexpected objects, in this paper, we introduce a drastically different strategy: It relies on the intuition that the network will produce spurious labels in regions depicting unexpected objects. Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image. In other words, we translate the problem of detecting unknown classes to one of identifying poorly-resynthesized image regions. We show that this outperforms both uncertainty- and autoencoder-based methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010663/,"['Semantics', 'Image segmentation', 'Uncertainty', 'Training', 'Roads', 'Bayes methods', 'Feature extraction']","['Deep Learning', 'Input Image', 'Semantic Segmentation', 'Trends In Area', 'Semantic Map', 'Semantic Segmentation Methods', 'False Positive', 'Receiver Operating Characteristic Curve', 'Segmentation Algorithm', 'Anomaly Detection', 'Synthetic Images', 'Semantic Network', 'Semantic Labels', 'Attack Detection', 'Adversarial Attacks', 'Semantic Segmentation Network', 'Adversarial Examples', 'Unknown Objects', 'Instance Labels', 'Construction Equipment', 'Anomaly Score']",,101,"Classical semantic segmentation methods, including the recent deep learning ones, assume that all classes observed at test time have been seen during training. In this paper, we tackle the more realistic scenario where unexpected objects of unknown classes can appear at test time. The main trends in this area either leverage the notion of prediction uncertainty to flag the regions with low confidence as unknown, or rely on autoencoders and highlight poorly-decoded regions. Having observed that, in both cases, the detected regions typically do not correspond to unexpected objects, in this paper, we introduce a drastically different strategy: It relies on the intuition that the network will produce spurious labels in regions depicting unexpected objects. Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image. In other words, we translate the problem of detecting unknown classes to one of identifying poorly-resynthesized image regions. We show that this outperforms both uncertainty- and autoencoder-based methods."
DewarpNet: Single-Image Document Unwarping With Stacked 3D and 2D Regression Networks,"Sagnik Das, Ke Ma, Zhixin Shu, Dimitris Samaras, Roy Shilkrot",Stony Brook University,100.0,usa,0.0,,"Capturing document images with hand-held devices in unstructured environments is a common practice nowadays. However, ""casual"" photos of documents are usually unsuitable for automatic information extraction, mainly due to physical distortion of the document paper, as well as various camera positions and illumination conditions. In this work, we propose DewarpNet, a deep-learning approach for document image unwarping from a single image. Our insight is that the 3D geometry of the document not only determines the warping of its texture but also causes the illumination effects. Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline. Also, we contribute the largest and most comprehensive dataset for document image unwarping to date - Doc3D. This dataset features multiple ground-truth annotations, including 3D shape, surface normals, UV map, albedo image, etc. Training with Doc3D, we demonstrate state-of-the-art performance for DewarpNet with extensive qualitative and quantitative evaluations. Our network also significantly improves OCR performance on captured document images, decreasing character error rate by 42% on average. Both the code and the dataset are released.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Das_DewarpNet_Single-Image_Document_Unwarping_With_Stacked_3D_and_2D_Regression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Das_DewarpNet_Single-Image_Document_Unwarping_With_Stacked_3D_and_2D_Regression_ICCV_2019_paper.pdf,https://www.cs.stonybrook.edu/~cvl/dewarpnet.html,,,main,Poster,https://ieeexplore.ieee.org/document/9010747/,"['Three-dimensional displays', 'Shape', 'Cameras', 'Strain', 'Lighting', 'Rendering (computer graphics)', 'Deformable models']","['Stacked 3D', 'Deep Learning', 'Illumination Conditions', '3D Shape', 'Camera Position', 'Optical Character Recognition', 'Multiple Annotations', 'Surface Normals', 'Model Parameters', 'Convolutional Neural Network', 'Input Image', 'Point Cloud', 'Depth Map', '3D Coordinates', 'Depth Camera', 'Real-world Images', 'Shape Representation', 'Natural Cubic Spline', 'Multi-view Images', 'Warped Image', 'Refinement Network', 'Inexpensive Sensors', 'Texture Map', 'Rich Annotations', 'Low-dimensional Model', 'Camera Viewpoint', 'Forward Mapping', 'Image Gradient', 'Text Lines']",,45,"Capturing document images with hand-held devices in unstructured environments is a common practice nowadays. However, ""casual"" photos of documents are usually unsuitable for automatic information extraction, mainly due to physical distortion of the document paper, as well as various camera positions and illumination conditions. In this work, we propose DewarpNet, a deep-learning approach for document image unwarping from a single image. Our insight is that the 3D geometry of the document not only determines the warping of its texture but also causes the illumination effects. Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline. Also, we contribute the largest and most comprehensive dataset for document image unwarping to date - Doc3D. This dataset features multiple ground-truth annotations, including 3D shape, surface normals, UV map, albedo image, etc. Training with Doc3D, we demonstrate state-of-the-art performance for DewarpNet with extensive qualitative and quantitative evaluations. Our network also significantly improves OCR performance on captured document images, decreasing character error rate by 42% on average. Both the code and the dataset are released."
Differentiable Kernel Evolution,"Yu Liu, Jihao Liu, Ailing Zeng, Xiaogang Wang","1CUHK-SenseTime Joint Laboratory, 3The Chinese University of Hong Kong; 2SenseTime Research; 3The Chinese University of Hong Kong",66.66666666666666,"Hong Kong, china",33.33333333333334,China,"This paper proposes a differentiable kernel evolution (DKE) algorithm to find a better layer-operator for the convolutional neural network. Unlike most of the other neural architecture searching (NAS) technologies, we consider the searching space in a fundamental scope: kernel space, which encodes the assembly of basic multiply-accumulate (MAC) operations into a conv-kernel. We first deduce a strict form of the generalized convolutional operator by some necessary constraints and construct a continuous searching space for its extra freedom-of-degree, namely, the connection of each MAC. Then a novel unsupervised greedy evolution algorithm called gradient agreement guided searching (GAGS) is proposed to learn the optimal location for each MAC in the spatially continuous searching space. We leverage DKE on multiple kinds of tasks such as object classification, face/object detection, large-scale fine-grained and recognition, with various kinds of backbone architecture. Not to mention the consistent performance gain, we found the proposed DKE can further act as an auto-dilated operator, which makes it easy to boost the performance of miniaturized neural networks in multiple tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Differentiable_Kernel_Evolution_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Differentiable_Kernel_Evolution_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009451/,"['Kernel', 'Task analysis', 'Convolution', 'Shape', 'Object detection', 'Neural networks', 'Interpolation']","['Neural Network', 'Convolutional Neural Network', 'Evolutionary Algorithms', 'Search Space', 'Convolution Operation', 'Neural Architecture', 'Multiple Tasks', 'Neural Architecture Search', 'Hyperparameters', 'Convolutional Layers', 'Partial Differential', 'Feature Maps', 'Search Algorithm', 'Object Detection', 'Receptive Field', 'ImageNet', 'Face Recognition', 'Convolution Kernel', 'Backbone Network', 'Input Channels', 'Kernel Shape', 'Interpolation Function', 'Face Detection', 'Conventional Convolution', 'Ideal Operation', 'Discrete Space', 'Attention Scores', 'Field Gradient', 'Equality Constraints', 'Deformable Convolution']",,3,"This paper proposes a differentiable kernel evolution (DKE) algorithm to find a better layer-operator for the convolutional neural network. Unlike most of the other neural architecture searching (NAS) technologies, we consider the searching space in a fundamental scope: kernel space, which encodes the assembly of basic multiplyaccumulate (MAC) operations into a conv-kernel. We first deduce a strict form of the generalized convolutional operator by some necessary constraints and construct a continuous searching space for its extra freedom-of-degree, namely, the connection of each MAC. Then a novel unsupervised greedy evolution algorithm called gradient agreement guided searching (GAGS) is proposed to learn the optimal location for each MAC in the spatially continuous searching space. We leverage DKE on multiple kinds of tasks such as object classification, face/object detection, large-scale finegrained and recognition, with various kinds of backbone architecture. Not to mention the consistent performance gain, we found the proposed DKE can further act as an autodilated operator, which makes it easy to boost the performance of miniaturized neural networks in multiple tasks."
Differentiable Learning-to-Group Channels via Groupable Convolutional Neural Networks,"Zhaoyang Zhang, Jingyu Li, Wenqi Shao, Zhanglin Peng, Ruimao Zhang, Xiaogang Wang, Ping Luo","The University of Hong Kong; CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong; SenseTime Research",66.66666666666666,"Hong Kong, china",33.33333333333334,China,"Group convolution, which divides the channels of ConvNets into groups, has achieved impressive improvement over the regular convolution operation. However, existing models, e.g. ResNext, still suffers from the sub-optimal performance due to manually defining the number of groups as a constant over all of the layers. Toward addressing this issue, we present Groupable ConvNet (GroupNet) built by using a novel dynamic grouping convolution (DGConv) operation, which is able to learn the number of groups in an end-to-end manner. The proposed approach has several appealing benefits. (1) DGConv provides a unified convolution representation and covers many existing convolution operations such as regular dense convolution, group convolution, and depthwise convolution. (2) DGConv is a differentiable and flexible operation which learns to perform various convolutions from training data. (3) GroupNet trained with DGConv learns different number of groups for different convolution layers. Extensive experiments demonstrate that GroupNet outperforms its counterparts such as ResNet and ResNeXt in terms of accuracy and computational complexity. We also present introspection and reproducibility study, for the first time, showing the learning dynamics of training group numbers.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Differentiable_Learning-to-Group_Channels_via_Groupable_Convolutional_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Differentiable_Learning-to-Group_Channels_via_Groupable_Convolutional_Neural_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/abstract/document/9008243/,"['Convolution', 'Manganese', 'Training', 'Computer architecture', 'Kernel', 'Network architecture', 'Convolutional neural networks']","['Computational Complexity', 'Convolutional Layers', 'Convolution Operation', 'Depthwise Convolution', 'Group Convolution', 'Deep Network', 'Cardinality', 'Feature Maps', 'Resource Constraints', 'Network Layer', 'Lower Layer', 'Stochastic Gradient Descent', 'Neural Architecture', 'Relationship Matrix', 'Group Learning', 'Submatrix', 'Binary Matrix', 'Sign Function', 'Block Diagonal Matrix', 'Extra Parameters', 'Top-1 Accuracy', 'Neural Architecture Search', 'Matrix Of Ones', 'Automatic Differentiation', 'Strong Representation', 'Kronecker Product', 'Convolution Kernel']",,24,"Group convolution, which divides the channels of ConvNets into groups, has achieved impressive improvement over the regular convolution operation. However, existing models, \eg ResNext, still suffers from the sub-optimal performance due to manually defining the number of groups as a constant over all of the layers. Toward addressing this issue, we present Groupable ConvNet (GroupNet) built by using a novel dynamic grouping convolution (DGConv) operation, which is able to learn the number of groups in an end-to-end manner. The proposed approach has several appealing benefits. (1) DGConv provides a unified convolution representation and covers many existing convolution operations such as regular dense convolution, group convolution, and depthwise convolution. (2) DGConv is a differentiable and flexible operation which learns to perform various convolutions from training data. (3) GroupNet trained with DGConv learns different number of groups for different convolution layers. Extensive experiments demonstrate that GroupNet outperforms its counterparts such as ResNet and ResNeXt in terms of accuracy and computational complexity. We also present introspection and reproducibility study, for the first time, showing the learning dynamics of training group numbers."
Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks,"Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, Junjie Yan","SenseTime Group Limited; State Key Laboratory of Software Development Environment, Beihang University; Beijing Institute of Technology",100.0,"China, china, usa",0.0,,"Hardware-friendly network quantization (e.g., binary/uniform quantization) can efficiently accelerate the inference and meanwhile reduce memory consumption of the deep neural networks, which is crucial for model deployment on resource-limited devices like mobile phones. However, due to the discreteness of low-bit quantization, existing quantization methods often face the unstable training process and severe performance degradation. To address this problem, in this paper we propose Differentiable Soft Quantization (DSQ) to bridge the gap between the full-precision and low-bit networks. DSQ can automatically evolve during training to gradually approximate the standard quantization. Owing to its differentiable property, DSQ can help pursue the accurate gradients in backward propagation, and reduce the quantization loss in forward process with an appropriate clipping range. Extensive experiments over several popular network structures show that training low-bit neural networks with DSQ can consistently outperform state-of-the-art quantization methods. Besides, our first efficient implementation for deploying 2 to 4-bit DSQ on devices with ARM architecture achieves up to 1.7x speed up, compared with the open-source 8-bit high-performance inference framework NCNN [31].",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010945/,"['Quantization (signal)', 'Training', 'Standards', 'Neural networks', 'Acceleration', 'Hardware', 'Backpropagation']","['Neural Network', 'Soft Quantization', 'Quantification Method', 'Inference Framework', 'Forward Process', 'Open-source Framework', 'Backward Propagation', 'Standard Quantum', 'Uniform Quantization', 'Network Quantization', 'Cylindrical', 'Quantum', 'Convolutional Layers', 'Quantization Error', 'Sign Function', 'Middle Range', 'Forward Pass', 'Quantification Model', 'Inference Speed', 'Backward Process', 'Asymptotic Function', 'Bit-width', 'Uniform Method', 'Bitwise Operations', 'Uniform Model']",,240,"Hardware-friendly network quantization (e.g., binary/uniform quantization) can efficiently accelerate the inference and meanwhile reduce memory consumption of the deep neural networks, which is crucial for model deployment on resource-limited devices like mobile phones. However, due to the discreteness of low-bit quantization, existing quantization methods often face the unstable training process and severe performance degradation. To address this problem, in this paper we propose Differentiable Soft Quantization (DSQ) to bridge the gap between the full-precision and low-bit networks. DSQ can automatically evolve during training to gradually approximate the standard quantization. Owing to its differentiable property, DSQ can help pursue the accurate gradients in backward propagation, and reduce the quantization loss in forward process with an appropriate clipping range. Extensive experiments over several popular network structures show that training low-bit neural networks with DSQ can consistently outperform state-of-the-art quantization methods. Besides, our first efficient implementation for deploying 2 to 4-bit DSQ on devices with ARM architecture achieves up to 1.7× speed up, compared with the open-source 8-bit high-performance inference framework NCNN [31]."
Digging Into Self-Supervised Monocular Depth Estimation,"ClÃ©ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J. Brostow","UCL; UCL, Niantic; Niantic; Caltech",75.0,"uk, usa",25.0,USA,"Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.pdf,,www.github.com/nianticlabs/monodepth2,,main,Poster,https://ieeexplore.ieee.org/document/9009796/,"['Training', 'Estimation', 'Predictive models', 'Cameras', 'Image color analysis', 'Image reconstruction', 'Image matching']","['Depth Estimation', 'Monocular Depth Estimation', 'Self-supervised Monocular Depth Estimation', 'Depth Map', 'Self-supervised Learning', 'Camera Motion', 'Isolation Of Components', 'Ground Truth Depth', 'Image Pixels', 'Single Image', 'Image Reconstruction', 'Color Images', 'Depth Images', 'Source Images', 'Optical Flow', 'Object Motion', 'Pose Estimation', 'Flow Estimation', 'Camera Pose', 'Relative Pose', 'Median Scale', 'Reprojection Error', 'Depth Prediction', 'Stereo Pairs', 'View Synthesis', 'Self-supervised Approach', 'Self-supervised Training', 'Mixed Training', 'Matching Loss', 'Recent Approaches']",,1428,"Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark."
Dilated Convolutional Neural Networks for Sequential Manifold-Valued Data,"Xingjian Zhen,  Rudrasis Chakraborty,  Nicholas Vogt,  Barbara B. Bendlin,  Vikas Singh","University of California, Berkeley; University of Wisconsin Madison",100.0,usa,0.0,,"Efforts are underway to study ways via which the power of deep neural networks can be extended to non-standard data types such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Often, sizable empirical improvements are possible when the geometry of such data spaces are incorporated into the design of the model, architecture, and algorithms. Motivated by neuroimaging applications, we study formulations where the data are   sequential manifold-valued measurements . This case is common in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of a recurrent model which poses computational/technical issues, and inspired by recent results showing the viability of dilated convolutional models for sequence prediction, we develop a dilated convolutional neural network architecture for this task. On the technical side, we show how the modules needed in our network can be derived while explicitly taking the Riemannian manifold structure into account. We show how the operations needed can leverage known results for calculating the weighted Frechet Mean (wFM). Finally, we present scientific results for group difference analysis in Alzheimer's disease (AD) where the groups are derived using AD pathology load: here the model finds several brain fiber bundles that are related to AD even when the subjects are all still cognitively healthy.",http://openaccess.thecvf.com/content_ICCV_2019/html/Zhen_Dilated_Convolutional_Neural_Networks_for_Sequential_Manifold-Valued_Data_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhen_Dilated_Convolutional_Neural_Networks_for_Sequential_Manifold-Valued_Data_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9010384/,"['Convolution', 'Manifolds', 'Data models', 'Machine learning', 'Kernel', 'Brain modeling', 'Geometry']","['Sequencing Data', 'Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Dilated Convolution', 'Dilated Convolutional Neural Network', 'Neuroimaging', 'Data Structure', 'Alzheimer’s Disease', 'Deep Neural Network', 'Alzheimer’s Disease Pathology', 'Positive Definite Matrix', 'Definite Matrix', 'Fiber Bundles', 'Convolutional Model', 'Convolutional Architecture', 'Recurrent Model', 'Symmetric Positive Definite Matrix', 'Riemannian Manifold', 'Orientation Distribution Function', 'Pittsburgh Compound B', 'Residual Connection', 'Diffusion Tensor Imaging', 'Contraction Mapping', 'Geometry Data', 'Superior Longitudinal Fasciculus', 'Dilated Convolution Layers', 'Convex Constraints', 'Equivalency', 'Deep Architecture']",,9,"Efforts are underway to study ways via which the power of deep neural networks can be extended to non-standard data types such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Often, sizable empirical improvements are possible when the geometry of such data spaces are incorporated into the design of the model, architecture, and algorithms. Motivated by neuroimaging applications, we study formulations where the data are {\em sequential manifold-valued measurements}. This case is common in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of a recurrent model which poses computational/technical issues, and inspired by recent results showing the viability of dilated convolutional models for sequence prediction, we develop a dilated convolutional neural network architecture for this task. On the technical side, we show how the modules needed in our network can be derived while explicitly taking the Riemannian manifold structure into account. We show how the operations needed can leverage known results for calculating the weighted Fr\'{e}chet Mean (wFM). Finally, we present scientific results for group difference analysis in Alzheimer's disease (AD) where the groups are derived using AD pathology load: here the model finds several brain fiber bundles that are related to AD even when the subjects are all still cognitively healthy."
DiscoNet: Shapes Learning on Disconnected Manifolds for 3D Editing,"Ãloi Mehr, Ariane Jourdan, Nicolas Thome, Matthieu Cord, Vincent Guitteny","LIP6, Sorbonne Universit´e",100.0,France,0.0,,"Editing 3D models is a very challenging task, as it requires complex interactions with the 3D shape to reach the targeted design, while preserving the global consistency and plausibility of the shape. In this work, we present an intelligent and user-friendly 3D editing tool, where the edited model is constrained to lie onto a learned manifold of realistic shapes. Due to the topological variability of real 3D models, they often lie close to a disconnected manifold, which cannot be learned with a common learning algorithm. Therefore, our tool is based on a new deep learning model, DiscoNet, which extends 3D surface autoencoders in two ways. Firstly, our deep learning model uses several autoencoders to automatically learn each connected component of a disconnected manifold, without any supervision. Secondly, each autoencoder infers the output 3D surface by deforming a pre-learned 3D template specific to each connected component. Both advances translate into improved 3D synthesis, thus enhancing the quality of our 3D editing tool.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008844/,"['Three-dimensional displays', 'Shape', 'Manifolds', 'Decoding', 'Solid modeling', 'Tools', 'Topology']","['Manifold', 'Shape Learning', '3D Editing', 'Plausibility', 'Deep Learning Models', '3D Shape', 'Real Shape', 'Point Cloud', 'Generative Adversarial Networks', 'Latent Space', '3D Point', '3D Mesh', 'Unit Sphere', 'Variational Autoencoder', 'Latent Vector', 'Deformation Field', 'Morphing', 'Kernel Regression', 'Signed Distance Function', 'Earth Mover’s Distance', 'Original Photographs']",,11,"Editing 3D models is a very challenging task, as it requires complex interactions with the 3D shape to reach the targeted design, while preserving the global consistency and plausibility of the shape. In this work, we present an intelligent and user-friendly 3D editing tool, where the edited model is constrained to lie onto a learned manifold of realistic shapes. Due to the topological variability of real 3D models, they often lie close to a disconnected manifold, which cannot be learned with a common learning algorithm. Therefore, our tool is based on a new deep learning model, DiscoNet, which extends 3D surface autoencoders in two ways. Firstly, our deep learning model uses several autoencoders to automatically learn each connected component of a disconnected manifold, without any supervision. Secondly, each autoencoder infers the output 3D surface by deforming a pre-learned 3D template specific to each connected component. Both advances translate into improved 3D synthesis, thus enhancing the quality of our 3D editing tool."
Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction,"Xiangyu Xu, Enrique Dunn","Stevens Institute of Technology, Hoboken, NJ, USA",100.0,usa,0.0,,"We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclidean shape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometry-based event segmentation and data association.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Discrete_Laplace_Operator_Estimation_for_Dynamic_3D_Reconstruction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Discrete_Laplace_Operator_Estimation_for_Dynamic_3D_Reconstruction_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010876/,"['Three-dimensional displays', 'Geometry', 'Laplace equations', 'Sequential analysis', 'Image reconstruction', 'Two dimensional displays', 'Optimization']","['Dynamic 3D', 'Dynamic Reconstruction', ""Discrete Green's Function"", 'Dynamic 3D Reconstruction', 'Sampling Density', 'Motion Capture', 'Optimization Framework', '3D Geometry', 'Temporal Sampling', 'Joint Estimation', 'Spatiotemporal Relationship', 'Discrete Operator', 'Local Topology', 'Cost Function', 'Line Segment', 'Translational Motion', '3D Point', 'Dimensionality Reduction Methods', 'Dynamic Time Warping', '2D Feature', 'Functional Affinity', 'Quadratic Programming Problem', 'Image Stream', 'Virtual Camera', 'Dictionary Learning', 'Global Sequence', 'Imaging Geometry', 'Affinity Matrix']",,9,"We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclidean shape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometry-based event segmentation and data association."
Discriminative Feature Learning With Consistent Attention Regularization for Person Re-Identification,"Sanping Zhou, Fei Wang, Zeyi Huang, Jinjun Wang","Robotics Institute, Carnegie Mellon University; School of Computer Science and Technology, Xi'an Jiaotong University; The Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University",100.0,"China, usa",0.0,,"Person re-identification (Re-ID) has undergone a rapid development with the blooming of deep neural network. Most methods are very easily affected by target misalignment and background clutter in the training process. In this paper, we propose a simple yet effective feedforward attention network to address the two mentioned problems, in which a novel consistent attention regularizer and an improved triplet loss are designed to learn foreground attentive features for person Re-ID. Specifically, the consistent attention regularizer aims to keep the deduced foreground masks similar from the low-level, mid-level and high-level feature maps. As a result, the network will focus on the foreground regions at the lower layers, which is benefit to learn discriminative features from the foreground regions at the higher layers. Last but not least, the improved triplet loss is introduced to enhance the feature learning capability, which can jointly minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Experimental results on the Market1501, DukeMTMC-reID and CUHK03 datasets have shown that our method outperforms most of the state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010421/,"['Training', 'Learning systems', 'Feature extraction', 'Clutter', 'Heating systems', 'Computer vision']","['Feature Learning', 'Discriminative Features', 'Discriminative Feature Learning', 'Consistency Regularization', 'Deep Neural Network', 'Feature Maps', 'Lower Layer', 'Feed-forward Network', 'Low-level Features', 'Higher Layers', 'Triplet Loss', 'Background Clutter', 'Foreground Regions', 'High-level Feature Maps', 'Low-level Feature Maps', 'Heatmap', 'Deep Learning', 'Convolutional Layers', 'Input Image', 'Stochastic Gradient Descent', 'Symmetric Loss', 'Multi-task Learning Framework', 'Attention Module', 'Final Performance', 'Line Method', 'Feature Learning Process', 'Gradient Backpropagation', 'Generative Adversarial Networks', 'Attention Learning', 'Deep Learning Features']",,82,"Person re-identification (Re-ID) has undergone a rapid development with the blooming of deep neural network. Most methods are very easily affected by target misalignment and background clutter in the training process. In this paper, we propose a simple yet effective feedforward attention network to address the two mentioned problems, in which a novel consistent attention regularizer and an improved triplet loss are designed to learn foreground attentive features for person Re-ID. Specifically, the consistent attention regularizer aims to keep the deduced foreground masks similar from the low-level, mid-level and high-level feature maps. As a result, the network will focus on the foreground regions at the lower layers, which is benefit to learn discriminative features from the foreground regions at the higher layers. Last but not least, the improved triplet loss is introduced to enhance the feature learning capability, which can jointly minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Experimental results on the Market1501, DukeMTMC-reID and CUHK03 datasets have shown that our method outperforms most of the state-of-the-art approaches."
Discriminative Feature Transformation for Occluded Pedestrian Detection,"Chunluan Zhou, Ming Yang, Junsong Yuan","Horizon Robotics, Northwestern University; Baidu Research, Wormpex AI Research, State University of New York at Buffalo; State University of New York at Buffalo",100.0,usa,0.0,,"Despite promising performance achieved by deep con- volutional neural networks for non-occluded pedestrian de- tection, it remains a great challenge to detect partially oc- cluded pedestrians. Compared with non-occluded pedes- trian examples, it is generally more difficult to distinguish occluded pedestrian examples from background in featue space due to the missing of occluded parts. In this paper, we propose a discriminative feature transformation which en- forces feature separability of pedestrian and non-pedestrian examples to handle occlusions for pedestrian detection. Specifically, in feature space it makes pedestrian exam- ples approach the centroid of easily classified non-occluded pedestrian examples and pushes non-pedestrian examples close to the centroid of easily classified non-pedestrian ex- amples. Such a feature transformation partially compen- sates the missing contribution of occluded parts in feature space, therefore improving the performance for occluded pedestrian detection. We implement our approach in the Fast R-CNN framework by adding one transformation net- work branch. We validate the proposed approach on two widely used pedestrian detection datasets: Caltech and CityPersons. Experimental results show that our approach achieves promising performance for both non-occluded and occluded pedestrian detection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Discriminative_Feature_Transformation_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Discriminative_Feature_Transformation_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009440/,"['Feature extraction', 'Detectors', 'Proposals', 'Transforms', 'Convolution', 'Robustness', 'Task analysis']","['Feature Transformation', 'Pedestrian Detection', 'Occluded Pedestrians', 'Convolutional Neural Network', 'Feature Space', 'Detection Performance', 'Deep Convolutional Neural Network', 'Part Of Space', 'Faster R-CNN', 'Training Set', 'Convolutional Layers', 'Feature Maps', 'Cross-entropy Loss', 'Intersection Over Union', 'Training Images', 'Partial Information', 'Attention Module', 'Fast Detection', 'Background Regions', 'Attention Map', 'Competitive Approach', 'Detection Part', 'Easy Examples', 'Convolutional Neural Network Features', 'Transformer Loss', 'Visible Part', 'Bounding Box Regression', 'Sigmoid Layer', 'Proposal Features', 'Centroid Point']",,42,"Despite promising performance achieved by deep con- volutional neural networks for non-occluded pedestrian de- tection, it remains a great challenge to detect partially oc- cluded pedestrians. Compared with non-occluded pedes- trian examples, it is generally more difficult to distinguish occluded pedestrian examples from background in featue space due to the missing of occluded parts. In this paper, we propose a discriminative feature transformation which en- forces feature separability of pedestrian and non-pedestrian examples to handle occlusions for pedestrian detection. Specifically, in feature space it makes pedestrian exam- ples approach the centroid of easily classified non-occluded pedestrian examples and pushes non-pedestrian examples close to the centroid of easily classified non-pedestrian ex- amples. Such a feature transformation partially compen- sates the missing contribution of occluded parts in feature space, therefore improving the performance for occluded pedestrian detection. We implement our approach in the Fast R-CNN framework by adding one transformation net- work branch. We validate the proposed approach on two widely used pedestrian detection datasets: Caltech and CityPersons. Experimental results show that our approach achieves promising performance for both non-occluded and occluded pedestrian detection."
Discriminatively Learned Convex Models for Set Based Face Recognition,"Hakan Cevikalp, Golara Ghorban Dordinejad","Eskisehir Osmangazi University, Machine Learning and Computer Vision Laboratory",100.0,Turkey,0.0,,"Majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual's face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generative convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on four of the five tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX and ESOGU video datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cevikalp_Discriminatively_Learned_Convex_Models_for_Set_Based_Face_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cevikalp_Discriminatively_Learned_Convex_Models_for_Set_Based_Face_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008119/,"['Face', 'Computational modeling', 'Face recognition', 'Symmetric matrices', 'Measurement', 'Manifolds', 'Optimization']","['Face Recognition', 'Convex Model', 'Best Fit', 'Improvement In Accuracy', 'Face Images', 'Model Discrimination', 'Convex Hull', 'Model Of Personality', 'Query Set', 'Video Dataset', 'Query Image', 'Simple Multiplication', 'Gallery Set', 'Optimization Problem', 'Sample Set', 'Convolutional Neural Network', 'General Method', 'Classification Of Samples', 'Vector-based', 'Local Binary Pattern Features', 'Discrimination Method', 'Convolutional Neural Network Features', 'Affine Model', 'Positive Class', 'Quadratic Optimization Problem', 'Query Sample', 'Definite Matrix', 'Projection Operator', 'Smallest Distance']",,3,"Majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual's face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generative convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on four of the five tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX and ESOGU video datasets."
Disentangled Image Matting,"Shaofan Cai, Xiaoshuai Zhang, Haoqiang Fan, Haibin Huang, Jiangyu Liu, Jiaming Liu, Jiaying Liu, Jue Wang, Jian Sun","Megvii Technology, Institute of Computer Science and Technology, Peking University; Megvii Technology; Institute of Computer Science and Technology, Peking University",66.66666666666666,china,33.33333333333334,China,"Most previous image matting methods require a roughly-specificed trimap as input, and estimate fractional alpha values for all pixels that are in the unknown region of the trimap. In this paper, we argue that directly estimating the alpha matte from a coarse trimap is a major limitation of previous methods, as this practice tries to address two difficult and inherently different problems at the same time: identifying true blending pixels inside the trimap region, and estimate accurate alpha values for them. We propose AdaMatting, a new end-to-end matting framework that disentangles this problem into two sub-tasks: trimap adaptation and alpha estimation. Trimap adaptation is a pixel-wise classification problem that infers the global structure of the input image by identifying definite foreground, background, and semi-transparent image regions. Alpha estimation is a regression problem that calculates the opacity value of each blended pixel. Our method separately handles these two sub-tasks within a single deep convolutional neural network (CNN). Extensive experiments show that AdaMatting has additional structure awareness and trimap fault-tolerance. Our method achieves the state-of-the-art performance on Adobe Composition-1k dataset both qualitatively and quantitatively. It is also the current best-performing method on the alphamatting.com online evaluation for all commonly-used metrics.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Disentangled_Image_Matting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Disentangled_Image_Matting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009562/,"['Task analysis', 'Estimation', 'Adaptation models', 'Image color analysis', 'Semantics', 'Pipelines', 'Image segmentation']","['Image Matting', 'Convolutional Neural Network', 'Input Image', 'Alpha Value', 'Background Regions', 'Unknown Regions', 'Foreground Regions', 'Alpha Estimates', 'Alpha Matte', 'Superior Performance', 'Classification Task', 'Long Short-term Memory', 'Receptive Field', 'Semantic Information', 'Semantic Segmentation', 'Multiple Tasks', 'Segmentation Task', 'Object Shape', 'Two-stage Method', 'Shared Representation', 'Multi-task Learning', 'Sum Of Absolute Differences', 'Objective Structured', 'Foreground Objects', 'Automatic Generation', 'Error Gradient', 'Minor Errors', 'Understanding Of Shape', 'High-level Features']",,85,"Most previous image matting methods require a roughly-specificed trimap as input, and estimate fractional alpha values for all pixels that are in the unknown region of the trimap. In this paper, we argue that directly estimating the alpha matte from a coarse trimap is a major limitation of previous methods, as this practice tries to address two difficult and inherently different problems at the same time: identifying true blending pixels inside the trimap region, and estimate accurate alpha values for them. We propose AdaMatting, a new end-to-end matting framework that disentangles this problem into two sub-tasks: trimap adaptation and alpha estimation. Trimap adaptation is a pixel-wise classification problem that infers the global structure of the input image by identifying definite foreground, background, and semi-transparent image regions. Alpha estimation is a regression problem that calculates the opacity value of each blended pixel. Our method separately handles these two sub-tasks within a single deep convolutional neural network (CNN). Extensive experiments show that AdaMatting has additional structure awareness and trimap fault-tolerance. Our method achieves the state-of-the-art performance on Adobe Composition-1k dataset both qualitatively and quantitatively. It is also the current best-performing method on the alphamatting.com online evaluation for all commonly-used metrics."
Disentangling Monocular 3D Object Detection,"Andrea Simonelli, Samuel Rota BulÃ², Lorenzo Porzi, Manuel LÃ³pez-Antequera, Peter Kontschieder","Mapillary Research; University of Trento, Fondazione Bruno Kessler; Mapillary Research⋆University of Trento, Fondazione Bruno Kessler",66.66666666666666,italy,33.33333333333334,Sweden,"In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies and set a new state-of-the-art on the KITTI3D Car class.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Simonelli_Disentangling_Monocular_3D_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Simonelli_Disentangling_Monocular_3D_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010618/,"['Three-dimensional displays', 'Two dimensional displays', 'Shape', 'Object detection', 'Measurement', 'Task analysis', 'Laser radar']","['3D Object Detection', 'Monocular 3D Object Detection', 'Detection Results', 'Bounding Box', 'Parameters In Group', 'RGB Images', '3D Detection', '3D Bounding Box', 'Loss Function', 'Point Cloud', 'Stochastic Gradient Descent', 'Average Precision', '3D Shape', 'Depth Estimation', 'Feature Pyramid Network', 'Natural Loss', 'Input Resolution', 'Object Pose', '3D Pose', 'Detection Head', 'Central Depth', 'Center Of The Bounding Box', 'Ground-truth Bounding Box', 'Camera Coordinate', 'Geometric Reasoning', 'Monocular Images', 'Warm-up Phase', 'Image Coordinates']",,332,"In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies and set a new state-of-the-art on the KITTI3D Car class."
Disentangling Propagation and Generation for Video Prediction,"Hang Gao, Huazhe Xu, Qi-Zhi Cai, Ruth Wang, Fisher Yu, Trevor Darrell",Columbia University; UC Berkeley; Sinovation Ventures AI Institute,100.0,"China, usa",0.0,,"A dynamic scene has two types of elements: those that move fluidly and can be predicted from previous frames, and those which are disoccluded (exposed) and cannot be extrapolated. Prior approaches to video prediction typically learn either to warp or to hallucinate future pixels, but not both. In this paper, we describe a computational model for high-fidelity video prediction which disentangles motion-specific propagation from motion-agnostic generation. We introduce a confidence-aware warping operator which gates the output of pixel predictions from a flow predictor for non-occluded regions and from a context encoder for occluded regions. Moreover, in contrast to prior works where confidence is jointly learned with flow and appearance using a single network, we compute confidence after a warping step, and employ a separate network to inpaint exposed regions. Empirical results on both synthetic and real datasets show that our disentangling approach provides better occlusion maps and produces both sharper and more realistic predictions compared to strong baselines.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Disentangling_Propagation_and_Generation_for_Video_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Disentangling_Propagation_and_Generation_for_Video_Prediction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010255/,"['Predictive models', 'Task analysis', 'Optical imaging', 'Streaming media', 'Computational modeling', 'History', 'Standards']","['Video Prediction', 'Single Network', 'Occluded Regions', 'Context Encoder', 'Training Set', 'Feature Maps', 'Intersection Over Union', 'Receptive Field', 'Flow Field', 'Generative Adversarial Networks', 'Cognitive Map', 'Prediction Task', 'Peak Signal-to-noise Ratio', 'Optical Flow', 'Separate Modules', 'Image Synthesis', 'Large Motion', 'Motion Prediction', 'Flow Prediction', 'Input Frames', 'Structural Similarity Index Measure', 'Future Frames', 'Pixel-based Methods', 'Target Frame', 'Regular Grid']",,59,"A dynamic scene has two types of elements: those that move fluidly and can be predicted from previous frames, and those which are disoccluded (exposed) and cannot be extrapolated. Prior approaches to video prediction typically learn either to warp or to hallucinate future pixels, but not both. In this paper, we describe a computational model for high-fidelity video prediction which disentangles motion-specific propagation from motion-agnostic generation. We introduce a confidence-aware warping operator which gates the output of pixel predictions from a flow predictor for non-occluded regions and from a context encoder for occluded regions. Moreover, in contrast to prior works where confidence is jointly learned with flow and appearance using a single network, we compute confidence after a warping step, and employ a separate network to inpaint exposed regions. Empirical results on both synthetic and real datasets show that our disentangling approach provides better occlusion maps and produces both sharper and more realistic predictions compared to strong baselines."
DistInit: Learning Video Representations Without a Single Labeled Video,"Rohit Girdhar, Du Tran, Lorenzo Torresani, Deva Ramanan",Carnegie Mellon University; Facebook; Dartmouth College; Argo AI,50.0,"USA, usa",50.0,USA,"Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models has not been able to keep up with the ever increasing depth and sophistication of these networks. In this work we propose an alternative approach to learning video representations that requires no semantically labeled videos, and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as ""teachers"" to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Girdhar_DistInit_Learning_Video_Representations_Without_a_Single_Labeled_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Girdhar_DistInit_Learning_Video_Representations_Without_a_Single_Labeled_Video_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009014/,"['Task analysis', 'Two dimensional displays', 'Spatiotemporal phenomena', 'Three-dimensional displays', 'Standards', 'Computer architecture', 'Computational modeling']","['Video Representation Learning', 'Image Dataset', 'Video Data', 'Handcrafted Features', 'Input Modalities', 'Semantic Labels', 'Raw Video', 'Spatiotemporal Network', 'Video Modeling', 'Inflation', 'Convolutional Neural Network', 'Unsupervised Learning', 'Large-scale Datasets', 'Action Recognition', '2D Model', 'Domain Adaptation', 'Pre-trained Network', '3D Architecture', 'Action Classes', 'Teacher Network', 'Action Recognition Task', 'Student Network', 'Video Dataset', 'Human Activity Recognition', 'Spatiotemporal Model', 'Video Understanding', 'Standard Benchmark', 'Average Classification Accuracy', 'Current Best Practice', 'Video Clips']",,35,"Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models has not been able to keep up with the ever increasing depth and sophistication of these networks. In this work we propose an alternative approach to learning video representations that requires no semantically labeled videos, and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as “teachers” to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data."
Distill Knowledge From NRSfM for Weakly Supervised 3D Pose Learning,"Chaoyang Wang, Chen Kong, Simon Lucey",Carnegie Mellon University,100.0,usa,0.0,,"We propose to learn a 3D pose estimator by distilling knowledge from Non-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark annotations. No 3D data, multi-view/temporal footage, or object specific prior is required. This alleviates the data bottleneck, which is one of the major concern for supervised methods. The challenge for using NRSfM as teacher is that they often make poor depth reconstruction when the 2D projections have strong ambiguity. Directly using those wrong depth as hard target would negatively impact the student. Instead, we propose a novel loss that ties depth prediction to the cost function used in NRSfM. This gives the student pose estimator freedom to reduce depth error by associating with image features. Validated on H3.6M dataset, our learned 3D pose estimation network achieves more accurate reconstruction compared to NRSfM methods. It also outperforms other weakly supervised methods, in spite of using significantly less supervision.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Distill_Knowledge_From_NRSfM_for_Weakly_Supervised_3D_Pose_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Distill_Knowledge_From_NRSfM_for_Weakly_Supervised_3D_Pose_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010405/,"['Three-dimensional displays', 'Two dimensional displays', 'Shape', 'Cameras', 'Dictionaries', 'Image reconstruction', 'Training']","['3D Pose', 'Learning Network', '3D Data', 'Pose Estimation', 'Estimation Network', '2D Projection', 'Structure From Motion', 'Human Pose Estimation', 'Depth Error', 'Loss Function', 'Training Set', 'Validation Set', 'Bounding Box', 'Approximate Solution', 'Lower Error', 'Motion Capture', 'Matrix M', '3D Shape', 'Z Coordinates', 'Depth Estimation', 'Image Coordinates', 'Sparse Coding', 'Reprojection Error', 'Student Network', 'Dictionary Learning', 'Code Vector', 'Shape Priors', 'Strong Baseline', 'Camera Position', 'Depth Values']",,32,"We propose to learn a 3D pose estimator by distilling knowledge from Non-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark annotations. No 3D data, multi-view/temporal footage, or object specific prior is required. This alleviates the data bottleneck, which is one of the major concern for supervised methods. The challenge for using NRSfM as teacher is that they often make poor depth reconstruction when the 2D projections have strong ambiguity. Directly using those wrong depth as hard target would negatively impact the student. Instead, we propose a novel loss that ties depth prediction to the cost function used in NRSfM. This gives the student pose estimator freedom to reduce depth error by associating with image features. Validated on H3.6M dataset, our learned 3D pose estimation network achieves more accurate reconstruction compared to NRSfM methods. It also outperforms other weakly supervised methods, in spite of using significantly less supervision."
Distillation-Based Training for Multi-Exit Architectures,"Mary Phuong, Christoph H. Lampert",IST Austria,100.0,austria,0.0,,"Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009834/,"['Training', 'Computer architecture', 'Computational modeling', 'Training data', 'Predictive models', 'Entropy', 'Task analysis']","['Training Data', 'Training Procedure', 'Stacked Layers', 'Unlabeled Data', 'Semi-supervised Learning', 'Model Selection', 'Classification Accuracy', 'Image Classification', 'Annealing Temperature', 'Multi-label', 'Prediction Time', 'Ground Truth Labels', 'Amount Of Training Data', 'Training Examples', 'Classification Loss', 'Loss Term', 'Training Objective', 'Multi-task Learning', 'Labeled Training Data', 'Distillation Loss', 'Neural Architecture Search', 'Training Labels', 'Hard Examples', 'Probabilistic Classification', 'Time Budget', 'Convolutional Network']",,88,"Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time."
Distilling Knowledge From a Deep Pose Regressor Network,"Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Yasin Almalioglu, Andrew Markham, Niki Trigoni","Department of Computer Science, University of Oxford",100.0,uk,0.0,,"This paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on ""dark knowledge"" for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12x faster in computation time.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009104/,"['Knowledge engineering', 'Training', 'Quantization (signal)', 'Standards', 'Upper bound', 'Linear programming', 'Uncertainty']","['Deep Network', 'Pose Regressor', 'Regressor Network', 'Computation Time', 'Knowledge Transfer', 'Regression Network', 'KITTI Dataset', 'Visual Odometry', 'Objective Function', 'Convolutional Neural Network', 'Deep Neural Network', 'Object Detection', 'Long Short-term Memory', 'Recurrent Neural Network', 'Model Size', 'Fully-connected Layer', 'Regression Problem', 'Intermediate Layer', 'Ground Truth Labels', 'Convolutional Neural Network Layers', 'Camera Pose', 'Student Network', 'Teacher Network', 'Feature Extraction Network', 'Low-rank Decomposition', 'Relative Pose', 'Output Trajectory', 'Binary Network', 'Final Layer', 'Additional Loss']",,62,"This paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on ''dark knowledge'' for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12x faster in computation time."
Diverse Image Synthesis From Semantic Layouts via Conditional IMLE,"Ke Li, Tianhao Zhang, Jitendra Malik",UC Berkeley; Nanjing University,100.0,"china, usa",0.0,,"Most existing methods for conditional image synthesis are only able to generate a single plausible image for any given input, or at best a fixed number of plausible images. In this paper, we focus on the problem of generating images from semantic segmentation maps and present a simple new method that can generate an arbitrary number of images with diverse appearance for the same semantic layout. Unlike most existing approaches which adopt the GAN framework, our method is based on the recently introduced Implicit Maximum Likelihood Estimation (IMLE) framework. Compared to the leading approach, our method is able to generate more diverse images while producing fewer artifacts despite using the same architecture. The learned latent space also has sensible structure despite the lack of supervision that encourages such behaviour.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Diverse_Image_Synthesis_From_Semantic_Layouts_via_Conditional_IMLE_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Diverse_Image_Synthesis_From_Semantic_Layouts_via_Conditional_IMLE_ICCV_2019_paper.pdf,https://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/,,,main,Poster,https://ieeexplore.ieee.org/document/9009043/,"['Gallium nitride', 'Image generation', 'Generators', 'Layout', 'Semantics', 'Neural networks', 'Probabilistic logic']","['Image Synthesis', 'Diverse Images', 'Semantic Layout', 'Maximum Likelihood Estimation', 'Input Image', 'Single Image', 'Number Of Images', 'Latent Space', 'Segmentation Map', 'Semantic Map', 'Image Quality', 'Probability Density', 'Probabilistic Model', 'Conditional Distribution', 'Random Vector', 'Video Frames', 'Marginal Distribution', 'Distribution Of Images', 'Mode Of Distribution', 'Output Image', 'Latent Vector', 'Neural Net', 'Ground Truth Image', 'Input Label', 'Noise Vector', 'Synthesis Problem', 'Implicit Model', 'Latent Code']",,47,"Most existing methods for conditional image synthesis are only able to generate a single plausible image for any given input, or at best a fixed number of plausible images. In this paper, we focus on the problem of generating images from semantic segmentation maps and present a simple new method that can generate an arbitrary number of images with diverse appearance for the same semantic layout. Unlike most existing approaches which adopt the GAN framework, our method is based on the recently introduced Implicit Maximum Likelihood Estimation (IMLE) framework. Compared to the leading approach, our method is able to generate more diverse images while producing fewer artifacts despite using the same architecture. The learned latent space also has sensible structure despite the lack of supervision that encourages such behaviour."
Diversity With Cooperation: Ensemble Methods for Few-Shot Classification,"Nikita Dvornik, Cordelia Schmid, Julien Mairal","Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France",100.0,France,0.0,,"Few-shot classification consists of learning a predictive model that is able to effectively adapt to a new class, given only a few annotated samples. To solve this challenging problem, meta-learning has become a popular paradigm that advocates the ability to ""learn to adapt"". Recent works have shown, however, that simple learning strategies without meta-learning could be competitive. In this paper, we go a step further and show that by addressing the fundamental high-variance issue of few-shot learning classifiers, it is possible to significantly outperform current meta-learning techniques. Our approach consists of designing an ensemble of deep networks to leverage the variance of the classifiers, and introducing new strategies to encourage the networks to cooperate, while encouraging prediction diversity. Evaluation is conducted on the mini-ImageNet, tiered-ImageNet and CUB datasets, where we show that even a single network obtained by distillation yields state-of-the-art results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010380/,"['Task analysis', 'Training', 'Predictive models', 'Probabilistic logic', 'Computer vision', 'Convolutional neural networks']","['Ensemble Method', 'Few-shot Classification', 'Deep Network', 'Single Network', 'Ensemble Of Networks', 'Few-shot Learning', 'Annotated Samples', 'Neural Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Validation Set', 'Classification Task', 'Parametrized', 'Domain Shift', 'Class Probabilities', 'Robust Strategy', 'Ground Truth Labels', 'Probability Vector', 'Number Of Networks', 'Predictor Of Diversity', 'Full Ensemble', 'Image X', 'Data Augmentation Techniques', 'Ensemble Strategy', 'Ensemble Members', 'Random Cropping', 'Ensemble Size', 'Computational Overhead', 'Data Streams']",,113,"Few-shot classification consists of learning a predictive model that is able to effectively adapt to a new class, given only a few annotated samples. To solve this challenging problem, meta-learning has become a popular paradigm that advocates the ability to ""learn to adapt''. Recent works have shown, however, that simple learning strategies without meta-learning could be competitive. In this paper, we go a step further and show that by addressing the fundamental high-variance issue of few-shot learning classifiers, it is possible to significantly outperform current meta-learning techniques. Our approach consists of designing an ensemble of deep networks to leverage the variance of the classifiers, and introducing new strategies to encourage the networks to cooperate, while encouraging prediction diversity. Evaluation is conducted on the mini-ImageNet, tiered-ImageNet and CUB datasets, where we show that even a single network obtained by distillation yields state-of-the-art results."
Domain Adaptation for Semantic Segmentation With Maximum Squares Loss,"Minghao Chen, Hongyang Xue, Deng Cai","Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China; Fabu Inc., Hangzhou, China; State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, China",66.66666666666666,China,33.33333333333334,China,"Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.pdf,,https://github.com/ZJULearning/MaxSquareLoss,,main,Poster,https://ieeexplore.ieee.org/document/9009798/,"['Entropy', 'Semantics', 'Semisupervised learning', 'Task analysis', 'Minimization methods', 'Training']","['Semantic Segmentation', 'Domain Adaptation', 'Maximum Square', 'Deep Neural Network', 'Target Sample', 'Adaptive Approach', 'Real-world Datasets', 'Class Imbalance', 'Target Domain', 'Maximum Loss', 'Semi-supervised Learning', 'Minimum Entropy', 'Unlabeled Target Domain', 'High Probability', 'Objective Function', 'Learning Rate', 'Weighting Factor', 'ImageNet', 'Stochastic Gradient Descent', 'Generative Adversarial Networks', 'Source Domain', 'Labeled Source Domain', 'Semantic Segmentation Task', 'ResNet-101 Backbone', 'Distribution Alignment', 'Frequent Class', 'Entropy Formula', 'Maximum Mean Discrepancy', 'Low-level Features', 'Prediction Probability']",,210,"Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss."
Domain Adaptation for Structured Output via Discriminative Patch Representations,"Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, Manmohan Chandraker","NEC Laboratories America, University of California, San Diego; NEC Laboratories America",50.0,usa,50.0,USA,"Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn supervised models like convolutional neural networks. However, models trained on one data domain may not generalize well to other domains without annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. We propose to learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. With such representations as guidance, we use an adversarial learning scheme to push the feature representations of target patches in the clustered space closer to the distributions of source patches. In addition, we show that our framework is complementary to existing domain adaptation techniques and achieves consistent improvements on semantic segmentation. Extensive ablations and results are demonstrated on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tsai_Domain_Adaptation_for_Structured_Output_via_Discriminative_Patch_Representations_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tsai_Domain_Adaptation_for_Structured_Output_via_Discriminative_Patch_Representations_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008285/,"['Semantics', 'Task analysis', 'Adaptation models', 'Training', 'Image segmentation', 'Data models', 'Indexes']","['Domain Adaptation', 'Output Structure', 'Patch Representation', 'Convolutional Neural Network', 'Feature Representation', 'Generative Adversarial Networks', 'Semantic Segmentation', 'Target Domain', 'Source Domain', 'Domain Adaptation Methods', 'Distribution Of Patches', 'Domain Adaptation Techniques', 'Loss Function', 'Training Set', 'Structure Prediction', 'Stochastic Gradient Descent', 'Representation Learning', 'Source Images', 'Target Data', 'Modulation Index', 'K-dimensional Vector', 'Output Space', 'Cluster Index', 'Unsupervised Domain Adaptation Methods', 'Domain Gap', 'Project Page', 'Pseudo Labels']",,226,"Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn supervised models like convolutional neural networks. However, models trained on one data domain may not generalize well to other domains without annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. We propose to learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. With such representations as guidance, we use an adversarial learning scheme to push the feature representations of target patches in the clustered space closer to the distributions of source patches. In addition, we show that our framework is complementary to existing domain adaptation techniques and achieves consistent improvements on semantic segmentation. Extensive ablations and results are demonstrated on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios."
Domain Intersection and Domain Difference,"Sagie Benaim, Michael Khaitov, Tomer Galanti, Lior Wolf","School of Computer Science, Tel Aviv University; Facebook AI Research; School of Computer Science, Tel Aviv University",66.66666666666666,israel,33.33333333333334,USA,"We present a method for recovering the shared content between two visual domains as well as the content that is unique to each domain. This allows us to map from one domain to the other, in a way in which the content that is specific for the first domain is removed and the content that is specific for the second is imported from any image in the second domain. In addition, our method enables generation of images from the intersection of the two domains as well as their union, despite having no such samples during training. The method is shown analytically to contain all the sufficient and necessary constraints. It also outperforms the literature methods in an extensive set of experiments.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Benaim_Domain_Intersection_and_Domain_Difference_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Benaim_Domain_Intersection_and_Domain_Difference_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008767/,"['Glass', 'Image coding', 'Visualization', 'Training', 'Encoding', 'Decoding', 'Neural networks']","['Visual Domain', 'Shared Content', 'Paired Samples', 'Autoencoder', 'Top Row', 'Latent Space', 'Left Column', 'Source Images', 'Target Domain', 'Domain Adaptation', 'Source Domain', 'Reconstruction Loss', 'Person Image', 'Common Information', 'Common Part', 'Running Example', 'Invertible Function', 'Shared Domain', 'Disentangled Representation', 'Wide Variety Of Domains', 'Domain-specific Information', 'Symmetric Way', 'Types Of Information']",,12,"We present a method for recovering the shared content between two visual domains as well as the content that is unique to each domain. This allows us to map from one domain to the other, in a way in which the content that is specific for the first domain is removed and the content that is specific for the second is imported from any image in the second domain. In addition, our method enables generation of images from the intersection of the two domains as well as their union, despite having no such samples during training. The method is shown analytically to contain all the sufficient and necessary constraints. It also outperforms the literature methods in an extensive set of experiments."
Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization Without Accessing Target Domain Data,"Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, Boqing Gong","University of California, Berkeley; Google; University of Central Florida",66.66666666666666,usa,33.33333333333334,USA,"We propose to harness the potential of simulation for semantic segmentation of real-world self-driving scenes in a domain generalization fashion. The segmentation network is trained without any information about target domains and tested on the unseen target domains. To this end, we propose a new approach of domain randomization and pyramid consistency to learn a model with high generalizability. First, we propose to randomize the synthetic images with styles of real images in terms of visual appearances using auxiliary datasets, in order to effectively learn domain-invariant representations. Second, we further enforce pyramid consistency across different ""stylized"" images and within an image, in order to learn domain-invariant and scale-invariant features, respectively. Extensive experiments are conducted on generalization from GTA and SYNTHIA to Cityscapes, BDDS, and Mapillary; and our method achieves superior results over the state-of-the-art techniques. Remarkably, our generalization results are on par with or even better than those obtained by state-of-the-art simulation-to-real domain adaptation methods, which access the target domain data at training time.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yue_Domain_Randomization_and_Pyramid_Consistency_Simulation-to-Real_Generalization_Without_Accessing_Target_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yue_Domain_Randomization_and_Pyramid_Consistency_Simulation-to-Real_Generalization_Without_Accessing_Target_ICCV_2019_paper.pdf,,https://github.com/xyyue/DRPC,,main,Poster,,,,,,
Domain-Adaptive Single-View 3D Reconstruction,"Pedro O. Pinheiro, Negar Rostamzadeh, Sungjin Ahn",Element AI; Rutgers University,50.0,usa,50.0,Canada,"Single-view 3D shape reconstruction is an important but challenging problem, mainly for two reasons. First, as shape annotation is very expensive to acquire, current methods rely on synthetic data, in which ground-truth 3D annotation is easy to obtain. However, this results in domain adaptation problem when applied to natural images. The second challenge is that there are multiple shapes that can explain a given 2D image. In this paper, we propose a framework to improve over these challenges using adversarial training. On one hand, we impose domain confusion between natural and synthetic image representations to reduce the distribution gap. On the other hand, we impose the reconstruction to be `realistic' by forcing it to lie on a (learned) manifold of realistic object shapes. Our experiments show that these constraints improve performance by a large margin over baseline reconstruction models. We achieve results competitive with the state of the art with a much simpler architecture.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pinheiro_Domain-Adaptive_Single-View_3D_Reconstruction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pinheiro_Domain-Adaptive_Single-View_3D_Reconstruction_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008399/,"['Three-dimensional displays', 'Shape', 'Image reconstruction', 'Training', 'Solid modeling', 'Two dimensional displays', 'Decoding']","['3D Reconstruction', 'Single-view 3D Reconstruction', 'State Of The Art', '2D Images', 'Natural Images', '3D Shape', 'Synthetic Images', 'Domain Adaptation', 'Simple Architecture', 'Reconstruction Model', 'Adversarial Training', 'Real Shape', 'Single Image', 'Intersection Over Union', 'Autoencoder', 'Point Cloud', 'Representation Learning', 'Latent Space', 'Computer-aided Design', 'Common Space', '3D Representation', 'Shape Priors', 'Point Cloud Representation', 'Latent Representation', 'Network Reconstruction', 'Surface Normals', 'Loss Term', 'Chamfer Distance', 'Strong Prior', 'Reconstruction Loss']",,10,"Single-view 3D shape reconstruction is an important but challenging problem, mainly for two reasons. First, as shape annotation is very expensive to acquire, current methods rely on synthetic data, in which ground-truth 3D annotation is easy to obtain. However, this results in domain adaptation problem when applied to natural images. The second challenge is that there are multiple shapes that can explain a given 2D image. In this paper, we propose a framework to improve over these challenges using adversarial training. On one hand, we impose domain confusion between natural and synthetic image representations to reduce the distribution gap. On the other hand, we impose the reconstruction to be `realistic' by forcing it to lie on a (learned) manifold of realistic object shapes. Our experiments show that these constraints improve performance by a large margin over baseline reconstruction models. We achieve results competitive with the state of the art with a much simpler architecture."
Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles,"Manuel Martin, Alina Roitberg, Monica Haurilet, Matthias Horne, Simon ReiÃ, Michael Voit, Rainer Stiefelhagen","Fraunhofer IOSB, Karlsruhe; Karlsruhe Institute of Technology (KIT)",50.0,germany,50.0,Germany,"We introduce the novel domain-specific Drive&Act benchmark for fine-grained categorization of driver behavior. Our dataset features twelve hours and over 9.6 million frames of people engaged in distractive activities during both, manual and automated driving. We capture color, infrared, depth and 3D body pose information from six views and densely label the videos with a hierarchical annotation scheme, resulting in 83 categories. The key challenges of our dataset are: (1) recognition of fine-grained behavior inside the vehicle cabin; (2) multi-modal activity recognition, focusing on diverse data streams; and (3) a cross view recognition benchmark, where a model handles data from an unfamiliar domain, as sensor type and placement in the cabin can change between vehicles. Finally, we provide challenging benchmarks by adopting prominent methods for video- and body pose-based action recognition.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.pdf,www.driveandact.com,,,main,Poster,https://ieeexplore.ieee.org/document/9009583/,"['Vehicles', 'Three-dimensional displays', 'Task analysis', 'Cameras', 'Benchmark testing', 'Manuals', 'Skeleton']","['Autonomous Vehicles', 'Driver Behavior', 'Driver Behavior Recognition', 'Data Streams', 'Types Of Sensors', 'Action Recognition', 'Hierarchical Strategy', '3D Body', 'Body Pose', 'Convolutional Neural Network', 'Long Short-term Memory', 'Recurrent Neural Network', 'Need For Further Research', 'Level Of Abstraction', 'Recognition Rate', 'Previous Datasets', 'Secondary Task', 'Atomic Units', 'Action Recognition Datasets', '3D Pose', 'Action Recognition Model', 'Near-infrared Camera', 'Hours Of Video', 'Head Pose', '3D Convolution', 'Random Baseline', 'Low Illumination', 'Frontal View']",,121,"We introduce the novel domain-specific Drive&Act benchmark for fine-grained categorization of driver behavior. Our dataset features twelve hours and over 9.6 million frames of people engaged in distractive activities during both, manual and automated driving. We capture color, infrared, depth and 3D body pose information from six views and densely label the videos with a hierarchical annotation scheme, resulting in 83 categories. The key challenges of our dataset are: (1) recognition of fine-grained behavior inside the vehicle cabin; (2) multi-modal activity recognition, focusing on diverse data streams; and (3) a cross view recognition benchmark, where a model handles data from an unfamiliar domain, as sensor type and placement in the cabin can change between vehicles. Finally, we provide challenging benchmarks by adopting prominent methods for video- and body pose-based action recognition."
Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks With Octave Convolution,"Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, Jiashi Feng",National University of Singapore; Facebook AI,50.0,singapore,50.0,USA,"In natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures. Similarly, the output feature maps of a convolution layer can also be seen as a mixture of information at different frequencies. In this work, we propose to factorize the mixed feature maps by their frequencies, and design a novel Octave Convolution (OctConv) operation to store and process feature maps that vary spatially ""slower"" at a lower spatial resolution reducing both memory and computation cost. Unlike existing multi-scale methods, OctConv is formulated as a single, generic, plug-and-play convolutional unit that can be used as a direct replacement of (vanilla) convolutions without any adjustments in the network architecture. It is also orthogonal and complementary to methods that suggest better topologies or reduce channel-wise redundancy like group or depth-wise convolutions. We experimentally show that by simply replacing convolutions with OctConv, we can consistently boost accuracy for both image and video recognition tasks, while reducing memory and computational cost. An OctConv-equipped ResNet-152 can achieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2 GFLOPs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Drop_an_Octave_Reducing_Spatial_Redundancy_in_Convolutional_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Drop_an_Octave_Reducing_Spatial_Redundancy_in_Convolutional_Neural_Networks_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/OctConv,,main,Poster,https://ieeexplore.ieee.org/document/9010309/,"['Convolution', 'Redundancy', 'Spatial resolution', 'Tensile stress', 'Feature extraction', 'Task analysis', 'Kernel']","['Convolutional Neural Network', 'Spatial Redundancy', 'Octave Convolution', 'Spatial Resolution', 'Computational Cost', 'Factorization', 'Convolutional Layers', 'Feature Maps', 'ImageNet', 'Image Recognition', 'Memory Cost', 'Depthwise Convolution', 'Group Convolution', 'Video Recognition', 'Image Classification', 'Feature Representation', 'Input Features', 'Spatial Dimensions', 'Receptive Field', 'Popular Convolutional Neural Networks', 'Video Action Recognition', 'Convolution Operation', 'Extra Memory', 'Convolutional Neural Networks Backbone', 'Improve Recognition Performance', 'Action Recognition', 'Feature Tensor', 'Large Receptive Field', 'Up-sampling Operation']",,412,"In natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures. Similarly, the output feature maps of a convolution layer can also be seen as a mixture of information at different frequencies. In this work, we propose to factorize the mixed feature maps by their frequencies, and design a novel Octave Convolution (OctConv) operation to store and process feature maps that vary spatially “slower” at a lower spatial resolution reducing both memory and computation cost. Unlike existing multi-scale methods, OctConv is formulated as a single, generic, plug-and-play convolutional unit that can be used as a direct replacement of (vanilla) convolutions without any adjustments in the network architecture. It is also orthogonal and complementary to methods that suggest better topologies or reduce channel-wise redundancy like group or depth-wise convolutions. We experimentally show that by simply replacing convolutions with OctConv, we can consistently boost accuracy for both image and video recognition tasks, while reducing memory and computational cost. An OctConv-equipped ResNet-152 can achieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2 GFLOPs."
Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation,"Seungmin Lee, Dongwan Kim, Namil Kim, Seong-Gyun Jeong",CODE42.ai; NAVER LABS; Seoul National Univ.,33.33333333333333,Korea,66.66666666666667,China,"Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at https://github.com/postBG/DTA.pytorch.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Drop_to_Adapt_Learning_Discriminative_Features_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Drop_to_Adapt_Learning_Discriminative_Features_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf,,https://github.com/postBG/DTA.pytorch,,main,Poster,https://ieeexplore.ieee.org/document/9008834/,"['Feature extraction', 'Training', 'Task analysis', 'Adaptation models', 'Linear programming', 'Data models', 'Neurons']","['Domain Adaptation', 'Objective Function', 'Image Classification', 'Feature Representation', 'Adversarial Training', 'Semantic Segmentation Task', 'Domain Discriminator', 'Divergence', 'Hyperparameters', 'Deep Neural Network', 'Convolutional Layers', 'Feature Space', 'Feature Maps', 'Small Datasets', 'Domain Shift', 'Fully-connected Layer', 'Ground Truth Labels', 'Target Domain', 'Target Data', 'Decision Boundary', 'Source Domain', 'ResNet-101 Backbone', 'Semi-supervised Learning', 'Synthetic Images', 'Fully Convolutional Network', 'Number Of Buildings']",,105,"Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at https://github.com/postBG/DTA.pytorch."
Dual Adversarial Inference for Text-to-Image Synthesis,"Qicheng Lao, Mohammad Havaei, Ahmad Pesaranghader, Francis Dutil, Lisa Di Jorio, Thomas Fevens",Concordia University; Dalhousie University; Imagia Inc.,66.66666666666666,Canada,33.33333333333334,USA,"Synthesizing images from a given text description involves engaging two types of information: the content, which includes information explicitly described in the text (e.g., color, composition, etc.), and the style, which is usually not well described in the text (e.g., location, quantity, size, etc.). However, in previous works, it is typically treated as a process of generating images only from the content, i.e., without considering learning meaningful style representations. In this paper, we aim to learn two variables that are disentangled in the latent space, representing content and style respectively. We achieve this by augmenting current text-to-image synthesis frameworks with a dual adversarial inference mechanism. Through extensive experiments, we show that our model learns, in an unsupervised manner, style representations corresponding to certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated on Oxford-102, CUB and COCO datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lao_Dual_Adversarial_Inference_for_Text-to-Image_Synthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lao_Dual_Adversarial_Inference_for_Text-to-Image_Synthesis_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010042/,"['Generators', 'Gallium nitride', 'Image resolution', 'Inference mechanisms', 'Image generation', 'Image color analysis', 'Task analysis']","['Adversarial Inference', 'Types Of Information', 'Meaningful Information', 'Latent Space', 'Textual Descriptions', 'Unsupervised Manner', 'COCO Dataset', 'Inference Mechanism', 'Objective Function', 'Imaging Modalities', 'Latent Variables', 'Information Content', 'Mutual Information', 'Generative Adversarial Networks', 'Independent Component Analysis', 'Image Generation', 'Baseline Methods', 'Model Inference', 'Source Images', 'Quantitative Metrics', 'Fréchet Inception Distance', 'Representational Content', 'Digital Identity', 'Unsupervised Way', 'Source Of Randomness', 'Generative Adversarial Networks Model', 'Conditional Generative Adversarial Network', 'Noise Vector', 'Joint Distribution', 'Conditioning Factors']",,17,"Synthesizing images from a given text description involves engaging two types of information: the content, which includes information explicitly described in the text (e.g., color, composition, etc.), and the style, which is usually not well described in the text (e.g., location, quantity, size, etc.). However, in previous works, it is typically treated as a process of generating images only from the content, i.e., without considering learning meaningful style representations. In this paper, we aim to learn two variables that are disentangled in the latent space, representing content and style respectively. We achieve this by augmenting current text-to-image synthesis frameworks with a dual adversarial inference mechanism. Through extensive experiments, we show that our model learns, in an unsupervised manner, style representations corresponding to certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated on Oxford-102, CUB and COCO datasets."
Dual Attention Matching for Audio-Visual Event Localization,"Yu Wu, Linchao Zhu, Yan Yan, Yi Yang","ReLER, University of Technology Sydney; Texas State University; Baidu Research, ReLER, University of Technology Sydney",100.0,"australia, usa",0.0,,"In this paper, we investigate the audio-visual event localization problem. This task is to localize a visible and audible event in a video. Previous methods first divide a video into short segments, and then fuse visual and acoustic features at the segment level. The duration of these segments is usually short, making the visual and acoustic feature of each segment possibly not well aligned. Direct concatenation of the two features at the segment level can be vulnerable to a minor temporal misalignment of the two signals. We propose a Dual Attention Matching (DAM) module to cover a longer video duration for better high-level event information modeling, while the local temporal information is attained by the global cross-check mechanism. Our premise is that one should watch the whole video to understand the high-level event, while shorter segments should be checked in detail for localization. Specifically, the global feature of one modality queries the local feature in the other modality in a bi-directional way. With temporal co-occurrence encoded between auditory and visual signals, DAM can be readily applied in various audio-visual event localization tasks, e.g., cross-modality localization, supervised event localization. Experiments on the AVE dataset show our method outperforms the state-of-the-art by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010703/,"['Visualization', 'Task analysis', 'Feature extraction', 'Synchronization', 'Acoustics', 'Fuses', 'Video sequences']","['Audiovisual Events', 'Audio-visual Event Localization', 'Local Information', 'Local Features', 'Visual Features', 'Global Features', 'Visual Signals', 'Large Margin', 'Acoustic Features', 'Localization Task', 'Global Mechanism', 'Segment Level', 'Video Events', 'Matching Module', 'Visual Information', 'Visual Representation', 'Final Prediction', 'Event Detection', 'Representation Learning', 'Dot Product', 'Event Categories', 'Visual Content', 'Visual Channels', 'Audio Content', 'Local Segments', 'Global Representation', 'Sequence Embedding', 'Temporal Localization', 'Sound Source', 'Visual Modality']",,108,"In this paper, we investigate the audio-visual event localization problem. This task is to localize a visible and audible event in a video. Previous methods first divide a video into short segments, and then fuse visual and acoustic features at the segment level. The duration of these segments is usually short, making the visual and acoustic feature of each segment possibly not well aligned. Direct concatenation of the two features at the segment level can be vulnerable to a minor temporal misalignment of the two signals. We propose a Dual Attention Matching (DAM) module to cover a longer video duration for better high-level event information modeling, while the local temporal information is attained by the global cross-check mechanism. Our premise is that one should watch the whole video to understand the high-level event, while shorter segments should be checked in detail for localization. Specifically, the global feature of one modality queries the local feature in the other modality in a bi-directional way. With temporal co-occurrence encoded between auditory and visual signals, DAM can be readily applied in various audio-visual event localization tasks, e.g., cross-modality localization, supervised event localization. Experiments on the AVE dataset show our method outperforms the state-of-the-art by a large margin."
Dual Directed Capsule Network for Very Low Resolution Image Recognition,"Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa","IIIT-Delhi, India",100.0,india,0.0,,"Very low resolution (VLR) image recognition corresponds to classifying images with resolution 16x16 or less. Though it has widespread applicability when objects are captured at a very large stand-off distance (e.g. surveillance scenario) or from wide angle mobile cameras, it has received limited attention. This research presents a novel Dual Directed Capsule Network model, termed as DirectCapsNet, for addressing VLR digit and face recognition. The proposed architecture utilizes a combination of capsule and convolutional layers for learning an effective VLR recognition model. The architecture also incorporates two novel loss functions: (i) the proposed HR-anchor loss and (ii) the proposed targeted reconstruction loss, in order to overcome the challenges of limited information content in VLR images. The proposed losses use high resolution images as auxiliary data during training to ""direct"" discriminative feature learning. Multiple experiments for VLR digit classification and VLR face recognition are performed along with comparisons with state-of-the-art algorithms. The proposed DirectCapsNet consistently showcases state-of-the-art results; for example, on the UCCS face database, it shows over 95% face recognition accuracy when 16x16 images are matched with 80x80 images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Singh_Dual_Directed_Capsule_Network_for_Very_Low_Resolution_Image_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_Dual_Directed_Capsule_Network_for_Very_Low_Resolution_Image_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010908/,"['Image resolution', 'Image recognition', 'Image reconstruction', 'Face recognition', 'Face', 'Feature extraction', 'Training']","['Low Resolution', 'Image Recognition', 'Recognition Network', 'Low Recognition', 'Capsule Network', 'High-resolution Images', 'Convolutional Layers', 'Information Content', 'Feature Learning', 'Face Recognition', 'Discriminative Features', 'Recognition Accuracy', 'Reconstruction Loss', 'Optical Character Recognition', 'Standoff Distance', 'Discriminative Feature Learning', 'Capsule Layer', 'Training Time', 'Input Image', 'Super-resolution', 'Marginal Loss', 'High-resolution Features', 'Final Layer', 'Recognition Task', 'Active Vectors', 'Improve Recognition Performance', 'Metric Learning', 'Meaningful Features', 'Network Reconstruction', 'Input Samples']",,45,"Very low resolution (VLR) image recognition corresponds to classifying images with resolution 16×16 or less. Though it has widespread applicability when objects are captured at a very large stand-off distance (e.g. surveillance scenario) or from wide angle mobile cameras, it has received limited attention. This research presents a novel Dual Directed Capsule Network model, termed as DirectCapsNet, for addressing VLR digit and face recognition. The proposed architecture utilizes a combination of capsule and convolutional layers for learning an effective VLR recognition model. The architecture also incorporates two novel loss functions: (i) the proposed HR-anchor loss and (ii) the proposed targeted reconstruction loss, in order to overcome the challenges of limited information content in VLR images. The proposed losses use high resolution images as auxiliary data during training to ""direct"" discriminative feature learning. Multiple experiments for VLR digit classification and VLR face recognition are performed along with comparisons with state-of-the-art algorithms. The proposed DirectCapsNet consistently showcases state-of-the-art results; for example, on the UCCS face database, it shows over 95% face recognition accuracy when 16×16 images are matched with 80×80 images."
Dual Student: Breaking the Limits of the Teacher in Semi-Supervised Learning,"Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, Rynson W.H. Lau",City University of Hong Kong; SenseTime Research,50.0,Hong Kong,50.0,China,"Recently, consistency-based methods have achieved state-of-the-art results in semi-supervised learning (SSL). These methods always involve two roles, an explicit or implicit teacher model and a student model, and penalize predictions under different perturbations by a consistency constraint. However, the weights of these two roles are tightly coupled since the teacher is essentially an exponential moving average (EMA) of the student. In this work, we show that the coupled EMA teacher causes a performance bottleneck. To address this problem, we introduce Dual Student, which replaces the teacher with another student. We also define a novel concept, stable sample, following which a stabilization constraint is designed for our structure to be trainable. Further, we discuss two variants of our method, which produce even higher performance. Extensive experiments show that our method improves the classification performance significantly on several main SSL benchmarks. Specifically, it reduces the error rate of the 13-layer CNN from 16.84% to 12.39% on CIFAR-10 with 1k labels and from 34.10% to 31.56% on CIFAR-100 with 10k labels. In addition, our method also achieves a clear improvement in domain adaptation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ke_Dual_Student_Breaking_the_Limits_of_the_Teacher_in_Semi-Supervised_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Dual_Student_Breaking_the_Limits_of_the_Teacher_in_Semi-Supervised_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009457/,"['Predictive models', 'Training', 'Perturbation methods', 'Task analysis', 'Computational modeling', 'Semisupervised learning', 'Benchmark testing']","['Semi-supervised Learning', 'Convolutional Neural Network', 'Teacher Model', 'Domain Adaptation', 'Student Model', 'Performance Bottleneck', 'Consistency Constraint', 'Mean Square Error', 'Deep Learning', 'Random Noise', 'ImageNet', 'Generative Adversarial Networks', 'Independent Model', 'RGB Images', 'Target Domain', 'Unlabeled Data', 'Ablation Experiments', 'Mean Education', 'Confidence Threshold', 'Number Of Buildings', 'Temporal Model', 'Source Domain', 'Smoothness Assumption', 'Confirmation Bias']",,131,"Recently, consistency-based methods have achieved state-of-the-art results in semi-supervised learning (SSL). These methods always involve two roles, an explicit or implicit teacher model and a student model, and penalize predictions under different perturbations by a consistency constraint. However, the weights of these two roles are tightly coupled since the teacher is essentially an exponential moving average (EMA) of the student. In this work, we show that the coupled EMA teacher causes a performance bottleneck. To address this problem, we introduce Dual Student, which replaces the teacher with another student. We also define a novel concept, stable sample, following which a stabilization constraint is designed for our structure to be trainable. Further, we discuss two variants of our method, which produce even higher performance. Extensive experiments show that our method improves the classification performance significantly on several main SSL benchmarks. Specifically, it reduces the error rate of the 13-layer CNN from 16.84% to 12.39% on CIFAR-10 with 1k labels and from 34.10% to 31.56% on CIFAR-100 with 10k labels. In addition, our method also achieves a clear improvement in domain adaptation."
Dynamic Anchor Feature Selection for Single-Shot Object Detection,"Shuai Li, Lingxiao Yang, Jianqiang Huang, Xian-Sheng Hua, Lei Zhang","DAMO Academy, Alibaba Group; The Hong Kong Polytechnic University, DAMO Academy, Alibaba Group; The Hong Kong Polytechnic University",100.0,"Hong Kong, china",0.0,,"The design of anchors is critical to the performance of one-stage detectors. Recently, the anchor refinement module (ARM) has been proposed to adjust the initialization of default anchors, providing the detector a better anchor reference. However, this module brings another problem: all pixels at a feature map have the same receptive field while the anchors associated with each pixel have different positions and sizes. This discordance may lead to a less effective detector. In this paper, we present a dynamic feature selection operation to select new pixels in a feature map for each refined anchor received from the ARM. The pixels are selected based on the new anchor position and size so that the receptive filed of these pixels can fit the anchor areas well, which makes the detector, especially the regression part, much easier to optimize. Furthermore, to enhance the representation ability of selected feature pixels, we design a bidirectional feature fusion module by combining features from early and deep layers. Extensive experiments on both PASCAL VOC and COCO demonstrate the effectiveness of our dynamic anchor feature selection (DAFS) operation. For the case of high IoU threshold, our DAFS can improve the mAP by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009487/,"['Detectors', 'Feature extraction', 'Object detection', 'Proposals', 'Head', 'Shape', 'Computer vision']","['Dynamic Characteristics', 'Object Detection', 'Dynamic Selection', 'Anchor Feature', 'Dynamic Feature Selection', 'Feature Maps', 'Receptive Field', 'One-stage Detectors', 'Loss Function', 'Convolutional Neural Network', 'Aspect Ratio', 'Fast Speed', 'Class Labels', 'Bounding Box', 'Feature Points', 'Input Size', 'Detection Framework', 'Extra Layer', 'Feature Pyramid', 'Distance Vector', 'Fusion Block', 'Two-stage Detectors', 'Detection Head', 'Inference Speed', 'Stack Of Convolutional Layers', 'Score Map', 'Feature Pyramid Network', 'Ground-truth Box']",,49,"The design of anchors is critical to the performance of one-stage detectors. Recently, the anchor refinement module (ARM) has been proposed to adjust the initialization of default anchors, providing the detector a better anchor reference. However, this module brings another problem: all pixels at a feature map have the same receptive field while the anchors associated with each pixel have different positions and sizes. This discordance may lead to a less effective detector. In this paper, we present a dynamic feature selection operation to select new pixels in a feature map for each refined anchor received from the ARM. The pixels are selected based on the new anchor position and size so that the receptive filed of these pixels can fit the anchor areas well, which makes the detector, especially the regression part, much easier to optimize. Furthermore, to enhance the representation ability of selected feature pixels, we design a bidirectional feature fusion module by combining features from early and deep layers. Extensive experiments on both PASCAL VOC and COCO demonstrate the effectiveness of our dynamic anchor feature selection (DAFS) operation. For the case of high IoU threshold, our DAFS can improve the mAP by a large margin."
Dynamic Context Correspondence Network for Semantic Alignment,"Shuaiyi Huang, Qiuyue Wang, Songyang Zhang, Shipeng Yan, Xuming He",ShanghaiTech University,100.0,china,0.0,,"Establishing semantic correspondence is a core problem in computer vision and remains challenging due to large intra-class variations and lack of annotated data. In this paper, we aim to incorporate global semantic context in a flexible manner to overcome the limitations of prior work that relies on local semantic representations. To this end, we first propose a context-aware semantic representation that incorporates spatial layout for robust matching against local ambiguities. We then develop a novel dynamic fusion strategy based on attention mechanism to weave the advantages of both local and context features by integrating semantic cues from multiple scales. We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet). To train the network, we adopt a multi-auxiliary task loss to improve the efficiency of our weakly-supervised learning procedure. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including PF-Pascal, PF-Willow, and TSS, demonstrating its effectiveness and generality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Dynamic_Context_Correspondence_Network_for_Semantic_Alignment_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Dynamic_Context_Correspondence_Network_for_Semantic_Alignment_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010314/,"['Semantics', 'Correlation', 'Task analysis', 'Robustness', 'Feature extraction', 'Computer vision', 'Pattern matching']","['Dynamic Network', 'Fluidic', 'Lack Of Data', 'Deep Network', 'Superior Performance', 'Computer Vision', 'Local Features', 'Multiple Scales', 'Attention Mechanism', 'Global Context', 'Dynamic Strategy', 'Computer Vision Problems', 'Semantic Cues', 'Task Loss', 'Dynamic Fusion', 'Convolutional Neural Network', 'Feature Representation', 'Image Pairs', 'Local Image', 'Semantic Features', 'Spatial Context', 'Auxiliary Loss', 'Semantic Matching', 'Spatial Encoding', 'Correlation Network', 'Repetitive Patterns', 'Convolutional Features', 'Spatial Cues', 'Conv Layer', 'Local Cues']",,61,"Establishing semantic correspondence is a core problem in computer vision and remains challenging due to large intra-class variations and lack of annotated data. In this paper, we aim to incorporate global semantic context in a flexible manner to overcome the limitations of prior work that relies on local semantic representations. To this end, we first propose a context-aware semantic representation that incorporates spatial layout for robust matching against local ambiguities. We then develop a novel dynamic fusion strategy based on attention mechanism to weave the advantages of both local and context features by integrating semantic cues from multiple scales. We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet). To train the network, we adopt a multi-auxiliary task loss to improve the efficiency of our weakly-supervised learning procedure. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including PF-Pascal, PF-Willow, and TSS, demonstrating its effectiveness and generality."
Dynamic Curriculum Learning for Imbalanced Data Classification,"Yiru Wang, Weihao Gan, Jie Yang, Wei Wu, Junjie Yan",SenseTime Group Limited,100.0,usa,0.0,,"Human attribute analysis is a challenging task in the field of computer vision. One of the significant difficulties is brought from largely imbalance-distributed data. Conventional techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to adaptively adjust the sampling strategy and loss weight in each batch, which results in better ability of generalization and discrimination. Inspired by curriculum learning, DCL consists of two-level curriculum schedulers: (1) sampling scheduler which manages the data distribution not only from imbalance to balance but also from easy to hard; (2) loss scheduler which controls the learning importance between classification and metric learning loss. With these two schedulers, we achieve state-of-the-art performance on the widely used face attribute dataset CelebA and pedestrian attribute dataset RAP.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Dynamic_Curriculum_Learning_for_Imbalanced_Data_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Dynamic_Curriculum_Learning_for_Imbalanced_Data_Classification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008373/,"['Training', 'Task analysis', 'Measurement', 'Dynamic scheduling', 'DSL', 'Face', 'Data models']","['Imbalanced Data', 'Curriculum Learning', 'Dynamic Curriculum Learning', 'Data Distribution', 'Metric Learning', 'Learning Loss', 'Field Task', 'Cost-sensitive Learning', 'Deep Learning', 'Convolutional Neural Network', 'Positive Samples', 'Learning Framework', 'Cross-entropy Loss', 'Classification Of Samples', 'Target Distribution', 'Balanced Distribution', 'Minority Class', 'Learning Speed', 'Imbalanced Distribution', 'Imbalance Ratio', 'Triplet Loss', 'Easy Samples', 'Minority Samples', 'Samples In The Feature Space', 'Imbalanced Learning', 'Imbalance In Levels', 'Current Batch', 'Minority Class Samples', 'Major Classes', 'Sampling Weights']",,142,"Human attribute analysis is a challenging task in the field of computer vision. One of the significant difficulties is brought from largely imbalance-distributed data. Conventional techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to adaptively adjust the sampling strategy and loss weight in each batch, which results in better ability of generalization and discrimination. Inspired by curriculum learning, DCL consists of two-level curriculum schedulers: (1) sampling scheduler which manages the data distribution not only from imbalance to balance but also from easy to hard; (2) loss scheduler which controls the learning importance between classification and metric learning loss. With these two schedulers, we achieve state-of-the-art performance on the widely used face attribute dataset CelebA and pedestrian attribute dataset RAP."
Dynamic Graph Attention for Referring Expression Comprehension,"Sibei Yang, Guanbin Li, Yizhou Yu",Sun Yat-sen University; The University of Hong Kong; The University of Hong Kong and Deepwise AI Lab,100.0,"China, Hong Kong",0.0,,"Referring expression comprehension aims to locate the object instance described by a natural language referring expression in an image. This task is compositional and inherently requires visual reasoning on top of the relationships among the objects in the image. Meanwhile, the visual reasoning process is guided by the linguistic structure of the referring expression. However, existing approaches treat the objects in isolation or only explore the first-order relationships between objects without being aligned with the potential complexity of the expression. Thus it is hard for them to adapt to the grounding of complex referring expressions. In this paper, we explore the problem of referring expression comprehension from the perspective of language-driven visual reasoning, and propose a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. In particular, we construct a graph for the image with the nodes and edges corresponding to the objects and their relationships respectively, propose a differential analyzer to predict a language-guided visual reasoning process, and perform stepwise reasoning on top of the graph to update the compound object representation at every node. Experimental results demonstrate that the proposed method can not only significantly surpass all existing state-of-the-art algorithms across three common benchmark datasets, but also generate interpretable visual evidences for stepwise locating the objects referred to in complex language descriptions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010701/,"['Cognition', 'Visualization', 'Compounds', 'Linguistics', 'Computers', 'Differential equations', 'Object recognition']","['Graph Attention', 'Referring Expression Comprehension', 'Differential Analysis', 'Natural Language', 'Visual Processing', 'Dynamic Network', 'Image Object', 'Reasoning Process', 'Common Datasets', 'Graph Attention Network', 'Visual Reasoning', 'Time Step', 'Visual Features', 'Image Regions', 'Attention Mechanism', 'Trainable Parameters', 'Edge Weights', 'Nodes In The Graph', 'Visual Object', 'Word Embedding', 'Ground Truth Object', 'Reasonable Steps', 'Matching Score', 'Triplet Loss', 'Types Of Edges', 'Individual Objects', 'Distribution Of Attention', 'Node Weights', 'Object Proposals', 'Current Time Step']",,129,"Referring expression comprehension aims to locate the object instance described by a natural language referring expression in an image. This task is compositional and inherently requires visual reasoning on top of the relationships among the objects in the image. Meanwhile, the visual reasoning process is guided by the linguistic structure of the referring expression. However, existing approaches treat the objects in isolation or only explore the first-order relationships between objects without being aligned with the potential complexity of the expression. Thus it is hard for them to adapt to the grounding of complex referring expressions. In this paper, we explore the problem of referring expression comprehension from the perspective of language-driven visual reasoning, and propose a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. In particular, we construct a graph for the image with the nodes and edges corresponding to the objects and their relationships respectively, propose a differential analyzer to predict a language-guided visual reasoning process, and perform stepwise reasoning on top of the graph to update the compound object representation at every node. Experimental results demonstrate that the proposed method can not only significantly surpass all existing state-of-the-art algorithms across three common benchmark datasets, but also generate interpretable visual evidences for stepwise locating the objects referred to in complex language descriptions."
Dynamic Kernel Distillation for Efficient Pose Estimation in Videos,"Xuecheng Nie, Yuncheng Li, Linjie Luo, Ning Zhang, Jiashi Feng","DawnLight Technologies Inc.; ByteDance AI Lab; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Snap Inc.",25.0,singapore,75.0,USA,"Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Dynamic_Kernel_Distillation_for_Efficient_Pose_Estimation_in_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Dynamic_Kernel_Distillation_for_Efficient_Pose_Estimation_in_Videos_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008554/,"['Kernel', 'Pose estimation', 'Videos', 'Training', 'Convolution', 'Computational modeling', 'Knowledge engineering']","['Pose Estimation', 'Estimation In Videos', 'Video Pose Estimation', 'Large Networks', 'Video Frames', 'Current Frame', 'Matching Procedure', 'Adversarial Training', 'Previous Frame', 'Body Joints', 'Human Pose Estimation', 'Human Pose', 'Temporal Cues', 'Simple Convolution', 'Convolutional Layers', 'Feature Maps', 'Temporal Dimension', 'Video Clips', 'Optical Flow', 'Consecutive Frames', 'Confidence Map', 'Convolutional Long Short-term Memory', 'Reference Distance', 'Knowledge Of Patterns', 'Motion Blur', 'Deconvolutional Layers']",,44,"Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy."
Dynamic Multi-Scale Filters for Semantic Segmentation,"Junjun He, Zhongying Deng, Yu Qiao","ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China and Shanghai Jiao Tong University; ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China",100.0,"China, china",0.0,,"Multi-scale representation provides an effective way to address scale variation of objects and stuff in semantic segmentation. Previous works construct multi-scale representation by utilizing different filter sizes, expanding filter sizes with dilated filters or pooling grids, and the parameters of these filters are fixed after training. These methods often suffer from heavy computational cost or have more parameters, and are not adaptive to the input image during inference. To address these problems, this paper proposes a Dynamic Multi-scale Network (DMNet) to adaptively capture multi-scale contents for predicting pixel-level semantic labels. DMNet is composed of multiple Dynamic Convolutional Modules (DCMs) arranged in parallel, each of which exploits context-aware filters to estimate semantic representation for a specific scale. The outputs of multiple DCMs are further integrated for final segmentation. We conduct extensive experiments to evaluate our DMNet on three challenging semantic segmentation and scene parsing datasets, PASCAL VOC 2012, Pascal-Context, and ADE20K. DMNet achieves a new record 84.4% mIoU on PASCAL VOC 2012 test set without MS COCO pre-trained and post-processing, and also obtains state-of-the-art performance on Pascal-Context and ADE20K.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Dynamic_Multi-Scale_Filters_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Dynamic_Multi-Scale_Filters_for_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010282/,"['Kernel', 'Convolution', 'Feature extraction', 'Semantics', 'Image segmentation', 'Training', 'Computational efficiency']","['Semantic Segmentation', 'Dynamic Filter', 'Input Image', 'Scale Variation', 'Filter Size', 'Dynamic Mode', 'Specific Scale', 'Multi-scale Representation', 'Heavy Computational Cost', 'Training Set', 'Convolutional Neural Network', 'Feature Maps', 'Feature Representation', 'Receptive Field', 'Deep Convolutional Neural Network', 'Segmentation Results', 'Multi-scale Features', 'Fully Convolutional Network', 'Specific Representation', 'Atrous Convolution', 'Atrous Spatial Pyramid Pooling', 'Pyramid Pooling Module', 'Dilation Rate', 'Depthwise Convolution', 'Convolutional Neural Networks Backbone', 'High-level Semantics', 'ResNet-101 Backbone', '3rd Row', 'Large-scale Object', 'Small Objects']",,181,"Multi-scale representation provides an effective way to address scale variation of objects and stuff in semantic segmentation. Previous works construct multi-scale representation by utilizing different filter sizes, expanding filter sizes with dilated filters or pooling grids, and the parameters of these filters are fixed after training. These methods often suffer from heavy computational cost or have more parameters, and are not adaptive to the input image during inference. To address these problems, this paper proposes a Dynamic Multi-scale Network (DMNet) to adaptively capture multi-scale contents for predicting pixel-level semantic labels. DMNet is composed of multiple Dynamic Convolutional Modules (DCMs) arranged in parallel, each of which exploits context-aware filters to estimate semantic representation for a specific scale. The outputs of multiple DCMs are further integrated for final segmentation. We conduct extensive experiments to evaluate our DMNet on three challenging semantic segmentation and scene parsing datasets, PASCAL VOC 2012, Pascal-Context, and ADE20K. DMNet achieves a new record 84.4% mIoU on PASCAL VOC 2012 test set without MS COCO pre-trained and post-processing, and also obtains state-of-the-art performance on Pascal-Context and ADE20K."
Dynamic PET Image Reconstruction Using Nonnegative Matrix Factorization Incorporated With Deep Image Prior,"Tatsuya Yokota, Kazuya Kawai, Muneyuki Sakata, Yuichi Kimura, Hidekata Hontani","Kindai University, Wakayama, Japan; Nagoya Institute of Technology, Nagoya, Japan; Tokyo Metropolitan Institute of Gerontology, Tokyo, Japan",100.0,"Japan, japan",0.0,,We propose a method that reconstructs dynamic positron emission tomography (PET) images from given sinograms by using non-negative matrix factorization (NMF) incorporated with a deep image prior (DIP) for appropriately constraining the spatial patterns of resultant images. The proposed method can reconstruct dynamic PET images with higher signal-to-noise ratio (SNR) and blindly decompose an image matrix into pairs of spatial and temporal factors. The former represent homogeneous tissues with different kinetic parameters and the latter represent the time activity curves that are observed in the corresponding homogeneous tissues. We employ U-Nets combined in parallel for DIP and each of the U-nets is used to extract each spatial factor decomposed from the data matrix. Experimental results show that the proposed method outperforms conventional methods and can extract spatial factors that represent the homogeneous tissues.,,http://openaccess.thecvf.com/content_ICCV_2019/html/Yokota_Dynamic_PET_Image_Reconstruction_Using_Nonnegative_Matrix_Factorization_Incorporated_With_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yokota_Dynamic_PET_Image_Reconstruction_Using_Nonnegative_Matrix_Factorization_Incorporated_With_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010878/,"['Image reconstruction', 'Positron emission tomography', 'Electronics packaging', 'Signal to noise ratio', 'Kinetic theory', 'Radon', 'Sparse matrices']","['Image Reconstruction', 'Positive Matrix', 'Positron Emission Tomography Imaging', 'Dynamic Imaging', 'Non-negative Matrix Factorization', 'Prior Imaging', 'Dynamic Positron Emission Tomography', 'Dynamic Reconstruction', 'Deep Image Prior', 'Signal-to-noise', 'Spatial Patterns', 'Kinetic Parameters', 'Image Pattern', 'Non-negative Factorization', 'Image Matrix', 'Temporal Factors', 'Time-activity Curves', 'Pairs Of Factors', 'Convolutional Neural Network', 'Simulated Data', 'Filtered Back Projection', 'Convolutional Neural Network Architecture', 'Total Variation Regularization', 'Quadratic Variation', 'Expectation Maximization', 'Case Method', 'Radon Transform', 'Spatial Basis', 'Streak Artifacts', 'Image Signal-to-noise Ratio']",,36,We propose a method that reconstructs dynamic positron emission tomography (PET) images from given sinograms by using non-negative matrix factorization (NMF) incorporated with a deep image prior (DIP) for appropriately constraining the spatial patterns of resultant images. The proposed method can reconstruct dynamic PET images with higher signal-to-noise ratio (SNR) and blindly decompose an image matrix into pairs of spatial and temporal factors. The former represent homogeneous tissues with different kinetic parameters and the latter represent the time activity curves that are observed in the corresponding homogeneous tissues. We employ U-Nets combined in parallel for DIP and each of the U-nets is used to extract each spatial factor decomposed from the data matrix. Experimental results show that the proposed method outperforms conventional methods and can extract spatial factors that represent the homogeneous tissues.
Dynamic Points Agglomeration for Hierarchical Point Sets Learning,"Jinxian Liu, Bingbing Ni, Caiyuan Li, Jiancheng Yang, Qi Tian","Huawei Noah's Ark Lab; Shanghai Jiao Tong University, Shanghai 200240, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China",66.66666666666666,"China, china",33.33333333333334,China,"Many previous works on point sets learning achieve excellent performance with hierarchical architecture. Their strategies towards points agglomeration, however, only perform points sampling and grouping in original Euclidean space in a fixed way. These heuristic and task-irrelevant strategies severely limit their ability to adapt to more varied scenarios. To this end, we develop a novel hierarchical point sets learning architecture, with dynamic points agglomeration. By exploiting the relation of points in semantic space, a module based on graph convolution network is designed to learn a soft points cluster agglomeration. We construct a hierarchical architecture that gradually agglomerates points by stacking this learnable and lightweight module. In contrast to fixed points agglomeration strategy, our method can handle more diverse situations robustly and efficiently. Moreover, we propose a parameter sharing scheme for reducing memory usage and computational burden induced by the agglomeration module. Extensive experimental results on several point cloud analytic tasks, including classification and segmentation, well demonstrate the superior performance of our dynamic hierarchical learning framework over current state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010020/,"['Three-dimensional displays', 'Computer architecture', 'Convolution', 'Task analysis', 'Semantics', 'Two dimensional displays', 'Machine learning']","['Hierarchical Set', 'Agglomerates', 'Point Cloud', 'Euclidean Space', 'Memory Usage', 'Graph Convolutional Network', 'Hierarchical Architecture', 'Graph Convolution', 'Semantic Space', 'Heuristic Strategy', 'Hierarchical Learning', 'Convolutional Neural Network', 'Multilayer Perceptron', 'Similarity Matrix', 'Regular Grid', '3D Data', 'Feature Points', 'Feature Matrix', 'Dynamic Mode', 'Max-pooling Layer', 'Architecture For Segmentation', '3D Point Cloud', 'Architecture For Classification', 'Multi-level Features', 'Partial Point', 'Input Point', '3D Convolution', 'Scene Segmentation', 'Input Point Cloud', 'Work Domain']",,70,"Many previous works on point sets learning achieve excellent performance with hierarchical architecture. Their strategies towards points agglomeration, however, only perform points sampling and grouping in original Euclidean space in a fixed way. These heuristic and task-irrelevant strategies severely limit their ability to adapt to more varied scenarios. To this end, we develop a novel hierarchical point sets learning architecture, with dynamic points agglomeration. By exploiting the relation of points in semantic space, a module based on graph convolution network is designed to learn a soft points cluster agglomeration. We construct a hierarchical architecture that gradually agglomerates points by stacking this learnable and lightweight module. In contrast to fixed points agglomeration strategy, our method can handle more diverse situations robustly and efficiently. Moreover, we propose a parameter sharing scheme for reducing memory usage and computational burden induced by the agglomeration module. Extensive experimental results on several point cloud analytic tasks, including classification and segmentation, well demonstrate the superior performance of our dynamic hierarchical learning framework over current state-of-the-art methods."
Dynamic-Net: Tuning the Objective Without Re-Training for Synthesis Tasks,"Alon Shoshan, Roey Mechrez, Lihi Zelnik-Manor","Technion, Israel; Technion & Alibaba Group",100.0,israel,0.0,,"One of the key ingredients for successful optimization of modern CNNs is identifying a suitable objective. To date, the objective is fixed a-priori at training time, and any variation to it requires re-training a new network. In this paper we present a first attempt at alleviating the need for re-training. Rather than fixing the network at training time, we train a ""Dynamic-Net"" that can be modified at inference time. Our approach considers an ""objective-space"" as the space of all linear combinations of two objectives, and the Dynamic-Net is emulating the traversing of this objective-space at test-time, without any further training. We show that this upgrades pre-trained networks by providing an out-of-learning extension, while maintaining the performance quality. The solution we propose is fast and allows a user to interactively modify the network, in real-time, in order to obtain the result he/she desires. We show the benefits of such an approach via several different applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shoshan_Dynamic-Net_Tuning_the_Objective_Without_Re-Training_for_Synthesis_Tasks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shoshan_Dynamic-Net_Tuning_the_Objective_Without_Re-Training_for_Synthesis_Tasks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008765/,"['Training', 'Interpolation', 'Tuning', 'Aerospace electronics', 'Image generation', 'Image resolution', 'Face']","['Training Time', 'Inference Time', 'Pre-trained Network', 'Latent Space', 'Image Generation', 'Deep Features', 'Loss Of Content', 'Features In Order', 'Output Image', 'Multiple Terms', 'Objective Space', 'Loss Term', 'Latent Representation', 'Hair Color', 'Working Points', 'Variety Of Objects', 'Intermediate Points', 'Perceptual Loss', 'Style Transfer', 'Style Image', 'Simple Interpolation', 'Official Implementation', 'Object Choice', 'Image Space']",,19,"One of the key ingredients for successful optimization of modern CNNs is identifying a suitable objective. To date, the objective is fixed a-priori at training time, and any variation to it requires re-training a new network. In this paper we present a first attempt at alleviating the need for re-training. Rather than fixing the network at training time, we train a ``Dynamic-Net'' that can be modified at inference time. Our approach considers an ``objective-space'' as the space of all linear combinations of two objectives, and the Dynamic-Net is emulating the traversing of this objective-space at test-time, without any further training. We show that this upgrades pre-trained networks by providing an out-of-learning extension, while maintaining the performance quality. The solution we propose is fast and allows a user to interactively modify the network, in real-time, in order to obtain the result he/she desires. We show the benefits of such an approach via several different applications."
DynamoNet: Dynamic Action and Motion Network,"Ali Diba, Vivek Sharma, Luc Van Gool, Rainer Stiefelhagen","CVL, ETH Zürich; CV:HCI, KIT; ESAT-PSI, KU Leuven",66.66666666666666,"Switzerland, belgium",33.33333333333334,Sweden,"In this paper, we are interested in self-supervised learning the motion cues in videos using dynamic motion filters for a better motion representation to finally boost human action recognition in particular. Thus far, the vision community has focused on spatio-temporal approaches using standard filters, rather we here propose dynamic filters that adaptively learn the video-specific internal motion representation by predicting the short-term future frames. We name this new motion representation, as dynamic motion representation (DMR) and is embedded inside of 3D convolutional network as a new layer, which captures the visual appearance and motion dynamics throughout entire video clip via end-to-end network learning. Simultaneously, we utilize these motion representation to enrich video classification. We have designed the frame prediction task as an auxiliary task to empower the classification problem. With these overall objectives, to this end, we introduce a novel unified spatio-temporal 3D-CNN architecture (DynamoNet) that jointly optimizes the video classification and learning motion representation by predicting future frames as a multi-task learning problem. We conduct experiments on challenging human action datasets: Kinetics 400, UCF101, HMDB51. The experiments using the proposed DynamoNet show promising results on all the datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Diba_DynamoNet_Dynamic_Action_and_Motion_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Diba_DynamoNet_Dynamic_Action_and_Motion_Network_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010710/,"['Dynamics', 'Videos', 'Three-dimensional displays', 'Kernel', 'Task analysis', 'Feature extraction', 'Convolution']","['Dynamic Motion', 'Motion Network', 'Video Clips', 'Video Analysis', 'Action Recognition', 'Joint Optimization', 'Self-supervised Learning', 'Standard Filter', 'Dynamic Representation', 'Motor Representations', '3D Convolution', 'Vision Community', 'Human Activity Recognition', 'Motion Cues', 'Frame Prediction', 'Future Frames', 'Dynamic Filter', 'Temporal Dimension', 'Temporal Information', 'Representation Learning', 'Action Classes', 'Temporal Convolution', 'Motion Features', 'Action Recognition Datasets', 'Motion Information', 'Optical Flow', 'Video Frames', 'Fully-connected Layer', 'Technical Approaches', 'Dynamic Datasets']",,79,"In this paper, we are interested in self-supervised learning the motion cues in videos using dynamic motion filters for a better motion representation to finally boost human action recognition in particular. Thus far, the vision community has focused on spatio-temporal approaches using standard filters, rather we here propose dynamic filters that adaptively learn the video-specific internal motion representation by predicting the short-term future frames. We name this new motion representation, as dynamic motion representation (DMR) and is embedded inside of 3D convolutional network as a new layer, which captures the visual appearance and motion dynamics throughout entire video clip via end-to-end network learning. Simultaneously, we utilize these motion representation to enrich video classification. We have designed the frame prediction task as an auxiliary task to empower the classification problem. With these overall objectives, to this end, we introduce a novel unified spatio-temporal 3D-CNN architecture (DynamoNet) that jointly optimizes the video classification and learning motion representation by predicting future frames as a multi-task learning problem. We conduct experiments on challenging human action datasets: Kinetics 400, UCF101, HMDB51. The experiments using the proposed DynamoNet show promising results on all the datasets."
EGNet: Edge Guidance Network for Salient Object Detection,"Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, Ming-Ming Cheng","TKLNDST, CS, Nankai University",100.0,China,0.0,,"Fully convolutional neural networks (FCNs) have shown their advantages in the salient object detection task. However, most existing FCNs-based methods still suffer from coarse object boundaries. In this paper, to solve this problem, we focus on the complementarity between salient edge information and salient object information. Accordingly, we present an edge guidance network (EGNet) for salient object detection with three steps to simultaneously model these two kinds of complementary information in a single network. In the first step, we extract the salient object features by a progressive fusion way. In the second step, we integrate the local edge information and global location information to obtain the salient edge features. Finally, to sufficiently leverage these complementary features, we couple the same salient edge features with salient object features at various resolutions. Benefiting from the rich edge information and location information in salient edge features, the fused features can help locate salient objects, especially their boundaries more accurately. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on six widely used datasets without any pre-processing and post-processing. The source code is available at http: //mmcheng.net/egnet/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_EGNet_Edge_Guidance_Network_for_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_EGNet_Edge_Guidance_Network_for_Salient_Object_Detection_ICCV_2019_paper.pdf,,http://mmcheng.net/egnet/,,main,Poster,https://ieeexplore.ieee.org/document/9008371/,"['Image edge detection', 'Feature extraction', 'Object detection', 'Task analysis', 'Convolutional neural networks', 'Semantics', 'Fuses']","['Salient Object', 'Object Detection Network', 'Salient Object Detection', 'Edge Guidance', 'Convolutional Network', 'Convolutional Neural Network', 'Local Information', 'Single Network', 'Object Boundaries', 'Salient Information', 'Edge Information', 'Complementary Features', 'Semantic', 'Convolutional Layers', 'Feature Maps', 'Image Object', 'Precision And Recall', 'Receptive Field', 'Explicit Model', 'Handcrafted Features', 'Saliency Map', 'Prediction Map', 'Global Contrast', 'U-Net Architecture', 'Ground Truth Map', 'ReLU Layer', 'Non-local Features', 'Complementary Models', 'Image Patches', 'Backbone Network']",,784,"Fully convolutional neural networks (FCNs) have shown their advantages in the salient object detection task. However, most existing FCNs-based methods still suffer from coarse object boundaries. In this paper, to solve this problem, we focus on the complementarity between salient edge information and salient object information. Accordingly, we present an edge guidance network (EGNet) for salient object detection with three steps to simultaneously model these two kinds of complementary information in a single network. In the first step, we extract the salient object features by a progressive fusion way. In the second step, we integrate the local edge information and global location information to obtain the salient edge features. Finally, to sufficiently leverage these complementary features, we couple the same salient edge features with salient object features at various resolutions. Benefiting from the rich edge information and location information in salient edge features, the fused features can help locate salient objects, especially their boundaries more accurately. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on six widely used datasets without any pre-processing and post-processing. The source code is available at http: //mmcheng.net/egnet/."
ELF: Embedded Localisation of Features in Pre-Trained CNN,"Assia Benbihi, Matthieu Geist, CÃ©dric Pradalier","GeorgiaTech Lorraine-UMI2958, GeorgiaTech-CNRS, Metz, France; Google Research, Brain Team; UMI2958 GeorgiaTech-CNRS, Centrale Sup ´elec, Universit ´e Paris-Saclay, Metz, France",33.33333333333333,France,66.66666666666667,France,"This paper introduces a novel feature detector based only on information embedded inside a CNN trained on standard tasks (e.g. classification). While previous works already show that the features of a trained CNN are suitable descriptors, we show here how to extract the feature locations from the network to build a detector. This information is computed from the gradient of the feature map with respect to the input image. This provides a saliency map with local maxima on relevant keypoint locations. Contrary to recent CNN-based detectors, this method requires neither supervised training nor finetuning. We evaluate how repeatable and how 'matchable' the detected keypoints are with the repeatability and matching scores. Matchability is measured with a simple descriptor introduced for the sake of the evaluation. This novel detector reaches similar performances on the standard evaluation HPatches dataset, as well as comparable robustness against illumination and viewpoint changes on Webcam and photo-tourism images. These results show that a CNN trained on a standard task embeds feature location information that is as relevant as when the CNN is specifically trained for feature detection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Benbihi_ELF_Embedded_Localisation_of_Features_in_Pre-Trained_CNN_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Benbihi_ELF_Embedded_Localisation_of_Features_in_Pre-Trained_CNN_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010682/,"['Feature extraction', 'Detectors', 'Ground penetrating radar', 'Geophysical measurement techniques', 'Training', 'Standards', 'Task analysis']","['Convolutional Neural Network', 'Convolutional Neural Network Features', 'Pre-trained Convolutional Neural Network', 'Illumination', 'Feature Maps', 'Local Maxima', 'Feature Detection', 'Simple Description', 'Standard Task', 'Matching Score', 'Convolutional Neural Network Training', 'Saliency Map', 'Relevant Locations', 'Viewpoint Changes', 'Keypoint Locations', 'Image Regions', 'Image Space', 'Vision Tasks', 'Drawback Of This Method', 'Light Changes', 'Sobel Operator', 'Non-maximum Suppression', 'AlexNet', 'Relevant Descriptors', 'Automatic Threshold', 'Structure From Motion', 'Histogram Of Gradients', 'Feature Matching', 'Distribution Of Pixels', 'Homography']",,21,"This paper introduces a novel feature detector based only on information embedded inside a CNN trained on standard tasks (e.g. classification). While previous works already show that the features of a trained CNN are suitable descriptors, we show here how to extract the feature locations from the network to build a detector. This information is computed from the gradient of the feature map with respect to the input image. This provides a saliency map with local maxima on relevant keypoint locations. Contrary to recent CNN-based detectors, this method requires neither supervised training nor finetuning. We evaluate how repeatable and how `matchable' the detected keypoints are with the repeatability and matching scores. Matchability is measured with a simple descriptor introduced for the sake of the evaluation. This novel detector reaches similar performances on the standard evaluation HPatches dataset, as well as comparable robustness against illumination and viewpoint changes on Webcam and photo-tourism images. These results show that a CNN trained on a standard task embeds feature location information that is as relevant as when the CNN is specifically trained for feature detection."
EM-Fusion: Dynamic Object-Level SLAM With Probabilistic Data Association,"Michael Strecke, JÃ¶rg StÃ¼ckler","Embodied Vision Group, Max Planck Institute for Intelligent Systems",100.0,Germany,0.0,,"The majority of approaches for acquiring dense 3D environment maps with RGB-D cameras assumes static environments or rejects moving objects as outliers. The representation and tracking of moving objects, however, has significant potential for applications in robotics or augmented reality. In this paper, we propose a novel approach to dynamic SLAM with dense object-level representations. We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. Our main novelty is a probabilistic formulation which naturally leads to strategies for data association and occlusion handling. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Strecke_EM-Fusion_Dynamic_Object-Level_SLAM_With_Probabilistic_Data_Association_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Strecke_EM-Fusion_Dynamic_Object-Level_SLAM_With_Probabilistic_Data_Association_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010932/,"['Simultaneous localization and mapping', 'Cameras', 'Image segmentation', 'Probabilistic logic', 'Three-dimensional displays', 'Clocks', 'Tracking']","['Simultaneous Localization And Mapping', 'Probabilistic Data Association', 'Depth Camera', 'Image Alignment', 'Direct Alignment', 'Probabilistic Formulation', 'Signed Distance Function', 'Significant Potential For Applications', 'Multi-object Tracking', 'Point Cloud', 'Nonlinear Least Squares', 'Volume Size', 'Depth Images', 'Large Objects', 'Instance Segmentation', 'Map Representation', 'Static Objects', 'Dynamic Objects', 'Camera Pose', 'Robust Tracking', 'Mask R-CNN', 'Iterative Closest Point', 'Map Objects', 'Object Volume', 'Ray Casting', 'Dynamic Scenes', 'Static Background', 'Geometric Cues', 'Trilinear Interpolation', 'Object Tracking']",,52,"The majority of approaches for acquiring dense 3D environment maps with RGB-D cameras assumes static environments or rejects moving objects as outliers. The representation and tracking of moving objects, however, has significant potential for applications in robotics or augmented reality. In this paper, we propose a novel approach to dynamic SLAM with dense object-level representations. We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. Our main novelty is a probabilistic formulation which naturally leads to strategies for data association and occlusion handling. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy."
EMPNet: Neural Localisation and Mapping Using Embedded Memory Points,"Gil Avraham, Yan Zuo, Thanuja Dharmasiri, Tom Drummond","ARC Centre of Excellence for Robotic Vision, Monash University, Australia",100.0,australia,0.0,,"Continuously estimating an agent's state space and a representation of its surroundings has proven vital towards full autonomy. A shared common ground among systems which successfully achieve this feat is the integration of previously encountered observations into the current state being estimated. This necessitates the use of a memory module for incorporating previously visited states whilst simultaneously offering an internal representation of the observed environment. In this work we develop a memory module which contains rigidly aligned point-embeddings that represent a coherent scene structure acquired from an RGB-D sequence of observations. The point-embeddings are extracted using modern convolutional neural network architectures, and alignment is performed by computing a dense correspondence matrix between a new observation and the current embeddings residing in the memory module. The whole framework is end-to-end trainable, resulting in a recurrent joint optimisation of the point-embeddings contained in the memory. This process amplifies the shared information across states, providing increased robustness and accuracy. We show significant improvement of our method across a set of experiments performed on the synthetic VIZDoom environment and a real world Active Vision Dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Avraham_EMPNet_Neural_Localisation_and_Mapping_Using_Embedded_Memory_Points_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Avraham_EMPNet_Neural_Localisation_and_Mapping_Using_Embedded_Memory_Points_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009526/,"['Three-dimensional displays', 'Memory modules', 'Task analysis', 'Cameras', 'Hafnium', 'Robots', 'Navigation']","['Convolutional Neural Network', 'Mental Representations', 'Memory Module', 'Scene Structure', 'Synthetic Environment', 'Quantitative Results', 'Short-term Memory', 'Real-world Data', 'Point Cloud', 'Weight Vector', 'Depth Map', 'Depth Information', 'Optical Flow', '3D Point', 'Consecutive Frames', 'Localization Task', 'Depth Estimation', 'Coordinate Frame', 'Map Representation', 'Translation Vector', 'Relative Pose', 'Pose Information', 'Latent Code', 'Decoder Block', 'Encoder Block', 'Batch Normalization', 'Dense Representation', 'Robotic Platform', 'Sequence Length', 'Error Metrics']",,8,"Continuously estimating an agent's state space and a representation of its surroundings has proven vital towards full autonomy. A shared common ground among systems which successfully achieve this feat is the integration of previously encountered observations into the current state being estimated. This necessitates the use of a memory module for incorporating previously visited states whilst simultaneously offering an internal representation of the observed environment. In this work we develop a memory module which contains rigidly aligned point-embeddings that represent a coherent scene structure acquired from an RGB-D sequence of observations. The point-embeddings are extracted using modern convolutional neural network architectures, and alignment is performed by computing a dense correspondence matrix between a new observation and the current embeddings residing in the memory module. The whole framework is end-to-end trainable, resulting in a recurrent joint optimisation of the point-embeddings contained in the memory. This process amplifies the shared information across states, providing increased robustness and accuracy. We show significant improvement of our method across a set of experiments performed on the synthetic VIZDoom environment and a real world Active Vision Dataset."
EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition,"Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen","Visual Geometry Group, University of Oxford; Visual Information Lab, University of Bristol",100.0,uk,0.0,,"We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multi-modal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities -- RGB, Flow and Audio -- and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities. We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010900/,"['Feature extraction', 'Fuses', 'Training', 'Computer vision', 'Computer architecture', 'Visualization', 'Microsoft Windows']","['Action Recognition', 'Temporal Binding', 'Egocentric Action', 'Egocentric Action Recognition', 'Individual Modules', 'Largest Dataset', 'Sparse Sampling', 'Temporal Aggregation', 'Offset Range', 'Synchronization', 'Contralateral', 'Multiple Modalities', 'Fusion Method', 'Temporal Window', 'Visual Modality', 'Optical Flow', 'Wearable Sensors', 'Effective Width', 'Window Width', 'Fusion Strategy', 'Late Fusion', 'Audio Stream', 'Fusion Mode', 'Segmentation Scores', 'Temporal Progression', 'Temporal Shift']",,187,"We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multimodal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities - RGB, Flow and Audio - and combine them with mid-level fusion alongside sparse temporal sampling off used representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality and fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities. We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard."
ERL-Net: Entangled Representation Learning for Single Image De-Raining,"Guoqing Wang, Changming Sun, Arcot Sowmya","University of New South Wales, Australia; University of New South Wales, Australia and CSIRO Data61, Australia; CSIRO Data61, Australia and University of New South Wales, Australia",100.0,australia,0.0,,"Despite the significant progress achieved in image de-raining by training an encoder-decoder network within the image-to-image translation formulation, blurry results with missing details indicate the deficiency of the existing models. By interpreting the de-raining encoder-decoder network as a conditional generator, within which the decoder acts as a generator conditioned on the embedding learned by the encoder, the unsatisfactory output can be attributed to the low-quality embedding learned by the encoder. In this paper, we hypothesize that there exists an inherent mapping between the low-quality embedding to a latent optimal one, with which the generator (decoder) can produce much better results. To improve the de-raining results significantly over existing models, we propose to learn this mapping by formulating a residual learning branch, that is capable of adaptively adding residuals to the original low-quality embedding in a representation entanglement manner. Using an embedding learned this way, the decoder is able to generate much more satisfactory de-raining results with better detail recovery and rain artefacts removal, providing new state-of-the-art results on four benchmark datasets with considerable improvement (i.e., on the challenging Rain100H data, an improvement of 4.19dB on PSNR and 5% on SSIM is obtained). The entanglement can be easily adopted into any encoder-decoder based image restoration networks. Besides, we propose a series of evaluation metrics to investigate the specific contribution of the proposed entangled representation learning mechanism. Codes are available at .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_ERL-Net_Entangled_Representation_Learning_for_Single_Image_De-Raining_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_ERL-Net_Entangled_Representation_Learning_for_Single_Image_De-Raining_ICCV_2019_paper.pdf,,https://github.com/RobinCSIRO/ERL-Net-for-Single-Image-Deraining,,main,Poster,https://ieeexplore.ieee.org/document/9010749/,"['Rain', 'Decoding', 'Generators', 'Network architecture', 'Task analysis', 'Training', 'Image restoration']","['Representation Learning', 'Image Deraining', 'Single Image Deraining', 'Entangled Representation', 'Benchmark Datasets', 'Encoder-decoder Network', 'Embedding Learning', 'Missing Details', 'Semantic', 'Convolutional Neural Network', 'Training Strategy', 'Clear Image', 'Equivalency', 'Extra Information', 'Essential Properties', 'Background Regions', 'Unsatisfactory Results', 'Translational Model', 'Binary Map', 'Attention Map', 'Reconstruction Loss', 'Image Embedding', 'Translation Network', 'Mean Intersection', 'Easy Samples', 'Latent Embedding', 'Dense Block', 'U-Net Structure']",,50,"Despite the significant progress achieved in image de-raining by training an encoder-decoder network within the image-to-image translation formulation, blurry results with missing details indicate the deficiency of the existing models. By interpreting the de-raining encoder-decoder network as a conditional generator, within which the decoder acts as a generator conditioned on the embedding learned by the encoder, the unsatisfactory output can be attributed to the low-quality embedding learned by the encoder. In this paper, we hypothesize that there exists an inherent mapping between the low-quality embedding to a latent optimal one, with which the generator (decoder) can produce much better results. To improve the de-raining results significantly over existing models, we propose to learn this mapping by formulating a residual learning branch, that is capable of adaptively adding residuals to the original low-quality embedding in a representation entanglement manner. Using an embedding learned this way, the decoder is able to generate much more satisfactory de-raining results with better detail recovery and rain artefacts removal, providing new state-of-the-art results on four benchmark datasets with considerable improvement (i.e., on the challenging Rain100H data, an improvement of 4.19dB on PSNR and 5% on SSIM is obtained). The entanglement can be easily adopted into any encoder-decoder based image restoration networks. Besides, we propose a series of evaluation metrics to investigate the specific contribution of the proposed entangled representation learning mechanism. Codes are available at 〈https://github.com/RobinCSIRO/ERL-Net-for-Single-Image-Deraining〉."
Efficient Learning on Point Clouds With Basis Point Sets,"Sergey Prokudin, Christoph Lassner, Javier Romero","Amazon, Barcelona, Spain; Amazon, T¨ubingen, Germany; Max Planck Institute for Intelligent Systems, T¨ubingen, Germany",33.33333333333333,Germany,66.66666666666667,USA,"With an increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to the unordered structure. One common approach is to apply voxelization, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models with hand-tailored architectures were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures use an increased number of parameters and are computationally inefficient. In this work we propose basis point sets as a highly efficient and fully general way to process point clouds with machine learning algorithms. Basis point sets are a residual representation that can be computed efficiently and can be used with standard neural network architectures. Using the proposed representation as the input to a relatively simple network allows us to match the performance of PointNet on a shape classification task while using three order of magnitudes less floating point operations. In a second experiment, we show how proposed representation can be used for obtaining high resolution meshes from noisy 3D scans. Here, our network achieves performance comparable to the state-of-the-art computationally intense multi-step frameworks, in one network pass that can be done in less than 1ms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9022338/,"['Three-dimensional displays', 'Encoding', 'Shape', 'Computational modeling', 'Task analysis', 'Neural networks', 'Surface reconstruction']","['Point Cloud', 'Basis Points', 'Neural Network', 'Machine Learning', 'Deep Learning', 'Learning Algorithms', 'Computational Efficiency', 'Deep Models', 'Deep Learning Models', '3D Scanning', 'Floating-point Operations', 'Shape Classification', 'Occupancy Grid', 'Random Sampling', 'Grid Cells', 'Regular Grid', 'Distance Map', 'Unit Sphere', 'Nearest Neighbor Search', 'Surface Reconstruction', 'Original Cloud', 'Original Point Cloud', '3D Convolution', 'Point Cloud Data', 'Hexagonal Close-packed', 'Raw Point Cloud', 'Surface Normals', 'CAD Model', '3D Point Cloud', 'Rotation Invariance']","['point cloud', '3D shape reconstruction', 'deep learning']",14,"With an increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to their unordered structure. One common approach is to apply occupancy grid mapping, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures often use an increased number of parameters and are computationally inefficient. In this work we propose basis point sets (BPS) as a highly efficient and fully general way to process point clouds with machine learning algorithms. The basis point set representation is a residual representation that can be computed efficiently and can be used with standard neural network architectures and other machine learning algorithms. Using the proposed representation as the input to a simple fully connected network allows us to match the performance of PointNet on a shape classification task, while using three orders of magnitude less floating point operations. In a second experiment, we show how the proposed representation can be used for registering high resolution meshes to noisy 3D scans. Here, we present the first method for single-pass high-resolution mesh registration, avoiding time-consuming per-scan optimization and allowing real-time execution."
Efficient Segmentation: Learning Downsampling Near Semantic Boundaries,"Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee, Sam Tsai, Fei Yang, Yuri Boykov","University of Waterloo, Canada; Facebook Inc, USA; TAL Education, China",66.66666666666666,"China, canada",33.33333333333334,USA,"Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Marin_Efficient_Segmentation_Learning_Downsampling_Near_Semantic_Boundaries_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Marin_Efficient_Segmentation_Learning_Downsampling_Near_Semantic_Boundaries_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008795/,"['Image segmentation', 'Semantics', 'Tensile stress', 'Adaptation models', 'Interpolation', 'Computational modeling', 'Image resolution']","['Semantic Boundaries', 'Sampling Locations', 'Target Class', 'Semantic Segmentation', 'Small Objects', 'Class Boundaries', 'Adaptive Sampling', 'Feature Maps', 'Image Size', 'Deeper Layers', 'Segmentation Model', 'Spatial Coordinates', 'Prediction Network', 'Bilinear Interpolation', 'Open Dataset', 'Original Resolution', 'Segment Boundaries', 'Segmentation Dataset', 'Input Resolution', 'Accurate Boundary', 'Negligible Cost', 'Objects Of Different Sizes', 'Semantic Segmentation Datasets', 'Resolution Of The Feature Map', 'Deformable Convolution']",,57,"Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects."
Efficient and Accurate Arbitrary-Shaped Text Detection With Pixel Aggregation Network,"Wenhai Wang, Enze Xie, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu, Gang Yu, Chunhua Shen","Megvii (Face++) Technology Inc.; Tongji University; The University of Adelaide; University of Electronic Science and Technology of China; National Key Lab for Novel Software Technology, Nanjing University",80.0,"australia, china",20.0,China,"Scene text detection, an important step of scene text reading systems, has witnessed rapid development with convolutional neural networks. Nonetheless, two main challenges still exist and hamper its deployment to real-world applications. The first problem is the trade-off between speed and accuracy. The second one is to model the arbitrary-shaped text instance. Recently, some methods have been proposed to tackle arbitrary-shaped text detection, but they rarely take the speed of the entire pipeline into consideration, which may fall short in practical applications. In this paper, we propose an efficient and accurate arbitrary-shaped text detector, termed Pixel Aggregation Network (PAN), which is equipped with a low computational-cost segmentation head and a learnable post-processing. More specifically, the segmentation head is made up of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a cascadable U-shaped module, which can introduce multi-level information to guide the better segmentation. FFM can gather the features given by the FPEMs of different depths into a final feature for segmentation. The learnable post-processing is implemented by Pixel Aggregation (PA), which can precisely aggregate text pixels by predicted similarity vectors. Experiments on several standard benchmarks validate the superiority of the proposed PAN. It is worth noting that our method can achieve a competitive F-measure of 79.9% at 84.2 FPS on CTW1500.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009483/,"['Kernel', 'Detectors', 'Pipelines', 'Benchmark testing', 'Fuses', 'Shape', 'Bones']","['Optical Character Recognition', 'Pixel Aggregation', 'Arbitrary-shaped Text', 'Pixel Aggregation Network', 'Convolutional Neural Network', 'Feature Pyramid', 'Feature Enhancement', 'Feature Fusion Module', 'Feature Maps', 'Dimensional Vector', 'Receptive Field', 'Stochastic Gradient Descent', 'Training Images', 'Bounding Box', 'Semantic Segmentation', 'Cluster Centers', 'Segmentation Results', 'Detection Dataset', 'Faster R-CNN', 'Fully Convolutional Network', 'Text Similarity', 'Features Of Different Scales', 'High-level Semantic Information', 'Small Receptive Field', 'Depthwise Convolution', 'Text Dataset', 'Separable Convolution', 'Inference Speed', 'Test Phase', 'Training Phase']",,341,"Scene text detection, an important step of scene text reading systems, has witnessed rapid development with convolutional neural networks. Nonetheless, two main challenges still exist and hamper its deployment to real-world applications. The first problem is the trade-off between speed and accuracy. The second one is to model the arbitrary-shaped text instance. Recently, some methods have been proposed to tackle arbitrary-shaped text detection, but they rarely take the speed of the entire pipeline into consideration, which may fall short in practical applications. In this paper, we propose an efficient and accurate arbitrary-shaped text detector, termed Pixel Aggregation Network (PAN), which is equipped with a low computational-cost segmentation head and a learnable post-processing. More specifically, the segmentation head is made up of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a cascadable U-shaped module, which can introduce multi-level information to guide the better segmentation. FFM can gather the features given by the FPEMs of different depths into a final feature for segmentation. The learnable post-processing is implemented by Pixel Aggregation (PA), which can precisely aggregate text pixels by predicted similarity vectors. Experiments on several standard benchmarks validate the superiority of the proposed PAN. It is worth noting that our method can achieve a competitive F-measure of 79.9% at 84.2 FPS on CTW1500."
Efficient and Robust Registration on the 3D Special Euclidean Group,"Uttaran Bhattacharya, Venu Madhav Govindu","Department of Computer Science, University of Maryland, College Park, MD 20740, USA; Department of Electrical Engineering, Indian Institute of Science, Bengaluru 560012, INDIA",100.0,"India, usa",0.0,,"We present a robust, fast and accurate method for registration of 3D scans. Using correspondences, our method optimizes a robust cost function on the intrinsic representation of rigid motions, i.e., the Special Euclidean group SE(3). We exploit the geometric properties of Lie groups as well as the robustness afforded by an iteratively reweighted least squares optimization. We also generalize our approach to a joint multiview method that simultaneously solves for the registration of a set of scans. Our approach significantly outperforms the state-of-the-art robust 3D registration method based on a line process in terms of both speed and accuracy. We show that this line process method is a special case of our principled geometric solution. Finally, we also present scenarios where global registration based on feature correspondences fails but multiview ICP based on our robust motion estimation is successful.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bhattacharya_Efficient_and_Robust_Registration_on_the_3D_Special_Euclidean_Group_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bhattacharya_Efficient_and_Robust_Registration_on_the_3D_Special_Euclidean_Group_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008111/,"['Robustness', 'Three-dimensional displays', 'Motion estimation', 'Algebra', 'Iterative closest point algorithm', 'Cost function']","['Robust Registration', 'Special Euclidean Group', 'Cost Function', 'Corresponding Points', '3D Scanning', 'Rigid Transformation', 'Motion Estimation', 'Registration Method', 'Lie Group', '3D Registration', 'Global Registration', 'Loss Function', 'System Of Equations', 'First Approximation', 'Pairwise Tests', 'Iteration Step', 'Choice Of Function', 'Depth Camera', 'Feature Matching', 'Lie Algebra', 'Scan Pairs', 'Optimization Routines', 'Global Reference Frame', 'Registration Error', 'Optimal Course', 'Least Squares Solution', 'L1 Loss', 'Update Step', 'Pair Of Cameras', 'Prior Methods']",,15,"We present a robust, fast and accurate method for registration of 3D scans. Using correspondences, our method optimizes a robust cost function on the intrinsic representation of rigid motions, i.e., the Special Euclidean group SE(3). We exploit the geometric properties of Lie groups as well as the robustness afforded by an iteratively reweighted least squares optimization. We also generalize our approach to a joint multiview method that simultaneously solves for the registration of a set of scans. Our approach significantly outperforms the state-of-the-art robust 3D registration method based on a line process in terms of both speed and accuracy. We show that this line process method is a special case of our principled geometric solution. Finally, we also present scenarios where global registration based on feature correspondences fails but multiview ICP based on our robust motion estimation is successful."
Ego-Pose Estimation and Forecasting As Real-Time PD Control,"Ye Yuan, Kris Kitani",Carnegie Mellon University,100.0,usa,0.0,,"We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.pdf,https://www.ye-yuan.com/ego-pose,,,main,Poster,https://ieeexplore.ieee.org/document/9008555/,"['Humanoid robots', 'Forecasting', 'Videos', 'Visualization', 'Three-dimensional displays', 'Pose estimation', 'Physics']","['Proportional-derivative Control', 'Value Function', 'Pose Estimation', 'Quantitative Metrics', 'Human Motion', 'Complex Motion', '3D Pose', 'Stable Motion', 'Fail-safe Mechanism', 'Future Motion', 'Multilayer Perceptron', 'Optimal Policy', 'Finite Difference Method', 'Optical Flow', 'Reward Function', 'Linear Velocity', 'Markov Decision Process', 'Physical Simulation', 'Joint Torque', 'Visual Context', 'Motion Phase', 'Human Pose Estimation', '2D Keypoints', 'Wearable Cameras', 'Policy Gradient Method', 'Pose Error', 'Past Frames', 'Reference Motion', 'State St']",,65,"We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications."
Elaborate Monocular Point and Line SLAM With Robust Initialization,"Sang Jun Lee, Sung Soo Hwang","School of Computer Science and Electrical Engineering, Handong Global University, Korea",100.0,Korea,0.0,,"This paper presents a monocular indirect SLAM system which performs robust initialization and accurate localization. For initialization, we utilize a matrix factorization-based method. Matrix factorization-based methods require that extracted feature points must be tracked in all used frames. Since consistent tracking is difficult in challenging environments, a geometric interpolation that utilizes epipolar geometry is proposed. For localization, 3D lines are utilized. We propose the use of Plu cker line coordinates to represent geometric information of lines. We also propose orthonormal representation of Plu cker line coordinates and Jacobians of lines for better optimization. Experimental results show that the proposed initialization generates consistent and robust map in linear time with fast convergence even in challenging scenes. And localization using proposed line representations is faster, more accurate and memory efficient than other state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Elaborate_Monocular_Point_and_Line_SLAM_With_Robust_Initialization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Elaborate_Monocular_Point_and_Line_SLAM_With_Robust_Initialization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009783/,"['Interpolation', 'Cameras', 'Simultaneous localization and mapping', 'Robustness', 'Three-dimensional displays', 'Visualization', 'Mathematical model']","['Simultaneous Localization And Mapping', 'Localization Accuracy', 'Faster Convergence', 'Feature Points', 'Linear Time', 'Feature Point Extraction', '3D Line', 'Invertible', 'Matrix Factorization', 'Singular Value Decomposition', 'Accurate Mapping', 'Line Segment', '3D Point', 'Line Graph', 'Current Frame', 'Matrix Completion', 'Line Features', 'Camera Pose', 'Inliers', 'Homography', 'Relative Pose', 'Reprojection Error', 'Homogeneous Coordinates', 'Inter-frame', 'Camera Coordinate', 'Fundamental Matrix', 'Outlier Rejection', 'Perpendicular Distance', 'Cost Function', 'Chain Rule']",,25,"This paper presents a monocular indirect SLAM system which performs robust initialization and accurate localization. For initialization, we utilize a matrix factorization-based method. Matrix factorization-based methods require that extracted feature points must be tracked in all used frames. Since consistent tracking is difficult in challenging environments, a geometric interpolation that utilizes epipolar geometry is proposed. For localization, 3D lines are utilized. We propose the use of Plücker line coordinates to represent geometric information of lines. We also propose orthonormal representation of Plücker line coordinates and Jacobians of lines for better optimization. Experimental results show that the proposed initialization generates consistent and robust map in linear time with fast convergence even in challenging scenes. And localization using proposed line representations is faster, more accurate and memory efficient than other state-of-the-art methods."
Embedded Block Residual Network: A Recursive Restoration Model for Single-Image Super-Resolution,"Yajun Qiu, Ruxin Wang, Dapeng Tao, Jun Cheng","SIAT, Chinese Academy of Sciences; Union Vision Innovation; Yunnan University",66.66666666666666,"China, china",33.33333333333334,USA,"Single-image super-resolution restores the lost structures and textures from low-resolved images, which has achieved extensive attention from the research community. The top performers in this field include deep or wide convolutional neural networks, or recurrent neural networks. However, the methods enforce a single model to process all kinds of textures and structures. A typical operation is that a certain layer restores the textures based on the ones recovered by the preceding layers, ignoring the characteristics of image textures. In this paper, we believe that the lower-frequency and higher-frequency information in images have different levels of complexity and should be restored by models of different representational capacity. Inspired by this, we propose a novel embedded block residual network (EBRN) which is an incremental recovering progress for texture super-resolution. Specifically, different modules in the model restores information of different frequencies. For lower-frequency information, we use shallower modules of the network to recover; for higher-frequency information, we use deeper modules to restore. Extensive experiments indicate that the proposed EBRN model achieves superior performance and visual improvements against the state-of-the-arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qiu_Embedded_Block_Residual_Network_A_Recursive_Restoration_Model_for_Single-Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Embedded_Block_Residual_Network_A_Recursive_Restoration_Model_for_Single-Image_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010860/,"['Image restoration', 'Complexity theory', 'Computational modeling', 'Spatial resolution', 'Signal resolution', 'Learning systems']","['Residual Network', 'Single Image Super-resolution', 'Convolutional Neural Network', 'Deep Network', 'Deep Neural Network', 'Recurrent Neural Network', 'Deep Convolutional Neural Network', 'Top Performers', 'Different Levels Of Complexity', 'Modulation Model', 'Representational Capacity', 'High Frequency Information', 'Frequency Band', 'Convolutional Layers', 'Feature Maps', 'Deeper Layers', 'Multiple Datasets', 'Residual Signal', 'Peak Signal-to-noise Ratio', 'Frequency Information', 'Complex Texture', 'Output Of Module', 'Residual Learning', 'L2 Loss', 'Deep Learning-based Methods', 'Bicubic Interpolation', 'Perceptual Loss', 'Training Rate', 'Basic Module', 'L1 Loss']",,82,"Single-image super-resolution restores the lost structures and textures from low-resolved images, which has achieved extensive attention from the research community. The top performers in this field include deep or wide convolutional neural networks, or recurrent neural networks. However, the methods enforce a single model to process all kinds of textures and structures. A typical operation is that a certain layer restores the textures based on the ones recovered by the preceding layers, ignoring the characteristics of image textures. In this paper, we believe that the lower-frequency and higher-frequency information in images have different levels of complexity and should be restored by models of different representational capacity. Inspired by this, we propose a novel embedded block residual network (EBRN) which is an incremental recovering progress for texture super-resolution. Specifically, different modules in the model restores information of different frequencies. For lower-frequency information, we use shallower modules of the network to recover; for higher-frequency information, we use deeper modules to restore. Extensive experiments indicate that the proposed EBRN model achieves superior performance and visual improvements against the state-of-the-arts."
Embodied Amodal Recognition: Learning to Move to Perceive Objects,"Jianwei Yang, Zhile Ren, Mingze Xu, Xinlei Chen, David J. Crandall, Devi Parikh, Dhruv Batra",Georgia Institute of Technology; Facebook AI Research; Indiana University,66.66666666666666,usa,33.33333333333334,USA,"Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Amodel Recognition (EAR): an agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this problem, we develop a new model called Embodied Mask R-CNN for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using a simulator for indoor environments. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones and 2) in order to improve visual recognition abilities, agents can learn strategic paths that are different from shortest paths.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Embodied_Amodal_Recognition_Learning_to_Move_to_Perceive_Objects_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Embodied_Amodal_Recognition_Learning_to_Move_to_Perceive_Objects_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008379/,"['Task analysis', 'Shape', 'Ear', 'Target recognition', 'Three-dimensional displays', 'Visualization']","['Semantic', 'Visual System', 'Shortest Path', 'Object Classification', 'Indoor Environments', 'Viewing Angle', 'Target Object', 'Recognition Performance', 'Visual Recognition', '3D Environment', 'Recognition Ability', 'Mask R-CNN', 'Training Set', 'Single Image', 'Object Detection', 'Policy Actors', 'Object Recognition', 'Multilayer Perceptron', 'Stochastic Gradient Descent', 'Bounding Box', 'Policy Learning', 'Occluded Objects', 'Policy Network', 'Perception Network', 'Semantic Segmentation', 'Feature Aggregation', 'Object In Frame', 'Learning Path', 'Image Borders']",,29,"Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Amodel Recognition (EAR): an agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this problem, we develop a new model called Embodied Mask R-CNN for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using a simulator for indoor environments. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones and 2) in order to improve visual recognition abilities, agents can learn strategic paths that are different from shortest paths."
Employing Deep Part-Object Relationships for Salient Object Detection,"Yi Liu, Qiang Zhang, Dingwen Zhang, Jungong Han","School of Mechano-Electronic Engineering, Xidian University, China; WMG, University of Warwick, U.K.",100.0,"China, uk",0.0,,"Despite Convolutional Neural Networks (CNNs) based methods have been successful in detecting salient objects, their underlying mechanism that decides the salient intensity of each image part separately cannot avoid inconsistency of parts within the same salient object. This would ultimately result in an incomplete shape of the detected salient object. To solve this problem, we dig into part-object relationships and take the unprecedented attempt to employ these relationships endowed by the Capsule Network (CapsNet) for salient object detection. The entire salient object detection system is built directly on a Two-Stream Part-Object Assignment Network (TSPOANet) consisting of three algorithmic steps. In the first step, the learned deep feature maps of the input image are transformed to a group of primary capsules. In the second step, we feed the primary capsules into two identical streams, within each of which low-level capsules (parts) will be assigned to their familiar high-level capsules (object) via a locally connected routing. In the final step, the two streams are integrated in the form of a fully connected layer, where the relevant parts can be clustered together to form a complete salient object. Experimental results demonstrate the superiority of the proposed salient object detection network over the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Employing_Deep_Part-Object_Relationships_for_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Employing_Deep_Part-Object_Relationships_for_Salient_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010923/,"['Object detection', 'Streaming media', 'Feature extraction', 'Object recognition', 'Routing', 'Image segmentation', 'Noise measurement']","['Salient Object', 'Salient Object Detection', 'Convolutional Neural Network', 'Input Image', 'Feature Maps', 'Feature Learning', 'Deep Features', 'Object Detection Network', 'Deep Learning Features', 'Complete Object', 'Capsule Network', 'Convolutional Layers', 'Quantitative Comparison', 'Transformation Matrix', 'Visual Comparison', 'Related Properties', 'Multiple Objects', 'Groups Of Neurons', 'Capsule Layer', 'Precision-recall Curve', 'Routing Algorithm', 'Saliency Map', 'Rich Features', 'Object Parts', 'Higher Layers', 'Group Convolution', 'Familiar Ones', 'ReLU Layer']",,74,"Despite Convolutional Neural Networks (CNNs) based methods have been successful in detecting salient objects, their underlying mechanism that decides the salient intensity of each image part separately cannot avoid inconsistency of parts within the same salient object. This would ultimately result in an incomplete shape of the detected salient object. To solve this problem, we dig into part-object relationships and take the unprecedented attempt to employ these relationships endowed by the Capsule Network (CapsNet) for salient object detection. The entire salient object detection system is built directly on a Two-Stream Part-Object Assignment Network (TSPOANet) consisting of three algorithmic steps. In the first step, the learned deep feature maps of the input image are transformed to a group of primary capsules. In the second step, we feed the primary capsules into two identical streams, within each of which low-level capsules (parts) will be assigned to their familiar high-level capsules (object) via a locally connected routing. In the final step, the two streams are integrated in the form of a fully connected layer, where the relevant parts can be clustered together to form a complete salient object. Experimental results demonstrate the superiority of the proposed salient object detection network over the state-of-the-art methods."
End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans,"Armen Avetisyan, Angela Dai, Matthias NieÃner",Technical University of Munich,100.0,germany,0.0,,"We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by 19.04% for CAD model alignment to scans, with approximately 250x faster runtime than previous data-driven approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Avetisyan_End-to-End_CAD_Model_Retrieval_and_9DoF_Alignment_in_3D_Scans_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Avetisyan_End-to-End_CAD_Model_Retrieval_and_9DoF_Alignment_in_3D_Scans_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/abstract/document/9009492/,"['Solid modeling', 'Three-dimensional displays', 'Predictive models', 'Computational modeling', 'Geometry', 'Heating systems', 'Feature extraction']","['3D Scanning', 'CAD Model', 'Object Location', 'Single Pass', 'High Precision', 'Autoencoder', 'Bounding Box', 'Latent Space', 'Scan Data', 'Feature Matching', 'Rotational Symmetry', 'Distance Map', 'Central Objective', '3D Features', 'Final Alignment', 'Two-step Strategy', 'Objective Description', 'Regression Loss', 'Decoder Output', 'Object Bounding Boxes', 'Dense Correspondence', 'Objective Scanner', 'Voxel Grid', 'Signed Distance Function', 'Backbone Network', 'Object Detection', 'Feature Maps', 'Alignment Algorithm', 'High Recall', 'Bounding Box Size']",,63,"We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by 19.04% for CAD model alignment to scans, with approximately ≈250x faster runtime than previous data-driven approaches."
End-to-End Hand Mesh Recovery From a Monocular RGB Image,"Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, Wen Zheng","Y-tech, Kwai; State Key Laboratory of VR, Beihang University",50.0,china,50.0,USA,"In this paper, we present a HAnd Mesh Recovery (HAMR) framework to tackle the problem of reconstructing the full 3D mesh of a human hand from a single RGB image. In contrast to existing research on 2D or 3D hand pose estimation from RGB or/and depth image data, HAMR can provide a more expressive and useful mesh representation for monocular hand image understanding. In particular, the mesh representation is achieved by parameterizing a generic 3D hand model with shape and relative 3D joint angles. By utilizing this mesh representation, we can easily compute the 3D joint locations via linear interpolations between the vertexes of the mesh, while obtain the 2D joint locations with a projection of the 3D joints. To this end, a differentiable re-projection loss can be defined in terms of the derived representations and the ground-truth labels, thus making our framework end-to-end trainable. Qualitative experiments show that our framework is capable of recovering appealing 3D hand mesh even in the presence of severe occlusions. Quantitatively, our approach also outperforms the state-of-the-art methods for both 2D and 3D hand pose estimation from a monocular RGB image on several benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_End-to-End_Hand_Mesh_Recovery_From_a_Monocular_RGB_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_End-to-End_Hand_Mesh_Recovery_From_a_Monocular_RGB_Image_ICCV_2019_paper.pdf,,https://github.com/MandyMo/HAMR,,main,Poster,https://ieeexplore.ieee.org/document/9008830,"['Three-dimensional displays', 'Two dimensional displays', 'Pose estimation', 'Heat-assisted magnetic recording', 'Image reconstruction', 'Shape', 'Solid modeling']","['RGB Images', 'Monocular RGB Images', 'Hand Mesh', 'Mesh Recovery', 'Single Image', 'Linear Interpolation', 'Benchmark Datasets', 'Depth Images', 'Ground Truth Labels', '3D Mesh', 'Joint Position', 'Pose Estimation', 'Human Hand', '3D Joint', '2D Pose', 'Mesh Representation', 'Model Parameters', 'Deep Network', 'Convolutional Layers', '3D Pose', 'Human Pose Estimation', '2D Keypoints', '3D Reconstruction', 'Number Of Joints', 'Mesh Model', 'Geometric Constraints', 'L2 Loss', 'Spatial Configuration', '3D Face']",,143,"In this paper, we present a HAnd Mesh Recovery (HAMR) framework to tackle the problem of reconstructing the full 3D mesh of a human hand from a single RGB image. In contrast to existing research on 2D or 3D hand pose estimation from RGB or/and depth image data, HAMR can provide a more expressive and useful mesh representation for monocular hand image understanding. In particular, the mesh representation is achieved by parameterizing a generic 3D hand model with shape and relative 3D joint angles. By utilizing this mesh representation, we can easily compute the 3D joint locations via linear interpolations between the vertexes of the mesh, while obtain the 2D joint locations with a projection of the 3D joints. To this end, a differentiable re-projection loss can be defined in terms of the derived representations and the ground-truth labels, thus making our framework end-to-end trainable. Qualitative experiments show that our framework is capable of recovering appealing 3D hand mesh even in the presence of severe occlusions. Quantitatively, our approach also outperforms the state-of-the-art methods for both 2D and 3D hand pose estimation from a monocular RGB image on several benchmark datasets."
End-to-End Learning for Graph Decomposition,"Jie Song, Bjoern Andres, Michael J. Black, Otmar Hilliges, Siyu Tang",Bosch Center for AI; ETH Zurich; MPI for Intelligent Systems,66.66666666666666,"germany, switzerland",33.33333333333334,Germany,"Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Song_End-to-End_Learning_for_Graph_Decomposition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Song_End-to-End_Learning_for_Graph_Decomposition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010957/,"['Task analysis', 'Optimization', 'Pose estimation', 'Training', 'Labeling', 'Random variables', 'Computer vision']","['Graph Decomposition', 'Neural Network', 'Random Variables', 'Optimization Problem', 'Objective Function', 'Convolutional Neural Network', 'Deep Network', 'Deep Neural Network', 'Computer Vision', 'Random Fields', 'Minimization Problem', 'Pose Estimation', 'Conditional Random Field', 'Binary Label', 'Consistency Constraint', 'Cycle Consistency', 'Standard Convolutional Neural Networks', 'Edge Labels', 'Feature Representation', 'Feature Learning', 'Simple Cycle', 'Conditional Random Field Model', 'Hard Constraints', 'Mean-field', 'Body Joints', 'Siamese Network', 'Human Pose Estimation', 'Images Of People', 'Recurrent Neural Network Layer', 'Confidence Map']",,11,"Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation."
End-to-End Learning of Representations for Asynchronous Event-Based Data,"Daniel Gehrig, Antonio Loquercio, Konstantinos G. Derpanis, Davide Scaramuzza","Ryerson University and Samsung AI Centre Toronto; Robotics and Perception Group, Depts. Informatics and Neuroinformatics, University of Zurich and ETH Zurich",100.0,"canada, switzerland",0.0,,"Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as ""events"". They have appealing advantages over frame based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatio-temporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations by means of strictly differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gehrig_End-to-End_Learning_of_Representations_for_Asynchronous_Event-Based_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gehrig_End-to-End_Learning_of_Representations_for_Asynchronous_Event-Based_Data_ICCV_2019_paper.pdf,,https://github.com/uzh-rpg/rpg_event_representation_learning,,main,Poster,https://ieeexplore.ieee.org/document/9009469/,"['Cameras', 'Task analysis', 'Standards', 'Computer vision', 'Brightness', 'Optical imaging', 'Spatiotemporal phenomena']","['Representation Learning', 'Asynchronous Data', 'Neural Network', 'Spatiotemporal', 'Convolutional Neural Network', 'Dynamic Range', 'Computer Vision', 'Object Recognition', 'Optical Flow', 'High Dynamic Range', 'Representation Of Events', 'Vision Sensors', 'Motion Blur', 'Brightness Changes', 'Literary Representations', 'Optical Flow Estimation', 'Event Stream', 'Dynamic Vision Sensor', 'Event Data', 'Spiking Neural Networks', 'Exponential Kernel', 'Voxel Grid', 'Temporal Information', 'Time Stamp', 'Traditional Cameras', 'Event Counts', 'Event Frames', 'Changes In Polarity', 'Low Latency']",,206,"Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as ""events”. They have appealing advantages over frame based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatio-temporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations by means of strictly differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods."
End-to-End Wireframe Parsing,"Yichao Zhou, Haozhi Qi, Yi Ma",UC Berkeley,100.0,usa,0.0,,"We present a conceptually simple yet effective algorithm to detect wireframes in a given image. Compared to the previous methods which first predict an intermediate heat map and then extract straight lines with heuristic algorithms, our method is end-to-end trainable and can directly output a vectorized wireframe that contains semantically meaningful and geometrically salient junctions and lines. To better understand the quality of the outputs, we propose a new metric for wireframe evaluation that penalizes overlapped line segments and incorrect line connectivities. We conduct extensive experiments and show that our method significantly outperforms the previous state-of-the-art wireframe and line extraction algorithms. We hope our simple approach can be served as a baseline for future wireframe parsing studies. Code has been made publicly available at https://github.com/zhou13/lcnn.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_End-to-End_Wireframe_Parsing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_End-to-End_Wireframe_Parsing_ICCV_2019_paper.pdf,,https://github.com/zhou13/lcnn,,main,Poster,https://ieeexplore.ieee.org/document/9008267/,"['Junctions', 'Feature extraction', 'Proposals', 'Neural networks', 'Heating systems', 'Measurement', 'Training']","['Heatmap', 'Line Segment', 'Heuristic Algorithm', 'Neural Network', 'Random Sampling', 'True Positive', 'Positive Samples', 'Input Image', 'Feature Maps', 'Negative Samples', 'Precision And Recall', 'Max-pooling', 'Vector Representation', 'Residual Block', 'Backbone Network', 'Structure From Motion', 'Non-maximum Suppression', 'Recall Curve', 'Feature Extraction Backbone', 'PR Curve', 'Candidate Lines', 'ReLU Nonlinearity', 'Head Network', 'Object Detection', 'Faster R-CNN', 'Precision-recall Curve', 'Interesting Line', 'Fully-connected Layer', 'Static Sampling', 'Pooling Layer']",,110,"We present a conceptually simple yet effective algorithm to detect wireframes in a given image. Compared to the previous methods which first predict an intermediate heat map and then extract straight lines with heuristic algorithms, our method is end-to-end trainable and can directly output a vectorized wireframe that contains semantically meaningful and geometrically salient junctions and lines. To better understand the quality of the outputs, we propose a new metric for wireframe evaluation that penalizes overlapped line segments and incorrect line connectivities. We conduct extensive experiments and show that our method significantly outperforms the previous state-of-the-art wireframe and line extraction algorithms. We hope our simple approach can be served as a baseline for future wireframe parsing studies. Code has been made publicly available at https://github.com/zhou13/lcnn."
Enforcing Geometric Constraints of Virtual Normal for Depth Prediction,"Wei Yin, Yifan Liu, Chunhua Shen, Youliang Yan","Noah’s Ark Lab, Huawei Technologies; The University of Adelaide, Australia",50.0,australia,50.0,China,"Monocular depth prediction plays a crucial role in understanding 3D scene geometry. Although recent methods have achieved impressive progress in evaluation metrics such as the pixel-wise relative error, most methods neglect the geometric constraints in the 3D space. In this work, we show the importance of the high-order 3D geometric constraints for depth prediction. By designing a loss term that enforces one simple type of geometric constraints, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, we can considerably improve the depth prediction accuracy. Furthermore, we can not only predict accurate depth but also achieve high-quality other 3D information from the depth without retraining new parameters, Significantly, the byproduct of this predicted depth being sufficiently accurate is that we are now able to recover good 3D structures of the scene such as the point cloud and surface normal directly from the depth, eliminating the necessity of training new sub-models as was previously done. Experiments on two challenging benchmarks: NYU Depth-V2 and KITTI demonstrate the effectiveness of our method and state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.pdf,,https://tinyurl.com/virtualnormal,,main,Poster,https://ieeexplore.ieee.org/document/9010407/,"['Three-dimensional displays', 'Surface reconstruction', 'Image reconstruction', 'Geometry', 'Training', 'Task analysis', 'Two dimensional displays']","['Geometric Constraints', 'Depth Prediction', 'Point Cloud', '3D Space', '3D Scene', 'Surface Normals', 'Root Mean Square Error', 'Mean Square Error', 'Deep Convolutional Neural Network', 'Depth Map', '3D Point', 'Geometric Relationship', 'Depth Estimation', '3D Geometry', '3D Features', '3D Point Cloud', 'Global Constraints', 'KITTI Dataset', 'Geometry Information', 'Point Cloud Reconstruction', '2nd Row', '1st Row', 'High-quality 3D', 'World Coordinate', 'Normal Vector Of Plane', 'Accurate Depth', 'Small Neighborhood', 'Conditional Random Field', '3D Coordinates']",,308,"Monocular depth prediction plays a crucial role in understanding 3D scene geometry. Although recent methods have achieved impressive progress in evaluation metrics such as the pixel-wise relative error, most methods neglect the geometric constraints in the 3D space. In this work, we show the importance of the high-order 3D geometric constraints for depth prediction. By designing a loss term that enforces one simple type of geometric constraints, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, we can considerably improve the depth prediction accuracy. Furthermore, we can not only predict accurate depth but also achieve high-quality other 3D information from the depth without retraining new parameters, Significantly, the byproduct of this predicted depth being sufficiently accurate is that we are now able to recover good 3D structures of the scene such as the point cloud and surface normal directly from the depth, eliminating the necessity of training new sub-models as was previously done. Experiments on two challenging benchmarks: NYU Depth-V2 and KITTI demonstrate the effectiveness of our method and state-of-the-art performance."
Enhancing 2D Representation via Adjacent Views for 3D Shape Retrieval,"Cheng Xu, Zhaoqun Li, Qiang Qiu, Biao Leng, Jingfei Jiang","School of Computer Science & Engineering, Beihang University; School of Computer Science & Engineering, Beihang University; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Duke University; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology",100.0,"China, china, usa",0.0,,"Multi-view shape descriptors obtained from various 2D images are commonly adopted in 3D shape retrieval. One major challenge is that significant shape information are discarded during 2D view rendering through projection. In this paper, we propose a convolutional neural network based method, CenterNet, to enhance each individual 2D view using its neighboring ones. By exploiting cross-view correlations, CenterNet learns how adjacent views can be maximally incorporated for an enhanced 2D representation to effectively describe shapes. We observe that a very small amount of, e.g., six, enhanced 2D views, are already sufficient for a panoramic shape description. Thus, by simply aggregating features from six enhanced 2D views, we arrive at a highly compact yet discriminative shape descriptor. The proposed shape descriptor significantly outperforms state-of-the-art 3D shape retrieval methods on the ModelNet and ShapeNetCore55 benchmarks, and also exhibits robustness against object occlusion.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Enhancing_2D_Representation_via_Adjacent_Views_for_3D_Shape_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Enhancing_2D_Representation_via_Adjacent_Views_for_3D_Shape_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009085/,"['Shape', 'Three-dimensional displays', 'Correlation', 'Two dimensional displays', 'Feature extraction', 'Solid modeling', 'Computational modeling']","['3D Shape', 'Adjacent Views', '3D Shape Retrieval', 'Convolutional Neural Network', 'Shape Descriptors', 'Occluded Objects', '2D Views', 'Visual Features', 'Max-pooling', 'Central Feature', 'Image Patches', '3D Mesh', 'Neighborhood Characteristics', 'Series Of Structures', 'Discriminatory Capacity', 'Basic Architecture', '3D Datasets', 'Feature Enhancement', 'Shape Representation', 'Central View', 'Central Loss', 'Softmax Loss', 'Adjacent Features', 'View Features', 'Intrinsic Correlation', 'Softmax Operation', 'High Discrimination', 'Deep Learning', 'Set Of Views']",,14,"Multi-view shape descriptors obtained from various 2D images are commonly adopted in 3D shape retrieval. One major challenge is that significant shape information are discarded during 2D view rendering through projection. In this paper, we propose a convolutional neural network based method, CenterNet, to enhance each individual 2D view using its neighboring ones. By exploiting cross-view correlations, CenterNet learns how adjacent views can be maximally incorporated for an enhanced 2D representation to effectively describe shapes. We observe that a very small amount of, e.g., six, enhanced 2D views, are already sufficient for a panoramic shape description. Thus, by simply aggregating features from six enhanced 2D views, we arrive at a highly compact yet discriminative shape descriptor. The proposed shape descriptor significantly outperforms state-of-the-art 3D shape retrieval methods on the ModelNet and ShapeNetCore55 benchmarks, and also exhibits robustness against object occlusion."
Enhancing Adversarial Example Transferability With an Intermediate Level Attack,"Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, Ser-Nam Lim",Cornell University; Facebook AI,50.0,usa,50.0,USA,"Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008298/,"['Perturbation methods', 'Training', 'Security', 'Convolutional neural networks', 'Facebook', 'Artificial intelligence']","['Adversarial Examples', 'Neural Network', 'Feature Representation', 'Layer Model', 'Source Model', 'Target Model', 'Intermediate Feature Maps', 'Convolutional Neural Network', 'Feature Space', 'ImageNet', 'Final Layer', 'Image Space', 'Intermediate Layer', 'Decision Boundary', 'Self-driving', 'Specific Layer', 'Adversarial Attacks', 'Image X', 'Index Of Layer', 'Early Layers', 'Black-box Attacks', 'White-box Attack', 'Fast Gradient Sign Method', 'True Boundary', 'Adversarial Perturbations', 'Attack Methods', 'Target Layer', 'Transfer Model', 'Full Results', 'Learning Rate']",,117,"Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps."
Enhancing Low Light Videos by Exploring High Sensitivity Camera Noise,"Wei Wang, Xin Chen, Cheng Yang, Xiang Li, Xuemei Hu, Tao Yue","Nanjing University, Nanjing, China; NJU institute of sensing and imaging engineering, Nanjing, China; Nanjing University, Nanjing, China",100.0,china,0.0,,"Enhancing low light videos, which consists of denoising and brightness adjustment, is an intriguing but knotty problem. Under low light condition, due to high sensitivity camera setting, commonly negligible noises become obvious and severely deteriorate the captured videos. To recover high quality videos, a mass of image/video denoising/enhancing algorithms are proposed, most of which follow a set of simple assumptions about the statistic characters of camera noise, e.g., independent and identically distributed(i.i.d.), white, additive, Gaussian, Poisson or mixture noises. However, the practical noise under high sensitivity setting in real captured videos is complex and inaccurate to model with these assumptions. In this paper, we explore the physical origins of the practical high sensitivity noise in digital cameras, model them mathematically, and propose to enhance the low light videos based on the noise model by using an LSTM-based neural network. Specifically, we generate the training data with the proposed noise model and train the network with the dark noisy video as input and clear-bright video as output. Extensive comparisons on both synthetic and real captured low light videos with the state-of-the-art methods are conducted to demonstrate the effectiveness of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Enhancing_Low_Light_Videos_by_Exploring_High_Sensitivity_Camera_Noise_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Enhancing_Low_Light_Videos_by_Exploring_High_Sensitivity_Camera_Noise_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9011000/,"['Videos', 'Cameras', 'Sensitivity', 'Colored noise', 'Image color analysis', 'Gaussian distribution']","['Low Light', 'High Noise', 'High-sensitivity Camera', 'Low-light Video', 'Neural Network', 'Training Data', 'Noise Model', 'Video Capture', 'Low Light Conditions', 'Extensive Comparison', 'Normal Distribution', 'White Noise', 'Gaussian Noise', 'Poisson Distribution', 'Mixture Model', 'Generative Adversarial Networks', 'Color Channels', 'Shot Noise', 'Dark-field Images', 'Low-light Image', 'Gaussian Mixture Distribution', 'Rolling Shutter', 'Low-light Environments', 'Noisy Images', 'Dynamic Noise', 'Camera Model', 'Motion Estimation', 'Noise Parameters', 'Luminance Levels']",,40,"Enhancing low light videos, which consists of denoising and brightness adjustment, is an intriguing but knotty problem. Under low light condition, due to high sensitivity camera setting, commonly negligible noises become obvious and severely deteriorate the captured videos. To recover high quality videos, a mass of image/video denoising/enhancing algorithms are proposed, most of which follow a set of simple assumptions about the statistic characters of camera noise, e.g., independent and identically distributed(i.i.d.), white, additive, Gaussian, Poisson or mixture noises. However, the practical noise under high sensitivity setting in real captured videos is complex and inaccurate to model with these assumptions. In this paper, we explore the physical origins of the practical high sensitivity noise in digital cameras, model them mathematically, and propose to enhance the low light videos based on the noise model by using an LSTM-based neural network. Specifically, we generate the training data with the proposed noise model and train the network with the dark noisy video as input and clear-bright video as output. Extensive comparisons on both synthetic and real captured low light videos with the state-of-the-art methods are conducted to demonstrate the effectiveness of the proposed method."
Enriched Feature Guided Refinement Network for Object Detection,"Jing Nie, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao","School of Electrical and Information Engineering, Tianjin University; Inception Institute of Artificial Intelligence (IIAI), UAE",100.0,"china, uae",0.0,,"We propose a single-stage detection framework that jointly tackles the problem of multi-scale object detection and class imbalance. Rather than designing deeper networks, we introduce a simple yet effective feature enrichment scheme to produce multi-scale contextual features. We further introduce a cascaded refinement scheme which first instills multi-scale contextual features into the prediction layers of the single-stage detector in order to enrich their discriminative power for multi-scale detection. Second, the cascaded refinement scheme counters the class imbalance problem by refining the anchors and enriched features to improve classification and regression. Experiments are performed on two benchmarks: PASCAL VOC and MS COCO. For a 320x320 input on the MS COCO test-dev, our detector achieves state-of-the-art single-stage detection accuracy with a COCO AP of 33.2 in the case of single-scale inference, while operating at 21 milliseconds on a Titan XP GPU. For a 512x512 input on the MS COCO test-dev, our approach obtains an absolute gain of 1.6% in terms of COCO AP, compared to the best reported single-stage results[5]. Source code and models are available at: https://github.com/Ranchentx/EFGRNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Enriched_Feature_Guided_Refinement_Network_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Enriched_Feature_Guided_Refinement_Network_for_Object_Detection_ICCV_2019_paper.pdf,,https://github.com/Ranchentx/EFGRNet,,main,Poster,https://ieeexplore.ieee.org/document/9008386/,"['Standards', 'Detectors', 'Feature extraction', 'Object detection', 'Iron', 'Semantics', 'Benchmark testing']","['Object Detection', 'Feature Enrichment', 'Detection Accuracy', 'Class Imbalance', 'Multi-scale Features', 'Class Imbalance Problem', 'Refinement Strategy', 'Detection In Order', 'Titan Xp GPU', 'Object Detection Problem', 'Convolutional Neural Network', 'Convolutional Layers', 'Contextual Information', 'Binary Classification', 'Convolution Operation', 'Two-stage Method', 'Shallow Layers', 'Dilated Convolution', 'Object Detection Methods', 'Dilation Rate', 'Single Shot Multibox Detector', 'Offset Position', 'Box Regression', 'MS COCO Dataset', 'Deformable Convolution', 'Bounding Box Regression', 'Map Objects', 'Initial Regression', 'ResNet-101 Backbone', 'Image Pyramid']",,59,"We propose a single-stage detection framework that jointly tackles the problem of multi-scale object detection and class imbalance. Rather than designing deeper networks, we introduce a simple yet effective feature enrichment scheme to produce multi-scale contextual features. We further introduce a cascaded refinement scheme which first instills multi-scale contextual features into the prediction layers of the single-stage detector in order to enrich their discriminative power for multi-scale detection. Second, the cascaded refinement scheme counters the class imbalance problem by refining the anchors and enriched features to improve classification and regression. Experiments are performed on two benchmarks: PASCAL VOC and MS COCO. For a 320×320 input on the MS COCO test-dev, our detector achieves state-of-the-art single-stage detection accuracy with a COCO AP of 33.2 in the case of single-scale inference, while operating at 21 milliseconds on a Titan XP GPU. For a 512×512 input on the MS COCO test-dev, our approach obtains an absolute gain of 1.6% in terms of COCO AP, compared to the best reported single-stage results[5]. Source code and models are available at: https://github.com/Ranchentx/EFGRNet."
Entangled Transformer for Image Captioning,"Guang Li, Linchao Zhu, Ping Liu, Yi Yang","ReLER, University of Technology Sydney",100.0,australia,0.0,,"In image captioning, the typical attention mechanisms are arduous to identify the equivalent visual signals especially when predicting highly abstract words. This phenomenon is known as the semantic gap between vision and language. This problem can be overcome by providing semantic attributes that are homologous to language. Thanks to the inherent recurrent nature and gated operating mechanism, Recurrent Neural Network (RNN) and its variants are the dominating architectures in image captioning. However, when designing elaborate attention mechanisms to integrate visual inputs and semantic attributes, RNN-like variants become unflexible due to their complexities. In this paper, we investigate a Transformer-based sequence modeling framework, built only with attention layers and feedforward layers. To bridge the semantic gap, we introduce EnTangled Attention (ETA) that enables the Transformer to exploit semantic and visual information simultaneously. Furthermore, Gated Bilateral Controller (GBC) is proposed to guide the interactions between the multimodal information. We name our model as ETA-Transformer. Remarkably, ETA-Transformer achieves state-of-the-art performance on the MSCOCO image captioning dataset. The ablation studies validate the improvements of our proposed modules.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008532/,"['Semantics', 'Visualization', 'Decoding', 'Encoding', 'Logic gates', 'Snow', 'Proposals']","['Image Captioning', 'Visual Information', 'Recurrent Neural Network', 'Attention Mechanism', 'Semantic Information', 'Semantic Properties', 'Attention Layer', 'Multimodal Information', 'Semantic Gap', 'Recurrent Nature', 'Convolutional Neural Network', 'Output Layer', 'Visual Representation', 'Visual Features', 'Visual Attention', 'Functional Identification', 'Semantic Features', 'Hidden State', 'Transformer Model', 'Visual Question Answering', 'Graph Convolutional Network', 'Target Modality', 'Vanishing Gradient', 'Word Embedding', 'Gating Mechanism', 'Strong Baseline', 'Transformer Encoder', 'Human Visual System', 'Positional Encoding']",,206,"In image captioning, the typical attention mechanisms are arduous to identify the equivalent visual signals especially when predicting highly abstract words. This phenomenon is known as the semantic gap between vision and language. This problem can be overcome by providing semantic attributes that are homologous to language. Thanks to the inherent recurrent nature and gated operating mechanism, Recurrent Neural Network (RNN) and its variants are the dominating architectures in image captioning. However, when designing elaborate attention mechanisms to integrate visual inputs and semantic attributes, RNN-like variants become unflexible due to their complexities. In this paper, we investigate a Transformer-based sequence modeling framework, built only with attention layers and feedforward layers. To bridge the semantic gap, we introduce EnTangled Attention (ETA) that enables the Transformer to exploit semantic and visual information simultaneously. Furthermore, Gated Bilateral Controller (GBC) is proposed to guide the interactions between the multimodal information. We name our model as ETA-Transformer. Remarkably, ETA-Transformer achieves state-of-the-art performance on the MSCOCO image captioning dataset. The ablation studies validate the improvements of our proposed modules."
Equivariant Multi-View Networks,"Carlos Esteves, Yinshuang Xu, Christine Allen-Blanchette, Kostas Daniilidis","GRASP Laboratory, University of Pennsylvania",100.0,usa,0.0,,"Several popular approaches to 3D vision tasks process multiple views of the input independently with deep neural networks pre-trained on natural images, where view permutation invariance is achieved through a single round of pooling over all views. We argue that this operation discards important information and leads to subpar global descriptors. In this paper, we propose a group convolutional approach to multiple view aggregation where convolutions are performed over a discrete subgroup of the rotation group, enabling, thus, joint reasoning over all views in an equivariant (instead of invariant) fashion, up to the very last layer. We further develop this idea to operate on smaller discrete homogeneous spaces of the rotation group, where a polar view representation is used to maintain equivariance with only a fraction of the number of input views. We set the new state of the art in several large scale 3D shape retrieval tasks, and show additional applications to panoramic scene classification.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Esteves_Equivariant_Multi-View_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Esteves_Equivariant_Multi-View_Networks_ICCV_2019_paper.pdf,,http://github.com/daniilidis-group/emvnlogits,,main,Oral,https://ieeexplore.ieee.org/document/9008839/,"['Three-dimensional displays', 'Shape', 'Task analysis', 'Correlation', 'Finite element analysis', 'Training', 'Solid modeling']","['State Of The Art', '3D Shape', 'Discrete Groups', 'Scene Classification', 'Rotation Group', 'Homogeneous Space', 'Large 3D', 'Group Convolution', 'Large-scale 3D', 'Volumetric', 'Classification Performance', 'Convolutional Layers', 'Feature Maps', 'Point Cloud', 'Receptive Field', 'Group Elements', 'Linear Representation', 'Input Representation', 'Dodecahedron', 'Cyclic Group', 'In-plane Rotation', 'Set Of Views', 'Icosahedral Symmetry', 'Triplet Loss', 'Polarization Imaging', 'Retrieval Performance', 'Continuous Rotation', 'Finite Group', 'Single View']",,44,"Several popular approaches to 3D vision tasks process multiple views of the input independently with deep neural networks pre-trained on natural images, where view permutation invariance is achieved through a single round of pooling over all views. We argue that this operation discards important information and leads to subpar global descriptors. In this paper, we propose a group convolutional approach to multiple view aggregation where convolutions are performed over a discrete subgroup of the rotation group, enabling, thus, joint reasoning over all views in an equivariant (instead of invariant) fashion, up to the very last layer. We further develop this idea to operate on smaller discrete homogeneous spaces of the rotation group, where a polar view representation is used to maintain equivariance with only a fraction of the number of input views. We set the new state of the art in several large scale 3D shape retrieval tasks, and show additional applications to panoramic scene classification."
Escaping Platoâs Cave: 3D Shape From Adversarial Rendering,"Philipp Henzler, Niloy J. Mitra, Tobias Ritschel",University College London,100.0,uk,0.0,,"We introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PlatonicGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PlatonicGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
Estimating the Fundamental Matrix Without Point Correspondences With Application to Transmission Imaging,"Tobias WÃ¼rfl, AndrÃ© Aichert, Nicole MaaÃ, Frank Dennerlein, Andreas Maier","Pattern Recognition Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU); Siemens Healthcare GmbH",50.0,Germany,50.0,Germany,"We present a general method to estimate the fundamental matrix from a pair of images under perspective projection without the need for image point correspondences. Our method is particularly well-suited for transmission imaging, where state-of-the-art feature detection and matching approaches generally do not perform well. Estimation of the fundamental matrix plays a central role in auto-calibration methods for reflection imaging. Such methods are currently not applicable to transmission imaging. Furthermore, our method extends an existing technique proposed for reflection imaging which potentially avoids the outlier-prone feature matching step from an orthographic projection model to a perspective model. Our method exploits the idea that under a linear attenuation model line integrals along corresponding epipolar lines are equal if we compute their derivatives in orthogonal direction to their common epipolar plane. We use the fundamental matrix to parametrize this equality. Our method estimates the matrix by formulating a non-convex optimization problem, minimizing an error in our measurement of this equality. We believe this technique will enable the application of the large body of work on image-based camera pose estimation to transmission imaging leading to more accurate and more general motion compensation and auto-calibration algorithms, particularly in medical X-ray and Computed Tomography imaging.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wurfl_Estimating_the_Fundamental_Matrix_Without_Point_Correspondences_With_Application_to_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wurfl_Estimating_the_Fundamental_Matrix_Without_Point_Correspondences_With_Application_to_ICCV_2019_paper.pdf,https://www5.cs.fau.de,,,main,Poster,https://ieeexplore.ieee.org/document/9009839/,"['Attenuation', 'Computed tomography', 'Geometry', 'X-ray imaging', 'Image reconstruction', 'Estimation']","['Corresponding Points', 'Transmission Images', 'Fundamental Matrix', 'Optimization Problem', 'General Method', 'Parametrized', 'Image Pairs', 'Feature Detection', 'Image Point', 'Model Projections', 'Matrix Estimation', 'Visual Perspective', 'Non-convex Problem', 'Feature Matching', 'Reflectance Images', 'Motion Compensation', 'Line Integral', 'Linear Attenuation', 'Mean Square Error', 'Medical Imaging', 'Radon Transform', 'Consistent Conditions', 'Projection Matrix', 'Cost Function', 'Relative Geometry', 'Reference Method', 'Null Space', 'Scene Reconstruction', 'Empty Regions', 'Classification Algorithms']",,4,"We present a general method to estimate the fundamental matrix from a pair of images under perspective projection without the need for image point correspondences. Our method is particularly well-suited for transmission imaging, where state-of-the-art feature detection and matching approaches generally do not perform well. Estimation of the fundamental matrix plays a central role in auto-calibration methods for reflection imaging. Such methods are currently not applicable to transmission imaging. Furthermore, our method extends an existing technique proposed for reflection imaging which potentially avoids the outlier-prone feature matching step from an orthographic projection model to a perspective model. Our method exploits the idea that under a linear attenuation model line integrals along corresponding epipolar lines are equal if we compute their derivatives in orthogonal direction to their common epipolar plane. We use the fundamental matrix to parametrize this equality. Our method estimates the matrix by formulating a non-convex optimization problem, minimizing an error in our measurement of this equality. We believe this technique will enable the application of the large body of work on image-based camera pose estimation to transmission imaging leading to more accurate and more general motion compensation and auto-calibration algorithms, particularly in medical X-ray and Computed Tomography imaging."
EvalNorm: Estimating Batch Normalization Statistics for Evaluation,"Saurabh Singh, Abhinav Shrivastava","University of Maryland, College Park; Google Research",50.0,usa,50.0,USA,"Batch normalization (BN) has been very effective for deep learning and is widely used. However, when training with small minibatches, models using BN exhibit a significant degradation in performance. In this paper we study this peculiar behavior of BN to gain a better understanding of the problem, and identify a cause. We propose `EvalNorm' to address the issue by estimating corrected normalization statistics to use for BN during evaluation. EvalNorm supports online estimation of the corrected statistics while the model is being trained, and does not affect the training scheme of the model. As a result, EvalNorm can also be used with existing pre-trained models allowing them to benefit from our method. EvalNorm yields large gains for models trained with smaller batches. Our experiments show that EvalNorm performs 6.18% (absolute) better than vanilla BN for a batchsize of 2 on ImageNet validation set and from 1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across a variety of setups.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Singh_EvalNorm_Estimating_Batch_Normalization_Statistics_for_Evaluation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_EvalNorm_Estimating_Batch_Normalization_Statistics_for_Evaluation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010006/,"['Training', 'Computational modeling', 'Estimation', 'Object detection', 'Standards', 'Computer vision', 'Google']","['Batch Normalization', 'Batch Normalization Statistics', 'Object Detection', 'Performance Degradation', 'Understanding Of The Problem', 'Small Batch', 'Online Estimation', 'Heuristic', 'Batch Size', 'Image Classification', 'Transfer Learning', 'First Approximation', 'Statistical Sample', 'Residual Block', 'Wider Network', 'Affine Transformation', 'Random Initialization', 'Faster R-CNN', 'Normalization Techniques', 'Object Detection Task', 'Mini-batch Of Samples', 'Standard Paradigm', 'Region Proposal Network', 'Object Detection Framework', 'Training Setup', 'High-resolution Input']",,24,"Batch normalization (BN) has been very effective for deep learning and is widely used. However, when training with small minibatches, models using BN exhibit a significant degradation in performance. In this paper we study this peculiar behavior of BN to gain a better understanding of the problem, and identify a cause. We propose `EvalNorm' to address the issue by estimating corrected normalization statistics to use for BN during evaluation. EvalNorm supports online estimation of the corrected statistics while the model is being trained, and does not affect the training scheme of the model. As a result, EvalNorm can also be used with existing pre-trained models allowing them to benefit from our method. EvalNorm yields large gains for models trained with smaller batches. Our experiments show that EvalNorm performs 6.18% (absolute) better than vanilla BN for a batchsize of 2 on ImageNet validation set and from 1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across a variety of setups."
Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks,"Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee","Department of Computer Science, University of California, Los Angeles; School of Integrated Technology, Yonsei University",100.0,"south korea, usa",0.0,,"Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many image processing applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010873/,"['Perturbation methods', 'Robustness', 'Signal resolution', 'Spatial resolution', 'Task analysis', 'Interpolation']","['Adversarial Attacks', 'Robust Method', 'Learning-based Methods', 'High Vulnerability', 'Low-resolution Images', 'Deep Learning-based Methods', 'Version Of Image', 'Single Image Super-resolution', 'Convolutional Layers', 'Classification Task', 'Input Image', 'Image Dataset', 'Model Size', 'Generative Adversarial Networks', 'Perception Of Quality', 'Part Of The Image', 'Residual Network', 'Peak Signal-to-noise Ratio', 'Output Image', 'Robust Indicator', 'Super-resolution Model', 'Peak Signal-to-noise Ratio Values', 'Adversarial Examples', 'Attack Methods', 'Fast Gradient Sign Method', 'Super-resolution Task', 'Bicubic Interpolation', 'Advanced Topics', 'Value Of Image', 'Target Image']",,35,"Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many image processing applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks."
Event-Based Motion Segmentation by Motion Compensation,"Timo Stoffregen, Guillermo Gallego, Tom Drummond, Lindsay Kleeman, Davide Scaramuzza","Dept. Informatics (Univ. Zurich) and Dept. Neuroinformatics (Univ. Zurich & ETH Zurich), Switzerland.; Dept. Electrical and Computer Systems Engineering, Monash University, Australia.",100.0,"australia, switzerland",0.0,,"In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called ""events""), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90% accuracy at 4 pixels relative displacement.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Stoffregen_Event-Based_Motion_Segmentation_by_Motion_Compensation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Stoffregen_Event-Based_Motion_Segmentation_by_Motion_Compensation_ICCV_2019_paper.pdf,https://youtu.be/0q6apOSBAk,,,main,Poster,https://ieeexplore.ieee.org/document/9010722/,"['Cameras', 'Motion segmentation', 'Computer vision', 'Image segmentation', 'Tracking', 'Motion compensation', 'Robot vision systems']","['Motion Compensation', 'Event-based Motion', 'Objective Function', 'Motion Parameters', 'Object Motion', 'Relative Displacement', 'Motion Blur', 'Dynamic Scenes', 'Dynamic Vision Sensor', 'Traditional Cameras', 'Computer Vision', 'Pedestrian', 'Image Plane', 'Bounding Box', 'Multiple Objects', 'Translational Motion', 'Rotational Motion', 'Segmentation Accuracy', 'Optical Flow', 'Motion Model', 'Objects In The Scene', 'Object Appearance', 'Simultaneous Localization And Mapping', 'Conventional Camera', 'Event Image', 'Occluded Objects', 'Camera Motion', 'Skateboarding']",,102,"In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called ""events""), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90% accuracy at 4 pixels relative displacement."
Everybody Dance Now,"Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros","UC Berkeley; MIT CSAIL; Humen, Inc.",66.66666666666666,usa,33.33333333333334,USA,"This paper presents a simple method for ""do as I do"" motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We approach this problem as video-to-video translation using pose as an intermediate representation. To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject. We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis. Although our method is quite simple, it produces surprisingly compelling results (see video). This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data. In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chan_Everybody_Dance_Now_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chan_Everybody_Dance_Now_ICCV_2019_paper.pdf,https://youtu.be/mSaIrz8lM1U,,,main,Poster,https://ieeexplore.ieee.org/document/8112897/,,,,,
Evolving Space-Time Neural Architectures for Videos,"AJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. Ryoo",Google Brain,0.0,,100.0,USA,"We present a new method for finding video CNN architectures that more optimally capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing CNN video architectures. We here develop a novel evolutionary algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new diverse and interesting video architectures that were unknown previously. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on four datasets: Kinetics, Charades, Moments in Time and HMDB. We will open source the code and models, to encourage future model development.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Piergiovanni_Evolving_Space-Time_Neural_Architectures_for_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Piergiovanni_Evolving_Space-Time_Neural_Architectures_for_Videos_ICCV_2019_paper.pdf,https://sites.google.com/corp/view/evanet-videoproblem,,,main,Poster,https://ieeexplore.ieee.org/document/9010647/,"['Videos', 'Kernel', 'Three-dimensional displays', 'Computer architecture', 'Two dimensional displays', 'Standards', 'Kinetic theory']","['Neural Architecture', 'Convolutional Neural Network', 'Search Algorithm', 'Evolutionary Algorithms', 'Multiple Datasets', 'Moment In Time', 'Convolutional Neural Network Architecture', 'Video Information', 'Variety Of Architectures', '3D Convolution', 'Evolutionary Search', 'Gaussian Kernel', 'Convolutional Layers', 'Mixture Model', 'Search Space', 'Pooling Layer', 'Action Recognition', 'Random Search', 'Development Of Architecture', 'Video Dataset', 'Neural Architecture Search', '1D Convolutional Layers', 'Parallel Layers', '3D Kernel', '3D Layer', 'Validation Videos', 'Video Understanding', 'Spatial Kernel', 'Standard 3D', 'Temporal Duration']",,34,"We present a new method for finding video CNN architectures that more optimally capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing CNN video architectures. We here develop a novel evolutionary algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new diverse and interesting video architectures that were unknown previously. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on four datasets: Kinetics, Charades, Moments in Time and HMDB. We will open source the code and models, to encourage future model development."
Expectation-Maximization Attention Networks for Semantic Segmentation,"Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, Hong Liu","Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University; Academy for Advanced Interdisciplinary Studies, Peking University; Key Laboratory of Machine Perception (MOE), School of EECS, Peking University; School of Computer Science and Technology, Shandong University",100.0,china,0.0,,"Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PASCAL VOC, PASCAL Context, and COCO Stuff, on which we set new records.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf,https://xialipku.github.io/EMANet,,,main,Oral,https://ieeexplore.ieee.org/document/9009057/,"['Semantics', 'Task analysis', 'Convolution', 'Image segmentation', 'Kernel', 'Computational complexity', 'Convergence']","['Expectation Maximization', 'Semantic Segmentation', 'Expectation-maximization Attention', 'Attention Mechanism', 'Compact Set', 'Attention Map', 'Self-attention Mechanism', 'Neural Network', 'Convolutional Neural Network', 'Posterior Probability', 'Latent Variables', 'Contextual Information', 'Feature Maps', 'Backpropagation', 'Recurrent Neural Network', 'Receptive Field', 'Convolution Kernel', 'High-level Features', 'Video Analysis', 'Gaussian Mixture Model', 'PASCAL VOC Dataset', 'Fully Convolutional Network', 'Challenging Dataset', 'Double Block', 'Memory Cost']",,411,"Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PASCAL VOC, PASCAL Context, and COCO Stuff, on which we set new records
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
Expert Sample Consensus Applied to Camera Re-Localization,"Eric Brachmann, Carsten Rother","Visual Learning Lab, Heidelberg University (HCI/IWR)",100.0,germany,0.0,,"Fitting model parameters to a set of noisy data points is a common problem in computer vision. In this work, we fit the 6D camera pose to a set of noisy correspondences between the 2D input image and a known 3D environment. We estimate these correspondences from the image using a neural network. Since the correspondences often contain outliers, we utilize a robust estimator such as Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the pose parameters. When the problem domain, e.g. the space of all 2D-3D correspondences, is large or ambiguous, a single network does not cover the domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem domain among an ensemble of specialized networks, so called experts, where a gating network decides which expert is responsible for a given input. In this work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a MoE. Our main technical contribution is an efficient method to train ESAC jointly and end-to-end. We demonstrate experimentally that ESAC handles two real-world problems better than competing methods, i.e. scalability and ambiguity. We apply ESAC to fitting simple geometric models to synthetic images, and to camera re-localization for difficult, real datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Brachmann_Expert_Sample_Consensus_Applied_to_Camera_Re-Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Brachmann_Expert_Sample_Consensus_Applied_to_Camera_Re-Localization_ICCV_2019_paper.pdf,http://vislearn.de,,,main,Poster,https://ieeexplore.ieee.org/document/9009994/,"['Cameras', 'Computational modeling', 'Neural networks', 'Robustness', 'Task analysis', 'Data models', 'Training']","['Camera Relocalization', 'Neural Network', 'Model Parameters', 'Ambiguity', 'Input Image', 'Single Network', '3D Environment', 'Problem Domain', 'Random Sample Consensus', 'Camera Pose', 'Fitted Model Parameters', 'Receptive Field', 'Model Parameter Estimates', 'Neural Network Training', 'Part Of Environment', 'Motion Blur', 'Feature-based Methods', 'Ensemble Of Networks', 'Network Of Experts', 'Model Selection Method', '2D Line', 'Inference Of Model Parameters', '2D Point']",,78,"Fitting model parameters to a set of noisy data points is a common problem in computer vision. In this work, we fit the 6D camera pose to a set of noisy correspondences between the 2D input image and a known 3D environment. We estimate these correspondences from the image using a neural network. Since the correspondences often contain outliers, we utilize a robust estimator such as Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the pose parameters. When the problem domain, e.g. the space of all 2D-3D correspondences, is large or ambiguous, a single network does not cover the domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem domain among an ensemble of specialized networks, so called experts, where a gating network decides which expert is responsible for a given input. In this work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a MoE. Our main technical contribution is an efficient method to train ESAC jointly and end-to-end. We demonstrate experimentally that ESAC handles two real-world problems better than competing methods, i.e. scalability and ambiguity. We apply ESAC to fitting simple geometric models to synthetic images, and to camera re-localization for difficult, real datasets."
Explaining Neural Networks Semantically and Quantitatively,"Runjin Chen, Hao Chen, Jie Ren, Ge Huang, Quanshi Zhang",Shanghai Jiao Tong University,100.0,China,0.0,,"This paper presents a method to pursue a semantic and quantitative explanation for the knowledge encoded in a convolutional neural network (CNN). The estimation of the specific rationale of each prediction made by the CNN presents a key issue of understanding neural networks, and it is of significant values in real applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, which explains the CNN prediction quantitatively. We discuss the problem of the biased interpretation of CNN predictions. To overcome the biased interpretation, we develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008520/,"['Visualization', 'Neural networks', 'Semantics', 'Additives', 'Knowledge engineering', 'Feature extraction', 'Task analysis']","['Neural Network', 'Convolutional Neural Network', 'Additive Model', 'Real Applications', 'Interpretation Bias', 'Convolutional Neural Network Prediction', 'Quantitative Explanation', 'Input Image', 'Feature Maps', 'Feature Representation', 'Prediction Score', 'Visual Network', 'Network Output', 'Network Input', 'Intermediate Layer', 'Softmax Layer', 'Semantic Knowledge', 'Object Parts', 'Random Images', 'Interpretation Of Features', 'Visual Concepts', 'Prior Weight', 'Facial Attributes', 'Interpretable Representation', 'Conditional Entropy', 'Early Epoch', 'Core Challenge', 'Pixel Level', 'Residual Network', 'Limited Capacity']",,39,"This paper presents a method to pursue a semantic and quantitative explanation for the knowledge encoded in a convolutional neural network (CNN). The estimation of the specific rationale of each prediction made by the CNN presents a key issue of understanding neural networks, and it is of significant values in real applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, which explains the CNN prediction quantitatively. We discuss the problem of the biased interpretation of CNN predictions. To overcome the biased interpretation, we develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method."
Explaining the Ambiguity of Object Detection and 6D Pose From Visual Data,"Fabian Manhardt, Diego MartÃ­n Arroyo, Christian Rupprecht, Benjamin Busam, Tolga Birdal, Nassir Navab, Federico Tombari",Technical University of Munich; Stanford University; Huawei; Google; University of Oxford,60.0,"germany, uk, usa",40.0,China,"3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with these ambiguities. For each object instance we predict multiple 6D pose outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Manhardt_Explaining_the_Ambiguity_of_Object_Detection_and_6D_Pose_From_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Manhardt_Explaining_the_Ambiguity_of_Object_Detection_and_6D_Pose_From_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009543/,"['Three-dimensional displays', 'Pose estimation', 'Object detection', 'Visualization', 'Shape', 'Quaternions', 'Two dimensional displays']","['Object Detection', '6D Pose', 'Pose Estimation', 'Human Pose Estimation', 'Object Instances', 'Object Pose', '3D Pose', '3D Object Detection', 'Multiple Poses', 'Unambiguous', 'Convolutional Neural Network', 'Singular Value', '3D Space', 'Multiple Hypothesis', 'Rotation Axis', 'Conditional Mean', 'Symmetry Axis', 'Current View', 'Rotational Symmetry', 'Domain Adaptation', '3D Rotation', 'Correct Pose', 'Current Pose', 'Ground Truth Pose', 'Pose Prediction', 'Single Hypothesis', 'Ambiguous Cases', 'Anchor Boxes', '3D Translation', 'CAD Model']",,78,"3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with these ambiguities. For each object instance we predict multiple 6D pose outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation."
Explicit Shape Encoding for Real-Time Instance Segmentation,"Wenqiang Xu, Haiyang Wang, Fubo Qi, Cewu Lu","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; SJTU-SenseTime AI lab",100.0,China,0.0,,"In this paper, we propose a novel top-down instance segmentation framework based on explicit shape encoding, named ESE-Seg. It largely reduces the computational consumption of the instance segmentation by explicitly decoding the multiple object shapes with tensor operations, thus performs the instance segmentation at almost the same speed as the object detection. ESE-Seg is based on a novel shape signature Inner-center Radius (IR), Chebyshev polynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3 outperforms the Mask R-CNN on Pascal VOC 2012 at mAP^r@0.5 while 7 times faster.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Explicit_Shape_Encoding_for_Real-Time_Instance_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Explicit_Shape_Encoding_for_Real-Time_Instance_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010340/,"['Shape', 'Object detection', 'Decoding', 'Encoding', 'Chebyshev approximation', 'Detectors', 'Training']","['Instance Segmentation', 'Explicit Shape', 'Shape Encoder', 'Decoding', 'Object Detection', 'Polynomial Regression', 'Object Shape', 'Mask R-CNN', 'PASCAL VOC', 'Chebyshev Polynomials', 'Tensor Operations', 'Parallelization', 'Grid Cells', 'Dimensional Vector', 'Top-down Approach', 'Bounding Box', 'Function Approximation', 'Fourier Series', 'Polar Coordinates', 'Faster R-CNN', 'Shape Vectors', 'Object Detection Framework', 'Shape Representation', 'Contour Points', 'IoU Threshold', 'Basic Detection', 'One-stage Detectors', 'Object Instances', 'Implicit Representation', 'Explicit Representation']",,67,"In this paper, we propose a novel top-down instance segmentation framework based on explicit shape encoding, named \textbf{ESE-Seg}. It largely reduces the computational consumption of the instance segmentation by explicitly decoding the multiple object shapes with tensor operations, thus performs the instance segmentation at almost the same speed as the object detection. ESE-Seg is based on a novel shape signature Inner-center Radius (IR), Chebyshev polynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3 outperforms the Mask R-CNN on Pascal VOC 2012 at mAP
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">r</sup>
@0.5 while 7 times faster."
Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks,"Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, Nadia Magnenat Thalmann","Nanyang Technological University, Singapore; Monash University, Australia; State University of New York at Buffalo University, Buffalo, NY, USA; Nanyang Technological University, Singapore",100.0,"Singapore, australia, usa",0.0,,"Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Exploiting_Spatial-Temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Exploiting_Spatial-Temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009459/,"['Three-dimensional displays', 'Pose estimation', 'Two dimensional displays', 'Convolution', 'Network architecture', 'Skeleton', 'Kernel']","['Pose Estimation', 'Graph Convolutional Network', 'Graph Convolution', 'Human Pose Estimation', '3D Pose', 'Convolution Operation', 'Spatial Dependence', 'Multi-scale Features', 'Human Hand', 'Temporal Consistency', 'Spatial Consistency', 'Dataset For Estimation', '3D Body', 'Body Configuration', 'Graph-based Representation', 'Convolutional Neural Network', 'Set Of Results', 'Central Node', 'Nodes In The Graph', 'Neighboring Nodes', '2D Pose', 'Hierarchical Architecture', 'Joint Position', 'Camera Coordinate System', 'Bottom-up Processes', 'Input Sequence Length', '3D Joint', 'Loss Of Symmetry', 'Pose Estimation Methods', 'Top-down Processes']",,303,"Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks."
Exploiting Temporal Consistency for Real-Time Video Depth Estimation,"Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, Youliang Yan","University of Adelaide; Noah’s Ark Lab, Huawei; Northwestern Polytechnical University",66.66666666666666,"australia, china",33.33333333333334,China,"Accuracy of depth estimation from static images has been significantly improved recently, by exploiting hierarchical features from deep convolutional neural networks (CNNs). Compared with static images, vast information exists among video frames and can be exploited to improve the depth estimation performance. In this work, we focus on exploring temporal information from monocular videos for depth estimation. Specifically, we take the advantage of convolutional long short-term memory (CLSTM) and propose a novel spatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture not only the spatial features but also the temporal correlations/consistency among consecutive video frames with negligible increase in computational cost. Additionally, in order to maintain the temporal consistency among the estimated depth frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. The temporal consistency loss is combined with the spatial loss to update the model in an end-to-end fashion. By taking advantage of the temporal information, we build a video depth estimation framework that runs in real-time and generates visually pleasant results. Moreover, our approach is flexible and can be generalized to most existing depth estimation frameworks. Code is available at: https://tinyurl.com/STCLSTM",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Exploiting_Temporal_Consistency_for_Real-Time_Video_Depth_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Exploiting_Temporal_Consistency_for_Real-Time_Video_Depth_Estimation_ICCV_2019_paper.pdf,,https://tinyurl.com/STCLSTM,,main,Poster,https://ieeexplore.ieee.org/document/9010400/,"['Estimation', 'Feature extraction', 'Gallium nitride', 'Generative adversarial networks', 'Correlation', 'Real-time systems', 'Streaming media']","['Real-time Estimation', 'Depth Estimation', 'Temporal Consistency', 'Convolutional Neural Network', 'Short-term Memory', 'Spatial Features', 'Long Short-term Memory', 'Deep Convolutional Neural Network', 'Temporal Information', 'Generative Adversarial Networks', 'Video Frames', 'Static Images', 'Monocular', 'General Scheme', 'Consecutive Frames', 'Convolutional Long Short-term Memory', 'Increase In Computational Cost', 'Temporal Loss', 'Consecutive Video Frames', 'Convolutional Layers', 'Ground Truth Depth', 'Spatial Feature Extraction', 'Bundle Adjustment', 'KITTI Dataset', 'Feature Maps', 'Temporal Correlation', 'Depth Map', '3D Convolutional Layers', 'Adversarial Training', 'Baseline Methods']",,68,"Accuracy of depth estimation from static images has been significantly improved recently, by exploiting hierarchical features from deep convolutional neural networks (CNNs). Compared with static images, vast information exists among video frames and can be exploited to improve the depth estimation performance. In this work, we focus on exploring temporal information from monocular videos for depth estimation. Specifically, we take the advantage of convolutional long short-term memory (CLSTM) and propose a novel spatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture not only the spatial features but also the temporal correlations/consistency among consecutive video frames with negligible increase in computational cost. Additionally, in order to maintain the temporal consistency among the estimated depth frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. The temporal consistency loss is combined with the spatial loss to update the model in an end-to-end fashion. By taking advantage of the temporal information, we build a video depth estimation framework that runs in real-time and generates visually pleasant results. Moreover, our approach is flexible and can be generalized to most existing depth estimation frameworks. Code is available at: https://tinyurl.com/STCLSTM"
Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style,"Hongwei Ge, Zehang Yan, Kai Zhang, Mingde Zhao, Liang Sun","Mila, McGill University, Montr´eal, Canada; College of Computer Science and Technology, Dalian University of Technology, Dalian, China",100.0,"canada, china",0.0,,"Image captioning is a research hotspot where encoder-decoder models combining convolutional neural network (CNN) and long short-term memory (LSTM) achieve promising results. Despite significant progress, these models generate sentences differently from human cognitive styles. Existing models often generate a complete sentence from the first word to the end, without considering the influence of the following words on the whole sentence generation. In this paper, we explore the utilization of a human-like cognitive style, i.e., building overall cognition for the image to be described and the sentence to be constructed, for enhancing computer image understanding. This paper first proposes a Mutual-aid network structure with Bidirectional LSTMs (MaBi-LSTMs) for acquiring overall contextual information. In the training process, the forward and backward LSTMs encode the succeeding and preceding words into their respective hidden states by simultaneously constructing the whole sentence in a complementary manner. In the captioning process, the LSTM implicitly utilizes the subsequent semantic information contained in its hidden states. In fact, MaBi-LSTMs can generate two sentences in forward and backward directions. To bridge the gap between cross-domain models and generate a sentence with higher quality, we further develop a cross-modal attention mechanism to retouch the two sentences by fusing their salient parts as well as the salient areas of the image. Experimental results on the Microsoft COCO dataset show that the proposed model improves the performance of encoder-decoder models and achieves state-of-the-art results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010625/,"['Feature extraction', 'Semantics', 'Visualization', 'Decoding', 'Training', 'Computational modeling', 'Cognition']","['Contextual Information', 'Cognitive Style', 'Image Captioning', 'Convolutional Neural Network', 'Short-term Memory', 'Long Short-term Memory', 'Attention Mechanism', 'Hidden State', 'Bidirectional Long Short-term Memory', 'Backward Direction', 'Complementary Manner', 'Text Generation', 'Encoder-decoder Model', 'Decoding', 'Image Features', 'Validation Set', 'Visual Information', 'Image Regions', 'Visual Attention', 'Attention Module', 'Auxiliary Structures', 'Word Embedding', 'Least Square Error', 'Multiple Instance Learning', 'Part Of The Sentence', 'Visual Aspects', 'Beam Search', 'Breadth-first Search', 'Subsequent Word', 'Semantic Aspects']",,12,"Image captioning is a research hotspot where encoder-decoder models combining convolutional neural network (CNN) and long short-term memory (LSTM) achieve promising results. Despite significant progress, these models generate sentences differently from human cognitive styles. Existing models often generate a complete sentence from the first word to the end, without considering the influence of the following words on the whole sentence generation. In this paper, we explore the utilization of a human-like cognitive style, i.e., building overall cognition for the image to be described and the sentence to be constructed, for enhancing computer image understanding. This paper first proposes a Mutual-aid network structure with Bidirectional LSTMs (MaBi-LSTMs) for acquiring overall contextual information. In the training process, the forward and backward LSTMs encode the succeeding and preceding words into their respective hidden states by simultaneously constructing the whole sentence in a complementary manner. In the captioning process, the LSTM implicitly utilizes the subsequent semantic information contained in its hidden states. In fact, MaBi-LSTMs can generate two sentences in forward and backward directions. To bridge the gap between cross-domain models and generate a sentence with higher quality, we further develop a cross-modal attention mechanism to retouch the two sentences by fusing their salient parts as well as the salient areas of the image. Experimental results on the Microsoft COCO dataset show that the proposed model improves the performance of encoder-decoder models and achieves state-of-the-art results."
Exploring Randomly Wired Neural Networks for Image Recognition,"Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He",Facebook AI Research (FAIR),0.0,,100.0,USA,"Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xie_Exploring_Randomly_Wired_Neural_Networks_for_Image_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Exploring_Randomly_Wired_Neural_Networks_for_Image_Recognition_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/RandWire,,main,Oral,https://ieeexplore.ieee.org/document/9010992/,"['Generators', 'Wiring', 'Biological neural networks', 'Computer architecture', 'Graph theory', 'Probability distribution']","['Neural Network', 'Random Model', 'Random Generation', 'ImageNet', 'Random Graph', 'Manual Design', 'Neural Architecture Search', 'Competitive Accuracy', 'Random Graph Models', 'Stochastic Generation', 'Classical Graph', 'Object Detection', 'Authors Of This Paper', 'Mean Accuracy', 'Graph Theory', 'Digital Networks', 'Random Networks', 'Directed Acyclic Graph', 'Random Search', 'Graph Generation', 'Floating-point Operations', 'Output Node', 'Convolution Type', 'Separable Convolution', 'Complete Network', 'Target Node', 'Strong Prior', 'Parameter Count', 'Original Node']",,159,"Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design."
Exploring the Limitations of Behavior Cloning for Autonomous Driving,"Felipe Codevilla, Eder Santana, Antonio M. LÃ³pez, Adrien Gaidon","Computer Vision Center (CVC), Campus UAB, Barcelona, Spain; Toyota Research Institute (TRI), Los Altos, CA, USA.",100.0,"Spain, usa",0.0,,"Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-of-the-art results, executing complex lateral and longitudinal maneuvers, even in unseen environments, without being explicitly programmed to do so. However, we confirm some limitations of the behavior cloning approach: some well-known limitations (e.g., dataset bias and overfitting), new generalization issues (e.g., dynamic objects and the lack of a causal modeling), and training instabilities, all requiring further research before behavior cloning can graduate to real-world driving. The code, dataset, benchmark, and agent studied in this paper can be found at github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md",,http://openaccess.thecvf.com/content_ICCV_2019/html/Codevilla_Exploring_the_Limitations_of_Behavior_Cloning_for_Autonomous_Driving_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Codevilla_Exploring_the_Limitations_of_Behavior_Cloning_for_Autonomous_Driving_ICCV_2019_paper.pdf,,http://github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md,,main,Oral,https://ieeexplore.ieee.org/document/9009463/,"['Cloning', 'Training', 'Benchmark testing', 'Training data', 'Computer vision', 'Autonomous vehicles', 'Vehicle dynamics']","['Autonomous Vehicles', 'Behavior Cloning', 'Limitations Of Approaches', 'Complex Conditions', 'Explicit Model', 'Causal Model', 'Dynamic Objects', 'Dataset Bias', 'Imitation Learning', 'Unseen Environments', 'Collision', 'Training Data', 'Deep Neural Network', 'State Of The Art', 'Single Image', 'Modularity', 'Generalization Performance', 'Stochastic Gradient Descent', 'Simulation Environment', 'Traffic Light', 'Speed Prediction', 'Safety-critical Applications', 'Policy Learning', 'Suburban Environments', 'Agent Dynamics', 'Strong Baseline', 'Km Of Roads', 'Training Environment', 'Percentage Of Success']",,281,"Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-ofthe-art results, executing complex lateral and longitudinal maneuvers, even in unseen environments, without being explicitly programmed to do so. However, we confirm some limitations of the behavior cloning approach: some wellknown limitations (e.g., dataset bias and overfitting), new generalization issues (e.g., dynamic objects and the lack of a causal modeling), and training instabilities, all requiring further research before behavior cloning can graduate to real-world driving. The code, dataset, benchmark, and agent studied in this paper can be found at http:// github.com/felipecode/coiltraine/blob/ master/docs/exploring_limitations.md."
Extreme View Synthesis,"Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H. Kim, Jan Kautz","KAIST; NVIDIA, KAIST; NVIDIA",66.66666666666666,south korea,33.33333333333334,USA,"We present Extreme View Synthesis, a solution for novel view extrapolation that works even when the number of input images is small---as few as two. In this context, occlusions and depth uncertainty are two of the most pressing issues, and worsen as the degree of extrapolation increases. We follow the traditional paradigm of performing depth-based warping and refinement, with a few key improvements. First, we estimate a depth probability volume, rather than just a single depth value for each pixel of the novel view. This allows us to leverage depth uncertainty in challenging regions, such as depth discontinuities. After using it to get an initial estimate of the novel view, we explicitly combine learned image priors and the depth uncertainty to synthesize a refined image with less artifacts. Our method is the first to show visually pleasing results for baseline magnifications of up to 30x.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Extreme_View_Synthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Extreme_View_Synthesis_ICCV_2019_paper.pdf,,https://github.com/NVlabs/extreme-view-synth,,main,Oral,https://ieeexplore.ieee.org/document/9008537/,"['Cameras', 'Uncertainty', 'Estimation', 'Extrapolation', 'Interpolation', 'Three-dimensional displays', 'Probability distribution']","['View Synthesis', 'Input Image', 'Depth Values', 'Prior Imaging', 'Single Image', 'Depth Images', 'Pixel Area', 'Skip Connections', 'Light Field', 'Geometric Constraints', 'Local Deformation', 'Depth Estimation', 'Camera Position', 'Depth Distribution', 'Real Scenes', 'Disparate Levels', 'Homography', 'Scene Representation', 'Stereo Pairs', 'Refinement Network', 'Virtual Camera']",,115,"We present Extreme View Synthesis, a solution for novel view extrapolation that works even when the number of input images is small---as few as two. In this context, occlusions and depth uncertainty are two of the most pressing issues, and worsen as the degree of extrapolation increases. We follow the traditional paradigm of performing depth-based warping and refinement, with a few key improvements. First, we estimate a depth probability volume, rather than just a single depth value for each pixel of the novel view. This allows us to leverage depth uncertainty in challenging regions, such as depth discontinuities. After using it to get an initial estimate of the novel view, we explicitly combine learned image priors and the depth uncertainty to synthesize a refined image with less artifacts. Our method is the first to show visually pleasing results for baseline magnifications of up to 30x."
FAB: A Robust Facial Landmark Detection Framework for Motion-Blurred Videos,"Keqiang Sun, Wayne Wu, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou, Zuochang Ye, Chen Qian","Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University; Amazon Rekognition; Institute of Microelectronics, Tsinghua University; SenseTime Research",50.0,"China, china",50.0,China,"Recently, facial landmark detection algorithms have achieved remarkable performance on static images. However, these algorithms are neither accurate nor stable in motion-blurred videos. The missing of structure information makes it difficult for state-of-the-art facial landmark detection algorithms to yield good results. In this paper, we propose a framework named FAB that takes advantage of structure consistency in the temporal dimension for facial landmark detection in motion-blurred videos. A structure predictor is proposed to predict the missing face structural information temporally, which serves as a geometry prior. This allows our framework to work as a virtuous circle. On one hand, the geometry prior helps our structure-aware deblurring network generates high quality deblurred images which lead to better landmark detection results. On the other hand, better landmark detection results help structure predictor generate better geometry prior for the next frame. Moreover, it is a flexible video-based framework that can incorporate any static image-based methods to provide a performance boost on video datasets. Extensive experiments on Blurred-300VW, the proposed Real-world Motion Blur (RWMB) datasets and 300VW demonstrate the superior performance to the state-of-the-art methods. Datasets and model will be publicly available at  https://github.com/KeqiangSun/FAB  https://github.com/KeqiangSun/FAB .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_FAB_A_Robust_Facial_Landmark_Detection_Framework_for_Motion-Blurred_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_FAB_A_Robust_Facial_Landmark_Detection_Framework_for_Motion-Blurred_Videos_ICCV_2019_paper.pdf,https://keqiangsun.github.io/projects/FAB/FAB.html,https://github.com/keqiangsun,,main,Poster,https://ieeexplore.ieee.org/document/9010316/,"['Face', 'Videos', 'Image edge detection', 'Optical imaging', 'Detection algorithms', 'Reliability', 'Geometry']","['Landmark Detection', 'Facial Landmark Detection', 'Structural Information', 'Structure Prediction', 'Static Images', 'Motion Blur', 'Virtuous Circle', 'Detection In Videos', 'Mean Square Error', 'Temporal Information', 'Face Recognition', 'Video Clips', 'Residual Block', 'Optical Flow', 'Current Frame', 'Large Motion', 'Part Segmentation', 'Previous Frame', 'Landmark Localization', 'Image Sharpness', 'Facial Structure', 'Boundary Map', 'Chicken-and-egg', 'Interocular Distance', 'Image Deblurring', 'Face Alignment']",,20,"Recently, facial landmark detection algorithms have achieved remarkable performance on static images. However, these algorithms are neither accurate nor stable in motion-blurred videos. The missing of structure information makes it difficult for state-of-the-art facial landmark detection algorithms to yield good results. In this paper, we propose a framework named FAB that takes advantage of structure consistency in the temporal dimension for facial landmark detection in motion-blurred videos. A structure predictor is proposed to predict the missing face structural information temporally, which serves as a geometry prior. This allows our framework to work as a virtuous circle. On one hand, the geometry prior helps our structure-aware deblurring network generates high quality deblurred images which lead to better landmark detection results. On the other hand, better landmark detection results help structure predictor generate better geometry prior for the next frame. Moreover, it is a flexible video-based framework that can incorporate any static image-based methods to provide a performance boost on video datasets. Extensive experiments on Blurred-300VW, the proposed Realworld Motion Blur (RWMB) datasets and 300VW demonstrate the superior performance to the state-of-the-art methods. Datasets and models will be publicly available at https://keqiangsun.github.io/projects/FAB/FAB.html."
FACSIMILE: Fast and Accurate Scans From an Image in Less Than a Second,"David Smith, Matthew Loper, Xiaochen Hu, Paris Mavroidis, Javier Romero",Amazon Body Labs,0.0,,100.0,USA,"Current methods for body shape estimation either lack detail or require many images. They are usually architecturally complex and computationally expensive. We propose FACSIMILE (FAX), a method that estimates a detailed body from a single photo, lowering the bar for creating virtual representations of humans. Our approach is easy to implement and fast to execute, making it easily deployable. FAX uses an image-translation network which recovers geometry at the original resolution of the image. Counterintuitively, the main loss which drives FAX is on per-pixel surface normals instead of per-pixel depth, making it possible to estimate detailed body geometry without any depth supervision. We evaluate our approach both qualitatively and quantitatively, and compare with a state-of-the-art method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Smith_FACSIMILE_Fast_and_Accurate_Scans_From_an_Image_in_Less_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Smith_FACSIMILE_Fast_and_Accurate_Scans_From_an_Image_in_Less_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008396/,"['Estimation', 'Geometry', 'Shape', 'Facsimile', 'Avatars', 'Image reconstruction', 'Robustness']","['Body Shape', 'Shape Estimation', 'Surface Normals', 'Training Data', 'Single Image', 'Focal Length', 'Depth Map', '3D Scanning', 'Convex Hull', 'Background Image', 'Depth Values', 'Depth Estimation', 'Direct Losses', 'Spatial Derivatives', 'L1 Loss', 'Single View', 'Normal Term', 'Minimal Clothing', 'Geometry Estimation', 'Synthetic Training Data', '3D Distance']",,38,"Current methods for body shape estimation either lack detail or require many images. They are usually architecturally complex and computationally expensive. We propose FACSIMILE (FAX), a method that estimates a detailed body from a single photo, lowering the bar for creating virtual representations of humans. Our approach is easy to implement and fast to execute, making it easily deployable. FAX uses an image-translation network which recovers geometry at the original resolution of the image. Counterintuitively, the main loss which drives FAX is on per-pixel surface normals instead of per-pixel depth, making it possible to estimate detailed body geometry without any depth supervision. We evaluate our approach both qualitatively and quantitatively, and compare with a state-of-the-art method."
"FAMNet: Joint Learning of Feature, Affinity and Multi-Dimensional Assignment for Online Multiple Object Tracking","Peng Chu, Haibin Ling","Temple University, Philadelphia, PA USA; Stony Brook University, Stony Brook, NY USA",100.0,usa,0.0,,"Data association-based multiple object tracking (MOT) involves multiple separated modules processed or optimized differently, which results in complex method design and requires non-trivial tuning of parameters. In this paper, we present an end-to-end model, named FAMNet, where Feature extraction, Affinity estimation and Multi-dimensional assignment are refined in a single network. All layers in FAMNet are designed differentiable thus can be optimized jointly to learn the discriminative features and higher-order affinity model for robust MOT, which is supervised by the loss directly from the assignment ground truth. In addition, we integrate single object tracking technique and a dedicated target management scheme into the FAMNet-based tracking system to further recover false negatives and inhibit noisy target candidates generated by the external detector. The proposed method is evaluated on a diverse set of benchmarks including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising performance on all of them in comparison with state-of-the-arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010916/,"['Feature extraction', 'Target tracking', 'Tensile stress', 'Trajectory', 'Object tracking', 'Estimation', 'Training']","['Object Tracking', 'Multiple Object Tracking', 'Multi-dimensional Assignment', 'Tracking System', 'Candidate Target', 'Single Network', 'External Detector', 'Affinity Estimation', 'Convolutional Neural Network', 'Deep Network', 'Long Short-term Memory', 'Recurrent Neural Network', 'Pedestrian', 'Bounding Box', 'Normalization Layer', 'Image Frames', 'Consecutive Frames', 'Training Sequences', 'Forward Pass', 'Adjacent Frames', 'Multiple Tracking', 'KITTI Dataset', 'Target Trajectory', 'Optimal Assignment', 'Ground-truth Bounding Box', 'Phase Tracking', 'Vehicle Track', 'Backward Pass', 'Deep Neural Network', 'Set Of Assignments']",,175,"Data association-based multiple object tracking (MOT) involves multiple separated modules processed or optimized differently, which results in complex method design and requires non-trivial tuning of parameters. In this paper, we present an end-to-end model, named FAMNet, where Feature extraction, Affinity estimation and Multi-dimensional assignment are refined in a single network. All layers in FAMNet are designed differentiable thus can be optimized jointly to learn the discriminative features and higher-order affinity model for robust MOT, which is supervised by the loss directly from the assignment ground truth. In addition, we integrate single object tracking technique and a dedicated target management scheme into the FAMNet-based tracking system to further recover false negatives and inhibit noisy target candidates generated by the external detector. The proposed method is evaluated on a diverse set of benchmarks including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising performance on all of them in comparison with state-of-the-arts."
FCOS: Fully Convolutional One-Stage Object Detection,"Zhi Tian, Chunhua Shen, Hao Chen, Tong He","The University of Adelaide, Australia",100.0,australia,0.0,,"We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: https://tinyurl.com/FCOSv1",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf,,https://tinyurl.com/FCOSv1,,main,Poster,https://ieeexplore.ieee.org/document/9010746/,"['Detectors', 'Training', 'Task analysis', 'Object detection', 'Semantics', 'Feature extraction', 'Head']","['Object Detection', 'Detection Performance', 'Semantic Segmentation', 'Detection Framework', 'Faster R-CNN', 'Non-maximum Suppression', 'Anchor Boxes', 'One-stage Detectors', 'Positive Samples', 'Convolutional Layers', 'Input Image', 'Feature Maps', 'Negative Samples', 'Intersection Over Union', 'Bounding Box', 'Large Margin', 'Minimum Area', 'Central Objective', 'Fully Convolutional Network', 'Feature Pyramid Network', 'Ground-truth Box', 'Region Proposal Network', 'Ground-truth Bounding Box', 'Predicted Bounding Box', 'Simple Alternatives', 'Regression Branch', 'CNN Backbone', 'Keypoint Detection', 'Group Normalization', 'Current Detection']",,3822,"We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: https://tinyurl.com/FCOSv1."
FDA: Feature Disruptive Attack,"Aditya Ganeshan, Vivek B.S., R. Venkatesh Babu","Video Analytics Lab, Indian Institute of Science, India; Preferred Networks Inc., Tokyo, Japan",50.0,India,50.0,Japan,"Though Deep Neural Networks (DNN) show excellent performance across various computer vision tasks, several works show their vulnerability to adversarial samples, i.e., image samples with imperceptible noise engineered to manipulate the network's prediction. Adversarial sample generation methods range from simple to complex optimization techniques. Majority of these methods generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new attack FDA: Feature Disruptive attack, to address the drawbacks of existing attacks. FDA works by generating image perturbation that disrupts features at each layer of the network and causes deep-features to be highly corrupt. This allows FDA adversaries to severely reduce the performance of deep networks. We experimentally validate that FDA generates stronger adversaries than other state-of-the-art methods for Image classification, even in the presence of various defense measures. More importantly, we show that FDA disrupts feature-representation based tasks even without access to the task-specific network or methodology.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ganeshan_FDA_Feature_Disruptive_Attack_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ganeshan_FDA_Feature_Disruptive_Attack_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008248/,"['Task analysis', 'Measurement', 'Training', 'Feature extraction', 'Optimization methods', 'Computer vision']","['Corruption', 'Deep Neural Network', 'Softmax', 'Network Layer', 'Adversarial Attacks', 'Image Classification Methods', 'Sample Characteristics', 'Defense Mechanisms', 'Feature Representation', 'Image Recognition', 'Deep Features', 'Prediction Probability', 'Clean Samples', 'ImageNet Dataset', 'Adversarial Training', 'Deep Neural Network Architecture', 'Intermediate Representation', 'Attack Methods', 'Label Probability', 'Fast Gradient Sign Method']",,58,"Though Deep Neural Networks (DNN) show excellent performance across various computer vision tasks, several works show their vulnerability to adversarial samples, i.e., image samples with imperceptible noise engineered to manipulate the network's prediction. Adversarial sample generation methods range from simple to complex optimization techniques. Majority of these methods generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new attack FDA: Feature Disruptive attack, to address the drawbacks of existing attacks. FDA works by generating image perturbation that disrupts features at each layer of the network and causes deep-features to be highly corrupt. This allows FDA adversaries to severely reduce the performance of deep networks. We experimentally validate that FDA generates stronger adversaries than other state-of-the-art methods for Image classification, even in the presence of various defense measures. More importantly, we show that FDA disrupts feature-representation based tasks even without access to the task-specific network or methodology."
FSGAN: Subject Agnostic Face Swapping and Reenactment,"Yuval Nirkin, Yosi Keller, Tal Hassner","Bar-Ilan University, Israel; The Open University of Israel, Israel",100.0,"Israel, israel",0.0,,"We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.pdf,https://nirkin.com/fsgan,,,main,Poster,https://ieeexplore.ieee.org/document/9010341/,"['Face', 'Image segmentation', 'Generators', 'Gallium nitride', 'Training', 'Three-dimensional displays', 'Hair']","['Deepfake', 'Recurrent Neural Network', 'Generative Adversarial Networks', 'Video Sequences', 'Perceptual Loss', 'Delaunay Triangulation', 'Face Pairs', 'Face View', 'Quantitative Results', 'Facial Expressions', 'Qualitative Results', 'Target Image', 'Latent Space', 'Small Step', 'Face Images', 'Target Domain', '3D Shape', 'Euler Angles', 'Reconstruction Loss', 'Consistency Loss', '3D Face', 'Pairs Of Subjects', 'Generative Adversarial Networks Training', 'Inpainting', 'Target Face', 'Background Removal', 'Facial Shape', 'Angular Domain', 'Roll Angle', 'NVIDIA Tesla V100 GPU']",,319,"We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior."
FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On,"Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, Jian Yin","School of Intelligent Systems Engineering, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou 510006, P.R.China; School of Data and Computer Science, Sun Yat-sen University; ByteDance AI Lab",60.0,China,40.0,China,"Beyond current image-based virtual try-on systems that have attracted increasing attention, we move a step forward to developing a video virtual try-on system that precisely transfers clothes onto the person and generates visually realistic videos conditioned on arbitrary poses. Besides the challenges in image-based virtual try-on (e.g., clothes fidelity, image synthesis), video virtual try-on further requires spatiotemporal consistency. Directly adopting existing image-based approaches often fails to generate coherent video with natural and realistic textures. In this work, we propose Flow-navigated Warping Generative Adversarial Network (FW-GAN), a novel framework that learns to synthesize the video of virtual try-on based on a person image, the desired clothes image, and a series of target poses. FW-GAN aims to synthesize the coherent and natural video while manipulating the pose and clothes. It consists of: (i) a flow-guided fusion module that warps the past frames to assist synthesis, which is also adopted in the discriminator to help enhance the coherence and quality of the synthesized video; (ii) a warping net that is designed to warp clothes image for the refinement of clothes textures; (iii) a parsing constraint loss that alleviates the problem caused by the misalignment of segmentation maps from images with different poses and various clothes. Experiments on our newly collected dataset show that FW-GAN can synthesize high-quality video of virtual try-on and significantly outperforms other methods both qualitatively and quantitatively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009020/,"['Integrated circuits', 'Generators', 'Gallium nitride', 'Three-dimensional displays', 'Image generation', 'Computational modeling', 'Visualization']","['Generative Adversarial Networks', 'Virtual Try-on', 'Image Synthesis', 'Person Image', 'Desirable Image', 'Target Pose', 'Objective Function', 'Quantitative Results', 'Input Image', 'Feature Maps', 'Single Image', 'Video Clips', 'Global View', 'Optical Flow', 'Consecutive Frames', 'Flow Mode', 'Conditional Generative Adversarial Network', 'Local View', 'Fr√©chet Inception Distance', 'Personal Appearance', 'Photo-realistic Images']",,62,"Beyond current image-based virtual try-on systems that have attracted increasing attention, we move a step forward to developing a video virtual try-on system that precisely transfers clothes onto the person and generates visually realistic videos conditioned on arbitrary poses. Besides the challenges in image-based virtual try-on (e.g., clothes fidelity, image synthesis), video virtual try-on further requires spatiotemporal consistency. Directly adopting existing image-based approaches often fails to generate coherent video with natural and realistic textures. In this work, we propose Flow-navigated Warping Generative Adversarial Network (FW-GAN), a novel framework that learns to synthesize the video of virtual try-on based on a person image, the desired clothes image, and a series of target poses. FW-GAN aims to synthesize the coherent and natural video while manipulating the pose and clothes. It consists of: (i) a flow-guided fusion module that warps the past frames to assist synthesis, which is also adopted in the discriminator to help enhance the coherence and quality of the synthesized video; (ii) a warping net that is designed to warp clothes image for the refinement of clothes textures; (iii) a parsing constraint loss that alleviates the problem caused by the misalignment of segmentation maps from images with different poses and various clothes. Experiments on our newly collected dataset show that FW-GAN can synthesize high-quality video of virtual try-on and significantly outperforms other methods both qualitatively and quantitatively."
Face Alignment With Kernel Density Deep Neural Network,"Lisha Chen, Hui Su, Qiang Ji","Rensselaer Polytechnic Institute; Rensselaer Polytechnic Institute, IBM Research",100.0,usa,0.0,,"Deep neural networks achieve good performance in many computer vision problems such as face alignment. However, when the testing image is challenging due to low resolution, occlusion or adversarial attacks, the accuracy of a deep neural network suffers greatly. Therefore, it is important to quantify the uncertainty in its predictions. A probabilistic neural network with Gaussian distribution over the target is typically used to quantify uncertainty for regression problems. However, in real-world problems especially computer vision tasks, the Gaussian assumption is too strong. To model more general distributions, such as multi-modal or asymmetric distributions, we propose to develop a kernel density deep neural network. Specifically, for face alignment, we adapt state-of-the-art hourglass neural network into a probabilistic neural network framework with landmark probability map as its output. The model is trained by maximizing the conditional log likelihood. To exploit the output probability map, we extend the model to multi-stage so that the logits map from the previous stage can feed into the next stage to progressively improve the landmark detection accuracy. Extensive experiments on benchmark datasets against state-of-the-art unconstrained deep learning method demonstrate that the proposed kernel density network achieves comparable or superior performance in terms of prediction accuracy. It further provides aleatoric uncertainty estimation in predictions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Face_Alignment_With_Kernel_Density_Deep_Neural_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Face_Alignment_With_Kernel_Density_Deep_Neural_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009569/,"['Face', 'Neural networks', 'Heating systems', 'Kernel', 'Uncertainty', 'Estimation', 'Gaussian distribution']","['Neural Network', 'Deep Neural Network', 'Kernel Density', 'Face Alignment', 'Normal Distribution', 'Deep Learning', 'Low Resolution', 'Bimodal', 'Probability Function', 'Regression Problem', 'Real-world Problems', 'General Distribution', 'Adversarial Attacks', 'Conditional Likelihood', 'Aleatoric Uncertainty', 'Landmark Detection', 'Probabilistic Neural Network', 'Loss Function', 'Gaussian Kernel', 'Gaussian Mixture Distribution', 'Prediction Uncertainty', 'L2 Loss', 'Kernel Estimation', 'Pixel Location', 'Negative Log-likelihood', 'Pose Estimation', 'Landmark Coordinates', 'Heuristic Method', 'Landmark Localization']",,20,"Deep neural networks achieve good performance in many computer vision problems such as face alignment. However, when the testing image is challenging due to low resolution, occlusion or adversarial attacks, the accuracy of a deep neural network suffers greatly. Therefore, it is important to quantify the uncertainty in its predictions. A probabilistic neural network with Gaussian distribution over the target is typically used to quantify uncertainty for regression problems. However, in real-world problems especially computer vision tasks, the Gaussian assumption is too strong. To model more general distributions, such as multi-modal or asymmetric distributions, we propose to develop a kernel density deep neural network. Specifically, for face alignment, we adapt state-of-the-art hourglass neural network into a probabilistic neural network framework with landmark probability map as its output. The model is trained by maximizing the conditional log likelihood. To exploit the output probability map, we extend the model to multi-stage so that the logits map from the previous stage can feed into the next stage to progressively improve the landmark detection accuracy. Extensive experiments on benchmark datasets against state-of-the-art unconstrained deep learning method demonstrate that the proposed kernel density network achieves comparable or superior performance in terms of prediction accuracy. It further provides aleatoric uncertainty estimation in predictions."
Face De-Occlusion Using 3D Morphable Model and Generative Adversarial Network,"Xiaowei Yuan, In Kyu Park","Dept. of Information and Communication Engineering, Inha University, Incheon 22212, Korea",100.0,Korea,0.0,,"In recent decades, 3D morphable model (3DMM) has been commonly used in image-based photorealistic 3D face reconstruction. However, face images are often corrupted by serious occlusion by non-face objects including eyeglasses, masks, and hands. Such objects block the correct capture of landmarks and shading information. Therefore, the reconstructed 3D face model is hardly reusable. In this paper, a novel method is proposed to restore de-occluded face images based on inverse use of 3DMM and generative adversarial network. We utilize the 3DMM prior to the proposed adversarial network and combine a global and local adversarial convolutional neural network to learn face de-occlusion model. The 3DMM serves not only as geometric prior but also proposes the face region for the local discriminator. Experiment results confirm the effectiveness and robustness of the proposed algorithm in removing challenging types of occlusions with various head poses and illumination. Furthermore, the proposed method reconstructs the correct 3D face model with de-occluded textures.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yuan_Face_De-Occlusion_Using_3D_Morphable_Model_and_Generative_Adversarial_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Face_De-Occlusion_Using_3D_Morphable_Model_and_Generative_Adversarial_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010326/,"['Face', 'Three-dimensional displays', 'Image reconstruction', 'Generators', 'Solid modeling', 'Gallium nitride']","['Generative Adversarial Networks', '3D Morphable Model', 'Face Images', 'Oral And Maxillofacial Surgery', 'Face Model', 'Head Pose', '3D Face', 'Convolution', 'Training Dataset', '3D Reconstruction', 'Face Recognition', 'Actual Image', 'Challenging Dataset', 'Image Synthesis', 'Reconstruction Loss', 'Generation Module', 'Generator Output', 'Masked Images', 'Facial Appearance', 'Detailed Reconstruction', 'Occluded Regions', 'Face Dataset', 'Facial Attributes', 'Total Variation Regularization', 'Missing Regions', 'Representation Of Geometry']",,37,"In recent decades, 3D morphable model (3DMM) has been commonly used in image-based photorealistic 3D face reconstruction. However, face images are often corrupted by serious occlusion by non-face objects including eyeglasses, masks, and hands. Such objects block the correct capture of landmarks and shading information. Therefore, the reconstructed 3D face model is hardly reusable. In this paper, a novel method is proposed to restore de-occluded face images based on inverse use of 3DMM and generative adversarial network. We utilize the 3DMM prior to the proposed adversarial network and combine a global and local adversarial convolutional neural network to learn face de-occlusion model. The 3DMM serves not only as geometric prior but also proposes the face region for the local discriminator. Experiment results confirm the effectiveness and robustness of the proposed algorithm in removing challenging types of occlusions with various head poses and illumination. Furthermore, the proposed method reconstructs the correct 3D face model with de-occluded textures."
Face Video Deblurring Using 3D Facial Priors,"Wenqi Ren, Jiaolong Yang, Senyou Deng, David Wipf, Xiaochun Cao, Xin Tong","Microsoft Research Asia; SKLOIS, IIE, CAS; University of Chinese Academy of Sciences",66.66666666666666,china,33.33333333333334,USA,"Existing face deblurring methods only consider single frames and do not account for facial structure and identity information. These methods struggle to deblur face videos that exhibit significant pose variations and misalignment. In this paper we propose a novel face video deblurring network capitalizing on 3D facial priors. The model consists of two main branches: i) a face video deblurring sub-network based on an encoder-decoder architecture, and ii) a 3D face reconstruction and rendering branch for predicting 3D priors of salient facial structures and identity knowledge. These structures encourage the deblurring branch to generate sharp faces with detailed structures. Our method not only uses low-level information (i.e., image intensity), but also middle-level information (i.e., 3D facial structure) and high-level knowledge (i.e., identity content) to further explore spatial constraints of facial components from blurry face frames. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ren_Face_Video_Deblurring_Using_3D_Facial_Priors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ren_Face_Video_Deblurring_Using_3D_Facial_Priors_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009456/,"['Face', 'Three-dimensional displays', 'Image reconstruction', 'Image restoration', 'Rendering (computer graphics)', 'Image edge detection', 'Semantics']","['Video Deblurring', 'Facial Priors', '3D Reconstruction', 'Identity Information', 'Intensity Information', 'Face Identity', 'Oral And Maxillofacial Surgery', 'Facial Structure', '3D Face', 'Training Data', 'Convolutional Neural Network', 'Quantitative Results', 'Generative Adversarial Networks', 'Natural Images', 'Face Images', 'Optical Flow', 'Model Projections', 'Semantic Labels', 'Image Sharpness', 'Motion Blur', 'Image Deblurring', 'Facial Components', 'Blur Kernel', 'Multi-scale Strategy', 'PSNR Values', 'Facial Shape', 'Dynamic Scenes', 'Blurred Images', 'Prior Imaging', 'Center Of Frame']",,35,"Existing face deblurring methods only consider single frames and do not account for facial structure and identity information. These methods struggle to deblur face videos that exhibit significant pose variations and misalignment. In this paper we propose a novel face video deblurring network capitalizing on 3D facial priors. The model consists of two main branches: i) a face video deblurring sub-network based on an encoder-decoder architecture, and ii) a 3D face reconstruction and rendering branch for predicting 3D priors of salient facial structures and identity knowledge. These structures encourage the deblurring branch to generate sharp faces with detailed structures. Our method not only uses low-level information (i.e., image intensity), but also middle-level information (i.e., 3D facial structure) and high-level knowledge (i.e., identity content) to further explore spatial constraints of facial components from blurry face frames. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods."
Face-to-Parameter Translation for Game Character Auto-Creation,"Tianyang Shi, Yi Yuan, Changjie Fan, Zhengxia Zou, Zhenwei Shi, Yong Liu","University of Michigan, Ann Arbor; Beihang University; Zhejiang University; NetEase Fuxi AI Lab",75.0,"China, china, usa",25.0,China,"Character customization system is an important component in Role-Playing Games (RPGs), where players are allowed to edit the facial appearance of their in-game characters with their own preferences rather than using default templates. This paper proposes a method for automatically creating in-game characters of players according to an input face photo. We formulate the above ""artistic creation"" process under a facial similarity measurement and parameter searching paradigm by solving an optimization problem over a large set of physically meaningful facial parameters. To effectively minimize the distance between the created face and the real one, two loss functions, i.e. a ""discriminative loss"" and a ""facial content loss"", are specifically designed. As the rendering process of a game engine is not differentiable, a generative network is further introduced as an ""imitator"" to imitate the physical behavior of the game engine so that the proposed method can be implemented under a neural style transfer framework and the parameters can be optimized by gradient descent. Experimental results demonstrate that our method achieves a high degree of generation similarity between the input face photo and the created in-game character in terms of both global appearance and local details. Our method has been deployed in a new game last year and has now been used by players over 1 million times.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010862/,"['Face', 'Games', 'Three-dimensional displays', 'Engines', 'Loss measurement', 'Solid modeling', 'Rendering (computer graphics)']","['Game Characters', 'Loss Function', 'Imitation', 'Gradient Descent', 'Similarity Measure', 'High Degree Of Similarity', 'Loss Of Content', 'Local Details', 'Creative Arts', 'Facial Appearance', 'Facial Photographs', 'Style Transfer', 'Discriminator Loss', 'Game Engine', 'Role-playing Games', 'Facial Measurements', 'Semantic', 'Deep Learning', 'Convolutional Neural Network', 'Deep Neural Network', 'Fréchet Inception Distance', '3D Face', 'Facial Features', 'Local Method', 'Generative Adversarial Networks', 'Face Recognition Model', 'Large Set Of Parameters', 'Face Images', 'One-hot Encoding', 'Face Alignment']",,34,"Character customization system is an important component in Role-Playing Games (RPGs), where players are allowed to edit the facial appearance of their in-game characters with their own preferences rather than using default templates. This paper proposes a method for automatically creating in-game characters of players according to an input face photo. We formulate the above ""artistic creation"" process under a facial similarity measurement and parameter searching paradigm by solving an optimization problem over a large set of physically meaningful facial parameters. To effectively minimize the distance between the created face and the real one, two loss functions, i.e. a ""discriminative loss"" and a ""facial content loss"", are specifically designed. As the rendering process of a game engine is not differentiable, a generative network is further introduced as an ""imitator"" to imitate the physical behavior of the game engine so that the proposed method can be implemented under a neural style transfer framework and the parameters can be optimized by gradient descent. Experimental results demonstrate that our method achieves a high degree of generation similarity between the input face photo and the created in-game character in terms of both global appearance and local details. Our method has been deployed in a new game last year and has now been used by players over 1 million times."
FaceForensics++: Learning to Detect Manipulated Facial Images,"Andreas RÃ¶ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias NieÃner",University of Erlangen-Nuremberg; Technical University of Munich; University Federico II of Naples,100.0,"Germany, Italy, germany",0.0,,"The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.pdf,http://kaldir.vc.in.tum.de/faceforensics_benchmark,https://github.com/ondyari/FaceForensics,,main,Poster,https://ieeexplore.ieee.org/document/9010912/,"['Face', 'Videos', 'Forgery', 'Benchmark testing', 'Forensics', 'Three-dimensional displays', 'Databases']","['Benchmark', 'Use Of Imaging', 'Digital Content', 'Human Observers', 'Fake News', 'Use Of Face', 'Deepfake', 'Model Performance', 'Deep Learning', 'Convolutional Neural Network', 'Support Vector Machine', 'Facial Expressions', 'User Study', 'Level Of Quality', 'Generative Adversarial Networks', 'Handcrafted Features', 'Field Of Computer Vision', 'Image Synthesis', 'Computer Graphics', 'Face Detection', 'Personal Face', 'Video Quality', 'Fake Images', 'Raw Video', 'Target Face', 'Domain-specific Information', 'Forensic Analysis', 'Fully-connected Layer', 'Manual Selection', 'Computer Vision']",,1313,"The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers."
Fair Loss: Margin-Aware Reinforcement Learning for Deep Face Recognition,"Bingyu Liu, Weihong Deng, Yaoyao Zhong, Mei Wang, Jiani Hu, Xunqiang Tao, Yaohai Huang","Beijing University of Posts and Telecommunications; Canon Information Technology (Beijing) Co., Ltd",50.0,China,50.0,China,"Recently, large-margin softmax loss methods, such as angular softmax loss (SphereFace), large margin cosine loss (CosFace), and additive angular margin loss (ArcFace), have demonstrated impressive performance on deep face recognition. These methods incorporate a fixed additive margin to all the classes, ignoring the class imbalance problem. However, imbalanced problem widely exists in various real-world face datasets, in which samples from some classes are in a higher number than others. We argue that the number of a class would influence its demand for the additive margin. In this paper, we introduce a new margin-aware reinforcement learning based loss function, namely fair loss, in which each class will learn an appropriate adaptive margin by Deep Q-learning. Specifically, we train an agent to learn a margin adaptive strategy for each class, and make the additive margins for different classes more reasonable. Our method has better performance than present large-margin loss functions on three benchmarks, Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace, which demonstrates that our method could learn better face representation on imbalanced face datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Fair_Loss_Margin-Aware_Reinforcement_Learning_for_Deep_Face_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Fair_Loss_Margin-Aware_Reinforcement_Learning_for_Deep_Face_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009488/,"['Training', 'Face', 'Face recognition', 'Additives', 'Adaptation models', 'Feature extraction', 'Learning (artificial intelligence)']","['Face Recognition', 'Deep Face', 'Deep Face Recognition', 'Loss Function', 'Large Margin', 'Class Imbalance', 'Imbalanced Datasets', 'Class Imbalance Problem', 'Face Dataset', 'Deep Q-learning', 'Softmax Loss', 'Additional Margin', 'Neural Network', 'Large Datasets', 'Training Dataset', 'Deep Models', 'Major Classes', 'Small Datasets', 'Number Of Images', 'Deep Convolutional Neural Network', 'Intra-class Variance', 'Trained Agent', 'Pre-trained Network', 'Minority Class', 'Discriminative Feature Learning', 'Q-function', 'Markov Decision Process', 'Imbalanced Learning', 'Gallery Set', 'CNN Model']",,53,"Recently, large-margin softmax loss methods, such as angular softmax loss (SphereFace), large margin cosine loss (CosFace), and additive angular margin loss (ArcFace), have demonstrated impressive performance on deep face recognition. These methods incorporate a fixed additive margin to all the classes, ignoring the class imbalance problem. However, imbalanced problem widely exists in various real-world face datasets, in which samples from some classes are in a higher number than others. We argue that the number of a class would influence its demand for the additive margin. In this paper, we introduce a new margin-aware reinforcement learning based loss function, namely fair loss, in which each class will learn an appropriate adaptive margin by Deep Q-learning. Specifically, we train an agent to learn a margin adaptive strategy for each class, and make the additive margins for different classes more reasonable. Our method has better performance than present large-margin loss functions on three benchmarks, Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace, which demonstrates that our method could learn better face representation on imbalanced face datasets."
Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid,"Zhanghui Kuang, Yiming Gao, Guanbin Li, Ping Luo, Yimin Chen, Liang Lin, Wayne Zhang",Sun Yat-sen University; The University of Hong Kong; SenseTime Research,66.66666666666666,"China, Hong Kong",33.33333333333334,China,"Matching clothing images from customers and online shopping stores has rich applications in E-commerce. Existing algorithms encoded an image as a global feature vector and performed retrieval with the global representation. However, discriminative local information on clothes are submerged in this global representation, resulting in sub-optimal performance. To address this issue, we propose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which learns similarities between a query and a gallery cloth by using both global and local representations in multiple scales. The similarity pyramid is represented by a Graph of similarity, where nodes represent similarities between clothing components at different scales, and the final matching score is obtained by message passing along edges. In GRNet, graph reasoning is solved by training a graph convolutional network, enabling to align salient clothing components to improve clothing retrieval. To facilitate future researches, we introduce a new benchmark FindFashion, containing rich annotations of bounding boxes, views, occlusions, and cropping. Extensive experiments show that GRNet obtains new state-of-the-art results on two challenging benchmarks, e.g. pushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%, and 75% (i.e. 4%, 10%, and 10% absolute improvements), outperforming competitors with large margins. On FindFashion, GRNet achieves considerable improvements on all empirical settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kuang_Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kuang_Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9200778/,"['Clothing', 'Cognition', 'Benchmark testing', 'Feature extraction', 'Task analysis', 'Training', 'Microsoft Windows']","['Graph Reasoning', 'Fashion Retrieval', 'Benchmark', 'Global Features', 'Bounding Box', 'Multi-scale Features', 'Graph Convolutional Network', 'Graph Convolution', 'Global Representation', 'Adaptive Modulation', 'Top-1 Accuracy', 'Multi-scale Representation', 'Absolute Improvement', 'Adaptive Window', 'Global Matching', 'Salient Components', 'Training Set', 'Convolutional Neural Network', 'Local Features', 'Global Similarity', 'Similar Position', 'Gallery Images', 'Spatial Window', 'Retrieval Performance', 'Feature Maps', 'Metric Learning', 'Query Image', 'Large-scale Scenarios', 'Graph Neural Networks']","['Fashion retrieval', 'graph reasoning', 'similarity pyramid']",10,"Matching clothing images from customers and online shopping stores has rich applications in e-commerce. Existing algorithms mostly encode an image as a global feature vector and perform retrieval via global representation matching. However, distinctive local information on clothing is immersed in this global representation, resulting in sub-optimized performance. To address this issue, we propose a novel graph reasoning network (GRNet) on a similarity pyramid, which learns similarities between a query and a gallery cloth by using both initial pairwise multi-scale feature representations and matching propagation for unaligned representations. The query local representations at each scale are aligned with those of the gallery via an adaptive window pooling module. The similarity pyramid is represented by a similarity graph, where nodes represent similarities between clothing components at different scales, and the final matching score is obtained by message propagation along edges. In GRNet, graph reasoning is solved by training a graph convolutional network, enabling the alignment of salient clothing components to improve clothing retrieval. To facilitate future research, we introduce a new benchmark, i.e. FindFashion, containing rich annotations of bounding boxes, views, occlusions, and cropping. Extensive experiments show that GRNet obtains new state-of-the-art results on three challenging benchmarks, e.g. pushing the accuracy of top-1, top-20, and top-50 on DeepFashion to 27, 66, and 75 percent (i.e. 6, 12, and 10 percent absolute improvements), outperforming competitors with large margins. On FindFashion, GRNet achieves considerable improvements on all empirical settings."
Fashion++: Minimal Edits for Outfit Improvement,"Wei-Lin Hsiao, Isay Katsman, Chao-Yuan Wu, Devi Parikh, Kristen Grauman",Georgia Tech; Cornell Tech; UT Austin,100.0,usa,0.0,,"Given an outfit, what small changes would most improve its fashionability? This question presents an intriguing new computer vision challenge. We introduce Fashion++, an approach that proposes minimal adjustments to a full-body clothing outfit that will have maximal impact on its fashionability. Our model consists of a deep image generation neural network that learns to synthesize clothing conditioned on learned per-garment encodings. The latent encodings are explicitly factorized according to shape and texture, thereby allowing direct edits for both fit/presentation and color/patterns/material, respectively. We show how to bootstrap Web photos to automatically train a fashionability model, and develop an activation maximization-style approach to transform the input image into its more fashionable self. The edits suggested range from swapping in a new garment to tweaking its color, how it is worn (e.g., rolling up sleeves), or its fit (e.g., making pants baggier). Experiments demonstrate that Fashion++ provides successful edits, both according to automated metrics and human opinion.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hsiao_Fashion_Minimal_Edits_for_Outfit_Improvement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hsiao_Fashion_Minimal_Edits_for_Outfit_Improvement_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009525/,"['Clothing', 'Shape', 'Image generation', 'Image color analysis', 'Market research', 'Generators', 'Encoding']","['Minimal Editing', 'Factorization', 'Image Generation', 'Feature Maps', 'Amount Of Change', 'User Study', 'Shape Features', 'Multilayer Perceptron', 'Image Pairs', 'Texture Features', 'Generative Adversarial Networks', 'Semantic Segmentation', 'Model Discrimination', 'Regional Mapping', 'Segmentation Map', 'Variational Autoencoder', 'Point Likert Scale', 'Image Synthesis', 'Image X', 'Gradient Step']",,53,"Given an outfit, what small changes would most improve its fashionability? This question presents an intriguing new computer vision challenge. We introduce Fashion++, an approach that proposes minimal adjustments to a full-body clothing outfit that will have maximal impact on its fashionability. Our model consists of a deep image generation neural network that learns to synthesize clothing conditioned on learned per-garment encodings. The latent encodings are explicitly factorized according to shape and texture, thereby allowing direct edits for both fit/presentation and color/patterns/material, respectively. We show how to bootstrap Web photos to automatically train a fashionability model, and develop an activation maximization-style approach to transform the input image into its more fashionable self. The edits suggested range from swapping in a new garment to tweaking its color, how it is worn (e.g., rolling up sleeves), or its fit (e.g., making pants baggier). Experiments demonstrate that Fashion++ provides successful edits, both according to automated metrics and human opinion."
Fast Computation of Content-Sensitive Superpixels and Supervoxels Using Q-Distances,"Zipeng Ye, Ran Yi, Minjing Yu, Yong-Jin Liu, Ying He",Tianjin University; Tsinghua University; Nanyang Technological University,100.0,"China, Singapore, china",0.0,,"State-of-the-art researches model the data of images and videos as low-dimensional manifolds and generate superpixels/supervoxels in a content-sensitive way, which is achieved by computing geodesic centroidal Voronoi tessellation (GCVT) on manifolds. However, computing exact GCVTs is slow due to computationally expensive geodesic distances. In this paper, we propose a much faster queue-based graph distance (called q-distance). Our key idea is that for manifold regions in which q-distances are different from geodesic distances, GCVT is prone to placing more generators in them, and therefore after few iterations, the q-distance-induced tessellation is an exact GCVT. This idea works well in practice and we also prove it theoretically under moderate assumption. Our method is simple and easy to implement. It runs 6-8 times faster than state-of-the-art GCVT computation, and has an optimal approximation ratio O(1) and a linear time complexity O(N) for N-pixel images or N-voxel videos. A thorough evaluation of 31 superpixel methods on five image datasets and 8 supervoxel methods on four video datasets shows that our method consistently achieves the best over-segmentation accuracy. We also demonstrate the advantage of our method on one image and two video applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Fast_Computation_of_Content-Sensitive_Superpixels_and_Supervoxels_Using_Q-Distances_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Fast_Computation_of_Content-Sensitive_Superpixels_and_Supervoxels_Using_Q-Distances_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008246/,"['Videos', 'Manifolds', 'Measurement', 'Approximation algorithms', 'Cascading style sheets', 'Image color analysis', 'Computer vision']","['Manifold', 'Input Image', 'Image Dataset', 'Diversity Metrics', 'Voronoi Diagram', 'Optimal Imaging', 'Linear Complexity', 'Geodesic Distance', 'Video Dataset', 'Graph Metrics', 'Linear Time Complexity', 'Supplemental Material', 'Running Time', 'F1 Score', 'Shortest Path', 'Shortest Distance', 'Cluster Centers', 'Dijkstraâ€™s Algorithm', 'Inverse Mapping', 'Pixel Color', 'Priority Queue', 'CIELAB Color Space', 'Ground Truth Segmentation']",,10,"State-of-the-art researches model the data of images and videos as low-dimensional manifolds and generate superpixels/supervoxels in a content-sensitive way, which is achieved by computing geodesic centroidal Voronoi tessellation (GCVT) on manifolds. However, computing exact GCVTs is slow due to computationally expensive geodesic distances. In this paper, we propose a much faster queue-based graph distance (called q-distance). Our key idea is that for manifold regions in which q-distances are different from geodesic distances, GCVT is prone to placing more generators in them, and therefore after few iterations, the q-distance-induced tessellation is an exact GCVT. This idea works well in practice and we also prove it theoretically under moderate assumption. Our method is simple and easy to implement. It runs 6-8 times faster than state-of-the-art GCVT computation, and has an optimal approximation ratio O(1) and a linear time complexity O(N) for N-pixel images or N-voxel videos. A thorough evaluation of 31 superpixel methods on five image datasets and 8 supervoxel methods on four video datasets shows that our method consistently achieves the best over-segmentation accuracy. We also demonstrate the advantage of our method on one image and two video applications."
Fast Image Restoration With Multi-Bin Trainable Linear Units,"Shuhang Gu, Wen Li, Luc Van Gool, Radu Timofte","ETH, Zurich/KU Leuven; ETH, Zurich",100.0,"Switzerland, belgium",0.0,,"Tremendous advances in image restoration tasks such as denoising and super-resolution have been achieved using neural networks. Such approaches generally employ very deep architectures, large number of parameters, large receptive fields and high nonlinear modeling capacity. In order to obtain efficient and fast image restoration networks one should improve upon the above mentioned requirements. In this paper we propose a novel activation function, the multi-bin trainable linear unit (MTLU), for increasing the nonlinear modeling capacity together with lighter and shallower networks. We validate the proposed fast image restoration networks for image denoising (FDnet) and super-resolution (FSRnet) on standard benchmarks. We achieve large improvements in both memory and runtime over current state-of-the-art for comparable or better PSNR accuracies.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Fast_Image_Restoration_With_Multi-Bin_Trainable_Linear_Units_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Fast_Image_Restoration_With_Multi-Bin_Trainable_Linear_Units_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010869/,"['Task analysis', 'Image restoration', 'Training', 'Neural networks', 'Computational modeling', 'Noise reduction', 'Image denoising']","['Fast Restoration', 'Neural Network', 'Activation Function', 'Denoising', 'Super-resolution', 'Receptive Field', 'Network Efficiency', 'Fast Network', 'Restoration Tasks', 'Learning Rate', 'Deep Network', 'Linear Function', 'Deep Neural Network', 'Nonlinear Function', 'Feature Maps', 'Training Phase', 'Computational Burden', 'Low-resolution Images', 'Bin Width', 'Super-resolution Results', 'Piecewise Linear Function', 'Computer Vision Area', 'Super-resolution Approaches', 'Super-resolution Performance', 'Super-resolution Task', 'Rectified Linear Unit Function', 'Noisy Images', 'Feature Map Channels', 'High-resolution Images']",,11,"Tremendous advances in image restoration tasks such as denoising and super-resolution have been achieved using neural networks. Such approaches generally employ very deep architectures, large number of parameters, large receptive fields and high nonlinear modeling capacity. In order to obtain efficient and fast image restoration networks one should improve upon the above mentioned requirements. In this paper we propose a novel activation function, the multi-bin trainable linear unit (MTLU), for increasing the nonlinear modeling capacity together with lighter and shallower networks. We validate the proposed fast image restoration networks for image denoising (FDnet) and super-resolution (FSRnet) on standard benchmarks. We achieve large improvements in both memory and runtime over current state-of-the-art for comparable or better PSNR accuracies."
Fast Object Detection in Compressed Video,"Shiyao Wang, Hongchao Lu, Zhidong Deng","Department of Computer Science and Technology, Tsinghua University1; Department of Computer Science and Technology, Tsinghua University1, Alibaba Group2",100.0,China,0.0,,"Object detection in videos has drawn increasing attention since it is more practical in real scenarios. Most of the deep learning methods use CNNs to process each decoded frame in a video stream individually. However, the free of charge yet valuable motion information already embedded in the video compression format is usually overlooked. In this paper, we propose a fast object detection method by taking advantage of this with a novel Motion aided Memory Network (MMNet). The MMNet has two major advantages: 1) It significantly accelerates the procedure of feature extraction for compressed videos. It only need to run a complete recognition network for I-frames, i.e. a few reference frames in a video, and it produces the features for the following P frames (predictive frames) with a light weight memory network, which runs fast; 2) Unlike existing methods that establish an additional network to model motion of frames, we take full advantage of both motion vectors and residual errors that are freely available in video streams. To our best knowledge, the MMNet is the first work that investigates a deep convolutional detector on compressed videos. Our method is evaluated on the large-scale ImageNet VID dataset, and the results show that it is 3x times faster than single image detector R-FCN and 10x times faster than high-performance detector MANet at a minor accuracy loss.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Fast_Object_Detection_in_Compressed_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Fast_Object_Detection_in_Compressed_Video_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010861/,"['Feature extraction', 'Object detection', 'Streaming media', 'Detectors', 'Image coding', 'Proposals', 'Machine learning']","['Object Detection', 'Fast Detection', 'Video Compression', 'Fast Object Detection', 'Reference Frame', 'Single Image', 'Residual Error', 'Video Frames', 'Memory Network', 'Motion Information', 'Recognition Network', 'Motion Vector', 'Detection In Videos', 'Fast Detection Method', 'Convolutional Layers', 'Feature Maps', 'Bounding Box', 'Consecutive Frames', 'Appearance Features', 'Still Images', 'Motion Cues', 'Feature Pyramid', 'Memory Module', 'Current Frame', 'Flow Estimation', 'Top Line', 'Faster R-CNN', 'Attention Weights', 'Original Video', 'Propagation Characteristics']",,37,"Object detection in videos has drawn increasing attention since it is more practical in real scenarios. Most of the deep learning methods use CNNs to process each decoded frame in a video stream individually. However, the free of charge yet valuable motion information already embedded in the video compression format is usually overlooked. In this paper, we propose a fast object detection method by taking advantage of this with a novel Motion aided Memory Network (MMNet). The MMNet has two major advantages: 1) It significantly accelerates the procedure of feature extraction for compressed videos. It only need to run a complete recognition network for I-frames, i.e. a few reference frames in a video, and it produces the features for the following P frames (predictive frames) with a light weight memory network, which runs fast; 2) Unlike existing methods that establish an additional network to model motion of frames, we take full advantage of both motion vectors and residual errors that are freely available in video streams. To our best knowledge, the MMNet is the first work that investigates a deep convolutional detector on compressed videos. Our method is evaluated on the large-scale ImageNet VID dataset, and the results show that it is 3× times faster than single image detector R-FCN and 10× times faster than high-performance detector MANet at a minor accuracy loss."
Fast Point R-CNN,"Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia",Tencent YouTu Lab; The Chinese University of Hong Kong; The Chinese University of Hong Kong and Tencent YouTu Lab,100.0,"Hong Kong, china",0.0,,"We present a unified, efficient and effective framework for point-cloud based 3D object detection. Our two-stage approach utilizes both voxel representation and raw point cloud data to exploit respective advantages. The first stage network, with voxel representation as input, only consists of light convolutional operations, producing a small number of high-quality initial predictions. Coordinate and indexed convolutional feature of each point in initial prediction are effectively fused with the attention mechanism, preserving both accurate localization and context information. The second stage works on interior points with their fused feature for further refining the prediction. Our method is evaluated on KITTI dataset, in terms of both 3D and Bird's Eye View (BEV) detection, and achieves state-of-the-arts with a 15FPS detection rate.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Fast_Point_R-CNN_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Fast_Point_R-CNN_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008235/,"['Three-dimensional displays', 'Convolution', 'Two dimensional displays', 'Object detection', 'Laser radar', 'Feature extraction', 'Detectors']","['Faster R-CNN', 'Local Information', 'Contextual Information', 'Object Detection', 'Attention Mechanism', 'Point Cloud', 'Interior Point', 'Bird’s Eye', 'Point Cloud Data', 'Convolutional Features', 'KITTI Dataset', 'Coordination Features', 'Raw Point Cloud', '3D Object Detection', 'Accurate Location Information', 'Convolutional Neural Network', 'Feature Maps', 'Pedestrian', 'Bounding Box', 'Coordinates Of Points', '3D Detection', 'Point Cloud Representation', '3D Convolution', 'Ground-truth Box', 'Nearby Objects', 'Compact Shape', 'RGB Images', 'Point Cloud Processing', 'Canonized', 'LiDAR Point']",,269,"We present a unified, efficient and effective framework for point-cloud based 3D object detection. Our two-stage approach utilizes both voxel representation and raw point cloud data to exploit respective advantages. The first stage network, with voxel representation as input, only consists of light convolutional operations, producing a small number of high-quality initial predictions. Coordinate and indexed convolutional feature of each point in initial prediction are effectively fused with the attention mechanism, preserving both accurate localization and context information. The second stage works on interior points with their fused feature for further refining the prediction. Our method is evaluated on KITTI dataset, in terms of both 3D and Bird's Eye View (BEV) detection, and achieves state-of-the-arts with a 15FPS detection rate."
Fast Video Object Segmentation via Dynamic Targeting Network,"Lu Zhang, Zhe Lin, Jianming Zhang, Huchuan Lu, You He","Adobe Research, USA; Dalian University of Technology, China; Naval Aviation University, China",66.66666666666666,china,33.33333333333334,USA,"We propose a new model for fast and accurate video object segmentation. It consists of two convolutional neural networks, a Dynamic Targeting Network (DTN) and a Mask Refinement Network (MRN). DTN locates the object by dynamically focusing on regions of interest surrounding the target object. The target region is predicted by DTN via two sub-streams, Box Propagation (BP) and Box Re-identification (BR). The BP stream is faster but less effective at objects with large deformation or occlusion. The BR stream performs better in difficult scenarios at a higher computation cost. We propose a Decision Module (DM) to adaptively determine which sub-stream to use for each frame. Finally, MRN is exploited to predict segmentation within the target region. Experimental results on two public datasets demonstrate that the proposed model significantly outperforms existing methods without online training in both accuracy and efficiency, and is comparable to online training-based methods in accuracy with an order of magnitude faster speed.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Fast_Video_Object_Segmentation_via_Dynamic_Targeting_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Fast_Video_Object_Segmentation_via_Dynamic_Targeting_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008270/,"['Object segmentation', 'Feature extraction', 'Computational modeling', 'Motion segmentation', 'Training', 'Optical propagation', 'Strain']","['Dynamic Network', 'Object Segmentation', 'Fast Segmentation', 'Fast Object', 'Video Object Segmentation', 'Fast Video', 'Convolutional Neural Network', 'Target Region', 'Fast Speed', 'Online Training', 'Large Deformation', 'Target Object', 'Segmentation Accuracy', 'Difficult Scenarios', 'Aspect Ratio', 'Feature Maps', 'Intersection Over Union', 'Bounding Box', 'Confidence Score', 'Video Frames', 'Multi-level Features', 'Region Proposal Network', 'Current Frame', 'Candidate Boxes', 'Optical Flow', 'Adjacent Frames', 'Object Annotations', 'Backbone Network', 'Pre-training Stage', 'Decision Network']",,40,"We propose a new model for fast and accurate video object segmentation. It consists of two convolutional neural networks, a Dynamic Targeting Network (DTN) and a Mask Refinement Network (MRN). DTN locates the object by dynamically focusing on regions of interest surrounding the target object. The target region is predicted by DTN via two sub-streams, Box Propagation (BP) and Box Re-identification (BR). The BP stream is faster but less effective at objects with large deformation or occlusion. The BR stream performs better in difficult scenarios at a higher computation cost. We propose a Decision Module (DM) to adaptively determine which sub-stream to use for each frame. Finally, MRN is exploited to predict segmentation within the target region. Experimental results on two public datasets demonstrate that the proposed model significantly outperforms existing methods without online training in both accuracy and efficiency, and is comparable to online training-based methods in accuracy with an order of magnitude faster speed."
Fast and Practical Neural Architecture Search,"Jiequan Cui, Pengguang Chen, Ruiyu Li, Shu Liu, Xiaoyong Shen, Jiaya Jia","The Chinese University of Hong Kong; YouTu Lab, Tencent; The Chinese University of Hong Kong and YouTu Lab, Tencent",100.0,"Hong Kong, china",0.0,,"In this paper, we propose a fast and practical neural architecture search (FPNAS) framework for automatic network design. FPNAS aims to discover extremely efficient networks with less than 300M FLOPs. Different from previous NAS methods, our approach searches for the whole network architecture to guarantee block diversity instead of stacking a set of similar blocks repeatedly. We model the search process as a bi-level optimization problem and propose an approximation solution. On CIFAR-10, our approach is capable of design networks with comparable performance to state-of-the-arts while using orders of magnitude less computational resource with only 20 GPU hours. Experimental results on ImageNet and ADE20K datasets further demonstrate transferability of the searched networks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cui_Fast_and_Practical_Neural_Architecture_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cui_Fast_and_Practical_Neural_Architecture_Search_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010737/,"['Computer architecture', 'Feature extraction', 'Convolution', 'Graphics processing units', 'Computational modeling', 'Learning (artificial intelligence)', 'Network architecture']","['Neural Architecture', 'Fast Search', 'Neural Architecture Search', 'Architectural Practice', 'Optimization Problem', 'Computational Resources', 'Network Efficiency', 'Bilevel Optimization', 'Search Framework', 'Bilevel Optimization Problem', 'Convolutional Neural Network', 'Mobile Devices', 'Validation Set', 'Evolutionary Algorithms', 'Search Space', 'Search Method', 'Stochastic Gradient Descent', 'Topological Structure', 'Optimal Efficiency', 'Semantic Segmentation', 'Semantic Segmentation Task', 'Operator Splitting', 'Lot Of Resources', 'Mobile Network', 'Reinforcement Learning Scheme', 'Identity Mapping', 'Size Of The Search Space', 'Expansion Ratio', 'Feature Reuse', 'Network Search']",,43,"In this paper, we propose a fast and practical neural architecture search (FPNAS) framework for automatic network design. FPNAS aims to discover extremely efficient networks with less than 300M FLOPs. Different from previous NAS methods, our approach searches for the whole network architecture to guarantee block diversity instead of stacking a set of similar blocks repeatedly. We model the search process as a bi-level optimization problem and propose an approximation solution. On CIFAR-10, our approach is capable of design networks with comparable performance to state-of-the-arts while using orders of magnitude less computational resource with only 20 GPU hours. Experimental results on ImageNet and ADE20K datasets further demonstrate transferability of the searched networks."
Fast-deepKCF Without Boundary Effect,"Linyu Zheng, Ming Tang, Yingying Chen, Jinqiao Wang, Hanqing Lu","University of Chinese Academy of Sciences, Beijing, China; National Lab of Pattern Recognition, Institute of Automation, CAS, Beijing 100190, China",100.0,china,0.0,,"In recent years, correlation filter based trackers (CF trackers) have received much attention because of their top performance. Most CF trackers, however, suffer from low frame-per-second (fps) in pursuit of higher localization accuracy by relaxing the boundary effect or exploiting the high-dimensional deep features. In order to achieve real-time tracking speed while maintaining high localization accuracy, in this paper, we propose a novel CF tracker, fdKCF*, which casts aside the popular acceleration tool, i.e., fast Fourier transform, employed by all existing CF trackers, and exploits the inherent high-overlap among real (i.e., noncyclic) and dense samples to efficiently construct the kernel matrix. Our fdKCF* enjoys the following three advantages. (i) It is efficiently trained in kernel space and spatial domain without the boundary effect. (ii) Its fps is almost independent of the number of feature channels. Therefore, it is almost real-time, i.e., 24 fps on OTB-2015, even though the high-dimensional deep features are employed. (iii) Its localization accuracy is state-of-the-art. Extensive experiments on four public benchmarks, OTB-2013, OTB-2015, VOT2016, and VOT2017, show that the proposed fdKCF* achieves the state-of-the-art localization performance with remarkably faster speed than C-COT and ECO.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_Fast-deepKCF_Without_Boundary_Effect_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_Fast-deepKCF_Without_Boundary_Effect_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009801/,"['Kernel', 'Training', 'Target tracking', 'Acceleration', 'Sampling methods', 'Feature extraction']","['Boundary Effects', 'Fast Fourier Transform', 'Localization Accuracy', 'Sampling Density', 'Real Samples', 'Localization Performance', 'Spatial Domain', 'Deep Features', 'High-dimensional Feature', 'Null Space', 'Top Performers', 'Tracking Speed', 'Public Benchmark', 'Kernel Space', 'High Localization Accuracy', 'Gaussian Kernel', 'Feature Maps', 'Time Complexity', 'Target Variable', 'Dot Product', 'Linear Kernel', 'Brute-force Approach', 'Dimensional Feature Vector', 'Dual Space', 'Ridge Regression', 'Cyclic Shift', 'Elimination Method', 'Nonlinear Kernel', 'Gaussian Matrix', 'Appearance Variations']",,10,"In recent years, correlation filter based trackers (CF trackers) have received much attention because of their top performance. Most CF trackers, however, suffer from low frame-per-second (fps) in pursuit of higher localization accuracy by relaxing the boundary effect or exploiting the high-dimensional deep features. In order to achieve real-time tracking speed while maintaining high localization accuracy, in this paper, we propose a novel CF tracker, fdKCF*, which casts aside the popular acceleration tool, i.e., fast Fourier transform, employed by all existing CF trackers, and exploits the inherent high-overlap among real (i.e., noncyclic) and dense samples to efficiently construct the kernel matrix. Our fdKCF* enjoys the following three advantages. (i) It is efficiently trained in kernel space and spatial domain without the boundary effect. (ii) Its fps is almost independent of the number of feature channels. Therefore, it is almost real-time, i.e., 24 fps on OTB-2015, even though the high-dimensional deep features are employed. (iii) Its localization accuracy is state-of-the-art. Extensive experiments on four public benchmarks, OTB-2013, OTB-2015, VOT2016, and VOT2017, show that the proposed fdKCF* achieves the state-of-the-art localization performance with remarkably faster speed than C-COT and ECO."
Feature Weighting and Boosting for Few-Shot Segmentation,"Khoi Nguyen, Sinisa Todorovic",Oregon State University,100.0,usa,0.0,,"This paper is about few-shot segmentation of foreground objects in images. We train a CNN on small subsets of training images, each mimicking the few-shot setting. In each subset, one image serves as the query and the other(s) as support image(s) with ground-truth segmentation. The CNN first extracts feature maps from the query and support images. Then, a class feature vector is computed as an average of the support's feature maps over the known foreground. Finally, the target object is segmented in the query image by using a cosine similarity between the class feature vector and the query's feature map. We make two contributions by: (1) Improving discriminativeness of features so their activations are high on the foreground and low elsewhere; and (2) Boosting inference with an ensemble of experts guided with the gradient of loss incurred when segmenting the support images in testing. Our evaluations on the PASCAL-5i and COCO-20i datasets demonstrate that we significantly outperform existing approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_Feature_Weighting_and_Boosting_for_Few-Shot_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_Feature_Weighting_and_Boosting_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008552/,"['Image segmentation', 'Training', 'Feature extraction', 'Testing', 'Boosting', 'Semantics', 'Computer architecture']","['Feature Weights', 'Few-shot Segmentation', 'Feature Maps', 'Image Object', 'Object Segmentation', 'Query Image', 'Ground Truth Segmentation', 'Ablation', 'Convolutional Layers', 'State Of The Art', 'Image Pixels', 'Object Classification', 'Target Class', 'Semantic Segmentation', 'Deep Architecture', 'Training Examples', 'Gradient Boosting', 'Challenging Dataset', 'Similarity Map', 'Fully Convolutional Network', 'Test Classes', 'Few-shot Classification', 'Large Training Set', 'MS COCO Dataset', 'COCO Dataset', 'Two-layer Network', 'Standard Cross-entropy Loss', 'Joint Analysis', 'Relevant Features', 'Learning Rate']",,218,"This paper is about few-shot segmentation of foreground objects in images. We train a CNN on small subsets of training images, each mimicking the few-shot setting. In each subset, one image serves as the query and the other(s) as support image(s) with ground-truth segmentation. The CNN first extracts feature maps from the query and support images. Then, a class feature vector is computed as an average of the support's feature maps over the known foreground. Finally, the target object is segmented in the query image by using a cosine similarity between the class feature vector and the query's feature map. We make two contributions by: (1) Improving discriminativeness of features so their activations are high on the foreground and low elsewhere; and (2) Boosting inference with an ensemble of experts guided with the gradient of loss incurred when segmenting the support images in testing. Our evaluations on the PASCAL-5
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sup>
 and COCO-20
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i</sup>
 datasets demonstrate that we significantly outperform existing approaches."
Few-Shot Adaptive Gaze Estimation,"Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar Iqbal, Otmar Hilliges, Jan Kautz",ETH Zürich; NVIDIA,50.0,Switzerland,50.0,USA,"Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (Faze) for learning person-specific gaze networks with very few (<= 9) calibration samples. Faze learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18-deg on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze",,http://openaccess.thecvf.com/content_ICCV_2019/html/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.pdf,,https://github.com/NVlabs/few_shot_gaze1,,main,Oral,https://ieeexplore.ieee.org/document/9008783/,"['Head', 'Estimation', 'Task analysis', 'Calibration', 'Training', 'Decoding', 'Robustness']","['Gaze Estimation', 'Calibration Samples', 'Interpersonal Differences', 'Error Of The Mean', 'Convolutional Neural Network', 'Multilayer Perceptron', 'Stochastic Gradient Descent', 'Image Pairs', 'Latent Space', 'Convolutional Neural Network Architecture', 'Subjective Image', 'Loss Term', 'Latent Features', 'Gaze Direction', 'Reconstruction Loss', 'Person Image', 'Head Orientation', 'Ensemble Of Networks', 'Few-shot Learning', 'Head Pose', 'Latent Code', 'Latent Embedding', 'Virtual Camera', 'Siamese Network', 'Feature Learning', 'Pitch Values', 'Equivalency', 'Training Set', 'Euler Angles']",,128,"Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (Faze) for learning person-specific gaze networks with very few (≤ 9) calibration samples. Faze learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18-deg on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze."
Few-Shot Adversarial Learning of Realistic Neural Talking Head Models,"Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky","Samsung AI Center, Moscow; Skolkovo Institute of Science and Technology",50.0,russia,50.0,Russia,"Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009591/,"['Training', 'Face', 'Generators', 'Video sequences', 'Task analysis', 'Adaptation models']","['Generative Adversarial Networks', 'Head Model', 'Few-shot Learning', 'Talking Head', 'Practical Scenarios', 'Humeral Head', 'Tens Of Millions', 'Realistic Images', 'Adversarial Training', 'Person Image', 'Millions Of Parameters', 'Personality', 'User Study', 'Dimensional Vector', 'Video Frames', 'Direct Synthesis', 'Projection Matrix', 'Single Frame', 'General Parameters', 'Residual Block', 'Video Sequences', 'Fr√©chet Inception Distance', 'Role Of Vectors', 'Uncanny Valley', 'Instance Normalization', 'Hinge Loss', 'Embedding Vectors', 'Fine-tuning Stage', 'Identity Preservation', 'Ground Truth Image']",,344,"Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings."
Few-Shot Generalization for Single-Image 3D Reconstruction via Priors,"Bram Wallace, Bharath Hariharan",Cornell University,100.0,usa,0.0,,"Recent work on single-view 3D reconstruction shows impressive results, but has been restricted to a few fixed categories where extensive training data is available. The problem of generalizing these models to new classes with limited training data is largely open. To address this problem, we present a new model architecture that reframes single-view 3D reconstruction as learnt, category agnostic refinement of a provided, category-specific prior. The provided prior shape for a novel class can be obtained from as few as one 3D shape from this class. Our model can start reconstructing objects from the novel class using this prior without seeing any training image for this class and without any retraining. Our model outperforms category-agnostic baselines and remains competitive with more sophisticated baselines that finetune on the novel categories. Additionally, our network is capable of improving the reconstruction given multiple views despite not being trained on task of multi-view reconstruction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wallace_Few-Shot_Generalization_for_Single-Image_3D_Reconstruction_via_Priors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wallace_Few-Shot_Generalization_for_Single-Image_3D_Reconstruction_via_Priors_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009556/,"['Shape', 'Three-dimensional displays', 'Training', 'Solid modeling', 'Image reconstruction', 'Training data', 'Generators']","['3D Reconstruction', 'Training Data', 'Fine-tuned', '3D Shape', 'Limited Training Data', 'Shape Priors', 'Neural Network', 'Training Set', 'Prior Information', 'Transfer Learning', 'Airplane', 'Problematic Aspects', 'Iterative Model', 'Base Classes', 'Iterative Refinement', 'Single View', 'Shape Representation', 'Target Shape', 'Output Space', 'Few-shot Learning', 'Series Of Convolutions', 'Input Shape', 'Training Categories', 'Image Encoder', 'Single Reconstruction', 'Multiple Baseline', 'Single 3D', 'Category Information']",,18,"Recent work on single-view 3D reconstruction shows impressive results, but has been restricted to a few fixed categories where extensive training data is available. The problem of generalizing these models to new classes with limited training data is largely open. To address this problem, we present a new model architecture that reframes single-view 3D reconstruction as learnt, category agnostic refinement of a provided, category-specific prior. The provided prior shape for a novel class can be obtained from as few as one 3D shape from this class. Our model can start reconstructing objects from the novel class using this prior without seeing any training image for this class and without any retraining. Our model outperforms category-agnostic baselines and remains competitive with more sophisticated baselines that finetune on the novel categories. Additionally, our network is capable of improving the reconstruction given multiple views despite not being trained on task of multi-view reconstruction."
Few-Shot Image Recognition With Knowledge Transfer,"Zhimao Peng, Zechao Li, Junge Zhang, Yan Li, Guo-Jun Qi, Jinhui Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology; Huawei Cloud; Institute of Automation, Chinese Academy of Sciences",66.66666666666666,china,33.33333333333334,China,"Human can well recognize images of novel categories just after browsing few examples of these categories. One possible reason is that they have some external discriminative visual information about these categories from their prior knowledge. Inspired from this, we propose a novel Knowledge Transfer Network architecture (KTN) for few-shot image recognition. The proposed KTN model jointly incorporates visual feature learning, knowledge inferring and classifier learning into one unified framework for their optimal compatibility. First, the visual classifiers for novel categories are learned based on the convolutional neural network with the cosine similarity optimization. To fully explore the prior knowledge, a semantic-visual mapping network is then developed to conduct knowledge inference, which enables to infer the classifiers for novel categories from base categories. Finally, we design an adaptive fusion scheme to infer the desired classifiers by effectively integrating the above knowledge and visual information. Extensive experiments are conducted on two widely-used Mini-ImageNet and ImageNet Few-Shot benchmarks to evaluate the effectiveness of the proposed method. The results compared with the state-of-the-art approaches show the encouraging performance of the proposed method, especially on 1-shot and 2-shot tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010012/,"['Visualization', 'Knowledge engineering', 'Measurement', 'Training', 'Image recognition', 'Knowledge transfer', 'Testing']","['Knowledge Transfer', 'Image Recognition', 'Few-shot Image Recognition', 'Convolutional Neural Network', 'Visual Information', 'Extensive Experiments', 'Forms Of Knowledge', 'Visual Features', 'Learning Classifiers', 'Base Classifiers', 'External Information', 'Adaptive Scheme', 'Visual Learning', 'Examples Of Categories', 'Training Data', 'Convolutional Layers', 'Stochastic Gradient Descent', 'Convolutional Neural Network Model', 'Latent Space', 'Few-shot Learning', 'Graph Convolutional Network', 'Support Set', 'Zero-shot', 'Node Representations', 'Nodes In The Graph', 'Score Categories', 'Convolutional Neural Network Layers', 'WordNet', 'Convolutional Block']",,147,"Human can well recognize images of novel categories just after browsing few examples of these categories. One possible reason is that they have some external discriminative visual information about these categories from their prior knowledge. Inspired from this, we propose a novel Knowledge Transfer Network architecture (KTN) for few-shot image recognition. The proposed KTN model jointly incorporates visual feature learning, knowledge inferring and classifier learning into one unified framework for their optimal compatibility. First, the visual classifiers for novel categories are learned based on the convolutional neural network with the cosine similarity optimization. To fully explore the prior knowledge, a semantic-visual mapping network is then developed to conduct knowledge inference, which enables to infer the classifiers for novel categories from base categories. Finally, we design an adaptive fusion scheme to infer the desired classifiers by effectively integrating the above knowledge and visual information. Extensive experiments are conducted on two widely-used Mini-ImageNet and ImageNet Few-Shot benchmarks to evaluate the effectiveness of the proposed method. The results compared with the state-of-the-art approaches show the encouraging performance of the proposed method, especially on 1-shot and 2-shot tasks."
Few-Shot Learning With Embedded Class Models and Shot-Free Meta Training,"Avinash Ravichandran, Rahul Bhotika, Stefano Soatto",Amazon Web Services and UCLA; Amazon Web Services,50.0,"uk, usa",50.0,USA,"We propose a method for learning embeddings for few-shot learning that is suitable for use with any number of shots (shot-free). Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per class with a simple constant-size architecture. The class embedding encompasses metric learning, that facilitates adding new classes without crowding the class representation space. Despite being general and not tuned to the benchmark, our approach achieves state-of-the-art performance on the standard few-shot benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009038/,"['Training', 'Benchmark testing', 'Measurement', 'Prototypes', 'Standards', 'Computational modeling', 'Web services']","['Few-shot Learning', 'Meta Training', 'Benchmark', 'Representative Class', 'Metric Learning', 'Class Prototypes', 'Loss Function', 'Training Set', 'Optimization Algorithm', 'Validation Set', 'Feature Space', 'Choice Of Model', 'Latent Space', 'Membership Function', 'Lifelong Learning', 'Open Set', 'Incremental Learning', 'Distance In Space', 'Ridge Regression', 'Effect Of Choice', 'Prototypical Network', 'Embedding Dimension', 'Class Identity', 'Approaches In The Literature', 'Unit Sphere', 'Explicit Function', 'Image Size']",,101,"We propose a method for learning embeddings for few-shot learning that is suitable for use with any number of shots (shot-free). Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per class with a simple constant-size architecture. The class embedding encompasses metric learning, that facilitates adding new classes without crowding the class representation space. Despite being general and not tuned to the benchmark, our approach achieves state-of-the-art performance on the standard few-shot benchmark datasets."
Few-Shot Learning With Global Class Representations,"Aoxue Li, Tiange Luo, Tao Xiang, Weiran Huang, Liwei Wang","School of EECS, Peking University, Beijing, China; Department of Electrical and Electronic Engineering, University of Surrey, UK; Huawei Noah’s Ark Lab, Beijing, China",66.66666666666666,"china, uk",33.33333333333334,China,"In this paper, we propose to tackle the challenging few-shot learning (FSL) problem by learning global class representations using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. Though following a similar episodic training pipeline as existing meta learning based approaches, our method differs significantly in that novel class training samples are involved in the training from the beginning. To compensate for the lack of novel class training samples, an effective sample synthesis strategy is developed to avoid overfitting. Importantly, by joint base-novel class training, our approach can be easily extended to a more practical yet challenging FSL setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. Extensive experiments show that our approach is effective for both of the two FSL settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Few-Shot_Learning_With_Global_Class_Representations_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Few-Shot_Learning_With_Global_Class_Representations_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009075/,"['Training', 'Feature extraction', 'Visualization', 'Data models', 'Machine learning', 'Optimization', 'Task analysis']","['Representative Class', 'Global Representation', 'Few-shot Learning', 'Test Data', 'Data Space', 'Synthesis Strategy', 'Classification Loss', 'Base Classes', 'Support Set', 'Query Set', 'Class Mean', 'Label Space', 'Training Episodes', 'Meta Learning', 'Training Set', 'Sample Characteristics', 'Learning Strategies', 'Similarity Score', 'Visual Features', 'Classification Of Samples', 'Set Of Classes', 'Query Image', 'Latent Space', 'Global Consistency', 'Stochastic Gradient Descent', 'Classifier Training', 'Contextual Embedding', 'Representation Learning', 'Batch Normalization Layer', 'Testing Stage']",,71,"In this paper, we propose to tackle the challenging few-shot learning (FSL) problem by learning global class representations using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. Though following a similar episodic training pipeline as existing meta learning based approaches, our method differs significantly in that novel class training samples are involved in the training from the beginning. To compensate for the lack of novel class training samples, an effective sample synthesis strategy is developed to avoid overfitting. Importantly, by joint base-novel class training, our approach can be easily extended to a more practical yet challenging FSL setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. Extensive experiments show that our approach is effective for both of the two FSL settings."
Few-Shot Object Detection via Feature Reweighting,"Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, Trevor Darrell","National University of Singapore; University of California, Berkeley",100.0,"singapore, usa",0.0,,"Conventional training of a deep CNN based object detector demands a large number of bounding box annotations, which may be unavailable for rare categories. In this work we develop a few-shot object detector that can learn to detect novel objects from only a few annotated examples. Our proposed model leverages fully labeled base classes and quickly adapts to novel classes, using a meta feature learner and a reweighting module within a one-stage detection architecture. The feature learner extracts meta features that are generalizable to detect novel object classes, using training data from base classes with sufficient samples. The reweighting module transforms a few support examples from the novel classes to a global vector that indicates the importance or relevance of meta features for detecting the corresponding objects. These two modules, together with a detection prediction module, are trained end-to-end based on an episodic few-shot learning scheme and a carefully designed loss function. Through extensive experiments we demonstrate that our model outperforms well-established baselines by a large margin for few-shot object detection, on multiple datasets and settings. We also present analysis on various aspects of our proposed model, aiming to provide some inspiration for future few-shot detection works.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Few-Shot_Object_Detection_via_Feature_Reweighting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kang_Few-Shot_Object_Detection_via_Feature_Reweighting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010944/,"['Feature extraction', 'Object detection', 'Detectors', 'Adaptation models', 'Training', 'Task analysis', 'Training data']","['Object Detection', 'Few-shot Object Detection', 'Feature Reweighting', 'Loss Function', 'Training Data', 'Convolutional Neural Network', 'Feature Learning', 'Bounding Box', 'Base Classes', 'Prediction Module', 'Few-shot Learning', 'Bounding Box Annotations', 'Feature Maps', 'Training Phase', 'Challenging Problem', 'Class Prediction', 'Detection Model', 'Multiple Objects', 'Target Class', 'Object Of Interest', 'Query Image', 'Softmax Loss', 'Classification Score', 'Target Object', 'Detection Framework', 'Learning Speed', 'Few-shot Classification']",,470,"Conventional training of a deep CNN based object detector demands a large number of bounding box annotations, which may be unavailable for rare categories. In this work we develop a few-shot object detector that can learn to detect novel objects from only a few annotated examples. Our proposed model leverages fully labeled base classes and quickly adapts to novel classes, using a meta feature learner and a reweighting module within a one-stage detection architecture. The feature learner extracts meta features that are generalizable to detect novel object classes, using training data from base classes with sufficient samples. The reweighting module transforms a few support examples from the novel classes to a global vector that indicates the importance or relevance of meta features for detecting the corresponding objects. These two modules, together with a detection prediction module, are trained end-to-end based on an episodic few-shot learning scheme and a carefully designed loss function. Through extensive experiments we demonstrate that our model outperforms well-established baselines by a large margin for few-shot object detection, on multiple datasets and settings. We also present analysis on various aspects of our proposed model, aiming to provide some inspiration for future few-shot detection works."
Few-Shot Unsupervised Image-to-Image Translation,"Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, Jan Kautz",Cornell University; Aalto University; NVIDIA,66.66666666666666,"finland, usa",33.33333333333334,USA,"Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.pdf,,https://github.com/NVlabs/FUNIT,,main,Poster,https://ieeexplore.ieee.org/document/9010865/,"['Training', 'Task analysis', 'Animals', 'Gallium nitride', 'Generators', 'Decoding', 'Convolutional codes']","['Training Time', 'Target Class', 'Baseline Methods', 'Translation Method', 'Source Class', 'Performance Metrics', 'Image Classification', 'Generative Adversarial Networks', 'Object Classification', 'Residual Block', 'Output Image', 'Translational Model', 'Translation Accuracy', 'Image X', 'Translation Task', 'Latent Code', 'Fréchet Inception Distance', '2D Convolutional Layers', 'Few-shot Classification', 'Translational Output', 'Translation Framework', 'Animal Faces', 'Top-5 Accuracy', 'Generative Adversarial Networks Loss', 'Object Pose', 'Affine Transformation', 'Inception Distance', 'Distance Perception']",,349,"Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT."
FiNet: Compatible and Diverse Fashion Image Inpainting,"Xintong Han, Zuxuan Wu, Weilin Huang, Matthew R. Scott, Larry S. Davis","Malong Technologies, Shenzhen, China; Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China; University of Maryland, College Park",33.33333333333333,usa,66.66666666666667,China,"Visual compatibility is critical for fashion analysis, yet is missing in existing fashion image synthesis systems. In this paper, we propose to explicitly model visual compatibility through fashion image inpainting. We present Fashion Inpainting Networks (FiNet), a two-stage image-to-image generation framework that is able to perform compatible and diverse inpainting. Disentangling the generation of shape and appearance to ensure photorealistic results, our framework consists of a shape generation network and an appearance generation network. More importantly, for each generation network, we introduce two encoders interacting with one another to learn latent codes in a shared compatibility space. The latent representations are jointly optimized with the corresponding generation network to condition the synthesis process, encouraging a diverse set of generated results that are visually compatible with existing fashion garments. In addition, our framework is readily extended to clothing reconstruction and fashion transfer. Extensive experiments on fashion synthesis quantitatively and qualitatively demonstrate the effectiveness of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Han_FiNet_Compatible_and_Diverse_Fashion_Image_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_FiNet_Compatible_and_Diverse_Fashion_Image_Inpainting_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010928/,"['Clothing', 'Shape', 'Visualization', 'Layout', 'Image generation', 'Computer vision', 'Image reconstruction']","['Image Inpainting', 'Fashion Images', 'General Framework', 'Latent Representation', 'Image Synthesis', 'Latent Code', 'Feature Maps', 'Shape Variation', 'Image Regions', 'Kullback-Leibler', 'Generative Adversarial Networks', 'Latent Space', 'Multiple Items', 'Segmentation Map', 'Variational Autoencoder', 'Reconstruction Loss', 'Latent Vector', 'Metric Learning', 'Shape Context', 'Clothing Items', 'Fashion Items', 'Missing Items', 'Missing Regions', 'Two-stage Framework', 'Fashion Design', 'Continuous Bag-of-words', 'Adversarial Training', 'Visual Relationship', 'Body Shape', 'Decoder Layer']",,56,"Visual compatibility is critical for fashion analysis, yet is missing in existing fashion image synthesis systems. In this paper, we propose to explicitly model visual compatibility through fashion image inpainting. We present Fashion Inpainting Networks (FiNet), a two-stage image-to-image generation framework that is able to perform compatible and diverse inpainting. Disentangling the generation of shape and appearance to ensure photorealistic results, our framework consists of a shape generation network and an appearance generation network. More importantly, for each generation network, we introduce two encoders interacting with one another to learn latent codes in a shared compatibility space. The latent representations are jointly optimized with the corresponding generation network to condition the synthesis process, encouraging a diverse set of generated results that are visually compatible with existing fashion garments. In addition, our framework is readily extended to clothing reconstruction and fashion transfer. Extensive experiments on fashion synthesis quantitatively and qualitatively demonstrate the effectiveness of our method."
Fingerspelling Recognition in the Wild With Iterative Visual Attention,"Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Diane Brentari, Greg Shakhnarovich, Karen Livescu","Toyota Technological Institute at Chicago, USA; University of Chicago, USA",100.0,usa,0.0,,"Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It out-performs prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Fingerspelling_Recognition_in_the_Wild_With_Iterative_Visual_Attention_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Fingerspelling_Recognition_in_the_Wild_With_Iterative_Visual_Attention_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010036/,"['Gesture recognition', 'Assistive technology', 'Visualization', 'Task analysis', 'Videos', 'Image recognition', 'Hidden Markov models']","['Visual Attention', 'High-resolution', 'Attention Mechanism', 'Sign Language', 'Recognition Problem', 'Sign Language Recognition', 'American Sign Language', 'Convolutional Layers', 'Input Image', 'Visual Features', 'Recurrent Neural Network', 'Sequence Features', 'Speech Recognition', 'Bounding Box', 'Supplementary Materials For Details', 'Language Model', 'Image Frames', 'Action Recognition', 'Optical Flow', 'Gesture Recognition', 'Sequence Of Frames', 'Attention Map', 'Face Detection', 'Sequence Of Letters', 'Pose Estimation', 'Beam Search', 'Convolutional Neural Network', 'Feature Maps', 'Intersection Over Union']",,29,"Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It out-performs prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set."
Flare in Interference-Based Hyperspectral Cameras,"Eden Sassoon, Yoav Y. Schechner, Tali Treibitz","Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, Israel; Charney School of Marine Sciences, University of Haifa, Haifa, Israel",100.0,israel,0.0,,"Stray light (flare) is formed inside cameras by internal reflections between optical elements. We point out a flare effect of significant magnitude and implication to snapshot hyperspectral imagers. Recent technologies enable placing interference-based filters on individual pixels in imaging sensors. These filters have narrow transmission bands around custom wavelengths and high transmission efficiency. Cameras using arrays of such filters are compact, robust and fast. However, as opposed to traditional broad-band filters, which often absorb unwanted light, narrow band-pass interference filters reflect non-transmitted light. This is a source of very significant flare which biases hyperspectral measurements. The bias in any pixel depends on spectral content in other pixels. We present a theoretical image formation model for this effect, and quantify it through simulations and experiments. In addition, we test deflaring of signals affected by such flare.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sassoon_Flare_in_Interference-Based_Hyperspectral_Cameras_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sassoon_Flare_in_Interference-Based_Hyperspectral_Cameras_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010360/,"['Lenses', 'Cameras', 'Interference', 'Sensors', 'Hyperspectral imaging', 'Optical imaging']","['Hyperspectral', 'Internal Reflection', 'Spectral Content', 'Optical Elements', 'Interference Filter', 'High-resolution', 'Illumination', 'Optical Tomography', 'Computer Vision', 'Optical System', 'Array Detector', 'Spectral Bands', 'Point Source', 'Optical Axis', 'Image Domain', 'Prior Methods', 'RGB Camera', 'Single Reflection', 'Fabry-Perot Interferometer', 'Lens Surface', 'Antireflection Coatings', 'Bright Source', 'Scene Point']",,4,"Stray light (flare) is formed inside cameras by internal reflections between optical elements. We point out a flare effect of significant magnitude and implication to snapshot hyperspectral imagers. Recent technologies enable placing interference-based filters on individual pixels in imaging sensors. These filters have narrow transmission bands around custom wavelengths and high transmission efficiency. Cameras using arrays of such filters are compact, robust and fast. However, as opposed to traditional broad-band filters, which often absorb unwanted light, narrow band-pass interference filters reflect non-transmitted light. This is a source of very significant flare which biases hyperspectral measurements. The bias in any pixel depends on spectral content in other pixels. We present a theoretical image formation model for this effect, and quantify it through simulations and experiments. In addition, we test deflaring of signals affected by such flare."
Floor-SP: Inverse CAD for Floorplans by Sequential Room-Wise Shortest Path,"Jiacheng Chen, Chen Liu, Jiaye Wu, Yasutaka Furukawa",Washington University in St. Louis; Simon Fraser University,100.0,"canada, usa",0.0,,"This paper proposes a new approach for automated floorplan reconstruction from RGBD scans, a major milestone in indoor mapping research. The approach, dubbed Floor-SP, formulates a novel optimization problem, where room-wise coordinate descent sequentially solves shortest path problems to optimize the floorplan graph structure. The objective function consists of data terms guided by deep neural networks, consistency terms encouraging adjacent rooms to share corners and walls, and the model complexity term. The approach does not require corner/edge primitive extraction unlike most other methods. We have evaluated our system on production-quality RGBD scans of 527 apartments or houses, including many units with non-Manhattan structures. Qualitative and quantitative evaluations demonstrate a significant performance boost over the current state-of-the-art. Please refer to our project website http://jcchen.me/floor-sp/ for code and data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Floor-SP_Inverse_CAD_for_Floorplans_by_Sequential_Room-Wise_Shortest_Path_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Floor-SP_Inverse_CAD_for_Floorplans_by_Sequential_Room-Wise_Shortest_Path_ICCV_2019_paper.pdf,http://jcchen.me/floor-sp/,,,main,Poster,https://ieeexplore.ieee.org/document/9008810/,"['Image reconstruction', 'Optimization', 'Topology', 'Image edge detection', 'Shortest path problem', 'Merging', 'Image segmentation']","['Shortest Path', 'Ground Plane', 'Optimization Problem', 'Deep Neural Network', 'Quantitative Evaluation', 'Apartment', 'Graph Structure', 'Coordinate Descent', 'Major Milestone', 'Shortest Path Problem', 'Degrees Of Freedom', 'Computer Vision', 'Precision And Recall', 'Bounding Box', 'Vibrational Energy', 'System Overview', '3D Point', 'Bottom-up Processes', 'Middle Point', 'Multiple Loops', 'Mask R-CNN', 'Minimum Bounding Box', 'Constraint Satisfaction', 'Edge Pixels', 'Top-down View', 'Corner Of The Room', 'Dominant Direction', 'Set Of Loops', 'Structure Inference']",,55,"This paper proposes a new approach for automated floorplan reconstruction from RGBD scans, a major milestone in indoor mapping research. The approach, dubbed Floor-SP, formulates a novel optimization problem, where room-wise coordinate descent sequentially solves shortest path problems to optimize the floorplan graph structure. The objective function consists of data terms guided by deep neural networks, consistency terms encouraging adjacent rooms to share corners and walls, and the model complexity term. The approach does not require corner/edge primitive extraction unlike most other methods. We have evaluated our system on production-quality RGBD scans of 527 apartments or houses, including many units with non-Manhattan structures. Qualitative and quantitative evaluations demonstrate a significant performance boost over the current state-of-the-art. Please refer to our project website http://jcchen.me/floor-sp/ for code and data."
Floorplan-Jigsaw: Jointly Estimating Scene Layout and Aligning Partial Scans,"Cheng Lin, Changjian Li, Wenping Wang",The University of Hong Kong,100.0,Hong Kong,0.0,,"We present a novel approach to align partial 3D reconstructions which may not have substantial overlap. Using floorplan priors, our method jointly predicts a room layout and estimates the transformations from a set of partial 3D data. Unlike the existing methods relying on feature descriptors to establish correspondences, we exploit the 3D ""box"" structure of a typical room layout that meets the Manhattan World property. We first estimate a local layout for each partial scan separately and then combine these local layouts to form a globally aligned layout with loop closure. Without the requirement of feature matching, the proposed method enables some novel applications ranging from large or featureless scene reconstruction and modeling from sparse input. We validate our method quantitatively and qualitatively on real and synthetic scenes of various sizes and complexities. The evaluations and comparisons show superior effectiveness and accuracy of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_Floorplan-Jigsaw_Jointly_Estimating_Scene_Layout_and_Aligning_Partial_Scans_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Floorplan-Jigsaw_Jointly_Estimating_Scene_Layout_and_Aligning_Partial_Scans_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010636/,"['Layout', 'Three-dimensional displays', 'Cameras', 'Image reconstruction', 'Feature extraction', 'Pose estimation']","['Partial Scans', 'Descriptive Characteristics', 'Partial Data', '3D Data', 'Substantial Overlap', 'Feature Matching', 'Loop Closure', 'Partial Reconstruction', 'Scene Reconstruction', 'Room Layout', 'Sparse Input', 'Single Image', 'Point Cloud', 'Point Source', 'Global Estimates', 'Local Estimates', 'Clear Boundaries', 'Feature Points', 'Convex Hull', 'Pose Estimation', 'Parallel Walls', 'Simultaneous Localization And Mapping', 'Sufficient Overlap', 'RGB-D Images', 'Camera Pose', 'Panoramic Images', 'Adjacent Fragments', 'Set Of Scans', 'Camera Pose Estimation', 'Optimal Placement']",,14,"We present a novel approach to align partial 3D reconstructions which may not have substantial overlap. Using floorplan priors, our method jointly predicts a room layout and estimates the transformations from a set of partial 3D data. Unlike the existing methods relying on feature descriptors to establish correspondences, we exploit the 3D ""box"" structure of a typical room layout that meets the Manhattan World property. We first estimate a local layout for each partial scan separately and then combine these local layouts to form a globally aligned layout with loop closure. Without the requirement of feature matching, the proposed method enables some novel applications ranging from large or featureless scene reconstruction and modeling from sparse input. We validate our method quantitatively and qualitatively on real and synthetic scenes of various sizes and complexities. The evaluations and comparisons show superior effectiveness and accuracy of our method."
Fooling Network Interpretation in Image Classification,"Akshayvarun Subramanya, Vipin Pillai, Hamed Pirsiavash","University of Maryland, Baltimore County",100.0,usa,0.0,,"Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Subramanya_Fooling_Network_Interpretation_in_Image_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Subramanya_Fooling_Network_Interpretation_in_Image_Classification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010911/,"['Prediction algorithms', 'Neural networks', 'Robustness', 'Tools', 'Task analysis']","['Neural Network', 'Deep Network', 'Deep Neural Network', 'Adversarial Attacks', 'Robust Interpretation', 'Interpretation Algorithm', 'Heatmap', 'Qualitative Results', 'Internet Of Things', 'Image Regions', 'Real-world Applications', 'Cross-entropy Loss', 'ImageNet', 'Prediction Network', 'Energy Ratio', 'Top Left Corner', 'Patch Area', 'Target Category', 'Explainable Artificial Intelligence', 'Sanity Check', 'Adversarial Examples', 'Soccer Ball', 'True Cause', 'Fast Gradient Sign Method', 'Form Of Attack', 'Wrong Predictions', 'Self-driving', 'Batch Normalization', 'Bounding Box', 'Corner Of The Image']",,33,"Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process."
Foreground-Aware Pyramid Reconstruction for Alignment-Free Occluded Person Re-Identification,"Lingxiao He, Yinggang Wang, Wu Liu, He Zhao, Zhenan Sun, Jiashi Feng","CRIPAC&NLPR, CASIA; JD AI Research, CRIPAC&NLPR, CASIA; JD AI Research; National University of Singapore",75.0,"china, singapore",25.0,China,"Re-identifying a person across multiple disjoint camera views is important for intelligent video surveillance, smart retailing and many other applications. However, existing person re-identification methods are challenged by the ubiquitous occlusion over persons and suffer performance degradation. This paper proposes a novel occlusion-robust and alignment-free model for occluded person ReID and extends its application to realistic and crowded scenarios. The proposed model first leverages the fully convolution network (FCN) and pyramid pooling to extract spatial pyramid features. Then an alignment-free matching approach namely Foreground-aware Pyramid Reconstruction (FPR) is developed to accurately compute matching scores between occluded persons, regardless of their different scales and sizes. FPR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two persons. More importantly, we design a occlusion-sensitive foreground probability generator that focuses more on clean human body parts to robustify the similarity computation with less contamination from occlusion. The FPR is easily embedded into any end-to-end person ReID models. The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30%), Partial iLIDS (68.08%), Occluded REID (81.00%), and three benchmark person datasets: Market1501 (95.42%), DukeMTMC (88.64%), CUHK03 (76.08%).",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Foreground-Aware_Pyramid_Reconstruction_for_Alignment-Free_Occluded_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Foreground-Aware_Pyramid_Reconstruction_for_Alignment-Free_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010027/,"['Image reconstruction', 'Feature extraction', 'Computational modeling', 'Generators', 'Probes', 'Cameras', 'Convolution']","['Occluded Person', 'Pyramid Reconstruction', 'Occluded Person Re-identification', 'Spatial Features', 'Matching Score', 'Pyramid Pooling', 'Spatial Pyramid', 'Spatial Feature Extraction', 'Training Set', 'Convolutional Layers', 'Feature Maps', 'Average Error', 'Bounding Box', 'Pooling Layer', 'Discriminative Features', 'External Cues', 'Feature Matching', 'Softmax Layer', 'Part Of Dataset', 'Person Image', 'Spatial Reconstruction', 'Triplet Loss', 'Spatial Probability', 'Occlusion Problem', 'Gallery Set', 'Presence Of Occlusion', 'Appearance Information', 'Images Of Different Sizes', 'Linear Representation', 'Image Size']",,36,"Re-identifying a person across multiple disjoint camera views is important for intelligent video surveillance, smart retailing and many other applications. However, existing person re-identification methods are challenged by the ubiquitous occlusion over persons and suffer performance degradation. This paper proposes a novel occlusion-robust and alignment-free model for occluded person ReID and extends its application to realistic and crowded scenarios. The proposed model first leverages the fully convolution network (FCN) and pyramid pooling to extract spatial pyramid features. Then an alignment-free matching approach namely Foreground-aware Pyramid Reconstruction (FPR) is developed to accurately compute matching scores between occluded persons, regardless of their different scales and sizes. FPR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two persons. More importantly, we design a occlusion-sensitive foreground probability generator that focuses more on clean human body parts to robustify the similarity computation with less contamination from occlusion. The FPR is easily embedded into any end-to-end person ReID models. The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30%), Partial iLIDS (68.08%), Occluded REID (81.00%), and three benchmark person datasets: Market1501 (95.42%), DukeMTMC (88.64%), CUHK03 (76.08%)."
ForkNet: Multi-Branch Volumetric Semantic Completion From a Single Depth Image,"Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari",Technische Universit ¨at M ¨unchen; Google Inc.,50.0,germany,50.0,USA,"We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers. Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise. We build the new dataset by sampling the features directly from latent space which generates a pair of partial volumetric surface and completed volumetric semantic surface. Moreover, we utilize multiple discriminators to increase the accuracy and realism of the reconstructions. We demonstrate the benefits of our approach on standard benchmarks for the two most common completion tasks: semantic 3D scene completion and 3D object completion.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_ForkNet_Multi-Branch_Volumetric_Semantic_Completion_From_a_Single_Depth_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_ForkNet_Multi-Branch_Volumetric_Semantic_Completion_From_a_Single_Depth_Image_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010715/,"['Three-dimensional displays', 'Semantics', 'Image reconstruction', 'Shape', 'Generators', 'Task analysis', 'Training']","['Volumetric', 'Single Image', 'Depth Images', 'Single Depth', 'Single Depth Image', 'Semantic Completion', 'Task Completion', 'Latent Space', 'Real Scenes', '3D Scene', 'Long Short-term Memory', 'Intersection Over Union', 'Autoencoder', 'Paired Data', 'Point Cloud', 'Semantic Information', 'Convolution Operation', 'Semantic Segmentation', '3D Shape', 'Geometric Information', 'Signed Distance Function', 'Semantic Labels', 'Latent Features', 'Loss Term', 'Learning Dataset', 'Scene Reconstruction', 'Objects In The Scene', 'RGB-D Images', 'Unsupervised Way', 'Scene Understanding']",,41,"We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers. Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise. We build the new dataset by sampling the features directly from latent space which generates a pair of partial volumetric surface and completed volumetric semantic surface. Moreover, we utilize multiple discriminators to increase the accuracy and realism of the reconstructions. We demonstrate the benefits of our approach on standard benchmarks for the two most common completion tasks: semantic 3D scene completion and 3D object completion."
Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation,"Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon","Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; ASRI, INMC, ISRC, and Institute of Engineering Research, Seoul National University",100.0,south korea,0.0,,"When a deep neural network is trained on data with only image-level labeling, the regions activated in each image tend to identify only a small region of the target object. We propose a method of using videos automatically harvested from the web to identify a larger region of the target object by using temporal information, which is not present in the static image. The temporal variations in a video allow different regions of the target object to be activated. We obtain an activated region in each frame of a video, and then aggregate the regions from successive frames into a single image, using a warping technique based on optical flow. The resulting localization maps cover more of the target object, and can then be used as proxy ground-truth to train a segmentation network. This simple approach outperforms existing methods under the same level of supervision, and even approaches relying on extra annotations. Based on VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Frame-to-Frame_Aggregation_of_Active_Regions_in_Web_Videos_for_Weakly_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Frame-to-Frame_Aggregation_of_Active_Regions_in_Web_Videos_for_Weakly_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
FrameNet: Learning Local Canonical Frames of 3D Surfaces From a Single RGB Image,"Jingwei Huang, Yichao Zhou, Thomas Funkhouser, Leonidas J. Guibas","Princeton University; Stanford University; University of California, Berkeley",100.0,usa,0.0,,"In this work, we introduce the novel problem of identifying dense canonical 3D coordinate frames from a single RGB image. We observe that each pixel in an image corresponds to a surface in the underlying 3D geometry, where a canonical frame can be identified as represented by three orthogonal axes, one along its normal direction and two in its tangent plane. We propose an algorithm to predict these axes from RGB. Our first insight is that canonical frames computed automatically with recently introduced direction field synthesis methods can provide training data for the task. Our second insight is that networks designed for surface normal prediction provide better results when trained jointly to predict canonical frames, and even better when trained to also predict 2D projections of canonical frames. We conjecture this is because projections of canonical tangent directions often align with local gradients in images, and because those directions are tightly linked to 3Dcanonical frames through projective geometry and orthogonality constraints. In our experiments, we find that our method predicts 3D canonical frames that can be used in applications ranging from surface normal estimation, feature matching, and augmented reality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_FrameNet_Learning_Local_Canonical_Frames_of_3D_Surfaces_From_a_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_FrameNet_Learning_Local_Canonical_Frames_of_3D_Surfaces_From_a_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010031/,"['Three-dimensional displays', 'Estimation', 'Geometry', 'Task analysis', 'Augmented reality', 'Two dimensional displays', 'Robustness']","['Single Image', 'RGB Images', '3D Surface', 'Local Frame', 'Canonical Frame', 'Image Pixels', 'Normal Approximation', 'Feature Matching', '2D Projection', 'Tangential Direction', 'Tangent Plane', 'Surface Normals', 'Orthogonality Constraint', 'Projective Geometry', 'Geometry Constraints', 'Neural Network', 'Image Plane', '3D Space', 'Local Estimates', 'Line Segment', 'Principal Directions', 'Depth Estimation', 'Principal Curvatures', 'Joint Estimation', 'Planar Regions', 'L2 Loss', 'Directions In Space', 'Scene Geometry', 'Homogeneous Coordinates', 'Correct Matches']",,31,"In this work, we introduce the novel problem of identifying dense canonical 3D coordinate frames from a single RGB image. We observe that each pixel in an image corresponds to a surface in the underlying 3D geometry, where a canonical frame can be identified as represented by three orthogonal axes, one along its normal direction and two in its tangent plane. We propose an algorithm to predict these axes from RGB. Our first insight is that canonical frames computed automatically with recently introduced direction field synthesis methods can provide training data for the task. Our second insight is that networks designed for surface normal prediction provide better results when trained jointly to predict canonical frames, and even better when trained to also predict 2D projections of canonical frames. We conjecture this is because projections of canonical tangent directions often align with local gradients in images, and because those directions are tightly linked to 3Dcanonical frames through projective geometry and orthogonality constraints. In our experiments, we find that our method predicts 3D canonical frames that can be used in applications ranging from surface normal estimation, feature matching, and augmented reality."
Free-Form Image Inpainting With Gated Convolution,"Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang",University of Illinois at Urbana-Champaign; Adobe Research; ByteDance AI Lab,33.33333333333333,usa,66.66666666666667,USA,"We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Free-Form_Image_Inpainting_With_Gated_Convolution_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Free-Form_Image_Inpainting_With_Gated_Convolution_ICCV_2019_paper.pdf,,https://github.com/JiahuiYu/generative_inpainting,,main,Oral,https://ieeexplore.ieee.org/document/9010689/,"['Convolution', 'Logic gates', 'Gallium nitride', 'Task analysis', 'Semantics', 'Image color analysis', 'Training']","['Image Inpainting', 'Dynamic Mechanism', 'Dynamic Selection', 'Millions Of Images', 'Channel Mechanisms', 'Semantic', 'Deep Network', 'Input Image', 'Deeper Layers', 'Input Features', 'User Study', 'Layer-by-layer', 'Generative Adversarial Networks', 'Low-level Features', 'Additional Input', 'Natural Scenes', 'Pixel Location', 'Masked Images', 'Long-range Dependencies', 'Perceptual Loss', 'Valid Pixels', 'Patch Matching', 'C Number', 'Refinement Network', 'Hinge Loss', 'Convolutional Network', 'Output Map', 'Point Of Failure', 'Receptive Field', 'Sigmoid Function']",,1110,"We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \url{https://github.com/JiahuiYu/generative_inpainting}."
Free-Form Video Inpainting With 3D Gated Convolution and Temporal PatchGAN,"Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, Winston Hsu","National Taiwan University, Taipei, Taiwan",100.0,taiwan,0.0,,"Free-form video inpainting is a very challenging task that could be widely used for video editing such as text removal. Existing patch-based methods could not handle non-repetitive structures such as faces, while directly applying image-based inpainting models to videos will result in temporal inconsistency (see this https://www.youtube.com/watch?v=BuTYfo4bO2I&list=PLnEeMdoBDCISRm0EZYFcQuaJ5ITUaaEIb&index=1). In this paper, we introduce a deep learning based free-form video inpainting model, with proposed 3D gated convolutions to tackle the uncertainty of free-form masks and a novel Temporal PatchGAN loss to enhance temporal consistency. In addition, we collect videos and design a free-form mask generation algorithm to build the free-form video inpainting (FVI) dataset for training and evaluation of video inpainting models. We demonstrate the benefits of these components and experiments on both the FaceForensics and our FVI dataset suggest that our method is superior to existing ones. Related source code, full-resolution result videos and the FVI dataset could be found on Github: https://github.com/amjltc295/Free-Form-Video-Inpainting",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chang_Free-Form_Video_Inpainting_With_3D_Gated_Convolution_and_Temporal_PatchGAN_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Free-Form_Video_Inpainting_With_3D_Gated_Convolution_and_Temporal_PatchGAN_ICCV_2019_paper.pdf,,https://github.com/...,,main,Poster,https://ieeexplore.ieee.org/document/9009508/,"['Logic gates', 'Three-dimensional displays', 'Convolution', 'Generators', 'Task analysis', 'Gallium nitride', 'Painting']","['Video Inpainting', 'Temporal Consistency', 'Video Editing', 'Temporal Loss', 'Patch-based Methods', 'Convolutional Neural Network', 'Convolutional Layers', 'Local Features', 'Bounding Box', 'Generative Adversarial Networks', 'Optical Flow', 'Two-stage Model', 'Learning-based Models', 'Nearest Neighbor Search', 'Masked Images', 'Video Quality', 'Perceptual Loss', '3D Convolution', 'Missing Regions', 'Similar Patches', 'Image Inpainting', 'Generative Adversarial Networks Loss', 'Masked Area', 'Video Output', 'Missing Areas', 'Quantitative Results', 'Temporal Information', 'Frames Per Second', 'Video Frames']",,126,"Free-form video inpainting is a very challenging task that could be widely used for video editing such as text removal (see Fig. 1). Existing patch-based methods could not handle non-repetitive structures such as faces, while directly applying image-based inpainting models to videos will result in temporal inconsistency (see videos). In this paper, we introduce a deep learning based free-form video inpainting model, with proposed 3D gated convolutions to tackle the uncertainty offree-form masks and a novel Temporal PatchGAN loss to enhance temporal consistency. In addition, we collect videos and design a free-form mask generation algorithm to build the free-form video inpainting (FVI) dataset for training and evaluation of video inpainting models. We demonstrate the benefits of these components and experiments on both the FaceForensics and our FVI dataset suggest that our method is superior to existing ones. Related source code, full-resolution result videos and the FVI dataset could be found on Github."
FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images,"Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, Thomas Brox",University of Freiburg; Adobe Research,50.0,germany,50.0,USA,"Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this real-world dataset, we propose an iterative, semi-automated `human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zimmermann_FreiHAND_A_Dataset_for_Markerless_Capture_of_Hand_Pose_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zimmermann_FreiHAND_A_Dataset_for_Markerless_Capture_of_Hand_Pose_and_ICCV_2019_paper.pdf,https://lmb.informatik.uni-freiburg.de/projects/freihand/,,,main,Poster,https://ieeexplore.ieee.org/document/9010946/,"['Three-dimensional displays', 'Shape', 'Training', 'Estimation', 'Benchmark testing', 'Cameras', 'Sensors']","['Single Image', 'RGB Images', 'Hand Shape', 'Hand Pose', 'Rate Set', '3D Shape', 'Shape Estimation', '3D Pose', 'Training Set', 'Large Datasets', 'Intersection Over Union', 'Large-scale Datasets', 'Benchmark Datasets', 'Confidence Score', 'Iterative Procedure', 'Manual Annotation', 'Pose Estimation', 'Fitting Process', 'Human Pose Estimation', '2D Keypoints', 'Current Pose', 'Global Translation', 'Human Hand', 'Cumulative Ranking', 'Hand Region', 'L2 Loss']",,260,"Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this real-world dataset, we propose an iterative, semi-automated `human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation."
From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer,"Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, Chunhua Shen","Huazhong University of Science and Technology, China; The University of Adelaide, Australia",100.0,"australia, china",0.0,,"Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0,+[?]) in theory. However, the collected images and labeled count values are limited in reality, which means only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting is decomposable. A dense region can always be divided until the count values of sub-regions are within the previously observed closed set. Inspired by this idea, we propose a simple but effective approach, Spatial Divide-and-Conquer Network (S-DCNet). S-DCNet learns to classify closed-set counts and can generalize to open-set counts via S-DC. S-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. S-DCNet achieves the state-of-the-art performance on three crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF), a vehicle counting dataset (TRANCOS) and a plant counting dataset (MTC). Compared to the previous best methods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTechPart B, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC. Code has been made available at: https://github.com/xhp-hust-2018-2011/S-DCNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_From_Open_Set_to_Closed_Set_Counting_Objects_by_Spatial_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_From_Open_Set_to_Closed_Set_Counting_Objects_by_Spatial_ICCV_2019_paper.pdf,,https://github.com/xhp-hust-2018-2011/S-DCNet,,main,Poster,https://ieeexplore.ieee.org/document/9010835,"['Decoding', 'Task analysis', 'Feature extraction', 'Image resolution', 'Visualization', 'Training', 'Estimation']","['Open Set', 'Closed Set', 'Object Counting', 'Count Data', 'Feature Maps', 'Number Of Objects', 'Count Datasets', 'Prediction Accuracy', 'Training Set', 'Convolutional Neural Network', 'Mean Absolute Error', 'Data Augmentation', 'Density Map', 'Count Model', 'Maximum Count', 'Inclusion Exclusion', 'Counting Error', 'Pre-trained VGG16', 'Crowded Scenes']",,111,"Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0,+∞) in theory. However, the collected images and labeled count values are limited in reality, which means only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting is decomposable. A dense region can always be divided until the count values of sub-regions are within the previously observed closed set. Inspired by this idea, we propose a simple but effective approach, Spatial Divide-and-Conquer Network (S-DCNet). S-DCNet learns to classify closed-set counts and can generalize to open-set counts via S-DC. S-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. S-DCNet achieves the state-of-the-art performance on three crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF), a vehicle counting dataset (TRANCOS) and a plant counting dataset (MTC). Compared to the previous best methods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTechPart B, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC. Code has been made available at: https://github.com/xhp-hust-2018-2011/S-DCNet."
From Strings to Things: Knowledge-Enabled VQA Model That Can Read and Reason,"Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar, Anirban Chakraborty","Indian Institute of Science, Bangalore, India; IIT Jodhpur, India; TCS Research, Pune, India",66.66666666666666,"India, india",33.33333333333334,India,"Text present in images are not merely strings, they provide useful cues about the image. Despite their utility in better image understanding, scene texts are not used in traditional visual question answering (VQA) models. In this work, we present a VQA model which can read scene texts and perform reasoning on a knowledge graph to arrive at an accurate answer. Our proposed model has three mutually interacting modules: i. proposal module to get word and visual content proposals from the image, ii. fusion module to fuse these proposals, question and knowledge base to mine relevant facts, and represent these facts as multi-relational graph, iii. reasoning module to perform a novel gated graph neural network based reasoning on this graph. The performance of our knowledge-enabled VQA model is evaluated on our newly introduced dataset, viz. text-KVQA. To the best of our knowledge, this is the first dataset which identifies the need for bridging text recognition with knowledge graph based reasoning. Through extensive experiments, we show that our proposed method outperforms traditional VQA as well as question-answering over knowledge base-based methods on text-KVQA.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Singh_From_Strings_to_Things_Knowledge-Enabled_VQA_Model_That_Can_Read_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_From_Strings_to_Things_Knowledge-Enabled_VQA_Model_That_Can_Read_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010987/,"['Visualization', 'Text recognition', 'Cognition', 'Knowledge based systems', 'Motion pictures', 'Knowledge discovery', 'Proposals']","['Visual Question Answering', 'Visual Question Answering Models', 'Neural Network', 'Knowledge Base', 'Relevant Factors', 'Question Answering', 'Graph Neural Networks', 'Visual Content', 'Optical Character Recognition', 'Accurate Answers', 'Visual Question', 'Training Set', 'Validation Set', 'Scoring Function', 'United States Of America', 'Visual Cues', 'Attention Mechanism', 'Large-scale Datasets', 'Brand Name', 'Confidence Score', 'Memory Network', 'Image Texture', 'Bidirectional Long Short-term Memory', 'Hidden State', 'Complex Reasons', 'Text Reading', 'Scene Images', 'World Knowledge', 'Memory Unit', 'Node Embeddings']",,26,"Text present in images are not merely strings, they provide useful cues about the image. Despite their utility in better image understanding, scene texts are not used in traditional visual question answering (VQA) models. In this work, we present a VQA model which can read scene texts and perform reasoning on a knowledge graph to arrive at an accurate answer. Our proposed model has three mutually interacting modules: i. proposal module to get word and visual content proposals from the image, ii. fusion module to fuse these proposals, question and knowledge base to mine relevant facts, and represent these facts as multi-relational graph, iii. reasoning module to perform a novel gated graph neural network based reasoning on this graph. The performance of our knowledge-enabled VQA model is evaluated on our newly introduced dataset, viz. text-KVQA. To the best of our knowledge, this is the first dataset which identifies the need for bridging text recognition with knowledge graph based reasoning. Through extensive experiments, we show that our proposed method outperforms traditional VQA as well as question-answering over knowledge base-based methods on text-KVQA."
Fully Convolutional Geometric Features,"Christopher Choy, Jaesik Park, Vladlen Koltun",POSTECH; Intel Labs; Stanford University,66.66666666666666,"south korea, usa",33.33333333333334,USA,"Extracting geometric features from 3D scans or point clouds is the first step in applications such as registration, reconstruction, and tracking. State-of-the-art methods require computing low-level features as input or extracting patch-based features with limited receptive field. In this work, we present fully-convolutional geometric features, computed in a single pass by a 3D fully-convolutional network. We also present new metric learning losses that dramatically improve performance. Fully-convolutional geometric features are compact, capture broad spatial context, and scale to large scenes. We experimentally validate our approach on both indoor and outdoor datasets. Fully-convolutional geometric features achieve state-of-the-art accuracy without requiring prepossessing, are compact (32 dimensions), and are 290 times faster than the most accurate prior method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009829/,"['Three-dimensional displays', 'Feature extraction', 'Measurement', 'Tensile stress', 'Kernel', 'Standards', 'Histograms']","['Geometric Features', 'Fully Convolutional Geometric Features', 'Point Cloud', 'Receptive Field', '3D Scanning', 'Fully Convolutional Network', 'Metric Learning', 'Learning Loss', 'False Negative', 'Data Augmentation', 'Multilayer Perceptron', 'Surgical Margins', 'Semantic Segmentation', 'Distance Threshold', 'Handcrafted Features', '3D Coordinates', 'Translation Invariance', '3D Features', 'Contrastive Loss', 'Translation Error', 'Sparse Tensor', 'Triplet Loss', 'Deep Metric Learning', 'Negative Margins', 'Intermediate Activity', 'Scan Pairs', 'Ground Truth Pose', '3D Convolution', '3D Data', 'Rotation Invariance']",,412,"Extracting geometric features from 3D scans or point clouds is the first step in applications such as registration, reconstruction, and tracking. State-of-the-art methods require computing low-level features as input or extracting patch-based features with limited receptive field. In this work, we present fully-convolutional geometric features, computed in a single pass by a 3D fully-convolutional network. We also present new metric learning losses that dramatically improve performance. Fully-convolutional geometric features are compact, capture broad spatial context, and scale to large scenes. We experimentally validate our approach on both indoor and outdoor datasets. Fully-convolutional geometric features achieve state-of-the-art accuracy without requiring prepossessing, are compact (32 dimensions), and are 290 times faster than the most accurate prior method."
Fully Convolutional Pixel Adaptive Image Denoiser,"Sungmin Cha, Taesup Moon","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea 16419; Department of Artiﬁcial Intelligence, Sungkyunkwan University, Suwon, Korea 16419",100.0,south korea,0.0,,"We propose a new image denoising algorithm, dubbed as Fully Convolutional Adaptive Image DEnoiser (FC-AIDE), that can learn from an offline supervised training set with a fully convolutional neural network as well as adaptively fine-tune the supervised model for each given noisy image. We significantly extend the framework of the recently proposed Neural AIDE, which formulates the denoiser to be context-based pixelwise mappings and utilizes the unbiased estimator of MSE for such denoisers. The two main contributions we make are; 1) implementing a novel fully convolutional architecture that boosts the base supervised model, and 2) introducing regularization methods for the adaptive fine-tuning such that a stronger and more robust adaptivity can be attained. As a result, FC-AIDE is shown to possess many desirable features; it outperforms the recent CNN-based state-of-the-art denoisers on all of the benchmark datasets we tested, and gets particularly strong for various challenging scenarios, e.g., with mismatched image/noise characteristics or with scarce supervised training data. The source code our algorithm is available at  https://github.com/csm9493/FC-AIDE-Keras  https://github.com/csm9493/FC-AIDE-Keras .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cha_Fully_Convolutional_Pixel_Adaptive_Image_Denoiser_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cha_Fully_Convolutional_Pixel_Adaptive_Image_Denoiser_ICCV_2019_paper.pdf,,https://github.com/csm9493/FC-AIDE-Keras,,main,Poster,https://ieeexplore.ieee.org/document/9010028/,"['Noise measurement', 'Training', 'Noise reduction', 'Adaptation models', 'Image denoising', 'Training data', 'Robustness']","['Neural Network', 'Training Set', 'Training Data', 'Convolutional Neural Network', 'Benchmark Datasets', 'Regularization Method', 'Noise Characteristics', 'Noisy Images', 'Convolutional Architecture', 'Supervised Learning', 'Gaussian Noise', 'Feature Maps', 'Data Augmentation', 'Super-resolution', 'Conditional Independence', 'Receptive Field', 'Natural Images', 'Network Output', 'Patch Size', 'Clear Image', 'Fully-connected Network', 'Empirical Risk Minimization', 'Training Data Size', 'Fine-tuned Model', 'Affine Function', 'Optimization-based Methods', 'Residual Learning', 'CNN-based Methods', 'Denoising Methods', 'Convolutional Neural Network Training']",,33,"We propose a new image denoising algorithm, dubbed as Fully Convolutional Adaptive Image DEnoiser (FC-AIDE), that can learn from an offline supervised training set with a fully convolutional neural network as well as adaptively fine-tune the supervised model for each given noisy image. We significantly extend the framework of the recently proposed Neural AIDE, which formulates the denoiser to be context-based pixelwise mappings and utilizes the unbiased estimator of MSE for such denoisers. The two main contributions we make are; 1) implementing a novel fully convolutional architecture that boosts the base supervised model, and 2) introducing regularization methods for the adaptive fine-tuning such that a stronger and more robust adaptivity can be attained. As a result, FC-AIDE is shown to possess many desirable features; it outperforms the recent CNN-based state-of-the-art denoisers on all of the benchmark datasets we tested, and gets particularly strong for various challenging scenarios, e.g., with mismatched image/noise characteristics or with scarce supervised training data. The source code our algorithm is available at https://github.com/csm9493/FC-AIDE-Keras."
G3raphGround: Graph-Based Language Grounding,"Mohit Bajaj, Lanjun Wang, Leonid Sigal","University of British Columbia, Vector Institute for AI; Huawei Technologies; University of British Columbia, Vector Institute for AI, Canada CIFAR AI Chair",66.66666666666666,canada,33.33333333333334,China,"In this paper we present an end-to-end framework for grounding of phrases in images. In contrast to previous works, our model, which we call GraphGround, uses graphs to formulate more complex, non-sequential dependencies among proposal image regions and phrases. We capture intra-modal dependencies using a separate graph neural network for each modality (visual and lingual), and then use conditional message-passing in another graph neural network to fuse their outputs and capture cross-modal relationships. This final representation results in grounding decisions. The framework supports many-to-many matching and is able to ground single phrase to multiple image regions and vice versa. We validate our design choices through a series of ablation studies and illustrate state-of-the-art performance on Flickr30k and ReferIt Game benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bajaj_G3raphGround_Graph-Based_Language_Grounding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bajaj_G3raphGround_Graph-Based_Language_Grounding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010008/,"['Grounding', 'Visualization', 'Neural networks', 'Proposals', 'Task analysis', 'Logic gates', 'Encoding']","['Language Grounding', 'Multiple Regions', 'Image Regions', 'Graph Neural Networks', 'Single Image', 'Object Detection', 'Global Context', 'Nodes In The Graph', 'Fully-connected Layer', 'Increase In Accuracy', 'Node Status', 'Prediction Network', 'Post Processing', 'Graph Convolutional Network', 'Visibility Graph', 'Gated Recurrent Unit', 'Region Proposal', 'Region Proposal Network', 'Non-maximum Suppression', 'Fully-connected Network', 'Visual Encoding', 'Bidirectional Recurrent Neural Network', 'Image Encoder', 'Fusion Step', 'Node Features', 'Training Images', 'Series Of Iterations', 'Contextual Information', 'Fine-tuned']",,36,"In this paper we present an end-to-end framework for grounding of phrases in images. In contrast to previous works, our model, which we call GraphGround, uses graphs to formulate more complex, non-sequential dependencies among proposal image regions and phrases. We capture intra-modal dependencies using a separate graph neural network for each modality (visual and lingual), and then use conditional message-passing in another graph neural network to fuse their outputs and capture cross-modal relationships. This final representation results in grounding decisions. The framework supports many-to-many matching and is able to ground single phrase to multiple image regions and vice versa. We validate our design choices through a series of ablation studies and illustrate state-of-the-art performance on Flickr30k and ReferIt Game benchmark datasets."
GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition,"Fangneng Zhan, Chuhui Xue, Shijian Lu",Nanyang Technological University,100.0,Singapore,0.0,,"Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning structure is designed which can convert a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency and greatly improves the concurrent learning in both appearance and geometry spaces. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010742/,"['Geometry', 'Text recognition', 'Adaptation models', 'Generators', 'Training', 'Gallium nitride', 'Robustness']","['Domain Adaptation', 'Optical Character Recognition', 'Scene Text', 'Scene Text Detection', 'Scene Text Recognition', 'Spatial Memory', 'Learning Network', 'Recognition Task', 'Multiple Images', 'Detection Task', 'Generative Adversarial Networks', 'Target Domain', 'Innovation Network', 'Multimodal Learning', 'Cycle Consistency Loss', 'Shift In Space', 'Innovative Adaptation', 'Deep Neural Network', 'Adaptive Method', 'Recurrent Neural Network', 'Source Domain', 'Deep Neural Network Model', 'Target Domain Images', 'Domain Shift', 'Target Dataset', 'Similar Appearance', 'Transformation Matrix', 'Binary Map', 'Spatial Module', 'Training Images']",,58,"Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning structure is designed which can convert a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency and greatly improves the concurrent learning in both appearance and geometry spaces. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training."
GAN-Based Projector for Faster Recovery With Convergence Guarantees in Linear Inverse Problems,"Ankit Raj, Yuqi Li, Yoram Bresler","University of Illinois at Urbana-Champaign, USA",100.0,usa,0.0,,"A Generative Adversarial Network (GAN) with generator G trained to model the prior of images has been shown to perform better than sparsity-based regularizers in ill-posed inverse problems. Here, we propose a new method of deploying a GAN-based prior to solve linear inverse problems using projected gradient descent (PGD). Our method learns a network-based projector for use in the PGD algorithm, eliminating expensive computation of the Jacobian of G. Experiments show that our approach provides a speed-up of 60-80x over earlier GAN-based recovery methods along with better accuracy in compressed sensing. Our main theoretical result is that if the measurement matrix is moderately conditioned on the manifold range(G) and the projector is d-approximate, then the algorithm is guaranteed to reach O(d) reconstruction error in O(log(1/d)) steps in the low noise regime. Additionally, we propose a fast method to design such measurement matrices for a given G. Extensive experiments demonstrate the efficacy of this method by requiring 5-10x fewer measurements than random Gaussian measurement matrices for comparable recovery performance. Because the learning of the GAN and projector is decoupled from the measurement operator, our GAN-based projector and recovery algorithm are applicable without retraining to all linear inverse problems in which the measurement operator is moderately conditioned for range(G), as confirmed by experiments on compressed sensing, super-resolution, and inpainting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Raj_GAN-Based_Projector_for_Faster_Recovery_With_Convergence_Guarantees_in_Linear_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Raj_GAN-Based_Projector_for_Faster_Recovery_With_Convergence_Guarantees_in_Linear_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009825/,"['Convergence', 'Gallium nitride', 'Training', 'Generators', 'Inverse problems', 'Jacobian matrices', 'Approximation algorithms']","['Projector', 'Linear Problem', 'Inverse Problem', 'Linear Inverse Problem', 'Gradient Descent', 'Generative Adversarial Networks', 'Random Matrix', 'Measurement Matrix', 'Inpainting', 'Recovery Algorithm', 'Gaussian Matrix', 'Step Error', 'Projected Gradient Descent', 'Main Theoretical Results', 'Random Gaussian', 'Learning Rate', 'Convolutional Layers', 'Adam Optimizer', 'Time Complexity', 'Finite Set', 'Transposed Convolution Layers', 'MNIST Dataset', 'Generative Adversarial Networks Training', 'Discrete Cosine Transform', 'Design Matrix', 'Proximal Mapping', 'Space Complexity', 'Idempotent', 'Self-attention Module', 'Local Optimum']",,24,"A Generative Adversarial Network (GAN) with generator G trained to model the prior of images has been shown to perform better than sparsity-based regularizers in ill-posed inverse problems. Here, we propose a new method of deploying a GAN-based prior to solve linear inverse problems using projected gradient descent (PGD). Our method learns a network-based projector for use in the PGD algorithm, eliminating expensive computation of the Jacobian of G. Experiments show that our approach provides a speed-up of 60-80x over earlier GAN-based recovery methods along with better accuracy in compressed sensing. Our main theoretical result is that if the measurement matrix is moderately conditioned on the manifold range(G) and the projector is \delta-approximate, then the algorithm is guaranteed to reach O(\delta) reconstruction error in O(log(1/\delta)) steps in the low noise regime. Additionally, we propose a fast method to design such measurement matrices for a given G. Extensive experiments demonstrate the efficacy of this method by requiring 5-10x fewer measurements than random Gaussian measurement matrices for comparable recovery performance. Because the learning of the GAN and projector is decoupled from the measurement operator, our GAN-based projector and recovery algorithm are applicable without retraining to all linear inverse problems in which the measurement operator is moderately conditioned for range(G), as confirmed by experiments on compressed sensing, super-resolution, and inpainting."
GAN-Tree: An Incrementally Learned Hierarchical Generative Framework for Multi-Modal Data Distributions,"Jogendra Nath Kundu, Maharshi Gor, Dakshit Agrawal, R. Venkatesh Babu","Video Analytics Lab, Indian Institute of Science, Bangalore, India",100.0,India,0.0,,"Despite the remarkable success of generative adversarial networks, their performance seems less impressive for diverse training sets, requiring learning of discontinuous mapping functions. Though multi-mode prior or multi-generator models have been proposed to alleviate this problem, such approaches may fail depending on the empirically chosen initial mode components. In contrast to such bottom-up approaches, we present GAN-Tree, which follows a hierarchical divisive strategy to address such discontinuous multi-modal data. Devoid of any assumption on the number of modes, GAN-Tree utilizes a novel mode-splitting algorithm to effectively split the parent mode to semantically cohesive children modes, facilitating unsupervised clustering. Further, it also enables incremental addition of new data modes to an already trained GAN-Tree, by updating only a single branch of the tree structure. As compared to prior approaches, the proposed framework offers a higher degree of flexibility in choosing a large variety of mutually exclusive and exhaustive tree nodes called GAN-Set. Extensive experiments on synthetic and natural image datasets including ImageNet demonstrate the superiority of GAN-Tree against the prior state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kundu_GAN-Tree_An_Incrementally_Learned_Hierarchical_Generative_Framework_for_Multi-Modal_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kundu_GAN-Tree_An_Incrementally_Learned_Hierarchical_Generative_Framework_for_Multi-Modal_Data_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
GANalyze: Toward Visual Definitions of Cognitive Image Properties,"Lore Goetschalckx, Alex Andonian, Aude Oliva, Phillip Isola","MIT; MIT, KU Leuven",100.0,"belgium, usa",0.0,,"We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability. These attributes are of interest because we do not have a concrete visual definition of what they entail. What does it look like for a dog to be more memorable? GANs allow us to generate a manifold of natural-looking images with fine-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more memorable. The resulting ""visual definitions"" surface image properties (like ""object size"") that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. ganalyze.csail.mit.edu.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Goetschalckx_GANalyze_Toward_Visual_Definitions_of_Cognitive_Image_Properties_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Goetschalckx_GANalyze_Toward_Visual_Definitions_of_Cognitive_Image_Properties_ICCV_2019_paper.pdf,ganalyze.csail.mit.edu,,,main,Poster,https://ieeexplore.ieee.org/document/9010379/,"['Visualization', 'Gallium nitride', 'Generators', 'Manifolds', 'Navigation', 'Image color analysis', 'Face']","['Image Properties', 'Behavioral Experiments', 'Generative Adversarial Networks', 'Object Size', 'Emotional Valence', 'Visual Properties', 'Human Memory', 'Logistic Regression', 'Transformer', 'Binary Data', 'Latent Space', 'Mixed-effects Regression', 'Mixed-effects Linear Regression', 'Noise Vector', 'Complete Series', 'Real Ones', 'Automatic Measurement', 'Style Transfer', 'Fr√©chet Inception Distance', 'Image Memorability']",,153,"We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability. These attributes are of interest because we do not have a concrete visual definition of what they entail. What does it look like for a dog to be more memorable? GANs allow us to generate a manifold of natural-looking images with fine-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more memorable. The resulting ``visual definitions"" surface image properties (like ``object size"") that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. ganalyze.csail.mit.edu."
GEOBIT: A Geodesic-Based Binary Descriptor Invariant to Non-Rigid Deformations for RGB-D Images,"Erickson R. Nascimento, Guilherme Potje, Renato Martins, Felipe Cadar, Mario F. M. Campos, Ruzena Bajcsy",Universidade Federal de Minas Gerais (UFMG); Universidade Federal de Minas Gerais (UFMG)2INRIA; University of California Berkeley,100.0,"Brazil, France, USA",0.0,,"At the core of most three-dimensional alignment and tracking tasks resides the critical problem of point correspondence. In this context, the design of descriptors that efficiently and uniquely identifies keypoints, to be matched, is of central importance. Numerous descriptors have been developed for dealing with affine/perspective warps, but few can also handle non-rigid deformations. In this paper, we introduce a novel binary RGB-D descriptor invariant to isometric deformations. Our method uses geodesic isocurves on smooth textured manifolds. It combines appearance and geometric information from RGB-D images to tackle non-rigid transformations. We used our descriptor to track multiple textured depth maps and demonstrate that it produces reliable feature descriptors even in the presence of strong non-rigid deformations and depth noise. The experiments show that our descriptor outperforms different state-of-the-art descriptors in both precision-recall and recognition rate metrics. We also provide to the community a new dataset composed of annotated RGB-D images of different objects (shirts, cloths, paintings, bags), subjected to strong non-rigid deformations, to evaluate point correspondence algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nascimento_GEOBIT_A_Geodesic-Based_Binary_Descriptor_Invariant_to_Non-Rigid_Deformations_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nascimento_GEOBIT_A_Geodesic-Based_Binary_Descriptor_Invariant_to_Non-Rigid_Deformations_for_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010660/,"['Strain', 'Feature extraction', 'Shape', 'Manifolds', 'Two dimensional displays', 'Heating systems', 'Image recognition']","['RGB-D Images', 'Non-rigid Deformation', 'Binary Descriptors', 'Geometric Information', 'Recognition Rate', 'Strong Deformation', 'Presence Of Deformation', 'Heat Transfer', 'Intrinsic Properties', 'Object Detection', 'Visual Features', 'Image Intensity', 'Physical Location', 'Geometric Features', 'Image Pairs', 'Computational Effort', 'Depth Images', 'Scale-invariant', 'Depth Information', 'Binary String', 'Geodesic Distance', 'Illumination Changes', 'Surface Deformation', 'Hamming Distance', 'Real-world Objects', 'Deformable Objects', 'Rotation Invariance', 'Field Gradient', 'Image Recognition']",,3,"At the core of most three-dimensional alignment and tracking tasks resides the critical problem of point correspondence. In this context, the design of descriptors that efficiently and uniquely identifies keypoints, to be matched, is of central importance. Numerous descriptors have been developed for dealing with affine/perspective warps, but few can also handle non-rigid deformations. In this paper, we introduce a novel binary RGB-D descriptor invariant to isometric deformations. Our method uses geodesic isocurves on smooth textured manifolds. It combines appearance and geometric information from RGB-D images to tackle non-rigid transformations. We used our descriptor to track multiple textured depth maps and demonstrate that it produces reliable feature descriptors even in the presence of strong non-rigid deformations and depth noise. The experiments show that our descriptor outperforms different state-of-the-art descriptors in both precision-recall and recognition rate metrics. We also provide to the community a new dataset composed of annotated RGB-D images of different objects (shirts, cloths, paintings, bags), subjected to strong non-rigid deformations, to evaluate point correspondence algorithms."
GLAMpoints: Greedily Learned Accurate Match Points,"Prune Truong,  Stefanos Apostolopoulos,  Agata Mosinska,  Samuel Stucky,  Carlos Ciller,  Sandro De Zanet","RetinAI Medical AG, Switzerland and ETH Zurich, Switzerland; RetinAI Medical AG, Switzerland",50.0,switzerland,50.0,Switzerland,"We introduce a novel CNN-based feature point detector - Greedily Learned Accurate Match Points (GLAMpoints) - learned in a semi-supervised manner. Our detector extracts repeatable, stable interest points with a dense coverage, specifically designed to maximize the correct matching in a specific domain, which is in contrast to conventional techniques that optimize indirect metrics. In this paper, we apply our method on challenging retinal slitlamp images, for which classical detectors yield unsatisfactory results due to low image quality and insufficient amount of low-level features. We show that GLAMpoints significantly outperforms classical detectors as well as state-of-the-art CNN-based methods in matching and registration quality for retinal images.",http://openaccess.thecvf.com/content_ICCV_2019/html/Truong_GLAMpoints_Greedily_Learned_Accurate_Match_Points_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Truong_GLAMpoints_Greedily_Learned_Accurate_Match_Points_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9009575/,"['Feature extraction', 'Retina', 'Detectors', 'Training', 'Computer vision', 'Medical diagnostic imaging']","['Low Quality', 'Slit-lamp', 'Feature Points', 'Retinal Images', 'Point Detection', 'Quality Register', 'Matching Quality', 'Correct Matches', 'Feature Point Detection', 'Medical Imaging', 'True Positive', 'Convolutional Neural Network', 'Image Registration', 'Raw Images', 'Age-related Macular Degeneration', 'Image Pairs', 'Natural Images', 'Image Preprocessing', 'Fundus Images', 'Median Error', 'Scale-invariant Feature Transform', 'Keypoint Detection', 'Simultaneous Localization And Mapping', 'Speeded Up Robust Features', 'Matching Performance', 'Bundle Adjustment', 'Fractional Cover', 'Rotation Invariance', 'Structure From Motion', 'True Point']",,38,"We introduce a novel CNN-based feature point detector - Greedily Learned Accurate Match Points (GLAMpoints) - learned in a semi-supervised manner. Our detector extracts repeatable, stable interest points with a dense coverage, specifically designed to maximize the correct matching in a specific domain, which is in contrast to conventional techniques that optimize indirect metrics. In this paper, we apply our method on challenging retinal slitlamp images, for which classical detectors yield unsatisfactory results due to low image quality and insufficient amount of low-level features. We show that GLAMpoints significantly outperforms classical detectors as well as state-of-the-art CNN-based methods in matching and registration quality for retinal images."
GLoSH: Global-Local Spherical Harmonics for Intrinsic Image Decomposition,"Hao Zhou, Xiang Yu, David W. Jacobs","University of Maryland, College Park, MD, USA; NEC Laboratories America",50.0,usa,50.0,USA,"Traditional intrinsic image decomposition focuses on decomposing images into reflectance and shading, leaving surfaces normals and lighting entangled in shading. In this work, we propose a Global-Local Spherical Harmonics (GLoSH) lighting model to improve the lighting component, and jointly predict reflectance and surface normals. The global SH models the holistic lighting while local SH account for the spatial variation of lighting. Also, a novel non-negative lighting constraint is proposed to encourage the estimated SH to be physically meaningful. To seamlessly reflect the GLoSH model, we design a coarse-to-fine network structure. The coarse network predicts global SH, reflectance and normals, and the fine network predicts their local residuals. Lacking labels for reflectance and lighting, we apply synthetic data for model pre-training and fine-tune the model with real data in a self-supervised way. Compared to the state-of-the-art methods only targeting normals or reflectance and shading, our method recovers all components and achieves consistently better results on three real datasets, IIW, SAW and NYUv2.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_GLoSH_Global-Local_Spherical_Harmonics_for_Intrinsic_Image_Decomposition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_GLoSH_Global-Local_Spherical_Harmonics_for_Intrinsic_Image_Decomposition_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010991/,"['Lighting', 'Harmonic analysis', 'Predictive models', 'Data models', 'Training', 'Image decomposition', 'Surface acoustic waves']","['Spherical Harmonics', 'Data In Way', 'Non-negativity Constraints', 'Surface Normals', 'Fine Network', 'Convolutional Neural Network', 'Deep Network', 'Single Image', 'Network Training', 'Adam Optimizer', 'Weight Decay', 'Single Object', 'Average Precision', 'Natural Scenes', 'Semidefinite Programming', 'Local Light', 'RGB-D Images', 'Semidefinite Programming Problem', 'Guided Filter']",,35,"Traditional intrinsic image decomposition focuses on decomposing images into reflectance and shading, leaving surfaces normals and lighting entangled in shading. In this work, we propose a Global-Local Spherical Harmonics (GLoSH) lighting model to improve the lighting component, and jointly predict reflectance and surface normals. The global SH models the holistic lighting while local SH account for the spatial variation of lighting. Also, a novel non-negative lighting constraint is proposed to encourage the estimated SH to be physically meaningful. To seamlessly reflect the GLoSH model, we design a coarse-to-fine network structure. The coarse network predicts global SH, reflectance and normals, and the fine network predicts their local residuals. Lacking labels for reflectance and lighting, we apply synthetic data for model pre-training and fine-tune the model with real data in a self-supervised way. Compared to the state-of-the-art methods only targeting normals or reflectance and shading, our method recovers all components and achieves consistently better results on three real datasets, IIW, SAW and NYUv2."
GODS: Generalized One-Class Discriminative Subspaces for Anomaly Detection,"Jue Wang, Anoop Cherian","Mitsubishi Electric Research Labs, Cambridge, MA; Australian National University, Canberra",100.0,"Australia, usa",0.0,,"One-class learning is the classic problem of fitting a model to data for which annotations are available only for a single class. In this paper, we propose a novel objective for one-class learning. Our key idea is to use a pair of orthonormal frames -- as subspaces -- to ""sandwich"" the labeled data via optimizing for two objectives jointly: i) minimize the distance between the origins of the two subspaces, and ii) to maximize the margin between the hyperplanes and the data, either subspace demanding the data to be in its positive and negative orthant respectively. Our proposed objective however leads to a non-convex optimization problem, to which we resort to Riemannian optimization schemes and derive an efficient conjugate gradient scheme on the Stiefel manifold. To study the effectiveness of our scheme, we propose a new dataset Dash-Cam-Pose, consisting of clips with skeleton poses of humans seated in a car, the task being to classify the clips as normal or abnormal; the latter is when any human pose is out-of-position with regard to say an airbag deployment. Our experiments on the proposed Dash-Cam-Pose dataset, as well as several other standard anomaly/novelty detection benchmarks demonstrate the benefits of our scheme, achieving state-of-the-art one-class accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_GODS_Generalized_One-Class_Discriminative_Subspaces_for_Anomaly_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_GODS_Generalized_One-Class_Discriminative_Subspaces_for_Anomaly_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008531/,"['Support vector machines', 'Data models', 'Kernel', 'Optimization', 'Automobiles', 'Principal component analysis', 'Manifolds']","['Anomaly Detection', 'Discriminative Subspace', 'Hyperplane', 'Conjugate Gradient', 'Human Pose', 'Support Vector Machine', 'F1 Score', 'Singular Value Decomposition', 'Nonlinear Programming', 'Optimal Efficiency', 'Action Recognition', 'False Alarm Rate', 'Pose Estimation', 'Video Surveillance', 'Labeled Training Data', 'Riemannian Manifold', 'Tangent Space', 'Passenger Cars', 'Human Activity Recognition', 'Temporal Convolutional Network', 'One-class Classification', 'Video Action Recognition', 'Hinge Loss', 'Passenger Safety', 'Suitable Distance', 'Problem Setup', 'Kernel Methods', 'Binary Label', 'Action Recognition Datasets']",,81,"One-class learning is the classic problem of fitting a model to data for which annotations are available only for a single class. In this paper, we propose a novel objective for one-class learning. Our key idea is to use a pair of orthonormal frames - as subspaces - to ""sandwich'' the labeled data via optimizing for two objectives jointly: i) minimize the distance between the origins of the two subspaces, and ii) to maximize the margin between the hyperplanes and the data, either subspace demanding the data to be in its positive and negative orthant respectively. Our proposed objective however leads to a non-convex optimization problem, to which we resort to Riemannian optimization schemes and derive an efficient conjugate gradient scheme on the Stiefel manifold. To study the effectiveness of our scheme, we propose a new dataset Dash-Cam-Pose, consisting of clips with skeleton poses of humans seated in a car, the task being to classify the clips as normal or abnormal; the latter is when any human pose is out-of-position with regard to say an airbag deployment. Our experiments on the proposed Dash-Cam-Pose dataset, as well as several other standard anomaly/novelty detection benchmarks demonstrate the benefits of our scheme, achieving state-of-the-art one-class accuracy."
GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild,"Alexander Grabner, Peter M. Roth, Vincent Lepetit","Laboratoire Bordelais de Recherche en Informatique, University of Bordeaux, France; Institute of Computer Graphics and Vision, Graz University of Technology, Austria",100.0,"France, austria",0.0,,"We present a joint 3D pose and focal length estimation approach for object categories in the wild. In contrast to previous methods that predict 3D poses independently of the focal length or assume a constant focal length, we explicitly estimate and integrate the focal length into the 3D pose estimation. For this purpose, we combine deep learning techniques and geometric algorithms in a two-stage approach: First, we estimate an initial focal length and establish 2D-3D correspondences from a single RGB image using a deep network. Second, we recover 3D poses and refine the focal length by minimizing the reprojection error of the predicted correspondences. In this way, we exploit the geometric prior given by the focal length for 3D pose estimation. This results in two advantages: First, we achieve significantly improved 3D translation and 3D pose accuracy compared to existing methods. Second, our approach finds a geometric consensus between the individual projection parameters, which is required for precise 2D-3D alignment. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple different metrics.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Grabner_GP2C_Geometric_Projection_Parameter_Consensus_for_Joint_3D_Pose_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Grabner_GP2C_Geometric_Projection_Parameter_Consensus_for_Joint_3D_Pose_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010630/,"['Three-dimensional displays', 'Two dimensional displays', 'Cameras', 'Machine learning', 'Pose estimation', 'Optimization']","['Focal Length', 'Pose Estimation', 'Length Estimation', 'Human Pose Estimation', '3D Pose', 'Focal Length Estimation', 'Deep Learning', 'Deep Network', 'Stanford', 'RGB Images', 'Constant Length', 'Reprojection Error', 'Geometric Algorithm', '3D Translation', 'Spatial Resolution', 'Computer Vision', 'Object Detection', 'Image Object', 'Bounding Box', 'Single Network', '3D Point', '3D Rotation', '2D Location', 'Geometric Optimization', '3D Bounding Box', 'Camera Intrinsics', 'Huber Loss', '3D Coordinates', 'Object Appearance', 'Translation Accuracy']",,12,"We present a joint 3D pose and focal length estimation approach for object categories in the wild. In contrast to previous methods that predict 3D poses independently of the focal length or assume a constant focal length, we explicitly estimate and integrate the focal length into the 3D pose estimation. For this purpose, we combine deep learning techniques and geometric algorithms in a two-stage approach: First, we estimate an initial focal length and establish 2D-3D correspondences from a single RGB image using a deep network. Second, we recover 3D poses and refine the focal length by minimizing the reprojection error of the predicted correspondences. In this way, we exploit the geometric prior given by the focal length for 3D pose estimation. This results in two advantages: First, we achieve significantly improved 3D translation and 3D pose accuracy compared to existing methods. Second, our approach finds a geometric consensus between the individual projection parameters, which is required for precise 2D-3D alignment. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple different metrics."
GSLAM: A General SLAM Framework and Benchmark,"Yong Zhao, Shibiao Xu, Shuhui Bu, Hongkai Jiang, Pengcheng Han","Northwestern Polytechnical University; NLPR, Institute of Automation, Chinese Academy of Sciences",100.0,china,0.0,,"SLAM technology has recently seen many successes and attracted the attention of high-technological companies. However, how to unify the interface of existing or emerging algorithms, and effectively perform benchmark about the speed, robustness and portability are still problems. In this paper, we propose a novel SLAM platform named GSLAM, which not only provides evaluation functionality, but also supplies useful toolkit for researchers to quickly develop their SLAM systems. Our core contribution is an universal, cross-platform and full open-source SLAM interface for both research and commercial usage, which is aimed to handle interactions with input dataset, SLAM implementation, visualization and applications in an unified framework. Through this platform, users can implement their own functions for better performance with plugin form and further boost the application to practical usage of the SLAM.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_GSLAM_A_General_SLAM_Framework_and_Benchmark_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_GSLAM_A_General_SLAM_Framework_and_Benchmark_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010050/,"['Simultaneous localization and mapping', 'Benchmark testing', 'Visualization', 'Machine learning', 'Transforms']","['Simultaneous Localization And Mapping', 'Commercial Users', 'Vocabulary', 'Standard Form', 'Point Cloud', 'Unmanned Aerial Vehicles', 'Monocular', 'Memory Usage', 'Rigid Transformation', 'Lie Algebra', 'Rotated Component', 'Camera Model', 'Camera Pose', 'Robotics Research', 'Bundle Adjustment', 'Graph Optimization', 'CPU Usage', 'CPU Resources', 'Place Recognition', 'Unit Quaternion', 'Unified Interface', 'Euler Angles', 'Data Structure', 'Sensor Data', 'Similarity Transformation', 'Extended Kalman Filter', 'Lot Of Memory', 'Computer Vision', 'Matrix Representation']",,17,"SLAM technology has recently seen many successes and attracted the attention of high-technological companies. However, how to unify the interface of existing or emerging algorithms, and effectively perform benchmark about the speed, robustness and portability are still problems. In this paper, we propose a novel SLAM platform named GSLAM, which not only provides evaluation functionality, but also supplies useful toolkit for researchers to quickly develop their SLAM systems. Our core contribution is an universal, cross-platform and full open-source SLAM interface for both research and commercial usage, which is aimed to handle interactions with input dataset, SLAM implementation, visualization and applications in an unified framework. Through this platform, users can implement their own functions for better performance with plugin form and further boost the application to practical usage of the SLAM."
GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping,"Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, Pascal Fua","CVLab, EPFL, Switzerland; Fision Technologies, Zurich, Switzerland",50.0,switzerland,50.0,Switzerland,"While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body. Specifically, we build upon the recent progress in 3D point cloud processing with deep networks to extract garment features at varying levels of detail, including point-wise, patch-wise and global features. We fuse these features with those extracted in parallel from the 3D body, so as to model the cloth-body interactions. The resulting two-stream architecture, which we call as GarNet, is trained using a loss function inspired by physics-based modeling, and delivers visually plausible garment shapes whose 3D points are, on average, less than 1 cm away from those of a PBS method, while running 100 times faster. Moreover, the proposed method can model various garment types with different cutting patterns when parameters of those patterns are given as input to the network.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gundogdu_GarNet_A_Two-Stream_Network_for_Fast_and_Accurate_3D_Cloth_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gundogdu_GarNet_A_Two-Stream_Network_for_Fast_and_Accurate_3D_Cloth_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010344/,"['Clothing', 'Three-dimensional displays', 'Shape', 'Computational modeling', 'Solid modeling', 'Computer architecture', 'Feature extraction']","['Loss Function', 'Global Features', 'Point Cloud', '3D Point', '3D Point Cloud', 'Cloud Processing', '3D Body', 'Point Cloud Processing', 'Physics-based Simulation', 'Training Set', 'Local Features', 'Multilayer Perceptron', 'Convolution Operation', 'Body Shape', 'Generative Adversarial Networks', 'Skip Connections', '3D Mesh', '3D Shape', 'Loss Term', 'Fusion Network', 'Body Characteristics', 'Normal Term', 'Global Ones', 'Body Pose', 'Low-dimensional Subspace', 'Mesh Vertices', 'Translation Vector', 'Conditional Generative Adversarial Network', 'Mesh Points', '3D Pose']",,99,"While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body. Specifically, we build upon the recent progress in 3D point cloud processing with deep networks to extract garment features at varying levels of detail, including point-wise, patch-wise and global features. We fuse these features with those extracted in parallel from the 3D body, so as to model the cloth-body interactions. The resulting two-stream architecture, which we call as GarNet, is trained using a loss function inspired by physics-based modeling, and delivers visually plausible garment shapes whose 3D points are, on average, less than 1 cm away from those of a PBS method, while running 100 times faster. Moreover, the proposed method can model various garment types with different cutting patterns when parameters of those patterns are given as input to the network."
Gated-SCNN: Gated Shape CNNs for Semantic Segmentation,"Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler","NVIDIA, University of Toronto, Vector Institute; NVIDIA; NVIDIA, University of Waterloo",66.66666666666666,"Canada, canada",33.33333333333334,USA,"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Takikawa_Gated-SCNN_Gated_Shape_CNNs_for_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Takikawa_Gated-SCNN_Gated_Shape_CNNs_for_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009833/,"['Shape', 'Streaming media', 'Semantics', 'Logic gates', 'Image segmentation', 'Computer architecture', 'Task analysis']","['Convolutional Neural Network', 'Semantic Segmentation', 'Deep Convolutional Neural Network', 'Small Objects', 'Intermediate Layer', 'Object Boundaries', 'Strong Baseline', 'Parallel Information', 'Loss Function', 'Convolutional Layers', 'Cross-entropy Loss', 'Intersection Over Union', 'Traffic Light', 'Distribution Of Categories', 'Multi-task Learning', 'Binary Cross Entropy', 'Granular Data', 'Attention Map', 'Dual Task', 'Conditional Random Field', 'Boundary Map', 'Boundary Prediction', 'Backbone Architecture', 'Intermediate Representation', 'Convolutional Neural Network For Segmentation', 'Boundary Information', 'Semantic Segmentation Network', 'Segmentation Prediction', 'Boundary Features', 'Network Output']",,464,"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines."
Gated2Depth: Real-Time Dense Lidar From Gated Images,"Tobias Gruber, Frank Julca-Aguilar, Mario Bijelic, Felix Heide",Algolux; Princeton University; Daimler AG; Ulm University,50.0,"germany, usa",50.0,Canada,"We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gruber_Gated2Depth_Real-Time_Dense_Lidar_From_Gated_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gruber_Gated2Depth_Real-Time_Dense_Lidar_From_Gated_Images_ICCV_2019_paper.pdf,,https://github.com/gruberto/Gated2Depth,,main,Oral,https://ieeexplore.ieee.org/document/9010016/,"['Logic gates', 'Laser radar', 'Cameras', 'Estimation', 'Photonics', 'Real-time systems']","['Semantic', 'Low Resolution', 'Depth Map', 'Depth Camera', 'Scene Understanding', 'Accurate Depth', 'LiDAR Scans', 'Lidar System', 'Dense Depth', 'Training Set', 'Pedestrian', 'Ambient Light', 'Action Recognition', 'Depth Estimation', 'Low Reflectance', 'Objects In The Scene', 'Avalanche Photodiode', 'Stereo Images', 'Stereo Camera', 'Vertical-cavity Surface-emitting Lasers', 'LiDAR Point', 'Pulse Profile', 'Time-of-flight Sensors', 'Monocular Depth Estimation', 'RGB Data', 'Exposure Profiles', 'Sparse Point', 'Sparse Measurements', 'Large Datasets']",,39,"We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth."
Gaussian Affinity for Max-Margin Class Imbalanced Learning,"Munawar Hayat, Salman Khan, Syed Waqas Zamir, Jianbing Shen, Ling Shao","Inception Institute of Artificial Intelligence, Australian National University; Inception Institute of Artificial Intelligence, Beijing Institute of Technology; Inception Institute of Artificial Intelligence; Inception Institute of Artificial Intelligence, University of Canberra",100.0,"Australia, China, uae",0.0,,"Real-world object classes appear in imbalanced ratios. This poses a significant challenge for classifiers which get biased towards frequent classes. We hypothesize that improving the generalization capability of a classifier should improve learning on imbalanced datasets. Here, we introduce the first hybrid loss function that jointly performs classification and clustering in a single formulation. Our approach is based on an `affinity measure' in Euclidean space that leads to the following benefits: (1) direct enforcement of maximum margin constraints on classification boundaries, (2) a tractable way to ensure uniformly spaced and equidistant cluster centers, (3) flexibility to learn multiple class prototypes to support diversity and discriminability in feature space. Our extensive experiments demonstrate the significant performance improvements on visual classification and verification tasks on multiple imbalanced datasets. The proposed loss can easily be plugged in any deep architecture as a differentiable block and demonstrates robustness against different levels of data imbalance and corrupted labels.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hayat_Gaussian_Affinity_for_Max-Margin_Class_Imbalanced_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hayat_Gaussian_Affinity_for_Max-Margin_Class_Imbalanced_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010065/,"['Prototypes', 'Training', 'Loss measurement', 'Extraterrestrial measurements', 'Robustness', 'Neural networks']","['Imbalanced Learning', 'Class Imbalance Learning', 'Loss Function', 'Feature Space', 'Euclidean Space', 'Imbalanced Data', 'Imbalanced Datasets', 'Frequent Class', 'Affinity Measurements', 'Class Boundaries', 'Imbalance In Levels', 'Class Prototypes', 'Training Set', 'Deep Network', 'Similarity Measure', 'Major Classes', 'Face Recognition', 'Basal Cell Carcinoma', 'Inverse Distance', 'Vector Magnitude', 'Softmax Loss', 'Noisy Labels', 'Gaussian Measurement', 'Minority Class', 'Loss Of Affinity', 'Inter-class Separability', 'Dot Product Of Vector', 'Angular Domain', 'Triplet Loss', 'Intra-class Variance']",,55,"Real-world object classes appear in imbalanced ratios. This poses a significant challenge for classifiers which get biased towards frequent classes. We hypothesize that improving the generalization capability of a classifier should improve learning on imbalanced datasets. Here, we introduce the first hybrid loss function that jointly performs classification and clustering in a single formulation. Our approach is based on an 'affinity measure' in Euclidean space that leads to the following benefits: (1) direct enforcement of maximum margin constraints on classification boundaries, (2) a tractable way to ensure uniformly spaced and equidistant cluster centers, (3) flexibility to learn multiple class prototypes to support diversity and discriminability in feature space. Our extensive experiments demonstrate the significant performance improvements on visual classification and verification tasks on multiple imbalanced datasets. The proposed loss can easily be plugged in any deep architecture as a differentiable block and demonstrates robustness against different levels of data imbalance and corrupted labels."
Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving,"Jiwoong Choi, Dayoung Chun, Hyun Kim, Hyuk-Jae Lee",Seoul National University of Science and Technology; Seoul National University,100.0,south korea,0.0,,"The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008565/,"['Autonomous vehicles', 'Uncertainty', 'Detectors', 'Object detection', 'Real-time systems', 'Feature extraction']","['Detection Accuracy', 'Object Detection', 'Autonomous Vehicles', 'Position Uncertainty', 'Loss Function', 'Real-time Detection', 'Detection Process', 'Fatal Accidents', 'Gaussian Parameters', 'One-stage Detectors', 'Training Set', 'Deep Learning', 'Validation Set', 'Pedestrian', 'Intersection Over Union', 'Gaussian Model', 'Image Sensor', 'Small Objects', 'Classification Score', 'Traffic Light', 'KITTI Dataset', 'Detection Speed', 'Two-stage Detectors', 'Objective Scores', 'Intersection Over Union Threshold', 'Dense Objects', 'Anchor Boxes', 'Input Resolution', 'Results In Row', 'Grid Unit']",,316,"The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications."
Gaze360: Physically Unconstrained Gaze Estimation in the Wild,"Petr Kellnhofer, AdriÃ  Recasens, Simon Stent, Wojciech Matusik, Antonio Torralba","Massachusetts Institute of Technology, Cambridge MA 02139, USA; Toyota Research Institute, Cambridge, MA, 02139, USA",100.0,usa,0.0,,"Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.pdf,http://gaze360.csail.mit.edu,,,main,Poster,https://ieeexplore.ieee.org/document/9010825/,"['Cameras', 'Three-dimensional displays', 'Estimation', 'Face', 'Adaptation models', 'Lighting']","['Gaze Estimation', 'Eye-tracking', 'Indoor Environments', 'Domain Adaptation', 'Dataset For Estimation', 'Head Pose', 'Convolutional Neural Network', 'Coordinate System', 'Prediction Error', 'Coordination Sphere', 'Eyeball', 'Geometric Model', 'Subject Positions', 'Pose Estimation', 'Quantile Regression', 'Error Bounds', 'Bidirectional Long Short-term Memory', 'Gaze Direction', 'Face Detection', 'Position Of The Foot', 'Ladybird', 'Actual Error', 'Unconstrained Environment', 'Gaze Data', 'Acquisition Procedure', 'Eye Contact', 'Subjective Views', 'Unseen Domains']",,230,"Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu."
Generating Diverse and Descriptive Image Captions Using Visual Paraphrases,"Lixin Liu, Jiajun Tang, Xiaojun Wan, Zongming Guo","Institute of Computer Science and Technology, Peking University",100.0,china,0.0,,"Recently there has been significant progress in image captioning with the help of deep learning. However, captions generated by current state-of-the-art models are still far from satisfactory, despite high scores in terms of conventional metrics such as BLEU and CIDEr. Human-written captions are diverse, informative and precise, but machine-generated captions seem to be simple, vague and dull. In this paper, aimed at improving diversity and descriptiveness characteristics of generated image captions, we propose a model utilizing visual paraphrases (different sentences describing the same image) in captioning datasets. We explore different strategies to select useful visual paraphrase pairs for training by designing a variety of scoring functions. Our model consists of two decoding stages, where a preliminary caption is generated in the first stage and then paraphrased into a more diverse and descriptive caption in the second stage. Extensive experiments are conducted on the benchmark MS COCO dataset, with automatic evaluation and human evaluation results verifying the effectiveness of our model.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Generating_Diverse_and_Descriptive_Image_Captions_Using_Visual_Paraphrases_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generating_Diverse_and_Descriptive_Image_Captions_Using_Visual_Paraphrases_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010984/,"['Visualization', 'Decoding', 'Training', 'Automobiles', 'Task analysis', 'Roads', 'Machine learning']","['Image Captioning', 'High Scores', 'Scoring Function', 'COCO Dataset', 'Decoding Stage', 'Detailed Description', 'Training Set', 'Image Features', 'Visual Information', 'Attention Mechanism', 'Generative Adversarial Networks', 'Latent Space', 'Baseline Methods', 'Hidden State', 'Diverse Expression', 'Image Retrieval', 'Faster R-CNN', 'Image Descriptors', 'Retrieval System', 'Retrieval Performance', 'Beam Search', 'Text Generation', 'Context Vector', 'Sentence Length', 'Conditional Generative Adversarial Network', 'Aspects Of Image', 'Natural Language']",,23,"Recently there has been significant progress in image captioning with the help of deep learning. However, captions generated by current state-of-the-art models are still far from satisfactory, despite high scores in terms of conventional metrics such as BLEU and CIDEr. Human-written captions are diverse, informative and precise, but machine-generated captions seem to be simple, vague and dull. In this paper, aimed at improving diversity and descriptiveness characteristics of generated image captions, we propose a model utilizing visual paraphrases (different sentences describing the same image) in captioning datasets. We explore different strategies to select useful visual paraphrase pairs for training by designing a variety of scoring functions. Our model consists of two decoding stages, where a preliminary caption is generated in the first stage and then paraphrased into a more diverse and descriptive caption in the second stage. Extensive experiments are conducted on the benchmark MS COCO dataset, with automatic evaluation and human evaluation results verifying the effectiveness of our model."
Generating Easy-to-Understand Referring Expressions for Target Identifications,"Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro Sato, Yoshitaka Ushiku, Tatsuya Harada","The University of Tokyo, RIKEN; The University of Tokyo; DENSO IT Laboratory, Inc.; DENSO CORPORATION",50.0,"Japan, japan",50.0,Japan,"This paper addresses the generation of referring expressions that not only refer to objects correctly but also let humans find them quickly. As a target becomes relatively less salient, identifying referred objects itself becomes more difficult. However, the existing studies regarded all sentences that refer to objects correctly as equally good, ignoring whether they are easily understood by humans. If the target is not salient, humans utilize relationships with the salient contexts around it to help listeners to comprehend it better. To derive this information from human annotations, our model is designed to extract information from the target and from the environment. Moreover, we regard that sentences that are easily understood are those that are comprehended correctly and quickly by humans. We optimized this by using the time required to locate the referred objects by humans and their accuracies. To evaluate our system, we created a new referring expression dataset whose images were acquired from Grand Theft Auto V (GTA V), limiting targets to persons. Experimental results show the effectiveness of our approach. Our code and dataset are available at https://github.com/mikittt/easy-to-understand-REG.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tanaka_Generating_Easy-to-Understand_Referring_Expressions_for_Target_Identifications_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tanaka_Generating_Easy-to-Understand_Referring_Expressions_for_Target_Identifications_ICCV_2019_paper.pdf,,https://github.com/mikittt/easy-to-understand-REG,,main,Poster,https://ieeexplore.ieee.org/document/9010706/,"['Feature extraction', 'Context modeling', 'Data mining', 'Robots', 'Symbiosis', 'Natural languages', 'Task analysis']","['Convolutional Neural Network', 'Deep Neural Network', 'Image Features', 'Natural Language', 'Comprehensive Evaluation', 'Long Short-term Memory', 'Image Object', 'Large-scale Datasets', 'Bounding Box', 'Baseline Methods', 'Additional Annotations', 'Sentence Context', 'Image Captioning', 'Automatic Evaluation', 'Low Salience', 'Ranking Loss', 'Local Attention', 'Attention Values', 'Comprehension Accuracy', 'Intelligence Agencies', 'Target Object', 'Global Attention', 'Comprehensive Performance']",,8,"This paper addresses the generation of referring expressions that not only refer to objects correctly but also let humans find them quickly. As a target becomes relatively less salient, identifying referred objects itself becomes more difficult. However, the existing studies regarded all sentences that refer to objects correctly as equally good, ignoring whether they are easily understood by humans. If the target is not salient, humans utilize relationships with the salient contexts around it to help listeners to comprehend it better. To derive this information from human annotations, our model is designed to extract information from the target and from the environment. Moreover, we regard that sentences that are easily understood are those that are comprehended correctly and quickly by humans. We optimized this by using the time required to locate the referred objects by humans and their accuracies. To evaluate our system, we created a new referring expression dataset whose images were acquired from Grand Theft Auto V (GTA V), limiting targets to persons. Experimental results show the effectiveness of our approach. Our code and dataset are available at https://github.com/mikittt/easy-to-understand-REG."
Generative Adversarial Minority Oversampling,"Sankha Subhra Mullick, Shounak Datta, Swagatam Das","Indian Statistical Institute, Kolkata, India; Duke University, Durham, NC, USA",100.0,"india, usa",0.0,,"Class imbalance is a long-standing problem relevant to a number of real-world applications of deep learning. Oversampling techniques, which are effective for handling class imbalance in classical learning systems, can not be directly applied to end-to-end deep learning systems. We propose a three-player adversarial game between a convex generator, a multi-class classifier network, and a real/fake discriminator to perform oversampling in deep learning systems. The convex generator generates new samples from the minority classes as convex combinations of existing instances, aiming to fool both the discriminator as well as the classifier into misclassifying the generated samples. Consequently, the artificial samples are generated at critical locations near the peripheries of the classes. This, in turn, adjusts the classifier induced boundaries in a way which is more likely to reduce misclassification from the minority classes. Extensive experiments on multiple class imbalanced image datasets establish the efficacy of our proposal.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mullick_Generative_Adversarial_Minority_Oversampling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mullick_Generative_Adversarial_Minority_Oversampling_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008836/,"['Generators', 'Machine learning', 'Games', 'Training', 'Feature extraction', 'Gallium nitride', 'Tuning']","['Deep Learning', 'Multi-label', 'Learning System', 'Class Imbalance', 'Minority Class', 'Oversampling Technique', 'Deep Learning System', 'Training Set', 'Least-squares', 'Optimization Problem', 'Experimental Section', 'Convolutional Layers', 'Cross-entropy Loss', 'Real Samples', 'Generative Adversarial Networks', 'Latent Space', 'Model Discrimination', 'Convex Hull', 'Representation Of Distribution', 'Variational Autoencoder', 'MNIST Dataset', 'Respective Classes', 'Fréchet Inception Distance', 'Least Squares Loss', 'Class Boundaries', 'Output Line', 'Convolutional Features', 'Geometric Mean', 'Traditional Classification', 'Class Labels']",,120,"Class imbalance is a long-standing problem relevant to a number of real-world applications of deep learning. Oversampling techniques, which are effective for handling class imbalance in classical learning systems, can not be directly applied to end-to-end deep learning systems. We propose a three-player adversarial game between a convex generator, a multi-class classifier network, and a real/fake discriminator to perform oversampling in deep learning systems. The convex generator generates new samples from the minority classes as convex combinations of existing instances, aiming to fool both the discriminator as well as the classifier into misclassifying the generated samples. Consequently, the artificial samples are generated at critical locations near the peripheries of the classes. This, in turn, adjusts the classifier induced boundaries in a way which is more likely to reduce misclassification from the minority classes. Extensive experiments on multiple class imbalanced image datasets establish the efficacy of our proposal."
Generative Adversarial Networks for Extreme Learned Image Compression,"Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, Luc Van Gool","ETH Zürich, Switzerland",100.0,Switzerland,0.0,,"We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Agustsson_Generative_Adversarial_Networks_for_Extreme_Learned_Image_Compression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Agustsson_Generative_Adversarial_Networks_for_Extreme_Learned_Image_Compression_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010721/,"['Image coding', 'Gallium nitride', 'Bit rate', 'Generative adversarial networks', 'Distortion', 'Semantics', 'Training']","['Generative Adversarial Networks', 'Image Compression', 'Learned Image Compression', 'User Study', 'Storage Cost', 'Least Significant Bit', 'Semantic Map', 'Deep Neural Network', 'Validation Set', 'Local Structure', 'Operation Mode', 'Recurrent Neural Network', 'Latent Space', 'Visual Quality', 'Peak Signal-to-noise Ratio', 'Loss Term', 'Bitstream', 'Validation Images', 'Mean Square Error Loss', 'Conditional Generative Adversarial Network', 'Generative Adversarial Networks Loss', 'Arithmetic Coding', 'Compression Artifacts', 'Compressed Representation', 'Compression Algorithm', 'Generative Adversarial Network Framework', 'Vector Graphics']",,307,"We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits."
Generative Adversarial Training for Weakly Supervised Cloud Matting,"Zhengxia Zou, Wenyuan Li, Tianyang Shi, Zhenwei Shi, Jieping Ye","University of Michigan, Ann Arbor; Beihang University; Didi Chuxing & University of Michigan, Ann Arbor; NetEase Fuxi AI Lab",75.0,"china, usa",25.0,China,"The detection and removal of cloud in remote sensing images are essential for earth observation applications. Most previous methods consider cloud detection as a pixel-wise semantic segmentation process (cloud v.s. background), which inevitably leads to a category-ambiguity problem when dealing with semi-transparent clouds. We re-examine the cloud detection under a totally different point of view, i.e. to formulate it as a mixed energy separation process between foreground and background images, which can be equivalently implemented under an image matting paradigm with a clear physical significance. We further propose a generative adversarial framework where the training of our model neither requires any pixel-wise ground truth reference nor any additional user interactions. Our model consists of three networks, a cloud generator G, a cloud discriminator D, and a cloud matting network F, where G and D aim to generate realistic and physically meaningful cloud images by adversarial training, and F learns to predict the cloud reflectance and attenuation. Experimental results on a global set of satellite images demonstrate that our method, without ever using any pixel-wise ground truth during training, achieves comparable and even higher accuracy over other fully supervised methods, including some recent popular cloud detectors and some well-known semantic segmentation frameworks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Generative_Adversarial_Training_for_Weakly_Supervised_Cloud_Matting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Generative_Adversarial_Training_for_Weakly_Supervised_Cloud_Matting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009465/,"['Cloud computing', 'Clouds', 'Training', 'Attenuation', 'Generators', 'Imaging', 'Generative adversarial networks']","['Adversarial Training', 'Generative Adversarial Training', 'Human-computer Interaction', 'Semantic Segmentation', 'Background Image', 'Cloud Images', 'Ground Truth Reference', 'Objective Function', 'Mean Absolute Error', 'Attention In Recent Years', 'Object Recognition', 'Generative Adversarial Networks', 'Precision-recall Curve', 'Background Regions', 'Mean Absolute Percentage Error', 'Ground Objects', 'Image X', 'Style Transfer', 'Ablation Analysis', 'Baseline Detection', 'Ground Truth For Training', 'Alpha Matte', 'Thin Clouds', 'High-resolution Aerial Images', 'Foreground Regions', 'Atmospheric Attenuation', 'Thick Clouds', 'Image Segmentation', 'Real Ones', 'Detection Methods']",,20,"The detection and removal of cloud in remote sensing images are essential for earth observation applications. Most previous methods consider cloud detection as a pixel-wise semantic segmentation process (cloud v.s. background), which inevitably leads to a category-ambiguity problem when dealing with semi-transparent clouds. We re-examine the cloud detection under a totally different point of view, i.e. to formulate it as a mixed energy separation process between foreground and background images, which can be equivalently implemented under an image matting paradigm with a clear physical significance. We further propose a generative adversarial framework where the training of our model neither requires any pixel-wise ground truth reference nor any additional user interactions. Our model consists of three networks, a cloud generator G, a cloud discriminator D, and a cloud matting network F, where G and D aim to generate realistic and physically meaningful cloud images by adversarial training, and F learns to predict the cloud reflectance and attenuation. Experimental results on a global set of satellite images demonstrate that our method, without ever using any pixel-wise ground truth during training, achieves comparable and even higher accuracy over other fully supervised methods, including some recent popular cloud detectors and some well-known semantic segmentation frameworks."
Generative Modeling for Small-Data Object Detection,"Lanlan Liu, Michael Muelly, Jia Deng, Tomas Pfister, Li-Jia Li","Google Cloud AI; University of Michigan, Ann Arbor; Stanford University; Princeton University",75.0,usa,25.0,USA,"This paper explores object detection in the small data regime, where only a limited number of annotated bounding boxes are available due to data rarity and annotation expense. This is a common challenge today with machine learning being applied to many new tasks where obtaining training data is more challenging, e.g. in medical images with rare diseases that doctors sometimes only see once in their life-time. In this work we explore this problem from a generative modeling perspective by learning to generate new images with associated bounding boxes, and using these for training an object detector. We show that simply training previously proposed generative models does not yield satisfactory performance due to them optimizing for image realism rather than object detection accuracy. To this end we develop a new model with a novel unrolling mechanism that jointly optimizes the generative model and a detector such that the generated images improve the performance of the detector. We show this method outperforms the state of the art on two challenging datasets, disease detection and small data pedestrian detection, improving the average precision on NIH Chest X-ray by a relative 20% and localization accuracy by a relative 50%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Generative_Modeling_for_Small-Data_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generative_Modeling_for_Small-Data_Object_Detection_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008794/,"['Detectors', 'Generators', 'Gallium nitride', 'Object detection', 'Task analysis', 'Diseases', 'Generative adversarial networks']","['Object Detection', 'Training Data', 'Medical Imaging', 'State Of The Art', 'Detection Performance', 'Localization Accuracy', 'Disease Detection', 'Small Data', 'Bounding Box', 'Average Precision', 'Bounding Box Annotations', 'Pedestrian Detection', 'Validation Set', 'Data Augmentation', 'Intersection Over Union', 'Detection Task', 'Generative Adversarial Networks', 'Additional Imaging', 'Clear Image', 'Synthetic Images', 'Generative Adversarial Networks Model', 'Object Detection Task', 'Translation Framework', 'Realistic Images', 'Qualitative Improvement', 'Discriminator Loss', 'Direct Feedback', 'Classification Loss', 'Ground-truth Box']",,42,"This paper explores object detection in the small data regime, where only a limited number of annotated bounding boxes are available due to data rarity and annotation expense. This is a common challenge today with machine learning being applied to many new tasks where obtaining training data is more challenging, e.g. in medical images with rare diseases that doctors sometimes only see once in their life-time. In this work we explore this problem from a generative modeling perspective by learning to generate new images with associated bounding boxes, and using these for training an object detector. We show that simply training previously proposed generative models does not yield satisfactory performance due to them optimizing for image realism rather than object detection accuracy. To this end we develop a new model with a novel unrolling mechanism that jointly optimizes the generative model and a detector such that the generated images improve the performance of the detector. We show this method outperforms the state of the art on two challenging datasets, disease detection and small data pedestrian detection, improving the average precision on NIH Chest X-ray by a relative 20% and localization accuracy by a relative 50%."
Generative Multi-View Human Action Recognition,"Lichen Wang, Zhengming Ding, Zhiqiang Tao, Yunyu Liu, Yun Fu","Northeastern University, USA; Indiana University-Purdue University Indianapolis, USA",100.0,"china, usa",0.0,,"Multi-view action recognition targets to integrate complementary information from different views to improve classification performance. It is a challenging task due to the distinct gap between heterogeneous feature domains. Moreover, most existing methods neglect to consider the incomplete multi-view data, which limits their potential compatibility in real-world applications. In this work, we propose a Generative Multi-View Action Recognition (GMVAR) framework to address the challenges above. The adversarial generative network is leveraged to generate one view conditioning on the other view, which fully explores the latent connections in both intra-view and cross-view aspects. Our approach enhances the model robustness by employing adversarial training, and naturally handles the incomplete view case by imputing the missing data. Moreover, an effective View Correlation Discovery Network (VCDN) is proposed to further fuse the multi-view information in a higher-level label space. Extensive experiments demonstrate the effectiveness of our proposed approach by comparing with state-of-the-art algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Generative_Multi-View_Human_Action_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Generative_Multi-View_Human_Action_Recognition_ICCV_2019_paper.pdf,,https://github.com/wanglichenxj/Generative-Multi-View-Human-Action-Recognition,,main,Oral,https://ieeexplore.ieee.org/document/9008775/,"['Generative adversarial networks', 'Gallium nitride', 'Correlation', 'Training', 'Sensors', 'Task analysis', 'Generators']","['Action Recognition', 'Human Activity Recognition', 'Adversarial Training', 'Label Space', 'Recognition Framework', 'Multi-view Data', 'Incomplete View', 'Objective Function', 'Support Vector Machine', 'Least Squares Regression', 'Feature Space', 'General Strategy', 'Training Procedure', 'Generative Adversarial Networks', 'Highest Performance', 'Semi-supervised Learning', 'Single View', 'Depth Features', 'Conditional Generative Adversarial Network', 'RGB Features', 'Three-layer Network', 'Triplet Loss', 'Kinect Sensor', 'Label Matrix', 'Label Vector', 'Trade-off Parameter', 'Zero-shot', 'Contralateral', 'Real Samples']",,74,"Multi-view action recognition targets to integrate complementary information from different views to improve classification performance. It is a challenging task due to the distinct gap between heterogeneous feature domains. Moreover, most existing methods neglect to consider the incomplete multi-view data, which limits their potential compatibility in real-world applications. In this work, we propose a Generative Multi-View Action Recognition (GMVAR) framework to address the challenges above. The adversarial generative network is leveraged to generate one view conditioning on the other view, which fully explores the latent connections in both intra-view and cross-view aspects. Our approach enhances the model robustness by employing adversarial training, and naturally handles the incomplete view case by imputing the missing data. Moreover, an effective View Correlation Discovery Network (VCDN) is proposed to further fuse the multi-view information in a higher-level label space. Extensive experiments demonstrate the effectiveness of our proposed approach by comparing with state-of-the-art algorithms."
GeoStyle: Discovering Fashion Trends and Events,"Utkarsh Mall, Kevin Matzen, Bharath Hariharan, Noah Snavely, Kavita Bala",Cornell University; Facebook,50.0,usa,50.0,USA,"Understanding fashion styles and trends is of great potential interest to retailers and consumers alike. The photos people upload to social media are a historical and public data source of how people dress across the world and at different times. While we now have tools to automatically recognize the clothing and style attributes of what people are wearing in these photographs, we lack the ability to analyze spatial and temporal trends in these attributes or make predictions about the future. In this paper we address this need by providing an automatic framework that analyzes large corpora of street imagery to (a) discover and forecast long-term trends of various fashion attributes as well as automatically discovered styles, and (b) identify spatio-temporally localized events that affect what people wear. We show that our framework makes long term trend forecasts that are > 20% more accurate than prior art, and identifies hundreds of socially meaningful events that impact fashion across the globe.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mall_GeoStyle_Discovering_Fashion_Trends_and_Events_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mall_GeoStyle_Discovering_Fashion_Trends_and_Events_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008132/,"['Market research', 'Clothing', 'Urban areas', 'Visualization', 'Parametric statistics', 'Social network services', 'Image recognition']","['Fashion Trends', 'Social Media', 'Long-term Trends', 'Model Parameters', 'Human Activities', 'Outgroup', 'Social Consequences', 'Festival', 'Trend Analysis', 'Individual Attributes', 'Event Detection', 'Seasonal Cycle', 'Annual Cycle', 'Sporting Events', 'Interesting Trends', 'Salient Events', 'Linear Component', 'Long-term Prediction', 'Cycle Components', 'Chinese New Year', 'Long-term Forecasting', 'One-off Event', 'Chinese New', 'Latent Space', 'Image Embedding', 'Annual Event', 'Social Gatherings']",,36,"Understanding fashion styles and trends is of great potential interest to retailers and consumers alike. The photos people upload to social media are a historical and public data source of how people dress across the world and at different times. While we now have tools to automatically recognize the clothing and style attributes of what people are wearing in these photographs, we lack the ability to analyze spatial and temporal trends in these attributes or make predictions about the future. In this paper we address this need by providing an automatic framework that analyzes large corpora of street imagery to (a) discover and forecast long-term trends of various fashion attributes as well as automatically discovered styles, and (b) identify spatio-temporally localized events that affect what people wear. We show that our framework makes long term trend forecasts that are > 20% more accurate than prior art, and identifies hundreds of socially meaningful events that impact fashion across the globe."
Geometric Disentanglement for Generative Latent Shape Models,"Tristan Aumentado-Armstrong, Stavros Tsogkas, Allan Jepson, Sven Dickinson","University of Toronto Vector Institute for AI; University of Toronto; Samsung AI Center, Toronto",66.66666666666666,"Canada, canada",33.33333333333334,South Korea,"Representing 3D shapes is a fundamental problem in artificial intelligence, which has numerous applications within computer vision and graphics. One avenue that has recently begun to be explored is the use of latent representations of generative models. However, it remains an open problem to learn a generative model of shapes that is interpretable and easily manipulated, particularly in the absence of supervised labels. In this paper, we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for 3D point clouds in a natural way, using only geometric information, that builds upon prior work utilizing generative adversarial models of point sets. Our method makes use of tools from spectral geometry to separate intrinsic and extrinsic shape information, and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner. We also propose a novel disentanglement penalty that penalizes the predicted change in the latent representation of the output,with respect to the latent variables of the initial shape. We show that the resulting latent representation exhibits intuitive and interpretable behaviour, enabling tasks such as pose transfer that cannot easily be performed by models with an entangled representation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Aumentado-Armstrong_Geometric_Disentanglement_for_Generative_Latent_Shape_Models_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Aumentado-Armstrong_Geometric_Disentanglement_for_Generative_Latent_Shape_Models_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010824/,"['Shape', 'Three-dimensional displays', 'Computational modeling', 'Quaternions', 'Task analysis', 'Gallium nitride', 'Geometry']","['Latent Shape', 'Decoding', 'Computer Vision', 'Point Cloud', 'Latent Space', '3D Shape', 'Geometric Information', 'Latent Representation', 'Variational Autoencoder', 'Use Of Representations', 'Differential Geometry', 'Random Sampling', 'Supplemental Material', 'Data Augmentation', 'Mutual Information', 'Generative Adversarial Networks', 'Representation Learning', 'Total Correlation', 'Reconstruction Loss', 'Point Cloud Data', 'Shape Representation', 'Latent Groups', 'Laplace-Beltrami Operator', 'Unit Quaternion', 'Intrinsic Component', 'Raw Point Cloud', 'Generative Adversarial Networks Loss', 'Triangular Mesh', 'Point Cloud Reconstruction', 'Disentangled Representation']",,36,"Representing 3D shapes is a fundamental problem in artificial intelligence, which has numerous applications within computer vision and graphics. One avenue that has recently begun to be explored is the use of latent representations of generative models. However, it remains an open problem to learn a generative model of shapes that is interpretable and easily manipulated, particularly in the absence of supervised labels. In this paper, we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for 3D point clouds in a natural way, using only geometric information, that builds upon prior work utilizing generative adversarial models of point sets. Our method makes use of tools from spectral geometry to separate intrinsic and extrinsic shape information, and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner. We also propose a novel disentanglement penalty that penalizes the predicted change in the latent representation of the output,with respect to the latent variables of the initial shape. We show that the resulting latent representation exhibits intuitive and interpretable behaviour, enabling tasks such as pose transfer that cannot easily be performed by models with an entangled representation."
Geometry Normalization Networks for Accurate Scene Text Detection,"Youjiang Xu, Jiaqi Duan, Zhanghui Kuang, Xiaoyu Yue, Hongbin Sun, Yue Guan, Wayne Zhang",Huazhong University of Science and Technology; SenseTime Research,50.0,china,50.0,China,"Large geometry (e.g., orientation) variances are the key challenges in the scene text detection. In this work, we first conduct experiments to investigate the capacity of networks for learning geometry variances on detecting scene texts, and find that networks can handle only limited text geometry variances. Then, we put forward a novel Geometry Normalization Module (GNM) with multiple branches, each of which is composed of one Scale Normalization Unit and one Orientation Normalization Unit, to normalize each text instance to one desired canonical geometry range through at least one branch. The GNM is general and readily plugged into existing convolutional neural network based text detectors to construct end-to-end Geometry Normalization Networks (GNNets). Moreover, we propose a geometry-aware training scheme to effectively train the GNNets by sampling and augmenting text instances from a uniform geometry variance distribution. Finally, experiments on popular benchmarks of ICDAR 2015 and ICDAR 2017 MLT validate that our method outperforms all the state-of-the-art approaches remarkably by obtaining one-forward test F-scores of 88.52 and 74.54 respectively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Geometry_Normalization_Networks_for_Accurate_Scene_Text_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Geometry_Normalization_Networks_for_Accurate_Scene_Text_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010643/,"['Geometry', 'Training', 'Feature extraction', 'Detectors', 'Benchmark testing', 'Robustness', 'Object detection']","['Optical Character Recognition', 'Scene Text', 'Scene Text Detection', 'Benchmark', 'Convolutional Neural Network', 'Training Strategy', 'Large Geometry', 'Large Variation', 'Feature Maps', 'Object Detection', 'Scale Variation', 'Range Of Scales', 'Bounding Box', 'Affine Transformation', 'Robust Detection', 'Feature Transformation', 'Text Box', 'Single Branch', 'Feasible Range', 'Detection Head', 'Robust Detection Method']",,16,"Large geometry (e.g., orientation) variances are the key challenges in the scene text detection. In this work, we first conduct experiments to investigate the capacity of networks for learning geometry variances on detecting scene texts, and find that networks can handle only limited text geometry variances. Then, we put forward a novel Geometry Normalization Module (GNM) with multiple branches, each of which is composed of one Scale Normalization Unit and one Orientation Normalization Unit, to normalize each text instance to one desired canonical geometry range through at least one branch. The GNM is general and readily plugged into existing convolutional neural network based text detectors to construct end-to-end Geometry Normalization Networks (GNNets). Moreover, we propose a geometry-aware training scheme to effectively train the GNNets by sampling and augmenting text instances from a uniform geometry variance distribution. Finally, experiments on popular benchmarks of ICDAR 2015 and ICDAR 2017 MLT validate that our method outperforms all the state-of-the-art approaches remarkably by obtaining one-forward test F-scores of 88.52 and 74.54 respectively."
Global Feature Guided Local Pooling,Takumi Kobayashi,"National Institute of Advanced Industrial Science and Technology, 1-1-1 Umezono, Tsukuba, Japan",100.0,Japan,0.0,,"In deep convolutional neural networks (CNNs), local pooling operation is a key building block to effectively downsize feature maps for reducing computation cost as well as increasing robustness against input variation. There are several types of pooling operation, such as average/max-pooling, from which one has to be manually selected for building CNNs. The optimal pooling type would be dependent on characteristics of features in CNNs and classification tasks, making it hard to find out the proper pooling module in advance. In this paper, we propose a flexible pooling method which adaptively tunes the pooling functionality based on input features without manually fixing it beforehand. In the proposed method, the parameterized pooling form is derived from a probabilistic perspective to flexibly represent various types of pooling and then the parameters are estimated by means of global statistics in the input feature map. Thus, the proposed local pooling guided by global features effectively works in the CNNs trained in an end-to-end manner. The experimental results on image classification tasks demonstrate the effectiveness of the proposed pooling method in various deep CNNs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kobayashi_Global_Feature_Guided_Local_Pooling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kobayashi_Global_Feature_Guided_Local_Pooling_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009537/,"['Convolution', 'Feature extraction', 'Entropy', 'Convolutional neural networks', 'Robustness', 'Task analysis', 'Probabilistic logic']","['Global Features', 'Local Pool', 'Convolutional Neural Network', 'Characteristic Features', 'Feature Maps', 'Input Features', 'Deep Convolutional Neural Network', 'Pooling Operation', 'Global Statistics', 'Probabilistic Perspective', 'Sigmoid Function', 'Function Of Type', 'Softmax', 'Deeper Layers', 'Large-scale Datasets', 'Receptive Field', 'Multilayer Perceptron', 'Global Information', 'Pooling Layer', 'Average Pooling', 'Pooling Function', 'Maximum Entropy Principle', 'Global Pooling', 'Local Ones', 'Spatial Pooling', 'Local Receptive Field', 'Max-pooling Layer', 'Multilayer Perceptron Model', 'Maximum Principle', 'Global Guidance']",,12,"In deep convolutional neural networks (CNNs), local pooling operation is a key building block to effectively downsize feature maps for reducing computation cost as well as increasing robustness against input variation. There are several types of pooling operation, such as average/max-pooling, from which one has to be manually selected for building CNNs. The optimal pooling type would be dependent on characteristics of features in CNNs and classification tasks, making it hard to find out the proper pooling module in advance. In this paper, we propose a flexible pooling method which adaptively tunes the pooling functionality based on input features without manually fixing it beforehand. In the proposed method, the parameterized pooling form is derived from a probabilistic perspective to flexibly represent various types of pooling and then the parameters are estimated by means of global statistics in the input feature map. Thus, the proposed local pooling guided by global features effectively works in the CNNs trained in an end-to-end manner. The experimental results on image classification tasks demonstrate the effectiveness of the proposed pooling method in various deep CNNs."
Global-Local Temporal Representations for Video Person Re-Identification,"Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, Shiliang Zhang","Huawei Noahs Ark Lab; School of Electronics Engineering and Computer Science, Peking University",50.0,china,50.0,China,"This paper proposes the Global-Local Temporal Representation (GLTR) to exploit the multi-scale temporal cues in video sequences for video person Re-Identification (ReID). GLTR is constructed by first modeling the short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. Specifically, the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestrian. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences. The short and long-term temporal cues are aggregated as the final GLTR by a simple single-stream CNN. GLTR shows substantial superiority to existing features learned with body part cues or metric learning on four widely-used video ReID datasets. For instance, it achieves Rank-1 Accuracy of 87.02% on MARS dataset without re-ranking, better than current state-of-the art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008397/,"['Feature extraction', 'Adaptive optics', 'Optical imaging', 'Three-dimensional displays', 'Optical sensors', 'Video sequences', 'Robustness']","['Temporal Representation', 'Video Person Re-identification', 'Feature Learning', 'Pedestrian', 'Video Sequences', 'Dilated Convolution', 'Metric Learning', 'Adjacent Frames', 'Dilation Rate', 'Temporal Cues', 'Recurrent Neural Network', 'Temporal Features', 'Large-scale Datasets', 'Receptive Field', 'Bounding Box', 'Convolution Kernel', 'Large Margin', 'Average Pooling', 'Optical Flow', 'Mean Average Precision', 'Temporal Convolution', 'Frame Features', 'Spatial-temporal Features', '3D Convolution', 'Temporal Learning', 'Video Features', 'Local Cues', 'Triplet Loss', 'Softmax Loss', 'Spatial Convolution']",,133,"This paper proposes the Global-Local Temporal Representation (GLTR) to exploit the multi-scale temporal cues in video sequences for video person Re-Identification (ReID). GLTR is constructed by first modeling the short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. Specifically, the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestrian. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences. The short and long-term temporal cues are aggregated as the final GLTR by a simple single-stream CNN. GLTR shows substantial superiority to existing features learned with body part cues or metric learning on four widely-used video ReID datasets. For instance, it achieves Rank-1 Accuracy of 87.02% on MARS dataset without re-ranking, better than current state-of-the art."
Goal-Driven Sequential Data Abstraction,"Umar Riaz Muhammad, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Yi-Zhe Song",University of Surrey; University of Edinburgh,100.0,uk,0.0,,"Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Muhammad_Goal-Driven_Sequential_Data_Abstraction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Muhammad_Goal-Driven_Sequential_Data_Abstraction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010398/,"['Gold', 'Task analysis', 'Training', 'Data models', 'Solid modeling', 'Machine learning', 'Computer vision']","['Sequencing Data', 'Text Data', 'Input Sequence', 'Original Input', 'Original Order', 'Contralateral', 'Time Step', 'Learning Rate', 'Convolutional Neural Network', 'Random Selection', 'Multi-label', 'Specific Goals', 'Recognition Accuracy', 'Fully-connected Layer', 'Squirrels', 'Output Sequence', 'Problem Setting', 'Reward Function', 'Gated Recurrent Unit', 'Product Reviews', 'Fixed-length Vector', 'Pool Of Candidates', 'Goal Function', 'Training Categories', 'Parkour', 'Implementation Details', 'Whisker']",,9,"Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains."
GradNet: Gradient-Guided Network for Visual Object Tracking,"Peixia Li, Boyu Chen, Wanli Ouyang, Dong Wang, Xiaoyun Yang, Huchuan Lu","Dalian University of Technology, China; The University of Sydney, Australia; China Science IntelliCloud Technology Co., Ltd",66.66666666666666,"australia, china",33.33333333333334,China,"The fully-convolutional siamese network based on template matching has shown great potentials in visual tracking. During testing, the template is fixed with the initial target feature and the performance totally relies on the general matching ability of the siamese network. However, this manner cannot capture the temporal variations of targets or background clutter. In this work, we propose a novel gradient-guided network to exploit the discriminative information in gradients and update the template in the siamese network through feed-forward and backward operations. To be specific, the algorithm can utilize the information from the gradient to update the template in the current frame. In addition, a template generalization training method is proposed to better use gradient information and avoid overfitting. To our knowledge, this work is the first attempt to exploit the information in the gradient for template update in siamese-based trackers. Extensive experiments on recent benchmarks demonstrate that our method achieves better performance than other state-of-the-art trackers.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_GradNet_Gradient-Guided_Network_for_Visual_Object_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_GradNet_Gradient-Guided_Network_for_Visual_Object_Tracking_ICCV_2019_paper.pdf,,https://github.com/LPXTT/GradNet-Tensorflow,,main,Oral,https://ieeexplore.ieee.org/document/9010014/,"['Target tracking', 'Training', 'Feature extraction', 'Visualization', 'Optimization', 'Clutter', 'Real-time systems']","['Visual Object Tracking', 'Training Methods', 'Target Variable', 'Target Features', 'Current Frame', 'Template Matching', 'Siamese Network', 'Background Clutter', 'Deep Network', 'Convolutional Layers', 'Target Location', 'Bounding Box', 'Image Pairs', 'Online Assessment', 'Gradient-based Optimization', 'Reliability In Sample', 'Previous Frame', 'Update Process', 'Appearance Variations', 'Search Region', 'Score Map', 'Online Update', 'Online Tracking', 'Meta Learning', 'Backward Propagation', 'Template Feature', 'AdaGrad', 'Value Of Map', 'Training Labels']",,246,"The fully-convolutional siamese network based on template matching has shown great potentials in visual tracking. During testing, the template is fixed with the initial target feature and the performance totally relies on the general matching ability of the siamese network. However, this manner cannot capture the temporal variations of targets or background clutter. In this work, we propose a novel gradient-guided network to exploit the discriminative information in gradients and update the template in the siamese network through feed-forward and backward operations. To be specific, the algorithm can utilize the information from the gradient to update the template in the current frame. In addition, a template generalization training method is proposed to better use gradient information and avoid overfitting. To our knowledge, this work is the first attempt to exploit the information in the gradient for template update in siamese-based trackers. Extensive experiments on recent benchmarks demonstrate that our method achieves better performance than other state-of-the-art trackers."
Graph Convolutional Networks for Temporal Action Localization,"Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, Chuang Gan","School of Software Engineering, South China University of Technology, China; MIT-IBM Watson AI Lab; Tencent AI Lab",66.66666666666666,"china, usa",33.33333333333334,China,"Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using GraphConvolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14(49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009541/,,"['Graph Convolutional Network', 'Temporal Localization', 'Graph Convolution', 'Action Localization', 'Temporal Action Localization', 'Contextual Information', 'Action Classes', 'Action Proposals', 'Validation Set', 'Computer Vision', 'Object Detection', 'Edge Weights', 'Information Aggregation', 'Sports Field', 'Action Labels', 'Types Of Edges', 'Neighboring Samples', 'Temporal Boundaries', 'Action Instances', 'Action Detection', 'Proposal Generation', 'Absolute Improvement']",,359,"Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using GraphConvolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14(49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships."
Graph-Based Object Classification for Neuromorphic Vision Sensing,"Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze, Yiannis Andreopoulos","Department of Electronic & Electrical Engineering, University College London, London, U.K.",100.0,uk,0.0,,"Neuromorphic vision sensing (NVS) devices represent visual information as sequences of asynchronous discrete events (a.k.a., ""spikes'"") in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, object classification with NVS streams cannot leverage on state-of-the-art convolutional neural networks (CNNs), since NVS does not produce frame representations. To circumvent this mismatch between sensing and processing with CNNs, we propose a compact graph representation for NVS. We couple this with novel residual graph CNN architectures and show that, when trained on spatio-temporal NVS data for object classification, such residual graph CNNs preserve the spatial and temporal coherence of spike events, while requiring less computation and memory. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we present and make available a 100k dataset of NVS recordings of the American sign language letters, acquired with an iniLabs DAVIS240c device under real-world conditions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Graph-Based_Object_Classification_for_Neuromorphic_Vision_Sensing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Graph-Based_Object_Classification_for_Neuromorphic_Vision_Sensing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010397/,"['Convolution', 'Neuromorphics', 'Sensors', 'Neural networks', 'Training', 'Task analysis', 'Cameras']","['Object Classification', 'Neuromorphic Vision', 'Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Graphical Representation', 'Real-world Datasets', 'Graph Convolutional Network', 'Real-world Conditions', 'Spike Events', 'American Sign Language', 'Residual Convolutional Neural Network', 'Spatial Resolution', 'Convolutional Layers', 'Feature Maps', 'Data Augmentation', 'Kernel Function', 'Deep Convolutional Neural Network', 'Convolution Operation', 'Batch Normalization', 'Conventional Convolutional Neural Networks', 'Graph Convolution', 'Spiking Neural Networks', 'Spatial Convolution', 'Fully-connected Layer', 'Floating-point Operations', 'Pooling Layer', 'Spectral Convolution', 'Nodes In The Graph', 'Residual Block']",,99,"Neuromorphic vision sensing (NVS) devices represent visual information as sequences of asynchronous discrete events (a.k.a., ""spikes'"") in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, object classification with NVS streams cannot leverage on state-of-the-art convolutional neural networks (CNNs), since NVS does not produce frame representations. To circumvent this mismatch between sensing and processing with CNNs, we propose a compact graph representation for NVS. We couple this with novel residual graph CNN architectures and show that, when trained on spatio-temporal NVS data for object classification, such residual graph CNNs preserve the spatial and temporal coherence of spike events, while requiring less computation and memory. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we present and make available a 100k dataset of NVS recordings of the American sign language letters, acquired with an iniLabs DAVIS240c device under real-world conditions."
GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion,"Anh-Duc Nguyen, Seonghwa Choi, Woojae Kim, Sanghoon Lee",Yonsei University,100.0,south korea,0.0,,"In this paper, we present a novel deep method to reconstruct a point cloud of an object from a single still image. Prior arts in the field struggle to reconstruct an accurate and scalable 3D model due to either the inefficient and expensive 3D representations, the dependency between the output and number of model parameters or the lack of a suitable computing operation. We propose to overcome these by deforming a random point cloud to the object shape through two steps: feature blending and deformation. In the first step, the global and point-specific shape features extracted from a 2D object image are blended with the encoded feature of a randomly generated point cloud, and then this mixture is sent to the deformation step to produce the final representative point set of the object. In the deformation process, we introduce a new layer termed as GraphX that considers the inter-relationship between points like common graph convolutions but operates on unordered sets. Moreover, with a simple trick, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments verify that we outperform existing models and halve the state-of-the-art distance score in single image 3D reconstruction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008115/,"['Three-dimensional displays', 'Shape', 'Feature extraction', 'Solid modeling', 'Two dimensional displays', 'Strain', 'Computational modeling']","['Point Cloud', '2D-to-3D Conversion', 'Point Cloud Deformation', 'Single Image', '3D Reconstruction', 'Global Features', 'Image Object', 'Object Shape', 'Still Images', '3D Representation', 'Graph Convolution', 'Global Shape', 'Single 3D', 'Single Reconstruction', 'Point Cloud Generation', 'Unordered Set', 'Semantic', 'Deep Learning', 'Convolutional Neural Network', 'Quantitative Results', 'Intersection Over Union', '3D Shape', 'Style Transfer', 'Fully-connected Layer', 'Set Point', 'Project Characteristics', 'Point Cloud Representation', 'Feature Maps', 'Input Image', 'Deep Convolutional Neural Network']",,25,"In this paper, we present a novel deep method to reconstruct a point cloud of an object from a single still image. Prior arts in the field struggle to reconstruct an accurate and scalable 3D model due to either the inefficient and expensive 3D representations, the dependency between the output and number of model parameters or the lack of a suitable computing operation. We propose to overcome these by deforming a random point cloud to the object shape through two steps: feature blending and deformation. In the first step, the global and point-specific shape features extracted from a 2D object image are blended with the encoded feature of a randomly generated point cloud, and then this mixture is sent to the deformation step to produce the final representative point set of the object. In the deformation process, we introduce a new layer termed as GraphX that considers the inter-relationship between points like common graph convolutions but operates on unordered sets. Moreover, with a simple trick, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments verify that we outperform existing models and halve the state-of-the-art distance score in single image 3D reconstruction."
Gravity as a Reference for Estimating a Personâs Height From Video,"Didier Bieler, Semih GÃ¼nel, Pascal Fua, Helge Rhodin","EPFL, Lausanne, Switzerland; EPFL, Lausanne, Switzerland and UBC, Vancouver, Canada",100.0,switzerland,0.0,,"Estimating the metric height of a person from monocular imagery without additional assumptions is ill-posed. Existing solutions either require manual calibration of ground plane and camera geometry, special cameras, or reference objects of known size. We focus on motion cues and exploit gravity on earth as an omnipresent reference 'object' to translate acceleration, and subsequently height, measured in image-pixels to values in meters. We require videos of motion as input, where gravity is the only external force. This limitation is different to those of existing solutions that recover a person's height and, therefore, our method opens up new application fields. We show theoretically and empirically that a simple motion trajectory analysis suffices to translate from pixel measurements to the person's metric height, reaching a MAE of up to 3.9 cm on jumping motions, and that this works without camera and ground plane calibration.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bieler_Gravity_as_a_Reference_for_Estimating_a_Persons_Height_From_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bieler_Gravity_as_a_Reference_for_Estimating_a_Persons_Height_From_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing,"Xiaohong Liu, Yongrui Ma, Zhihao Shi, Jun Chen",McMaster University,100.0,canada,0.0,,"We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_GridDehazeNet_Attention-Based_Multi-Scale_Network_for_Image_Dehazing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_GridDehazeNet_Attention-Based_Multi-Scale_Network_for_Image_Dehazing_ICCV_2019_paper.pdf,https://proteus1991.github.io/GridDehazeNet/,https://github.com/proteus1991/GridDehazeNet,,main,Poster,https://ieeexplore.ieee.org/document/9010659/,"['Atmospheric modeling', 'Estimation', 'Scattering', 'Image restoration', 'Image color analysis', 'Data models']","['Multi-scale Network', 'Image Dehazing', 'Convolutional Neural Network', 'Synthetic Images', 'Convolutional Neural Network Training', 'Preprocessing Methods', 'Real-world Images', 'Pre-processing Module', 'Convolutional Layers', 'Feature Maps', 'Image Dataset', 'Model Design', 'Attention Mechanism', 'Real-world Datasets', 'Data-driven Methods', 'Clear Image', 'Feature Fusion', 'Attention Weights', 'Perceptual Loss', 'Encoder-decoder Network', 'Multi-scale Convolutional Neural Network', 'Transmission Map', 'Smooth L1 Loss', 'Color Distortion', 'Structural Similarity Values', 'Red Path', 'Diversity Gain', 'White Balance', 'Gamma Correction']",,500,"We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned."
Ground-to-Aerial Image Geo-Localization With a Hard Exemplar Reweighting Triplet Loss,"Sudong Cai, Yulan Guo, Salman Khan, Jiwei Hu, Gongjian Wen",Sun Yat-Sen University; National University of Defense Technology; Inception Institute of Artiﬁcial Intelligence; Wuhan University of Technology,100.0,"China, china, uae",0.0,,"The task of ground-to-aerial image geo-localization can be achieved by matching a ground view query image to a reference database of aerial/satellite images. It is highly challenging due to the dramatic viewpoint changes and unknown orientations. In this paper, we propose a novel in-batch reweighting triplet loss to emphasize the positive effect of hard exemplars during end-to-end training. We also integrate an attention mechanism into our model using feature-level contextual information. To analyze the difficulty level of each triplet, we first enforce a modified logistic regression to triplets with a distance rectifying factor. Then, the reference negative distances for corresponding anchors are set, and the relative weights of triplets are computed by comparing their difficulty to the corresponding references. To reduce the influence of extreme hard data and less useful simple exemplars, the final weights are pruned using upper and lower bound constraints. Experiments on two benchmark datasets show that the proposed approach significantly outperforms the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Ground-to-Aerial_Image_Geo-Localization_With_a_Hard_Exemplar_Reweighting_Triplet_Loss_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Ground-to-Aerial_Image_Geo-Localization_With_a_Hard_Exemplar_Reweighting_Triplet_Loss_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010868,"['Convolution', 'Machine learning', 'Feature extraction', 'Training', 'Buildings', 'Databases', 'Benchmark testing']","['Triplet Loss', 'Image Geo-localization', 'Attention Mechanism', 'Difficulty Level', 'Benchmark Datasets', 'Query Image', 'Deep Learning', 'Feature Maps', 'Image Registration', 'Multilayer Perceptron', 'Discriminative Features', 'Aerial Images', 'Attention Module', 'Spatial Attention', 'Handcrafted Features', 'Element-wise Multiplication', 'Squared Euclidean Distance', 'Channel Attention', 'Sine And Cosine', 'Contrastive Loss', 'CNN Features', 'Siamese Network', 'Auxiliary Loss', 'FC Layer', 'Spatial Attention Map', 'Learning Layer', 'Attention Map', 'Convolution', 'CNN Model', 'Dual Mode']",,89,"The task of ground-to-aerial image geo-localization can be achieved by matching a ground view query image to a reference database of aerial/satellite images. It is highly challenging due to the dramatic viewpoint changes and unknown orientations. In this paper, we propose a novel in-batch reweighting triplet loss to emphasize the positive effect of hard exemplars during end-to-end training. We also integrate an attention mechanism into our model using feature-level contextual information. To analyze the difficulty level of each triplet, we first enforce a modified logistic regression to triplets with a distance rectifying factor. Then, the reference negative distances for corresponding anchors are set, and the relative weights of triplets are computed by comparing their difficulty to the corresponding references. To reduce the influence of extreme hard data and less useful simple exemplars, the final weights are pruned using upper and lower bound constraints. Experiments on two benchmark datasets show that the proposed approach significantly outperforms the state-of-the-art methods."
Grounded Human-Object Interaction Hotspots From Video,"Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman",UT Austin and Facebook AI Research; Facebook AI Research; UT Austin,66.66666666666666,usa,33.33333333333334,USA,"Learning how to interact with objects is an important step towards embodied visual intelligence, but existing techniques suffer from heavy supervision or sensing requirements. We propose an approach to learn human-object interaction ""hotspots"" directly from video. Rather than treat affordances as a manually supervised semantic segmentation task, our approach learns about interactions by watching videos of real human behavior and anticipating afforded actions. Given a novel image or video, our model infers a spatial hotspot map indicating where an object would be manipulated in a potential interaction even if the object is currently at rest. Through results with both first and third person video, we show the value of grounding affordances in real human-object interactions. Not only are our weakly supervised hotspots competitive with strongly supervised affordance methods, but they can also anticipate object interaction for novel object categories. Project page: http://vision.cs.utexas.edu/projects/interaction-hotspots/",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nagarajan_Grounded_Human-Object_Interaction_Hotspots_From_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nagarajan_Grounded_Human-Object_Interaction_Hotspots_From_Video_ICCV_2019_paper.pdf,http://vision.cs.utexas.edu/projects/interaction-hotspots/,,,main,Poster,https://ieeexplore.ieee.org/document/9009479/,"['Training', 'Task analysis', 'Visualization', 'Three-dimensional displays', 'Predictive models', 'Facebook']","['Human-object Interaction', 'Real Interactions', 'Object Interaction', 'Video Of A Person', 'Objective Function', 'Long Short-term Memory', 'Image Object', 'Video Clips', 'Object Classification', 'Activation Maps', 'Understanding Of Activity', 'Action Recognition', 'Action Classes', 'Familiar Objects', 'Important Note', 'Training Videos', 'Action Labels', 'Image Embedding', 'Object Affordances', 'Videos Of People', 'Unfamiliar Objects', 'Affordance Theory', 'Coffee Machine', 'Action Recognition Model', 'Manual Annotation', 'Computer Vision', 'Object Labels', 'Object Trajectory']",,86,"Learning how to interact with objects is an important step towards embodied visual intelligence, but existing techniques suffer from heavy supervision or sensing requirements. We propose an approach to learn human-object interaction ""hotspots"" directly from video. Rather than treat affordances as a manually supervised semantic segmentation task, our approach learns about interactions by watching videos of real human behavior and anticipating afforded actions. Given a novel image or video, our model infers a spatial hotspot map indicating where an object would be manipulated in a potential interaction even if the object is currently at rest. Through results with both first and third person video, we show the value of grounding affordances in real human-object interactions. Not only are our weakly supervised hotspots competitive with strongly supervised affordance methods, but they can also anticipate object interaction for novel object categories. Project page: http://vision.cs.utexas.edu/projects/interaction-hotspots/."
Group-Wise Deep Object Co-Segmentation With Co-Attention Recurrent Neural Network,"Bo Li, Zhengxing Sun, Qian Li, Yunjie Wu, Anqi Hu",Nanjing University,100.0,china,0.0,,"Effective feature representations which should not only express the images individual properties, but also reflect the interaction among group images are essentially crucial for real-world co-segmentation. This paper proposes a novel end-to-end deep learning approach for group-wise object co-segmentation with a recurrent network architecture. Specifically, the semantic features extracted from a pre-trained CNN of each image are first processed by single image representation branch to learn the unique properties. Meanwhile, a specially designed Co-Attention Recurrent Unit (CARU) recurrently explores all images to generate the final group representation by using the co-attention between images, and simultaneously suppresses noisy information. The group feature which contains synergetic information is broadcasted to each individual image and fused with multi-scale fine-resolution features to facilitate the inferring of co-segmentation. Moreover, we propose a groupwise training objective to utilize the co-object similarity and figure-ground distinctness as the additional supervision. The whole modules are collaboratively optimized in an end-to-end manner, further improving the robustness of the approach. Comprehensive experiments on three benchmarks can demonstrate the superiority of our approach in comparison with the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Group-Wise_Deep_Object_Co-Segmentation_With_Co-Attention_Recurrent_Neural_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Group-Wise_Deep_Object_Co-Segmentation_With_Co-Attention_Recurrent_Neural_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009517/,"['Image segmentation', 'Feature extraction', 'Robustness', 'Training', 'Machine learning', 'Semantics', 'Computer vision']","['Neural Network', 'Recurrent Neural Network', 'Deep Learning', 'Convolutional Neural Network', 'Single Image', 'Representative Group', 'Individual Attributes', 'Individual Images', 'Semantic Features', 'Single Instance', 'Multi-scale Features', 'Recurrent Unit', 'Single Branch', 'Pre-trained Convolutional Neural Network', 'Recurrent Architecture', 'Number Of Images', 'Intersection Over Union', 'Image Pairs', 'Deep Features', 'Common Objects', 'Group Of Images', 'PASCAL VOC Dataset', 'Object Segmentation', 'Attention Map', 'Softplus', 'Triplet Loss', 'Reset Gate', 'Update Gate', 'Background Clutter', 'COCO Dataset']",,41,"Effective feature representations which should not only express the images individual properties, but also reﬂect the interaction among group images are essentially crucial for real-world co-segmentation. This paper proposes a novel end-to-end deep learning approach for group-wise object co-segmentation with a recurrent network architecture. Speciﬁcally, the semantic features extracted from a pre-trained CNN of each image are ﬁrst processed by single image representation branch to learn the unique properties. Meanwhile, a specially designed Co-Attention Recurrent Unit (CARU) recurrently explores all images to generate the ﬁnal group representation by using the co-attention between images, and simultaneously suppresses noisy information. The group feature which contains synergetic information is broadcasted to each individual image and fused with multi-scale ﬁne-resolution features to facilitate the inferring of co-segmentation. Moreover, we propose a groupwise training objective to utilize the co-object similarity and ﬁgure-ground distinctness as the additional supervision. The whole modules are collaboratively optimized in an end-to-end manner, further improving the robustness of the approach. Comprehensive experiments on three benchmarks can demonstrate the superiority of our approach in comparison with the state-of-the-art methods."
Grouped Spatial-Temporal Aggregation for Efficient Action Recognition,"Chenxu Luo, Alan L. Yuille","Department of Computer Science, The Johns Hopkins University, Baltimore, MD 21218, USA",100.0,usa,0.0,,"Temporal reasoning is an important aspect of video analysis. 3D CNN shows good performance by exploring spatial-temporal features jointly in an unconstrained way, but it also increases the computational cost a lot. Previous works try to reduce the complexity by decoupling the spatial and temporal filters. In this paper, we propose a novel decomposition method that decomposes the feature channels into spatial and temporal groups in parallel. This decomposition can make two groups focus on static and dynamic cues separately. We call this grouped spatial-temporal aggregation (GST). This decomposition is more parameter-efficient and enables us to quantitatively analyze the contributions of spatial and temporal features in different layers. We verify our model on several action recognition tasks that require temporal reasoning and show its effectiveness.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Grouped_Spatial-Temporal_Aggregation_for_Efficient_Action_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Grouped_Spatial-Temporal_Aggregation_for_Efficient_Action_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009790/,"['Three-dimensional displays', 'Two dimensional displays', 'Kernel', 'Computational modeling', 'Cognition', 'Task analysis', 'Computational efficiency']","['Action Recognition', 'Computational Cost', 'Spatial Features', 'Temporal Features', 'Spatial-temporal Features', 'Action Recognition Task', 'Validation Set', 'Spatial Information', 'Dynamic Characteristics', 'Temporal Information', 'High-level Features', 'Single Frame', 'Optical Flow', 'Input Channels', 'Output Channels', 'Temporal Model', 'Channel Capacity', 'Video Dataset', '3D Convolution', 'Temporal Cues', 'Group Convolution', 'Spatial Part', 'BN Layer', 'Static Information', 'V2 Dataset', 'Spatial Path', 'Late Fusion', 'Motion Features', 'Channel Dimension', 'Action Recognition Datasets']",,110,"Temporal reasoning is an important aspect of video analysis. 3D CNN shows good performance by exploring spatial-temporal features jointly in an unconstrained way, but it also increases the computational cost a lot. Previous works try to reduce the complexity by decoupling the spatial and temporal filters. In this paper, we propose a novel decomposition method that decomposes the feature channels into spatial and temporal groups in parallel. This decomposition can make two groups focus on static and dynamic cues separately. We call this grouped spatial-temporal aggregation (GST). This decomposition is more parameter-efficient and enables us to quantitatively analyze the contributions of spatial and temporal features in different layers. We verify our model on several action recognition tasks that require temporal reasoning and show its effectiveness."
Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks,"Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll",Technical University of Munich; fortiss GmbH,50.0,germany,50.0,Germany,"We consider adversarial examples for image classification in the black-box decision-based setting. Here, an attacker cannot access confidence scores, but only the final label. Most attacks for this scenario are either unreliable or inefficient. Focusing on the latter, we show that a specific class of attacks, Boundary Attacks, can be reinterpreted as a biased sampling framework that gains efficiency from domain knowledge. We identify three such biases, image frequency, regional masks and surrogate gradients, and evaluate their performance against an ImageNet classifier. We show that the combination of these biases outperforms the state of the art by a wide margin. We also showcase an efficient way to attack the Google Cloud Vision API, where we craft convincing perturbations with just a few hundred queries. Finally, the methods we propose have also been found to work very well against strong defenses: Our targeted attack won second place in the NeurIPS 2018 Adversarial Vision Challenge.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Brunner_Guessing_Smart_Biased_Sampling_for_Efficient_Black-Box_Adversarial_Attacks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Brunner_Guessing_Smart_Biased_Sampling_for_Efficient_Black-Box_Adversarial_Attacks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008375/,"['Perturbation methods', 'Robustness', 'Computational modeling', 'Cats', 'Transforms', 'Estimation', 'Training']","['Adversarial Attacks', 'Cognitive Domains', 'State Of The Art', 'Adversarial Examples', 'ImageNet Classification', 'Hyperparameters', 'Alternative Models', 'Pedestrian', 'Parametrized', 'Decision Boundary', 'Noise Patterns', 'Projected Gradient Descent', 'White-box Attack', 'Black-box Attacks', 'Fast Gradient Sign Method']",,58,"We consider adversarial examples for image classification in the black-box decision-based setting. Here, an attacker cannot access confidence scores, but only the final label. Most attacks for this scenario are either unreliable or inefficient. Focusing on the latter, we show that a specific class of attacks, Boundary Attacks, can be reinterpreted as a biased sampling framework that gains efficiency from domain knowledge. We identify three such biases, image frequency, regional masks and surrogate gradients, and evaluate their performance against an ImageNet classifier. We show that the combination of these biases outperforms the state of the art by a wide margin. We also showcase an efficient way to attack the Google Cloud Vision API, where we craft convincing perturbations with just a few hundred queries. Finally, the methods we propose have also been found to work very well against strong defenses: Our targeted attack won second place in the NeurIPS 2018 Adversarial Vision Challenge."
Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation,"Christos Sakaridis, Dengxin Dai, Luc Van Gool","ETH Zürich; ETH Zürich, KU Leuven",100.0,"Switzerland, belgium",0.0,,"Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sakaridis_Guided_Curriculum_Model_Adaptation_and_Uncertainty-Aware_Evaluation_for_Semantic_Nighttime_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sakaridis_Guided_Curriculum_Model_Adaptation_and_Uncertainty-Aware_Evaluation_for_Semantic_Nighttime_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9011028/,"['Semantics', 'Adaptation models', 'Image segmentation', 'Task analysis', 'Measurement', 'Lighting', 'Uncertainty']","['Semantic Segmentation', 'Night-time Images', 'Adverse Conditions', 'Evaluation Framework', 'Semantic Segmentation Models', 'Time Of Day', 'Single Step', 'Model Domain', 'Ground Truth Labels', 'Target Domain', 'Unlabeled Data', 'Domain Adaptation', 'Prediction Confidence', 'Semantic Content', 'Semantic Labels', 'Illumination Changes', 'Indiscernible', 'Dark Images', 'Camera Pose', 'Style Image', 'Privileged Information', 'Semantic Annotation', 'Refinement Approach', 'Valid Pixels', 'Unlabeled Set', 'Label Of Pixel', 'Confidence Threshold', 'Dark Level', 'Correct Predictions']",,160,"Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs."
Guided Image-to-Image Translation With Bi-Directional Feature Transformation,"Badour AlBahar, Jia-Bin Huang",Virginia Tech,100.0,usa,0.0,,"We address the problem of guided image-to-image translation where we translate an input image into another while respecting the constraints provided by an external, user-provided guidance image. Various types of conditioning mechanisms for leveraging the given guidance image have been explored, including input concatenation, feature concatenation, and conditional affine transformation of feature activations. All these conditioning mechanisms, however, are uni-directional, i.e., no information flow from the input image back to the guidance. To better utilize the constraints of the guidance image, we present a bi-directional feature transformation (bFT) scheme. We show that our novel bFT scheme outperforms other conditioning schemes and has comparable results to state-of-the-art methods on different tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/AlBahar_Guided_Image-to-Image_Translation_With_Bi-Directional_Feature_Transformation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/AlBahar_Guided_Image-to-Image_Translation_With_Bi-Directional_Feature_Transformation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009818/,"['Bidirectional control', 'Task analysis', 'Image generation', 'Gallium nitride', 'Image color analysis', 'Visualization', 'Generators']","['Feature Transformation', 'Bidirectional Transformer', 'Information Flow', 'Input Image', 'Input Features', 'Affine Transformation', 'Image Guidance', 'Feature Concatenation', 'Objective Function', 'Feature Representation', 'User Study', 'Generative Adversarial Networks', 'Normalization Layer', 'Depth Map', 'Source Images', 'Image Synthesis', 'U-Net Architecture', 'Person Image', 'Conditional Generative Adversarial Network', 'Transfer Task', 'Fréchet Inception Distance', 'Guidance Signaling', 'Style Transfer', 'Visual Reasoning', 'Bidirectional Approach', 'Instance Normalization', 'ResNet Architecture', 'Translation Task', 'Implementation Details', 'Input Levels']",,79,"We address the problem of guided image-to-image translation where we translate an input image into another while respecting the constraints provided by an external, user-provided guidance image. Various types of conditioning mechanisms for leveraging the given guidance image have been explored, including input concatenation, feature concatenation, and conditional affine transformation of feature activations. All these conditioning mechanisms, however, are uni-directional, i.e., no information flow from the input image back to the guidance. To better utilize the constraints of the guidance image, we present a bi-directional feature transformation (bFT) scheme. We show that our novel bFT scheme outperforms other conditioning schemes and has comparable results to state-of-the-art methods on different tasks."
Guided Super-Resolution As Pixel-to-Pixel Transformation,"Riccardo de Lutio, Stefano D'Aronco, Jan Dirk Wegner, Konrad Schindler","EcoVision Lab, Photogrammetry and Remote Sensing, ETH Zürich",100.0,Switzerland,0.0,,"Guided super-resolution is a unifying framework for several computer vision tasks where the inputs are a low-resolution source image of some target quantity (e.g., perspective depth acquired with a time-of-flight camera) and a high-resolution guide image from a different domain (e.g., a grey-scale image from a conventional camera); and the target output is a high-resolution version of the source (in our example, a high-res depth map). The standard way of looking at this problem is to formulate it as a super-resolution task, i.e., the source image is upsampled to the target resolution, while transferring the missing high-frequency details from the guide. Here, we propose to turn that interpretation on its head and instead see it as a pixel-to-pixel mapping of the guide image to the domain of the source image. The pixel-wise mapping is parametrised as a multi-layer perceptron, whose weights are learned by minimising the discrepancies between the source image and the downsampled target image. Importantly, our formulation makes it possible to regularise only the mapping function, while avoiding regularisation of the outputs; thus producing crisp, natural-looking images. The proposed method is unsupervised, using only the specific source and guide images to fit the mapping. We evaluate our method on two different tasks, super-resolution of depth maps and of tree height maps. In both cases, we clearly outperform recent baselines in quantitative comparisons, while delivering visually much sharper outputs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/de_Lutio_Guided_Super-Resolution_As_Pixel-to-Pixel_Transformation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/de_Lutio_Guided_Super-Resolution_As_Pixel-to-Pixel_Transformation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010307/,"['Spatial resolution', 'Task analysis', 'Cameras', 'Standards', 'Computer vision', 'Vegetation']","['Multilayer Perceptron', 'Target Image', 'Depth Map', 'Source Images', 'Tree Height', 'Low-resolution Images', 'Target Output', 'Target Resolution', 'Conventional Camera', 'Height Map', 'Time-of-flight Sensors', 'Training Data', 'Fine Structure', 'Semantic Segmentation', 'Mean Square Error Values', 'Error Metrics', 'Original Implementation', 'Vegetation Height', 'Bicubic Interpolation', 'Lower Mean Square Error', 'Bilateral Filter', 'Guided Filter', 'Nearby Pixels', 'Lot Of Details']",,51,"Guided super-resolution is a unifying framework for several computer vision tasks where the inputs are a low-resolution source image of some target quantity (e.g., perspective depth acquired with a time-of-flight camera) and a high-resolution guide image from a different domain (e.g., a grey-scale image from a conventional camera); and the target output is a high-resolution version of the source (in our example, a high-res depth map). The standard way of looking at this problem is to formulate it as a super-resolution task, i.e., the source image is upsampled to the target resolution, while transferring the missing high-frequency details from the guide. Here, we propose to turn that interpretation on its head and instead see it as a pixel-to-pixel mapping of the guide image to the domain of the source image. The pixel-wise mapping is parametrised as a multi-layer perceptron, whose weights are learned by minimising the discrepancies between the source image and the downsampled target image. Importantly, our formulation makes it possible to regularise only the mapping function, while avoiding regularisation of the outputs; thus producing crisp, natural-looking images. The proposed method is unsupervised, using only the specific source and guide images to fit the mapping. We evaluate our method on two different tasks, super-resolution of depth maps and of tree height maps. In both cases, we clearly outperform recent baselines in quantitative comparisons, while delivering visually much sharper outputs."
HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization,"Hang Zhao, Antonio Torralba, Lorenzo Torresani, Zhicheng Yan",Massachusetts Institute of Technology; Dartmouth College; University of Illinois at Urbana-Champaign,100.0,"USA, usa",0.0,,"This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large-scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transfer learning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1M as a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_HACS_Human_Action_Clips_and_Segments_Dataset_for_Recognition_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_HACS_Human_Action_Clips_and_Segments_Dataset_for_Recognition_and_ICCV_2019_paper.pdf,http://hacs.csail.mit.edu,,,main,Poster,https://ieeexplore.ieee.org/document/9009030/,"['Videos', 'Benchmark testing', 'Training', 'Sampling methods', 'Spatiotemporal phenomena', 'Kinetic theory', 'YouTube']","['Human Activities', 'Temporal Localization', 'Recognition Dataset', 'Benchmark', 'Feature Learning', 'Transfer Learning', 'Large-scale Datasets', 'Action Recognition', 'Action Classes', 'Target Dataset', 'Training Set', 'Random Sampling', 'Validation Set', 'Image Classification', 'Negative Samples', 'Actual Context', 'Optical Flow', 'Scale Datasets', 'Sparse Sampling', 'Video Segments', 'Training Deep Models', 'Part Of The Video', 'Annotation Guidelines', 'Scale Growth', 'Detailed Guidelines', 'Video Retrieval', 'Action Labels', 'Action Recognition Datasets', 'Input RGB', 'Google Images']",,144,"This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large-scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transfer learning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1M as a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations."
HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision,"Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer","University of California, Berkeley",100.0,usa,0.0,,"Model size and inference speed/power have become a major challenge in the deployment of neural networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra-low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with 8x activation compression ratio on ResNet20, as compared to DNAS, and up to 1% higher accuracy with up to 14% smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant and HAQ. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above 68% top1 accuracy on ImageNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_HAWQ_Hessian_AWare_Quantization_of_Neural_Networks_With_Mixed-Precision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_HAWQ_Hessian_AWare_Quantization_of_Neural_Networks_With_Mixed-Precision_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009512/,"['Quantization (signal)', 'Artificial neural networks', 'Computational modeling', 'Eigenvalues and eigenfunctions', 'Sensitivity', 'Image resolution']","['Neural Network', 'Model Size', 'ImageNet', 'Precise Quantification', 'Compression Ratio', 'Top-1 Accuracy', 'Brute-force Approach', 'Eigenvalues', 'Convolutional Network', 'Artificial Neural Network', 'High Precision', 'Network Layer', 'Large Model', 'Random Vector', 'Quantification Results', 'Hessian Matrix', 'Small Footprint', 'Least Significant Bit', 'Memory Footprint', 'Accuracy Drop', 'Second-order Information', 'Network Block', 'Minimum Description Length', 'Single Precision', 'Uniform Quantization', 'SqueezeNet', 'Notable Work', 'Most Significant Bit', 'Input Resolution', 'Sensitivity Metrics']",,217,"Model size and inference speed/power have become a major challenge in the deployment of neural networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra-low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with 8× activation compression ratio on ResNet20, as compared to DNAS, and up to 1% higher accuracy with up to 14% smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant and HAQ. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above 68% top1 accuracy on ImageNet."
HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions,"Duo Li, Aojun Zhou, Anbang Yao",Intel Labs China,0.0,,100.0,USA,"MobileNets, a class of top-performing convolutional neural network architectures in terms of accuracy and efficiency trade-off, are increasingly used in many resource-aware vision applications. In this paper, we present Harmonious Bottleneck on two Orthogonal dimensions (HBO), a novel architecture unit, specially tailored to boost the accuracy of extremely lightweight MobileNets at the level of less than 40 MFLOPs. Unlike existing bottleneck designs that mainly focus on exploring the interdependencies among the channels of either groupwise or depthwise convolutional features, our HBO improves bottleneck representation while maintaining similar complexity via jointly encoding the feature interdependencies across both spatial and channel dimensions. It has two reciprocal components, namely spatial contraction-expansion and channel expansion-contraction, nested in a bilaterally symmetric structure. The combination of two interdependent transformations performing on orthogonal dimensions of feature maps enhances the representation and generalization ability of our proposed module, guaranteeing compelling performance with limited computational resource and power. By replacing the original bottlenecks in MobileNetV2 backbone with HBO modules, we construct HBONets which are evaluated on ImageNet classification, PASCAL VOC object detection and Market-1501 person re-identification. Extensive experiments show that with the severe constraint of computational budget our models outperform MobileNetV2 counterparts by remarkable margins of at most 6.6%, 6.3% and 5.0% on the above benchmarks respectively. Code and pretrained models are available at https://github.com/d-li14/HBONet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_HBONet_Harmonious_Bottleneck_on_Two_Orthogonal_Dimensions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_HBONet_Harmonious_Bottleneck_on_Two_Orthogonal_Dimensions_ICCV_2019_paper.pdf,,https://github.com/d-li14/HBONet,,main,Poster,https://ieeexplore.ieee.org/document/9011007/,"['Neural networks', 'Computer architecture', 'Standards', 'Computational modeling', 'Computational efficiency', 'Task analysis', 'Tensile stress']","['Orthogonal Dimensions', 'Benchmark', 'Neural Network', 'Convolutional Neural Network', 'Feature Maps', 'Computational Resources', 'Object Detection', 'Spatial Dimensions', 'Convolutional Neural Network Architecture', 'Channel Dimension', 'Similar Complexity', 'Representation Ability', 'Depthwise Convolution', 'ImageNet Classification', 'Computational Cost', 'Learning Rate', 'Building Blocks', 'Deep Neural Network', 'Convolutional Layers', 'Image Classification', 'Depthwise Separable Convolution', 'Standard Convolution', 'Single Shot Detector', 'Group Convolution', 'Pointwise Convolution', 'Lightweight Convolutional Neural Network', 'Image Dataset', 'Modular Design', 'Spatial Operation', 'Lightweight Architecture']",,22,"MobileNets, a class of top-performing convolutional neural network architectures in terms of accuracy and efficiency trade-off, are increasingly used in many resource-aware vision applications. In this paper, we present Harmonious Bottleneck on two Orthogonal dimensions (HBO), a novel architecture unit, specially tailored to boost the accuracy of extremely lightweight MobileNets at the level of less than 40 MFLOPs. Unlike existing bottleneck designs that mainly focus on exploring the interdependencies among the channels of either groupwise or depthwise convolutional features, our HBO improves bottleneck representation while maintaining similar complexity via jointly encoding the feature interdependencies across both spatial and channel dimensions. It has two reciprocal components, namely spatial contraction-expansion and channel expansion-contraction, nested in a bilaterally symmetric structure. The combination of two interdependent transformations performing on orthogonal dimensions of feature maps enhances the representation and generalization ability of our proposed module, guaranteeing compelling performance with limited computational resource and power. By replacing the original bottlenecks in MobileNetV2 backbone with HBO modules, we construct HBONets which are evaluated on ImageNet classification, PASCAL VOC object detection and Market-1501 person re-identification. Extensive experiments show that with the severe constraint of computational budget our models outperform MobileNetV2 counterparts by remarkable margins of at most 6.6%, 6.3% and 5.0% on the above benchmarks respectively. Code and pretrained models are available at https://github.com/d-li14/HBONet."
HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation,"Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu","South China University of Technology; Shenzhen Cloudream Technology Co., Ltd.; The Chinese University of Hong Kong (Shenzhen)",66.66666666666666,"Hong Kong, china",33.33333333333334,China,"Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network(ConvNet) is first trained to predict HEMlests from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (by 20% on Human3.6M). The proposed method naturally supports training with ""in-the-wild"" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_HEMlets_Pose_Learning_Part-Centric_Heatmap_Triplets_for_Accurate_3D_Human_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_HEMlets_Pose_Learning_Part-Centric_Heatmap_Triplets_for_Accurate_3D_Human_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010743/,"['Three-dimensional displays', 'Two dimensional displays', 'Pose estimation', 'Task analysis', 'Space heating', 'Training']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Convolutional Network', 'Convolutional Neural Network', 'Challenging Task', 'Single Image', 'Related Information', '3D Space', 'Joint Position', 'Relative Depth', '3D Pose', 'Training Data', '2D Images', 'Quantization Error', 'Training Loss', '3D Coordinates', 'Domain Adaptation', '3D Joint', '3D Datasets', '2D Pose', 'Intermediate Representation', 'Standard Benchmark', '3D Information', 'Pose Estimation Methods']",,73,"Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network(ConvNet) is first trained to predict HEMlests from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (by 20% on Human3.6M). The proposed method naturally supports training with ""in-the-wild'' images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images."
Habitat: A Platform for Embodied AI Research,"Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra",Facebook AI Research; Georgia Institute of Technology; Facebook AI Research; Facebook Reality Labs; Georgia Institute of Technology; Facebook AI Research; UC Berkeley; Facebook AI Research; Simon Fraser University; Intel Labs; Facebook AI Research; Facebook Reality Labs,33.33333333333333,"canada, usa",66.66666666666667,USA,"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments  train, test  x  Matterport3D, Gibson  for multiple sensors  blind, RGB, RGBD, D  and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/habitat,,main,Oral,https://ieeexplore.ieee.org/document/9010745/,"['Sensors', 'Three-dimensional displays', 'Artificial intelligence', 'Task analysis', 'Navigation', 'Training', 'Robots']","['Question Answering', 'Depth Camera', '3D Simulation', '3D Datasets', 'Single GPU', 'Shortest Path', 'Simulation Environment', 'Visual Input', 'Start Position', '3D Environment', 'Progress In This Area', 'Vision Sensors', 'Geodesic Distance', '3D Scene', 'Learning Agent', 'Supplement For Details', 'Goal Position', 'Trained Agent', 'Scene Dataset', 'Task Definition', 'Scene Graph', 'Software Stack', 'Sensor Suite', 'Collision Dynamics', 'Forward Activity', 'Classical Methods', 'Machine Vision', 'Image Synthesis', 'Computer Vision', 'Indoor Environments']",,554,"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI."
Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition With CNNs,"Lei Wang, Piotr Koniusz, Du Q. Huynh","University of Western Australia; Data61/CSIRO, Australian National University",100.0,Australia,0.0,,"In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Hallucinating_IDT_Descriptors_and_I3D_Optical_Flow_Features_for_Action_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Hallucinating_IDT_Descriptors_and_I3D_Optical_Flow_Features_for_Action_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008573/,"['Optical imaging', 'Detectors', 'Pipelines', 'Nonlinear optics', 'Trajectory', 'Encoding', 'Training']","['Hallucinations', 'Optical Characteristics', 'Action Recognition', 'Optical Flow', 'Optical Flow Features', 'Improved Dense Trajectory', 'Training Stage', 'Contralateral', 'State Of The Art', 'Fully-connected Layer', 'Average Pooling', 'Multi-task Learning', 'Local Descriptors', 'Human Detection', 'Global Descriptors', 'Camera Motion', 'Backward Pass', 'Histogram Of Oriented Gradients', 'MSE Loss', 'Two-stream Network', 'Burstiness']",,68,"In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets."
HarDNet: A Low Memory Traffic Network,"Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang Huang, Youn-Long Lin","National Tsing Hua University; National Tsing Hua University, University of Michigan",100.0,"taiwan, usa",0.0,,"State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chao_HarDNet_A_Low_Memory_Traffic_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chao_HarDNet_A_Low_Memory_Traffic_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010717/,"['Random access memory', 'Harmonic analysis', 'Tensile stress', 'Neural networks', 'Task analysis', 'Computer architecture', 'Power demand']","['Neural Network', 'Feature Maps', 'Object Detection', 'Model Size', 'Semantic Segmentation', 'Inference Time', 'Dense Connections', 'Intermediate Feature Maps', 'Real-time Object Detection', 'Contralateral', 'Convolutional Neural Network', 'Computational Efficiency', 'Convolutional Layers', 'Power Consumption', 'Convolutional Neural Network Model', 'Convolutional Neural Network Architecture', 'Input Channels', 'Output Channels', 'Transition Layer', 'PASCAL VOC Dataset', 'Bottleneck Layer', 'Single Shot Detector', 'MS COCO Dataset', 'Input Tensor', 'Depthwise Convolution', 'Pointwise Convolution', 'VOC Dataset', 'COCO Dataset', 'Max-pooling']",,217,"State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge."
HiPPI: Higher-Order Projected Power Iterations for Scalable Multi-Matching,"Florian Bernard, Johan Thunberg, Paul Swoboda, Christian Theobalt","Halmstad University; MPI Informatics, Saarland Informatics Campus",100.0,"germany, sweden",0.0,,"The matching of multiple objects (e.g. shapes or images) is a fundamental problem in vision and graphics. In order to robustly handle ambiguities, noise and repetitive patterns in challenging real-world settings, it is essential to take geometric consistency between points into account. Computationally, the multi-matching problem is difficult. It can be phrased as simultaneously solving multiple (NP-hard) quadratic assignment problems (QAPs) that are coupled via cycle-consistency constraints. The main limitations of existing multi-matching methods are that they either ignore geometric consistency and thus have limited robustness, or they are restricted to small-scale problems due to their (relatively) high computational cost. We address these shortcomings by introducing a Higher-order Projected Power Iteration method, which is (i) efficient and scales to tens of thousands of points, (ii) straightforward to implement, (iii) able to incorporate geometric consistency, (iv) guarantees cycle-consistent multi-matchings, and (iv) comes with theoretical convergence guarantees. Experimentally we show that our approach is superior to existing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bernard_HiPPI_Higher-Order_Projected_Power_Iterations_for_Scalable_Multi-Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bernard_HiPPI_Higher-Order_Projected_Power_Iterations_for_Scalable_Multi-Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010401/,"['Synchronization', 'Matrix decomposition', 'Shape', 'Runtime', 'Robustness', 'Computational modeling', 'Sparse matrices']","['Repetitive Patterns', 'Visual Problems', 'Quadratic Problem', 'Assignment Problem', 'Thousands Of Points', 'Geometric Consistency', 'Quadratic Function', 'Similarity Score', 'Linear Problem', 'Feature Points', 'Pair Of Points', 'Geometric Relationship', 'Positive Semidefinite', 'Biquadratic', 'Matching Problem', 'Geodesic Distance', 'Semidefinite Programming', 'Object Pairs', 'Partial Match', 'Second-order Terms', 'Pairwise Matching', 'Graph Matching', 'Matching Cost', 'Permutation Matrix', 'Quadratic Objective', 'Synchronization Method', 'Matching Score', 'Perturbation Theory', 'Collection Of Objects', 'Eigenvectors']",,19,"The matching of multiple objects (e.g. shapes or images) is a fundamental problem in vision and graphics. In order to robustly handle ambiguities, noise and repetitive patterns in challenging real-world settings, it is essential to take geometric consistency between points into account. Computationally, the multi-matching problem is difficult. It can be phrased as simultaneously solving multiple (NP-hard) quadratic assignment problems (QAPs) that are coupled via cycle-consistency constraints. The main limitations of existing multi-matching methods are that they either ignore geometric consistency and thus have limited robustness, or they are restricted to small-scale problems due to their (relatively) high computational cost. We address these shortcomings by introducing a Higher-order Projected Power Iteration method, which is (i) efficient and scales to tens of thousands of points, (ii) straightforward to implement, (iii) able to incorporate geometric consistency, (iv) guarantees cycle-consistent multi-matchings, and (iv) comes with theoretical convergence guarantees. Experimentally we show that our approach is superior to existing methods."
Hiding Video in Audio via Reversible Generative Models,"Hyukryul Yang, Hao Ouyang, Vladlen Koltun, Qifeng Chen",Intel Labs; HKUST,50.0,hong kong,50.0,USA,"We present a method for hiding video content inside audio files while preserving the perceptual fidelity of the cover audio. This is a form of cross-modal steganography and is particularly challenging due to the high bitrate of video. Our scheme uses recent advances in flow-based generative models, which enable mapping audio to latent codes such that nearby codes correspond to perceptually similar signals. We show that compressed video data can be concealed in the latent codes of audio sequences while preserving the fidelity of both the hidden video and the cover audio. We can embed 128x128 video inside same-duration audio, or higher-resolution video inside longer audio sequences. Quantitative experiments show that our approach outperforms relevant baselines in steganographic capacity and fidelity.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Hiding_Video_in_Audio_via_Reversible_Generative_Models_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Hiding_Video_in_Audio_via_Reversible_Generative_Models_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008510/,"['Image reconstruction', 'Binary codes', 'Image coding', 'Gold', 'Robustness', 'Convolutional codes', 'Decoding']","['Audio Files', 'Latent Code', 'Decoding', 'Latent Variables', 'User Study', 'Generative Adversarial Networks', 'Video Frames', 'Jacobian Matrix', 'Channel Dimension', 'Binary Code', 'Variational Autoencoder', 'Human Visual System', 'Least Significant Bit', 'COCO Dataset', 'Floating-point Numbers', 'Deep Generative Models', 'Secret Information', 'Audio Clips', 'Code Size', 'Human Auditory System', 'Substitution Rule', 'Binary Encoding', 'Reconstructed Image Quality']",,8,"We present a method for hiding video content inside audio files while preserving the perceptual fidelity of the cover audio. This is a form of cross-modal steganography and is particularly challenging due to the high bitrate of video. Our scheme uses recent advances in flow-based generative models, which enable mapping audio to latent codes such that nearby codes correspond to perceptually similar signals. We show that compressed video data can be concealed in the latent codes of audio sequences while preserving the fidelity of both the hidden video and the cover audio. We can embed 128x128 video inside same-duration audio, or higher-resolution video inside longer audio sequences. Quantitative experiments show that our approach outperforms relevant baselines in steganographic capacity and fidelity."
Hierarchical Encoding of Sequential Data With Compact and Sub-Linear Storage Cost,"Huu Le, Ming Xu, Tuan Hoang, Michael Milford","Queensland University of Technology (QUT), Australia; Singapore University of Technology and Design (SUTD), Singapore",100.0,"australia, singapore",0.0,,"Snapshot-based visual localization is an important problem in several computer vision and robotics applications such as Simultaneous Localization And Mapping (SLAM). To achieve real-time performance in very large-scale environments with massive amounts of training and map data, techniques such as approximate nearest neighbor search (ANN) algorithms are used. While several state-of-the-art variants of quantization and indexing techniques have demonstrated to be efficient in practice, their theoretical memory cost still scales at least linearly with the training data (i.e., O(n) where n is the number of instances in the database), since each data point must be associated with at least one code vector. To address these limitations, in this paper we present a totally new hierarchical encoding approach that enables a sub-linear storage scale. The algorithm exploits the widespread sequential nature of sensor information streams in robotics and autonomous vehicle applications and achieves, both theoretically and experimentally, sub-linear scalability in storage required for a given environment size. Furthermore, the associated query time of our algorithm is also of sub-linear complexity. We benchmark the performance of the proposed algorithm on several real-world benchmark datasets and experimentally validate the theoretical sub-linearity of our approach, while also showing that our approach yields competitive absolute storage performance as well.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Le_Hierarchical_Encoding_of_Sequential_Data_With_Compact_and_Sub-Linear_Storage_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Hierarchical_Encoding_of_Sequential_Data_With_Compact_and_Sub-Linear_Storage_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010310/,"['Visualization', 'Eigenvalues and eigenfunctions', 'Simultaneous localization and mapping', 'Quantization (signal)', 'Encoding', 'Indexing', 'Real-time systems']","['Sequencing Data', 'Hierarchical Encoder', 'Training Data', 'Number Of Observations', 'Autonomous Vehicles', 'Real-world Datasets', 'Video Sequences', 'Robotic Applications', 'Nearest Neighbor Search', 'Storage Requirements', 'Quantification Techniques', 'Simultaneous Localization And Mapping', 'Visual Localization', 'Code Vector', 'Indexing Techniques', 'Codebook', 'Class Labels', 'Point Cloud', 'Transformation Matrix', 'Linear Transformation', 'Query Vector', 'Linear Support Vector Machine', 'Image Descriptors', 'Query Image', 'SVM Classifier', 'Quantification Approach', 'Subspace Clustering', 'Material For More Details', 'Linearly Separable', 'Metric Learning']",,,"Snapshot-based visual localization is an important problem in several computer vision and robotics applications such as Simultaneous Localization And Mapping (SLAM). To achieve real-time performance in very large-scale environments with massive amounts of training and map data, techniques such as approximate nearest neighbor search (ANN) algorithms are used. While several state-of-the-art variants of quantization and indexing techniques have demonstrated to be efficient in practice, their theoretical memory cost still scales at least linearly with the training data (i.e., O(n) where n is the number of instances in the database), since each data point must be associated with at least one code vector. To address these limitations, in this paper we present a totally new hierarchical encoding approach that enables a sub-linear storage scale. The algorithm exploits the widespread sequential nature of sensor information streams in robotics and autonomous vehicle applications and achieves, both theoretically and experimentally, sub-linear scalability in storage required for a given environment size. Furthermore, the associated query time of our algorithm is also of sub-linear complexity. We benchmark the performance of the proposed algorithm on several real-world benchmark datasets and experimentally validate the theoretical sub-linearity of our approach, while also showing that our approach yields competitive absolute storage performance as well."
Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation,"Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya Jia",Tencent YouTu Lab; The Chinese University of Hong Kong; The Chinese University of Hong Kong and Tencent YouTu Lab,100.0,"Hong Kong, china",0.0,,"We achieve 3D semantic scene labeling by exploring semantic relation between each point and its contextual neighbors through edges. Besides an encoder-decoder branch for predicting point labels, we construct an edge branch to hierarchically integrate point features and generate edge features. To incorporate point features in the edge branch, we establish a hierarchical graph framework, where the graph is initialized from a coarse layer and gradually enriched along the point decoding process. For each edge in the final graph, we predict a label to indicate the semantic consistency of the two connected points to enhance point prediction. At different layers, edge features are also fed into the corresponding point module to integrate contextual information for message passing enhancement in local regions. The two branches interact with each other and cooperate in segmentation. Decent experimental results on several 3D semantic labeling datasets demonstrate the effectiveness of our work.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010952/,"['Three-dimensional displays', 'Semantics', 'Convolution', 'Feature extraction', 'Decoding', 'Neural networks', 'Labeling']","['Point Cloud', 'Semantic Segmentation', 'Contextual Information', 'Feature Points', 'Edge Features', 'Semantic Labels', 'Final Graph', 'Message Passing', 'Hierarchical Graph', 'Contralateral', 'Neural Network', 'Deep Neural Network', 'Hierarchical Structure', 'Validation Set', 'Local Features', 'K-nearest Neighbor', 'Multilayer Perceptron', 'Final Prediction', '3D Data', '3D Point', 'Hierarchical Construction', 'Layer Edge', 'Edge Loss', 'Graph Convolution', 'Graph Construction', 'Coat Layer', 'Encoder Layer', 'Hadamard Product', 'Receptive Field']",,141,"We achieve 3D semantic scene labeling by exploring semantic relation between each point and its contextual neighbors through edges. Besides an encoder-decoder branch for predicting point labels, we construct an edge branch to hierarchically integrate point features and generate edge features. To incorporate point features in the edge branch, we establish a hierarchical graph framework, where the graph is initialized from a coarse layer and gradually enriched along the point decoding process. For each edge in the final graph, we predict a label to indicate the semantic consistency of the two connected points to enhance point prediction. At different layers, edge features are also fed into the corresponding point module to integrate contextual information for message passing enhancement in local regions. The two branches interact with each other and cooperate in segmentation. Decent experimental results on several 3D semantic labeling datasets demonstrate the effectiveness of our work."
Hierarchical Self-Attention Network for Action Localization in Videos,"Rizard Renanda Adhi Pramono, Yie-Tarng Chen, Wen-Hsien Fang","National Taiwan University of Science and Technology, Taipei, Taiwan, R.O.C.",100.0,taiwan,0.0,,"This paper presents a novel Hierarchical Self-Attention Network (HISAN) to generate spatial-temporal tubes for action localization in videos. The essence of HISAN is to combine the two-stream convolutional neural network (CNN) with hierarchical bidirectional self-attention mechanism, which comprises of two levels of bidirectional self-attention to efficaciously capture both of the long-term temporal dependency information and spatial context information to render more precise action localization. Also, a sequence rescoring (SR) algorithm is employed to resolve the dilemma of inconsistent detection scores incurred by occlusion or background clutter. Moreover, a new fusion scheme is invoked, which integrates not only the appearance and motion information from the two-stream network, but also the motion saliency to mitigate the effect of camera motion. Simulations reveal that the new approach achieves competitive performance as the state-of-the-art works in terms of action localization and recognition accuracy on the widespread UCF101-24 and J-HMDB datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pramono_Hierarchical_Self-Attention_Network_for_Action_Localization_in_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pramono_Hierarchical_Self-Attention_Network_for_Action_Localization_in_Videos_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010637/,"['Videos', 'Proposals', 'Electron tubes', 'Detectors', 'Clutter', 'Three-dimensional displays', 'Training']","['Self-attention Network', 'Action Localization', 'Localization In Videos', 'Action Localization In Videos', 'Convolutional Network', 'Convolutional Neural Network', 'Spatial Information', 'Precise Location', 'Localization Accuracy', 'Temporal Information', 'Recognition Accuracy', 'Competitive Performance', 'Effect Of Motion', 'Action Recognition', 'Temporal Dependencies', 'Fusion Strategy', 'Motion Information', 'Long-term Information', 'Camera Motion', 'Background Clutter', 'Bounding Box', 'Faster R-CNN', 'Long Short-term Memory', 'Single Shot Multibox Detector', 'Wooden Frame', 'Video Dataset', 'Attention Weights', 'Capsule Network', 'Multi-object Tracking', 'Region Proposal']",,32,"This paper presents a novel Hierarchical Self-Attention Network (HISAN) to generate spatial-temporal tubes for action localization in videos. The essence of HISAN is to combine the two-stream convolutional neural network (CNN) with hierarchical bidirectional self-attention mechanism, which comprises of two levels of bidirectional self-attention to efficaciously capture both of the long-term temporal dependency information and spatial context information to render more precise action localization. Also, a sequence rescoring (SR) algorithm is employed to resolve the dilemma of inconsistent detection scores incurred by occlusion or background clutter. Moreover, a new fusion scheme is invoked, which integrates not only the appearance and motion information from the two-stream network, but also the motion saliency to mitigate the effect of camera motion. Simulations reveal that the new approach achieves competitive performance as the state-of-the-art works in terms of action localization and recognition accuracy on the widespread UCF101-24 and J-HMDB datasets."
Hierarchical Shot Detector,"Jiale Cao, Yanwei Pang, Jungong Han, Xuelong Li",Tianjin University; Northwestern Polytechnical University; University of Warwick,100.0,"china, uk",0.0,,"Single shot detector simultaneously predicts object categories and regression offsets of the default boxes. Despite of high efficiency, this structure has some inappropriate designs: (1) The classification result of the default box is improperly assigned to that of the regressed box during inference, (2) Only regression once is not good enough for accurate object detection. To solve the first problem, a novel reg-offset-cls (ROC) module is proposed. It contains three hierarchical steps: box regression, the feature sampling location predication, and the regressed box classification with the features of offset locations. To further solve the second problem, a hierarchical shot detector (HSD) is proposed, which stacks two ROC modules and one feature enhanced module. The second ROC treats the regressed boxes and the feature sampling locations of features in the first ROC as the inputs. Meanwhile, the feature enhanced module injected between two ROCs aims to extract the local and non-local context. Experiments on the MS COCO and PASCAL VOC datasets demonstrate the superiority of proposed HSD. Without the bells or whistles, HSD outperforms all one-stage methods at real-time speed.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.pdf,,https://github.com/JialeCao001/HSD,,main,Poster,https://ieeexplore.ieee.org/document/9009086,"['Iron', 'Head', 'Feature extraction', 'Object detection', 'Detectors', 'Training', 'Proposals']","['Local Features', 'Sampling Locations', 'Object Detection', 'Feature Enhancement', 'COCO Dataset', 'Box Regression', 'One-stage Methods', 'Inappropriate Design', 'PASCAL VOC Dataset', 'Convolutional Layers', 'Contextual Information', 'Feature Maps', 'Classification Of Samples', 'Deep Convolutional Neural Network', 'Training Images', 'Training Stage', 'Two-stage Method', 'Classification Loss', 'Strict Threshold', 'Class Imbalance Problem', 'Simultaneous Regression', 'Regression Sample', 'Cascade Structure', 'Non-local Information', 'Deformable Convolution', 'Non-maximum Suppression', 'Regression Loss', 'Improve Detection Accuracy', 'IoU Threshold', 'Input Size']",,57,"Single shot detector simultaneously predicts object categories and regression offsets of the default boxes. Despite of high efficiency, this structure has some inappropriate designs: (1) The classification result of the default box is improperly assigned to that of the regressed box during inference, (2) Only regression once is not good enough for accurate object detection. To solve the first problem, a novel reg-offset-cls (ROC) module is proposed. It contains three hierarchical steps: box regression, the feature sampling location predication, and the regressed box classification with the features of offset locations. To further solve the second problem, a hierarchical shot detector (HSD) is proposed, which stacks two ROC modules and one feature enhanced module. The second ROC treats the regressed boxes and the feature sampling locations of features in the first ROC as the inputs. Meanwhile, the feature enhanced module injected between two ROCs aims to extract the local and non-local context. Experiments on the MS COCO and PASCAL VOC datasets demonstrate the superiority of proposed HSD. Without the bells or whistles, HSD outperforms all one-stage methods at real-time speed."
Hierarchy Parsing for Image Captioning,"Ting Yao, Yingwei Pan, Yehao Li, Tao Mei","JD AI Research, Beijing, China",0.0,,100.0,China,"It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010299/,"['Hip', 'Semantics', 'Task analysis', 'Decoding', 'Visualization', 'Context modeling', 'Image segmentation']","['Image Captioning', 'Regional Level', 'Hierarchical Structure', 'Short-term Memory', 'Neural Model', 'Long Short-term Memory', 'Semantic Similarity', 'Set Of Regions', 'Graph Convolutional Network', 'Hierarchical Tree', 'Image Encoder', 'Convolutional Neural Network', 'Structural Images', 'Input Image', 'Image Regions', 'Attention Mechanism', 'Cross-entropy Loss', 'Intersection Over Union', 'Precision And Recall', 'Root Node', 'Faster R-CNN', 'Text Generation', 'Mask R-CNN', 'Semantic Labels', 'Image Pattern', 'Semantic Graph', 'Leaf Layer', 'Long Short-term Memory Unit', 'Root Layer', 'Bottom-up Manner']",,147,"It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%."
Hilbert-Based Generative Defense for Adversarial Examples,"Yang Bai, Yan Feng, Yisen Wang, Tao Dai, Shu-Tao Xia, Yong Jiang","Dept. of Computer Science and Engineering, Shanghai Jiao Tong University; Tsinghua-Berkeley Shenzhen Institute, Tsinghua University; Graduate School at Shenzhen, Tsinghua University",100.0,China,0.0,,"Adversarial perturbations of clean images are usually imperceptible for human eyes, but can confidently fool deep neural networks (DNNs) to make incorrect predictions. Such vulnerability of DNNs raises serious security concerns about their practicability in security-sensitive applications. To defend against such adversarial perturbations, recently developed PixelDefend purifies a perturbed image based on PixelCNN in a raster scan order (row/column by row/column). However, such scan mode insufficiently exploits the correlations between pixels, which further limits its robustness performance. Therefore, we propose a more advanced Hilbert curve scan order to model the pixel dependencies in this paper. Hilbert curve could well preserve local consistency when mapping from 2-D image to 1-D vector, thus the local features in neighboring pixels can be more effectively modeled. Moreover, the defensive power can be further improved via ensembles of Hilbert curve with different orientations. Experimental results demonstrate the superiority of our method over the state-of-the-art defenses against various adversarial attacks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bai_Hilbert-Based_Generative_Defense_for_Adversarial_Examples_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bai_Hilbert-Based_Generative_Defense_for_Adversarial_Examples_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010387/,"['Perturbation methods', 'Training', 'Feature extraction', 'Neural networks', 'Correlation', 'Heating systems', 'Computer vision']","['Adversarial Examples', 'Deep Neural Network', 'Local Features', 'Clear Image', 'Neighboring Pixels', 'Raster Scan', 'Adversarial Attacks', 'Adversarial Perturbations', 'Scan Order', 'Step Size', 'Cross-entropy Loss', 'Ensemble Model', 'Spatial Proximity', 'Self-similarity', 'L1-norm', 'Adversarial Training', 'Pixel Information', 'Image X', 'Attack Methods', 'Defense Methods', 'Projected Gradient Descent', 'Fast Gradient Sign Method', 'Black-box Attacks', 'White-box Attack', 'Greedy Search Algorithm']",,27,"Adversarial perturbations of clean images are usually imperceptible for human eyes, but can confidently fool deep neural networks (DNNs) to make incorrect predictions. Such vulnerability of DNNs raises serious security concerns about their practicability in security-sensitive applications. To defend against such adversarial perturbations, recently developed PixelDefend purifies a perturbed image based on PixelCNN in a raster scan order (row/column by row/column). However, such scan mode insufficiently exploits the correlations between pixels, which further limits its robustness performance. Therefore, we propose a more advanced Hilbert curve scan order to model the pixel dependencies in this paper. Hilbert curve could well preserve local consistency when mapping from 2-D image to 1-D vector, thus the local features in neighboring pixels can be more effectively modeled. Moreover, the defensive power can be further improved via ensembles of Hilbert curve with different orientations. Experimental results demonstrate the superiority of our method over the state-of-the-art defenses against various adversarial attacks."
HistoSegNet: Semantic Segmentation of Histological Tissue Type in Whole Slide Images,"Lyndon Chan,  Mahdi S. Hosseini,  Corwyn Rowsell,  Konstantinos N. Plataniotis,  Savvas Damaskinos","Division of Pathology, St. Michaels Hospital, Toronto, ON, M4N 1X3, Canada; The Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto; Huron Digital Pathology, St. Jacobs, ON, N0B 2N0, Canada",33.33333333333333,"Canada, canada",66.66666666666667,Canada,"In digital pathology, tissue slides are scanned into Whole Slide Images (WSI) and pathologists first screen for diagnostically-relevant Regions of Interest (ROIs) before reviewing them. Screening for ROIs is a tedious and time-consuming visual recognition task which can be exhausting. The cognitive workload could be reduced by developing a visual aid to narrow down the visual search area by highlighting (or segmenting) regions of diagnostic relevance, enabling pathologists to spend more time diagnosing relevant ROIs. In this paper, we propose HistoSegNet, a method for semantic segmentation of histological tissue type (HTT). Using the HTT-annotated Atlas of Digital Pathology (ADP) database, we train a Convolutional Neural Network on the patch annotations, infer Gradient-Weighted Class Activation Maps, average overlapping predictions, and post-process the segmentation with a fully-connected Conditional Random Field. Our method out-performs more complicated weakly-supervised semantic segmentation methods and can generalize to other datasets without retraining.",http://openaccess.thecvf.com/content_ICCV_2019/html/Chan_HistoSegNet_Semantic_Segmentation_of_Histological_Tissue_Type_in_Whole_Slide_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chan_HistoSegNet_Semantic_Segmentation_of_Histological_Tissue_Type_in_Whole_Slide_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9009552/,"['Image segmentation', 'Semantics', 'Pathology', 'Databases', 'Visualization', 'Tools', 'Cams']","['Tissue Types', 'Semantic Segmentation', 'Slide Images', 'Convolutional Neural Network', 'Cognitive Load', 'Segmentation Method', 'Activation Maps', 'Visual Search', 'Relevant Regions', 'Visual Aids', 'Digital Pathology', 'Conditional Random Field', 'Class Activation Maps', 'Semantic Segmentation Methods', 'Gradient-weighted Class Activation Mapping', 'Pathology Database', 'Functional Imaging', 'Healthy Tissue', 'Function Of Type', 'Image Segmentation', 'Patch Level', 'Convolutional Neural Network Classifier', 'Pixel Level', 'Typical Morphology', 'Histopathological Images', 'Morphological Images', 'Global Pooling Layer', 'White Adipose', 'Ground Truth Segmentation', 'Confidence Score']",,72,"In digital pathology, tissue slides are scanned into Whole Slide Images (WSI) and pathologists first screen for diagnostically-relevant Regions of Interest (ROIs) before reviewing them. Screening for ROIs is a tedious and time-consuming visual recognition task which can be exhausting. The cognitive workload could be reduced by developing a visual aid to narrow down the visual search area by highlighting (or segmenting) regions of diagnostic relevance, enabling pathologists to spend more time diagnosing relevant ROIs. In this paper, we propose HistoSegNet, a method for semantic segmentation of histological tissue type (HTT). Using the HTT-annotated Atlas of Digital Pathology (ADP) database, we train a Convolutional Neural Network on the patch annotations, infer Gradient-Weighted Class Activation Maps, average overlapping predictions, and post-process the segmentation with a fully-connected Conditional Random Field. Our method out-performs more complicated weakly-supervised semantic segmentation methods and can generalize to other datasets without retraining."
Holistic++ Scene Understanding: Single-View 3D Holistic Scene Parsing and Human Pose Estimation With Human-Object Interaction and Physical Commonsense,"Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, Song-Chun Zhu","University of California, Los Angeles (UCLA); University of California, Los Angeles (UCLA); International Center for AI and Robot Autonomy (CARA)",100.0,"china, uk, usa",0.0,,"We propose a new 3D holistic++ scene understanding problem, which jointly tackles two tasks from a single-view image: (i) holistic scene parsing and reconstruction---3D estimations of object bounding boxes, camera pose, and room layout, and (ii) 3D human pose estimation. The intuition behind is to leverage the coupled nature of these two tasks to improve the granularity and performance of scene understanding. We propose to exploit two critical and essential connections between these two tasks: (i) human-object interaction (HOI) to model the fine-grained relations between agents and objects in the scene, and (ii) physical commonsense to model the physical plausibility of the reconstructed scene. The optimal configuration of the 3D scene, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable joint solution space. Experimental results demonstrate that the proposed algorithm significantly improves the performance of the two tasks on three datasets, showing an improved generalization ability.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Holistic_Scene_Understanding_Single-View_3D_Holistic_Scene_Parsing_and_Human_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Holistic_Scene_Understanding_Single-View_3D_Holistic_Scene_Parsing_and_Human_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010686/,"['Three-dimensional displays', 'Layout', 'Task analysis', 'Image reconstruction', 'Pose estimation', 'Two dimensional displays']","['Pose Estimation', 'Scene Understanding', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Human-object Interaction', 'Markov Chain Monte Carlo', 'Bounding Box', 'Solution Space', 'Objects In The Scene', '3D Scene', 'Camera Pose', 'Object Bounding Boxes', '3D Pose', 'Parse Tree', 'Room Layout', 'Human Activities', 'Single Image', 'Types Of Relationships', '3D Reconstruction', '2D Pose', '3D Bounding Box', '3D Joint', 'Action Labels', 'World Coordinate', 'Local Coordinate', 'Scene Reconstruction', '2D Image Plane', 'Terminal Nodes', 'Maximum A Posteriori']",,67,"We propose a new 3D holistic
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">++</sup>
 scene understanding problem, which jointly tackles two tasks from a single-view image: (i) holistic scene parsing and reconstruction-3D estimations of object bounding boxes, camera pose, and room layout, and (ii) 3D human pose estimation. The intuition behind is to leverage the coupled nature of these two tasks to improve the granularity and performance of scene understanding. We propose to exploit two critical and essential connections between these two tasks: (i) human-object interaction (HOI) to model the fine-grained relations between agents and objects in the scene, and (ii) physical commonsense to model the physical plausibility of the reconstructed scene. The optimal configuration of the 3D scene, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable joint solution space. Experimental results demonstrate that the proposed algorithm significantly improves the performance of the two tasks on three datasets, showing an improved generalization ability."
HoloGAN: Unsupervised Learning of 3D Representations From Natural Images,"Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, Yong-Liang Yang",Twitter; Lambda Labs; University of Bath,33.33333333333333,uk,66.66666666666667,USA,"We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9022241/,"['Three-dimensional displays', 'Two dimensional displays', 'Gallium nitride', 'Shape', 'Training', 'Solid modeling', 'Generators']","['Natural Images', 'Representation Learning', '3D Representation', '2D Images', 'Generative Adversarial Networks', 'Visual Quality', '3D Shape', '3D Features', 'Objective View', 'Rigid Body Transformation', 'Explicit Control', '3D World', '3D Learning', 'High Visual Quality', 'Convolution', 'Computer Vision', 'Random Noise', 'Multilayer Perceptron', 'Visual Perspective', 'Noise Vector', '3D Transformation', '2D Feature', 'StyleGAN', 'Latent Vector', 'Generative Adversarial Networks Model', 'Computer Graphics', 'Disentangled Representation']","['unsupervised', '3Drepresentation', 'GAN']",34,"We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner."
Homography From Two Orientation- and Scale-Covariant Features,"DÃ¡niel BarÃ¡th, Zuzana Kukelova","VRG, Department of Cybernetics, Czech Technical University in Prague, Czech Republic; VRG, Department of Cybernetics, Czech Technical University in Prague, Czech Republic; Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary",100.0,"Czech Republic, hungary",0.0,,"This paper proposes a geometric interpretation of the angles and scales which the orientation- and scale-covariant feature detectors, e.g. SIFT, provide. Two new general constraints are derived on the scales and rotations which can be used in any geometric model estimation tasks. Using these formulas, two new constraints on homography estimation are introduced. Exploiting the derived equations, a solver for estimating the homography from the minimal number of two correspondences is proposed. Also, it is shown how the normalization of the point correspondences affects the rotation and scale parameters, thus achieving numerically stable results. Due to requiring merely two feature pairs, robust estimators, e.g. RANSAC, do significantly fewer iterations than by using the four-point algorithm. When using covariant features, e.g. SIFT, this additional information is given at no cost. The method is tested in a synthetic environment and on publicly available real-world datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Barath_Homography_From_Two_Orientation-_and_Scale-Covariant_Features_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Barath_Homography_From_Two_Orientation-_and_Scale-Covariant_Features_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008807/,"['Mathematical model', 'Estimation', 'Geometry', 'Feature extraction', 'Detectors', 'Generators', 'Matrix decomposition']","['Geometric Model', 'Numerical Stability', 'Feature Pairs', 'General Constraints', 'Geometric Approximation', 'Synthetic Environment', 'Computational Complexity', 'Linear Equation', 'Coefficient Matrix', 'Image Pairs', 'Coordinates Of Points', 'Affine Transformation', 'Null Space', 'Inliers', 'Homogeneous Equation', 'Tangent Plane', 'Affine Parameter', 'Differential Geometry', 'Reprojection Error', 'Uniform Scale', 'Fundamental Matrix', 'Homography Matrix', 'Affine Model', 'Surface Normals', 'Transformative Elements', 'Function Of Noise']",,32,"This paper proposes a geometric interpretation of the angles and scales which the orientation- and scale-covariant feature detectors, e.g. SIFT, provide. Two new general constraints are derived on the scales and rotations which can be used in any geometric model estimation tasks. Using these formulas, two new constraints on homography estimation are introduced. Exploiting the derived equations, a solver for estimating the homography from the minimal number of two correspondences is proposed. Also, it is shown how the normalization of the point correspondences affects the rotation and scale parameters, thus achieving numerically stable results. Due to requiring merely two feature pairs, robust estimators, e.g. RANSAC, do significantly fewer iterations than by using the four-point algorithm. When using covariant features, e.g. SIFT, the information about the scale and orientation is given at no cost. The proposed homography estimation method is tested in a synthetic environment and on publicly available real-world datasets."
How Do Neural Networks See Depth in Single Images?,"Tom van Dijk, Guido de Croon","Technische Universiteit Delft, Delft, The Netherlands",100.0,Netherlands,0.0,,"Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned. In this work we take four previously published networks and investigate what depth cues they exploit. We find that all networks ignore the apparent size of known obstacles in favor of their vertical position in the image. The use of the vertical position requires the camera pose to be known; however, we find that these networks only partially recognize changes in camera pitch and roll angles. Small changes in camera pitch are shown to disturb the estimated distance towards obstacles. The use of the vertical image position allows the networks to estimate depth towards arbitrary obstacles - even those not appearing in the training set - but may depend on features that are not universally present.",,http://openaccess.thecvf.com/content_ICCV_2019/html/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009532/,"['Cameras', 'Estimation', 'Training', 'Automobiles', 'Visualization', 'Biological neural networks']","['Neural Network', 'Single Image', 'Training Set', 'Deep Network', 'Deep Neural Network', 'Vertical Position', 'Depth Estimation', 'Image Position', 'Pitch Angle', 'Roll Angle', 'Depth Perception', 'Apparent Size', 'Camera Pose', 'Pitch Change', 'Use Of Position', 'Convolutional Neural Network', 'Data Augmentation', 'Image Object', 'Road Surface', 'Object Position', 'Monocular Depth Estimation', 'Look For Correlations', 'Depth Map', 'Object Size', 'KITTI Dataset', 'Bottom Edge', 'True Depth', 'Eye Level', 'Distance Estimation']",,104,"Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned. In this work we take four previously published networks and investigate what depth cues they exploit. We find that all networks ignore the apparent size of known obstacles in favor of their vertical position in the image. The use of the vertical position requires the camera pose to be known; however, we find that these networks only partially recognize changes in camera pitch and roll angles. Small changes in camera pitch are shown to disturb the estimated distance towards obstacles. The use of the vertical image position allows the networks to estimate depth towards arbitrary obstacles - even those not appearing in the training set - but may depend on features that are not universally present."
HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips,"Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic","´Ecole Normale Supérieure, Inria, CIIRC, CTU; ´Ecole Normale Supérieure, Inria; Inria; Inria, Now at DeepMind",100.0,France,0.0,,"Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models are publicly available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.pdf,https://www.di.ens.fr/willow/research/howto100m,,,main,Poster,https://ieeexplore.ieee.org/document/9009806/,"['Task analysis', 'Visualization', 'YouTube', 'Motion pictures', 'Data models', 'Manuals', 'Computational modeling']","['Video Clips', 'Video Narratives', 'Natural Language', 'Contributions Of This Work', 'Visual Task', 'Video For Instructions', 'Video Data', 'Manual Annotation', 'Video Dataset', 'Annotation Format', 'Large Amount Of Data', 'Small Datasets', 'Latent Space', 'Language Model', 'Textual Descriptions', 'Visual Content', 'Caption Text', 'Embedding Learning', 'Video Captioning', 'Video Features', '2D Feature', 'Joint Learning']",,452,"Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models are publicly available."
Human Attention in Image Captioning: Dataset and Analysis,"Sen He, Hamed R. Tavakoli, Ali Borji, Nicolas Pugeault","MarkableAI; Nokia Technologies, Aalto University; University of Exeter",66.66666666666666,"finland, uk",33.33333333333334,USA,"In this work, we present a novel dataset consisting of eye movements and verbal descriptions recorded synchronously over images. Using this data, we study the differences in human attention during free-viewing and image captioning tasks. We look into the relationship between human atten- tion and language constructs during perception and sen- tence articulation. We also analyse attention deployment mechanisms in the top-down soft attention approach that is argued to mimic human attention in captioning tasks, and investigate whether visual saliency can help image caption- ing. Our study reveals that (1) human attention behaviour differs in free-viewing and image description tasks. Hu- mans tend to fixate on a greater variety of regions under the latter task, (2) there is a strong relationship between de- scribed objects and attended objects (97% of the described objects are being attended), (3) a convolutional neural net- work as feature encoder accounts for human-attended re- gions during image captioning to a great extent (around 78%), (4) soft-attention mechanism differs from human at- tention, both spatially and temporally, and there is low correlation between caption scores and attention consis- tency scores. These indicate a large gap between humans and machines in regards to top-down attention, and (5) by integrating the soft attention model with image saliency, we can significantly improve the model's performance on Flickr30k and MSCOCO benchmarks. The dataset can be found at: https://github.com/SenHe/ Human-Attention-in-Image-Captioning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.pdf,,https://github.com/SenHe/Human-Attention-in-Image-Captioning,,main,Poster,https://ieeexplore.ieee.org/document/9010621/,"['Task analysis', 'Visualization', 'Data collection', 'Cows', 'Computer vision', 'Computational modeling', 'Adaptation models']","['Human Attention', 'Image Captioning', 'Convolutional Neural Network', 'Eye Movements', 'Attention Model', 'Image Descriptors', 'Visual Saliency', 'Top-down Attention', 'Image Saliency', 'Deep Neural Network', 'Long Short-term Memory', 'Image Regions', 'Attention Mechanism', 'Eye-tracking', 'Visual Attention', 'Language Model', 'Spatial Attention', 'Saliency Map', 'Scene Classification', 'Deep Neural Architecture', 'Dynamic Time Warping', 'Saliency Models', 'Bottom-up Model', 'Spatial Consistency', 'Bottom-up Attention', 'Scene Description', 'Fixation Location', 'Scene Elements', 'Salient Regions', 'Activation Maps']",,20,"In this work, we present a novel dataset consisting of eye movements and verbal descriptions recorded synchronously over images. Using this data, we study the differences in human attention during free-viewing and image captioning tasks. We look into the relationship between human atten- tion and language constructs during perception and sen- tence articulation. We also analyse attention deployment mechanisms in the top-down soft attention approach that is argued to mimic human attention in captioning tasks, and investigate whether visual saliency can help image caption- ing. Our study reveals that (1) human attention behaviour differs in free-viewing and image description tasks. Hu- mans tend to fixate on a greater variety of regions under the latter task, (2) there is a strong relationship between de- scribed objects and attended objects (97% of the described objects are being attended), (3) a convolutional neural net- work as feature encoder accounts for human-attended re- gions during image captioning to a great extent (around 78%), (4) soft-attention mechanism differs from human at- tention, both spatially and temporally, and there is low correlation between caption scores and attention consis- tency scores. These indicate a large gap between humans and machines in regards to top-down attention, and (5) by integrating the soft attention model with image saliency, we can significantly improve the model's performance on Flickr30k and MSCOCO benchmarks. The dataset can be found at: https://github.com/SenHe/ Human-Attention-in-Image-Captioning."
Human Mesh Recovery From Monocular Images via a Skeleton-Disentangled Representation,"Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, Tao Mei",Harbin Institute of Technology; JD AI Research,50.0,china,50.0,China,"We describe an end-to-end method for recovering 3D human body mesh from single images and monocular videos. Different from the existing methods try to obtain all the complex 3D pose, shape, and camera parameters from one coupling feature, we propose a skeleton-disentangling based framework, which divides this task into multi-level spatial and temporal granularity in a decoupling manner. In spatial, we propose an effective and pluggable ""disentangling the skeleton from the details"" (DSD) module. It reduces the complexity and decouples the skeleton, which lays a good foundation for temporal modeling. In temporal, the self-attention based temporal convolution network is proposed to efficiently exploit the short and long-term temporal cues. Furthermore, an unsupervised adversarial training strategy, temporal shuffles and order recovery, is designed to promote the learning of motion dynamics. The proposed method outperforms the state-of-the-art 3D human mesh recovery methods by 15.4% MPJPE and 23.8% PA-MPJPE on Human3.6M. State-of-the-art results are also achieved on the 3D pose in the wild (3DPW) dataset without any fine-tuning. Especially, ablation studies demonstrate that skeleton-disentangled representation is crucial for better temporal modeling and generalization.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Human_Mesh_Recovery_From_Monocular_Images_via_a_Skeleton-Disentangled_Representation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Human_Mesh_Recovery_From_Monocular_Images_via_a_Skeleton-Disentangled_Representation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009452/,"['Three-dimensional displays', 'Videos', 'Shape', 'Two dimensional displays', 'Skeleton', 'Biological system modeling', 'Couplings']","['Monocular Images', 'Human Mesh', 'Mesh Recovery', 'Single Image', 'Shape Parameter', '3D Mesh', '3D Shape', 'Dynamic Motion', 'Temporal Model', 'Unsupervised Training', '3D Pose', '3D Body', 'Temporal Convolutional Network', 'Pose Parameters', 'Temporal Granularity', 'Unsupervised Strategy', 'Body Shape', 'Representation Learning', 'Sequence Position', 'Fully-connected Layer', '2D Pose', 'Self-attention Module', 'Correct Order', 'Human Pose Estimation', 'Pose Estimation', 'Temporal Coherence', 'Dynamic Video', 'Human Body Shape', 'Human Body Model', 'Two-stage Method']",,135,"We describe an end-to-end method for recovering 3D human body mesh from single images and monocular videos. Different from the existing methods try to obtain all the complex 3D pose, shape, and camera parameters from one coupling feature, we propose a skeleton-disentangling based framework, which divides this task into multi-level spatial and temporal granularity in a decoupling manner. In spatial, we propose an effective and pluggable “disentangling the skeleton from the details” (DSD) module. It reduces the complexity and decouples the skeleton, which lays a good foundation for temporal modeling. In temporal, the self-attention based temporal convolution network is proposed to efficiently exploit the short and long-term temporal cues. Furthermore, an unsupervised adversarial training strategy, temporal shuffles and order recovery, is designed to promote the learning of motion dynamics. The proposed method outperforms the state-of-the-art 3D human mesh recovery methods by 15.4% MPJPE and 23.8% PA-MPJPE on Human3.6M. State-of-the-art results are also achieved on the 3D pose in the wild (3DPW) dataset without any fine-tuning. Especially, ablation studies demonstrate that skeleton-disentangled representation is crucial for better temporal modeling and generalization."
Human Motion Prediction via Spatio-Temporal Inpainting,"Alejandro Hernandez, JÃ¼rgen Gall, Francesc Moreno-Noguer","Institut de Rob `otica i Inform `atica Industrial, CSIC-UPC, Barcelona, Spain; Computer Vision Group, University of Bonn, Germany",100.0,"germany, spain",0.0,,"We propose a Generative Adversarial Network (GAN) to forecast 3D human motion given a sequence of past 3D skeleton poses. While recent GANs have shown promising results, they can only forecast plausible motion over relatively short periods of time (few hundred milliseconds) and typically ignore the absolute position of the skeleton w.r.t. the camera. Our scheme provides long term predictions (two seconds or more) for both the body pose and its absolute position. Our approach builds upon three main contributions. First, we represent the data using a spatio-temporal tensor of 3D skeleton coordinates which allows formulating the prediction problem as an inpainting one, for which GANs work particularly well. Secondly, we design an architecture to learn the joint distribution of body poses and global motion, capable to hypothesize large chunks of the input 3D tensor with missing data. And finally, we argue that the L2 metric, considered so far by most approaches, fails to capture the actual distribution of long-term human motion. We propose two alternative metrics, based on the distribution of frequencies, that are able to capture more realistic motion patterns. Extensive experiments demonstrate our approach to significantly improve the state of the art, while also handling situations in which past observations are corrupted by occlusions, noise and missing frames.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hernandez_Human_Motion_Prediction_via_Spatio-Temporal_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hernandez_Human_Motion_Prediction_via_Spatio-Temporal_Inpainting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008530/,"['Measurement', 'Skeleton', 'Three-dimensional displays', 'Gallium nitride', 'Generators', 'Machine learning', 'Training']","['Human Motion', 'Motion Prediction', 'Human Motion Prediction', 'State Of The Art', 'Generative Adversarial Networks', '3D Coordinates', 'Absolute Position', 'Past Observations', 'Global Motion', 'Body Pose', 'Deep Learning', 'Partial Sequences', 'Long Short-term Memory', 'Recurrent Neural Network', 'Kullback-Leibler', 'Network Output', 'Wide Range Of Values', 'Distribution Of Sequences', 'Reconstruction Loss', 'L2 Loss', 'Fréchet Inception Distance', 'Absolute Prediction', 'Motion Sequences', 'Image Inpainting', 'Output Of Block', 'Generative Adversarial Network Architecture', 'Motion Discrimination', 'Disk Model', 'Absolute Coordinates']",,111,"We propose a Generative Adversarial Network (GAN) to forecast 3D human motion given a sequence of past 3D skeleton poses. While recent GANs have shown promising results, they can only forecast plausible motion over relatively short periods of time (few hundred milliseconds) and typically ignore the absolute position of the skeleton w.r.t. the camera. Our scheme provides long term predictions (two seconds or more) for both the body pose and its absolute position. Our approach builds upon three main contributions. First, we represent the data using a spatio-temporal tensor of 3D skeleton coordinates which allows formulating the prediction problem as an inpainting one, for which GANs work particularly well. Secondly, we design an architecture to learn the joint distribution of body poses and global motion, capable to hypothesize large chunks of the input 3D tensor with missing data. And finally, we argue that the L2 metric, considered so far by most approaches, fails to capture the actual distribution of long-term human motion. We propose two alternative metrics, based on the distribution of frequencies, that are able to capture more realistic motion patterns. Extensive experiments demonstrate our approach to significantly improve the state of the art, while also handling situations in which past observations are corrupted by occlusions, noise and missing frames."
Human Uncertainty Makes Classification More Robust,"Joshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths, Olga Russakovsky","Princeton University, Department of Computer Science",100.0,usa,0.0,,"The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010969/,"['Training', 'Robustness', 'Uncertainty', 'Benchmark testing', 'Standards', 'Task analysis', 'Dogs']","['Training Set', 'Test Dataset', 'Label Distribution', 'Explicit Training', 'Human Labeling', 'Accuracy Of Model', 'Convolutional Neural Network', 'Image Classification', 'Cross-entropy', 'ImageNet', 'Domain Shift', 'Airplane', 'Convolutional Neural Network Architecture', 'Ground Truth Labels', 'Self-driving', 'Adversarial Examples', 'Examples Of Pairs', 'Soft Labels', 'Test Set Of Images', 'Confusable', 'Projected Gradient Descent', 'One-hot Label', 'Fast Gradient Sign Method', 'Softmax Probability', 'Dataset Construction', 'Hierarchical Classification', 'Adversarial Robustness', 'Perceptual Similarity', 'Classification Model']",,70,"The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks."
Human-Aware Motion Deblurring,"Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling, Tingfa Xu, Ling Shao","Inception Institute of Artiﬁcial Intelligence, UAE; Stony Brook University, USA; Beijing Institute of Technology, China; Inception Institute of Artiﬁcial Intelligence, UAE & Beijing Institute of Technology, China",100.0,"China, uae, usa",0.0,,"This paper proposes a human-aware deblurring model that disentangles the motion blur between foreground (FG) humans and background (BG). The proposed model is based on a triple-branch encoder-decoder architecture. The first two branches are learned for sharpening FG humans and BG details, respectively; while the third one produces global, harmonious results by comprehensively fusing multi-scale deblurring information from the two domains. The proposed model is further endowed with a supervised, human-aware attention mechanism in an end-to-end fashion. It learns a soft mask that encodes FG human information and explicitly drives the FG/BG decoder-branches to focus on their specific domains. Above designs lead to a fully differentiable motion deblurring network, which can be trained end-to-end. To further benefit the research towards Human-aware Image Deblurring, we introduce a large-scale dataset, named HIDE, which consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes. HIDE is specifically built to span a broad range of scenes, human object sizes, motion patterns, and background complexities. Extensive experiments on public benchmarks and our dataset demonstrate that our model performs favorably against the state-of-the-art motion deblurring methods, especially in capturing semantic details.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Human-Aware_Motion_Deblurring_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Human-Aware_Motion_Deblurring_ICCV_2019_paper.pdf,,https://github.com/joanshen0508/HA_deblur,,main,Poster,https://ieeexplore.ieee.org/document/9010839/,"['Cameras', 'Image restoration', 'Kernel', 'Dynamics', 'Videos', 'Decoding', 'Task analysis']","['Motion Deblurring', 'Human-aware Motion', 'Specific Domains', 'Attention Mechanism', 'Large-scale Datasets', 'Bounding Box', 'Image Pairs', 'Motion Patterns', 'Object Motion', 'Human Motion', 'Image Sharpness', 'Motion Blur', 'Blurry Images', 'Foreground-background', 'Image Deblurring', 'Neural Network', 'Training Set', 'Convolutional Layers', 'Pedestrian', 'Visual Comparison', 'Blur Kernel', 'Attention Network', 'Blurred Images', 'Dynamic Scenes', 'Attention Module', 'Attention Map', 'Camera Motion', 'Background Regions', 'Latent Image', 'Background Branches']",,168,"This paper proposes a human-aware deblurring model that disentangles the motion blur between foreground (FG) humans and background (BG). The proposed model is based on a triple-branch encoder-decoder architecture. The first two branches are learned for sharpening FG humans and BG details, respectively; while the third one produces global, harmonious results by comprehensively fusing multi-scale deblurring information from the two domains. The proposed model is further endowed with a supervised, human-aware attention mechanism in an end-to-end fashion. It learns a soft mask that encodes FG human information and explicitly drives the FG/BG decoder-branches to focus on their specific domains. Above designs lead to a fully differentiable motion deblurring network, which can be trained end-to-end. To further benefit the research towards Human-aware Image Deblurring, we introduce a large-scale dataset, named HIDE, which consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes. HIDE is specifically built to span a broad range of scenes, human object sizes, motion patterns, and background complexities. Extensive experiments on public benchmarks and our dataset demonstrate that our model performs favorably against the state-of-the-art motion deblurring methods, especially in capturing semantic details."
Hyperpixel Flow: Semantic Correspondence With Multi-Layer Neural Features,"Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho","Inria, DI ENS†; POSTECH, NPRC∗",100.0,"France, south korea",0.0,,"Establishing visual correspondences under large intra-class variations requires analyzing images at different levels, from features linked to semantics and context to local patterns, while being invariant to instance-specific details. To tackle these challenges, we represent images by ""hyperpixels"" that leverage a small number of relevant features selected among early to late layers of a convolutional neural network. Taking advantage of the condensed features of hyperpixels, we develop an effective real-time matching algorithm based on Hough geometric voting. The proposed method, hyperpixel flow, sets a new state of the art on three standard benchmarks as well as a new dataset, SPair-71k, which contains a significantly larger number of image pairs than existing datasets, with more accurate and richer annotations for in-depth analysis.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Min_Hyperpixel_Flow_Semantic_Correspondence_With_Multi-Layer_Neural_Features_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Min_Hyperpixel_Flow_Semantic_Correspondence_With_Multi-Layer_Neural_Features_ICCV_2019_paper.pdf,http://cvlab.postech.ac.kr/research/HPF/,,,main,Poster,https://ieeexplore.ieee.org/document/9008129/,"['Semantics', 'Proposals', 'Visualization', 'Benchmark testing', 'Tensile stress', 'Prognostics and health management', 'Real-time systems']","['Multilayer Feature', 'Semantic Correspondence', 'Convolutional Neural Network', 'State Of The Art', 'Network Layer', 'Image Pairs', 'Matching Algorithm', 'Neural Network Layers', 'Standard Benchmark', 'Large Number Of Pairs', 'Rich Annotations', 'Exponent', 'Convolutional Layers', 'Feature Maps', 'Search Algorithm', 'Object Detection', 'Level Characteristics', 'Receptive Field', 'Bounding Box', 'Backbone Network', 'Neural Architecture Search', 'Hough Transform', 'Beam Search', 'Object Segmentation', 'Global Alignment', 'Region Proposal', 'Semantic Matching', 'High-level Semantics', 'Base Map', 'Nearest Neighbor Matching']",,61,"Establishing visual correspondences under large intra-class variations requires analyzing images at different levels, from features linked to semantics and context to local patterns, while being invariant to instance-specific details. To tackle these challenges, we represent images by “hyperpixels” that leverage a small number of relevant features selected among early to late layers of a convolutional neural network. Taking advantage of the condensed features of hyperpixels, we develop an effective real-time matching algorithm based on Hough geometric voting. The proposed method, hyperpixel flow, sets a new state of the art on three standard benchmarks as well as a new dataset, SPair-71k, which contains a significantly larger number of image pairs than existing datasets, with more accurate and richer annotations for in-depth analysis."
Hyperspectral Image Reconstruction Using Deep External and Internal Learning,"Tao Zhang, Ying Fu, Lizhi Wang, Hua Huang","School of Computer Science and Technology, Beijing Institute of Technology",100.0,China,0.0,,"To solve the low spatial and/or temporal resolution problem which the conventional hypelrspectral cameras often suffer from, coded snapshot hyperspectral imaging systems have attracted more attention recently. Recovering a hyperspectral image (HSI) from its corresponding coded image is an ill-posed inverse problem, and learning accurate prior of HSI is essential to solve this inverse problem. In this paper, we present an effective convolutional neural network (CNN) based method for coded HSI reconstruction, which learns the deep prior from the external dataset as well as the internal information of input coded image with spatial-spectral constraint. Our method can effectively exploit spatial-spectral correlation and sufficiently represent the variety nature of HSIs. Experimental results show our method outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008574/,"['Image reconstruction', 'Hyperspectral imaging', 'Correlation', 'Spatial resolution', 'Cameras', 'Image coding', 'Apertures']","['Internal Learning', 'Spatial Resolution', 'Imaging System', 'Convolutional Neural Network', 'Inverse Problem', 'Quantitative Metrics', 'External Dataset', 'Ill-posed Inverse Problem', 'Hyperspectral Imaging System', 'Training Data', 'Convolutional Layers', 'Generalization Ability', 'Spatial Dimensions', 'Spatial Domain', 'Learning-based Methods', 'Peak Signal-to-noise Ratio', 'Model-based Methods', 'Reconstruction Results', 'Network Reconstruction', 'Channel Attention', 'Low-rank Approximation', 'Panchromatic Image', 'Dense Block', 'Hyperspectral Image Datasets', 'Image Compression', 'Spectral Correlation', 'Learning Layer', 'Features Of Hyperspectral Image', 'Spectral Angle Mapper', 'Network Parameters']",,48,"To solve the low spatial and/or temporal resolution problem which the conventional hypelrspectral cameras often suffer from, coded snapshot hyperspectral imaging systems have attracted more attention recently. Recovering a hyperspectral image (HSI) from its corresponding coded image is an ill-posed inverse problem, and learning accurate prior of HSI is essential to solve this inverse problem. In this paper, we present an effective convolutional neural network (CNN) based method for coded HSI reconstruction, which learns the deep prior from the external dataset as well as the internal information of input coded image with spatial-spectral constraint. Our method can effectively exploit spatial-spectral correlation and sufficiently represent the variety nature of HSIs. Experimental results show our method outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality."
IL2M: Class Incremental Learning With Dual Memory,"Eden Belouadah, Adrian Popescu","CEA, LIST, F-91191 Gif-sur-Yvette, France",0.0,,100.0,France,"This paper presents a class incremental learning (IL) method which exploits fine tuning and a dual memory to reduce the negative effect of catastrophic forgetting in image recognition. First, we simplify the current fine tuning based approaches which use a combination of classification and distillation losses to compensate for the limited availability of past data. We find that the distillation term actually hurts performance when a memory is allowed. Then, we modify the usual class IL memory component. Similar to existing works, a first memory stores exemplar images of past classes. A second memory is introduced here to store past class statistics obtained when they were initially learned. The intuition here is that classes are best modeled when all their data are available and that their initial statistics are useful across different incremental states. A prediction bias towards newly learned classes appears during inference because the dataset is imbalanced in their favor. The challenge is to make predictions of new and past classes more comparable. To do this, scores of past classes are rectified by leveraging contents from both memories. The method has negligible added cost, both in terms of memory and of inference complexity. Experiments with three large public datasets show that the proposed approach is more effective than a range of competitive state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009019/,"['Task analysis', 'Feature extraction', 'Tuning', 'Adaptation models', 'Training', 'Data models', 'Computer architecture']","['Incremental Learning', 'Class-incremental Learning', 'Dual Memory', 'Fine-tuned', 'Classification Score', 'Prediction Bias', 'Combination Of Classifiers', 'Catastrophic Forgetting', 'Distillation Loss', 'Training Data', 'Deep Neural Network', 'Prediction Score', 'Deep Architecture', 'Vanilla', 'Imbalanced Datasets', 'Deep Neural Network Architecture', 'Memory Size', 'Strong Baseline', 'Past Memories', 'Memory Constraints']",,166,"This paper presents a class incremental learning (IL) method which exploits fine tuning and a dual memory to reduce the negative effect of catastrophic forgetting in image recognition. First, we simplify the current fine tuning based approaches which use a combination of classification and distillation losses to compensate for the limited availability of past data. We find that the distillation term actually hurts performance when a memory is allowed. Then, we modify the usual class IL memory component. Similar to existing works, a first memory stores exemplar images of past classes. A second memory is introduced here to store past class statistics obtained when they were initially learned. The intuition here is that classes are best modeled when all their data are available and that their initial statistics are useful across different incremental states. A prediction bias towards newly learned classes appears during inference because the dataset is imbalanced in their favor. The challenge is to make predictions of new and past classes more comparable. To do this, scores of past classes are rectified by leveraging contents from both memories. The method has negligible added cost, both in terms of memory and of inference complexity. Experiments with three large public datasets show that the proposed approach is more effective than a range of competitive state-of-the-art methods."
IMP: Instance Mask Projection for High Accuracy Semantic Segmentation of Things,"Cheng-Yang Fu, Tamara L. Berg, Alexander C. Berg",Facebook AI,0.0,,100.0,USA,"In this work, we present a new operator, called Instance Mask Projection (IMP), which projects a predicted instance segmentation as a new feature for semantic segmentation. It also supports back propagation and is trainable end-to end. By adding this operator, we introduce a new way to combine top-down and bottom-up information in semantic segmentation. Our experiments show the effectiveness of IMP on both clothing parsing (with complex layering, large deformations, and non-convex objects), and on street scene segmentation (with many overlapping instances and small objects). On the Varied Clothing Parsing dataset (VCP), we show instance mask projection can improve mIOU by 3 points over a state-of-the-art Panoptic FPN segmentation approach. On the ModaNet clothing parsing dataset, we show a dramatic improvement of 20.4% compared to existing baseline semantic segmentation results. In addition, the Instance Mask Projection operator works well on other (non-clothing) datasets, providing an improvement in mIOU of 3 points on ""thing"" classes of Cityscapes, a self-driving dataset, over a state-of-the-art approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fu_IMP_Instance_Mask_Projection_for_High_Accuracy_Semantic_Segmentation_of_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_IMP_Instance_Mask_Projection_for_High_Accuracy_Semantic_Segmentation_of_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010836/,"['Semantics', 'Feature extraction', 'Convolution', 'Clothing', 'Image segmentation', 'Task analysis', 'Object detection']","['Semantic Segmentation', 'Instance Masks', 'Layering', 'Small Objects', 'Segmentation Results', 'Top-down And Bottom-up', 'Projection Operator', 'Instance Segmentation', 'Feature Pyramid Network', 'Top-down Information', 'Semantic Segmentation Results', 'Learning Rate', 'Object Detection', 'Data Augmentation', 'Bounding Box', 'Color Difference', 'Graphical Model', 'Object Location', 'Visual Search', 'Group Norms', 'Mask R-CNN', 'Semantic Segmentation Models', 'Dilated Convolution', 'Fully Convolutional Network', 'Semantic Segmentation Methods']",,14,"In this work, we present a new operator, called Instance Mask Projection (IMP), which projects a predicted instance segmentation as a new feature for semantic segmentation. It also supports back propagation and is trainable end-to end. By adding this operator, we introduce a new way to combine top-down and bottom-up information in semantic segmentation. Our experiments show the effectiveness of IMP on both clothing parsing (with complex layering, large deformations, and non-convex objects), and on street scene segmentation (with many overlapping instances and small objects). On the Varied Clothing Parsing dataset (VCP), we show instance mask projection can improve mIOU by 3 points over a state-of-the-art Panoptic FPN segmentation approach. On the ModaNet clothing parsing dataset, we show a dramatic improvement of 20.4% compared to existing baseline semantic segmentation results. In addition, the Instance Mask Projection operator works well on other (non-clothing) datasets, providing an improvement in mIOU of 3 points on “thing” classes of Cityscapes, a self-driving dataset, over a state-of-the-art approach."
"Identity From Here, Pose From There: Self-Supervised Disentanglement and Generation of Objects Using Unlabeled Videos","Fanyi Xiao, Haotian Liu, Yong Jae Lee","University of California, Davis",100.0,usa,0.0,,"We propose a novel approach that disentangles the identity and pose of objects for image generation. Our model takes as input an ID image and a pose image, and generates an output image with the identity of the ID image and the pose of the pose image. Unlike most previous unsupervised work which rely on cyclic constraints, which can often be brittle, we instead propose to learn this in a self-supervised way. Specifically, we leverage unlabeled videos to automatically construct pseudo ground-truth targets to directly supervise our model. To enforce disentanglement, we propose a novel disentanglement loss, and to improve realism, we propose a pixel-verification loss in which the generated image's pixels must trace back to the ID input. We conduct extensive experiments on both synthetic and real images to demonstrate improved realism, diversity, and ID/pose disentanglement compared to existing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiao_Identity_From_Here_Pose_From_There_Self-Supervised_Disentanglement_and_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiao_Identity_From_Here_Pose_From_There_Self-Supervised_Disentanglement_and_Generation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010624/,"['Videos', 'Shape', 'Training', 'Task analysis', 'Generators', 'Automobiles', 'Public transportation']","['Unlabeled Videos', 'Object Generation', 'Object Pose', 'Ground Truth Target', 'Feature Maps', 'Generative Adversarial Networks', 'Video Clips', 'Target Image', 'Reference Image', 'Composite Image', 'Nearest Neighbor Search', 'Identical Images', 'Latent Code', 'Fr√©chet Inception Distance', 'Simple Datasets', 'View Synthesis', 'Pose Changes', 'Supervisory Signal', 'Disentangled Representation']",,12,"We propose a novel approach that disentangles the identity and pose of objects for image generation. Our model takes as input an ID image and a pose image, and generates an output image with the identity of the ID image and the pose of the pose image. Unlike most previous unsupervised work which rely on cyclic constraints, which can often be brittle, we instead propose to learn this in a self-supervised way. Specifically, we leverage unlabeled videos to automatically construct pseudo ground-truth targets to directly supervise our model. To enforce disentanglement, we propose a novel disentanglement loss, and to improve realism, we propose a pixel-verification loss in which the generated image's pixels must trace back to the ID input. We conduct extensive experiments on both synthetic and real images to demonstrate improved realism, diversity, and ID/pose disentanglement compared to existing methods."
"Image Aesthetic Assessment Based on Pairwise Comparison Â­ A Unified Approach to Score Regression, Binary Classification, and Personalization","Jun-Tae Lee, Chang-Su Kim","School of Electrical Engineering, Korea University, Seoul, Korea",100.0,South Korea,0.0,,"We propose a unified approach to three tasks of aesthetic score regression, binary aesthetic classification, and personalized aesthetics. First, we develop a comparator to estimate the ratio of aesthetic scores for two images. Then, we construct a pairwise comparison matrix for multiple reference images and an input image, and predict the aesthetic score of the input via the eigenvalue decomposition of the matrix. By varying the reference images, the proposed algorithm can be used for binary aesthetic classification and personalized aesthetics, as well as generic score regression. Experimental results demonstrate that the proposed unified algorithm provides the state-of-the-art performances in all three tasks of image aesthetics.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Image_Aesthetic_Assessment_Based_on_Pairwise_Comparison__A_Unified_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Image_Aesthetic_Assessment_Based_on_Pairwise_Comparison__A_Unified_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
Image Generation From Small Datasets via Batch Statistics Adaptation,"Atsuhiro Noguchi, Tatsuya Harada","The University of Tokyo; The University of Tokyo, RIKEN",100.0,"Japan, japan",0.0,,"Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small ( 100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain. Code is available at github.com/nogu-atsu/small-dataset-image-generation",,http://openaccess.thecvf.com/content_ICCV_2019/html/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.pdf,,github.com/nogu-atsu/small-dataset-image-generation,,main,Poster,https://ieeexplore.ieee.org/document/9009051/,"['Generators', 'Training', 'Data models', 'Gallium nitride', 'Adaptation models', 'Feature extraction', 'Convolution']","['Small Datasets', 'Image Generation', 'Batch Statistics', 'Large Datasets', 'Target Domain', 'Deep Generative Models', 'Generation Layer', 'Training Data', 'Convolutional Neural Network', 'Image Dataset', 'Scale Parameter', 'Dataset Size', 'Autoregressive Model', 'Transfer Learning', 'Batch Normalization', 'Generative Adversarial Networks', 'General Parameters', 'Pixel Level', 'Semi-supervised Learning', 'Variational Autoencoder', 'Fr√©chet Inception Distance', 'Shift Parameter', 'Latent Vector', 'Source Domain', 'Face Dataset', 'Neural Net', 'Target Dataset', 'Passiflora', 'Kernel Parameters', 'Trainable Parameters']",,107,"Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain. Code is available at github.com/nogu-atsu/small-dataset-image-generation."
Image Inpainting With Learnable Bidirectional Attention Maps,"Chaohao Xie, Shaohui Liu, Chao Li, Ming-Ming Cheng, Wangmeng Zuo, Xiao Liu, Shilei Wen, Errui Ding","Harbin Institute of Technology; Nankai University; Department of Computer Vision Technology (VIS), Baidu Inc.; Harbin Institute of Technology, Peng Cheng Laboratory, Shenzhen",100.0,"China, china",0.0,,"Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial convolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature re-normalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available at: https://github.com/Vious/LBAM_inpainting/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf,,https://github.com/Vious/LBAM_inpainting/,,main,Poster,https://ieeexplore.ieee.org/document/9010337/,"['Convolution', 'Decoding', 'Frequency modulation', 'Image color analysis', 'Semantics', 'Training', 'Image reconstruction']","['Attention Map', 'Image Inpainting', 'Convolutional Layers', 'Plausible Results', 'Semantic', 'Activation Function', 'Input Image', 'Feature Maps', 'Hallucinations', 'Reconstruction Loss', 'CNN-based Methods', 'Encoder Layer', 'Feature Encoder', 'Attention Layer', 'Convolutional Features', 'Perceptual Loss', 'Decoder Layer', 'Semantic Structure', 'Texture Details', 'Convolutional Feature Maps', 'Patch Matching', 'Decoder Features', 'Reverse Mapping', 'Forward Mapping', 'User Study', 'Layers Of VGG16', 'Low-level Vision', 'Visual Quality', 'Feature Map Size']",,156,"Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial convolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature re-normalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available at: https://github.com/Vious/LBAM_inpainting/."
Image Synthesis From Reconfigurable Layout and Style,"Wei Sun, Tianfu Wu","Department of ECE and the Visual Narrative Initiative, North Carolina State University",100.0,USA,0.0,,"Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a long- standing problem to learn generative models that are capable of synthesizing realistic and sharp images from re- configurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout- and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at https://github.com/iVMCL/LostGANs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Image_Synthesis_From_Reconfigurable_Layout_and_Style_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Image_Synthesis_From_Reconfigurable_Layout_and_Style_ICCV_2019_paper.pdf,,https://github.com/iVMCL/LostGANs,,main,Poster,https://ieeexplore.ieee.org/document/9008768/,"['Layout', 'Image generation', 'Generators', 'Gallium nitride', 'Task analysis', 'Semantics', 'Image resolution']","['Image Synthesis', 'Reconfigurable Layout', 'Bounding Box', 'Multiple Images', 'Generative Adversarial Networks', 'Genomic Datasets', 'Remarkable Progress', 'Latent Vector', 'Image Sharpness', 'Latent Code', 'StyleGAN', 'Classification Accuracy', 'Deep Neural Network', 'Standard Normal Distribution', 'Solution Space', 'Image Generation', 'Objective Information', 'Fully-connected Layer', 'Textual Descriptions', 'Affine Transformation', 'Fr√©chet Inception Distance', 'Diversity Score', 'Scene Graph', 'Style Image', 'Global Average Pooling Layer', 'Object Instances', 'Beta Parameters', 'Noise Matrix', 'Linear Projection', 'Output Units']",,69,"Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a long- standing problem to learn generative models that are capable of synthesizing realistic and sharp images from re- configurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout- and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at https://github.com/iVMCL/LostGANs."
Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?,"Rameen Abdal, Yipeng Qin, Peter Wonka",KAUST,100.0,saudi arabia,0.0,,"We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHD dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008515/,"['Face', 'Gallium nitride', 'Neural networks', 'Image resolution', 'Automobiles', 'Computer vision', 'Task analysis']","['Latent Space', 'Latent Space Of StyleGAN', 'Image Classification', 'Efficient Algorithm', 'Structure Of Space', 'Morphing', 'Style Transfer', 'Image Editing', 'Embedding Algorithm', 'High-quality', 'Loss Function', 'Neural Network', 'Computer Vision', 'Feature Space', 'Input Image', 'Types Of Images', 'Linear Interpolation', 'Design Choices', 'Face Images', 'Optimization Step', 'Latent Code', 'Perceptual Loss', 'Human Faces', 'Latent Vector', 'Pixel Spacing', 'Computer Graphics', 'Random Initialization', 'Affine Transformation', 'Computer Vision Applications', 'Image Generation']",,626,"We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHD dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful."
Imitation Learning for Human Pose Prediction,"Borui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang, Juan Carlos Niebles",Stanford University,100.0,usa,0.0,,"Modeling and prediction of human motion dynamics has long been a challenging problem in computer vision, and most existing methods rely on the end-to-end supervised training of various architectures of recurrent neural networks. Inspired by the recent success of deep reinforcement learning methods, in this paper we propose a new reinforcement learning formulation for the problem of human pose prediction, and develop an imitation learning algorithm for predicting future poses under this formulation through a combination of behavioral cloning and generative adversarial imitation learning. Our experiments show that our proposed method outperforms all existing state-of-the-art baseline models by large margins on the task of human pose prediction in both short-term predictions and long-term predictions, while also enjoying huge advantage in training speed.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Imitation_Learning_for_Human_Pose_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Imitation_Learning_for_Human_Pose_Prediction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009816/,['Computer vision'],"['Human Pose', 'Imitation Learning', 'Pose Prediction', 'Neural Network', 'Learning Algorithms', 'Computer Vision', 'Recurrent Neural Network', 'Generative Adversarial Networks', 'Large Margin', 'Prediction Task', 'Prediction Problem', 'Training Speed', 'Deep Reinforcement Learning', 'Human Motion', 'Advantages Of Speed', 'Long-term Prediction', 'Reinforcement Learning Methods', 'Short-term Prediction', 'Combination Of Learning', 'Long Short-term Memory Cell', 'Wasserstein Generative Adversarial Networks', 'Markov Decision Process', 'Proximal Policy Optimization', 'Reinforcement Learning Problem', 'Expert Demonstrations', 'Gated Recurrent Unit', 'Inverse Reinforcement Learning', 'State-action Pair', 'Deterministic Policy']",,73,"Modeling and prediction of human motion dynamics has long been a challenging problem in computer vision, and most existing methods rely on the end-to-end supervised training of various architectures of recurrent neural networks. Inspired by the recent success of deep reinforcement learning methods, in this paper we propose a new reinforcement learning formulation for the problem of human pose prediction, and develop an imitation learning algorithm for predicting future poses under this formulation through a combination of behavioral cloning and generative adversarial imitation learning. Our experiments show that our proposed method outperforms all existing state-of-the-art baseline models by large margins on the task of human pose prediction in both short-term predictions and long-term predictions, while also enjoying huge advantage in training speed."
Implicit Surface Representations As Layers in Neural Networks,"Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack, Mahsa Baktashmotlagh, Anders Eriksson","School of Electrical Engineering and Computer Science, Queensland University of Technology; School of Information Technology and Electrical Engineering, University of Queensland",100.0,australia,0.0,,"Implicit shape representations, such as Level Sets, provide a very elegant formulation for performing computations involving curves and surfaces. However, including implicit representations into canonical Neural Network formulations is far from straightforward. This has consequently restricted existing approaches to shape inference, to significantly less effective representations, perhaps most commonly voxels occupancy maps or sparse point clouds. To overcome this limitation we propose a novel formulation that permits the use of implicit representations of curves and surfaces, of arbitrary topology, as individual layers in Neural Network architectures with end-to-end trainability. Specifically, we propose to represent the output as an oriented level set of a continuous and discretised embedding function. We investigate the benefits of our approach on the task of 3D shape prediction from a single image; and demonstrate its ability to produce a more accurate reconstruction compared to voxel-based representations. We further show that our model is flexible and can be applied to a variety of shape inference problems.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Michalkiewicz_Implicit_Surface_Representations_As_Layers_in_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Michalkiewicz_Implicit_Surface_Representations_As_Layers_in_Neural_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010266/,"['Three-dimensional displays', 'Shape', 'Level set', 'Image reconstruction', 'Neural networks', 'Computer architecture', 'Geometry']","['Neural Network', 'Neural Network Layers', 'Surface Representation', 'Implicit Surface', 'Implicit Surface Representation', 'Discretion', 'Single Image', 'Point Cloud', 'Neural Architecture', 'Reconstruction Accuracy', '3D Shape', 'Individual Layers', 'Use Of Representations', 'Shape Representation', 'Implicit Representation', 'Sparse Point Cloud', 'Occupancy Map', 'Loss Function', 'Convolutional Neural Network', 'Convolutional Layers', 'Level Set Method', 'Graph Convolutional Network', 'Chamfer Distance', 'Structure From Motion', 'Simple Architecture', 'Deep Architecture', 'Intersection Over Union', '3D Mesh', 'Signed Distance Function', '3D Reconstruction']",,147,"Implicit shape representations, such as Level Sets, provide a very elegant formulation for performing computations involving curves and surfaces. However, including implicit representations into canonical Neural Network formulations is far from straightforward. This has consequently restricted existing approaches to shape inference, to significantly less effective representations, perhaps most commonly voxels occupancy maps or sparse point clouds. To overcome this limitation we propose a novel formulation that permits the use of implicit representations of curves and surfaces, of arbitrary topology, as individual layers in Neural Network architectures with end-to-end trainability. Specifically, we propose to represent the output as an oriented level set of a continuous and discretised embedding function. We investigate the benefits of our approach on the task of 3D shape prediction from a single image; and demonstrate its ability to produce a more accurate reconstruction compared to voxel-based representations. We further show that our model is flexible and can be applied to a variety of shape inference problems."
Improved Conditional VRNNs for Video Prediction,"Lluis Castrejon, Nicolas Ballas, Aaron Courville","Mila, Universit ´e de Montr ´eal; CIFAR, Mila, Universit ´e de Montr ´eal; Facebook AI Research",66.66666666666666,canada,33.33333333333334,Canada,"Predicting future frames for a video sequence is a challenging generative modeling task. Promising approaches include probabilistic latent variable models such as the Variational Auto-Encoder. While VAEs can handle uncertainty and model multiple possible future outcomes, they have a tendency to produce blurry predictions. In this work we argue that this is a sign of underfitting. To address this issue, we propose to increase the expressiveness of the latent distributions and to use higher capacity likelihood models. Our approach relies on a hierarchy of latent variables, which defines a family of flexible prior and posterior distributions in order to better model the probability of future sequences. We validate our proposal through a series of ablation experiments and compare our approach to current state-of-the-art latent variable models. Our method performs favorably under several metrics in three different datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Castrejon_Improved_Conditional_VRNNs_for_Video_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Castrejon_Improved_Conditional_VRNNs_for_Video_Prediction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008382/,"['Predictive models', 'Data models', 'Training', 'Stochastic processes', 'Uncertainty', 'Computational modeling', 'Measurement']","['Video Prediction', 'Variational Recurrent Neural Network', 'Posterior Probability', 'Latent Variables', 'Latent Model', 'Likelihood Model', 'Latent Variable Model', 'Variational Autoencoder', 'Latent Distribution', 'Future Frames', 'Structural Similarity', 'Mean Square Error', 'Feature Maps', 'Hierarchical Model', 'Single Level', 'Recurrent Neural Network', 'Parametrized', 'Test Sequences', 'Levels Of Hierarchy', 'Peak Signal-to-noise Ratio', 'Evidence Lower Bound', 'Dense Connections', 'Variational Autoencoder Model', 'Spatial Topology', 'Decoder Layer', 'Contextual Framing', 'Latent Level', 'Robotic Arm', 'Future Uncertainty', 'Top Level']",,78,"Predicting future frames for a video sequence is a challenging generative modeling task. Promising approaches include probabilistic latent variable models such as the Variational Auto-Encoder. While VAEs can handle uncertainty and model multiple possible future outcomes, they have a tendency to produce blurry predictions. In this work we argue that this is a sign of underfitting. To address this issue, we propose to increase the expressiveness of the latent distributions and to use higher capacity likelihood models. Our approach relies on a hierarchy of latent variables, which defines a family of flexible prior and posterior distributions in order to better model the probability of future sequences. We validate our proposal through a series of ablation experiments and compare our approach to current state-of-the-art latent variable models. Our method performs favorably under several metrics in three different datasets."
Improved Techniques for Training Adaptive Deep Networks,"Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, Gao Huang",Tsinghua University; University of Oxford; Baidu Inc.,66.66666666666666,"China, uk",33.33333333333334,China,"Adaptive inference is a promising technique to improve the computational efficiency of deep models at test time. In contrast to static models which use the same computation graph for all instances, adaptive networks can dynamically adjust their structure conditioned on each input. While existing research on adaptive inference mainly focuses on designing more advanced architectures, this paper investigates how to train such networks more effectively. Specifically, we consider a typical adaptive deep network with multiple intermediate classifiers. We present three techniques to improve its training efficacy from two aspects: 1) a Gradient Equilibrium algorithm to resolve the conflict of learning of different classifiers; 2) an Inline Subnetwork Collaboration approach and a One-for-all Knowledge Distillation algorithm to enhance the collaboration among classifiers. On multiple datasets (CIFAR-10, CIFAR-100 and ImageNet), we show that the proposed approach consistently leads to further improved efficiency on top of state-of-the-art adaptive deep networks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Improved_Techniques_for_Training_Adaptive_Deep_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Improved_Techniques_for_Training_Adaptive_Deep_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010043/,"['Adaptive systems', 'Adaptation models', 'Training', 'Computational modeling', 'Collaboration', 'Computer architecture', 'Knowledge transfer']","['Deep Network', 'ImageNet', 'Learning Classifiers', 'Intermediate Class', 'Neural Network', 'Convolutional Neural Network', 'Validation Set', 'Knowledge Transfer', 'Teacher Model', 'Training Strategy', 'Stochastic Gradient Descent', 'Kullback-Leibler', 'Adaptive Model', 'Amount Of Computation', 'Validation Accuracy', 'Adaptive Learning', 'Adaptive Selection', 'Network Depth', 'Sufficient Confidence', 'Time Budget', 'Inference Stage', 'Student Model', 'Multi-scale Network']",,78,"Adaptive inference is a promising technique to improve the computational efficiency of deep models at test time. In contrast to static models which use the same computation graph for all instances, adaptive networks can dynamically adjust their structure conditioned on each input. While existing research on adaptive inference mainly focuses on designing more advanced architectures, this paper investigates how to train such networks more effectively. Specifically, we consider a typical adaptive deep network with multiple intermediate classifiers. We present three techniques to improve its training efficacy from two aspects: 1) a Gradient Equilibrium algorithm to resolve the conflict of learning of different classifiers; 2) an Inline Subnetwork Collaboration approach and a One-for-all Knowledge Distillation algorithm to enhance the collaboration among classifiers. On multiple datasets (CIFAR-10, CIFAR-100 and ImageNet), we show that the proposed approach consistently leads to further improved efficiency on top of state-of-the-art adaptive deep networks."
Improving Adversarial Robustness via Guided Complement Entropy,"Hao-Yun Chen, Jhao-Hong Liang, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan","Department of Computer Science, National Tsing-Hua University, Hsinchu, Taiwan; Electronic and Optoelectronic System Research Laboratories, ITRI, Hsinchu, Taiwan; Google Research, Mountain View, CA, USA; Department of Computer Science, National Tsing-Hua University, Hsinchu, Taiwan",50.0,Taiwan,50.0,Taiwan,"Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving ""adversarial defense for free,"" which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a ""guided"" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009563/,"['Robustness', 'Training', 'Entropy', 'Computational modeling', 'Analytical models', 'Predictive models', 'Xenon']","['Adversarial Robustness', 'Model Performance', 'Additional Procedures', 'Training Objective', 'Adversarial Training', 'Incorrect Classification', 'Ground-truth Class', 'Loss Function', 'Loss Value', 'Jacobian Matrix', 'Training Tasks', 'Maximization Problem', 'Output Probability', 'Deep Neural Model', 'Saliency Map', 'Adversarial Attacks', 'Additional Computational Cost', 'Adversarial Examples', 'Log Loss', 'Projected Gradient Descent', 'Fast Gradient Sign Method', 'White-box Attack', 'Empirical Risk Minimization', 'Properties Of Entropy']",,30,"Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving ""adversarial defense for free,"" which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a ""guided"" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance."
Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization,"Chufeng Tang, Lu Sheng, Zhaoxiang Zhang, Xiaolin Hu","Institute of Automation, Chinese Academy of Sciences; College of Software, Beihang University; State Key Laboratory of Intelligent Technology and Systems, Institute for Artificial Intelligence, Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University",100.0,"China, china",0.0,,"Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to define local feature areas. We propose a flexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-specific localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_Improving_Pedestrian_Attribute_Recognition_With_Weakly-Supervised_Multi-Scale_Attribute-Specific_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_Improving_Pedestrian_Attribute_Recognition_With_Weakly-Supervised_Multi-Scale_Attribute-Specific_Localization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010997/,"['Feature extraction', 'Semantics', 'Computer architecture', 'Image recognition', 'Visualization', 'Video surveillance', 'Task analysis']","['Attribute Recognition', 'Pedestrian Attribute', 'Pedestrian Attribute Recognition', 'Local Features', 'Attention Mechanism', 'Visual Attention', 'Local Module', 'Feature Pyramid', 'Video Surveillance', 'Annotated Regions', 'Discriminative Regions', 'High-level Semantics', 'Flexible Module', 'Deep Supervision', 'Visual Attention Mechanism', 'Convolutional Neural Network', 'Input Image', 'F1 Score', 'Feature Maps', 'Multi-label', 'Spatial Transformer Network', 'Bounding Box', 'Feature Pyramid Network', 'Attention Regions', 'Voting Scheme', 'Pose Estimation', 'Holistic Method', 'Lateral Connections', 'Rigid Parts', 'Local Learning']",,74,"Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to define local feature areas. We propose a flexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-specific localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K."
InGAN: Capturing and Retargeting the âDNAâ of a Natural Image,"Assaf Shocher, Shai Bagon, Phillip Isola, Michal Irani","Dept. of Computer Science and Applied Math, The Weizmann Institute of Science; Computer Science and Artiﬁcial Intelligence Lab Massachusetts Institute of Technology; Weizmann Artiﬁcial Intelligence Center (WAIC)",66.66666666666666,"israel, usa",33.33333333333334,USA,"Generative Adversarial Networks (GANs) typically learn a distribution of images in a large image dataset, and are then able to generate new images from this distribution. However, each natural image has its own internal statistics, captured by its unique distribution of patches. In this paper we propose an ""Internal GAN"" (InGAN) -- an image-specific GAN -- which trains on a single input image and learns its internal distribution of patches. It is then able to synthesize a plethora of new natural images of significantly different sizes, shapes and aspect-ratios - all with the same internal patch-distribution (same ""DNA"") as the input image. In particular, despite large changes in global size/shape of the image, all elements inside the image maintain their local size/shape. InGAN is fully unsupervised, requiring no additional data other than the input image itself. Once trained on the input image, it can remap the input to any size or shape in a single feedforward pass, while preserving the same internal patch distribution. InGAN provides a unified framework for a variety of tasks, bridging the gap between textures and natural images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shocher_InGAN_Capturing_and_Retargeting_the_DNA_of_a_Natural_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shocher_InGAN_Capturing_and_Retargeting_the_DNA_of_a_Natural_Image_ICCV_2019_paper.pdf,http://www.wisdom.weizmann.ac.il/~vision/ingan/,,,main,Oral,,,,,,
Incremental Class Discovery for Semantic Segmentation With RGBD Sensing,"Yoshikatsu Nakajima, Byeongkeun Kang, Hideo Saito, Kris Kitani",Carnegie Mellon University and Keio University; Carnegie Mellon University; Keio University,100.0,"japan, usa",0.0,,"This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at 10.7 Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nakajima_Incremental_Class_Discovery_for_Semantic_Segmentation_With_RGBD_Sensing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nakajima_Incremental_Class_Discovery_for_Semantic_Segmentation_With_RGBD_Sensing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009083/,"['Three-dimensional displays', 'Image segmentation', 'Semantics', 'Image reconstruction', 'Two dimensional displays', 'Buildings', 'Image color analysis']","['Semantic Segmentation', 'Processing Time', 'Image Segmentation', 'Density Map', 'Segmentation Method', 'Object Classification', 'Geometric Information', 'Segmentation Map', 'Color Information', 'Unseen Classes', 'Convolutional Neural Network', '3D Reconstruction', 'Feature Dimension', 'Geometric Features', 'Classifier Training', 'Classification Datasets', 'Deep Features', 'Memory Usage', 'Sequence Of Frames', 'Subset Of Classes', '3D Segmentation', 'Local Reference Frame', 'Segmentation Feature', 'RGB-D Images', 'Unseen Objects', '3D Clusters', 'Semantic Map', 'Memory Footprint']",,11,"This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at 10.7 Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component."
Incremental Learning Using Conditional Adversarial Networks,"Ye Xiang, Ying Fu, Pan Ji, Hua Huang",Beijing Institute of Technology; NEC Labs America,50.0,China,50.0,USA,"Incremental learning using Deep Neural Networks (DNNs) suffers from catastrophic forgetting. Existing methods mitigate it by either storing old image examples or only updating a few fully connected layers of DNNs, which, however, requires large memory footprints or hurts the plasticity of models. In this paper, we propose a new incremental learning strategy based on conditional adversarial networks. Our new strategy allows us to use memory-efficient statistical information to store old knowledge, and fine-tune both convolutional layers and fully connected layers to consolidate new knowledge. Specifically, we propose a model consisting of three parts, i.e., a base sub-net, a generator, and a discriminator. The base sub-net works as a feature extractor which can be pre-trained on large scale datasets and shared across multiple image recognition tasks. The generator conditioned on labeled embeddings aims to construct pseudo-examples with the same distribution as the old data. The discriminator combines real-examples from new data and pseudo-examples generated from the old data distribution to learn representation for both old and new classes. Through adversarial training of the discriminator and generator, we accomplish the multiple continuous incremental learning. Comparison with the state-of-the-arts on public CIFAR-100 and CUB-200 datasets shows that our method achieves the best accuracies on both old and new classes while requiring relatively less memory storage.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009031/,"['Generators', 'Gallium nitride', 'Training', 'Task analysis', 'Convolution', 'Support vector machines', 'Computer architecture']","['Incremental Learning', 'Conditional Adversarial Networks', 'Large Datasets', 'Deep Network', 'Deep Neural Network', 'Convolutional Layers', 'Public Datasets', 'Large-scale Datasets', 'Fully-connected Layer', 'Discrimination Training', 'Catastrophic Forgetting', 'Loss Function', 'Imaging Data', 'Data Storage', 'Test Accuracy', 'Stochastic Gradient Descent', 'Generative Adversarial Networks', 'Normalization Layer', 'Class Information', 'Classification Loss', 'Generator Training', 'Incremental Model', 'Reconstruction Loss', 'Convolutional Feature Maps', 'Auxiliary Parameters', 'Intermediate Features', 'Performance Of Different Methods', 'Subset Of Images', 'Convolutional Generative Adversarial Network', 'Embedding Vectors']",,91,"Incremental learning using Deep Neural Networks (DNNs) suffers from catastrophic forgetting. Existing methods mitigate it by either storing old image examples or only updating a few fully connected layers of DNNs, which, however, requires large memory footprints or hurts the plasticity of models. In this paper, we propose a new incremental learning strategy based on conditional adversarial networks. Our new strategy allows us to use memory-efficient statistical information to store old knowledge, and fine-tune both convolutional layers and fully connected layers to consolidate new knowledge. Specifically, we propose a model consisting of three parts, i.e., a base sub-net, a generator, and a discriminator. The base sub-net works as a feature extractor which can be pre-trained on large scale datasets and shared across multiple image recognition tasks. The generator conditioned on labeled embeddings aims to construct pseudo-examples with the same distribution as the old data. The discriminator combines real-examples from new data and pseudo-examples generated from the old data distribution to learn representation for both old and new classes. Through adversarial training of the discriminator and generator, we accomplish the multiple continuous incremental learning. Comparison with the state-of-the-arts on public CIFAR-100 and CUB-200 datasets shows that our method achieves the best accuracies on both old and new classes while requiring relatively less memory storage."
Indices Matter: Learning to Index for Deep Image Matting,"Hao Lu, Yutong Dai, Chunhua Shen, Songcen Xu","Noah’s Ark Lab, Huawei Technologies; The University of Adelaide, Australia",50.0,australia,50.0,China,"We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of 'learning to index', and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the pooling and upsampling operators, without extra training supervision. At the core of this framework is a flexible network module, termed IndexNet, which dynamically generates indices conditioned on the feature map. Due to its flexibility, IndexNet can be used as a plug-in applying to almost all off-the-shelf convolutional networks that have coupled downsampling and upsampling stages. We demonstrate the effectiveness of IndexNet on the task of natural image matting where the quality of learned indices can be visually observed from predicted alpha mattes. Results on the Composition-1k matting dataset show that our model built on MobileNetv2 exhibits at least 16.1% improvement over the seminal VGG-16 based deep matting baseline, with less training data and lower model capacity. Code and models have been made available at: https://tinyurl.com/IndexNetV1.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lu_Indices_Matter_Learning_to_Index_for_Deep_Image_Matting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Indices_Matter_Learning_to_Index_for_Deep_Image_Matting_ICCV_2019_paper.pdf,,https://tinyurl.com/IndexNetV1,,main,Poster,https://ieeexplore.ieee.org/document/9008779/,"['Indexes', 'Interpolation', 'Task analysis', 'Decoding', 'Semantics', 'Image resolution', 'Deconvolution']","['Image Matting', 'Convolutional Network', 'Feature Maps', 'Bilinear Interpolation', 'Notion Of Function', 'Flexible Module', 'Core Framework', 'Alpha Matte', 'Convolutional Neural Network', 'Deep Network', 'Deconvolution', 'Convolutional Layers', 'Dynamic Network', 'Visual Task', 'Deep Convolutional Neural Network', 'Max-pooling', 'Pooling Layer', 'Semantic Segmentation', 'Index Function', 'Depth Estimation', 'Learning Index', 'Nearest Neighbor Interpolation', 'Sum Of Absolute Differences', 'Max Function', 'Feature Map Size', 'Semantic Segmentation Models', 'Spatial Index', 'Standard Configuration', 'Pooling Operation', 'Local Index']",,121,"We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of 'learning to index', and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the pooling and upsampling operators, without extra training supervision. At the core of this framework is a flexible network module, termed IndexNet, which dynamically generates indices conditioned on the feature map. Due to its flexibility, IndexNet can be used as a plug-in applying to almost all off-the-shelf convolutional networks that have coupled downsampling and upsampling stages. We demonstrate the effectiveness of IndexNet on the task of natural image matting where the quality of learned indices can be visually observed from predicted alpha mattes. Results on the Composition-1k matting dataset show that our model built on MobileNetv2 exhibits at least 16.1% improvement over the seminal VGG-16 based deep matting baseline, with less training data and lower model capacity. Code and models have been made available at: https://tinyurl.com/IndexNetV1."
Information Entropy Based Feature Pooling for Convolutional Neural Networks,"Weitao Wan, Jiansheng Chen, Tianpeng Li, Yiqing Huang, Jingqi Tian, Cheng Yu, Youze Xue","Department of Electronic Engineering, Tsinghua University",100.0,China,0.0,,"In convolutional neural networks (CNNs), we propose to estimate the importance of a feature vector at a spatial location in the feature maps by the network's uncertainty on its class prediction, which can be quantified using the information entropy. Based on this idea, we propose the entropy-based feature weighting method for semantics-aware feature pooling which can be readily integrated into various CNN architectures for both training and inference. We demonstrate that such a location-adaptive feature weighting mechanism helps the network to concentrate on semantically important image regions, leading to improvements in the large-scale classification and weakly-supervised semantic segmentation tasks. Furthermore, the generated feature weights can be utilized in visual tasks such as weakly-supervised object localization. We conduct extensive experiments on different datasets and CNN architectures, outperforming recently proposed pooling methods and attention mechanisms in ImageNet classification as well as achieving state-of-the-arts in weakly-supervised semantic segmentation on PASCAL VOC 2012 dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wan_Information_Entropy_Based_Feature_Pooling_for_Convolutional_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wan_Information_Entropy_Based_Feature_Pooling_for_Convolutional_Neural_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010725/,"['Entropy', 'Computer architecture', 'Semantics', 'Image segmentation', 'Task analysis', 'Feature extraction', 'Training']","['Convolutional Neural Network', 'Information Entropy', 'Feature Pooling', 'Classification Task', 'Feature Maps', 'Image Regions', 'Attention Mechanism', 'Important Regions', 'Class Prediction', 'Semantic Segmentation', 'Convolutional Neural Network Architecture', 'Feature Weights', 'ImageNet Classification', 'Validation Set', 'Classification Performance', 'Convolutional Layers', 'Large-scale Datasets', 'Stochastic Gradient Descent', 'Training Images', 'Pooling Layer', 'Global Average Pooling Layer', 'Intermediate Feature Maps', 'Class Activation Maps', 'Convolutional Feature Maps', 'FC Layer', 'Entropy Weight', 'Intermediate Layer', 'Convolutional Neural Networks Backbone', 'Global Average Pooling', 'Image-level Labels']",,20,"In convolutional neural networks (CNNs), we propose to estimate the importance of a feature vector at a spatial location in the feature maps by the network's uncertainty on its class prediction, which can be quantified using the information entropy. Based on this idea, we propose the entropy-based feature weighting method for semantics-aware feature pooling which can be readily integrated into various CNN architectures for both training and inference. We demonstrate that such a location-adaptive feature weighting mechanism helps the network to concentrate on semantically important image regions, leading to improvements in the large-scale classification and weakly-supervised semantic segmentation tasks. Furthermore, the generated feature weights can be utilized in visual tasks such as weakly-supervised object localization. We conduct extensive experiments on different datasets and CNN architectures, outperforming recently proposed pooling methods and attention mechanisms in ImageNet classification as well as achieving state-of-the-arts in weakly-supervised semantic segmentation on PASCAL VOC 2012 dataset."
Instance-Guided Context Rendering for Cross-Domain Person Re-Identification,"Yanbei Chen, Xiatian Zhu, Shaogang Gong",Vision Semantics Ltd.; Queen Mary University of London,50.0,uk,50.0,UK,"Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008370/,"['Cameras', 'Rendering (computer graphics)', 'Context modeling', 'Generators', 'Image generation', 'Gallium nitride', 'Data models']","['Cross-domain Person Re-identification', 'Training Data', 'Variety Of Contexts', 'Generative Adversarial Networks', 'Image Generation', 'Variety Of Domains', 'Target Domain', 'Person Image', 'Dual Network', 'Unlabeled Target Domain', 'Synthetic Training Data', 'Unlabeled Instances', 'High Diversity', 'Ablation', 'Learning Context', 'Source Images', 'Images In Set', 'Target Data', 'Synthetic Images', 'Domain Adaptation', 'Fr√©chet Inception Distance', 'Background Clutter', 'Style Transfer', 'Domain Gap', 'Diverse Outputs', 'Domain Discriminator', 'Gallery Set', 'Paired Box', 'Distribution Alignment', 'Encoder-decoder Network']",,123,"Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning."
Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression,"Kyung-Rae Kim, Whan Choi, Yeong Jun Koh, Seong-Gyun Jeong, Chang-Su Kim",CODE42.ai; Korea University; Chungnam National University,66.66666666666666,"South Korea, south korea",33.33333333333334,China,"A novel algorithm to estimate instance-level future motion in a single image is proposed in this paper. We first represent the future motion of an instance with its direction, speed, and action classes. Then, we develop a deep neural network that exploits different levels of semantic information to perform the future motion estimation. For effective future motion classification, we adopt ordinal regression. Especially, we develop the cyclic ordinal regression scheme using binary classifiers. Experiments demonstrate that the proposed algorithm provides reliable performance and thus can be used effectively for vision applications, including single and multi object tracking. Furthermore, we release the future motion (FM) dataset, collected from diverse sources and annotated manually, as a benchmark for single-image future motion estimation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009521/,"['Frequency modulation', 'Estimation', 'Reliability', 'Prediction algorithms', 'Motion estimation', 'Feature extraction', 'Task analysis']","['Single Image', 'Ordinal Regression', 'Motion Estimation', 'Future Motion', 'Deep Network', 'Deep Neural Network', 'Binary Classification', 'Single Object', 'Action Classes', 'Object Tracking', 'Single Tracking', 'Multi-object Tracking', 'Processing Speed', 'Pedestrian', 'Global Features', 'Multi-label', 'Image Object', 'Bounding Box', 'Pooling Layer', 'Multiple Object Tracking', 'Direct Classification', 'Object Features', 'Optical Flow', 'Fully-connected Layer', 'Still Images', 'Variational Autoencoder', 'Motion Vector', 'YouTube', 'Conditional Variational Autoencoder']",,13,"A novel algorithm to estimate instance-level future motion in a single image is proposed in this paper. We first represent the future motion of an instance with its direction, speed, and action classes. Then, we develop a deep neural network that exploits different levels of semantic information to perform the future motion estimation. For effective future motion classification, we adopt ordinal regression. Especially, we develop the cyclic ordinal regression scheme using binary classifiers. Experiments demonstrate that the proposed algorithm provides reliable performance and thus can be used effectively for vision applications, including single and multi object tracking. Furthermore, we release the future motion (FM) dataset, collected from diverse sources and annotated manually, as a benchmark for single-image future motion estimation."
Integral Object Mining via Online Attention Accumulation,"Peng-Tao Jiang, Qibin Hou, Yang Cao, Ming-Ming Cheng, Yunchao Wei, Hong-Kai Xiong","UTS; TKLNDST, CS, Nankai University; Shanghai Jiaotong University",100.0,"China, australia",0.0,,"Object attention maps generated by image classifiers are usually used as priors for weakly-supervised segmentation approaches. However, normal image classifiers produce attention only at the most discriminative object parts, which limits the performance of weakly-supervised segmentation task. Therefore, how to effectively identify entire object regions in a weakly-supervised manner has always been a challenging and meaningful problem. We observe that the attention maps produced by a classification network continuously focus on different object parts during training. In order to accumulate the discovered different object parts, we propose an online attention accumulation (OAA) strategy which maintains a cumulative attention map for each target category in each training image so that the integral object regions can be gradually promoted as the training goes. These cumulative attention maps, in turn, serve as the pixel-level supervision, which can further assist the network in discovering more integral object regions. Our method (OAA) can be plugged into any classification network and progressively accumulate the discriminative regions into integral objects as the training process goes. Despite its simplicity, when applying the resulting attention maps to the weakly-supervised semantic segmentation task, our approach improves the existing state-of-the-art methods on the PASCAL VOC 2012 segmentation benchmark, achieving a mIoU score of 66.4% on the test set. Code is available at https://mmcheng.net/oaa/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Integral_Object_Mining_via_Online_Attention_Accumulation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Integral_Object_Mining_via_Online_Attention_Accumulation_ICCV_2019_paper.pdf,,https://mmcheng.net/oaa/,,main,Poster,https://ieeexplore.ieee.org/document/9009076/,"['Training', 'Semantics', 'Image segmentation', 'Convolution', 'Visualization', 'Task analysis', 'Benchmark testing']","['Training Images', 'Classification Network', 'Semantic Segmentation', 'Segmentation Task', 'Regional Integration', 'Segmentation Approach', 'Object Parts', 'Attention Map', 'Target Category', 'Object Regions', 'Accumulation Strategy', 'Discriminative Regions', 'Loss Function', 'Convolutional Neural Network', 'Validation Set', 'Convolutional Layers', 'Integrated Model', 'Large-scale Datasets', 'Target Object', 'Global Average Pooling', 'Global Average Pooling Layer', 'Attention Regions', 'Fusion Strategy', 'Attention Values', 'Semantic Regions', 'Semantic Objects', 'Function In Eq', 'Weak Supervision', 'Attention Model', 'Intermediate Maps']",,165,"Object attention maps generated by image classifiers are usually used as priors for weakly-supervised segmentation approaches. However, normal image classifiers produce attention only at the most discriminative object parts, which limits the performance of weakly-supervised segmentation task. Therefore, how to effectively identify entire object regions in a weakly-supervised manner has always been a challenging and meaningful problem. We observe that the attention maps produced by a classification network continuously focus on different object parts during training. In order to accumulate the discovered different object parts, we propose an online attention accumulation (OAA) strategy which maintains a cumulative attention map for each target category in each training image so that the integral object regions can be gradually promoted as the training goes. These cumulative attention maps, in turn, serve as the pixel-level supervision, which can further assist the network in discovering more integral object regions. Our method (OAA) can be plugged into any classification network and progressively accumulate the discriminative regions into integral objects as the training process goes. Despite its simplicity, when applying the resulting attention maps to the weakly-supervised semantic segmentation task, our approach improves the existing state-of-the-art methods on the PASCAL VOC 2012 segmentation benchmark, achieving a mIoU score of 66.4% on the test set. Code is available at https://mmcheng.net/oaa/."
Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation,"Arnab Ghosh, Richard Zhang, Puneet K. Dokania, Oliver Wang, Alexei A. Efros, Philip H. S. Torr, Eli Shechtman","Adobe Research; University of Oxford; Adobe Research, UC Berkeley",66.66666666666666,"uk, usa",33.33333333333334,USA,"We propose an interactive GAN-based sketch-to-image translation method that helps novice users easily create images of simple objects. The user starts with a sparse sketch and a desired object category, and the network then recommends its plausible completion(s) and shows a corresponding synthesized image. This enables a feedback loop, where the user can edit the sketch based on the network's recommendations, while the network is able to better synthesize the image that the user might have in mind. In order to use a single model for a wide array of object classes, we introduce a gating-based approach for class conditioning, which allows us to generate distinct classes without feature mixing, from a single generator network.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ghosh_Interactive_Sketch__Fill_Multiclass_Sketch-to-Image_Translation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ghosh_Interactive_Sketch__Fill_Multiclass_Sketch-to-Image_Translation_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
Interpolated Convolutional Networks for 3D Point Cloud Understanding,"Jiageng Mao, Xiaogang Wang, Hongsheng Li","CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong",100.0,"Hong Kong, china",0.0,,"Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010291/,"['Three-dimensional displays', 'Kernel', 'Neural networks', 'Interpolation', 'Two dimensional displays', 'Standards', 'Data structures']","['Convolutional Network', 'Point Cloud', '3D Point', '3D Point Cloud', 'Neural Network', 'Convolutional Neural Network', 'Local Structure', 'Feature Points', 'Irregular Structure', 'Kernel Weight', 'Part Segmentation', 'Interpolation Function', 'Normal Term', 'Irregular Data', 'Shape Classification', 'Fine-grained Structure', 'Point Cloud Features', 'Multilayer Perceptron', 'Weight Vector', 'Classification Network', 'Graph Neural Networks', 'Standard Convolution', 'Kernel Length', 'Input Point', 'Regular Grid', 'Trilinear Interpolation', 'Euclidean Space', 'Gaussian Blur', 'Convolution Kernel', 'Input Point Cloud']",,151,"Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS."
Invariant Information Clustering for Unsupervised Image Classification and Segmentation,"Xu Ji, JoÃ£o F. Henriques, Andrea Vedaldi",University of Oxford,100.0,uk,0.0,,"We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ji_Invariant_Information_Clustering_for_Unsupervised_Image_Classification_and_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ji_Invariant_Information_Clustering_for_Unsupervised_Image_Classification_and_Segmentation_ICCV_2019_paper.pdf,,https://github.com/xu-ji/IIC,,main,Poster,https://ieeexplore.ieee.org/document/9008122/,"['Semantics', 'Mutual information', 'Image segmentation', 'Training', 'Machine learning', 'Neural networks', 'Entropy']","['Image Classification', 'Image Segmentation', 'Unsupervised Learning', 'Unsupervised Image Segmentation', 'Invariant Information Clustering', 'Neural Network', 'Paired Samples', 'Clustering Method', 'Mutual Information', 'Unsupervised Clustering', 'Unlabeled Data', 'Clustering Objective', 'High-dimensional Representation', 'Semantic Clustering', 'Use Of Small Amounts', 'Training Set', 'Deep Learning', 'Random Variables', 'Convolution', 'Deep Neural Network', 'Random Transformation', 'Main Headings', 'Conditional Entropy', 'Geometric Transformation', 'Unsupervised Methods', 'Random Initialization', 'Representation Learning', 'Image X', 'Cluster Assignment', 'Paired Data']",,454,"We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC"
Is This the Right Place? Geometric-Semantic Pose Verification for Indoor Visual Localization,"Hajime Taira, Ignacio Rocco, Jiri Sedlar, Masatoshi Okutomi, Josef Sivic, Tomas Pajdla, Torsten Sattler, Akihiko Torii","Chalmers University of Technology; Inria∗; Tokyo Institute of Technology; Inria∗, CIIRC CTU in Prague†; CIIRC CTU in Prague†",100.0,"France, czech republic, japan, sweden",0.0,,"Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there has been considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Taira_Is_This_the_Right_Place_Geometric-Semantic_Pose_Verification_for_Indoor_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Taira_Is_This_the_Right_Place_Geometric-Semantic_Pose_Verification_for_Indoor_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008526/,"['Visualization', 'Semantics', 'Three-dimensional displays', 'Cameras', 'Databases', 'Feature extraction', 'Geometry']","['Challenging Problem', 'Camera Pose', 'Visual Localization', 'Query Image', 'Accurate Pose', 'Convolutional Neural Network', 'Point Cloud', 'Semantic Information', 'Semantic Segmentation', 'Point Model', 'Depth Information', '3D Point', 'Feature Matching', 'Pose Estimation', 'Synthetic Images', 'Image Database', 'Image Retrieval', 'Geometry Information', 'Reprojection Error', 'Surface Normals', 'Normal Map', 'Verification Approach', 'Camera Pose Estimation', 'View Synthesis', 'Semantic Consistency', 'Part Of The Scene', 'Indoor Localization', 'Pixel Position', 'Appearance Information', '3D Point Cloud']",,34,"Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there has been considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset."
Is an Affine Constraint Needed for Affine Subspace Clustering?,"Chong You, Chun-Guang Li, Daniel P. Robinson, RenÃ© Vidal","Applied Mathematics and Statistics, Johns Hopkins University, MD, USA; SICE, Beijing University of Posts and Telecommunications, Beijing, China; Mathematical Institute for Data Science, Johns Hopkins University, MD, USA; EECS, University of California, Berkeley, CA, USA",100.0,"China, china, usa",0.0,,"Subspace clustering methods based on expressing each data point as a linear combination of other data points have achieved great success in computer vision applications such as motion segmentation, face and digit clustering. In face clustering, the subspaces are linear and subspace clustering methods can be applied directly. In motion segmentation, the subspaces are affine and an additional affine constraint on the coefficients is often enforced. However, since affine subspaces can always be embedded into linear subspaces of one extra dimension, it is unclear if the affine constraint is really necessary. This paper shows, both theoretically and empirically, that when the dimension of the ambient space is high relative to the sum of the dimensions of the affine subspaces, the affine constraint has a negligible effect on clustering performance. Specifically, our analysis provides conditions that guarantee the correctness of affine subspace clustering methods both with and without the affine constraint, and shows that these conditions are satisfied for high-dimensional data. Underlying our analysis is the notion of affinely independent subspaces, which not only provides geometrically interpretable correctness conditions, but also clarifies the relationships between existing results for affine subspace clustering.",,http://openaccess.thecvf.com/content_ICCV_2019/html/You_Is_an_Affine_Constraint_Needed_for_Affine_Subspace_Clustering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/You_Is_an_Affine_Constraint_Needed_for_Affine_Subspace_Clustering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010620/,"['Clustering methods', 'Computer vision', 'Motion segmentation', 'Optimization', 'Geometry', 'Face', 'Data models']","['Subspace Clustering', 'Affine Subspace', 'Dimensional Space', 'Computer Vision', 'Clustering Performance', 'Extra Dimension', 'Linear Subspace', 'Combination Of Points', 'Linear Clustering', 'Ambient Space', 'Affine Space', 'Optimization Problem', 'Random Model', 'Least Squares Regression', 'Data Matrix', 'Independent Model', 'Probability 1', 'Theoretical Questions', 'MNIST Dataset', 'Clustering Accuracy', 'Random Subspace', 'Geometric Interpretation', 'Sample Data Points', 'Clustering Problem', 'Geometric Conditions']",,13,"Subspace clustering methods based on expressing each data point as a linear combination of other data points have achieved great success in computer vision applications such as motion segmentation, face and digit clustering. In face clustering, the subspaces are linear and subspace clustering methods can be applied directly. In motion segmentation, the subspaces are affine and an additional affine constraint on the coefficients is often enforced. However, since affine subspaces can always be embedded into linear subspaces of one extra dimension, it is unclear if the affine constraint is really necessary. This paper shows, both theoretically and empirically, that when the dimension of the ambient space is high relative to the sum of the dimensions of the affine subspaces, the affine constraint has a negligible effect on clustering performance. Specifically, our analysis provides conditions that guarantee the correctness of affine subspace clustering methods both with and without the affine constraint, and shows that these conditions are satisfied for high-dimensional data. Underlying our analysis is the notion of affinely independent subspaces, which not only provides geometrically interpretable correctness conditions, but also clarifies the relationships between existing results for affine subspace clustering."
JPEG Artifacts Reduction via Deep Convolutional Sparse Coding,"Xueyang Fu, Zheng-Jun Zha, Feng Wu, Xinghao Ding, John Paisley","Department of Electrical Engineering & Data Science Institute, Columbia University, USA; School of Informatics, Xiamen University, China; School of Information Science and Technology, University of Science and Technology of China, China",100.0,"china, usa",0.0,,"To effectively reduce JPEG compression artifacts, we propose a deep convolutional sparse coding (DCSC) network architecture. We design our DCSC in the framework of classic learned iterative shrinkage-threshold algorithm. To focus on recognizing and separating artifacts only, we sparsely code the feature maps instead of the raw image. The final de-blocked image is directly reconstructed from the coded features. We use dilated convolution to extract multi-scale image features, which allows our single model to simultaneously handle multiple JPEG compression levels. Since our method integrates model-based convolutional sparse coding with a learning-based deep neural network, the entire network structure is compact and more explainable. The resulting lightweight model generates comparable or better de-blocking results when compared with state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fu_JPEG_Artifacts_Reduction_via_Deep_Convolutional_Sparse_Coding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_JPEG_Artifacts_Reduction_via_Deep_Convolutional_Sparse_Coding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008801/,"['Transform coding', 'Image coding', 'Convolutional codes', 'Convolution', 'Feature extraction', 'Task analysis', 'Encoding']","['Sparse Coding', 'Artifact Reduction', 'Convolutional Codes', 'Convolutional Sparse Coding', 'JPEG Artifacts', 'Deep Network', 'Deep Neural Network', 'Feature Maps', 'Multi-scale Features', 'Dilated Convolution', 'Multi-scale Feature Extraction', 'JPEG Compression', 'Compression Artifacts', 'Deep Learning', 'Convolutional Neural Network', 'Convolutional Layers', 'Deep Models', 'Convolution Operation', 'Matrix Multiplication', 'Model-based Methods', 'JPEG Images', 'Discrete Cosine Transform', 'Image Domain', 'Sparse Coefficients', 'Dilation Factor', 'Red Rectangle', 'Mean Square Error Loss', 'Vision Tasks', 'Learning-based Methods']",,75,"To effectively reduce JPEG compression artifacts, we propose a deep convolutional sparse coding (DCSC) network architecture. We design our DCSC in the framework of classic learned iterative shrinkage-threshold algorithm. To focus on recognizing and separating artifacts only, we sparsely code the feature maps instead of the raw image. The final de-blocked image is directly reconstructed from the coded features. We use dilated convolution to extract multi-scale image features, which allows our single model to simultaneously handle multiple JPEG compression levels. Since our method integrates model-based convolutional sparse coding with a learning-based deep neural network, the entire network structure is compact and more explainable. The resulting lightweight model generates comparable or better de-blocking results when compared with state-of-the-art methods."
Joint Acne Image Grading and Counting via Label Distribution Learning,"Xiaoping Wu,  Ni Wen,  Jie Liang,  Yu-Kun Lai,  Dongyu She,  Ming-Ming Cheng,  Jufeng Yang","Beijing Tsinghua Changgung Hospital; College of Computer Science, Nankai University; School of Computer Science and Informatics, Cardiff University",66.66666666666666,"China, uk",33.33333333333334,China,"Accurate grading of skin disease severity plays a crucial role in precise treatment for patients. Acne vulgaris, the most common skin disease in adolescence, can be graded by evidence-based lesion counting as well as experience-based global estimation in the medical field. However, due to the appearance similarity of acne with close severity, it is challenging to count and grade acne accurately. In this paper, we address the problem of acne image analysis via Label Distribution Learning (LDL) considering the ambiguous information among acne severity. Based on the professional grading criterion, we generate two acne label distributions considering the relationship between the similar number of lesions and severity of acne, respectively. We also propose a unified framework for joint acne image grading and counting, which is optimized by the multi-task learning loss. In addition, we further build the ACNE04 dataset with annotations of acne severity and lesion number of each image for evaluation. Experiments demonstrate that our proposed framework performs favorably against state-of-the-art methods. We make the code and dataset publicly available at https://github.com/xpwu95/ldl.",http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Joint_Acne_Image_Grading_and_Counting_via_Label_Distribution_Learning_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Joint_Acne_Image_Grading_and_Counting_via_Label_Distribution_Learning_ICCV_2019_paper.pdf,,https://github.com/xpwu95/ldl,,,,https://ieeexplore.ieee.org/document/9010021/,"['Lesions', 'Task analysis', 'Medical diagnostic imaging', 'Diseases', 'Skin', 'Training']","['Label Distribution', 'Label Distribution Learning', 'Skin Diseases', 'Medical Field', 'Global Estimates', 'Number Of Lesions', 'Severe Grade', 'Multi-task Learning', 'Common Skin Disease', 'Lesion Count', 'Severe Acne', 'Acne Lesions', 'Convolutional Neural Network', 'Classification Results', 'Classification Task', 'Test Phase', 'Training Procedure', 'Deep Convolutional Neural Network', 'Kullback-Leibler', 'Counting Task', 'Medical Criteria', 'Single Label', 'Object Counting', 'Handcrafted Features', 'Vision Community', 'Youden Index', 'Medical Image Processing', 'Object Detection Methods', 'Deep Features']",,42,"Accurate grading of skin disease severity plays a crucial role in precise treatment for patients. Acne vulgaris, the most common skin disease in adolescence, can be graded by evidence-based lesion counting as well as experience-based global estimation in the medical field. However, due to the appearance similarity of acne with close severity, it is challenging to count and grade acne accurately. In this paper, we address the problem of acne image analysis via Label Distribution Learning (LDL) considering the ambiguous information among acne severity. Based on the professional grading criterion, we generate two acne label distributions considering the relationship between the similar number of lesions and severity of acne, respectively. We also propose a unified framework for joint acne image grading and counting, which is optimized by the multi-task learning loss. In addition, we further build the ACNE04 dataset with annotations of acne severity and lesion number of each image for evaluation. Experiments demonstrate that our proposed framework performs favorably against state-of-the-art methods. We make the code and dataset publicly available at https://github.com/xpwu95/ldl."
Joint Demosaicking and Denoising by Fine-Tuning of Bursts of Raw Images,"Thibaud Ehret, Axel Davy, Pablo Arias, Gabriele Facciolo","CMLA, CNRS, ENS Paris-Saclay, Université Paris-Saclay; Université Paris-Saclay, 94235 Cachan, France",100.0,"France, france",0.0,,"Demosaicking and denoising are the first steps of any camera image processing pipeline and are key for obtaining high quality RGB images. A promising current research trend aims at solving these two problems jointly using convolutional neural networks. Due to the unavailability of ground truth data these networks cannot be currently trained using real RAW images. Instead, they resort to simulated data. In this paper we present a method to learn demosaicking directly from mosaicked images, without requiring ground truth RGB data. We apply this to learn joint demosaicking and denoising only from RAW images, thus enabling the use of real data. In addition we show that for this application fine-tuning a network to a specific burst improves the quality of restoration for both demosaicking and denoising.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ehret_Joint_Demosaicking_and_Denoising_by_Fine-Tuning_of_Bursts_of_Raw_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ehret_Joint_Demosaicking_and_Denoising_by_Fine-Tuning_of_Bursts_of_Raw_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010951/,"['Noise reduction', 'Training', 'Cameras', 'Noise measurement', 'Pipelines', 'Image color analysis', 'Sensor arrays']","['Raw Images', 'Burst Images', 'Neural Network', 'Convolutional Neural Network', 'Simulated Data', 'RGB Images', 'Processing Pipeline', 'Input Image', 'Image Size', 'Network Training', 'Challenging Problem', 'Image Pairs', 'Reference Image', 'Gaussian Mixture Model', 'Model-based Methods', 'Types Of Noise', 'Bilinear Interpolation', 'Lexicographic', 'Noisy Images', 'Noise-free Image', 'Denoising Methods']",,31,"Demosaicking and denoising are the first steps of any camera image processing pipeline and are key for obtaining high quality RGB images. A promising current research trend aims at solving these two problems jointly using convolutional neural networks. Due to the unavailability of ground truth data these networks cannot be currently trained using real RAW images. Instead, they resort to simulated data. In this paper we present a method to learn demosaicking directly from mosaicked images, without requiring ground truth RGB data. We apply this to learn joint demosaicking and denoising only from RAW images, thus enabling the use of real data. In addition we show that for this application fine-tuning a network to a specific burst improves the quality of restoration for both demosaicking and denoising."
Joint Embedding of 3D Scan and CAD Objects,"Manuel Dahnert, Angela Dai, Leonidas J. Guibas, Matthias NieÃner",Technical University of Munich; Stanford University,100.0,"germany, usa",0.0,,"3D scan geometry and CAD models often contain complementary information towards understanding environments, which could be leveraged through establishing a mapping between the two domains. However, this is a challenging task due to strong, lower-level differences between scan and CAD geometry. We propose a novel approach to learn a joint embedding space between scan and CAD geometry, where semantically similar objects from both domains lie close together. To achieve this, we introduce a new 3D CNN-based approach to learn a joint embedding space representing object similarities across these domains. To learn a shared space where scan objects and CAD models can interlace, we propose a stacked hourglass approach to separate foreground and background from a scan object, and transform it to a complete, CAD-like representation to produce a shared embedding space. This embedding space can then be used for CAD model retrieval; to further enable this task, we introduce a new dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. Our learned joint embedding outperforms current state of the art for CAD model retrieval by 12% in instance retrieval accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dahnert_Joint_Embedding_of_3D_Scan_and_CAD_Objects_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dahnert_Joint_Embedding_of_3D_Scan_and_CAD_Objects_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009028/,"['Solid modeling', 'Three-dimensional displays', 'Geometry', 'Shape', 'Task analysis', 'Clutter', 'Decoding']","['3D Scanning', 'Joint Embedding', 'Objective Scanner', 'CAD Object', 'Latent Space', 'Semantic Similarity', 'Similar Objects', 'Shared Space', 'Retrieval Accuracy', 'CAD Model', 'Embedding Learning', 'CNN-based Approaches', 'Neural Network', 'Feature Space', 'Object Detection', '3D Reconstruction', 'Feature Learning', 'Point Cloud', 'Classification Categories', 'Availability Of Datasets', 'Object Geometry', 'Occupancy Grid', 'Background Clutter', 'Triplet Loss', 'Shape Descriptors', 'Learning Spaces', 'Final Classification Layer', 'Quality Ranking', '3D Shape', 'Real-world Objects']",,25,"3D scan geometry and CAD models often contain complementary information towards understanding environments, which could be leveraged through establishing a mapping between the two domains. However, this is a challenging task due to strong, lower-level differences between scan and CAD geometry. We propose a novel approach to learn a joint embedding space between scan and CAD geometry, where semantically similar objects from both domains lie close together. To achieve this, we introduce a new 3D CNN-based approach to learn a joint embedding space representing object similarities across these domains. To learn a shared space where scan objects and CAD models can interlace, we propose a stacked hourglass approach to separate foreground and background from a scan object, and transform it to a complete, CAD-like representation to produce a shared embedding space. This embedding space can then be used for CAD model retrieval; to further enable this task, we introduce a new dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. Our learned joint embedding outperforms current state of the art for CAD model retrieval by 12% in instance retrieval accuracy."
Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking,"Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler","Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK; School of Internet of Things Engineering, Jiangnan University, Wuxi, China",100.0,"China, uk",0.0,,"We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system. In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters. To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at https://github.com/XU-TIANYANG/GFS-DCF.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Joint_Group_Feature_Selection_and_Discriminative_Filter_Learning_for_Robust_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Joint_Group_Feature_Selection_and_Discriminative_Filter_Learning_for_Robust_ICCV_2019_paper.pdf,,https://github.com/XU-TIANYANG/GFS-DCF,,main,Poster,https://ieeexplore.ieee.org/document/9010061/,"['Feature extraction', 'Visualization', 'Object tracking', 'Robustness', 'Correlation', 'Task analysis', 'Redundancy']","['Discriminative Learning', 'Visual Object Tracking', 'Discriminative Filter', 'Deep Network', 'Deep Neural Network', 'Relevant Features', 'Spatial Dimensions', 'Redundant Information', 'Deep Features', 'Feature Selection Methods', 'Channel Dimension', 'Channel Selection', 'Tracking Process', 'Spatial Selectivity', 'Low-rank Approximation', 'Spatial Regularization', 'Efficient Approximation', 'Learned Filters', 'Correlation Filter', 'Convolutional Neural Network', 'Handcrafted Features', 'Object In Frame', 'Temporal Smoothing', 'Successive Frames', 'Low-rank Property', 'Histogram Of Oriented Gradients', 'Regularization Term', 'Area Under Curve', 'Color Word', 'Appearance Variations']",,128,"We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system. In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters. To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at \url{https://github.com/XU-TIANYANG/GFS-DCF}."
Joint Learning of Saliency Detection and Weakly Supervised Semantic Segmentation,"Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang","Dalian University of Technology, China",100.0,china,0.0,,"Existing weakly supervised semantic segmentation (WSSS) methods usually utilize the results of pre-trained saliency detection (SD) models without explicitly modelling the connections between the two tasks, which is not the most efficient configuration. Here we propose a unified multi-task learning framework to jointly solve WSSS and SD using a single network, i.e. saliency and segmentation network (SSNet). SSNet consists of a segmentation network (SN) and a saliency aggregation module (SAM). For an input image, SN generates the segmentation result and, SAM predicts the saliency of each category and aggregating the segmentation masks of all categories into a saliency map. The proposed network is trained end-to-end with image-level category labels and class-agnostic pixel-level saliency labels. Experiments on PASCAL VOC 2012 segmentation dataset and four saliency benchmark datasets show the performance of our method compares favorably against state-of-the-art weakly supervised segmentation methods and fully supervised saliency detection methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Joint_Learning_of_Saliency_Detection_and_Weakly_Supervised_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Joint_Learning_of_Saliency_Detection_and_Weakly_Supervised_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010293/,"['Image segmentation', 'Semantics', 'Saliency detection', 'Task analysis', 'Feature extraction', 'Training', 'Computational modeling']","['Semantic Segmentation', 'Joint Learning', 'Saliency Detection', 'Weakly Supervised Semantic Segmentation', 'Input Image', 'Segmentation Method', 'Segmentation Results', 'Salience Network', 'Multi-task Learning', 'Category Labels', 'Saliency Map', 'Semantic Segmentation Methods', 'Efficient Configuration', 'Convolutional Neural Network', 'Convolutional Layers', 'Deep Convolutional Neural Network', 'Bounding Box', 'Training Stage', 'Segmentation Model', 'Multiple Objects', 'Semantic Labels', 'Image-level Labels', 'Dilation Rate', 'Object Regions', 'Class Activation Maps', 'Weak Supervision', 'Input Image Size', 'Scribble', 'Salient Object', 'Convolutional Block']",,39,"Existing weakly supervised semantic segmentation (WSSS) methods usually utilize the results of pre-trained saliency detection (SD) models without explicitly modelling the connections between the two tasks, which is not the most efficient configuration. Here we propose a unified multi-task learning framework to jointly solve WSSS and SD using a single network, i.e. saliency and segmentation network (SSNet). SSNet consists of a segmentation network (SN) and a saliency aggregation module (SAM). For an input image, SN generates the segmentation result and, SAM predicts the saliency of each category and aggregating the segmentation masks of all categories into a saliency map. The proposed network is trained end-to-end with image-level category labels and class-agnostic pixel-level saliency labels. Experiments on PASCAL VOC 2012 segmentation dataset and four saliency benchmark datasets show the performance of our method compares favorably against state-of-the-art weakly supervised segmentation methods and fully supervised saliency detection methods."
Joint Learning of Semantic Alignment and Object Landmark Detection,"Sangryul Jeon, Dongbo Min, Seungryong Kim, Kwanghoon Sohn",Ewha Womans University; Yonsei University; ´Ecole Polytechnique Fédérale de Lausanne (EPFL),100.0,"france, south korea, switzerland",0.0,,"Convolutional neural networks (CNNs) based approaches for semantic alignment and object landmark detection have improved their performance significantly. Current efforts for the two tasks focus on addressing the lack of massive training data through weakly- or unsupervised learning frameworks. In this paper, we present a joint learning approach for obtaining dense correspondences and discovering object landmarks from semantically similar images. Based on the key insight that the two tasks can mutually provide supervisions to each other, our networks accomplish this through a joint loss function that alternatively imposes a consistency constraint between the two tasks, thereby boosting the performance and addressing the lack of training data in a principled manner. To the best of our knowledge, this is the first attempt to address the lack of training data for the two tasks through the joint learning. To further improve the robustness of our framework, we introduce a probabilistic learning formulation that allows only reliable matches to be used in the joint learning process. With the proposed method, state-of-the-art performance is attained on several benchmarks for semantic matching and landmark detection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010907/,"['Semantics', 'Task analysis', 'Feature extraction', 'Training data', 'Reliability', 'Strain', 'Boosting']","['Fluidic', 'Joint Learning', 'Landmark Detection', 'Object Landmarks', 'Loss Function', 'Benchmark', 'Training Data', 'Lack Of Data', 'Convolutional Neural Network', 'Image Pairs', 'Probabilistic Formulation', 'Consistency Constraint', 'Lack Of Training Data', 'Semantic Matching', 'Dense Correspondence', 'Principled Manner', 'Richness Diversity', 'Probability Function', 'Target Image', 'Spatial Coordinates', 'Source Images', 'Weak Supervision', 'Appearance Variations', 'Semantic Network', 'Matching Score', 'Corresponding Points', 'Alignment Module', 'Equivalency', 'Uncertainty Map', 'Self-similarity']",,12,"Convolutional neural networks (CNNs) based approaches for semantic alignment and object landmark detection have improved their performance significantly. Current efforts for the two tasks focus on addressing the lack of massive training data through weakly- or unsupervised learning frameworks. In this paper, we present a joint learning approach for obtaining dense correspondences and discovering object landmarks from semantically similar images. Based on the key insight that the two tasks can mutually provide supervisions to each other, our networks accomplish this through a joint loss function that alternatively imposes a consistency constraint between the two tasks, thereby boosting the performance and addressing the lack of training data in a principled manner. To the best of our knowledge, this is the first attempt to address the lack of training data for the two tasks through the joint learning. To further improve the robustness of our framework, we introduce a probabilistic learning formulation that allows only reliable matches to be used in the joint learning process. With the proposed method, state-of-the-art performance is attained on several benchmarks for semantic matching and landmark detection."
Joint Monocular 3D Vehicle Detection and Tracking,"Hou-Ning Hu, Qi-Zhi Cai, Dequan Wang, Ji Lin, Min Sun, Philipp KrÃ¤henbÃ¼hl, Trevor Darrell, Fisher Yu",UT Austin; MIT; Sinovation Ventures AI Institute; National Tsing Hua University; UC Berkeley,100.0,"China, taiwan, usa",0.0,,"Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Joint_Monocular_3D_Vehicle_Detection_and_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Joint_Monocular_3D_Vehicle_Detection_and_Tracking_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009549/,"['Three-dimensional displays', 'Two dimensional displays', 'Trajectory', 'Estimation', 'Proposals', 'Tracking', 'Videos']","['Vehicle Track', '3D Tracking', 'Joint Tracking', '3D Vehicle', '3D Vehicle Detection', 'Bounding Box', 'Image-based Methods', 'Robust Tracking', 'KITTI Dataset', 'Complete 3D', '3D Bounding Box', 'Deep Network', 'Object Detection', 'Object-oriented', '3D Position', '3D Information', 'Object Tracking', 'Faster R-CNN', '3D Motion', '3D Detection', '3D Projection', 'World Coordinate', 'Multiple Object Tracking', 'LSTM Model', 'GPS Sensors', '3D Depth', 'Occlusion Problem', '3D World', 'Object Proposals']",,142,"Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods."
Joint Optimization for Cooperative Image Captioning,"Gilad Vered, Gal Oren, Yuval Atzmon, Gal Chechik","Bar-Ilan University, NVIDIA; Bar-Ilan University",100.0,israel,0.0,,"When describing images with natural language, descriptions can be made more informative if tuned for downstream tasks. This can be achieved by training two networks: a ""speaker"" that generates sentences given an image and a ""listener"" that uses them to perform a task. Unfortunately, training multiple networks jointly to communicate, faces two major challenges. First, the descriptions generated by a speaker network are discrete and stochastic, making optimization very hard and inefficient. Second, joint training usually causes the vocabulary used during communication to drift and diverge from natural language. To address these challenges, we present an effective optimization technique based on partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. We then show that the generated descriptions can be kept close to natural by constraining them to be similar to human descriptions. Together, this approach creates descriptions that are both more discriminative and more natural than previous approaches. Evaluations on the COCO benchmark show that PSST improve the recall@10 from 60% to 86% maintaining comparable language naturalness. Human evaluations show that it also increases naturalness while keeping the discriminative power of generated captions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010870/,"['Training', 'Natural languages', 'Stochastic processes', 'Optimization', 'Task analysis', 'Standards', 'Loss measurement']","['Joint Optimization', 'Image Captioning', 'Vocabulary', 'Natural Language', 'Multinomial Distribution', 'Joint Training', 'Deterministic', 'Input Image', 'Parametrized', 'Training Images', 'Gradient Approximation', 'Image Retrieval', 'Image Correction', 'Test Split', 'Forward Pass', 'Conditional Generative Adversarial Network']",,6,"When describing images with natural language, descriptions can be made more informative if tuned for downstream tasks. This can be achieved by training two networks: a ""speaker"" that generates sentences given an image and a ""listener"" that uses them to perform a task. Unfortunately, training multiple networks jointly to communicate, faces two major challenges. First, the descriptions generated by a speaker network are discrete and stochastic, making optimization very hard and inefficient. Second, joint training usually causes the vocabulary used during communication to drift and diverge from natural language. To address these challenges, we present an effective optimization technique based on partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. We then show that the generated descriptions can be kept close to natural by constraining them to be similar to human descriptions. Together, this approach creates descriptions that are both more discriminative and more natural than previous approaches. Evaluations on the COCO benchmark show that PSST improve the recall@10 from 60% to 86% maintaining comparable language naturalness. Human evaluations show that it also increases naturalness while keeping the discriminative power of generated captions."
Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes,"Huikun Bi, Zhong Fang, Tianlu Mao, Zhaoqi Wang, Zhigang Deng","University of Houston; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences",100.0,"USA, china",0.0,,"Trajectory prediction for objects is challenging and critical for various applications (e.g., autonomous driving, and anomaly detection). Most of the existing methods focus on homogeneous pedestrian trajectories prediction, where pedestrians are treated as particles without size. However, they fall short of handling crowded vehicle-pedestrian-mixed scenes directly since vehicles, limited with kinematics in reality, should be treated as rigid, non-particle objects ideally. In this paper, we tackle this problem using separate LSTMs for heterogeneous vehicles and pedestrians. Specifically, we use an oriented bounding box to represent each vehicle, calculated based on its position and orientation, to denote its kinematic trajectories. We then propose a framework called VP-LSTM to predict the kinematic trajectories of both vehicles and pedestrians simultaneously. In order to evaluate our model, a large dataset containing the trajectories of both vehicles and pedestrians in vehicle-pedestrian-mixed scenes is specially built. Through comparisons between our method with state-of-the-art approaches, we show the effectiveness and advantages of our method on kinematic trajectories prediction in vehicle-pedestrian-mixed scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008812/,"['Trajectory', 'Kinematics', 'Predictive models', 'Vehicle dynamics', 'Computer vision', 'Autonomous vehicles', 'Anomaly detection']","['Joint Prediction', 'Kinematic Trajectories', 'Anomaly Detection', 'Trajectory Prediction', 'Vehicle Trajectory', 'Pedestrian Trajectory', 'Normal Distribution', 'Covariance Matrix', 'Quantitative Evaluation', 'Rigid Body', 'Traffic Congestion', 'Consecutive Steps', 'Motion Patterns', 'Latent Representation', 'Cholesky Decomposition', 'Upper Triangular', 'Vehicle Motion', 'Vehicle Parameters', 'Heterogeneous Agents', 'Accurate Trajectory', 'Human Trajectory', 'Human-human Interaction', 'Crowded Spaces', 'Occupancy Map', 'Trajectory Dataset', 'Heterogeneous Interactions', 'Highest Error', 'Prediction Accuracy', 'Historical Trajectory', 'Attention Mechanism']",,20,"Trajectory prediction for objects is challenging and critical for various applications (e.g., autonomous driving, and anomaly detection). Most of the existing methods focus on homogeneous pedestrian trajectories prediction, where pedestrians are treated as particles without size. However, they fall short of handling crowded vehicle-pedestrian-mixed scenes directly since vehicles, limited with kinematics in reality, should be treated as rigid, non-particle objects ideally. In this paper, we tackle this problem using separate LSTMs for heterogeneous vehicles and pedestrians. Specifically, we use an oriented bounding box to represent each vehicle, calculated based on its position and orientation, to denote its kinematic trajectories. We then propose a framework called VP-LSTM to predict the kinematic trajectories of both vehicles and pedestrians simultaneously. In order to evaluate our model, a large dataset containing the trajectories of both vehicles and pedestrians in vehicle-pedestrian-mixed scenes is specially built. Through comparisons between our method with state-of-the-art approaches, we show the effectiveness and advantages of our method on kinematic trajectories prediction in vehicle-pedestrian-mixed scenes."
Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning,"Jingyi Hou, Xinxiao Wu, Wentian Zhao, Jiebo Luo, Yunde Jia","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing 100081, China; Department of Computer Science, University of Rochester, Rochester NY 14627, USA",100.0,"China, usa",0.0,,"Video captioning is a challenging task that involves not only visual perception but also syntax representation learning. Recent progress in video captioning has been achieved through visual perception, but syntax representation learning is still under-explored. We propose a novel video captioning approach that takes into account both visual perception and syntax representation learning to generate accurate descriptions of videos. Specifically, we use sentence templates composed of Part-of-Speech (POS) tags to represent the syntax structure of captions, and accordingly, syntax representation learning is performed by directly inferring POS tags from videos. The visual perception is implemented by a mixture model which translates visual cues into lexical words that are conditional on the learned syntactic structure of sentences. Thus, a video captioning task consists of two sub-tasks: video POS tagging and visual cue translation, which are jointly modeled and trained in an end-to-end fashion. Evaluations on three public benchmark datasets demonstrate that our proposed method achieves substantially better performance than the state-of-the-art methods, which validates the superiority of joint modeling of syntax representation learning and visual perception for video captioning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Joint_Syntax_Representation_Learning_and_Visual_Cue_Translation_for_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Joint_Syntax_Representation_Learning_and_Visual_Cue_Translation_for_Video_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010931/,"['Visualization', 'Syntactics', 'Semantics', 'Tagging', 'Visual perception', 'Feature extraction', 'Mixture models']","['Visual Cues', 'Video Captioning', 'Representation Of Syntax', 'Public Datasets', 'Mixture Model', 'Visual Perception', 'Part-of-speech', 'Joint Model', 'Postage', 'Syntactic Structure', 'Typical Features', 'Local Features', 'Natural Language', 'Visual Representation', 'Motion Features', 'Semantic Knowledge', 'Function Words', 'Average Pooling Layer', 'Visual Components', 'RGB Features', 'Constraint Term', 'Attention Operation', 'Content Words', 'Video Features', 'Via Video', 'Template-based Methods', 'Previous Word', 'Probability Of Sequence', 'Softmax Classifier']",,66,"Video captioning is a challenging task that involves not only visual perception but also syntax representation learning. Recent progress in video captioning has been achieved through visual perception, but syntax representation learning is still under-explored. We propose a novel video captioning approach that takes into account both visual perception and syntax representation learning to generate accurate descriptions of videos. Specifically, we use sentence templates composed of Part-of-Speech (POS) tags to represent the syntax structure of captions, and accordingly, syntax representation learning is performed by directly inferring POS tags from videos. The visual perception is implemented by a mixture model which translates visual cues into lexical words that are conditional on the learned syntactic structure of sentences. Thus, a video captioning task consists of two sub-tasks: video POS tagging and visual cue translation, which are jointly modeled and trained in an end-to-end fashion. Evaluations on three public benchmark datasets demonstrate that our proposed method achieves substantially better performance than the state-of-the-art methods, which validates the superiority of joint modeling of syntax representation learning and visual perception for video captioning."
Jointly Aligning Millions of Images With Deep Penalised Reconstruction Congealing,"Roberto Annunziata, Christos Sagonas, Jacques Cali","Blue Prism; Onfido, UK",0.0,,100.0,UK,"Extrapolating fine-grained pixel-level correspondences in a fully unsupervised manner from a large set of misaligned images can benefit several computer vision and graphics problems, e.g. co-segmentation, super-resolution, image edit propagation, structure-from-motion, and 3D reconstruction. Several joint image alignment and congealing techniques have been proposed to tackle this problem, but robustness to initialisation, ability to scale to large datasets, and alignment accuracy seem to hamper their wide applicability. To overcome these limitations, we propose an unsupervised joint alignment method leveraging a densely fused spatial transformer network to estimate the warping parameters for each image and a low-capacity auto-encoder whose reconstruction error is used as an auxiliary measure of joint alignment. Experimental results on digits from multiple versions of MNIST (i.e., original, perturbed, affNIST and infiMNIST) and faces from LFW, show that our approach is capable of aligning millions of images with high accuracy and robustness to different levels and types of perturbation. Moreover, qualitative and quantitative results suggest that the proposed method outperforms state-of-the-art approaches both in terms of alignment quality and robustness to initialisation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Annunziata_Jointly_Aligning_Millions_of_Images_With_Deep_Penalised_Reconstruction_Congealing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Annunziata_Jointly_Aligning_Millions_of_Images_With_Deep_Penalised_Reconstruction_Congealing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010690/,"['Optimization', 'Image reconstruction', 'Image coding', 'Robustness', 'Computer vision', 'Image resolution', 'Task analysis']","['Millions Of Images', 'Transformer', 'Wide Application', 'Computer Vision', '3D Reconstruction', 'Alignment Quality', 'Image Alignment', 'Computer Vision Problems', 'Art Approaches', 'Convolutional Neural Network', 'Stochastic Gradient Descent', 'Bounding Box', 'Image Pairs', 'Singular Value Decomposition', 'Optimal Efficiency', 'Reference Image', 'Alignment Results', 'Large-scale Problems', 'Average Image', 'Face Detection', 'Nonlinear Perturbation', 'Alignment Problem', 'Partial Occlusion', 'Intra-class Variance', 'Alignment Errors', 'Batch Of Images', 'Distortion Measure', 'Perturbation Magnitude', 'Sum Of Entropy', 'Pixel Location']",,6,"Extrapolating fine-grained pixel-level correspondences in a fully unsupervised manner from a large set of misaligned images can benefit several computer vision and graphics problems, e.g. co-segmentation, super-resolution, image edit propagation, structure-from-motion, and 3D reconstruction. Several joint image alignment and congealing techniques have been proposed to tackle this problem, but robustness to initialisation, ability to scale to large datasets, and alignment accuracy seem to hamper their wide applicability. To overcome these limitations, we propose an unsupervised joint alignment method leveraging a densely fused spatial transformer network to estimate the warping parameters for each image and a low-capacity auto-encoder whose reconstruction error is used as an auxiliary measure of joint alignment. Experimental results on digits from multiple versions of MNIST (i.e., original, perturbed, affNIST and infiMNIST) and faces from LFW, show that our approach is capable of aligning millions of images with high accuracy and robustness to different levels and types of perturbation. Moreover, qualitative and quantitative results suggest that the proposed method outperforms state-of-the-art approaches both in terms of alignment quality and robustness to initialisation."
K-Best Transformation Synchronization,"Yifan Sun, Jiacheng Zhuo, Arnav Mohan, Qixing Huang","UT Austin, Austin, TX 78712; Liberal Arts and Science Academy, Austin, TX 78745",100.0,"USA, usa",0.0,,"In this paper, we introduce the problem of K-best transformation synchronization for the purpose of multiple scan matching. Given noisy pair-wise transformations computed between a subset of depth scan pairs, K-best transformation synchronization seeks to output multiple consistent relative transformations. This problem naturally arises in many geometry reconstruction applications, where the underlying object possesses self-symmetry. For approximately symmetric or even non-symmetric objects, K-best solutions offer an intermediate presentation for recovering the underlying single-best solution. We introduce a simple yet robust iterative algorithm for K-best transformation synchronization, which alternates between transformation propagation and transformation clustering. We present theoretical guarantees on the robust and exact recoveries of our algorithm. Experimental results demonstrate the advantage of our approach against state-of-the-art transformation synchronization techniques on both synthetic and real datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_K-Best_Transformation_Synchronization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_K-Best_Transformation_Synchronization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010631/,"['Synchronization', 'Optimization', 'Clustering algorithms', 'Standards', 'Computer vision', 'Art', 'Noise measurement']","['Propagation Clustering', 'Scan Pairs', 'Synchronization Techniques', 'Running Time', 'Sampling Density', 'Iterative Procedure', 'Convex Optimization', 'Multiple Solutions', 'Symmetry Group', 'Post-processing Step', 'Rigid Transformation', 'Local Coordinate System', 'Translation Error', 'Collection Of Objects', 'Set Of Transformations', 'Rotation Error', 'Rotation Translation', 'Input Transformation', 'RGB-D Images', 'World Coordinate System', 'Symmetric Objects', 'Convex Optimization Techniques', 'Input Object', 'Coordinate System', '3D Reconstruction', 'Problem Statement', 'Angular Error', 'Recovery Status', 'Data Matrix', 'Projection Operator']",,3,"In this paper, we introduce the problem of K-best transformation synchronization for the purpose of multiple scan matching. Given noisy pair-wise transformations computed between a subset of depth scan pairs, K-best transformation synchronization seeks to output multiple consistent relative transformations. This problem naturally arises in many geometry reconstruction applications, where the underlying object possesses self-symmetry. For approximately symmetric or even non-symmetric objects, K-best solutions offer an intermediate presentation for recovering the underlying single-best solution. We introduce a simple yet robust iterative algorithm for K-best transformation synchronization, which alternates between transformation propagation and transformation clustering. We present theoretical guarantees on the robust and exact recoveries of our algorithm. Experimental results demonstrate the advantage of our approach against state-of-the-art transformation synchronization techniques on both synthetic and real datasets."
KPConv: Flexible and Deformable Convolution for Point Clouds,"Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, FranÃ§ois Goulette, Leonidas J. Guibas",Mines ParisTech; Stanford University; Facebook AI Research,66.66666666666666,"France, usa",33.33333333333334,USA,"We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf,,https://github.com/HuguesTHOMAS/KPConv,,main,Poster,https://ieeexplore.ieee.org/document/9010002/,"['Kernel', 'Convolution', 'Three-dimensional displays', 'Correlation', 'Robustness', 'Two dimensional displays', 'Geometry']","['Point Cloud', 'Deformable Convolution', 'Euclidean Space', 'Input Point', 'Descriptive Power', 'Validation Set', 'Correlation Function', 'Kernel Function', '3D Space', 'Multilayer Perceptron', 'Convolution Operation', 'Convolution Kernel', 'Corresponding Points', 'Segmentation Task', 'Convolutional Block', 'Kernel Weight', 'Graph Convolution', 'Sparse Structure', 'Regularization Loss', 'Input Point Cloud', 'Image Convolution', 'Scene Segmentation', 'Shape Classification', 'Gradient Backpropagation', '2D Grid', 'Receptive Field', 'Architecture For Classification', 'Deep Learning']",,1634,"We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv."
Kernel Modeling Super-Resolution on Real Low-Resolution Images,"Ruofan Zhou, Sabine SÃ¼sstrunk","IC, EPFL",100.0,switzerland,0.0,,"Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super-resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their training data: low-resolution (LR) images are obtained by bicubic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images. To improve generalization and robustness of deep super-resolution CNNs on real photographs, we present a kernel modeling super-resolution network (KMSR) that incorporates blur-kernel modeling in the training. Our proposed KMSR consists of two stages: we first build a pool of realistic blur-kernels with a generative adversarial network (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental validations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Kernel_Modeling_Super-Resolution_on_Real_Low-Resolution_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Kernel_Modeling_Super-Resolution_on_Real_Low-Resolution_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010978/,"['Kernel', 'Gallium nitride', 'Image resolution', 'Training', 'Estimation', 'Signal resolution', 'Generative adversarial networks']","['Low-resolution Images', 'Kernel Model', 'Real Low-resolution Images', 'Training Data', 'Convolutional Neural Network', 'High-resolution Images', 'Deep Convolutional Neural Network', 'Generative Adversarial Networks', 'Convolution Kernel', 'Bicubic Interpolation', 'Super-resolution Network', 'Single Image Super-resolution', 'Large Datasets', 'Training Dataset', 'Factorization', 'Gaussian Kernel', 'Network Performance', 'Focal Length', 'Patch Size', 'Visual Quality', 'Blur Kernel', 'Generative Adversarial Networks Training', 'Kernel Estimation', 'Paired Datasets', 'Synthetic Images', 'Maximum A Posteriori', 'Kernel Samples', 'Optical Aberrations', 'Discriminator Network', 'Camera Model']",,101,"Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super-resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their training data: low-resolution (LR) images are obtained by bicubic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images. To improve generalization and robustness of deep super-resolution CNNs on real photographs, we present a kernel modeling super-resolution network (KMSR) that incorporates blur-kernel modeling in the training. Our proposed KMSR consists of two stages: we first build a pool of realistic blur-kernels with a generative adversarial network (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental validations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels."
Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters,"Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, Krystian Mikolajczyk",Imperial College London; Computer Vision Center,100.0,"Spain, uk",0.0,,"We introduce a novel approach for keypoint detection task that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches benchmark. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance and complexity.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Barroso-Laguna_Key.Net_Keypoint_Detection_by_Handcrafted_and_Learned_CNN_Filters_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Barroso-Laguna_Key.Net_Keypoint_Detection_by_Handcrafted_and_Learned_CNN_Filters_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010664/,"['Detectors', 'Feature extraction', 'Computer architecture', 'Robustness', 'Training', 'Indexes', 'Microsoft Windows']","['Convolutional Neural Network', 'Learning Convolutional Neural Network', 'Keypoint Detection', 'Loss Function', 'Range Of Scales', 'Matching Performance', 'Learned Filters', 'Validation Set', 'Window Size', 'Feature Maps', 'Feature Learning', 'Scale Changes', 'Learnable Parameters', 'Scale Level', 'Large Window', 'Local Detection', 'Matching Score', 'Local Descriptors', 'Correct Detection', 'Geometric Transformation', 'Multi-scale Representation', 'Response Map', 'Keypoint Locations', 'Si Detector', 'Pyramid Level', 'Non-maximum Suppression', 'Orientation Estimation', 'Local Features', 'Feature Detection', 'Convolutional Layers']",,168,"We introduce a novel approach for keypoint detection task that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches benchmark. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance and complexity."
LADN: Local Adversarial Disentangling Network for Facial Makeup and De-Makeup,"Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, Chi-Keung Tang","Tencent; HKUST; UIUC; CMU, HKUST; Stanford University, HKUST",80.0,"hong kong, usa",20.0,China,"We propose a local adversarial disentangling network (LADN) for facial makeup and de-makeup. Central to our method are multiple and overlapping local adversarial discriminators in a content-style disentangling network for achieving local detail transfer between facial images, with the use of asymmetric loss functions for dramatic makeup styles with high-frequency details. Existing techniques do not demonstrate or fail to transfer high-frequency details in a global adversarial setting, or train a single local discriminator only to ensure image structure consistency and thus work only for relatively simple styles. Unlike others, our proposed local adversarial discriminators can distinguish whether the generated local image details are consistent with the corresponding regions in the given reference image in cross-image style transfer in an unsupervised setting. Incorporating these technical contributions, we achieve not only state-of-the-art results on conventional styles but also novel results involving complex and dramatic styles with high-frequency details covering large areas across multiple facial features. A carefully designed dataset of unpaired before and after makeup images is released at https://georgegu1997.github.io/LADN-project-page.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gu_LADN_Local_Adversarial_Disentangling_Network_for_Facial_Makeup_and_De-Makeup_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_LADN_Local_Adversarial_Disentangling_Network_for_Facial_Makeup_and_De-Makeup_ICCV_2019_paper.pdf,https://georgegu1997.github.io/LADN-project-page,,,main,Poster,https://ieeexplore.ieee.org/document/9009577/,"['Face', 'Generators', 'Image color analysis', 'Gallium nitride', 'Periodic structures', 'Machine learning', 'Mouth']","['Facial Makeup', 'Facial Features', 'Reference Image', 'Face Images', 'Global Set', 'Style Transfer', 'Asymmetric Loss', 'Latent Variables', 'Skin Color', 'Generative Adversarial Networks', 'Identification Of Sources', 'Source Images', 'Target Domain', 'Image Domain', 'Reconstruction Loss', 'Face Identity', 'Transfer Problem', 'Specific Style', 'Eyelashes', 'Facial Structure', 'Smoothness Loss', 'Facial Color', 'Local Path', 'Translation Problems', 'Eye Area', 'Disentangled Representation', 'Range Of Styles', 'Smooth Transition']",,58,"We propose a local adversarial disentangling network (LADN) for facial makeup and de-makeup. Central to our method are multiple and overlapping local adversarial discriminators in a content-style disentangling network for achieving local detail transfer between facial images, with the use of asymmetric loss functions for dramatic makeup styles with high-frequency details. Existing techniques do not demonstrate or fail to transfer high-frequency details in a global adversarial setting, or train a single local discriminator only to ensure image structure consistency and thus work only for relatively simple styles. Unlike others, our proposed local adversarial discriminators can distinguish whether the generated local image details are consistent with the corresponding regions in the given reference image in cross-image style transfer in an unsupervised setting. Incorporating these technical contributions, we achieve not only state-of-the-art results on conventional styles but also novel results involving complex and dramatic styles with high-frequency details covering large areas across multiple facial features. A carefully designed dataset of unpaired before and after makeup images is released at https://georgegu1997.github.io/LADN-project-page."
LAP-Net: Level-Aware Progressive Network for Image Dehazing,"Yunan Li, Qiguang Miao, Wanli Ouyang, Zhenxin Ma, Huijuan Fang, Chao Dong, Yining Quan","School of Computer Science and Technology, Xidian Univeristy, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; The University of Sydney, SenseTime Computer Vision Research Group, Australia",100.0,"China, australia, china",0.0,,"In this paper, we propose a level-aware progressive network (LAP-Net) for single image dehazing. Unlike previous multi-stage algorithms that generally learn in a coarse-to-fine fashion, each stage of LAP-Net learns different levels of haze with different supervision. Then the network can progressively learn the gradually aggravating haze. With this design, each stage can focus on a region with specific haze level and restore clear details. To effectively fuse the results of varying haze levels at different stages, we develop an adaptive integration strategy to yield the final dehazed image. This strategy is achieved by a hierarchical integration scheme, which is in cooperation with the memory network and the domain knowledge of dehazing to highlight the best-restored regions of each stage. Extensive experiments on both real-world images and two dehazing benchmarks validate the effectiveness of our proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_LAP-Net_Level-Aware_Progressive_Network_for_Image_Dehazing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_LAP-Net_Level-Aware_Progressive_Network_for_Image_Dehazing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009462/,"['Image restoration', 'Atmospheric modeling', 'Scattering', 'Entropy', 'Degradation', 'Cameras', 'Fuses']","['Image Dehazing', 'Integration Scheme', 'Hierarchical Strategy', 'Classification Task', 'Feature Maps', 'Object Detection', 'Pedestrian', 'Attention Mechanism', 'Distal Regions', 'Pose Estimation', 'Stage Of Network', 'Real-world Scenes', 'Local Entropy', 'Scene Depth', 'Transmission Map', 'Pixels In Channel', 'Atmospheric Light', 'Dark Channel']",,58,"In this paper, we propose a level-aware progressive network (LAP-Net) for single image dehazing. Unlike previous multi-stage algorithms that generally learn in a coarse-to-fine fashion, each stage of LAP-Net learns different levels of haze with different supervision. Then the network can progressively learn the gradually aggravating haze. With this design, each stage can focus on a region with specific haze level and restore clear details. To effectively fuse the results of varying haze levels at different stages, we develop an adaptive integration strategy to yield the final dehazed image. This strategy is achieved by a hierarchical integration scheme, which is in cooperation with the memory network and the domain knowledge of dehazing to highlight the best-restored regions of each stage. Extensive experiments on both real-world images and two dehazing benchmarks validate the effectiveness of our proposed method."
LIP: Local Importance-Based Pooling,"Ziteng Gao, Limin Wang, Gangshan Wu","State Key Laboratory for Novel Software Technology, Nanjing University, China",100.0,china,0.0,,"Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for discriminative tasks, there is a possibility that these layers lose the discriminative details due to improper pooling strategies, which could hinder the learning process and eventually result in suboptimal models. In this paper, we present a unified framework over the existing downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a local importance view. In this framework, we analyze the issues of these widely-used pooling layers and figure out the criteria for designing an effective downsampling layer. According to this analysis, we propose a conceptually simple, general, and effective pooling layer based on local importance modeling, termed as Local Importance-based Pooling (LIP). LIP can automatically enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. Experiment results show that LIP consistently yields notable gains with different depths and different architectures on ImageNet classification. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent improvement (>=1.4%) over the vanilla ResNets, and especially achieve the current state-of-the-art performance in detecting small objects under the single-scale testing scheme.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_LIP_Local_Importance-Based_Pooling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_LIP_Local_Importance-Based_Pooling_ICCV_2019_paper.pdf,,https://github.com/sebgao/LIP,,main,Poster,https://ieeexplore.ieee.org/document/9010729/,"['Lips', 'Convolution', 'Task analysis', 'Clutter', 'Feature extraction', 'Aggregates', 'Neural networks']","['Local Pool', 'Convolutional Neural Network', 'Feature Maps', 'Receptive Field', 'Max-pooling', 'Pooling Layer', 'Discriminative Features', 'Downscaling', 'Average Pooling', 'Small Objects', 'Memory Consumption', 'Importance Weights', 'Local View', 'ImageNet Classification', 'Strided Convolution', 'Important Functions', 'Challenging Task', 'Sigmoid Function', 'Window Size', 'Faster R-CNN', 'Deformable Convolution', 'Spatial Normalization', 'Global Average Pooling', 'Background Clutter', 'Shift Invariance', 'Attention Weights', 'Principled Way', 'Deformation Model']",,59,"Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for discriminative tasks, there is a possibility that these layers lose the discriminative details due to improper pooling strategies, which could hinder the learning process and eventually result in suboptimal models. In this paper, we present a unified framework over the existing downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a local importance view. In this framework, we analyze the issues of these widely-used pooling layers and figure out the criteria for designing an effective downsampling layer. According to this analysis, we propose a conceptually simple, general, and effective pooling layer based on local importance modeling, termed as Local Importance-based Pooling (LIP). LIP can automatically enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. Experiment results show that LIP consistently yields notable gains with different depths and different architectures on ImageNet classification. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent improvement (≥1.4%) over the vanilla ResNets, and especially achieve the current state-of-the-art performance in detecting small objects under the single-scale testing scheme
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
."
LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis,"Zhe Liu, Shunbo Zhou, Chuanzhe Suo, Peng Yin, Wen Chen, Hesheng Wang, Haoang Li, Yun-Hui Liu",The Chinese University of Hong Kong; Shanghai Jiao Tong University; Carnegie Mellon University,100.0,"China, Hong Kong, usa",0.0,,"Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments. In this paper, we develop a novel deep neural network, named LPD-Net (Large-scale Place Description Network), which can extract discriminative and generalizable global descriptors from the raw 3D point cloud. Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. We implement the proposed global descriptor in solving point cloud based retrieval tasks to achieve the large-scale place recognition. Comparison results show that our LPD-Net is much better than PointNetVLAD and reaches the state-of-the-art. We also compare our LPD-Net with the vision-based solutions to show the robustness of our approach to different weather and light conditions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_LPD-Net_3D_Point_Cloud_Learning_for_Large-Scale_Place_Recognition_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_LPD-Net_3D_Point_Cloud_Learning_for_Large-Scale_Place_Recognition_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009029/,"['Three-dimensional displays', 'Feature extraction', 'Task analysis', 'Two dimensional displays', 'Graphical models', 'Distribution functions', 'Robustness']","['Point Cloud', '3D Point', '3D Point Cloud', 'Large-scale Environments', 'Place Recognition', 'Large-scale Place Recognition', 'Neural Network', 'Deep Neural Network', 'Light Conditions', 'Distribution Characteristics', 'Local Features', 'Local Structure', 'Adaptive Feature', 'Local Module', 'Spatial Distribution Characteristics', 'Adaptive Modulation', 'Global Descriptors', 'Raw Point', 'Raw Point Cloud', 'Convolutional Neural Network', 'Graph Neural Networks', 'Local Structure Information', 'Bad Weather Conditions', 'Point Cloud Data', 'Cartesian Space', 'Feature Space', 'Illumination Variations', 'Local Distribution', 'Feature Learning', 'K-nearest Neighbor']",,203,"Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments. In this paper, we develop a novel deep neural network, named LPD-Net (Large-scale Place Description Network), which can extract discriminative and generalizable global descriptors from the raw 3D point cloud. Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. We implement the proposed global descriptor in solving point cloud based retrieval tasks to achieve the large-scale place recognition. Comparison results show that our LPD-Net is much better than PointNetVLAD and reaches the state-of-the-art. We also compare our LPD-Net with the vision-based solutions to show the robustness of our approach to different weather and light conditions."
Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation,"Weifeng Ge, Sheng Guo, Weilin Huang, Matthew R. Scott","1Malong Technologies, Shenzhen, China; 2Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China; 1Malong Technologies, Shenzhen, China; 2Shenzhen Malong Artiﬁcial Intelligence Research Center, Shenzhen, China; 3The University of Hong Kong",20.0,Hong Kong,80.0,China,"Weakly-supervised instance segmentation aims to detect and segment object instances precisely, given image-level labels only. Unlike previous methods which are composed of multiple offline stages, we propose Sequential Label Propagation and Enhancement Networks (referred as Label-PEnet) that progressively transforms image-level labels to pixel-wise labels in a coarse-to-fine manner. We design four cascaded modules including multi-label classification, object detection, instance refinement and instance segmentation, which are implemented sequentially by sharing the same backbone. The cascaded pipeline is trained alternatively with a curriculum learning strategy that generalizes labels from high level images to low-level pixels gradually with increasing accuracy. In addition, we design a proposal calibration module to explore the ability of classification networks to find key pixels that identify object parts, which serves as a post validation strategy running in the inverse order. We evaluate the efficiency of our Label-PEnet in mining instance masks on standard benchmarks: PASCAL VOC 2007 and 2012. Experimental results show that Label-PEnet outperforms the state-of-art algorithms by a clear margin, and obtains comparable performance even with fully supervised approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ge_Label-PEnet_Sequential_Label_Propagation_and_Enhancement_Networks_for_Weakly_Supervised_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ge_Label-PEnet_Sequential_Label_Propagation_and_Enhancement_Networks_for_Weakly_Supervised_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010838/,"['Proposals', 'Image segmentation', 'Object detection', 'Object recognition', 'Calibration', 'Training', 'Task analysis']","['Instance Segmentation', 'Network Propagation', 'Sequential Propagation', 'Object Detection', 'Multi-label', 'Surgical Margins', 'Object Segmentation', 'Standard Benchmark', 'Curriculum Learning', 'Object Instances', 'PASCAL VOC', 'Inverse Order', 'Image-level Labels', 'Final Results', 'Convolutional Neural Network', 'Deep Neural Network', 'Convolutional Layers', 'Bounding Box', 'Semantic Segmentation', 'Object Location', 'Object Proposals', 'Attention Map', 'Region Proposal Network', 'Detection Module', 'Neural Attention', 'Object Bounding Boxes', 'Backbone Network', 'Image Tags', 'ImageNet Pre-trained Model', 'Conditional Random Field']",,37,"Weakly-supervised instance segmentation aims to detect and segment object instances precisely, given image-level labels only. Unlike previous methods which are composed of multiple offline stages, we propose Sequential Label Propagation and Enhancement Networks (referred as Label-PEnet) that progressively transforms image-level labels to pixel-wise labels in a coarse-to-fine manner. We design four cascaded modules including multi-label classification, object detection, instance refinement and instance segmentation, which are implemented sequentially by sharing the same backbone. The cascaded pipeline is trained alternatively with a curriculum learning strategy that generalizes labels from high level images to low-level pixels gradually with increasing accuracy. In addition, we design a proposal calibration module to explore the ability of classification networks to find key pixels that identify object parts, which serves as a post validation strategy running in the inverse order. We evaluate the efficiency of our Label-PEnet in mining instance masks on standard benchmarks: PASCAL VOC 2007 and 2012. Experimental results show that Label-PEnet outperforms the state-of-art algorithms by a clear margin, and obtains comparable performance even with fully supervised approaches."
Language Features Matter: Effective Language Representations for Vision-Language Tasks,"Andrea Burns, Reuben Tan, Kate Saenko, Stan Sclaroff, Bryan A. Plummer",Boston University,100.0,usa,0.0,,"Shouldn't language and vision features be treated equally in vision-language (VL) tasks? Many VL approaches treat the language component as an afterthought, using simple language models that are either built upon fixed word embeddings trained on text-only data or are learned from scratch. We conclude that language features deserve more attention, which has been informed by experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms a LSTM on retrieval-style tasks; state-of-the-art representations such as BERT perform relatively poorly on vision-language tasks. From this comprehensive set of experiments we can propose a set of best practices for incorporating the language component of vision-language tasks. To further elevate language features, we also show that knowledge in vision-language problems can be transferred across tasks to gain performance with multi-task training. This multi-task training is applied to a new Graph Oriented Vision-Language Embedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original visual-language graph built from Visual Genome, providing a ready-to-use vision-language embedding: http://ai.bu.edu/grovle.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Burns_Language_Features_Matter_Effective_Language_Representations_for_Vision-Language_Tasks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Burns_Language_Features_Matter_Effective_Language_Representations_for_Vision-Language_Tasks_ICCV_2019_paper.pdf,http://ai.bu.edu/grovle,,,main,Poster,https://ieeexplore.ieee.org/document/9008550/,"['Task analysis', 'Visualization', 'Training', 'Computer architecture', 'Bit error rate', 'Adaptation models', 'Grounding']","['Vision-language Tasks', 'Language Model', 'Word Embedding', 'Image Captioning', 'WordNet', 'Language Components', 'Visual Question Answering', 'Multi-task Training', 'Task Performance', 'Natural Language', 'Hidden Layer', 'Intersection Over Union', 'Weight Decay', 'Feed-forward Network', 'Words In Sentences', 'Word Representations', 'Mean Of Recall', 'Vocabulary Size', 'Target Task', 'Relation Graph', 'Pre-trained Word Embeddings', 'Projection Layer', 'Pre-trained Embeddings', 'Pointwise Mutual Information', 'Continuous Bag-of-words', 'Visual Context', 'Current Word', 'Original Word', 'Context Words', 'Masked Language Model']",,13,"Shouldn't language and vision features be treated equally in vision-language (VL) tasks? Many VL approaches treat the language component as an afterthought, using simple language models that are either built upon fixed word embeddings trained on text-only data or are learned from scratch. We conclude that language features deserve more attention, which has been informed by experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms a LSTM on retrieval-style tasks; state-of-the-art representations such as BERT perform relatively poorly on vision-language tasks. From this comprehensive set of experiments we can propose a set of best practices for incorporating the language component of vision-language tasks. To further elevate language features, we also show that knowledge in vision-language problems can be transferred across tasks to gain performance with multi-task training. This multi-task training is applied to a new Graph Oriented Vision-Language Embedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original visual-language graph built from Visual Genome, providing a ready-to-use vision-language embedding: http://ai.bu.edu/grovle."
Language-Agnostic Visual-Semantic Embeddings,"JÃ´natas Wehrmann, Douglas M. Souza, MaurÃ­cio A. Lopes, Rodrigo C. Barros","School of Technology, Pontif ´ıcia Universidade Cat ´olica do Rio Grande do Sul",100.0,Brazil,0.0,,"This paper proposes a framework for training language-invariant cross-modal retrieval models. We also introduce a novel character-based word-embedding approach, allowing the model to project similar words across languages into the same word-embedding space. In addition, by performing cross-modal retrieval at the character level, the storage requirements for a text encoder decrease substantially, allowing for lighter and more scalable retrieval architectures. The proposed language-invariant textual encoder based on characters is virtually unaffected in terms of storage requirements when novel languages are added to the system. Our contributions include new methods for building character-level-based word-embeddings, an improved loss function, and a novel cross-language alignment module that not only makes the architecture language-invariant, but also presents better predictive performance. We show that our models outperform the current state-of-the-art in both single and multi-language scenarios. This work can be seen as the basis of a new path on retrieval research, now allowing for the effective use of captions in multiple-language scenarios. Code is available at https://github.com/jwehrmann/lavse.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wehrmann_Language-Agnostic_Visual-Semantic_Embeddings_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wehrmann_Language-Agnostic_Visual-Semantic_Embeddings_ICCV_2019_paper.pdf,,https://github.com/jwehrmann/lavse,,main,Poster,https://ieeexplore.ieee.org/document/9010736/,"['Semantics', 'Training', 'Task analysis', 'Image coding', 'Vocabulary', 'Encoding', 'Architecture']","['Loss Function', 'Predictive Performance', 'Word Embedding', 'Text Encoder', 'English Language', 'Convolutional Layers', 'Similarity Measure', 'Bilingual', 'Training Images', 'Latent Space', 'Fully-connected Layer', 'Textual Descriptions', 'Incarnation', 'Image Retrieval', 'Final Representation', 'Semantic Space', 'Image Captioning', 'Japanese Language', 'Distinct Language', 'Transliteration', 'Query Examples', 'Image Encoder', 'Image Retrieval Task', 'Concatenation Of Vectors', 'Similarity Matrix']",,25,"This paper proposes a framework for training language-invariant cross-modal retrieval models. We also introduce a novel character-based word-embedding approach, allowing the model to project similar words across languages into the same word-embedding space. In addition, by performing cross-modal retrieval at the character level, the storage requirements for a text encoder decrease substantially, allowing for lighter and more scalable retrieval architectures. The proposed language-invariant textual encoder based on characters is virtually unaffected in terms of storage requirements when novel languages are added to the system. Our contributions include new methods for building character-level-based word-embeddings, an improved loss function, and a novel cross-language alignment module that not only makes the architecture language-invariant, but also presents better predictive performance. We show that our models outperform the current state-of-the-art in both single and multi-language scenarios. This work can be seen as the basis of a new path on retrieval research, now allowing for the effective use of captions in multiple-language scenarios. Code is available at https://github.com/jwehrmann/lavse."
Language-Conditioned Graph Networks for Relational Reasoning,"Ronghang Hu, Anna Rohrbach, Trevor Darrell, Kate Saenko","Boston University; University of California, Berkeley",100.0,usa,0.0,,"Solving grounded language tasks often requires reasoning about relationships between objects in the context of a given task. For example, to answer the question ""What color is the mug on the plate?"" we must check the color of the specific mug that satisfies the ""on"" relationship with respect to the plate. Recent work has proposed various methods capable of complex relational reasoning. However, most of their power is in the inference structure, while the scene is represented with simple local appearance features. In this paper, we take an alternate approach and build contextualized representations for objects in a visual scene to support relational reasoning. We propose a general framework of Language-Conditioned Graph Networks (LCGN), where each node represents an object, and is described by a context-aware representation from related objects through iterative message passing conditioned on the textual input. E.g., conditioning on the ""on"" relationship to the plate, the object ""mug"" gathers messages from the object ""plate"" to update its representation to ""mug on the plate"", which can be easily consumed by a simple classifier for answer prediction. We experimentally show that our LCGN approach effectively supports relational reasoning and improves performance across several tasks and datasets. Our code is available at http://ronghanghu.com/lcgn.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Language-Conditioned_Graph_Networks_for_Relational_Reasoning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Language-Conditioned_Graph_Networks_for_Relational_Reasoning_ICCV_2019_paper.pdf,,http://ronghanghu.com/lcgn,,main,Poster,https://ieeexplore.ieee.org/document/9010268/,"['Cognition', 'Task analysis', 'Visualization', 'Message passing', 'Context modeling', 'Feature extraction', 'Knowledge discovery']","['Network Graph', 'Relational Reasoning', 'Local Features', 'Appearance Features', 'Input Text', 'Contextual Representation', 'Feature Maps', 'Related Information', 'Object Detection', 'Visual Features', 'Improvement In Accuracy', 'Bounding Box', 'Question Answering', 'Target Object', 'Connection Weights', 'Inference Procedure', 'Objects In The Scene', 'Relevant Objects', 'Language Input', 'Visual Question Answering', 'Scene Graph', 'Reasoning Tasks', 'Scene Representation', 'Convolutional Feature Maps', 'Higher-order Relationships', 'Representation Of Entities', 'Static Weight', 'Relational Context', 'Final Representation']",,90,"Solving grounded language tasks often requires reasoning about relationships between objects in the context of a given task. For example, to answer the question ""What color is the mug on the plate?"" we must check the color of the specific mug that satisfies the ""on"" relationship with respect to the plate. Recent work has proposed various methods capable of complex relational reasoning. However, most of their power is in the inference structure, while the scene is represented with simple local appearance features. In this paper, we take an alternate approach and build contextualized representations for objects in a visual scene to support relational reasoning. We propose a general framework of Language-Conditioned Graph Networks (LCGN), where each node represents an object, and is described by a context-aware representation from related objects through iterative message passing conditioned on the textual input. E.g., conditioning on the ""on"" relationship to the plate, the object ""mug"" gathers messages from the object ""plate"" to update its representation to ""mug on the plate"", which can be easily consumed by a simple classifier for answer prediction. We experimentally show that our LCGN approach effectively supports relational reasoning and improves performance across several tasks and datasets. Our code is available at http://ronghanghu.com/lcgn."
Laplace Landmark Localization,"Joseph P. Robinson, Yuncheng Li, Ning Zhang, Yun Fu, Sergey Tulyakov",Northeastern University; Snap Inc.,50.0,china,50.0,USA,"Landmark localization in images and videos is a classic problem solved in various ways. Nowadays, with deep networks prevailing throughout machine learning, there are revamped interests in pushing facial landmark detectors to handle more challenging data. Most efforts use network objectives based on L1 or L2 norms, which have several disadvantages. First of all, the generated heatmaps translate to the locations of landmarks (i.e. confidence maps) from which predicted landmark locations (i.e. the means) get penalized without accounting for the spread: a high- scatter corresponds to low confidence and vice-versa. For this, we introduce a LaplaceKL objective that penalizes for low confidence. Another issue is a dependency on labeled data, which are expensive to obtain and susceptible to error. To address both issues, we propose an adversarial training framework that leverages unlabeled data to improve model performance. Our method claims state-of-the-art on all of the 300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in the Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size: 1/8 the number of channels (i.e. 0.0398 MB) is comparable to the state-of-the-art in real-time on CPU. Thus, this work is of high practical value to real-life application.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Robinson_Laplace_Landmark_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Robinson_Laplace_Landmark_Localization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008763/,"['Heating systems', 'Training', 'Data models', 'Face', 'Gallium nitride', 'Three-dimensional displays', 'Videos']","['Landmark Localization', 'Heatmap', 'Unlabeled Data', 'Adversarial Training', 'Landmark Detection', 'Neural Network', 'Convolutional Layers', 'Data Augmentation', 'Parametrized', 'Bounding Box', 'Generative Adversarial Networks', 'Normalization Factor', 'Domain Adaptation', 'Pixel Location', 'Challenging Dataset', 'Pixel Coordinates', 'Laplace Distribution', 'Normalized Mean Square Error', 'Head Pose', 'Face Size', 'Face Alignment', 'Number Of Landmarks']",,37,"Landmark localization in images and videos is a classic problem solved in various ways. Nowadays, with deep networks prevailing throughout machine learning, there are revamped interests in pushing facial landmark detectors to handle more challenging data. Most efforts use network objectives based on L1 or L2 norms, which have several disadvantages. First of all, the generated heatmaps translate to the locations of landmarks (i.e. confidence maps) from which predicted landmark locations (i.e. the means) get penalized without accounting for the spread: a high- scatter corresponds to low confidence and vice-versa. For this, we introduce a LaplaceKL objective that penalizes for low confidence. Another issue is a dependency on labeled data, which are expensive to obtain and susceptible to error. To address both issues, we propose an adversarial training framework that leverages unlabeled data to improve model performance. Our method claims state-of-the-art on all of the 300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in the Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size: 1/8 the number of channels (i.e. 0.0398 MB) is comparable to the state-of-the-art in real-time on CPU. Thus, this work is of high practical value to real-life application."
Large-Scale Tag-Based Font Retrieval With Generative Feature Learning,"Tianlang Chen, Zhaowen Wang, Ning Xu, Hailin Jin, Jiebo Luo",University of Rochester; Adobe Research,50.0,usa,50.0,USA,"Font selection is one of the most important steps in a design workflow. Traditional methods rely on ordered lists which require significant domain knowledge and are often difficult to use even for trained professionals. In this paper, we address the problem of large-scale tag-based font retrieval which aims to bring semantics to the font selection process and enable people without expert knowledge to use fonts effectively. We collect a large-scale font tagging dataset of high-quality professional fonts. The dataset contains nearly 20,000 fonts, 2,000 tags, and hundreds of thousands of font-tag relations. We propose a novel generative feature learning algorithm that leverages the unique characteristics of fonts. The key idea is that font images are synthetic and can therefore be controlled by the learning algorithm. We design an integrated rendering and learning process so that the visual feature from one image can be used to reconstruct another image with different text. The resulting feature captures important font design details while is robust to nuisance factors such as text. We propose a novel attention mechanism to re-weight the visual feature for joint visual-text modeling. We combine the feature and the attention mechanism in a novel recognition-retrieval model. Experimental results show that our method significantly outperforms the state-of-the-art for the important problem of large-scale tag-based font retrieval.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Large-Scale_Tag-Based_Font_Retrieval_With_Generative_Feature_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Large-Scale_Tag-Based_Font_Retrieval_With_Generative_Feature_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010967/,"['Tagging', 'Generative adversarial networks', 'Visualization', 'Gallium nitride', 'Image retrieval', 'Machine learning', 'Image recognition']","['Semantic', 'Learning Algorithms', 'Attention Mechanism', 'Large-scale Datasets', 'Training Set', 'Convolutional Neural Network', 'Image Features', 'Cross-entropy Loss', 'Generative Adversarial Networks', 'Fully-connected Layer', 'Attention Module', 'Recognition Model', 'Synthetic Images', 'Image Texture', 'Element-wise Multiplication', 'Image Retrieval', 'Attention Map', 'Attention Weights', 'Query Set', 'Affinity Score', 'Retrieval Results', 'Query Features', 'Ranking Loss', 'Input Image', 'Textual Features', 'Image Retrieval Task']",,11,"Font selection is one of the most important steps in a design workflow. Traditional methods rely on ordered lists which require significant domain knowledge and are often difficult to use even for trained professionals. In this paper, we address the problem of large-scale tag-based font retrieval which aims to bring semantics to the font selection process and enable people without expert knowledge to use fonts effectively. We collect a large-scale font tagging dataset of high-quality professional fonts. The dataset contains nearly 20,000 fonts, 2,000 tags, and hundreds of thousands of font-tag relations. We propose a novel generative feature learning algorithm that leverages the unique characteristics of fonts. The key idea is that font images are synthetic and can therefore be controlled by the learning algorithm. We design an integrated rendering and learning process so that the visual feature from one image can be used to reconstruct another image with different text. The resulting feature captures important font design details while is robust to nuisance factors such as text. We propose a novel attention mechanism to re-weight the visual feature for joint visual-text modeling. We combine the feature and the attention mechanism in a novel recognition-retrieval model. Experimental results show that our method significantly outperforms the state-of-the-art for the important problem of large-scale tag-based font retrieval."
Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation,"Ruijia Xu, Guanbin Li, Jihan Yang, Liang Lin","School of Data and Computer Science, Sun Yat-sen University, China; School of Data and Computer Science, Sun Yat-sen University, China; DarkMatter AI Research",66.66666666666666,"China, china",33.33333333333334,USA,"Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task. In this paper, we empirically reveal that the erratic discrimination of the target domain mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we propose a novel parameter-free Adaptive Feature Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range of values can result in significant transfer gains, implying that those task-specific features with larger norms are more transferable. Our method successfully unifies the computation of both standard and partial domain adaptation with more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective approach will shed some light on the future research of transfer learning. Code is available at https://github.com/jihanyang/AFN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Larger_Norm_More_Transferable_An_Adaptive_Feature_Norm_Approach_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Larger_Norm_More_Transferable_An_Adaptive_Feature_Norm_Approach_for_ICCV_2019_paper.pdf,,https://github.com/jihanyang/AFN,,main,Oral,https://ieeexplore.ieee.org/document/9009009/,"['Task analysis', 'Adaptation models', 'Computational modeling', 'Standards', 'Degradation', 'Robustness', 'Neural networks']","['Domain Adaptation', 'Large Norm', 'Feature Norm', 'Domain Shift', 'Target Domain', 'Source Domain', 'Target Task', 'Negative Transfer', 'Task-specific Features', 'Divergence', 'Deep Neural Network', 'Training Phase', 'Target Sample', 'Decision Boundary', 'Robust Evaluation', 'Challenging Dataset', 'Target Category', 'Transfer Task', 'Domain Gap', 'Maximum Mean Discrepancy', 'Label Space', 'Unlabeled Target Data', 'Adversarial Domain Adaptation', 'Source Labels', 'Negative Gap', 'Asymmetric Manner', 'Statistical Distance', 'Unrelated Samples', 'Evaluation Phase']",,306,"Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task. In this paper, we empirically reveal that the erratic discrimination of the target domain mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we propose a novel parameter-free Adaptive Feature Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range of values can result in significant transfer gains, implying that those task-specific features with larger norms are more transferable. Our method successfully unifies the computation of both standard and partial domain adaptation with more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective approach will shed some light on the future research of transfer learning. Code is available at https://github.com/jihanyang/AFN."
Layout-Induced Video Representation for Recognizing Agent-in-Place Actions,"Ruichi Yu, Hongcheng Wang, Ang Li, Jingxiao Zheng, Vlad I. Morariu, Larry S. Davis","Comcast Applied AI Research; University of Maryland, College Park; Adobe Research",66.66666666666666,"USA, usa",33.33333333333334,USA,"We address scene layout modeling for recognizing agent-in-place actions, which are actions associated with agents who perform them and the places where they occur, in the context of outdoor home surveillance. We introduce a novel representation to model the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training scenes to unseen scenes in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places to explicitly model scene layout. LIVR partitions the semantic features of a scene into different places to force the network to learn generic place-based feature descriptions which are independent of specific scene layouts; then, LIVR dynamically aggregates features based on connectivities of places in each specific scene to model its layout. We introduce a new Agent-in-Place Action (APA) dataset(The dataset is pending legal review and will be released upon the acceptance of this paper.) to show that our method allows neural network models to generalize significantly better to unseen scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Layout-Induced_Video_Representation_for_Recognizing_Agent-in-Place_Actions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Layout-Induced_Video_Representation_for_Recognizing_Agent-in-Place_Actions_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010382/,"['Feature extraction', 'Layout', 'Semantics', 'Surveillance', 'Transforms', 'Three-dimensional displays', 'Aggregates']","['Descriptive Characteristics', 'Feature Aggregation', 'Actual Dataset', 'Geometry And Topology', 'Training Scenes', 'Discretion', 'Training Time', 'Knowledge Transfer', 'Max-pooling', 'Semantic Segmentation', 'Action Recognition', 'Similar Appearance', 'Distance Map', 'Action Classes', 'Segmentation Map', 'Number Of Filters', 'Element-wise Multiplication', 'Mean Average Precision', 'Video Action Recognition', 'Video Surveillance', 'Raw Pixel', 'Semantic Map', 'Action Recognition Datasets', 'Abstract Features', 'Validation Set', 'Motion Patterns', 'Feature Maps']",,,"We address scene layout modeling for recognizing agent-in-place actions, which are actions associated with agents who perform them and the places where they occur, in the context of outdoor home surveillance. We introduce a novel representation to model the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training scenes to unseen scenes in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places to explicitly model scene layout. LIVR partitions the semantic features of a scene into different places to force the network to learn generic place-based feature descriptions which are independent of specific scene layouts; then, LIVR dynamically aggregates features based on connectivities of places in each specific scene to model its layout. We introduce a new Agent-in-Place Action (APA) dataset to show that our method allows neural network models to generalize significantly better to unseen scenes."
LayoutVAE: Stochastic Scene Layout Generation From a Label Set,"Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, Greg Mori","University of British Columbia, Borealis AI; Simon Fraser University, Borealis AI",100.0,canada,0.0,,"Recently there is an increasing interest in scene generation within the research community. However, models used for generating scene layouts from textual description largely ignore plausible visual variations within the structure dictated by the text. We propose LayoutVAE, a variational autoencoder based framework for generating stochastic scene layouts. LayoutVAE is a versatile modeling framework that allows for generating full image layouts given a label set, or per label layouts for an existing image given a new label. In addition, it is also capable of detecting unusual layouts, potentially providing a way to evaluate layout generation problem. Extensive experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset verifies the effectiveness of our proposed framework.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jyothi_LayoutVAE_Stochastic_Scene_Layout_Generation_From_a_Label_Set_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jyothi_LayoutVAE_Stochastic_Scene_Layout_Generation_From_a_Label_Set_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010959/,"['Layout', 'Stochastic processes', 'Semantics', 'Image generation', 'Gallium nitride', 'Predictive models', 'Three-dimensional displays']","['Stochastic Generation', 'Layout Generation', 'Textual Descriptions', 'Variational Autoencoder', 'Latent Variables', 'Bounding Box', 'Variables In Equation', 'Network Inference', 'Distribution Of Counts', 'Objects In The Scene', 'Negative Log-likelihood', 'COCO Dataset', 'Details Of Objects', 'Predicted Bounding Box', 'Latent Code', 'Bidirectional Model', 'Object Counting', 'Bounding Box Coordinates', 'Labels In Order', 'Evidence Lower Bound', 'Scene Graph', 'Conditional Variational Autoencoder', 'Variational Autoencoder Model', 'Category Counts', 'Input Sentence', 'Training Set', 'Semantic Segmentation', 'Image Generation', 'Stochastic Model', 'Number Of Objects']",,69,"Recently there is an increasing interest in scene generation within the research community. However, models used for generating scene layouts from textual description largely ignore plausible visual variations within the structure dictated by the text. We propose LayoutVAE, a variational autoencoder based framework for generating stochastic scene layouts. LayoutVAE is a versatile modeling framework that allows for generating full image layouts given a label set, or per label layouts for an existing image given a new label. In addition, it is also capable of detecting unusual layouts, potentially providing a way to evaluate layout generation problem. Extensive experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset verifies the effectiveness of our proposed framework."
Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting,"Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, Xiang Bai",Microsoft Research Asia; Huazhong University of Science and Technology; University of Oxford,66.66666666666666,"china, uk",33.33333333333334,USA,"Dense crowd counting aims to predict thousands of human instances from an image, by calculating integrals of a density map over image pixels. Existing approaches mainly suffer from the extreme density variations. Such density pattern shift poses challenges even for multi-scale model ensembling. In this paper, we propose a simple yet effective approach to tackle this problem. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets. Second, each patch density map is automatically normalized by an online center learning strategy with a multipolar center loss. Such a design can significantly condense the density distribution into several clusters, and enable that the density variance can be learned by a single model. Extensive experiments demonstrate the superiority of the proposed method. Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on the ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Learn_to_Scale_Generating_Multipolar_Normalized_Density_Maps_for_Crowd_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Learn_to_Scale_Generating_Multipolar_Normalized_Density_Maps_for_Crowd_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009068/,"['Head', 'Feature extraction', 'Estimation', 'Kernel', 'Robustness', 'Training', 'Asia']","['Density Map', 'Crowd Counting', 'Density Distribution', 'Density Estimation', 'Variation In Density', 'Density Levels', 'Density Patterns', 'Central Loss', 'Convolutional Neural Network', 'Gaussian Kernel', 'Scaling Factor', 'Image Dataset', 'Dense Regions', 'Regional Groups', 'Huge Variety', 'Number Of Centers', 'Large Density', 'Multiple Centers', 'Selective Regions', 'Sparse Regions', 'CNN-based Methods', 'Ratio Scale', 'Feature Pooling', 'Time Overhead', 'Rational Behavior', 'Multi-scale Features', 'Counting Accuracy', 'Image Patches', 'Small Head']",,78,"Dense crowd counting aims to predict thousands of human instances from an image, by calculating integrals of a density map over image pixels. Existing approaches mainly suffer from the extreme density variations. Such density pattern shift poses challenges even for multi-scale model ensembling. In this paper, we propose a simple yet effective approach to tackle this problem. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets. Second, each patch density map is automatically normalized by an online center learning strategy with a multipolar center loss. Such a design can significantly condense the density distribution into several clusters, and enable that the density variance can be learned by a single model. Extensive experiments demonstrate the superiority of the proposed method. Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on the ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively."
Learnable Triangulation of Human Pose,"Karim Iskakov, Egor Burkov, Victor Lempitsky, Yury Malkov","Samsung AI Center, Moscow",0.0,,100.0,Russia,"We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second, more complex, solution is based on volumetric aggregation of 2D feature maps from the 2D backbone followed by refinement via 3D convolutions that produce final 3D joint heatmaps. Crucially, both of the approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Iskakov_Learnable_Triangulation_of_Human_Pose_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Iskakov_Learnable_Triangulation_of_Human_Pose_ICCV_2019_paper.pdf,https://saic-violet.github.io/learnable-triangulation,,,main,Oral,https://ieeexplore.ieee.org/document/9010013/,"['Three-dimensional displays', 'Two dimensional displays', 'Heating systems', 'Pose estimation', 'Cameras', 'Convolutional neural networks', 'Detectors']","['Human Pose', 'State Of The Art', 'Pose Estimation', 'Triangulation Method', 'Human Pose Estimation', '3D Pose', 'Project Page', '3D Joint', 'Convolutional Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Average Error', 'Bounding Box', 'Motion Capture', '3D Coordinates', 'Joint Position', 'Maximum Position', 'Camera View', 'L1 Loss', 'Ground Truth Annotations', 'Volumetric Approach', 'Global Coordinates', 'Huber Loss', 'Triangulation Approach', 'Volumetric Method', 'Reprojection Error', 'Pose Estimation Methods', '2D Position', 'Projection Matrix', 'Convolutional Neural Networks Backbone']",,237,"We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow implicit modelling a human pose prior. Crucially, both approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multiview state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page."
Learned Video Compression,"Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander G. Anderson, Lubomir Bourdev","WaveOne, Inc.",0.0,,100.0,USA,"We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so. We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs in the low-latency mode. On standard-definition videos, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger than our algorithm. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing. We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual). Secondly, we present a framework for ML-based spatial rate control --- a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rippel_Learned_Video_Compression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rippel_Learned_Video_Compression_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010297/,"['Image coding', 'Bit rate', 'Optical imaging', 'Video compression', 'Video coding', 'Video codecs']","['Video Compression', 'Learned Video Compression', 'Reference Frame', 'Optical Flow', 'Spatial Control', 'Video Coding', 'Compression Artifacts', 'Simple Translation', 'Neural Network', 'Recurrent Neural Network', 'Procedure Codes', 'Compressor', 'Human Visual System', 'Signal Classification', 'Inverse Operation', 'Video Dataset', 'Image Compression', 'Coding Efficiency', 'Flow Map', 'Compression Algorithm', 'Motion Compensation', 'Video Encoding', 'Compression Approach', 'Entropy Coding', 'State St', 'Spatial Multiplexing', 'ML Models', 'ML-based Approaches', 'Interpolation']",,123,"We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so. We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs in the low-latency mode. On standard-definition videos, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger than our algorithm. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing. We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual). Secondly, we present a framework for ML-based spatial rate control - a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting."
Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking,"Ziyuan Huang, Changhong Fu, Yiming Li, Fuling Lin, Peng Lu","School of Automotive Studies, Tongji University, China; Adaptive Robotic Controls Lab, Hong Kong Polytechnic University, Hong Kong, China; School of Mechanical Engineering, Tongji University, China",100.0,"Hong Kong, china",0.0,,"Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009064/,"['Correlation', 'Training', 'Frequency-domain analysis', 'Unmanned aerial vehicles', 'Object tracking', 'Target tracking', 'Visualization']","['Unmanned Aerial Vehicles', 'Real-time Tracking', 'Correlation Filter', 'Unmanned Aerial Vehicle Tracking', 'Background Information', 'Detection Process', 'Changes In Appearance', 'Boundary Effects', 'Object Tracking', 'Illumination Variations', 'Response Map', 'Search Region', 'Benchmark', 'Learning Rate', 'Frequency Domain', 'Negative Samples', 'Regularization Term', 'Deep Features', 'Handcrafted Features', 'Small Problems', 'Histogram Of Oriented Gradients', 'Inverse Operation', 'Cases Of Occlusion', 'Object Appearance', 'Previous Frame', 'Current Frame', 'Original Objective']",,271,"Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications."
Learning Across Tasks and Domains,"Pierluigi Zama Ramirez, Alessio Tonioni, Samuele Salti, Luigi Di Stefano","Department of Computer Science and Engineering (DISI), University of Bologna, Italy",100.0,italy,0.0,,"Recent works have proven that many relevant visual tasks are closely related one to another. Yet, this connection is seldom deployed in practice due to the lack of practical methodologies to transfer learned concepts across different training processes. In this work, we introduce a novel adaptation framework that can operate across both task and domains. Our framework learns to transfer knowledge across tasks in a fully supervised domain (e.g., synthetic data) and use this knowledge on a different domain where we have only partial supervision (e.g., real data). Our proposal is complementary to existing domain adaptation techniques and extends them to cross tasks scenarios providing additional performance gains. We prove the effectiveness of our framework across two challenging tasks (i.e., monocular depth estimation and semantic segmentation) and four different domains (Synthia, Carla, Kitti, and Cityscapes).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ramirez_Learning_Across_Tasks_and_Domains_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ramirez_Learning_Across_Tasks_and_Domains_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008279/,"['Task analysis', 'Training', 'Semantics', 'Feature extraction', 'Visualization', 'Estimation', 'Transforms']","['Knowledge Transfer', 'Transfer Learning', 'Visual Task', 'Semantic Segmentation', 'Domain Adaptation', 'Depth Estimation', 'Domain Adaptation Techniques', 'Monocular Depth Estimation', 'Root Mean Square Error', 'Training Set', 'Deep Learning', 'Deep Neural Network', 'Convolutional Layers', 'Batch Normalization', 'Domain Shift', 'Deep Features', 'Final Layer', 'Target Domain', 'Related Tasks', 'Pixel Level', 'Annotated Samples', 'Multi-task Learning', 'Transfer Task', 'Source Domain', 'Real Domain', 'Task Network', 'Transfer Network', 'Semantic Segmentation Network', 'Specific Domains']",,27,"Recent works have proven that many relevant visual tasks are closely related one to another. Yet, this connection is seldom deployed in practice due to the lack of practical methodologies to transfer learned concepts across different training processes. In this work, we introduce a novel adaptation framework that can operate across both task and domains. Our framework learns to transfer knowledge across tasks in a fully supervised domain (e.g., synthetic data) and use this knowledge on a different domain where we have only partial supervision (e.g., real data). Our proposal is complementary to existing domain adaptation techniques and extends them to cross tasks scenarios providing additional performance gains. We prove the effectiveness of our framework across two challenging tasks (i.e., monocular depth estimation and semantic segmentation) and four different domains (Synthia, Carla, Kitti, and Cityscapes)."
Learning Combinatorial Embedding Networks for Deep Graph Matching,"Runzhong Wang, Junchi Yan, Xiaokang Yang","Department of Computer Science and Engineering, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",100.0,China,0.0,,"Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010940/,"['Peer-to-peer computing', 'Feature extraction', 'Tensile stress', 'Neural networks', 'Kernel', 'Training', 'Pipelines']","['Graph Matching', 'Deep Graph Matching', 'Deep Network', 'Gaussian Kernel', 'Nodes In The Graph', 'Combinatorial Nature', 'Functional Affinity', 'Deep Embedding', 'Training Set', 'Deep Learning', 'Positive Matrix', 'Cross-entropy Loss', 'Graph Structure', 'Matching Model', 'Graph Convolutional Network', 'Node Features', 'Graph Neural Networks', 'Matching Accuracy', 'Affinity Matrix', 'Spectral Matching', 'Node Embeddings', 'Linear Assignment', 'Permutation Matrix', 'Affine Model', 'Delaunay Triangulation', 'Offset Vector', 'PASCAL VOC', 'CNN Features', 'Higher-order Information', 'Neural Network']",,124,"Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods."
Learning Compositional Neural Information Fusion for Human Parsing,"Wenguan Wang, Zhijie Zhang, Siyuan Qi, Jianbing Shen, Yanwei Pang, Ling Shao","Inception Institute of Artiﬁcial Intelligence, UAE; School of Electrical and Information Engineering, Tianjin University; University of California, Los Angeles, USA",100.0,"china, uae, usa",0.0,,"This work proposes to combine neural networks with the compositional hierarchy of human bodies for efficient and complete human parsing. We formulate the approach as a neural information fusion framework. Our model assembles the information from three inference processes over the hierarchy: direct inference (directly predicting each part of a human body using image information), bottom-up inference (assembling knowledge from constituent parts), and top-down inference (leveraging context from parent nodes). The bottom-up and top-down inferences explicitly model the compositional and decompositional relations in human bodies, respectively. In addition, the fusion of multi-source information is conditioned on the inputs, i.e., by estimating and considering the confidence of the sources. The whole model is end-to-end differentiable, explicitly modeling information flows and structures. Our approach is extensively evaluated on four popular datasets, outperforming the state-of-the-arts in all cases, with a fast processing speed of 23fps. Our code and results have been released to help ease future research in this direction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Learning_Compositional_Neural_Information_Fusion_for_Human_Parsing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Compositional_Neural_Information_Fusion_for_Human_Parsing_ICCV_2019_paper.pdf,,https://github.com/ZzzjzzZ/CompositionalHumanParsing,,main,Poster,https://ieeexplore.ieee.org/document/9009026/,"['Neural networks', 'Computational modeling', 'Task analysis', 'Semantics', 'Biological system modeling', 'Computer vision', 'Estimation']","['Parsing', 'Information Fusion', 'Composition Information', 'Neural Network', 'Root Node', 'Top-down And Bottom-up', 'Neural Framework', 'Direct Inference', 'Multi-source Information', 'Convolutional Layers', 'Composite Structure', 'Input Image', 'Raw Images', 'Final Prediction', 'Graphical Model', 'Ensemble Method', 'Backbone Network', 'Leaf Node', 'Bottom-up Processes', 'Pose Estimation', 'Fashion Clothing', 'Fully Convolutional Network', 'Network Inference', 'Fusion Network', 'Child Nodes', 'Feature Extraction Network', 'Directed Networks', 'Power Of Neural Networks', 'Sunglasses', 'Deep Neural Network']",,91,"This work proposes to combine neural networks with the compositional hierarchy of human bodies for efficient and complete human parsing. We formulate the approach as a neural information fusion framework. Our model assembles the information from three inference processes over the hierarchy: direct inference (directly predicting each part of a human body using image information), bottom-up inference (assembling knowledge from constituent parts), and top-down inference (leveraging context from parent nodes). The bottom-up and top-down inferences explicitly model the compositional and decompositional relations in human bodies, respectively. In addition, the fusion of multi-source information is conditioned on the inputs, i.e., by estimating and considering the confidence of the sources. The whole model is end-to-end differentiable, explicitly modeling information flows and structures. Our approach is extensively evaluated on four popular datasets, outperforming the state-of-the-arts in all cases, with a fast processing speed of 23fps. Our code and results have been released to help ease future research in this direction."
Learning Compositional Representations for Few-Shot Recognition,"Pavel Tokmakov, Yu-Xiong Wang, Martial Hebert","Robotics Institute, Carnegie Mellon University",100.0,usa,0.0,,"One of the key limitations of modern deep learning approaches lies in the amount of data required to train them. Humans, by contrast, can learn to recognize novel categories from just a few examples. Instrumental to this rapid learning ability is the compositional structure of concept representations in the human brain --- something that deep learning models are lacking. In this work, we make a step towards bridging this gap between human and machine learning by introducing a simple regularization technique that allows the learned representation to be decomposable into parts. Our method uses category-level attribute annotations to disentangle the feature space of a network into subspaces corresponding to the attributes. These attributes can be either purely visual, like object parts, or more abstract, like openness and symmetry. We demonstrate the value of compositional representations on three datasets: CUB-200-2011, SUN397, and ImageNet, and show that they require fewer examples to learn classifiers for novel categories.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tokmakov_Learning_Compositional_Representations_for_Few-Shot_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tokmakov_Learning_Compositional_Representations_for_Few-Shot_Recognition_ICCV_2019_paper.pdf,https://sites.google.com/view/comprepr/home,,,main,Poster,https://ieeexplore.ieee.org/document/9010671/,"['Image representation', 'Training', 'Machine learning', 'Task analysis', 'Computer vision', 'Visualization', 'Image coding']","['Representation Of Composition', 'Deep Learning', 'Deep Learning Models', 'ImageNet', 'Representation Learning', 'Object Parts', 'Modern Deep Learning', 'Convolutional Neural Network', 'Deep Network', 'Data Augmentation', 'Base Classifiers', 'Levels Of Hierarchy', 'Image Representation', 'Linear Classifier', 'Training Examples', 'Representational Analysis', 'Linear Layer', 'Single Example', 'Hard Constraints', 'Image Categories', 'Soft Constraints', 'Few-shot Learning', 'Representation Of Properties', 'Image Embedding', 'Half Of The Image', 'Zero-shot', 'Deep Representation', 'Seagull', 'Sum Of Similarities', 'Computer Vision']",,71,"One of the key limitations of modern deep learning approaches lies in the amount of data required to train them. Humans, by contrast, can learn to recognize novel categories from just a few examples. Instrumental to this rapid learning ability is the compositional structure of concept representations in the human brain --- something that deep learning models are lacking. In this work, we make a step towards bridging this gap between human and machine learning by introducing a simple regularization technique that allows the learned representation to be decomposable into parts. Our method uses category-level attribute annotations to disentangle the feature space of a network into subspaces corresponding to the attributes. These attributes can be either purely visual, like object parts, or more abstract, like openness and symmetry. We demonstrate the value of compositional representations on three datasets: CUB-200-2011, SUN397, and ImageNet, and show that they require fewer examples to learn classifiers for novel categories."
Learning Deep Priors for Image Dehazing,"Yang Liu, Jinshan Pan, Jimmy Ren, Zhixun Su",Dalian University of Technology; Nanjing University of Science and Technology; Guilin University of Electronic Technology; SenseTime Research,75.0,"China, china",25.0,China,"Image dehazing is a well-known ill-posed problem, which usually requires some image priors to make the problem well-posed. We propose an effective iteration algorithm with deep CNNs to learn haze-relevant priors for image dehazing. We formulate the image dehazing problem as the minimization of a variational model with favorable data fidelity terms and prior terms to regularize the model. We solve the variational model based on the classical gradient descent method with built-in deep CNNs so that iteration-wise image priors for the atmospheric light, transmission map and clear image can be well estimated. Our method combines the properties of both the physical formation of image dehazing as well as deep learning approaches. We show that it is able to generate clear images as well as accurate atmospheric light and transmission maps. Extensive experimental results demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in both benchmark datasets and real-world images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_Deep_Priors_for_Image_Dehazing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_Deep_Priors_for_Image_Dehazing_ICCV_2019_paper.pdf,https://lewisyangliu.github.io/projects/LDP,,,main,Poster,https://ieeexplore.ieee.org/document/9010821/,"['Atmospheric modeling', 'Data models', 'Image color analysis', 'Minimization', 'Optimization methods', 'Machine learning']","['Image Dehazing', 'Deep Learning', 'Gradient Descent', 'Iterative Algorithm', 'Deep Convolutional Neural Network', 'Benchmark Datasets', 'Clear Image', 'Light Images', 'Gradient Descent Method', 'Real-world Images', 'Prior Imaging', 'Atmospheric Transmittance', 'Prior Term', 'Data Fidelity Term', 'Transmission Map', 'Atmospheric Light', 'Learning Process', 'Deep Network', 'Step Size', 'Color Distortion', 'Iterative Scheme', 'Depth Images', 'Quantitative Results', 'Image Properties', 'Deep Learning Techniques', 'Running Time', 'Deep Learning-based Methods']",,81,"Image dehazing is a well-known ill-posed problem, which usually requires some image priors to make the problem well-posed. We propose an effective iteration algorithm with deep CNNs to learn haze-relevant priors for image dehazing. We formulate the image dehazing problem as the minimization of a variational model with favorable data fidelity terms and prior terms to regularize the model. We solve the variational model based on the classical gradient descent method with built-in deep CNNs so that iteration-wise image priors for the atmospheric light, transmission map and clear image can be well estimated. Our method combines the properties of both the physical formation of image dehazing as well as deep learning approaches. We show that it is able to generate clear images as well as accurate atmospheric light and transmission maps. Extensive experimental results demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in both benchmark datasets and real-world images."
Learning Discriminative Model Prediction for Tracking,"Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte","CVL, ETH Zürich, Switzerland",100.0,Switzerland,0.0,,"The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bhat_Learning_Discriminative_Model_Prediction_for_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bhat_Learning_Discriminative_Model_Prediction_for_Tracking_ICCV_2019_paper.pdf,,https://github.com/visionml/pytracking,,main,Oral,https://ieeexplore.ieee.org/document/9010649/,"['Target tracking', 'Predictive models', 'Feature extraction', 'Computer architecture', 'Training', 'Computational modeling', 'Adaptation models']","['Prediction Model', 'Online Learning', 'Target Information', 'Target Model', 'Discriminative Learning', 'Inference Stage', 'Appearance Information', 'Final Model', 'Training Set', 'Gradient Descent', 'Target Region', 'Large-scale Datasets', 'Bounding Box', 'Target Class', 'Step Length', 'Backbone Network', 'Background Regions', 'Optimal Mode', 'Siamese Network', 'AUC Score', 'Future Frames', 'Test Frame', 'Final Architecture', 'Online Tracking', 'Hinge Loss', 'Feature Extraction Backbone', 'Offline Learning', 'Feature Space', 'Regression Residuals', 'Average Overlap']",,882,"The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking."
Learning Feature-to-Feature Translator by Alternating Back-Propagation for Generative Zero-Shot Learning,"Yizhe Zhu, Jianwen Xie, Bingchen Liu, Ahmed Elgammal","Hikvision Research Institute; Department of Computer Science, Rutgers University",100.0,"china, usa",0.0,,"We investigate learning feature-to-feature translator networks by alternating back-propagation as a general-purpose solution to zero-shot learning (ZSL) problems. It is a generative model-based ZSL framework. In contrast to models based on generative adversarial networks (GAN) or variational autoencoders (VAE) that require auxiliary networks to assist the training, our model consists of a single conditional generator that maps class-level semantic features and Gaussian white noise vectors accounting for instance-level latent factors to visual features, and is trained by maximum likelihood estimation. The training process is a simple yet effective alternating back-propagation process that iterates the following two steps: (i) the inferential back-propagation to infer the latent noise vector of each observed example, and (ii) the learning back-propagation to update the model parameters. We show that, with slight modifications, our model is capable of learning from incomplete visual features for ZSL. We conduct extensive comparisons with existing generative ZSL methods on five benchmarks, demonstrating the superiority of our method in not only ZSL performance but also convergence speed and computational cost. Specifically, our model outperforms the existing state-of-the-art methods by a remarkable margin up to 3.1% and 4.0% in ZSL and generalized ZSL settings, respectively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Learning_Feature-to-Feature_Translator_by_Alternating_Back-Propagation_for_Generative_Zero-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Learning_Feature-to-Feature_Translator_by_Alternating_Back-Propagation_for_Generative_Zero-Shot_Learning_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
Learning Filter Basis for Convolutional Neural Network Compression,"Yawei Li, Shuhang Gu, Luc Van Gool, Radu Timofte","Computer Vision Lab, ETH Zurich, Switzerland; KU Leuven, Belgium; Computer Vision Lab, ETH Zurich, Switzerland",100.0,"belgium, switzerland",0.0,,"Convolutional neural networks (CNNs) based solutions have achieved state-of-the-art performances for many computer vision tasks, including classification and super-resolution of images. Usually the success of these methods comes with a cost of millions of parameters due to stacking deep convolutional layers. Moreover, quite a large number of filters are also used for a single convolutional layer, which exaggerates the parameter burden of current methods. Thus, in this paper, we try to reduce the number of parameters of CNNs by learning a basis of the filters in convolutional layers. For the forward pass, the learned basis is used to approximate the original filters and then used as parameters for the convolutional layers. We validate our proposed solution for multiple CNN architectures on image classification and image super-resolution benchmarks and compare favorably to the existing state-of-the-art in terms of reduction of parameters and preservation of accuracy. Code is available at https://github.com/ofsoundof/learning_filter_basis",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Learning_Filter_Basis_for_Convolutional_Neural_Network_Compression_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Learning_Filter_Basis_for_Convolutional_Neural_Network_Compression_ICCV_2019_paper.pdf,,https://github.com/ofsoundof/learning_filter_basis,,main,Poster,https://ieeexplore.ieee.org/document/9009046/,"['Three-dimensional displays', 'Convolution', 'Kernel', 'Task analysis', 'Two dimensional displays', 'Image coding', 'Neural networks']","['Neural Network', 'Convolutional Neural Network', 'Network Compression', 'Convolutional Layers', 'Image Classification', 'Super-resolution', 'Vision Tasks', 'Millions Of Parameters', 'Filters In Each Convolutional Layer', 'Original Filter', 'Error Rate', 'Deep Neural Network', 'Linear Combination', 'Feature Maps', 'Network Parameters', 'Residual Block', 'Input Channels', 'Output Channels', 'Channel Dimension', 'Compression Ratio', '3D Filter', 'Compression Rate', 'Modern Networks', 'Compression Method', 'Super-resolution Network', 'Network Quantization', 'Network Pruning', 'Low-rank Approximation', 'Lowest Error Rate', '3D Kernel']",,51,"Convolutional neural networks (CNNs) based solutions have achieved state-of-the-art performances for many computer vision tasks, including classification and super-resolution of images. Usually the success of these methods comes with a cost of millions of parameters due to stacking deep convolutional layers. Moreover, quite a large number of filters are also used for a single convolutional layer, which exaggerates the parameter burden of current methods. Thus, in this paper, we try to reduce the number of parameters of CNNs by learning a basis of the filters in convolutional layers. For the forward pass, the learned basis is used to approximate the original filters and then used as parameters for the convolutional layers. We validate our proposed solution for multiple CNN architectures on image classification and image super-resolution benchmarks and compare favorably to the existing state-of-the-art in terms of reduction of parameters and preservation of accuracy. Code is available at https://github.com/ofsoundof/learning_filter_basis."
Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization,"Md Mahfuzur Rahman Siddiquee, Zongwei Zhou, Nima Tajbakhsh, Ruibin Feng, Michael B. Gotway, Yoshua Bengio, Jianming Liang",Mila – Quebec Artiﬁcial Intelligence Institute; Arizona State University; Arizona State University; Mila – Quebec Artiﬁcial Intelligence Institute; Mayo Clinic,80.0,"Canada, usa",20.0,Canada,"Generative adversarial networks (GANs) have ushered in a revolution in image-to-image translation. The development and proliferation of GANs raises an interesting question: can we train a GAN to remove an object, if present, from an image while otherwise preserving the image? Specifically, can a GAN ""virtually heal"" anyone by turning his medical image, with an unknown health status (diseased or healthy), into a healthy one, so that diseased regions could be revealed by subtracting those two images? Such a task requires a GAN to identify a minimal subset of target pixels for domain translation, an ability that we call fixed-point translation, which no GAN is equipped with yet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1) supervising same-domain translation through a conditional identity loss, and (2) regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss. Based on fixed-point translation, we further derive a novel framework for disease detection and localization using only image-level annotation. Qualitative and quantitative evaluations demonstrate that the proposed method outperforms the state of the art in multi-domain image-to-image translation and that it surpasses predominant weakly-supervised localization methods in both disease detection and localization. Implementation is available at https://github.com/jlianglab/Fixed-Point-GAN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Siddiquee_Learning_Fixed_Points_in_Generative_Adversarial_Networks_From_Image-to-Image_Translation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Siddiquee_Learning_Fixed_Points_in_Generative_Adversarial_Networks_From_Image-to-Image_Translation_ICCV_2019_paper.pdf,,https://github.com/jlianglab/Fixed-Point-GAN,,main,Poster,https://ieeexplore.ieee.org/document/9008137/,"['Gallium nitride', 'Diseases', 'Generative adversarial networks', 'Hair', 'Image color analysis', 'Biomedical imaging', 'Face']","['Disease Detection', 'Generative Adversarial Networks', 'Detection Methods', 'Medical Imaging', 'State Of The Art', 'Domain Classifier', 'Loss Of Identity', 'Cycle Consistency', 'Cycle Consistency Loss', 'Receiver Operating Characteristic Curve', 'Input Image', 'Training Strategy', 'Natural Images', 'Target Domain', 'Anomaly Detection', 'Difference Map', 'Inference Time', 'Synthetic Images', 'Computer-aided Diagnosis', 'Hair Color', 'Class Activation Maps', 'Source Domain', 'Computed Tomography Pulmonary Angiography', 'Blond Hair', 'Facial Color', 'Translation Method', 'Black Hair', 'Data-generating Model', 'Radiology Reports', 'Identity Transformation']",,57,"Generative adversarial networks (GANs) have ushered in a revolution in image-to-image translation. The development and proliferation of GANs raises an interesting question: can we train a GAN to remove an object, if present, from an image while otherwise preserving the image? Specifically, can a GAN ``virtually heal'' anyone by turning his medical image, with an unknown health status (diseased or healthy), into a healthy one, so that diseased regions could be revealed by subtracting those two images? Such a task requires a GAN to identify a minimal subset of target pixels for domain translation, an ability that we call fixed-point translation, which no GAN is equipped with yet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1) supervising same-domain translation through a conditional identity loss, and (2) regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss. Based on fixed-point translation, we further derive a novel framework for disease detection and localization using only image-level annotation. Qualitative and quantitative evaluations demonstrate that the proposed method outperforms the state of the art in multi-domain image-to-image translation and that it surpasses predominant weakly-supervised localization methods in both disease detection and localization. Implementation is available at https://github.com/jlianglab/Fixed-Point-GAN."
Learning Implicit Generative Models by Matching Perceptual Features,"Cicero Nogueira dos Santos, Youssef Mroueh, Inkit Padhi, Pierre Dognin","IBM Research, T.J. Watson Research Center, NY",0.0,,100.0,USA,"Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves state-of-the-art results for challenging benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/dos_Santos_Learning_Implicit_Generative_Models_by_Matching_Perceptual_Features_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/dos_Santos_Learning_Implicit_Generative_Models_by_Matching_Perceptual_Features_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008236/,"['Feature extraction', 'Kernel', 'Training', 'Generators', 'Games', 'Convergence', 'Method of moments']","['Implicit Generative Model', 'Transfer Learning', 'Kernel Function', 'Generative Adversarial Networks', 'Feature Matching', 'Mean Matching', 'Style Transfer', 'Moment Matching', 'Average Time', 'Quantitative Results', 'Feature Maps', 'Adam Optimizer', 'Stochastic Gradient Descent', 'Deep Convolutional Neural Network', 'Use Of Features', 'Second Moment', 'Supplementary Materials For Details', 'Method Of Moments', 'General Architecture', 'Variational Autoencoder', 'Maximum Mean Discrepancy', 'Fr√©chet Inception Distance', 'Reproducing Kernel Hilbert Space', 'ImageNet Classification', 'Latent Code', 'Stable Training', 'Target Dataset', 'Encoder Network', 'Loss Function', 'Neural Network']",,8,"Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves state-of-the-art results for challenging benchmarks."
Learning Joint 2D-3D Representations for Depth Completion,"Yun Chen, Bin Yang, Ming Liang, Raquel Urtasun","Uber Advanced Technologies Group, University of Toronto; Uber Advanced Technologies Group",50.0,Canada,50.0,USA,"In this paper, we tackle the problem of depth completion from RGBD data. Towards this goal, we design a simple yet effective neural network block that learns to extract joint 2D and 3D features. Specifically, the block consists of two domain-specific sub-networks that apply 2D convolution on image pixels and continuous convolution on 3D points, with their output features fused in image space. We build the depth completion network simply by stacking the proposed block, which has the advantage of learning hierarchical representations that are fully fused between 2D and 3D spaces at multiple levels. We demonstrate the effectiveness of our approach on the challenging KITTI depth completion benchmark and show that our approach outperforms the state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008241/,"['Three-dimensional displays', 'Two dimensional displays', 'Convolution', 'Feature extraction', 'Fuses', 'Task analysis', 'Sensors']","['Joint Learning', 'Depth Completion', 'Neural Network', '3D Space', 'Output Feature', '3D Point', '3D Features', '2D Space', 'RGB-D Data', 'Root Mean Square Error', 'Objective Function', 'Convolutional Network', 'Convolutional Neural Network', 'Validation Set', 'Feature Maps', 'Point Cloud', 'Geometric Features', 'Light Detection And Ranging', 'Semantic Segmentation', 'Depth Images', 'Dense Depth', 'Depth Estimation', 'Appearance Features', 'Density Imaging', '2D Representation', '3D Representation', 'Sparse Point', 'Smoothness Loss', 'Multi-task Learning', 'Multilayer Perceptron']",,113,"In this paper, we tackle the problem of depth completion from RGBD data. Towards this goal, we design a simple yet effective neural network block that learns to extract joint 2D and 3D features. Specifically, the block consists of two domain-specific sub-networks that apply 2D convolution on image pixels and continuous convolution on 3D points, with their output features fused in image space. We build the depth completion network simply by stacking the proposed block, which has the advantage of learning hierarchical representations that are fully fused between 2D and 3D spaces at multiple levels. We demonstrate the effectiveness of our approach on the challenging KITTI depth completion benchmark and show that our approach outperforms the state-of-the-art."
Learning Lightweight Lane Detection CNNs by Self Attention Distillation,"Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy",The Chinese University of Hong Kong; SenseTime Group Limited; Nanyang Technological University,100.0,"Hong Kong, Singapore, usa",0.0,,"Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing top- down and layer-wise attention distillation within the net- work itself. SAD can be easily incorporated in any feed- forward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet- 18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Learning_Lightweight_Lane_Detection_CNNs_by_Self_Attention_Distillation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Learning_Lightweight_Lane_Detection_CNNs_by_Self_Attention_Distillation_ICCV_2019_paper.pdf,,https://github.com/cardwing/Codes-for-Lane-Detection,,main,Poster,https://ieeexplore.ieee.org/document/9009567/,"['Training', 'Image segmentation', 'Task analysis', 'Feature extraction', 'Roads', 'Semantics', 'Message passing']","['Convolutional Neural Network', 'Lane Detection', 'Attention Distillation', 'Contextual Information', 'Representation Learning', 'Attention Map', 'Additional Labels', 'Lightweight Model', 'Popular Benchmark', 'Severe Occlusion', 'Supervisory Signal', 'Poor Lighting Conditions', 'Image Pixels', 'Network Layer', 'Final Output', 'Cross-entropy Loss', 'Probability Function', 'Lower Layer', 'Generative Adversarial Networks', 'Visual Attention', 'Deep Supervision', 'Message Passing', 'Multi-task Learning', 'Semantic Segmentation Task', 'Segmentation Map', 'Video Frames', 'Distillation Loss', 'Teacher Network', 'Semantic Segmentation', 'Background Pixels']",,435,"Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of `free' supervision for further representation learning through performing top- down and layer-wise attention distillation within the net- work itself. SAD can be easily incorporated in any feed- forward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet- 18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 × fewer parameters and runs 10 × faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks."
Learning Local Descriptors With a CDF-Based Dynamic Soft Margin,"Linguang Zhang, Szymon Rusinkiewicz",Princeton University,100.0,usa,0.0,,"The triplet loss is adopted by a variety of learning tasks, such as local feature descriptor learning. However, its standard formulation with a hard margin only leverages part of the training data in each mini-batch. Moreover, the margin is often empirically chosen or determined through computationally expensive validation, and stays unchanged during the entire training session. In this work, we propose a simple yet effective method to overcome the above limitations. The core idea is to replace the hard margin with a non-parametric soft margin, which is dynamically updated. The major observation is that the difficulty of a triplet can be inferred from the cumulative distribution function of the triplets' signed distances to the decision boundary. We demonstrate through experiments on both real-valued and binary local feature descriptors that our method leads to state-of-the-art performance on popular benchmarks, while eliminating the need to determine the best margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010244/,"['Training', 'Hamming distance', 'Euclidean distance', 'Principal component analysis', 'Task analysis', 'Probability density function']","['Local Descriptors', 'Soft Margin', 'Descriptive Characteristics', 'Decision Boundary', 'Triplet Loss', 'Loss Function', 'Hardness', 'Distance Matrix', 'Probability Density Function', 'Image Registration', 'Stochastic Gradient Descent', 'Training Stage', 'Hyperbolic Tangent', 'Image Patches', '3D Point', 'Matched Pairs', 'Image Retrieval', 'Binary String', 'Hamming Distance', 'Siamese Network', 'Distortion Types', 'Input Image Patch', 'Softplus', 'Illumination Changes', 'Patch Pairs', 'Sign Function']",,20,"The triplet loss is adopted by a variety of learning tasks, such as local feature descriptor learning. However, its standard formulation with a hard margin only leverages part of the training data in each mini-batch. Moreover, the margin is often empirically chosen or determined through computationally expensive validation, and stays unchanged during the entire training session. In this work, we propose a simple yet effective method to overcome the above limitations. The core idea is to replace the hard margin with a non-parametric soft margin, which is dynamically updated. The major observation is that the difficulty of a triplet can be inferred from the cumulative distribution function of the triplets' signed distances to the decision boundary. We demonstrate through experiments on both real-valued and binary local feature descriptors that our method leads to state-of-the-art performance on popular benchmarks, while eliminating the need to determine the best margin."
Learning Local RGB-to-CAD Correspondences for Object Pose Estimation,"Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jana KoÅ¡eckÃ¡","Department of Computer Science, George Mason University, Fairfax VA; Siemens Corporate Technology, Princeton NJ",50.0,USA,50.0,USA,"We consider the problem of 3D object pose estimation. While much recent work has focused on the RGB domain, the reliance on accurately annotated images limits generalizability and scalability. On the other hand, the easily available object CAD models are rich sources of data, providing a large number of synthetically rendered images. In this paper, we solve this key problem of existing methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. Our key innovations compared to existing work include removing the need for either real-world textures for CAD models or explicit 3D pose annotations for RGB images. We achieve this through a series of objectives that learn how to select keypoints and enforce viewpoint and modality invariance across RGB images and CAD model renderings. Our experiments demonstrate that the proposed method can reliably estimate object pose in RGB images and generalize to object instances not seen during training.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009793/,"['Three-dimensional displays', 'Solid modeling', 'Pose estimation', 'Training', 'Two dimensional displays', 'Rendering (computer graphics)', 'Shape']","['Pose Estimation', 'Human Pose Estimation', 'Object Pose', 'Estimation Problem', 'RGB Images', 'Image Annotation', 'CAD Model', 'Object Instances', 'Rich Source Of Data', '3D Pose', 'Series Of Objects', 'Training Data', 'Convolutional Neural Network', 'Convolutional Layers', 'Local Features', 'Object Detection', 'Bounding Box', 'Representation Learning', 'Depth Images', 'Quadruplet', 'Triplet Loss', 'Relative Pose', 'Real Depth', 'Reprojection Error', 'Keypoint Locations', '3D Distance', 'Instances Of Categories', 'Alternative Training', 'Convolutional Neural Networks Backbone', 'Texture Model']",,11,"We consider the problem of 3D object pose estimation. While much recent work has focused on the RGB domain, the reliance on accurately annotated images limits generalizability and scalability. On the other hand, the easily available object CAD models are rich sources of data, providing a large number of synthetically rendered images. In this paper, we solve this key problem of existing methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. Our key innovations compared to existing work include removing the need for either real-world textures for CAD models or explicit 3D pose annotations for RGB images. We achieve this through a series of objectives that learn how to select keypoints and enforce viewpoint and modality invariance across RGB images and CAD model renderings. Our experiments demonstrate that the proposed method can reliably estimate object pose in RGB images and generalize to object instances not seen during training."
Learning Meshes for Dense Visual SLAM,"Michael Bloesch, Tristan Laidlow, Ronald Clark, Stefan Leutenegger, Andrew J. Davison","Dyson Robotics Laboratory at Imperial College London, UK; Smart Robotics Lab, Imperial College London, UK",100.0,uk,0.0,,"Estimating motion and surrounding geometry of a moving camera remains a challenging inference problem. From an information theoretic point of view, estimates should get better as more information is included, such as is done in dense SLAM, but this is strongly dependent on the validity of the underlying models. In the present paper, we use triangular meshes as both compact and dense geometry representation. To allow for simple and fast usage, we propose a view-based formulation for which we predict the in-plane vertex coordinates directly from images and then employ the remaining vertex depth components as free variables. Flexible and continuous integration of information is achieved through the use of a residual based inference technique. This so-called factor graph encodes all information as mapping from free variables to residuals, the squared sum of which is minimised during inference. We propose the use of different types of learnable residuals, which are trained end-to-end to increase their suitability as information bearing models and to enable accurate and reliable estimation. Detailed evaluation of all components is provided on both synthetic and real data which confirms the practicability of the presented approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bloesch_Learning_Meshes_for_Dense_Visual_SLAM_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bloesch_Learning_Meshes_for_Dense_Visual_SLAM_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009776/,"['Simultaneous localization and mapping', 'Geometry', 'Optimization', 'Visualization', 'Three-dimensional displays', 'Neural networks', 'Computational modeling']","['Simultaneous Localization And Mapping', 'Integration Of Information', 'Compact Representation', 'Residue Type', 'Triangular Mesh', 'Continuous Integration', 'Free Variables', 'Factor Graph', 'Vertex Coordinates', 'Degrees Of Freedom', 'Deep Neural Network', 'Single Image', 'Multilayer Perceptron', 'Sparse Matrix', 'Latent Space', 'Depth Map', 'Iteration Step', 'Monocular', 'Optimization Step', 'Density Of The System', 'Regular Mesh', 'Dense Reconstruction', 'Choice Of Representation', 'Scene Geometry', 'Dense Correspondence', 'Mesh Vertices', 'Inverse Mapping', 'Training Setup', 'Ground Truth Pose', 'Final Loss']",,13,"Estimating motion and surrounding geometry of a moving camera remains a challenging inference problem. From an information theoretic point of view, estimates should get better as more information is included, such as is done in dense SLAM, but this is strongly dependent on the validity of the underlying models. In the present paper, we use triangular meshes as both compact and dense geometry representation. To allow for simple and fast usage, we propose a view-based formulation for which we predict the in-plane vertex coordinates directly from images and then employ the remaining vertex depth components as free variables. Flexible and continuous integration of information is achieved through the use of a residual based inference technique. This so-called factor graph encodes all information as mapping from free variables to residuals, the squared sum of which is minimised during inference. We propose the use of different types of learnable residuals, which are trained end-to-end to increase their suitability as information bearing models and to enable accurate and reliable estimation. Detailed evaluation of all components is provided on both synthetic and real data which confirms the practicability of the presented approach."
Learning Motion in Feature Space: Locally-Consistent Deformable Convolution Networks for Fine-Grained Action Detection,"Khoi-Nguyen C. Mac, Dhiraj Joshi, Raymond A. Yeh, Jinjun Xiong, Rogerio S. Feris, Minh N. Do",University of Illinois at Urbana-Champaign; IBM Research AI,50.0,usa,50.0,USA,"Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mac_Learning_Motion_in_Feature_Space_Locally-Consistent_Deformable_Convolution_Networks_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mac_Learning_Motion_in_Feature_Space_Locally-Consistent_Deformable_Convolution_Networks_for_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010064/,"['Convolution', 'Feature extraction', 'Optical imaging', 'Adaptation models', 'Computational modeling', 'Standards', 'Deformable models']","['Feature Space', 'Deformable Convolution', 'Action Detection', 'Learned Motion', 'Deformable Convolutional Network', 'Fine-grained Action', 'Recent Paper', 'Receptive Field', 'Optical Flow', 'Spatiotemporal Characteristics', 'Temporal Model', 'Temporal Component', 'Motion Information', 'Local Constraints', 'Pixel Spacing', 'Spatial Dimensions', 'Red Arrows', 'Temporal Information', 'Blue Dots', 'Standard Convolution', 'Multiple Frames', 'Temporal Convolutional Network', 'Motion Field', 'Motion Vector', 'Deformable Layer', '3D Convolution', 'FC Layer', 'Standard Datasets']",,24,"Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively."
Learning Object-Specific Distance From a Monocular Image,"Jing Zhu, Yi Fang","NYU Multimedia and Visual Computing Lab, USA; New York University, USA; New York University Abu Dhabi, UAE",100.0,"USA, canada, usa",0.0,,"Environment perception, including object detection and distance estimation, is one of the most crucial tasks for autonomous driving. Many attentions have been paid on the object detection task, but distance estimation only arouse few interests in the computer vision community. Observing that the traditional inverse perspective mapping algorithm performs poorly for objects far away from the camera or on the curved road, in this paper, we address the challenging distance estimation problem by developing the first end-to-end learning-based model to directly predict distances for given objects in the images. Besides the introduction of a learning-based base model, we further design an enhanced model with a keypoint regressor, where a projection loss is defined to enforce a better distance estimation, especially for objects close to the camera. To facilitate the research on this task, we construct the extented KITTI and nuScenes (mini) object detection datasets with a distance for each object. Our experiments demonstrate that our proposed methods outperform alternative approaches (e.g., the traditional IPM, SVR) on object-specific distance estimation, particularly for the challenging cases that objects are on a curved road. Moreover, the performance margin implies the effectiveness of our enhanced method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Learning_Object-Specific_Distance_From_a_Monocular_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Learning_Object-Specific_Distance_From_a_Monocular_Image_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008840/,"['Estimation', 'Feature extraction', 'Cameras', 'Predictive models', 'Task analysis', 'Roads', 'Training']","['Computer Vision', 'Object Detection', 'Image Object', 'Mapping Algorithm', 'Distance Estimation', 'Object Distance', 'Challenging Cases', 'Inversion Algorithm', 'Inverse Mapping', 'Traditional Mapping', 'Computer Vision Community', 'KITTI Dataset', 'Object Detection Dataset', 'Deep Learning', 'Precise Estimates', 'Visual Perception', 'Pedestrian', 'Multi-label', 'Point Cloud', 'Bounding Box', 'RGB Images', 'Object Bounding Boxes', 'Birdâ€™s Eye', 'Monocular Depth Estimation', 'Precise Distance', 'Object Classification', 'Validation Subset', 'Categories In Dataset', 'Deep Learning Techniques', 'Depth Prediction']",,49,"Environment perception, including object detection and distance estimation, is one of the most crucial tasks for autonomous driving. Many attentions have been paid on the object detection task, but distance estimation only arouse few interests in the computer vision community. Observing that the traditional inverse perspective mapping algorithm performs poorly for objects far away from the camera or on the curved road, in this paper, we address the challenging distance estimation problem by developing the first end-to-end learning-based model to directly predict distances for given objects in the images. Besides the introduction of a learning-based base model, we further design an enhanced model with a keypoint regressor, where a projection loss is defined to enforce a better distance estimation, especially for objects close to the camera. To facilitate the research on this task, we construct the extented KITTI and nuScenes (mini) object detection datasets with a distance for each object. Our experiments demonstrate that our proposed methods outperform alternative approaches (e.g., the traditional IPM, SVR) on object-specific distance estimation, particularly for the challenging cases that objects are on a curved road. Moreover, the performance margin implies the effectiveness of our enhanced method."
Learning Perspective Undistortion of Portraits,"Yajie Zhao, Zeng Huang, Tianye Li, Weikai Chen, Chloe LeGendre, Xinglei Ren, Ari Shapiro, Hao Li",USC Institute for Creative Technologies; University of Southern California; Pinscreen; USC Institute for Creative Technologies; University of Southern California; USC Institute for Creative Technologies,83.33333333333334,usa,16.666666666666657,USA,"Near-range portrait photographs often contain perspective distortion artifacts that bias human perception and challenge both facial recognition and reconstruction techniques. We present the first deep learning based approach to remove such artifacts from unconstrained portraits. In contrast to the previous state-of-the-art approach [??], our method handles even portraits with extreme perspective distortion, as we avoid the inaccurate and error-prone step of first fitting a 3D face model. Instead, we predict a distortion correction flow map that encodes a per-pixel displacement that removes distortion artifacts when applied to the input image. Our method also automatically infers missing facial features, i.e. occluded ears caused by strong perspective distortion, with coherent details. We demonstrate that our approach significantly outperforms the previous state-of-the-art both qualitatively and quantitatively, particularly for portraits with extreme perspective distortion or facial expressions. We further show that our technique benefits a number of fundamental tasks, significantly improving the accuracy of both face recognition and 3D reconstruction and enables a novel camera calibration technique from a single portrait. Moreover, we also build the first perspective portrait database with a large diversity in identities, expression and poses.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Learning_Perspective_Undistortion_of_Portraits_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Learning_Perspective_Undistortion_of_Portraits_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010290/,"['Cameras', 'Distortion', 'Face', 'Three-dimensional displays', 'Image reconstruction', 'Solid modeling']","['Input Image', 'Facial Expressions', '3D Reconstruction', 'Face Recognition', 'Facial Features', 'Distortion Correction', 'Oral And Maxillofacial Surgery', 'Camera Calibration', 'Flow Map', 'Face Model', 'Missing Features', '3D Face', 'Perspective Distortion', 'Computer Vision', 'Single Image', 'Average Error', 'Focal Length', 'Osteopontin', 'Complete Network', '3D Geometry', 'Head Pose', 'Camera Distance', 'Landmark Detection', 'L1 Loss', 'Cascade Network', 'Distance Prediction', 'Forward Flow', 'Computer Graphics', 'Camera Head']",,16,"Near-range portrait photographs often contain perspective distortion artifacts that bias human perception and challenge both facial recognition and reconstruction techniques. We present the first deep learning based approach to remove such artifacts from unconstrained portraits. In contrast to the previous state-of-the-art approach [23], our method handles even portraits with extreme perspective distortion, as we avoid the inaccurate and error-prone step of first fitting a 3D face model. Instead, we predict a distortion correction flow map that encodes a per-pixel displacement that removes distortion artifacts when applied to the input image. Our method also automatically infers missing facial features, i.e. occluded ears caused by strong perspective distortion, with coherent details. We demonstrate that our approach significantly outperforms the previous state-of-the-art [23] both qualitatively and quantitatively, particularly for portraits with extreme perspective distortion or facial expressions. We further show that our technique benefits a number of fundamental tasks, significantly improving the accuracy of both face recognition and 3D reconstruction and enables a novel camera calibration technique from a single portrait. Moreover, we also build the first perspective portrait database with a large diversity in identities, expression and poses."
Learning Propagation for Arbitrarily-Structured Data,"Sifei Liu, Xueting Li, Varun Jampani, Shalini De Mello, Jan Kautz","University of California, Merced; NVIDIA",50.0,usa,50.0,USA,"Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels, and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_Propagation_for_Arbitrarily-Structured_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_Propagation_for_Arbitrarily-Structured_Data_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009550/,"['Three-dimensional displays', 'Computational modeling', 'Image segmentation', 'Semantics', 'Data models', 'Task analysis', 'Convolution']","['Deep Learning', 'Computer Vision', 'Types Of Networks', 'Point Cloud', 'Semantic Segmentation', 'Pairwise Relationships', 'Generation Module', 'Joint Training', 'Network Propagation', 'Arbitrary Structure', 'Point Cloud Segmentation', 'Linear Diffusion', 'Graphs Of Groups', 'Training Set', 'Convolution', 'Deep Neural Network', 'Feature Maps', 'Image Pixels', 'Image Segmentation', 'Regular Grid', 'Directed Acyclic Graph', 'Multi-view Images', 'Explicit Model', 'Semantic Labels', 'Euclidean Space', 'Pairwise Model', 'Tangent Space', 'Low-level Features', 'Segmentation Task', 'Geometry Information']",,,"Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels, and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data."
Learning Relationships for Multi-View 3D Object Recognition,"Ze Yang, Liwei Wang","Center for Data Science, Peking University; Key Laboratory of Machine Perception, MOE, School of EECS, Peking University",100.0,china,0.0,,"Recognizing 3D object has attracted plenty of attention recently, and view-based methods have achieved best results until now. However, previous view-based methods ignore the region-to-region and view-to-view relationships between different view images, which are crucial for multi-view 3D object representation. To tackle this problem, we propose a Relation Network to effectively connect corresponding regions from different viewpoints, and therefore reinforce the information of individual view image. In addition, the Relation Network exploits the inter-relationships over a group of views, and integrates those views to obtain a discriminative 3D object representation. Systematic experiments conducted on ModelNet dataset demonstrate the effectiveness of our proposed methods for both 3D object recognition and retrieval tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Learning_Relationships_for_Multi-View_3D_Object_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_Relationships_for_Multi-View_3D_Object_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008297/,"['Three-dimensional displays', 'Feature extraction', 'Object recognition', 'Solid modeling', 'Two dimensional displays', 'Task analysis', 'Computer architecture']","['3D Object Recognition', 'Multi-view 3D Object', 'Network Of Relationships', '3D Tasks', 'Convolutional Neural Network', 'Feature Maps', 'Regional Characteristics', 'Confusion Matrix', 'Point Cloud', 'Singular Value Decomposition', 'Softmax Function', 'Fully-connected Layer', 'Matching Model', 'Final Objective', 'Importance Scores', 'Object Parts', 'Field Of Computer Vision', 'Matching Score', 'Self-attention Mechanism', 'Voxel-based Approach', 'Matching Regions', 'Voxel Grid', 'Average Classification Accuracy', 'Average Accuracy', 'Deep Network', 'Convolutional Layers', '3D Representation', 'Handcrafted Features', 'Training Examples']",,111,"Recognizing 3D object has attracted plenty of attention recently, and view-based methods have achieved best results until now. However, previous view-based methods ignore the region-to-region and view-to-view relationships between different view images, which are crucial for multi-view 3D object representation. To tackle this problem, we propose a Relation Network to effectively connect corresponding regions from different viewpoints, and therefore reinforce the information of individual view image. In addition, the Relation Network exploits the inter-relationships over a group of views, and integrates those views to obtain a discriminative 3D object representation. Systematic experiments conducted on ModelNet dataset demonstrate the effectiveness of our proposed methods for both 3D object recognition and retrieval tasks."
Learning Rich Features at High-Speed for Single-Shot Object Detection,"Tiancai Wang, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao","Inception Institute of Artiﬁcial Intelligence (IIAI), UAE; School of Electrical and Information Engineering, Tianjin University",100.0,"china, uae",0.0,,"Single-stage object detection methods have received significant attention recently due to their characteristic realtime capabilities and high detection accuracies. Generally, most existing single-stage detectors follow two common practices: they employ a network backbone that is pretrained on ImageNet for the classification task and use a top-down feature pyramid representation for handling scale variations. Contrary to common pre-training strategy, recent works have demonstrated the benefits of training from scratch to reduce the task gap between classification and localization, especially at high overlap thresholds. However, detection models trained from scratch require significantly longer training time compared to their typical finetuning based counterparts. We introduce a single-stage detection framework that combines the advantages of both fine-tuning pretrained models and training from scratch. Our framework constitutes a standard network that uses a pre-trained backbone and a parallel light-weight auxiliary network trained from scratch. Further, we argue that the commonly used top-down pyramid representation only focuses on passing high-level semantics from the top layers to bottom layers. We introduce a bi-directional network that efficiently circulates both low-/mid-level and high-level semantic information in the detection framework. Experiments are performed on MS COCO and UAVDT datasets. Compared to the baseline, our detector achieives an absolute gain of 7.4% and 4.2% in average precision (AP) on MS COCO and UAVDT datasets, respectively using VGG backbone. For a 300x300 input on the MS COCO test set, our detector with ResNet backbone surpasses existing single-stage detection methods for single-scale inference achieving 34.3 AP, while operating at an inference time of 19 milliseconds on a single Titan X GPU. Code is avail- able at https://github.com/vaesl/LRF-Net.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf,,https://github.com/vaesl/LRF-Net,,main,Poster,https://ieeexplore.ieee.org/document/9008401/,"['Feature extraction', 'Standards', 'Detectors', 'Bidirectional control', 'Semantics', 'Object detection', 'Training']","['Object Detection', 'ImageNet', 'Average Precision', 'Backbone Network', 'Detection Framework', 'Feature Pyramid', 'High Detection Accuracy', 'Bidirectional Network', 'COCO Dataset', 'High-level Semantic Information', 'MS COCO Dataset', 'High-level Semantics', 'Convolutional Neural Network', 'Significantly Improved', 'Detection Performance', 'Network Layer', 'Small Objects', 'Top-down And Bottom-up', 'Two-stage Method', 'Pre-trained Network', 'Forward Feature', 'Backbone Feature', 'Feature Pyramid Network', 'Small Object Detection', 'Intermediate Features', 'Feature Extraction Strategy', 'ResNet-101 Backbone']",,81,"Single-stage object detection methods have received significant attention recently due to their characteristic realtime capabilities and high detection accuracies. Generally, most existing single-stage detectors follow two common practices: they employ a network backbone that is pretrained on ImageNet for the classification task and use a top-down feature pyramid representation for handling scale variations. Contrary to common pre-training strategy, recent works have demonstrated the benefits of training from scratch to reduce the task gap between classification and localization, especially at high overlap thresholds. However, detection models trained from scratch require significantly longer training time compared to their typical finetuning based counterparts. We introduce a single-stage detection framework that combines the advantages of both fine-tuning pretrained models and training from scratch. Our framework constitutes a standard network that uses a pre-trained backbone and a parallel light-weight auxiliary network trained from scratch. Further, we argue that the commonly used top-down pyramid representation only focuses on passing high-level semantics from the top layers to bottom layers. We introduce a bi-directional network that efficiently circulates both low-/mid-level and high-level semantic information in the detection framework. Experiments are performed on MS COCO and UAVDT datasets. Compared to the baseline, our detector achieives an absolute gain of 7.4% and 4.2% in average precision (AP) on MS COCO and UAVDT datasets, respectively using VGG backbone. For a 300×300 input on the MS COCO test set, our detector with ResNet backbone surpasses existing single-stage detection methods for single-scale inference achieving 34.3 AP, while operating at an inference time of 19 milliseconds on a single Titan X GPU. Code is avail- able at https://github.com/vaesl/LRF-Net."
Learning Robust Facial Landmark Detection via Hierarchical Structured Ensemble,"Xu Zou, Sheng Zhong, Luxin Yan, Xiangyun Zhao, Jiahuan Zhou, Ying Wu","National Key Laboratory of Science & Technology on Multi-Spectral Information Processing, HUST, Wuhan, China; Department of Electrical and Computer Engineering, Northwestern University, IL, USA; College of Life Science and Technology, Huazhong University of Science & Technology (HUST), Wuhan, China; School of Aritiﬁcial Intelligence and Automation, HUST, Wuhan, China",100.0,"China, usa",0.0,,"Heatmap regression-based models have significantly advanced the progress of facial landmark detection. However, the lack of structural constraints always generates inaccurate heatmaps resulting in poor landmark detection performance. While hierarchical structure modeling methods have been proposed to tackle this issue, they all heavily rely on manually designed tree structures. The designed hierarchical structure is likely to be completely corrupted due to the missing or inaccurate prediction of landmarks. To the best of our knowledge, in the context of deep learning, no work before has investigated how to automatically model proper structures for facial landmarks, by discovering their inherent relations. In this paper, we propose a novel Hierarchical Structured Landmark Ensemble (HSLE) model for learning robust facial landmark detection, by using it as the structural constraints. Different from existing approaches of manually designing structures, our proposed HSLE model is constructed automatically via discovering the most robust patterns so HSLE has the ability to robustly depict both local and holistic landmark structures simultaneously. Our proposed HSLE can be readily plugged into any existing facial landmark detection baselines for further performance improvement. Extensive experimental results demonstrate our approach significantly outperforms the baseline by a large margin to achieve a state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Learning_Robust_Facial_Landmark_Detection_via_Hierarchical_Structured_Ensemble_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Learning_Robust_Facial_Landmark_Detection_via_Hierarchical_Structured_Ensemble_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010741/,"['Robustness', 'Heating systems', 'Detectors', 'Face', 'Shape', 'Mathematical model']","['Hierarchical Structure', 'Robust Detection', 'Landmark Detection', 'Facial Landmark Detection', 'Local Structure', 'Hierarchical Model', 'Tree Structure', 'Structural Constraints', 'Robust Patterns', 'Inherent Relationship', 'Convolutional Neural Network', 'Bounding Box', 'Directed Graph', 'Entire Model', 'Pose Estimation', 'Collection Of Sets', 'Robust Structure', 'Head Pose', 'Number Of Landmarks', 'Interocular Distance']",,44,"Heatmap regression-based models have significantly advanced the progress of facial landmark detection. However, the lack of structural constraints always generates inaccurate heatmaps resulting in poor landmark detection performance. While hierarchical structure modeling methods have been proposed to tackle this issue, they all heavily rely on manually designed tree structures. The designed hierarchical structure is likely to be completely corrupted due to the missing or inaccurate prediction of landmarks. To the best of our knowledge, in the context of deep learning, no work before has investigated how to automatically model proper structures for facial landmarks, by discovering their inherent relations. In this paper, we propose a novel Hierarchical Structured Landmark Ensemble (HSLE) model for learning robust facial landmark detection, by using it as the structural constraints. Different from existing approaches of manually designing structures, our proposed HSLE model is constructed automatically via discovering the most robust patterns so HSLE has the ability to robustly depict both local and holistic landmark structures simultaneously. Our proposed HSLE can be readily plugged into any existing facial landmark detection baselines for further performance improvement. Extensive experimental results demonstrate our approach significantly outperforms the baseline by a large margin to achieve a state-of-the-art performance."
Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition,"Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, Liang Lin","Sun Yat-sen University; Sun Yat-sen University, DarkMatter AI Research",100.0,China,0.0,,"Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Semantic-Specific_Graph_Representation_for_Multi-Label_Image_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Learning_Semantic-Specific_Graph_Representation_for_Multi-Label_Image_Recognition_ICCV_2019_paper.pdf,,https://github.com/HCPLab-SYSU/SSGRL,,main,Poster,https://ieeexplore.ieee.org/document/9010734/,"['Semantics', 'Feature extraction', 'Proposals', 'Neural networks', 'Logic gates', 'Image recognition', 'Task analysis']","['Multi-label Image', 'Multi-label Image Recognition', 'Learning Framework', 'Lack Of Guidance', 'Multiple Labels', 'PASCAL VOC', 'Crucial Modulator', 'Semantic Regions', 'Training Set', 'Deep Neural Network', 'Image Features', 'Sigmoid Function', 'Input Image', 'Feature Maps', 'Number Of Images', 'Attention Mechanism', 'Baseline Methods', 'Hyperbolic Tangent', 'Hidden State', 'Feature Aggregation', 'COCO Dataset', 'Graph Neural Networks', 'Hair Dryer', 'Semantic Vectors', 'Average Pooling Layer', 'Attention Regions', 'Presence Of Categories', 'Category Probability', 'Average Pooling', 'Feature Categories']",,194,"Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL."
Learning Shape Templates With Structured Implicit Functions,"Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T. Freeman, Thomas Funkhouser",Princeton University; Google Research,50.0,usa,50.0,USA,"Template 3D shapes are useful for many tasks in graphics and vision, including fitting observation data, analyzing shape collections, and transferring shape attributes. Because of the variety of geometry and topology of real-world shapes, previous methods generally use a library of hand-made templates. In this paper, we investigate learning a general shape template from data. To allow for widely varying geometry and topology, we choose an implicit surface representation based on composition of local shape elements. While long known to computer graphics, this representation has not yet been explored in the context of machine learning for vision. We show that structured implicit functions are suitable for learning and allow a network to smoothly and simultaneously fit multiple classes of shapes. The learned shape template supports applications such as shape exploration, correspondence, abstraction, interpolation, and semantic segmentation from an RGB image.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Genova_Learning_Shape_Templates_With_Structured_Implicit_Functions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Genova_Learning_Shape_Templates_With_Structured_Implicit_Functions_ICCV_2019_paper.pdf,http://templates.cs.princeton.edu,,,main,Poster,https://ieeexplore.ieee.org/document/9008389/,"['Shape', 'Three-dimensional displays', 'Topology', 'Geometry', 'Semantics', 'Neural networks', 'Computer architecture']","['Implicit Function', 'Template Shape', 'Semantic Segmentation', 'RGB Images', '3D Shape', 'Shape Classification', 'Geometry And Topology', 'Loss Function', 'Neural Network', 'Free Space', 'Similar Shape', 'Local Function', 'Bounding Box', 'Depth Images', 'Sparse Representation', 'Classification Loss', 'Influence Of Location', 'Surface Reconstruction', 'Thin Structures', 'Part Segmentation', 'Input Shape', 'Shape Representation', 'Latent Vector']",,195,"Template 3D shapes are useful for many tasks in graphics and vision, including fitting observation data, analyzing shape collections, and transferring shape attributes. Because of the variety of geometry and topology of real-world shapes, previous methods generally use a library of hand-made templates. In this paper, we investigate learning a general shape template from data. To allow for widely varying geometry and topology, we choose an implicit surface representation based on composition of local shape elements. While long known to computer graphics, this representation has not yet been explored in the context of machine learning for vision. We show that structured implicit functions are suitable for learning and allow a network to smoothly and simultaneously fit multiple classes of shapes. The learned shape template supports applications such as shape exploration, correspondence, abstraction, interpolation, and semantic segmentation from an RGB image."
Learning Similarity Conditions Without Explicit Supervision,"Reuben Tan, Mariya I. Vasileva, Kate Saenko, Bryan A. Plummer",University of Illinois at Urbana-Champaign; Boston University,100.0,usa,0.0,,"Many real-world tasks require models to compare images along multiple similarity conditions (e.g. similarity in color, category or shape). Existing methods often reason about these complex similarity relationships by learning condition-aware embeddings. While such embeddings aid models in learning different notions of similarity, they also limit their capability to generalize to unseen categories since they require explicit labels at test time. To address this deficiency, we propose an approach that jointly learns representations for the different similarity conditions and their contributions as a latent variable without explicit supervision. Comprehensive experiments across three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k, demonstrate the effectiveness of our approach: our model outperforms the state-of-the-art methods, even those that are strongly supervised with pre-defined similarity conditions, on fill-in-the-blank, outfit compatibility prediction and triplet prediction tasks. Finally, we show that our model learns different visually-relevant semantic sub-spaces that allow it to generalize well to unseen categories.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tan_Learning_Similarity_Conditions_Without_Explicit_Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Learning_Similarity_Conditions_Without_Explicit_Supervision_ICCV_2019_paper.pdf,,https://github.com/rxtan2/Learning-Similarity-Conditions,,main,Poster,https://ieeexplore.ieee.org/document/9010759/,"['Visualization', 'Task analysis', 'Image color analysis', 'Computer vision', 'Computational modeling', 'Semantics', 'Measurement']","['Similar Conditions', 'Latent Variables', 'Representation Learning', 'Prediction Task', 'Similar Note', 'Training Set', 'Image Features', 'Computer Vision', 'Training Time', 'Visual Representation', 'Visual Features', 'Image Pairs', 'Latent Space', 'Textual Descriptions', 'Visual Properties', 'Intuitive Way', 'Textual Features', 'Item Category', 'Metric Learning', 'Fashion Items', 'Triplet Loss', 'Subspace Learning', 'Image Categories', 'Item Characteristics', 'Objective Function', 'Disentangled Representation']",,58,"Many real-world tasks require models to compare images along multiple similarity conditions (e.g. similarity in color, category or shape). Existing methods often reason about these complex similarity relationships by learning condition-aware embeddings. While such embeddings aid models in learning different notions of similarity, they also limit their capability to generalize to unseen categories since they require explicit labels at test time. To address this deficiency, we propose an approach that jointly learns representations for the different similarity conditions and their contributions as a latent variable without explicit supervision. Comprehensive experiments across three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k, demonstrate the effectiveness of our approach: our model outperforms the state-of-the-art methods, even those that are strongly supervised with pre-defined similarity conditions, on fill-in-the-blank, outfit compatibility prediction and triplet prediction tasks. Finally, we show that our model learns different visually-relevant semantic sub-spaces that allow it to generalize well to unseen categories."
Learning Single Camera Depth Estimation Using Dual-Pixels,"Rahul Garg, Neal Wadhwa, Sameer Ansari, Jonathan T. Barron",Google Research,0.0,,100.0,USA,"Deep learning techniques have enabled rapid progress in monocular depth estimation, but their quality is limited by the ill-posed nature of the problem and the scarcity of high quality datasets. We estimate depth from a single cam-era by leveraging the dual-pixel auto-focus hardware that is increasingly common on modern camera sensors. Classic stereo algorithms and prior learning-based depth estimation techniques underperform when applied on this dual-pixel data, the former due to too-strong assumptions about RGB image matching, and the latter due to not leveraging the understanding of optics of dual-pixel image formation. To allow learning based methods to work well on dual-pixel imagery, we identify an inherent ambiguity in the depth estimated from dual-pixel cues, and develop an approach to estimate depth up to this ambiguity. Using our approach, existing monocular depth estimation techniques can be effectively applied to dual-pixel data, and much smaller models can be constructed that still infer high quality depth. To demonstrate this, we capture a large dataset of in-the-wild 5-viewpoint RGB images paired with corresponding dual-pixel data, and show how view supervision with this data can be used to learn depth up to the unknown ambiguities. On our new task, our model is 30% more accurate than any prior work on learning-based monocular or stereoscopic depth estimation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Garg_Learning_Single_Camera_Depth_Estimation_Using_Dual-Pixels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Garg_Learning_Single_Camera_Depth_Estimation_Using_Dual-Pixels_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010304/,"['Cameras', 'Estimation', 'Sensors', 'Apertures', 'Lenses', 'Image sensors', 'Training']","['Depth Camera', 'Depth Estimation', 'Single Camera', 'Deep Learning', 'Imagery', 'RGB Images', 'Image Formation', 'Inherent Ambiguity', 'Monocular Depth Estimation', 'Conventional Imaging', 'Depth Map', 'Scale-invariant', 'Affine Transformation', 'Point Spread Function', 'Light Field', 'Left Image', 'Camera Pose', 'Stereo Images', 'Stereo Camera', 'Stereo Pairs', 'Scale Ambiguity', 'Ground Truth Depth', 'Camera Array', 'Scene Depth', 'Depth Prediction', 'Absolute Depth', 'Conventional Camera', 'Warped Image', 'View Of The Scene', 'Accurate Depth']",,81,"Deep learning techniques have enabled rapid progress in monocular depth estimation, but their quality is limited by the ill-posed nature of the problem and the scarcity of high quality datasets. We estimate depth from a single cam-era by leveraging the dual-pixel auto-focus hardware that is increasingly common on modern camera sensors. Classic stereo algorithms and prior learning-based depth estimation techniques underperform when applied on this dual-pixel data, the former due to too-strong assumptions about RGB image matching, and the latter due to not leveraging the understanding of optics of dual-pixel image formation. To allow learning based methods to work well on dual-pixel imagery, we identify an inherent ambiguity in the depth estimated from dual-pixel cues, and develop an approach to estimate depth up to this ambiguity. Using our approach, existing monocular depth estimation techniques can be effectively applied to dual-pixel data, and much smaller models can be constructed that still infer high quality depth. To demonstrate this, we capture a large dataset of in-the-wild 5-viewpoint RGB images paired with corresponding dual-pixel data, and show how view supervision with this data can be used to learn depth up to the unknown ambiguities. On our new task, our model is 30% more accurate than any prior work on learning-based monocular or stereoscopic depth estimation."
Learning Spatial Awareness to Improve Crowd Counting,"Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Alexander G. Hauptmann",Southwest Jiaotong University; Microsoft Research; Carnegie Mellon University,66.66666666666666,"china, usa",33.33333333333334,USA,"The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., L_2 loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by [??] to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Learning_Spatial_Awareness_to_Improve_Crowd_Counting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Learning_Spatial_Awareness_to_Improve_Crowd_Counting_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010678/,"['Head', 'Feature extraction', 'Estimation', 'Optimization', 'Task analysis', 'Training', 'Benchmark testing']","['Spatial Awareness', 'Crowd Counting', 'High Diversity', 'Absolute Difference', 'Significantly Improved', 'Gradient Descent', 'Pedestrian', 'Density Map', 'Head Size', 'Spatial Network', 'Zero-mean Noise', 'Images Of People', 'L2 Loss', 'Training Set', 'Convolutional Layers', 'Input Image', 'Learning Curve', 'Deeper Layers', 'Mean Absolute Error', 'Stochastic Gradient Descent', 'Patch Pairs', 'CNN-based Methods', 'International Exhibition', 'Peak Signal-to-noise Ratio', 'Scale-invariant', 'Crowd Density', 'Image Patches', 'Weighting Scheme', 'Low-density Areas', 'Element-wise Product']",,89,"The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., $L_2$ loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by~\cite{nips-10} to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets."
Learning Temporal Action Proposals With Fewer Labels,"Jingwei Ji, Kaidi Cao, Juan Carlos Niebles",Stanford University,100.0,usa,0.0,,"Temporal action proposals are a common module in action detection pipelines today. Most current methods for training action proposal modules rely on fully supervised approaches that require large amounts of annotated temporal action intervals in long video sequences. The large cost and effort in annotation that this entails motivate us to study the problem of training proposal modules with less supervision. In this work, we propose a semi-supervised learning algorithm specifically designed for training temporal action proposal networks. When only a small number of labels are available, our semi-supervised method generates significantly better proposals than the fully-supervised counterpart and other strong semi-supervised baselines. We validate our method on two challenging action detection video datasets, ActivityNet v1.3 and THUMOS14. We show that our semi-supervised approach consistently matches or outperforms the fully supervised state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ji_Learning_Temporal_Action_Proposals_With_Fewer_Labels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ji_Learning_Temporal_Action_Proposals_With_Fewer_Labels_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008841/,"['Proposals', 'Training', 'Task analysis', 'Perturbation methods', 'Data models', 'Semisupervised learning', 'Supervised learning']","['Action Proposals', 'Temporal Action Proposal', 'Video Sequences', 'Semi-supervised Learning', 'Strong Baseline', 'Action Detection', 'Training Set', 'Validation Set', 'Teacher Model', 'Mean Education', 'Labeling Efficiency', 'Student Model', 'Temporal Localization', 'Consistency Loss', 'Time Warping', 'Average Recall', 'Degree Of Distortion', 'Temporal Boundaries', 'Precise Boundaries', 'Smooth Manifold', 'Unsupervised Tasks', 'Part Of The Video', 'Video Encoding', 'Proposal Generation', 'Consistency Regularization', 'Amount Of Labels', 'Text Classification', 'Unlabeled Data', 'Mean Average Precision', 'Dropout Regularization']",,26,"Temporal action proposals are a common module in action detection pipelines today. Most current methods for training action proposal modules rely on fully supervised approaches that require large amounts of annotated temporal action intervals in long video sequences. The large cost and effort in annotation that this entails motivate us to study the problem of training proposal modules with less supervision. In this work, we propose a semi-supervised learning algorithm specifically designed for training temporal action proposal networks. When only a small number of labels are available, our semi-supervised method generates significantly better proposals than the fully-supervised counterpart and other strong semi-supervised baselines. We validate our method on two challenging action detection video datasets, ActivityNet v1.3 and THUMOS14. We show that our semi-supervised approach consistently matches or outperforms the fully supervised state-of-the-art approaches."
Learning Trajectory Dependencies for Human Motion Prediction,"Wei Mao, Miaomiao Liu, Mathieu Salzmann, Hongdong Li","Australian National University; Australian National University, Australia Centre for Robotic Vision; CVLab, EPFL",100.0,"Australia, switzerland",0.0,,"Human motion prediction, i.e., forecasting future body poses given observed pose sequence, has typically been tackled with recurrent neural networks (RNNs). However, as evidenced by prior work, the resulted RNN models suffer from prediction errors accumulation, leading to undesired discontinuities in motion prediction. In this paper, we propose a simple feed-forward deep network for motion prediction, which takes into account both temporal smoothness and spatial dependencies among human body joints. In this context, we then propose to encode temporal information by working in trajectory space, instead of the traditionally-used pose space. This alleviates us from manually defining the range of temporal dependencies (or temporal convolutional filter size, as done in previous work). Moreover, spatial dependency of human pose is encoded by treating a human pose as a generic graph (rather than a human skeletal kinematic tree) formed by links between every pair of body joints. Instead of using a pre-defined graph structure, we design a new graph convolutional network to learn graph connectivity automatically. This allows the network to capture long range dependencies beyond that of human kinematic tree. We evaluate our approach on several standard benchmark datasets for motion prediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our experiments clearly demonstrate that the proposed approach achieves state of the art performance, and is applicable to both angle-based and position-based pose representations. The code is available at https://github.com/wei-mao-2019/LearnTrajDep",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mao_Learning_Trajectory_Dependencies_for_Human_Motion_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Learning_Trajectory_Dependencies_for_Human_Motion_Prediction_ICCV_2019_paper.pdf,,https://github.com/wei-mao-2019/LearnTrajDep,,main,Oral,https://ieeexplore.ieee.org/document/9009559/,"['Discrete cosine transforms', 'Trajectory', 'Kinematics', 'Three-dimensional displays', 'Convolutional codes', 'Standards', 'Hidden Markov models']","['Human Motion', 'Motion Prediction', 'Human Motion Prediction', 'Recurrent Neural Network', 'Temporal Information', 'Feed-forward Network', 'Filter Size', 'Motion Capture', 'Spatial Dependence', 'Graph Structure', 'Temporal Dependencies', 'Graph Convolutional Network', 'Standard Benchmark', 'Graph Convolution', 'Long-range Dependencies', 'Trajectories In Space', 'Human Pose', 'Results Of Experiments', 'Spatial Structure', '3D Space', 'Discrete Cosine Transform', '3D Coordinates', 'Joint Angles', '3D Pose', 'Spatial Structure Of The Data', 'Angle Space', '3D Error', 'Long-term Prediction', 'Set Of Angles', 'L Matrix']",,268,"Human motion prediction, i.e., forecasting future body poses given observed pose sequence, has typically been tackled with recurrent neural networks (RNNs). However, as evidenced by prior work, the resulted RNN models suffer from prediction errors accumulation, leading to undesired discontinuities in motion prediction. In this paper, we propose a simple feed-forward deep network for motion prediction, which takes into account both temporal smoothness and spatial dependencies among human body joints. In this context, we then propose to encode temporal information by working in trajectory space, instead of the traditionally-used pose space. This alleviates us from manually defining the range of temporal dependencies (or temporal convolutional filter size, as done in previous work). Moreover, spatial dependency of human pose is encoded by treating a human pose as a generic graph (rather than a human skeletal kinematic tree) formed by links between every pair of body joints. Instead of using a pre-defined graph structure, we design a new graph convolutional network to learn graph connectivity automatically. This allows the network to capture long range dependencies beyond that of human kinematic tree. We evaluate our approach on several standard benchmark datasets for motion prediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our experiments clearly demonstrate that the proposed approach achieves state of the art performance, and is applicable to both angle-based and position-based pose representations. The code is available at https://github.com/wei-mao-2019/LearnTrajDep."
Learning Two-View Correspondences and Geometry Using Order-Aware Network,"Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, Hongen Liao",Tsinghua University; Hong Kong University of Science and Technology; Intel Labs China; Everest Innovation Technology (Altizure),50.0,"China, Hong Kong",50.0,USA,"Establishing correspondences between two images requires both local and global spatial context. Given putative correspondences of feature points in two views, in this paper, we propose Order-Aware Network, which infers the probabilities of correspondences being inliers and regresses the relative pose encoded by the essential matrix. Specifically, this proposed network is built hierarchically and comprises three novel operations. First, to capture the local context of sparse correspondences, the network clusters unordered input correspondences by learning a soft assignment matrix. These clusters are in a canonical order and invariant to input permutations. Next, the clusters are spatially correlated to form the global context of correspondences. After that, the context-encoded clusters are recovered back to the original size through a proposed upsampling operator. We intensively experiment on both outdoor and indoor datasets. The accuracy of the two-view geometry and correspondences are significantly improved over the state-of-the-arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_Two-View_Correspondences_and_Geometry_Using_Order-Aware_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Two-View_Correspondences_and_Geometry_Using_Order-Aware_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010915/,"['Geometry', 'Feature extraction', 'Machine learning', 'Neural networks', 'Filtering', 'Task analysis', 'Computer architecture']","['Geometry Learning', 'Order-aware Network', 'Local Context', 'Global Context', 'Soft Matrix', 'Relative Pose', 'Canonical Order', 'Neural Network', 'Deep Learning', 'Convolutional Neural Network', 'Feature Maps', 'Spatial Dimensions', 'Multilayer Perceptron', 'Channel Dimension', 'Graph Neural Networks', 'Node Level', 'Structure From Motion', 'Simultaneous Localization And Mapping', 'Camera Pose', 'Outdoor Scenes', 'Outlier Rejection', 'L2 Loss', 'Spatial Layer', 'Permutation Matrix', 'Feature Pooling']",,237,"Establishing correspondences between two images requires both local and global spatial context. Given putative correspondences of feature points in two views, in this paper, we propose Order-Aware Network, which infers the probabilities of correspondences being inliers and regresses the relative pose encoded by the essential matrix. Specifically, this proposed network is built hierarchically and comprises three novel operations. First, to capture the local context of sparse correspondences, the network clusters unordered input correspondences by learning a soft assignment matrix. These clusters are in a canonical order and invariant to input permutations. Next, the clusters are spatially correlated to form the global context of correspondences. After that, the context-encoded clusters are recovered back to the original size through a proposed upsampling operator. We intensively experiment on both outdoor and indoor datasets. The accuracy of the two-view geometry and correspondences are significantly improved over the state-of-the-arts."
Learning With Average Precision: Training Image Retrieval With a Listwise Loss,"JÃ©rÃ´me Revaud, Jon AlmazÃ¡n, Rafael S. Rezende, CÃ©sar Roberto de Souza",NAVER LABS Europe,0.0,,100.0,South Korea,"Image retrieval can be formulated as a ranking problem where the goal is to order database images by decreasing similarity to the query. Recent deep models for image retrieval have outperformed traditional methods by leveraging ranking-tailored loss functions, but important theoretical and practical problems remain. First, rather than directly optimizing the global ranking, they minimize an upper-bound on the essential loss, which does not necessarily result in an optimal mean average precision (mAP). Second, these methods require significant engineering efforts to work well, e.g., special pre-training and hard-negative mining. In this paper we propose instead to directly optimize the global mAP by leveraging recent advances in listwise loss formulations. Using a histogram binning approximation, the AP can be differentiated and thus employed to end-to-end learning. Compared to existing losses, the proposed method considers thousands of images simultaneously at each iteration and eliminates the need for ad hoc tricks. It also establishes a new state of the art on many standard retrieval benchmarks. Models and evaluation scripts have been made available at: https://europe.naverlabs.com/Deep-Image-Retrieval/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.pdf,https://europe.naverlabs.com/Deep-Image-Retrieval/,,,main,Poster,https://ieeexplore.ieee.org/document/9010047/,"['Training', 'Image retrieval', 'Measurement', 'Task analysis', 'Histograms', 'Optimization']","['Average Precision', 'Image Retrieval', 'Listwise Loss', 'Loss Function', 'Deep Models', 'State Of The Art', 'Image Database', 'Mean Average Precision', 'Histogram Bins', 'Training Set', 'Deep Network', 'High-resolution Images', 'Batch Size', 'Training Time', 'Feed-forward Network', 'Latent Space', 'Local Loss', 'Network Depth', 'Ranking Loss', 'Triplet Loss', 'Backward Pass', 'Local Descriptors', 'Forward Pass', 'Backpropagation', 'Theoretical Guarantees', 'Image Descriptors']",,260,"Image retrieval can be formulated as a ranking problem where the goal is to order database images by decreasing similarity to the query. Recent deep models for image retrieval have outperformed traditional methods by leveraging ranking-tailored loss functions, but important theoretical and practical problems remain. First, rather than directly optimizing the global ranking, they minimize an upper-bound on the essential loss, which does not necessarily result in an optimal mean average precision (mAP). Second, these methods require significant engineering efforts to work well, e.g., special pre-training and hard-negative mining. In this paper we propose instead to directly optimize the global mAP by leveraging recent advances in listwise loss formulations. Using a histogram binning approximation, the AP can be differentiated and thus employed to end-to-end learning. Compared to existing losses, the proposed method considers thousands of images simultaneously at each iteration and eliminates the need for ad hoc tricks. It also establishes a new state of the art on many standard retrieval benchmarks. Models and evaluation scripts have been made available at: https://europe.naverlabs.com/Deep-Image-Retrieval/."
Learning With Unsure Data for Medical Image Diagnosis,"Botong Wu, Xinwei Sun, Lingjing Hu, Yizhou Wang","Microsoft Research, Asia; Yanjing Medical College, Capital Medical University; Computer Science Dept., Peking University; Deepwise AI Lab, Peng Cheng Laboratory",75.0,"China, china",25.0,USA,"In image-based disease prediction, it can be hard to give certain cases a deterministic ""disease/normal"" label due to lack of enough information, e.g., at its early stage. We call such cases ""unsure"" data. Labeling such data as unsure suggests follow-up examinations so as to avoid irreversible medical accident/loss in contrast to incautious prediction. This is a common practice in clinical diagnosis, however, mostly neglected by existing methods. Learning with unsure data also interweaves with two other practical issues: (i) data imbalance issue that may incur model-bias towards the majority class, and (ii) conservative/aggressive strategy consideration, i.e., the negative (normal) samples and positive (disease) samples should NOT be treated equally \-- the former should be detected with a high precision (conservativeness) and the latter should be detected with a high recall (aggression) to avoid missing opportunity for treatment. Mixed with these issues, learning with unsure data becomes particularly challenging. In this paper, we raise ""learning with unsure data"" problem and formulate it as an ordinal regression and propose a unified end-to-end learning framework, which also considers the aforementioned two issues: (i) incorporate cost-sensitive parameters to alleviate the data imbalance problem, and (ii) execute the conservative and aggressive strategies by introducing two parameters in the training procedure. The benefits of learning with unsure data and validity of our models are demonstrated on the prediction of Alzheimer's Disease and lung nodules.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Learning_With_Unsure_Data_for_Medical_Image_Diagnosis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Learning_With_Unsure_Data_for_Medical_Image_Diagnosis_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008553/,"['Diseases', 'Data models', 'Predictive models', 'Medical diagnostic imaging', 'Hippocampus', 'Training']","['Positive Samples', 'Major Classes', 'Ordinal Regression', 'Conservation Strategies', 'Disease Prediction', 'Opportunities For Intervention', 'Follow-up Examination', 'Imbalanced Data', 'Lung Nodules', 'Aggressive Strategies', 'Imbalance Issue', 'Validation Set', 'Convolutional Layers', 'Active Learning', 'Binary Classification', 'Mild Cognitive Impairment', 'Cross-entropy Loss', 'Multi-label', 'Precision And Recall', 'Stochastic Gradient Descent', 'Positive Class', 'Threshold Parameter', 'Benign Nodules', 'Negative Ones', 'Malignant Nodules', 'Medical Image Analysis', '3D Convolutional Layers', 'Noisy Labels', 'Explicit Labels', 'Cost Matrix']",,21,"In image-based disease prediction, it can be hard to give certain cases a deterministic ``disease/normal"" label due to lack of enough information, \eg, at its early stage. We call such cases ``unsure"" data. Labeling such data as unsure suggests follow-up examinations so as to avoid irreversible medical accident/loss in contrast to incautious prediction. This is a common practice in clinical diagnosis, however, mostly neglected by existing methods. Learning with unsure data also interweaves with two other practical issues: (i) data imbalance issue that may incur model-bias towards the majority class, and (ii) conservative/aggressive strategy consideration, \ie, the negative (normal) samples and positive (disease) samples should NOT be treated equally \-- the former should be detected with a high precision (conservativeness) and the latter should be detected with a high recall (aggression) to avoid missing opportunity for treatment. Mixed with these issues, learning with unsure data becomes particularly challenging. In this paper, we raise ``learning with unsure data"" problem and formulate it as an ordinal regression and propose a unified end-to-end learning framework, which also considers the aforementioned two issues: (i) incorporate cost-sensitive parameters to alleviate the data imbalance problem, and (ii) execute the conservative and aggressive strategies by introducing two parameters in the training procedure. The benefits of learning with unsure data and validity of our models are demonstrated on the prediction of Alzheimer's Disease and lung nodules."
Learning a Mixture of Granularity-Specific Experts for Fine-Grained Categorization,"Lianbo Zhang, Shaoli Huang, Wei Liu, Dacheng Tao","Advanced Analytics Institute, School of Computer Science, FEIT, University of Technology Sydney, Chippendale, NSW, Australia; UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Darlington, NSW 2008, Australia",100.0,australia,0.0,,"We aim to divide the problem space of fine-grained recognition into some specific regions. To achieve this, we develop a unified framework based on a mixture of experts. Due to limited data available for the fine-grained recognition problem, it is not feasible to learn diverse experts by using a data division strategy. To tackle the problem, we promote diversity among experts by combing an expert gradually-enhanced learning strategy and a Kullback-Leibler divergence based constraint. The strategy learns new experts on the dataset with the prior knowledge from former experts and adds them to the model sequentially, while the introduced constraint forces the experts to produce diverse prediction distribution. These drive the experts to learn the task from different aspects, making them specialized in different subspace problems. Experiments show that the resulting model improves the classification performance and achieves the state-of-the-art performance on several fine-grained benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_a_Mixture_of_Granularity-Specific_Experts_for_Fine-Grained_Categorization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_a_Mixture_of_Granularity-Specific_Experts_for_Fine-Grained_Categorization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010629/,"['Computer science', 'Australia', 'Computer vision', 'Artificial intelligence']","['Mixture Of Experts', 'Predictive Distribution', 'Problem Space', 'Neural Network', 'Results Of Experiments', 'Convolutional Neural Network', 'Descriptive Data', 'Deep Neural Network', 'Small Region', 'Feature Maps', 'Multiple Scales', 'Image Regions', 'Bounding Box', 'Baseline Methods', 'Line Of Work', 'Global Pooling', 'Convolutional Block', 'Target Distribution', 'Attention Map', 'Random Flipping', 'Global Max Pooling', 'Limited Training Data', 'Class Activation Maps', 'Learning Rate']",,116,"We aim to divide the problem space of fine-grained recognition into some specific regions. To achieve this, we develop a unified framework based on a mixture of experts. Due to limited data available for the fine-grained recognition problem, it is not feasible to learn diverse experts by using a data division strategy. To tackle the problem, we promote diversity among experts by combing an expert gradually-enhanced learning strategy and a Kullback-Leibler divergence based constraint. The strategy learns new experts on the dataset with the prior knowledge from former experts and adds them to the model sequentially, while the introduced constraint forces the experts to produce diverse prediction distribution. These drive the experts to learn the task from different aspects, making them specialized in different subspace problems. Experiments show that the resulting model improves the classification performance and achieves the state-of-the-art performance on several fine-grained benchmark datasets."
Learning an Effective Equivariant 3D Descriptor Without Supervision,"Riccardo Spezialetti, Samuele Salti, Luigi Di Stefano","Department of Computer Science and Engineering (DISI), University of Bologna, Italy",100.0,italy,0.0,,"Establishing correspondences between 3D shapes is a fundamental task in 3D Computer Vision, typically ad- dressed by matching local descriptors. Recently, a few at- tempts at applying the deep learning paradigm to the task have shown promising results. Yet, the only explored way to learn rotation invariant descriptors has been to feed neural networks with highly engineered and invariant representa- tions provided by existing hand-crafted descriptors, a path that goes in the opposite direction of end-to-end learning from raw data so successfully deployed for 2D images. In this paper, we explore the benefits of taking a step back in the direction of end-to-end learning of 3D descrip- tors by disentangling the creation of a robust and distinctive rotation equivariant representation, which can be learned from unoriented input data, and the definition of a good canonical orientation, required only at test time to obtain an invariant descriptor. To this end, we leverage two re- cent innovations: spherical convolutional neural networks to learn an equivariant descriptor and plane folding de- coders to learn without supervision. The effectiveness of the proposed approach is experimentally validated by out- performing hand-crafted and learned descriptors on a stan- dard benchmark.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009530/,"['Three-dimensional displays', 'Correlation', 'Machine learning', 'Image reconstruction', 'Neural networks', 'Two dimensional displays', 'Proposals']","['Raw Data', 'Benchmark', 'Neural Network', 'Deep Learning', 'Linear Representation', 'Local Descriptors', 'Rotation Invariance', '3D Descriptors', 'Invariant Representation', 'Training Time', 'Feature Maps', 'Point Cloud', 'Multilayer Perceptron', 'Coordination Sphere', 'Latent Space', 'Feature Points', '3D Point', 'Value Of Map', 'Input Representation', 'Local Reference Frame', 'Local Frame', 'Rotation Group', 'Input Point', 'Surface Matching', 'Pose Information', 'Learned Filters', 'Voxel Grid', 'Reference Axis', 'External Frame']",,24,"Establishing correspondences between 3D shapes is a fundamental task in 3D Computer Vision, typically ad- dressed by matching local descriptors. Recently, a few at- tempts at applying the deep learning paradigm to the task have shown promising results. Yet, the only explored way to learn rotation invariant descriptors has been to feed neural networks with highly engineered and invariant representations provided by existing hand-crafted descriptors, a path that goes in the opposite direction of end-to-end learning from raw data so successfully deployed for 2D images. In this paper, we explore the benefits of taking a step back in the direction of end-to-end learning of 3D descriptors by disentangling the creation of a robust and distinctive rotation equivariant representation, which can be learned from unoriented input data, and the definition of a good canonical orientation, required only at test time to obtain an invariant descriptor. To this end, we leverage two re- cent innovations: spherical convolutional neural networks to learn an equivariant descriptor and plane folding de- coders to learn without supervision. The effectiveness of the proposed approach is experimentally validated by out- performing hand-crafted and learned descriptors on a standard benchmark."
Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo,"Stepan Tulyakov, Francois Fleuret, Martin Kiefel, Peter Gehler, Michael Hirsch","Ecole Polytechnique Fédérale de Lausanne and Idiap Research Institute; Space Engineering Center at Ecole Polytechnique Fédérale de Lausanne; Amazon, Tübingen, Germany",66.66666666666666,"france, switzerland",33.33333333333334,Germany,"Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tulyakov_Learning_an_Event_Sequence_Embedding_for_Dense_Event-Based_Deep_Stereo_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tulyakov_Learning_an_Event_Sequence_Embedding_for_Dense_Event-Based_Deep_Stereo_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008838/,"['Cameras', 'Image sensors', 'Tensile stress', 'Three-dimensional displays', 'Robot sensing systems', 'Machine learning', 'Power demand']","['Sequence Of Events', 'Deep Stereo', 'Data Rate', 'Dynamic Range', 'Learning-based Methods', 'Image Sensor', 'Fully-connected Layer', 'Deep Learning-based Methods', 'High Dynamic Range', 'High Power Consumption', 'Fixed Time Interval', 'Low Dynamic Range', 'Dynamic Vision Sensor', 'Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Recurrent Neural Network', 'Multilayer Perceptron', 'Spatial Context', 'Stereo Matching', 'Temporal Aggregation', 'Shallow Network', 'Temporal Convolutional Network', 'Event Timestamps', 'Camera Motion', 'Temporal Convolution', 'Markov Random Field', 'Temporal Context', 'First-in-first-out']",,54,"Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods."
Learning the Model Update for Siamese Trackers,"Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer, Martin Danelljan, Fahad Shahbaz Khan","Computer Vision Laboratory, ETH Zurich, Switzerland; Inception Institute of Artiﬁcial Intelligence, UAE and Computer Vision Laboratory, Linkoping University, Sweden; Computer Vision Center, Universitat Autonoma de Barcelona, Spain",100.0,"Spain, switzerland, uae",0.0,,"Siamese approaches address the visual tracking problem by extracting an appearance template from the current frame, which is used to localize the target in the next frame. In general, this template is linearly combined with the accumulated template from the previous frame, resulting in an exponential decay of information over time. While such an approach to updating has led to improved results, its simplicity limits the potential gain likely to be obtained by learning to update. Therefore, we propose to replace the handcrafted update function with a method which learns to update. We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame aims to estimate the optimal template for the next frame. The UpdateNet is compact and can easily be integrated into existing Siamese trackers. We demonstrate the generality of the proposed approach by applying it to two Siamese trackers, SiamFC and DaSiamRPN. Extensive experiments on VOT2016, VOT2018, LaSOT, and TrackingNet datasets demonstrate that our UpdateNet effectively predicts the new target template, outperforming the standard linear update. On the large-scale TrackingNet dataset, our UpdateNet improves the results of DaSiamRPN with an absolute gain of 3.9% in terms of success score.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_the_Model_Update_for_Siamese_Trackers_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_the_Model_Update_for_Siamese_Trackers_ICCV_2019_paper.pdf,,https://github.com/zhanglichao/updatenet,,main,Poster,https://ieeexplore.ieee.org/document/9008117/,"['Target tracking', 'Computational modeling', 'Adaptation models', 'Training', 'Computer vision', 'Correlation']","['Siamese Trackers', 'Neural Network', 'Convolutional Network', 'Convolutional Neural Network', 'Current Frame', 'Previous Frame', 'Update Function', 'Target Template', 'Target Frame', 'Long Short-term Memory', 'Intersection Over Union', 'Recent Approaches', 'Target Image', 'Video Frames', 'Object Location', 'Tracking Performance', 'Changes In Appearance', 'Update Strategy', 'Response Map', 'Siamese Network', 'Location In Frame', 'Update Rate', 'Correlation Filter', 'Online Tracking', 'Object Appearance', 'Tracking Speed', 'Search Region', 'Ground Truth Location', 'Tracking Framework', 'Focus Of This Paper']",,294,"Siamese approaches address the visual tracking problem by extracting an appearance template from the current frame, which is used to localize the target in the next frame. In general, this template is linearly combined with the accumulated template from the previous frame, resulting in an exponential decay of information over time. While such an approach to updating has led to improved results, its simplicity limits the potential gain likely to be obtained by learning to update. Therefore, we propose to replace the handcrafted update function with a method which learns to update. We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame aims to estimate the optimal template for the next frame. The UpdateNet is compact and can easily be integrated into existing Siamese trackers. We demonstrate the generality of the proposed approach by applying it to two Siamese trackers, SiamFC and DaSiamRPN. Extensive experiments on VOT2016, VOT2018, LaSOT, and TrackingNet datasets demonstrate that our UpdateNet effectively predicts the new target template, outperforming the standard linear update. On the large-scale TrackingNet dataset, our UpdateNet improves the results of DaSiamRPN with an absolute gain of 3.9% in terms of success score."
Learning to Assemble Neural Module Tree Networks for Visual Grounding,"Daqing Liu, Hanwang Zhang, Feng Wu, Zheng-Jun Zha",University of Science and Technology of China; Nanyang Technological University,100.0,"Singapore, china",0.0,,"Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009000/,"['Grounding', 'Visualization', 'Cognition', 'Task analysis', 'Artificial neural networks', 'Natural languages', 'Training']","['Neural Modulation', 'Visual Grounding', 'Natural Language', 'Qualitative Results', 'Linguistic Features', 'Assembly Module', 'Parse Tree', 'Syntax Errors', 'Visual Reasoning', 'Scoring Function', 'Object Detection', 'Visual Features', 'Image Regions', 'Tree Structure', 'Top-down And Bottom-up', 'Word Embedding', 'Postage', 'Reasoning Process', 'Attention Map', 'Forward Pass', 'Language Representation', 'Additional Annotations', 'Hidden Vector', 'Gumbel Distribution', 'Composite Language', 'Bottom-up Fashion', 'Linguistic Information', 'Backward Pass', 'Video Games']",,154,"Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail."
Learning to Caption Images Through a Lifetime by Asking Questions,"Tingke Shen, Amlan Kar, Sanja Fidler","Vector Institute, University of Toronto, NVIDIA; Vector Institute, University of Toronto",100.0,Canada,0.0,,"In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Learning_to_Caption_Images_Through_a_Lifetime_by_Asking_Questions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Learning_to_Caption_Images_Through_a_Lifetime_by_Asking_Questions_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009050/,"['Generators', 'Task analysis', 'Natural languages', 'Training', 'Cognition', 'Dogs', 'Automobiles']","['Decision-making', 'Active Learning', 'Natural Language', 'Human Supervision', 'Time Step', 'Multilayer Perceptron', 'Maximum Entropy', 'Learning Settings', 'Word Embedding', 'Collection Phase', 'Vocabulary Size', 'Learning Agent', 'Human Education', 'Question Asks', 'Human-in-the-loop', 'Image Captioning', 'Large Vocabulary', 'Active Learning Approach', 'Visual Question Answering', 'Update Phase', 'Synthetic Environment', 'Supervision Cost']",,7,"In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset."
Learning to Collocate Neural Modules for Image Captioning,"Xu Yang, Hanwang Zhang, Jianfei Cai","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Faculty of Information Technology, Monash University, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore",100.0,"Singapore, australia",0.0,,"We do not speak word by word from scratch; our brain quickly structures a pattern like sth do sth at someplace and then fill in the detailed description. To render existing encoder-decoder image captioners such human-like reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the ""inner pattern"" connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q&A, where the language (i.e., question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design --- one for function words and three for visual content words (e.g., noun, adjective, and verb), 2) soft module fusion and multi-step module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (e.g., adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, e.g., by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010688/,"['Visualization', 'Training', 'Task analysis', 'Cognition', 'Dogs', 'Decoding', 'Neural networks']","['Neural Modulation', 'Image Captioning', 'Function Words', 'Partial Observation', 'Strong Baseline', 'Word-by-word', 'Visual Reasoning', 'Time Step', 'Parsing', 'Functional Modules', 'Cross-entropy Loss', 'Training Images', 'Pie Chart', 'Postage', 'Prepositions', 'Input Modalities', 'Residual Connection', 'Dataset Bias', 'Related Modules', 'Inductive Bias', 'Language Loss', 'Vector C', 'Fewer Training Samples', 'Fusion Weights', 'Training Matrix', 'Context Vector', 'Visual Clues', 'Feature Transformation', 'Higher Ones', 'Visual Modality']",,55,"We do not speak word by word from scratch; our brain quickly structures a pattern like STH DO STH AT SOMEPLACE and then fills in the detailed descriptions. To render existing encoder-decoder image captioners such humanlike reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the “inner pattern” connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q&A, where the language (i.e., question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design - one for function words and three for visual content words (e.g., noun, adjective, and verb), 2) soft module fusion and multistep module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (e.g., adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDErD on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, e.g., by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline."
Learning to Discover Novel Visual Categories via Deep Transfer Clustering,"Kai Han, Andrea Vedaldi, Andrew Zisserman","Visual Geometry Group, University of Oxford",100.0,uk,0.0,,"We consider the problem of discovering novel object categories in an image collection. While these images are unlabelled, we also assume prior knowledge of related but different image classes. We use such prior knowledge to reduce the ambiguity of clustering, and improve the quality of the newly discovered classes. Our contributions are twofold. The first contribution is to extend Deep Embedded Clustering to a transfer learning setting; we also improve the algorithm by introducing a representation bottleneck, temporal ensembling, and consistency. The second contribution is a method to estimate the number of classes in the unlabelled data. This also transfers knowledge from the known classes, using them as probes to diagnose different choices for the number of classes in the unlabelled subset. We thoroughly evaluate our method, substantially outperforming state-of-the-art techniques in a large number of benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Han_Learning_to_Discover_Novel_Visual_Categories_via_Deep_Transfer_Clustering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Learning_to_Discover_Novel_Visual_Categories_via_Deep_Transfer_Clustering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010037/,"['Cats', 'Clustering algorithms', 'Data models', 'Visualization', 'Semisupervised learning', 'Task analysis', 'Dogs']","['Visual Classification', 'Transfer Learning', 'Unlabeled Data', 'Clustering Algorithm', 'Data Clustering', 'Representation Learning', 'Clustering Approach', 'Cluster Centers', 'Domain Adaptation', 'Similar Color', 'Semi-supervised Learning', 'Target Distribution', 'Sample Prediction', 'Target Dataset', 'Challenging Scenarios', 'Metric Learning', 'Background Set', 'Temporal Consistency', 'Temporal Prediction', 'Consistency Constraint', 'Subset Of Classes', 'ImageNet Pre-trained Model', 'Unlabeled Set', 'Cluster Model', 'Clustering Accuracy', 'Kullback-Leibler', 'Clustering Quality', 'Realistic Assumption', 'Source Dataset', 'Horizontal Flip']",,133,"We consider the problem of discovering novel object categories in an image collection. While these images are unlabelled, we also assume prior knowledge of related but different image classes. We use such prior knowledge to reduce the ambiguity of clustering, and improve the quality of the newly discovered classes. Our contributions are twofold. The first contribution is to extend Deep Embedded Clustering to a transfer learning setting; we also improve the algorithm by introducing a representation bottleneck, temporal ensembling, and consistency. The second contribution is a method to estimate the number of classes in the unlabelled data. This also transfers knowledge from the known classes, using them as probes to diagnose different choices for the number of classes in the unlabelled subset. We thoroughly evaluate our method, substantially outperforming state-of-the-art techniques in a large number of benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN."
Learning to Find Common Objects Across Few Image Collections,"Amirreza Shaban, Amir Rahimi, Shray Bansal, Stephen Gould, Byron Boots, Richard Hartley","ACRV, ANU Canberra; Georgia Tech",50.0,usa,50.0,Unknown,"Given a collection of bags where each bag is a set of images, our goal is to select one image from each bag such that the selected images are from the same object class. We model the selection as an energy minimization problem with unary and pairwise potential functions. Inspired by recent few-shot learning algorithms, we propose an approach to learn the potential functions directly from the data. Furthermore, we propose a fast greedy inference algorithm for energy minimization. We evaluate our approach on few-shot common object recognition as well as object co-localization tasks. Our experiments show that learning the pairwise and unary terms greatly improves the performance of the model over several well-known methods for these tasks. The proposed greedy optimization algorithm achieves performance comparable to state-of-the-art structured inference algorithms while being  10 times faster.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shaban_Learning_to_Find_Common_Objects_Across_Few_Image_Collections_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaban_Learning_to_Find_Common_Objects_Across_Few_Image_Collections_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010848/,"['Task analysis', 'Training', 'Labeling', 'Inference algorithms', 'Computer vision', 'Minimization', 'Graphical models']","['Common Objects', 'Energy Minimization', 'Object Classification', 'Inference Algorithm', 'Few-shot Learning', 'Collection Bag', 'Pairwise Potential', 'Common Recognition', 'Greedy Optimization', 'Pairwise Terms', 'Energy Minimization Problem', 'Training Set', 'Training Dataset', 'Network Of Relationships', 'Energy Function', 'Image Pairs', 'Graphical Model', 'Root Node', 'Target Class', 'Part Of The Solution', 'Multiple Instance Learning', 'Unseen Classes', 'Query Image', 'Set Of Classes', 'Tree Level', 'Embedding Module', 'Related Modules', 'Child Nodes', 'Leaf Node', 'Negative Images']",,6,"Given a collection of bags where each bag is a set of images, our goal is to select one image from each bag such that the selected images are from the same object class. We model the selection as an energy minimization problem with unary and pairwise potential functions. Inspired by recent few-shot learning algorithms, we propose an approach to learn the potential functions directly from the data. Furthermore, we propose a fast greedy inference algorithm for energy minimization. We evaluate our approach on few-shot common object recognition as well as object co-localization tasks. Our experiments show that learning the pairwise and unary terms greatly improves the performance of the model over several well-known methods for these tasks. The proposed greedy optimization algorithm achieves performance comparable to state-of-the-art structured inference algorithms while being ~10 times faster."
Learning to Jointly Generate and Separate Reflections,"Daiqian Ma, Renjie Wan, Boxin Shi, Alex C. Kot, Ling-Yu Duan","The Peng Cheng Laboratory, Shenzhen, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; The SECE of Shenzhen Graduate School, Peking University, Shenzhen, China; The National Engineering Lab for Video Technology, Peking University, Beijing, China",100.0,"Singapore, china",0.0,,"Existing learning-based single image reflection removal methods using paired training data have fundamental limitations about the generalization capability on real-world reflections due to the limited variations in training pairs. In this work, we propose to jointly generate and separate reflections within a weakly-supervised learning framework, aiming to model the reflection image formation more comprehensively with abundant unpaired supervision. By imposing the adversarial losses and combinable mapping mechanism in a multi-task structure, the proposed framework elegantly integrates the two separate stages of reflection generation and separation into a unified model. The gradient constraint is incorporated into the concurrent training process of the multi-task learning as well. In particular, we built up an unpaired reflection dataset with 4,027 images, which is useful for facilitating the weakly-supervised learning of reflection removal model. Extensive experiments on a public benchmark dataset show that our framework performs favorably against state-of-the-art methods and consistently produces visually appealing results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Learning_to_Jointly_Generate_and_Separate_Reflections_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Learning_to_Jointly_Generate_and_Separate_Reflections_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009453/,"['Training', 'Generators', 'Image edge detection', 'Mathematical model', 'Particle separators', 'Glass', 'Task analysis']","['Public Datasets', 'Learning Framework', 'Paired Data', 'Benchmark Datasets', 'Unified Model', 'Image Formation', 'Multi-task Learning', 'Reflectance Images', 'Learning Manner', 'Deep Learning', 'Convolutional Layers', 'Image Dataset', 'Generative Adversarial Networks', 'Image Generation', 'Visual Quality', 'Background Image', 'Variational Autoencoder', 'Reconstruction Loss', 'Error Metrics', 'Pixel Domain', 'Auxiliary Task', 'Reflective Layer', 'Translation Problems', 'Prior Imaging']",,30,"Existing learning-based single image reflection removal methods using paired training data have fundamental limitations about the generalization capability on real-world reflections due to the limited variations in training pairs. In this work, we propose to jointly generate and separate reflections within a weakly-supervised learning framework, aiming to model the reflection image formation more comprehensively with abundant unpaired supervision. By imposing the adversarial losses and combinable mapping mechanism in a multi-task structure, the proposed framework elegantly integrates the two separate stages of reflection generation and separation into a unified model. The gradient constraint is incorporated into the concurrent training process of the multi-task learning as well. In particular, we built up an unpaired reflection dataset with 4,027 images, which is useful for facilitating the weakly-supervised learning of reflection removal model. Extensive experiments on a public benchmark dataset show that our framework performs favorably against state-of-the-art methods and consistently produces visually appealing results."
Learning to Paint With Model-Based Deep Reinforcement Learning,"Zhewei Huang, Wen Heng, Shuchang Zhou","Megvii Inc; Megvii Inc, Peking University",50.0,china,50.0,China,"We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Learning_to_Paint_With_Model-Based_Deep_Reinforcement_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_to_Paint_With_Model-Based_Deep_Reinforcement_Learning_ICCV_2019_paper.pdf,,https://github.com/hzwer/ICCV2019-LearningToPaint,,main,Poster,https://ieeexplore.ieee.org/document/9010329/,"['Painting', 'Training', 'Paints', 'Task analysis', 'Image color analysis', 'Adaptation models', 'Rendering (computer graphics)']","['Deep Reinforcement Learning', 'Model-based Deep Reinforcement Learning', 'Neural Network', 'Recurrent Neural Network', 'Types Of Images', 'Target Image', 'Reward Function', 'Natural Scenes', 'Discrete Space', 'Transition Function', 'Scribble', 'Real-world Images', 'Performance Of Agents', 'Trained Agent', 'Continuous Action Space', 'Bezier Curve', 'Natural Scene Images', 'Deep Reinforcement Learning Framework', 'Deep Reinforcement Learning Agent', 'Wasserstein Generative Adversarial Networks']",,79,"We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint."
Learning to Rank Proposals for Object Detection,"Zhiyu Tan, Xuecheng Nie, Qi Qian, Nan Li, Hao Li","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Alibaba Group, Beijing, China",50.0,singapore,50.0,China,"Non-Maximum Suppression (NMS) is an essential step of modern object detection models for removing duplicated candidates. The efficacy of NMS heavily affects the final detection results. Prior works exploit suppression criterions relying on either the objectiveness derived from classification or the overlapness produced by regression, both of which are heuristically designed and fail to explicitly link with the suppression rank. To address this issue, in this paper, we propose a novel Learning-to-Rank (LTR) model to produce the suppression rank via a learning procedure, thus facilitating the candidate generation and lifting the detection performance. In particular, we define a ranking score based on IoU to indicate the ranks of candidates during the NMS step, where candidates with high ranking score will be reserved and the ones with low ranking score will be eliminated. We design a lightweight network to predict the ranking score. We introduce a ranking loss to supervise the generation of these ranking scores, which encourages candidates with IoU to the ground-truth to rank higher. To facilitate the training procedure, we design a novel sampling strategy via dividing candidates into different levels and select hard pairs to adopt in the training. During the inference phase, this module can be exploited as a plugin to the current object detector. The training and inference of the overall framework is end-to-end. Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the generality and effectiveness of our model for facilitating existing object detectors to state-of-the-art accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tan_Learning_to_Rank_Proposals_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Learning_to_Rank_Proposals_for_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010683/,"['Training', 'Detectors', 'Object detection', 'Feature extraction', 'Task analysis', 'Quantization (signal)', 'Computer vision']","['Object Detection', 'Learning Procedure', 'Ranking Score', 'Non-maximum Suppression', 'Object Detection Model', 'Inference Phase', 'Ranking Loss', 'Convolutional Neural Network', 'Positive Samples', 'Validation Set', 'Computer Vision', 'Classification Task', 'Training Phase', 'Data Augmentation', 'Bounding Box', 'Large Margin', 'Classification Score', 'Faster R-CNN', 'Training Pairs', 'Weight Balance', 'Intersection Over Union Value', 'MS COCO Dataset', 'Mask R-CNN', 'PASCAL VOC Dataset', 'Region Proposal Network', 'Candidate Scores', 'Ratio Of Positive Samples']",,36,"Non-Maximum Suppression (NMS) is an essential step of modern object detection models for removing duplicated candidates. The efficacy of NMS heavily affects the final detection results. Prior works exploit suppression criterions relying on either the objectiveness derived from classification or the overlapness produced by regression, both of which are heuristically designed and fail to explicitly link with the suppression rank. To address this issue, in this paper, we propose a novel Learning-to-Rank (LTR) model to produce the suppression rank via a learning procedure, thus facilitating the candidate generation and lifting the detection performance. In particular, we define a ranking score based on IoU to indicate the ranks of candidates during the NMS step, where candidates with high ranking score will be reserved and the ones with low ranking score will be eliminated. We design a lightweight network to predict the ranking score. We introduce a ranking loss to supervise the generation of these ranking scores, which encourages candidates with IoU to the ground-truth to rank higher. To facilitate the training procedure, we design a novel sampling strategy via dividing candidates into different levels and select hard pairs to adopt in the training. During the inference phase, this module can be exploited as a plugin to the current object detector. The training and inference of the overall framework is end-to-end. Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the generality and effectiveness of our model for facilitating existing object detectors to state-of-the-art accuracy."
Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop,"Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, Kostas Daniilidis",University of Pennsylvania; Max Planck Institute for Intelligent Systems,100.0,"Germany, usa",0.0,,"Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/ nkolot/projects/spin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.pdf,https://seas.upenn.edu/~nkolot/projects/spin,,,main,Poster,https://ieeexplore.ieee.org/document/9010949/,"['Two dimensional displays', 'Optimization', 'Three-dimensional displays', 'Shape', 'Training', 'Solid modeling', 'Pose estimation']","['3D Shape', 'Human Pose', '3D Human Pose', 'Model Parameters', 'Deep Network', 'Model-based Approach', 'Iterative Optimization', 'Training Iterations', 'Set Of Approaches', 'Pose Estimation', 'Fitting Accuracy', 'Body Model', 'Model-based Estimates', 'Human Pose Estimation', 'Optimization-based Methods', 'Regression-based Methods', 'Ground Truth 3D', 'Strong Supervision', 'Optimization Routines', 'Direct Regression', '3D Pose', '2D Keypoints', 'Iterative Fitting', 'Shape Estimation', 'Regression-based Approach', 'Reprojection Error', 'Optimization-based Approach', 'Regression Network', 'Single Image', 'Neural Network']",,666,"Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/~nkolot/projects/spin."
Learning to Reconstruct 3D Manhattan Wireframes From a Single Image,"Yichao Zhou, Haozhi Qi, Yuexiang Zhai, Qi Sun, Zhili Chen, Li-Yi Wei, Yi Ma",Adobe Research; UC Berkeley,50.0,usa,50.0,USA,"From a single view of an urban environment, we propose a method to effectively exploit the global structural regularities for obtaining a compact, accurate, and intuitive 3D wireframe representation. Our method trains a single convolutional neural network to simultaneously detect salient junctions and straight lines, as well as predict their 3D depth and vanishing points. Compared with state-of-the-art learning-based wireframe detection methods, our network is much simpler and more unified, leading to better 2D wireframe detection. With a global structural prior (such as Manhattan assumption), our method further reconstructs a full 3D wireframe model, a compact vector representation suitable for a variety of high-level vision tasks such as AR and CAD. We conduct extensive evaluations of our method on a large new synthetic dataset of urban scenes as well as real images. Our code and datasets will be published along with the paper.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Learning_to_Reconstruct_3D_Manhattan_Wireframes_From_a_Single_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Learning_to_Reconstruct_3D_Manhattan_Wireframes_From_a_Single_Image_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010693/,"['Junctions', 'Three-dimensional displays', 'Geometry', 'Neural networks', 'Heating systems', 'Feature extraction', 'Image reconstruction']","['Single Image', 'Neural Network', 'Global Structure', 'Compact Representation', 'Accurate 3D', 'Urban Scenes', 'Heatmap', 'Convolutional Layers', 'Local Features', 'Input Image', '3D Reconstruction', 'Number Of Images', 'Intersection Over Union', 'Point Cloud', 'Depth Map', 'Depth Camera', 'Depth Estimation', '3D Geometry', 'Structure From Motion', 'Types Of Junctions', 'Hand-held Camera', 'Joint Training', 'Calibration Matrix', '3D Depth', 'Scene Geometry', 'Geometric Constraints', 'Image Loss', 'Loss Function', 'Depth Values']",,39,"From a single view of an urban environment, we propose a method to effectively exploit the global structural regularities for obtaining a compact, accurate, and intuitive 3D wireframe representation. Our method trains a single convolutional neural network to simultaneously detect salient junctions and straight lines, as well as predict their 3D depth and vanishing points. Compared with state-of-the-art learning-based wireframe detection methods, our network is much simpler and more unified, leading to better 2D wireframe detection. With a global structural prior (such as Manhattan assumption), our method further reconstructs a full 3D wireframe model, a compact vector representation suitable for a variety of high-level vision tasks such as AR and CAD. We conduct extensive evaluations of our method on a large new synthetic dataset of urban scenes as well as real images. Our code and datasets will be published along with the paper."
Learning to See Moving Objects in the Dark,"Haiyang Jiang, Yinqiang Zheng","University of Southern California, California, USA; National Institue of Informatics, Tokyo, Japan",100.0,"Japan, usa",0.0,,"Video surveillance systems have wide range of utilities, yet easily suffer from great quality degeneration under dim light circumstances. Industrial solutions mainly use extra near-infrared illuminations, even though it doesn't preserve color and texture information. A variety of researches enhanced low-light videos shot by visible light cameras, while they either relied on task specific preconditions or trained with synthetic datasets. We propose a novel optical system to capture bright and dark videos of the exact same scenes, generating training and groud truth pairs for authentic low-light video dataset. A fully convolutional network with 3D and 2D miscellaneous operations is utilized to learn an enhancement mapping with proper spatial-temporal transformation from raw camera sensor data to bright RGB videos. Experiments show promising results by our method, and it outperforms state-of-the-art low-light image/video enhancement algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Learning_to_See_Moving_Objects_in_the_Dark_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Learning_to_See_Moving_Objects_in_the_Dark_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010274/,"['Cameras', 'Training', 'Optical filters', 'Lighting', 'Image color analysis', 'Optical imaging', 'Task analysis']","['Blackbody', 'Optical System', 'Raw Sensor Data', 'Enhancement Algorithm', 'Illumination', 'Mean Square Error', 'Training Dataset', 'Denoising', 'Qualitative Results', 'Pedestrian', 'Low Light', 'Temporal Information', 'Deep Convolutional Network', 'Video Frames', 'Camera System', 'Spatial Size', 'Image Enhancement', 'Street View', 'Neutral Density', 'Neutral Density Filter', 'Low-light Image', 'Key Frames', 'Gamma Correction', 'Dark Images', 'Video Clips', 'Video Information', 'Data Augmentation']",,87,"Video surveillance systems have wide range of utilities, yet easily suffer from great quality degeneration under dim light circumstances. Industrial solutions mainly use extra near-infrared illuminations, even though it doesn't preserve color and texture information. A variety of researches enhanced low-light videos shot by visible light cameras, while they either relied on task specific preconditions or trained with synthetic datasets. We propose a novel optical system to capture bright and dark videos of the exact same scenes, generating training and groud truth pairs for authentic low-light video dataset. A fully convolutional network with 3D and 2D miscellaneous operations is utilized to learn an enhancement mapping with proper spatial-temporal transformation from raw camera sensor data to bright RGB videos. Experiments show promising results by our method, and it outperforms state-of-the-art low-light image/video enhancement algorithms."
Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection,"Mykhailo Shvets, Wei Liu, Alexander C. Berg",UNC at Chapel Hill; Nuro Inc,0.0,,100.0,USA,"Single-frame object detectors perform well on videos sometimes, even without temporal context. However, challenges such as occlusion, motion blur, and rare poses of objects are hard to resolve without temporal awareness. Thus, there is a strong need to improve video object detection by considering long-range temporal dependencies. In this paper, we present a light-weight modification to a single-frame detector that accounts for arbitrary long dependencies in a video. It improves the accuracy of a single-frame detector significantly with negligible compute overhead. The key component of our approach is a novel temporal relation module, operating on object proposals, that learns the similarities between proposals from different frames and selects proposals from past and/or future to support current proposals. Our final ""causal"" model, without any offline post-processing steps, runs at a similar speed as a single-frame detector and achieves state-of-the-art video object detection on ImageNet VID dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shvets_Leveraging_Long-Range_Temporal_Relationships_Between_Proposals_for_Video_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shvets_Leveraging_Long-Range_Temporal_Relationships_Between_Proposals_for_Video_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010864/,"['Proposals', 'Feature extraction', 'Detectors', 'Correlation', 'Object detection', 'Task analysis', 'Indexes']","['Object Detection', 'Video Object Detection', 'Post-processing Step', 'Temporal Context', 'Similar Speed', 'Motion Blur', 'Related Modules', 'Object Proposals', 'Attention Mechanism', 'Bounding Box', 'Relationship Matrix', 'Video Frames', 'Target Features', 'Action Recognition', 'Optical Flow', 'Consecutive Frames', 'Appearance Features', 'Feature Aggregation', 'Linear Layer', 'Relational Reasoning', 'Target Frame', 'Support Frame', 'Frame Detection', 'Non-local Block', 'Symmetric Mode', 'Graph Convolution', 'Part Of The Video', 'Detection In Videos', 'Geometric Relationship', 'Softmax']",,69,"Single-frame object detectors perform well on videos sometimes, even without temporal context. However, challenges such as occlusion, motion blur, and rare poses of objects are hard to resolve without temporal awareness. Thus, there is a strong need to improve video object detection by considering long-range temporal dependencies. In this paper, we present a light-weight modification to a single-frame detector that accounts for arbitrary long dependencies in a video. It improves the accuracy of a single-frame detector significantly with negligible compute overhead. The key component of our approach is a novel temporal relation module, operating on object proposals, that learns the similarities between proposals from different frames and selects proposals from past and/or future to support current proposals. Our final “causal"" model, without any offline post-processing steps, runs at a similar speed as a single-frame detector and achieves state-of-the-art video object detection on ImageNet VID dataset."
Lifelong GAN: Continual Learning for Conditional Image Generation,"Mengyao Zhai, Lei Chen, Frederick Tung, Jiawei He, Megha Nawhal, Greg Mori","Simon Fraser University, Borealis AI; Simon Fraser University",100.0,canada,0.0,,"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009516/,"['Task analysis', 'Gallium nitride', 'Image generation', 'Data models', 'Training', 'Knowledge engineering', 'Training data']","['Image Generation', 'Incremental Learning', 'Conditional Image Generation', 'Deep Neural Network', 'General Framework', 'Lifelong Learning', 'Current Task', 'Previous Tasks', 'Catastrophic Forgetting', 'Current Data', 'Image Size', 'Model Discrimination', 'Results Of Task', 'Learning Resources', 'Sequence Learning', 'Challenging Dataset', 'Ground Truth Image', 'Respective Domains', 'Generator Output', 'Auxiliary Data', 'Latent Code', 'Distillation Loss', 'Joint Learning', 'Translation Task']",,101,"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method."
Linearized Multi-Sampling for Differentiable Image Transformation,"Wei Jiang, Weiwei Sun, Andrea Tagliasacchi, Eduard Trulls, Kwang Moo Yi","Visual Computing Group, University of Victoria; Visual Computing Group, University of Victoria and Google Research; Google Research",66.66666666666666,canada,33.33333333333334,USA,"We propose a novel image sampling method for differentiable image transformation in deep neural networks. The sampling schemes currently used in deep learning, such as Spatial Transformer Networks, rely on bilinear interpolation, which performs poorly under severe scale changes, and more importantly, results in poor gradient propagation. This is due to their strict reliance on direct neighbors. Instead, we propose to generate random auxiliary samples in the vicinity of each pixel in the sampled image, and create a linear approximation with their intensity values. We then use this approximation as a differentiable formula for the transformed image. We demonstrate that our approach produces more representative gradients with a wider basin of convergence for image alignment, which leads to considerable performance improvements when training networks for registration and classification tasks. This is not only true under large downsampling, but also when there are no scale changes. We compare our approach with multi-scale sampling and show that we outperform it. We then demonstrate that our improvements to the sampler are compatible with other tangential improvements to Spatial Transformer Networks and that it further improves their performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Linearized_Multi-Sampling_for_Differentiable_Image_Transformation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Linearized_Multi-Sampling_for_Differentiable_Image_Transformation_ICCV_2019_paper.pdf,,https://github.com/vcg-uvic/linearized_multisampling_release,,main,Oral,https://ieeexplore.ieee.org/document/9008815/,"['Interpolation', 'Linear approximation', 'Kernel', 'Convergence', 'Machine learning', 'Optimization', 'Task analysis']","['Image Transformation', 'Differential Transformation', 'Deep Learning', 'Sampling Method', 'Deep Network', 'Deep Neural Network', 'Network Training', 'Linear Approximation', 'Scale Changes', 'Bilinear Interpolation', 'Spatial Network', 'Image Alignment', 'Direct Neighbors', 'Considerable Performance Improvement', 'Spatial Transformer Network', 'Gaussian Noise', 'Feature Maps', 'Markov Chain Monte Carlo', 'Network Performance', 'Attention Mechanism', 'Classification Network', 'PASCAL VOC', 'PASCAL VOC Dataset', 'Transformation Parameters', 'Neighboring Pixels', 'Point Spread Function', 'Optical Flow', 'ReLU Activation', 'Natural Images', 'Gradient Flow']",,16,"We propose a novel image sampling method for differentiable image transformation in deep neural networks. The sampling schemes currently used in deep learning, such as Spatial Transformer Networks, rely on bilinear interpolation, which performs poorly under severe scale changes, and more importantly, results in poor gradient propagation. This is due to their strict reliance on direct neighbors. Instead, we propose to generate random auxiliary samples in the vicinity of each pixel in the sampled image, and create a linear approximation with their intensity values. We then use this approximation as a differentiable formula for the transformed image. We demonstrate that our approach produces more representative gradients with a wider basin of convergence for image alignment, which leads to considerable performance improvements when training networks for registration and classification tasks. This is not only true under large downsampling, but also when there are no scale changes. We compare our approach with multi-scale sampling and show that we outperform it. We then demonstrate that our improvements to the sampler are compatible with other tangential improvements to Spatial Transformer Networks and that it further improves their performance."
Linearly Converging Quasi Branch and Bound Algorithms for Global Rigid Registration,"Nadav Dym, Shahar Ziv Kovalsky",Duke University,100.0,usa,0.0,,"In recent years, several branch-and-bound (BnB) algorithms have been proposed to globally optimize rigid registration problems. In this paper, we suggest a general framework to improve upon the BnB approach, which we name Quasi BnB. Quasi BnB replaces the linear lower bounds used in BnB algorithms with quadratic quasi-lower bounds which are based on the quadratic behavior of the energy in the vicinity of the global minimum. While quasi-lower bounds are not truly lower bounds, the Quasi-BnB algorithm is globally optimal. In fact we prove that it exhibits linear convergence -- it achieves epsilon accuracy in O(log(1/epsilon)) time while the time complexity of other rigid registration BnB algorithms is polynomial in 1/epsilon. Our experiments verify that Quasi-BnB is significantly more efficient than state-of-the-art BnB algorithms, especially for problems where high accuracy is desired.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dym_Linearly_Converging_Quasi_Branch_and_Bound_Algorithms_for_Global_Rigid_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dym_Linearly_Converging_Quasi_Branch_and_Bound_Algorithms_for_Global_Rigid_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010695/,"['Three-dimensional displays', 'Optimization', 'Shape', 'Two dimensional displays', 'Convergence', 'Complexity theory', 'Iterative closest point algorithm']","['Rigid Registration', 'Lower Bound', 'Global Optimization', 'Accuracy Time', 'Global Minimum', 'Branch-and-bound Algorithm', 'Optimization Problem', 'Convergence Rate', 'First Approximation', 'Point Cloud', 'Matching Algorithm', 'Closest Point', 'Rigid Transformation', 'Matching Problem', 'Iterative Closest Point', 'Global Optimization Algorithm', '3D Problem', '2D Problem', 'Unit Cube']",,6,"In recent years, several branch-and-bound (BnB) algorithms have been proposed to globally optimize rigid registration problems. In this paper, we suggest a general-framework to improve upon the BnB approach, which we name Quasi BnB. Quasi BnB replaces the linear lower bounds used in BnB algorithms with quadratic quasi-lower bounds which are based on the quadratic behavior of the energy in the vicinity of the global minimum. While quasi-lower bounds are not truly lower bounds, the Quasi-BnB algorithm is globally optimal. In fact we prove that it exhibits linear convergence - it achieves €-accuracy in O(log(1/ε)) time while the time complexity of other rigid registration BnB algorithms is polynomial in 1/ε. Our experiments verify that Quasi-BnB is significantly more efficient than state-of-the-art BnB algorithms, especially for problems where high accuracy is desired."
"Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis","Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, Shenghua Gao",Tencent AI Lab; ShanghaiTech University,50.0,china,50.0,China,"We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.pdf,https://svip-lab.github.io/project/impersonator.html,https://github.com/svip-lab/impersonator,,main,Poster,https://ieeexplore.ieee.org/document/9010740/,"['Gallium nitride', 'Task analysis', 'Three-dimensional displays', 'Feature extraction', 'Liquids', 'Image color analysis', 'Face']","['Imitation', 'Generative Adversarial Networks', 'Framework Synthesis', 'View Synthesis', 'Source Of Information', 'Feature Space', 'Body Shape', 'Image Space', '3D Mesh', 'Source Characteristics', 'Face Identity', 'Joint Rotation', '2D Keypoints', 'Loss Function', 'Target Image', 'Final Image', 'Reference Image', 'Color Map', 'Source Images', 'Pose Parameters', '2D Pose', 'Part Features', 'Blurred Images', 'Background Image', 'Lth Layer', 'Binding Pose', 'Attention Map', 'Perceptual Loss', 'Realistic Images']",,176,"We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html."
Live Face De-Identification in Video,"Oran Gafni, Lior Wolf, Yaniv Taigman",Facebook AI Research and Tel-Aviv University; Facebook AI Research,50.0,Israel,50.0,USA,"We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gafni_Live_Face_De-Identification_in_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gafni_Live_Face_De-Identification_in_Video_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010903/,"['Face', 'Training', 'Lighting', 'Face recognition', 'Task analysis', 'Image reconstruction', 'Distortion']","['Face Deidentification', 'Distortion', 'Illumination', 'Face Images', 'Time Distortion', 'Low Resolution', 'Input Image', 'Identity Information', 'Face Recognition', 'Target Image', 'Latent Space', 'Training Loss', 'Residual Block', 'Output Image', 'Reconstruction Loss', 'Generator Output', 'Spatial Derivatives', 'Person Image', 'Raw Output', 'Perceptual Loss', 'Deepfake', 'Adversarial Examples', 'Image X', 'Facial Recognition Technology', 'Source Video', 'Still Images', 'Instance Normalization', 'Source Images', 'Target Person', 'Model Resolution']",,89,"We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time."
Local Aggregation for Unsupervised Learning of Visual Embeddings,"Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins",Stanford University,100.0,usa,0.0,,"Unsupervised approaches to learning in neural networks are of substantial interest for furthering artificial intelligence, both because they would enable the training of networks without the need for large numbers of expensive annotations, and because they would be better models of the kind of general-purpose learning deployed by humans. However, unsupervised networks have long lagged behind the performance of their supervised counterparts, especially in the domain of large-scale visual recognition. Recent developments in training deep convolutional embeddings to maximize non-parametric instance separation and clustering objectives have shown promise in closing this gap. Here, we describe a method that trains an embedding function to maximize a metric of local aggregation, causing similar data instances to move together in the embedding space, while allowing dissimilar instances to separate. This aggregation metric is dynamic, allowing soft clusters of different scales to emerge. We evaluate our procedure on several large-scale visual recognition datasets, achieving state-of-the-art unsupervised transfer learning performance on object recognition in ImageNet, scene recognition in Places 205, and object detection in PASCAL VOC.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhuang_Local_Aggregation_for_Unsupervised_Learning_of_Visual_Embeddings_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhuang_Local_Aggregation_for_Unsupervised_Learning_of_Visual_Embeddings_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9011034/,"['Task analysis', 'Visualization', 'Training', 'Neural networks', 'Measurement', 'Image recognition', 'Unsupervised learning']","['Unsupervised Learning', 'Local Assembly', 'Neural Network', 'Object Detection', 'Transfer Learning', 'Object Recognition', 'Latent Space', 'Neural Network Learning', 'Convolutional Neural Network', 'K-nearest Neighbor', 'Visual Task', 'Deep Convolutional Neural Network', 'Unsupervised Methods', 'Representation Learning', 'Final Performance', 'Successful Examples', 'Unsupervised Algorithm', 'AlexNet', 'Linear Layer', 'Close Neighbors', 'Memory Bank', 'Visual Learning', 'ImageNet Classification', 'Faster R-CNN', 'Cluster Labels', 'Supplement For Details', 'Top-1 Accuracy', 'Latent Representation', 'Learning Rate']",,238,"Unsupervised approaches to learning in neural networks are of substantial interest for furthering artificial intelligence, both because they would enable the training of networks without the need for large numbers of expensive annotations, and because they would be better models of the kind of general-purpose learning deployed by humans. However, unsupervised networks have long lagged behind the performance of their supervised counterparts, especially in the domain of large-scale visual recognition. Recent developments in training deep convolutional embeddings to maximize non-parametric instance separation and clustering objectives have shown promise in closing this gap. Here, we describe a method that trains an embedding function to maximize a metric of local aggregation, causing similar data instances to move together in the embedding space, while allowing dissimilar instances to separate. This aggregation metric is dynamic, allowing soft clusters of different scales to emerge. We evaluate our procedure on several large-scale visual recognition datasets, achieving state-of-the-art unsupervised transfer learning performance on object recognition in ImageNet, scene recognition in Places 205, and object detection in PASCAL VOC."
Local Relation Networks for Image Recognition,"Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin","Microsoft Research Asia; Microsoft Research Asia, Tsinghua University",50.0,China,50.0,USA,"The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Local_Relation_Networks_for_Image_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Local_Relation_Networks_for_Image_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010392,"['Convolution', 'Feature extraction', 'Visualization', 'Computer vision', 'Computational modeling', 'Task analysis', 'Adaptation models']","['Local Network', 'Image Features', 'Convolutional Layers', 'Computer Vision', 'Visual Elements', 'Spatial Aggregation', 'Weight Aggregates', 'ImageNet Classification', 'Convolutional Neural Network', 'Deep Network', 'Input Features', 'Output Feature', 'Latent Space', 'Residual Block', 'Input Channels', 'Translation Invariance', 'Local Constraints', 'Bottom-up Methods', 'Adversarial Attacks', 'Top-down Methods', 'Spatial Scope', 'Deep Network Layers', 'Capsule Network', 'Geometric Terms', 'Geometric Values', 'Top-down Manner', 'Deformable Convolution', 'Depthwise Convolution', 'Feature Maps', 'Top-1 Accuracy']",,318,"The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification."
Local Supports Global: Deep Camera Relocalization With Sequence Enhancement,"Fei Xue, Xin Wang, Zike Yan, Qiuyuan Wang, Junqiu Wang, Hongbin Zha","Beijing Changcheng Aviation Measurement and Control Institute, A VIC; UISEE Technology Inc.; PKU-SenseTime Machine Vision Joint Lab, Peking University; Key Laboratory of Machine Perception, Peking University",75.0,china,25.0,China,"We propose to leverage the local information in a image sequence to support global camera relocalization. In contrast to previous methods that regress global poses from single images, we exploit the spatial-temporal consistency in sequential images to alleviate uncertainty due to visual ambiguities by incorporating a visual odometry (VO) component. Specifically, we introduce two effective steps called content-augmented pose estimation and motion-based refinement. The content-augmentation step focuses on alleviating the uncertainty of pose estimation by augmenting the observation based on the co-visibility in local maps built by the VO stream. Besides, the motion-based refinement is formulated as a pose graph, where the camera poses are further optimized by adopting relative poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoor Oxford RobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challenging cases, e.g., insufficient texture, highly repetitive textures, similar appearances, and over-exposure.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xue_Local_Supports_Global_Deep_Camera_Relocalization_With_Sequence_Enhancement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_Local_Supports_Global_Deep_Camera_Relocalization_With_Sequence_Enhancement_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010841/,"['Feature extraction', 'Cameras', 'Visualization', 'Visual odometry', 'Pose estimation', 'Task analysis']","['Camera Relocalization', 'Local Information', 'Sequence Information', 'Single Image', 'Additional Constraints', 'Local Map', 'Similar Appearance', 'Pose Estimation', 'Camera Pose', 'Relative Pose', 'Visual Odometry', 'Motion Constraints', 'Neural Network', 'Convolutional Neural Network', 'Feature Space', 'Feature Maps', 'Long Short-term Memory', 'Recurrent Neural Network', 'Unified Model', 'RGB Images', 'Global Average Pooling Layer', 'Hidden State', 'Raw Features', 'Convolutional Long Short-term Memory', 'Consecutive Frames', 'Recurrent Unit', 'Promising Accuracy', 'Rotation Error', 'Bidirectional Long Short-term Memory', 'Original Prediction']",,37,"We propose to leverage the local information in a image sequence to support global camera relocalization. In contrast to previous methods that regress global poses from single images, we exploit the spatial-temporal consistency in sequential images to alleviate uncertainty due to visual ambiguities by incorporating a visual odometry (VO) component. Specifically, we introduce two effective steps called content-augmented pose estimation and motion-based refinement. The content-augmentation step focuses on alleviating the uncertainty of pose estimation by augmenting the observation based on the co-visibility in local maps built by the VO stream. Besides, the motion-based refinement is formulated as a pose graph, where the camera poses are further optimized by adopting relative poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoor Oxford RobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challenging cases, e.g., insufficient texture, highly repetitive textures, similar appearances, and over-exposure."
Localization of Deep Inpainting Using High-Pass Fully Convolutional Network,"Haodong Li, Jiwu Huang","Guangdong Key Laboratory of Intelligent Information Processing and Shenzhen Key Laboratory of Media Security, and National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society, China",100.0,"China, china",0.0,,"Image inpainting has been substantially improved with deep learning in the past years. Deep inpainting can fill image regions with plausible contents, which are not visually apparent. Although inpainting is originally designed to repair images, it can even be used for malicious manipulations, e.g., removal of specific objects. Therefore, it is necessary to identify the presence of inpainting in an image. This paper presents a method to locate the regions manipulated by deep inpainting. The proposed method employs a fully convolutional network that is based on high-pass filtered image residuals. Firstly, we analyze and observe that the inpainted regions are more distinguishable from the untouched ones in the residual domain. Hence, a high-pass pre-filtering module is designed to get image residuals for enhancing inpainting traces. Then, a feature extraction module, which learns discriminative features from image residuals, is built with four concatenated ResNet blocks. The learned feature maps are finally enlarged by an up-sampling module, so that a pixel-wise inpainting localization map is obtained. The whole network is trained end-to-end with a loss addressing the class imbalance. Extensive experimental results evaluated on both synthetic and realistic images subjected to deep inpainting have shown the effectiveness of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Localization_of_Deep_Inpainting_Using_High-Pass_Fully_Convolutional_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Localization_of_Deep_Inpainting_Using_High-Pass_Fully_Convolutional_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009804/,"['Forensics', 'Feature extraction', 'Convolution', 'Kernel', 'Machine learning', 'Computer vision', 'Probability']","['Fully Convolutional Network', 'Deep Inpainting', 'Deep Learning', 'Feature Maps', 'Feature Learning', 'High-pass Filter', 'Local Map', 'Class Imbalance', 'Synthetic Images', 'Realistic Images', 'Image Inpainting', 'Spatial Resolution', 'Convolutional Neural Network', 'Positive Samples', 'Convolutional Layers', 'Image Regions', 'Quality Factor', 'Transition Probabilities', 'Image Object', 'Intersection Over Union', 'Transition Probability Matrix', 'Standard Cross-entropy Loss', 'Hyperbaric Oxygen', 'Forensic Methods', 'Pixel Domain', 'Focal Loss', 'Transposed Convolution Layers', 'JPEG Images', 'ResNet Block', 'Standard Entropy']",,67,"Image inpainting has been substantially improved with deep learning in the past years. Deep inpainting can fill image regions with plausible contents, which are not visually apparent. Although inpainting is originally designed to repair images, it can even be used for malicious manipulations, e.g., removal of specific objects. Therefore, it is necessary to identify the presence of inpainting in an image. This paper presents a method to locate the regions manipulated by deep inpainting. The proposed method employs a fully convolutional network that is based on high-pass filtered image residuals. Firstly, we analyze and observe that the inpainted regions are more distinguishable from the untouched ones in the residual domain. Hence, a high-pass pre-filtering module is designed to get image residuals for enhancing inpainting traces. Then, a feature extraction module, which learns discriminative features from image residuals, is built with four concatenated ResNet blocks. The learned feature maps are finally enlarged by an up-sampling module, so that a pixel-wise inpainting localization map is obtained. The whole network is trained end-to-end with a loss addressing the class imbalance. Extensive experimental results evaluated on both synthetic and realistic images subjected to deep inpainting have shown the effectiveness of the proposed method."
Looking to Relations for Future Trajectory Forecast,"Chiho Choi, Behzad Dariush",Honda Research Institute USA,100.0,usa,0.0,,"Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast. Our system aims to infer relational information from the interactions of road users with each other and with the environment. The first module involves visual encoding of spatio-temporal features, which captures human-human and human-space interactions over time. The following module explicitly constructs pair-wise relations from spatio-temporal interactions and identifies more descriptive relations that highly influence future motion of the target road user by considering its past trajectory. The resulting relational features are used to forecast future locations of the target, in the form of heatmaps with an additional guidance of spatial dependencies and consideration of the uncertainty. Extensive evaluations on the public benchmark datasets demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010989/,"['Roads', 'Trajectory', 'Feature extraction', 'Logic gates', 'Encoding', 'Predictive models', 'Navigation']","['Future Prediction', 'Future Trajectories', 'Related Features', 'Spatial Dependence', 'Spatiotemporal Characteristics', 'Pairwise Relationships', 'Future Position', 'Road Users', 'Form Of A Heatmap', 'Navigation Strategies', 'Spatiotemporal Interactions', 'Human-human Interaction', 'Past Trajectories', 'Future Motion', 'Interactive', 'Human Behavior', 'Training Time', 'Long Short-term Memory', 'Receptive Field', 'User Behavior', 'Road Layout', 'Bayesian Neural Network', 'Temporal Interactions', '3D Convolution', 'Far Future', 'Road Structure', 'Trajectory Prediction', 'Scene Context', 'Spatial Interaction', 'Past Behavior']",,47,"Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast. Our system aims to infer relational information from the interactions of road users with each other and with the environment. The first module involves visual encoding of spatio-temporal features, which captures human-human and human-space interactions over time. The following module explicitly constructs pair-wise relations from spatio-temporal interactions and identifies more descriptive relations that highly influence future motion of the target road user by considering its past trajectory. The resulting relational features are used to forecast future locations of the target, in the form of heatmaps with an additional guidance of spatial dependencies and consideration of the uncertainty. Extensive evaluations on the public benchmark datasets demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods."
M2FPA: A Multi-Yaw Multi-Pitch High-Quality Dataset and Benchmark for Facial Pose Analysis,"Peipei Li, Xiang Wu, Yibo Hu, Ran He, Zhenan Sun","CRIPAC & NLPR & CEBSIT, CASIA; CRIPAC & NLPR & CEBSIT, CASIA and University of Chinese Academy of Sciences",100.0,china,0.0,,"Facial images in surveillance or mobile scenarios often have large view-point variations in terms of pitch and yaw angles. These jointly occurred angle variations make face recognition challenging. Current public face databases mainly consider the case of yaw variations. In this paper, a new large-scale Multi-yaw Multi-pitch high-quality database is proposed for Facial Pose Analysis (M2FPA), including face frontalization, face rotation, facial pose estimation and pose-invariant face recognition. It contains 397,544 images of 229 subjects with yaw, pitch, attribute, illumination and accessory. M2FPA is the most comprehensive multi-view face database for facial pose analysis. Further, we provide an effective benchmark for face frontalization and pose-invariant face recognition on M2FPA with several state-of-the-art methods, including DR-GAN, TP-GAN and CAPG-GAN. We believe that the new database and benchmark can significantly push forward the advance of facial pose analysis in real-world applications. Moreover, a simple yet effective parsing guided discriminator is introduced to capture the local consistency during GAN optimization. Extensive quantitative and qualitative results on M2FPA and Multi-PIE demonstrate the superiority of our face frontalization method. Baseline results for both face synthesis and face recognition from state-of-the-art methods demonstrate the challenge offered by this new database.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_M2FPA_A_Multi-Yaw_Multi-Pitch_High-Quality_Dataset_and_Benchmark_for_Facial_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_M2FPA_A_Multi-Yaw_Multi-Pitch_High-Quality_Dataset_and_Benchmark_for_Facial_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010887/,"['Face', 'Databases', 'Face recognition', 'Cameras', 'Lighting', 'Gallium nitride', 'Benchmark testing']","['Face Pose', 'Illumination', 'Parsing', 'Qualitative Results', 'Database Analysis', 'Face Recognition', 'Generative Adversarial Networks', 'Face Images', 'Pitch Angle', 'Yaw Angle', 'Face Database', 'Mobility Scenarios', 'High-quality Database', 'Training Set', 'Quantitative Evaluation', 'Acquisition System', 'Recognition Accuracy', 'Facial Features', 'Image Scale', 'Sunglasses', 'Frontal Images', 'Pitch Variation', 'Popular Databases', 'Total Variation Regularization', 'Typical Glass', 'Front Face', 'Photo-realistic Images', 'Gallery Images', 'Extreme Angles']",,26,"Facial images in surveillance or mobile scenarios often have large view-point variations in terms of pitch and yaw angles. These jointly occurred angle variations make face recognition challenging. Current public face databases mainly consider the case of yaw variations. In this paper, a new large-scale Multi-yaw Multi-pitch high-quality database is proposed for Facial Pose Analysis (M2FPA), including face frontalization, face rotation, facial pose estimation and pose-invariant face recognition. It contains 397,544 images of 229 subjects with yaw, pitch, attribute, illumination and accessory. M2FPA is the most comprehensive multi-view face database for facial pose analysis. Further, we provide an effective benchmark for face frontalization and pose-invariant face recognition on M2FPA with several state-of-the-art methods, including DR-GAN, TP-GAN and CAPG-GAN. We believe that the new database and benchmark can significantly push forward the advance of facial pose analysis in real-world applications. Moreover, a simple yet effective parsing guided discriminator is introduced to capture the local consistency during GAN optimization. Extensive quantitative and qualitative results on M2FPA and Multi-PIE demonstrate the superiority of our face frontalization method. Baseline results for both face synthesis and face recognition from state-of-the-art methods demonstrate the challenge offered by this new database."
M3D-RPN: Monocular 3D Region Proposal Network for Object Detection,"Garrick Brazil, Xiaoming Liu","Michigan State University, East Lansing MI",100.0,usa,0.0,,"Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Brazil_M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Brazil_M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010867/,"['Three-dimensional displays', 'Two dimensional displays', 'Proposals', 'Cameras', 'Object detection', 'Convolution', 'Laser radar']","['Object Detection', 'Region Proposal Network', 'Object Detection Network', '3D Region Proposal Network', 'External Data', 'Birdâ€™s Eye', 'External Networks', '3D Detection', '3D Object Detection', 'Pedestrian', 'Intersection Over Union', 'Point Cloud', 'Bounding Box', 'High-level Features', 'Single Network', 'Ground Plane', 'Viewing Angle', 'Projection Matrix', '3D Position', '2D Space', 'Camera Coordinate System', 'Camera Coordinate', 'Urban Scenes', 'Faster R-CNN', 'Hard Set', 'Strong Prior', 'LiDAR Point', '3D Depth', '3D Tasks']",,346,"Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model."
MIC: Mining Interclass Characteristics for Improved Metric Learning,"Karsten Roth, Biagio Brattoli, BjÃ¶rn Ommer","HCI/IWR, Heidelberg University, Germany",100.0,germany,0.0,,"Metric learning seeks to embed images of objects such that class-defined relations are captured by the embedding space. However, variability in images is not just due to different depicted object classes, but also depends on other latent characteristics such as viewpoint or illumination. In addition to these structured properties, random noise further obstructs the visual relations of interest. The common approach to metric learning is to enforce a representation that is invariant under all factors but the ones of interest. In contrast, we propose to explicitly learn the latent characteristics that are shared by and go across object classes. We can then directly explain away structured visual variability, rather than assuming it to be unknown random noise. We propose a novel surrogate task to learn visual characteristics shared across classes with a separate encoder. This encoder is trained jointly with the encoder for class information by reducing their mutual information. On five standard image retrieval benchmarks the approach significantly improves upon the state-of-the-art. Code is available at https://github.com/Confusezius/metric-learning-mining-interclass-characteristics.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf,,https://github.com/Confusezius/metric-learning-mining-interclass-characteristics,,main,Poster,https://ieeexplore.ieee.org/document/9010968/,"['Measurement', 'Task analysis', 'Training', 'Feature extraction', 'Standards', 'Automobiles', 'Image color analysis']","['Metric Learning', 'Mutual Information', 'Object Classification', 'Latent Space', 'Image Retrieval', 'Latent Features', 'Interesting Ones', 'Standardised', 'Neural Network', 'Deep Learning', 'High-dimensional', 'Image Features', 'Information Exchange', 'Discriminative Features', 'Image Representation', 'Latent Structure', 'Communitarian', 'Standard Setup', 'Intra-class Variance', 'Triplet Loss', 'Deep Metric Learning', 'Zero-shot', 'Marginal Loss', 'Learning Loss']",,31,"Metric learning seeks to embed images of objects such that class-defined relations are captured by the embedding space. However, variability in images is not just due to different depicted object classes, but also depends on other latent characteristics such as viewpoint or illumination. In addition to these structured properties, random noise further obstructs the visual relations of interest. The common approach to metric learning is to enforce a representation that is invariant under all factors but the ones of interest. In contrast, we propose to explicitly learn the latent characteristics that are shared by and go across object classes. We can then directly explain away structured visual variability, rather than assuming it to be unknown random noise. We propose a novel surrogate task to learn visual characteristics shared across classes with a separate encoder. This encoder is trained jointly with the encoder for class information by reducing their mutual information. On five standard image retrieval benchmarks the approach significantly improves upon the state-of-the-art. Code is available at https://github.com/Confusezius/metric-learning-mining-interclass-characteristics."
MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding,"Quan Kong, Ziming Wu, Ziwei Deng, Martin Klinkigt, Bin Tong, Tomokazu Murakami","Hitachi, Ltd. R&D Group, Japan; Hong Kong University of Science and Technology; Hitachi, Ltd. R&D Group, Japan and Alibaba Group, China",33.33333333333333,Hong Kong,66.66666666666667,Japan,"Unlike vision modalities, body-worn sensors or passive sensing can avoid the failure of action understanding in vision related challenges, e.g. occlusion and appearance variation. However, a standard large-scale dataset does not exist, in which different types of modalities across vision and sensors are integrated. To address the disadvantage of vision-based modalities and push towards multi/cross modal action understanding, this paper introduces a new large-scale dataset recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. On the basis of our dataset, we propose a novel multi modality distillation model with attention mechanism to realize an adaptive knowledge transfer from sensor-based modalities to vision-based modalities. The proposed model significantly improves performance of action recognition compared to models trained with only RGB information. The experimental results confirm the effectiveness of our model on cross-subject, -view, -scene and -session evaluation criteria. We believe that this new large-scale multimodal dataset will contribute the community of multimodal based action understanding.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009579/,"['Videos', 'Three-dimensional displays', 'Sensors', 'Gyroscopes', 'Adaptation models', 'Cameras', 'Task analysis']","['Human Activities', 'Large-scale Datasets', 'Understanding Of Activity', 'Understanding Of Human Action', 'Significantly Improved', 'Knowledge Transfer', 'Attention Mechanism', 'Types Of Modes', 'Action Recognition', 'Visual Modality', 'Action Classes', 'Pressure Signals', 'Appearance Variations', 'Acceleration Signal', 'Distinct Scenarios', 'Multimodal Dataset', 'WiFi Signals', 'Multimodal Activity', 'RGB Video', 'RGB Information', 'Student Network', 'Teacher Model', 'Human Activity Recognition', 'Action Detection', 'Sensor Modalities', 'Random Walk', 'Teacher Network', 'Inertial Measurement Unit', 'Student Model']",,53,"Unlike vision modalities, body-worn sensors or passive sensing can avoid the failure of action understanding in vision related challenges, e.g. occlusion and appearance variation. However, a standard large-scale dataset does not exist, in which different types of modalities across vision and sensors are integrated. To address the disadvantage of vision-based modalities and push towards multi/cross modal action understanding, this paper introduces a new large-scale dataset recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. On the basis of our dataset, we propose a novel multi modality distillation model with attention mechanism to realize an adaptive knowledge transfer from sensor-based modalities to vision-based modalities. The proposed model significantly improves performance of action recognition compared to models trained with only RGB information. The experimental results confirm the effectiveness of our model on cross-subject, -view, -scene and -session evaluation criteria. We believe that this new large-scale multimodal dataset will contribute the community of multimodal based action understanding."
MONET: Multiview Semi-Supervised Keypoint Detection via Epipolar Divergence,"Yuan Yao, Yasamin Jafarian, Hyun Soo Park",University of Minnesota,100.0,usa,0.0,,"This paper presents MONET---an end-to-end semi-supervised learning framework for a keypoint detector using multiview image streams. In particular, we consider general subjects such as non-human species where attaining a large scale annotated dataset is challenging. While multiview geometry can be used to self-supervise the unlabeled data, integrating the geometry into learning a keypoint detector is challenging due to representation mismatch. We address this mismatch by formulating a new differentiable representation of the epipolar constraint called epipolar divergence---a generalized distance from the epipolar lines to the corresponding keypoint distribution. Epipolar divergence characterizes when two view keypoint distributions produce zero reprojection error. We design a twin network that minimizes the epipolar divergence through stereo rectification that can significantly alleviate computational complexity and sampling aliasing in training. We demonstrate that our framework can localize customized keypoints of diverse species, e.g., humans, dogs, and monkeys.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yao_MONET_Multiview_Semi-Supervised_Keypoint_Detection_via_Epipolar_Divergence_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_MONET_Multiview_Semi-Supervised_Keypoint_Detection_via_Epipolar_Divergence_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009991/,"['Three-dimensional displays', 'Geometry', 'Detectors', 'Cameras', 'Two dimensional displays', 'Training', 'Image reconstruction']","['Keypoint Detection', 'Learning Framework', 'Unlabeled Data', 'Semi-supervised Learning', 'Multi-view Images', 'Reprojection Error', 'Semi-supervised Learning Framework', 'Human Subjects', 'Convolutional Neural Network', '3D Reconstruction', 'Parametrized', 'Point Cloud', 'Bounding Box', 'Manual Annotation', 'Optical Flow', 'Image Point', '3D Point', 'Distribution Of Images', 'Pose Estimation', 'Intrinsic Parameters', 'Geometric Consistency', '3D Prediction', 'Fundamental Matrix', 'Sampling Artifacts', 'Multi-camera System', 'Homography', 'Camera Pose', 'Accuracy And Precision', 'Motion Capture', 'HD Camera']",,28,"This paper presents MONET-an end-to-end semi-supervised learning frameworkfor a keypoint detector using multiview image streams. In particular, we consider general subjects such as non-human species where attaining a large scale annotated dataset is challenging. While multiview geometry can be used to self-supervise the unlabeled data, integrating the geometry into learning a keypoint detector is challenging due to representation mismatch. We address this mismatch by formulating a new differentiable representation of the epipolar constraint called epipolar divergence-a generalized distance from the epipolar lines to the corresponding keypoint distribution. Epipolar divergence characterizes when two view keypoint distributions produce zero reprojection error. We design a twin network that minimizes the epipolar divergence through stereo rectification that can significantly alleviate computational complexity and sampling aliasing in training. We demonstrate that our framework can localize customized keypoints of diverse species, e.g., humans, dogs, and monkeys."
"MVP Matching: A Maximum-Value Perfect Matching for Mining Hard Samples, With Application to Person Re-Identification","Han Sun, Zhiyuan Chen, Shiyang Yan, Lin Xu","Nanjing Institute of Advanced Artificial Intelligence, Horizon Robotics; Queen’s University Belfast",100.0,"china, uk",0.0,,"How to correctly stress hard samples in metric learning is critical for visual recognition tasks, especially in challenging person re-ID applications. Pedestrians across cameras with significant appearance variations are easily confused, which could bias the learned metric and slow down the convergence rate. In this paper, we propose a novel weighted complete bipartite graph based maximum-value perfect (MVP) matching for mining the hard samples from a batch of samples. It can emphasize the hard positive and negative sample pairs respectively, and thus relieve adverse optimization and sample imbalance problems. We then develop a new batch-wise MVP matching based loss objective and combine it in an end-to-end deep metric learning manner. It leads to significant improvements in both convergence rate and recognition performance. Extensive empirical results on five person re-ID benchmark datasets, i.e., Market-1501, CUHK03-Detected, CUHK03-Labeled, Duke-MTMC, and MSMT17, demonstrate the superiority of the proposed method. It can accelerate the convergence rate significantly while achieving state-of-the-art performance. The source code of our method is available at https://github.com/IAAI-CVResearchGroup/MVP-metric.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.pdf,,https://github.com/IAAI-CVResearchGroup/MVP-metric,,main,Poster,https://ieeexplore.ieee.org/document/9008786/,"['Measurement', 'Bipartite graph', 'Convergence', 'Optimization', 'Feature extraction', 'Visualization', 'Benchmark testing']","['Perfect Match', 'Deep Learning', 'Paired Samples', 'Positive Samples', 'Convergence Rate', 'Negative Samples', 'Benchmark Datasets', 'Batch Of Samples', 'Recognition Performance', 'Metric Learning', 'Appearance Variations', 'Complete Bipartite Graph', 'Deep Metric Learning', 'Improve Recognition Performance', 'Learning Rate', 'Sampling Weights', 'Small Distance', 'Similar Samples', 'Surgical Margins', 'Edge Weights', 'Triplet Loss', 'Metric Learning Methods', 'Assignment Problem', 'Contrastive Loss', 'Top-5 Accuracy', 'Human Pose', 'Deep Embedding', 'Hard Loss', 'Hinge Loss', 'Optimal Transport']",,28,"How to correctly stress hard samples in metric learning is critical for visual recognition tasks, especially in challenging person re-ID applications. Pedestrians across cameras with significant appearance variations are easily confused, which could bias the learned metric and slow down the convergence rate. In this paper, we propose a novel weighted complete bipartite graph based maximum-value perfect (MVP) matching for mining the hard samples from a batch of samples. It can emphasize the hard positive and negative sample pairs respectively, and thus relieve adverse optimization and sample imbalance problems. We then develop a new batch-wise MVP matching based loss objective and combine it in an end-to-end deep metric learning manner. It leads to significant improvements in both convergence rate and recognition performance. Extensive empirical results on five person re-ID benchmark datasets, i.e., Market-1501, CUHK03-Detected, CUHK03-Labeled, Duke-MTMC, and MSMT17, demonstrate the superiority of the proposed method. It can accelerate the convergence rate significantly while achieving state-of-the-art performance. The source code of our method is available at https: //github.com/IAAI-CVResearchGroup/MVP-metric."
MVSCRF: Learning Multi-View Stereo With Conditional Random Fields,"Youze Xue, Jiansheng Chen, Weitao Wan, Yiqing Huang, Cheng Yu, Tianpeng Li, Jiayu Bao","Department of Electronic Engineering, Tsinghua University",100.0,China,0.0,,"We present a deep-learning architecture for multi-view stereo with conditional random fields (MVSCRF). Given an arbitrary number of input images, we first use a U-shape neural network to extract deep features incorporating both global and local information, and then build a 3D cost volume for the reference camera. Unlike previous learning based methods, we explicitly constraint the smoothness of depth maps by using conditional random fields (CRFs) after the stage of cost volume regularization. The CRFs module is implemented as recurrent neural networks so that the whole pipeline can be trained end-to-end. Our results show that the proposed pipeline outperforms previous state-of-the-arts on large-scale DTU dataset. We also achieve comparable results with state-of-the-art learning based methods on outdoor Tanks and Temples dataset without fine-tuning, which demonstrates our method's generalization ability.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xue_MVSCRF_Learning_Multi-View_Stereo_With_Conditional_Random_Fields_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_MVSCRF_Learning_Multi-View_Stereo_With_Conditional_Random_Fields_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008564/,"['Feature extraction', 'Three-dimensional displays', 'Pipelines', 'Task analysis', 'Estimation', 'Image reconstruction', 'Semantics']","['Conditional Random Field', 'Multi-view Stereo', 'Neural Network', 'Deep Learning', 'Local Information', 'Input Image', 'Recurrent Network', 'Recurrent Neural Network', 'Global Information', 'Learning-based Methods', 'Deep Features', 'Depth Map', 'Deep Feature Extraction', 'Cost Volume', 'Convolutional Neural Network', 'Image Features', 'Convolutional Layers', 'Local Features', 'Feature Maps', 'Point Cloud', 'Public Benchmark', 'Smoothness Constraint', 'Depth Estimation', 'Explicit Constraints', 'Image Pairs', 'Reference Image', 'Source Images', 'U-shaped Structure', 'Skip Connections', '3D U-Net']",,53,"We present a deep-learning architecture for multi-view stereo with conditional random fields (MVSCRF). Given an arbitrary number of input images, we first use a U-shape neural network to extract deep features incorporating both global and local information, and then build a 3D cost volume for the reference camera. Unlike previous learning based methods, we explicitly constraint the smoothness of depth maps by using conditional random fields (CRFs) after the stage of cost volume regularization. The CRFs module is implemented as recurrent neural networks so that the whole pipeline can be trained end-to-end. Our results show that the proposed pipeline outperforms previous state-of-the-arts on large-scale DTU dataset. We also achieve comparable results with state-of-the-art learning based methods on outdoor Tanks and Temples dataset without fine-tuning, which demonstrates our method's generalization ability."
Make a Face: Towards Arbitrary High Fidelity Face Manipulation,"Shengju Qian, Kwan-Yee Lin, Wayne Wu, Yangxiaokang Liu, Quan Wang, Fumin Shen, Chen Qian, Ran He","The Chinese University of Hong Kong; Tsinghua University; University of Electronic Science and Technology of China; NLPR, CASIA; SenseTime Research",80.0,"China, Hong Kong, china",20.0,China,"Recent studies have shown remarkable success in face manipulation task with the advance of GANs and VAEs paradigms, but the outputs are sometimes limited to low-resolution and lack of diversity. In this work, we propose Additive Focal Variational Auto-encoder (AF-VAE), a novel approach that can arbitrarily manipulate high-resolution face images using a simple yet effective model and only weak supervision of reconstruction and KL divergence losses. First, a novel additive Gaussian Mixture assumption is introduced with an unsupervised clustering mechanism in the structural latent space, which endows better disentanglement and boosts multi-modal representation with external memory. Second, to improve the perceptual quality of synthesized results, two simple strategies in architecture design are further tailored and discussed on the behavior of Human Visual System (HVS) for the first time, allowing for fine control over the model complexity and sample quality. Human opinion studies and new state-of-the-art Inception Score (IS) / Frechet Inception Distance (FID) demonstrate the superiority of our approach over existing algorithms, advancing both the fidelity and extremity of face manipulation task.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qian_Make_a_Face_Towards_Arbitrary_High_Fidelity_Face_Manipulation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_Make_a_Face_Towards_Arbitrary_High_Fidelity_Face_Manipulation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008287/,"['Face', 'Task analysis', 'Gallium nitride', 'Training', 'Image resolution', 'Additives', 'Generative adversarial networks']","['Use Of Face', 'High-resolution Images', 'Kullback-Leibler', 'Generative Adversarial Networks', 'Perception Of Quality', 'Latent Space', 'Structure Of Space', 'Face Images', 'Variational Autoencoder', 'Human Visual System', 'Reconstruction Loss', 'External Memory', 'Inception Distance', 'Fr√©chet Inception Distance', 'Divergence Loss', 'Body Mass Index', 'Focal Point', 'Quantitative Evaluation', 'Facial Expressions', 'Training Procedure', 'Latent Representation', 'Source Images', 'Conditional Variational Autoencoder', 'Facial Structure', 'Facial Appearance', 'Boundary Map', 'Face Dataset', 'Action Units', 'Head Pose']",,37,"Recent studies have shown remarkable success in face manipulation task with the advance of GANs and VAEs paradigms, but the outputs are sometimes limited to low-resolution and lack of diversity. In this work, we propose Additive Focal Variational Auto-encoder (AF-VAE), a novel approach that can arbitrarily manipulate high-resolution face images using a simple yet effective model and only weak supervision of reconstruction and KL divergence losses. First, a novel additive Gaussian Mixture assumption is introduced with an unsupervised clustering mechanism in the structural latent space, which endows better disentanglement and boosts multi-modal representation with external memory. Second, to improve the perceptual quality of synthesized results, two simple strategies in architecture design are further tailored and discussed on the behavior of Human Visual System (HVS) for the first time, allowing for fine control over the model complexity and sample quality. Human opinion studies and new state-of-the-art Inception Score (IS) / Frechet Inception Distance (FID) demonstrate the superiority of our approach over existing algorithms, advancing both the fidelity and extremity of face manipulation task."
Making History Matter: History-Advantage Sequence Training for Visual Dialog,"Tianhao Yang, Zheng-Jun Zha, Hanwang Zhang",University of Science and Technology of China; Nanyang Technological University,100.0,"Singapore, china",0.0,,"We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage -- a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Making_History_Matter_History-Advantage_Sequence_Training_for_Visual_Dialog_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Making_History_Matter_History-Advantage_Sequence_Training_for_Visual_Dialog_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009518/,"['History', 'Visualization', 'Training', 'Codecs', 'Gold', 'Task analysis', 'Decoding']","['Visual Dialog', 'Policy Gradient', 'Impact Of History', 'Dialogic Nature', 'Contextual Reasons', 'Image Features', 'Attention Mechanism', 'Visual Task', 'Visual Attention', 'Attention Module', 'Historical Changes', 'Hidden State', 'Language Model', 'Ablation Experiments', 'Word Embedding', 'Concatenation Operation', 'Historical Features', 'Self-attention Mechanism', 'Image Captioning', 'Visual Question Answering', 'Bottom-up Attention', 'Irrelevant Ones', 'Visual Context', 'Visual Resolution']",,32,"We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage - a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts."
Making the Invisible Visible: Action Recognition Through Walls and Occlusions,"Tianhong Li, Lijie Fan, Mingmin Zhao, Yingcheng Liu, Dina Katabi",MIT CSAIL,100.0,usa,0.0,,"Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community. But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Making_the_Invisible_Visible_Action_Recognition_Through_Walls_and_Occlusions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Making_the_Invisible_Visible_Action_Recognition_Through_Walls_and_Occlusions_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009025/,"['Skeleton', 'Three-dimensional displays', 'Wireless communication', 'Radio frequency', 'Visualization', 'Feature extraction', 'Neural networks']","['Action Recognition', 'Neural Network', 'Human Activities', 'Artificial Neural Network', 'Data Visualization', 'Computer Vision', 'Human Bone', 'Radio Frequency Signal', 'Multiple People', 'Intermediate Representation', 'Interaction Of People', 'Vision-based System', '3D Skeleton', 'Poor Lighting Conditions', 'Convolutional Neural Network', 'Recurrent Neural Network', 'Single Activity', 'Attention Module', 'Camera System', 'Radio Waves', 'Action Detection', 'Radio Frequency Data', 'Action Recognition Datasets', 'Latent Features', 'Spatiotemporal Characteristics', 'Multimodal Training', 'Action Recognition Model', 'Graph Neural Networks', 'Mean Average Precision', 'Spatiotemporal Modulation']",,86,"Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community. But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition."
Many Task Learning With Task Routing,"Gjorgji Strezoski, Nanne van Noord, Marcel Worring",University of Amsterdam,100.0,Netherlands,0.0,,"Typical multi-task learning (MTL) methods rely on architectural adjustments and a large trainable parameter set to jointly optimize over several tasks. However, when the number of tasks increases so do the complexity of the architectural adjustments and resource requirements. In this paper, we introduce a method which applies a conditional feature-wise transformation over the convolutional activations that enables a model to successfully perform a large number of tasks. To distinguish from regular MTL, we introduce Many Task Learning (MaTL) as a special case of MTL where more than 20 tasks are performed by a single model. Our method dubbed Task Routing (TR) is encapsulated in a layer we call the Task Routing Layer (TRL), which applied in an MaTL scenario successfully fits hundreds of classification tasks in one model. We evaluate on 5 datasets and the Visual Decathlon (VD) challenge against strong baselines and state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Strezoski_Many_Task_Learning_With_Task_Routing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Strezoski_Many_Task_Learning_With_Task_Routing_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010933/,"['Task analysis', 'Routing', 'Training', 'Computational modeling', 'Birds', 'Robustness', 'Adaptation models']","['Learning Task', 'Classification Task', 'Multi-task Learning', 'Strong Baseline', 'Multi-task Learning Method', 'Training Set', 'Scalable', 'Learning Process', 'Convolutional Layers', 'Parameter Space', 'Evolutionary Algorithms', 'Precision And Recall', 'Alternative Route', 'Latent Space', 'Multiple Tasks', 'Active Task', 'Additional Tasks', 'Convolutional Block', 'Set Of Classifiers', 'Binary Classification Task', 'Multi-task Learning Model', 'Regular Form', 'Fashion-MNIST', 'Classification Branch', 'Generalization Performance Of The Model', 'Seagull', 'Transfer Learning']",,42,"Typical multi-task learning (MTL) methods rely on architectural adjustments and a large trainable parameter set to jointly optimize over several tasks. However, when the number of tasks increases so do the complexity of the architectural adjustments and resource requirements. In this paper, we introduce a method which applies a conditional feature-wise transformation over the convolutional activations that enables a model to successfully perform a large number of tasks. To distinguish from regular MTL, we introduce Many Task Learning (MaTL) as a special case of MTL where more than 20 tasks are performed by a single model. Our method dubbed Task Routing (TR) is encapsulated in a layer we call the Task Routing Layer (TRL), which applied in an MaTL scenario successfully fits hundreds of classification tasks in one model. We evaluate on 5 datasets and the Visual Decathlon (VD) challenge against strong baselines and state-of-the-art approaches."
Mask-Guided Attention Network for Occluded Pedestrian Detection,"Yanwei Pang, Jin Xie, Muhammad Haris Khan, Rao Muhammad Anwer, Fahad Shahbaz Khan, Ling Shao","Tianjin University; Inception Institute of Artiﬁcial Intelligence, UAE & CVL, Linköping University, Sweden; Inception Institute of Artiﬁcial Intelligence, UAE",100.0,"Sweden, china, sweden, uae",0.0,,"Pedestrian detection relying on deep convolution neural networks has made significant progress. Though promising results have been achieved on standard pedestrians, the performance on heavily occluded pedestrians remains far from satisfactory. The main culprits are intra-class occlusions involving other pedestrians and inter-class occlusions caused by other objects, such as cars and bicycles. These results in a multitude of occlusion patterns. We propose an approach for occluded pedestrian detection with the following contributions. First, we introduce a novel mask-guided attention network that fits naturally into popular pedestrian detection pipelines. Our attention network emphasizes on visible pedestrian regions while suppressing the occluded ones by modulating full body features. Second, we empirically demonstrate that coarse-level segmentation annotations provide reasonable approximation to their dense pixel-wise counterparts. Experiments are performed on CityPersons and Caltech datasets. Our approach sets a new state-of-the-art on both datasets. Our approach obtains an absolute gain of 9.5% in log-average miss rate, compared to the best reported results [32] on the heavily occluded HO pedestrian set of CityPersons test set. Further, on the HO pedestrian set of Caltech dataset, our method achieves an absolute gain of 5.0% in log-average miss rate, compared to the best reported results [13]. Code and models are available at: https://github.com/Leotju/MGAN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.pdf,,https://github.com/Leotju/MGAN,,main,Poster,https://ieeexplore.ieee.org/document/9008120/,"['Standards', 'Detectors', 'Proposals', 'Feature extraction', 'Pipelines', 'Benchmark testing', 'Computer architecture']","['Attention Network', 'Pedestrian Detection', 'Occluded Pedestrians', 'Mask-guided Attention', 'Visible Light', 'Convolutional Neural Network', 'Deep Convolutional Neural Network', 'Missing Rate', 'Occlusal Pattern', 'Computer Vision', 'Visual Information', 'Bounding Box', 'Spatial Attention', 'Classification Loss', 'Loss Term', 'Faster R-CNN', 'Standard Benchmark', 'Visible Part', 'Two-stage Strategy', 'Region Proposal Network', 'Feature Module', 'Input Scale', 'Level Of Supervision', 'Proposal Generation', 'Attentional Strategies', 'Labeling Density']",,138,"Pedestrian detection relying on deep convolution neural networks has made significant progress. Though promising results have been achieved on standard pedestrians, the performance on heavily occluded pedestrians remains far from satisfactory. The main culprits are intra-class occlusions involving other pedestrians and inter-class occlusions caused by other objects, such as cars and bicycles. These results in a multitude of occlusion patterns. We propose an approach for occluded pedestrian detection with the following contributions. First, we introduce a novel mask-guided attention network that fits naturally into popular pedestrian detection pipelines. Our attention network emphasizes on visible pedestrian regions while suppressing the occluded ones by modulating full body features. Second, we empirically demonstrate that coarse-level segmentation annotations provide reasonable approximation to their dense pixel-wise counterparts. Experiments are performed on CityPersons and Caltech datasets. Our approach sets a new state-of-the-art on both datasets. Our approach obtains an absolute gain of 9.5% in log-average miss rate, compared to the best reported results [32] on the heavily occluded HO pedestrian set of CityPersons test set. Further, on the HO pedestrian set of Caltech dataset, our method achieves an absolute gain of 5.0% in log-average miss rate, compared to the best reported results [13]. Code and models are available at: https://github.com/Leotju/MGAN."
Mask-ShadowGAN: Learning to Remove Shadows From Unpaired Data,"Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, Pheng-Ann Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong; Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China",100.0,"Hong Kong, china",0.0,,"This paper presents a new method for shadow removal using unpaired data, enabling us to avoid tedious annotations and obtain more diverse training samples. However, directly employing adversarial learning and cycle-consistency constraints is insufficient to learn the underlying relationship between the shadow and shadow-free domains, since the mapping between shadow and shadow-free images is not simply one-to-one. To address the problem, we formulate Mask-ShadowGAN, a new deep framework that automatically learns to produce a shadow mask from the input shadow image and then takes the mask to guide the shadow generation via re-formulated cycle-consistency constraints. Particularly, the framework simultaneously learns to produce shadow masks and learns to remove shadows, to maximize the overall performance. Also, we prepared an unpaired dataset for shadow removal and demonstrated the effectiveness of Mask-ShadowGAN on various experiments, even it was trained on unpaired data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010942/,"['Generators', 'Training', 'Manganese', 'Training data', 'Image color analysis', 'Shape', 'Cameras']","['Generative Adversarial Networks', 'Shadow Images', 'Training Set', 'Convolutional Neural Network', 'Deep Network', 'Deep Neural Network', 'Input Image', 'Network Training', 'Paired Data', 'Convolution Operation', 'Image Pairs', 'Luminosity', 'Loss Of Identity', 'Realistic Images', 'Training Pairs', 'Conditional Generative Adversarial Network', 'Variety Of Scenes', 'Camera Exposure', 'Public Code', 'Shadow Regions', 'Cycle Consistency', 'Cycle Consistency Loss', 'Output Image', 'Root Mean Square Error']",,132,"This paper presents a new method for shadow removal using unpaired data, enabling us to avoid tedious annotations and obtain more diverse training samples. However, directly employing adversarial learning and cycle-consistency constraints is insufficient to learn the underlying relationship between the shadow and shadow-free domains, since the mapping between shadow and shadow-free images is not simply one-to-one. To address the problem, we formulate Mask-ShadowGAN, a new deep framework that automatically learns to produce a shadow mask from the input shadow image and then takes the mask to guide the shadow generation via re-formulated cycle-consistency constraints. Particularly, the framework simultaneously learns to produce shadow masks and learns to remove shadows, to maximize the overall performance. Also, we prepared an unpaired dataset for shadow removal and demonstrated the effectiveness of Mask-ShadowGAN on various experiments, even it was trained on unpaired data."
Maximum-Margin Hamming Hashing,"Rong Kang, Yue Cao, Mingsheng Long, Jianmin Wang, Philip S. Yu","Research Center for Big Data, Tsinghua University; National Engineering Laboratory for Big Data Software; School of Software, BNRist, Tsinghua University",100.0,China,0.0,,"Deep hashing enables computation and memory efficient image search through end-to-end learning of feature representations and binary codes. While linear scan over binary hash codes is more efficient than over the high-dimensional representations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for each query, there is a Hamming ball centered at the query and the data points within the ball are returned as relevant. Since inside the Hamming ball implies retrievable while outside irretrievable, it is crucial to explicitly characterize the Hamming ball. The main idea of this work is to directly embody the Hamming radius into the loss functions, leading to Maximum-Margin Hamming Hashing (MMHH), a new model specifically optimized for Hamming space retrieval. We introduce a max-margin t-distribution loss, where the t-distribution concentrates more similar data points to be within the Hamming ball, and the margin characterizes the Hamming radius such that less penalization is applied to similar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems. The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data. Our method yields state-of-the-art results on four datasets and shows superior performance on noisy data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Maximum-Margin_Hamming_Hashing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kang_Maximum-Margin_Hamming_Hashing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010904/,"['Binary codes', 'Robustness', 'Hamming distance', 'Databases', 'Data models', 'Quantization (signal)', 'Semantics']","['Loss Function', 'Noisy Data', 'Hash Function', 'Binary Code', 'Image Retrieval', 'Linear Scan', 'Semantic', 'Training Set', 'Convolutional Neural Network', 'Paired Data', 'ImageNet', 'Training Images', 'Image Pairs', 'Unsupervised Methods', 'Average Precision', 'Hyperbolic Tangent', 'Similar Information', 'Quantization Error', 'Stochastic Neighbor Embedding', 'Hamming Distance', 'Locality Sensitive Hashing', 'Retrieval Performance', 'Query Set', 'Noisy Labels', 'Mean Average Precision', 'Validation Set', 'Validation Images', 'Cauchy Distribution', 'Lookup Table']",,20,"Deep hashing enables computation and memory efficient image search through end-to-end learning of feature representations and binary codes. While linear scan over binary hash codes is more efficient than over the high-dimensional representations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for each query, there is a Hamming ball centered at the query and the data points within the ball are returned as relevant. Since inside the Hamming ball implies retrievable while outside irretrievable, it is crucial to explicitly characterize the Hamming ball. The main idea of this work is to directly embody the Hamming radius into the loss functions, leading to Maximum-Margin Hamming Hashing (MMHH), a new model specifically optimized for Hamming space retrieval. We introduce a max-margin t-distribution loss, where the t-distribution concentrates more similar data points to be within the Hamming ball, and the margin characterizes the Hamming radius such that less penalization is applied to similar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems. The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data. Our method yields state-of-the-art results on four datasets and shows superior performance on noisy data."
Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection,"Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, Anton van den Hengel","A2I2, Deakin University; University of Western Australia; The University of Adelaide, Australia",100.0,"Australia, australia",0.0,,"Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder ""generalizes"" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gong_Memorizing_Normality_to_Detect_Anomaly_Memory-Augmented_Deep_Autoencoder_for_Unsupervised_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Memorizing_Normality_to_Detect_Anomaly_Memory-Augmented_Deep_Autoencoder_for_Unsupervised_ICCV_2019_paper.pdf,,https://donggong1.github.io/anomdec-memae,,main,Poster,https://ieeexplore.ieee.org/document/9010977/,"['Decoding', 'Encoding', 'Anomaly detection', 'Image reconstruction', 'Training', 'Memory modules', 'Micromechanical devices']","['Anomaly Detection', 'Deep Autoencoder', 'Unsupervised Anomaly Detection', 'Data Normalization', 'Normal Samples', 'Memory Items', 'Memory Content', 'Memory Module', 'Training Set', 'Decoding', 'Large Errors', 'Normal Pattern', 'Area Under Curve', 'Gaussian Mixture Model', 'Optical Flow', 'Entropy Loss', 'Sparse Representation', 'Similar Capacity', 'Variational Autoencoder', 'Video Surveillance', 'Prototypical Patterns', 'Memory Patterns', 'One-class Classification', 'Sparse Representation Method', 'Memory Size', '3D Convolution', 'Sparse Regularization', 'Abnormal Samples', 'Single Slot', 'Encoder-decoder']",,945,"Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder ""generalizes"" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE."
Memory-Based Neighbourhood Embedding for Visual Recognition,"Suichan Li, Dapeng Chen, Bin Liu, Nenghai Yu, Rui Zhao","School of Information Science and Technology, University of Science and Technology of China; SenseTime Research",50.0,china,50.0,China,"Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, i.e., how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Memory-Based_Neighbourhood_Embedding_for_Visual_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Memory-Based_Neighbourhood_Embedding_for_Visual_Recognition_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010371/,"['Training', 'Feature extraction', 'Neural networks', 'Tree graphs', 'Visualization', 'Task analysis', 'Image recognition']","['Neighborhood Embedding', 'Episodic Memory', 'Discriminative Features', 'Root Node', 'Image Retrieval', 'Search Task', 'Neighborhood Information', 'Weight Aggregates', 'Tree Graph', 'Bottom-up Manner', 'CNN Features', 'Neural Network', 'Deep Neural Network', 'K-nearest Neighbor', 'Cross-entropy Loss', 'Training Images', 'Nodes In The Graph', 'Abundant Information', 'Leaf Node', 'Few-shot Learning', 'CNN Backbone', 'Final Embedding', 'Feature Aggregation', 'Attention Weights', 'Graph Neural Networks', 'Query Image', 'Memory Updating', 'Tree Depth', 'Aggregation Scheme']",,34,"Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, i.e., how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches."
Mesh R-CNN,"Georgia Gkioxari, Jitendra Malik, Justin Johnson",Facebook AI Research (FAIR),0.0,,100.0,USA,"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.pdf,https://gkioxari.github.io/meshrcnn/,,,main,Poster,https://ieeexplore.ieee.org/document/9008508/,"['Three-dimensional displays', 'Shape', 'Two dimensional displays', 'Topology', 'Solid modeling', 'Feature extraction', 'Benchmark testing']","['Mesh R-CNN', 'Image Object', '3D Shape', 'Graph Convolution', 'Triangular Mesh', 'Real-world Images', 'Mask R-CNN', '3D Prediction', 'Joint Detection', 'Synthetic Benchmark', 'Feature Maps', 'Single Image', 'Input Features', 'Point Cloud', 'ImageNet', 'Bounding Box', '3D Mesh', 'Appendix For Details', 'Mesh Network', 'Instance Segmentation', 'Vertex Position', 'Mesh Refinement', 'Chamfer Distance', 'Camera Coordinate System', 'Learnable Weight Matrix', 'Tesla V100 GPU', 'Shape Priors', 'Quantitative Performance', '3D Input', 'Shape Reconstruction']",,315,"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes."
Meta R-CNN: Towards General Solver for Instance-Level Low-Shot Learning,"Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, Liang Lin","Sun Yat-sen University; Sun Yat-sen University, DarkMatter AI Research",100.0,China,0.0,,"Resembling the rapid learning capability of human, low-shot learning empowers vision systems to understand new concepts by training with few samples. Leading approaches derived from meta-learning on images with a single visual object. Obfuscated by a complex background and multiple objects in one image, they are hard to promote the research of low-shot object detection/segmentation. In this work, we present a flexible and general methodology to achieve these tasks. Our work extends Faster /Mask R-CNN by proposing meta-learning over RoI (Region-of-Interest) features instead of a full image feature. This simple spirit disentangles multi-object information merged with the background, without bells and whistles, enabling Faster /Mask R-CNN turn into a meta-learner to achieve the tasks. Specifically, we introduce a Predictor-head Remodeling Network (PRN) that shares its main backbone with Faster /Mask R-CNN. PRN receives images containing low-shot objects with their bounding boxes or masks to infer their class attentive vectors. The vectors take channel-wise soft-attention on RoI features, remodeling those R-CNN predictor heads to detect or segment the objects consistent with the classes these vectors represent. In our experiments, Meta R-CNN yields the new state of the art in low-shot object detection and improves low-shot object segmentation by Mask R-CNN. Code: https://yanxp.github.io/metarcnn.html.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Meta_R-CNN_Towards_General_Solver_for_Instance-Level_Low-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Meta_R-CNN_Towards_General_Solver_for_Instance-Level_Low-Shot_Learning_ICCV_2019_paper.pdf,,https://yanxp.github.io/metarcnn.html,,main,Poster,https://ieeexplore.ieee.org/document/9011009/,"['Image segmentation', 'Feature extraction', 'Head', 'Proposals', 'Task analysis', 'Object detection', 'Object recognition']","['Low-shot Learning', 'Meta R-CNN', 'State Of The Art', 'Object Detection', 'Bounding Box', 'Single Object', 'Visual Object', 'Object Segmentation', 'Faster R-CNN', 'Main Backbone', 'Ablation', 'Training Set', 'Bayesian Model', 'Image Object', 'Classification Of Samples', 'Training Strategy', 'Object Classification', 'Base Classes', 'Instance Segmentation', 'Region Proposal', 'Mask R-CNN', 'Attention Vector', 'Region Proposal Network', 'Similar Learning', 'Input Object']",,277,"Resembling the rapid learning capability of human, low-shot learning empowers vision systems to understand new concepts by training with few samples. Leading approaches derived from meta-learning on images with a single visual object. Obfuscated by a complex background and multiple objects in one image, they are hard to promote the research of low-shot object detection/segmentation. In this work, we present aflexible and general methodology to achieve these tasks. Our work extends Faster /Mask R-CNN by proposing meta-learning over RoI (Region-of-Interest) features instead of a full image feature. This simple spirit disentangles multi-object information merged with the background, without bells and whistles, enabling Faster /Mask R-CNN turn into a meta-learner to achieve the tasks. Specifically, we introduce a Predictor-head Remodeling Network (PRN) that shares its main backbone with Faster /Mask R-CNN. PRN receives images containing low-shot objects with their bounding boxes or masks to infer their class attentive vectors. The vectors take channel-wise soft-attention on RoI features, remodeling those R-CNN predictor heads to detect or segment the objects consistent with the classes these vectors represent. In our experiments, Meta R-CNN yields the new state of the art in low-shot object detection and improves low-shot object segmentation byMaskR-CNN.Code: https://yanxp.github.io/metarcnn.html."
Meta-Learning to Detect Rare Objects,"Yu-Xiong Wang, Deva Ramanan, Martial Hebert","Robotics Institute, Carnegie Mellon University",100.0,usa,0.0,,"Few-shot learning, i.e., learning novel concepts from few examples, is fundamental to practical visual recognition systems. While most of existing work has focused on few-shot classification, we make a step towards few-shot object detection, a more challenging yet under-explored task. We develop a conceptually simple but powerful meta-learning based framework that simultaneously tackles few-shot classification and few-shot localization in a unified, coherent way. This framework leverages meta-level knowledge about ""model parameter generation"" from base classes with abundant data to facilitate the generation of a detector for novel classes. Our key insight is to disentangle the learning of category-agnostic and category-specific components in a CNN based detection model. In particular, we introduce a weight prediction meta-model that enables predicting the parameters of category-specific components from few examples. We systematically benchmark the performance of modern detectors in the small-sample size regime. Experiments in a variety of realistic scenarios, including within-domain, cross-domain, and long-tailed settings, demonstrate the effectiveness and generality of our approach under different notions of novel classes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009847/,"['Detectors', 'Task analysis', 'Object detection', 'Proposals', 'Visualization', 'Benchmark testing', 'Training']","['Benchmark', 'Convolutional Neural Network', 'Variety Of Settings', 'Detection Performance', 'Object Detection', 'Detection Model', 'Variety Of Scenarios', 'Visual Recognition', 'Base Classes', 'Component Parameters', 'Few-shot Learning', 'Coherent Way', 'Few-shot Classification', 'ImageNet', 'Deep Convolutional Neural Network', 'Bounding Box', 'Detection Task', 'Set Of Classes', 'Faster R-CNN', 'Validation Images', 'PASCAL VOC Dataset', 'Region Proposal Network', 'Fully-connected Network', 'Basic Detection', 'Weight Type', 'Bounding Box Annotations', 'iNaturalist', 'Attempt In This Direction', 'Metric Learning', 'Leaky ReLU']",,207,"Few-shot learning, i.e., learning novel concepts from few examples, is fundamental to practical visual recognition systems. While most of existing work has focused on few-shot classification, we make a step towards few-shot object detection, a more challenging yet under-explored task. We develop a conceptually simple but powerful meta-learning based framework that simultaneously tackles few-shot classification and few-shot localization in a unified, coherent way. This framework leverages meta-level knowledge about ""model parameter generation"" from base classes with abundant data to facilitate the generation of a detector for novel classes. Our key insight is to disentangle the learning of category-agnostic and category-specific components in a CNN based detection model. In particular, we introduce a weight prediction meta-model that enables predicting the parameters of category-specific components from few examples. We systematically benchmark the performance of modern detectors in the small-sample size regime. Experiments in a variety of realistic scenarios, including within-domain, cross-domain, and long-tailed settings, demonstrate the effectiveness and generality of our approach under different notions of novel classes."
Meta-Sim: Learning to Generate Synthetic Datasets,"Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler","MIT; NVIDIA, University of Toronto, Vector Institute; NVIDIA",66.66666666666666,"Canada, usa",33.33333333333334,USA,"Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kar_Meta-Sim_Learning_to_Generate_Synthetic_Datasets_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kar_Meta-Sim_Learning_to_Generate_Synthetic_Datasets_ICCV_2019_paper.pdf,,http://nv-tlabs.github.io/meta-sim,,main,Oral,https://ieeexplore.ieee.org/document/9009021/,"['Task analysis', 'Grammar', 'Probabilistic logic', 'Graphics', 'Three-dimensional displays', 'Neural networks', 'Engines']","['Neural Network', 'Validation Set', 'Parametrized', 'Target Data', 'Image Synthesis', 'Distribution Gap', 'Scene Graph', 'Quantitative Results', 'Convolutional Layers', 'Qualitative Results', 'Object Detection', 'Pedestrian', 'Data Augmentation', 'Semantic Segmentation', 'Real Distribution', 'Domain Adaptation', 'Graph Convolutional Network', 'Target Distribution', 'Objects In The Scene', 'Target Dataset', 'Maximum Mean Discrepancy', 'Task Network', 'Scene Structure', 'Domain Gap', 'Distribution Matching', 'Titan Xp GPU', 'Subsequent Part', 'Input Distribution', 'Synthetic Generation', 'Robot Control']",,139,"Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task."
MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning,"Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, Jian Sun",Tsinghua University; Hong Kong University of Science and Technology; Megvii Technology; Huazhong University of Science and Technology,75.0,"China, Hong Kong, china",25.0,China,"In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github.com/liuzechun/MetaPruning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.pdf,,https://github.com/liuzechun/MetaPruning,,main,Poster,https://ieeexplore.ieee.org/document/9010041/,"['Encoding', 'Training', 'Neural networks', 'Tuning', 'Computer architecture', 'Learning (artificial intelligence)', 'Task analysis']","['Neural Network', 'Meta Learning', 'Channel Pruning', 'Deep Neural Network', 'Target Network', 'Stochastic Sampling', 'Pruning Method', 'Convolutional Layers', 'Feature Maps', 'Weight Matrix', 'Multiple-choice', 'Evolutionary Algorithms', 'Lookup Table', 'Fully-connected Layer', 'Channel Width', 'Input Channels', 'Output Channels', 'Compression Ratio', 'Channel Layer', 'Neural Architecture Search', 'Encoding Vector', 'Hard Constraints', 'Evolutionary Search', 'Iterative Modification']",,305,"In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github.com/liuzechun/MetaPruning."
Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings,"Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein","C3N, P ˆole Judiciaire de la Gendarmerie Nationale, 5 boulevard de l’Hautil, 95000 Cergy, France; ETIS UMR 8051, Universit ´e Paris Seine, UCP, ENSEA, CNRS, F-95000, Cergy, France; LIGM, UMR 8049, ´Ecole des Ponts, UPE, Champs-sur-Marne, France",100.0,"France, france",0.0,,"Learning an effective similarity measure between image representations is key to the success of recent advances in visual search tasks (e.g. verification or zero-shot learning). Although the metric learning part is well addressed, this metric is usually computed over the average of the extracted deep features. This representation is then trained to be discriminative. However, these deep features tend to be scattered across the feature space. Consequently, the representations are not robust to outliers, object occlusions, background variations, etc. In this paper, we tackle this scattering problem with a distribution-aware regularization named HORDE. This regularizer enforces visually-close images to have deep features with the same distribution which are well localized in the feature space. We provide a theoretical analysis supporting this regularization effect. We also show the effectiveness of our approach by obtaining state-of-the-art results on 4 well-known datasets (Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes Retrieval).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jacob_Metric_Learning_With_HORDE_High-Order_Regularizer_for_Deep_Embeddings_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jacob_Metric_Learning_With_HORDE_High-Order_Regularizer_for_Deep_Embeddings_ICCV_2019_paper.pdf,,https://github.com/pierre-jacob/ICCV2019-Horde,,main,Poster,https://ieeexplore.ieee.org/document/9010751/,"['Feature extraction', 'Measurement', 'Image representation', 'Scattering', 'Computer architecture', 'Standards', 'Robustness']","['Metric Learning', 'Theoretical Analysis', 'Feature Space', 'Deep Features', 'Visual Search', 'Image Representation', 'Visual Search Task', 'Zero-shot', 'Loss Function', 'Standard Practice', 'Sample Mean', 'Regions Of Space', 'Latent Space', 'Random Vector', 'Projection Matrix', 'Ensemble Method', 'Similar Images', 'Inference Time', 'Moment Estimation', 'Global Average Pooling', 'Deep Metric Learning', 'Higher-order Moments', 'Maximum Mean Discrepancy', 'Reproducing Kernel Hilbert Space', 'Wasserstein Distance', 'Robust Representation', 'Higher-order Components', 'Consistent Improvement']",,42,"Learning an effective similarity measure between image representations is key to the success of recent advances in visual search tasks (e.g. verification or zero-shot learning). Although the metric learning part is well addressed, this metric is usually computed over the average of the extracted deep features. This representation is then trained to be discriminative. However, these deep features tend to be scattered across the feature space. Consequently, the representations are not robust to outliers, object occlusions, background variations, etc. In this paper, we tackle this scattering problem with a distribution-aware regularization named HORDE. This regularizer enforces visually-close images to have deep features with the same distribution which are well localized in the feature space. We provide a theoretical analysis supporting this regularization effect. We also show the effectiveness of our approach by obtaining state-of-the-art results on 4 well-known datasets (Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes Retrieval)."
Micro-Baseline Structured Light,"Vishwanath Saragadam, Jian Wang, Mohit Gupta, Shree Nayar","NYC Research Lab, Snap Inc.; Department of Computer Science, University of Wisconsin-Madison",50.0,usa,50.0,USA,"We propose Micro-baseline Structured Light (MSL), a novel 3D imaging approach designed for small form-factor devices such as cell-phones and miniature robots. MSL operates with small projector-camera baseline and low-cost projection hardware, and can recover scene depths with computationally lightweight algorithms. The main observation is that a small baseline leads to small disparities, enabling a first-order approximation of the non-linear SL image formation model. This leads to the key theoretical result of the paper: the MSL equation, a linearized version of SL image formation. MSL equation is under-constrained due to two unknowns (depth and albedo) at each pixel, but can be efficiently solved using a local least squares approach. We analyze the performance of MSL in terms of various system parameters such as projected pattern and baseline, and provide guidelines for optimizing performance. Armed with these insights, we build a prototype to experimentally examine the theory and its practicality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Saragadam_Micro-Baseline_Structured_Light_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Saragadam_Micro-Baseline_Structured_Light_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009499/,"['Cameras', 'Mathematical model', 'Hardware', 'Three-dimensional displays', 'Complexity theory', 'Heuristic algorithms', 'Integrated circuits']","['Structured Illumination', '3D Images', 'First Approximation', 'Least Squares Approach', 'Small Baseline', 'Microrobots', 'Scene Depth', 'Small Disparity', 'Computational Complexity', 'Window Size', 'Low Complexity', 'Set Of Equations', 'Focal Length', 'Presence Of Noise', 'Depth Range', 'Window Analysis', 'Low Computational Complexity', 'Mobile Platform', 'Depth Estimation', 'Low Computation', 'Block Matching', 'Camera Pixel', 'Large Baseline', 'Duplicate Images', 'Single Pattern', 'Structure Tensor', 'Camera Focal Length', 'Decoding Algorithm', 'Local Neighborhood']",,2,"We propose Micro-baseline Structured Light (MSL), a novel 3D imaging approach designed for small form-factor devices such as cell-phones and miniature robots. MSL operates with small projector-camera baseline and low-cost projection hardware, and can recover scene depths with computationally lightweight algorithms. The main observation is that a small baseline leads to small disparities, enabling a first-order approximation of the non-linear SL image formation model. This leads to the key theoretical result of the paper: the MSL equation, a linearized version of SL image formation. MSL equation is under-constrained due to two unknowns (depth and albedo) at each pixel, but can be efficiently solved using a local least squares approach. We analyze the performance of MSL in terms of various system parameters such as projected pattern and baseline, and provide guidelines for optimizing performance. Armed with these insights, we build a prototype to experimentally examine the theory and its practicality."
Minimum Delay Object Detection From Video,"Dong Lao, Ganesh Sundaramoorthi","King Abdullah University of Science and Technology (KAUST), Saudi Arabia",100.0,saudi arabia,0.0,,"We consider the problem of detecting objects, as they come into view, from videos in an online fashion. We provide the first real-time solution that is guaranteed to minimize the delay, i.e., the time between when the object comes in view and the declared detection time, subject to acceptable levels of detection accuracy. The method leverages modern CNN-based object detectors that operate on a single frame, to aggregate detection results over frames to provide reliable detection at a rate, specified by the user, in guaranteed minimal delay. To do this, we formulate the problem as a Quickest Detection problem, which provides the aforementioned guarantees. We derive our algorithms from this theory. We show in experiments, that with an overhead of just 50 fps, we can increase the number of correct detections and decrease the overall computational cost compared to running a modern single-frame detector.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lao_Minimum_Delay_Object_Detection_From_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lao_Minimum_Delay_Object_Detection_From_Video_ICCV_2019_paper.pdf,,https://github.com/donglao/mindelayer,,main,Poster,https://ieeexplore.ieee.org/document/9008558/,"['Delays', 'Detectors', 'Real-time systems', 'Reliability', 'Streaming media', 'Computational efficiency', 'Object detection']","['Object Detection', 'Minimum Delay', 'Computational Cost', 'Detection Accuracy', 'Level Of Accuracy', 'Reliable Detection', 'Correct Detection', 'Online Fashion', 'Likelihood Ratio', 'False Alarm', 'Start Time', 'Bounding Box', 'Object Classification', 'Class Probabilities', 'Object Of Interest', 'False Alarm Rate', 'Stopping Rule', 'Self-driving', 'Posterior Mode', 'Unknown Time', 'Two-stage Detectors', 'Imaging Box', 'Detection Delay', 'Recursive Estimation', 'One-stage Detectors', 'Optimal Rule', 'Accuracy Constraints', 'Trajectory Generation', 'Image Object', 'Single Image']",,3,"We consider the problem of detecting objects, as they come into view, from videos in an online fashion. We provide the first real-time solution that is guaranteed to minimize the delay, i.e., the time between when the object comes in view and the declared detection time, subject to acceptable levels of detection accuracy. The method leverages modern CNN-based object detectors that operate on a single frame, to aggregate detection results over frames to provide reliable detection at a rate, specified by the user, in guaranteed minimal delay. To do this, we formulate the problem as a Quickest Detection problem, which provides the aforementioned guarantees. We derive our algorithms from this theory. We show in experiments, that with an overhead of just 50 fps, we can increase the number of correct detections and decrease the overall computational cost compared to running a modern single-frame detector."
Miss Detection vs. False Alarm: Adversarial Learning for Small Object Segmentation in Infrared Images,"Huan Wang, Luping Zhou, Lei Wang","Nanjing University of Science & Technology, Nanjing, P.R.China; University of Sydney, Sydney, Australia; University of Wollongong, Wollongong, Australia",100.0,"australia, china",0.0,,"A key challenge of infrared small object segmentation (ISOS) is to balance miss detection (MD) and false alarm (FA). This usually needs ""opposite"" strategies to suppress the two terms, and has not been well resolved in the literature. In this paper, we propose a deep adversarial learning framework to improve this situation. Departing from the tradition of jointly reducing MD and FA via a single objective, we decompose this difficult task into two sub-tasks handled by two models trained adversarially, with each focusing on reducing either MD or FA. Such a new design brings forth at least three advantages. First, as each model focuses on a relatively simpler sub-task, the overall difficulty of ISOS is somehow decreased. Second, the adversarial training of the two models naturally produces a delicate balance of MD and FA, and low rates for both MD and FA could be achieved at Nash equilibrium. Third, this MD-FA detachment gives us more flexibility to develop specific models dedicated to each sub-task. To realize the above design, we propose a conditional Generative Adversarial Network comprising of two generators and one discriminator. Each generator strives for one sub-task, while the discriminator differentiates the three segmentation results from the two generators and the ground truth. Moreover, in order to better serve the sub-tasks, the two generators, based on context aggregation networks, utilzse different size of receptive fields, providing both local and global views of objects for segmentation. As verified on multiple infrared image data sets, our method consistently achieves better segmentation than many state-of-the-art ISOS methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Miss_Detection_vs._False_Alarm_Adversarial_Learning_for_Small_Object_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Miss_Detection_vs._False_Alarm_Adversarial_Learning_for_Small_Object_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009584/,"['Image segmentation', 'ISO', 'Generators', 'Task analysis', 'Object segmentation', 'Gallium nitride', 'Visualization']","['Infrared Imaging', 'False Alarm', 'Generative Adversarial Networks', 'Small Objects', 'Small Segments', 'Sound Detection', 'Segmentation Of Small Objects', 'Deep Learning', 'Receptive Field', 'Delicate Balance', 'Single Object', 'Segmentation Results', 'Nash Equilibrium', 'Receptive Field Size', 'Conditional Generative Adversarial Network', 'Training Set', 'Comparative Method', 'Deep Models', 'Input Image', 'Visual Task', 'Area Under Receiver Operating Characteristic Curve', 'Confidence Map', 'Synthetic Images', 'Precision And Recall', 'Fully Convolutional Network', 'Binary Segmentation', 'Power-of-two', 'Data Loss', 'Ground Truth Segmentation', 'General Segmentation']",,212,"A key challenge of infrared small object segmentation (ISOS) is to balance miss detection (MD) and false alarm (FA). This usually needs ``opposite'' strategies to suppress the two terms, and has not been well resolved in the literature. In this paper, we propose a deep adversarial learning framework to improve this situation. Departing from the tradition of jointly reducing MD and FA via a single objective, we decompose this difficult task into two sub-tasks handled by two models trained adversarially, with each focusing on reducing either MD or FA. Such a new design brings forth at least three advantages. First, as each model focuses on a relatively simpler sub-task, the overall difficulty of ISOS is somehow decreased. Second, the adversarial training of the two models naturally produces a delicate balance of MD and FA, and low rates for both MD and FA could be achieved at Nash equilibrium. Third, this MD-FA detachment gives us more flexibility to develop specific models dedicated to each sub-task. To realize the above design, we propose a conditional Generative Adversarial Network comprising of two generators and one discriminator. Each generator strives for one sub-task, while the discriminator differentiates the three segmentation results from the two generators and the ground truth. Moreover, in order to better serve the sub-tasks, the two generators, based on context aggregation networks, utilzse different size of receptive fields, providing both local and global views of objects for segmentation. As verified on multiple infrared image data sets, our method consistently achieves better segmentation than many state-of-the-art ISOS methods."
Mixed High-Order Attention Network for Person Re-Identification,"Binghui Chen, Weihong Deng, Jiani Hu",Beijing University of Posts and Telecommunications,100.0,China,0.0,,"Attention has become more attractive in person re-identification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at http://www.bhchen.cn.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.pdf,http://www.bhchen.cn/,,,main,Poster,https://ieeexplore.ieee.org/document/9009039/,"['Tensile stress', 'Visualization', 'Proposals', 'Task analysis', 'Zirconium', 'Videos', 'Cameras']","['Mixed Network', 'High-order Attention', 'Mixed High-order Attention Network', 'Attention Mechanism', 'Complex Information', 'Attention Module', 'Spatial Attention', 'Zero-shot', 'Higher-order Statistics', 'Benchmark', 'Complex Interactions', 'Learning Rate', 'Behavioral Model', 'Deep Models', 'Feature Representation', 'Learning Behavior', 'Adversarial Network', 'Cognitive Biases', 'Diverse Information', 'Hadamard Product', 'Channel Attention', 'Attention Map', 'Higher-order Relationships', 'Local Descriptors', 'Gallery Images', '3D Tensor', 'Video Capture', 'Toy Example', 'Spatial Attention Module', 'Training Images']",,287,"Attention has become more attractive in person re-identification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at http://www.bhchen.cn."
Mixture-Kernel Graph Attention Network for Situation Recognition,"Mohammed Suhail, Leonid Sigal","University of British Columbia, Vector Institute for AI; University of British Columbia, Vector Institute for AI, Canada CIFAR AI Chair",100.0,canada,0.0,,"Understanding images beyond salient actions involves reasoning about scene context, objects, and the roles they play in the captured event. Situation recognition has recently been introduced as the task of jointly reasoning about the verbs (actions) and a set of semantic-role and entity (noun) pairs in the form of action frames. Labeling an image with an action frame requires an assignment of values (nouns) to the roles based on the observed image content. Among the inherent challenges are the rich conditional structured dependencies between the output role assignments and the overall semantic sparsity. In this paper, we propose a novel mixture-kernel attention graph neural network (GNN) architecture designed to address these challenges. Our GNN enables dynamic graph structure during training and inference, through the use of a graph attention mechanism, and context-aware interactions between role pairs. We illustrate the efficacy of our model and design choices by conducting experiments on imSitu benchmark dataset, with accuracy improvements of up to 10% over the state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Suhail_Mixture-Kernel_Graph_Attention_Network_for_Situation_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Suhail_Mixture-Kernel_Graph_Attention_Network_for_Situation_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009835/,"['Task analysis', 'Kernel', 'Semantics', 'Neural networks', 'Image recognition', 'Predictive models', 'Training']","['Graph Attention', 'Graph Attention Network', 'Recognition Of Situations', 'Structural Dynamics', 'Attention Mechanism', 'Graph Structure', 'Use Of Mechanisms', 'Graph Neural Networks', 'Action Frames', 'Semantic Role', 'Entity Pairs', 'Graph Kernel', 'Convolution', 'Computer Vision', 'Number Of Steps', 'Object Detection', 'Edge Weights', 'Semantic Similarity', 'Nodes In The Graph', 'Hidden State', 'Basis Kernel', 'Information Propagation', 'Ground Truth Annotations', 'Single Kernel', 'Action Recognition', 'Set Of Kernels']",,12,"Understanding images beyond salient actions involves reasoning about scene context, objects, and the roles they play in the captured event. Situation recognition has recently been introduced as the task of jointly reasoning about the verbs (actions) and a set of semantic-role and entity (noun) pairs in the form of action frames. Labeling an image with an action frame requires an assignment of values (nouns) to the roles based on the observed image content. Among the inherent challenges are the rich conditional structured dependencies between the output role assignments and the overall semantic sparsity. In this paper, we propose a novel mixture-kernel attention graph neural network (GNN) architecture designed to address these challenges. Our GNN enables dynamic graph structure during training and inference, through the use of a graph attention mechanism, and context-aware interactions between role pairs. We illustrate the efficacy of our model and design choices by conducting experiments on imSitu benchmark dataset, with accuracy improvements of up to 10% over the state-of-the-art."
Modeling Inter and Intra-Class Relations in the Triplet Loss for Zero-Shot Learning,"Yannick Le Cacheux, HervÃ© Le Borgne, Michel Crucianu",CEA LIST; CEDRIC – CNAM,50.0,France,50.0,China,"Recognizing visual unseen classes, i.e. for which no training data is available, is known as Zero Shot Learning (ZSL). Some of the best performing methods apply the triplet loss to seen classes to learn a mapping between visual representations of images and attribute vectors that constitute class prototypes. They nevertheless make several implicit assumptions that limit their performance on real use cases, particularly with fine-grained datasets comprising a large number of classes. We identify three of these assumptions and put forward corresponding novel contributions to address them. Our approach consists in taking into account both inter-class and intra-class relations, respectively by being more permissive with confusions between similar classes, and by penalizing visual samples which are atypical to their class. The approach is tested on four datasets, including the large-scale ImageNet, and exhibits performances significantly above recent methods, even generative methods based on more restrictive hypotheses.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Le_Cacheux_Modeling_Inter_and_Intra-Class_Relations_in_the_Triplet_Loss_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Cacheux_Modeling_Inter_and_Intra-Class_Relations_in_the_Triplet_Loss_for_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008372/,"['Visualization', 'Prototypes', 'Training', 'Semantics', 'Task analysis', 'Testing', 'Covariance matrices']","['Triplet Loss', 'Zero-shot', 'Intra-class Relations', 'General Method', 'Visual Representation', 'Similar Classification', 'Class Prototypes', 'Unseen Classes', 'Real Use Case', 'Vector Of Attributes', 'Training Set', 'Evaluation Method', 'Validation Set', 'Set Of Models', 'General Approach', 'Visual Features', 'Bird Species', 'Harmonic Mean', 'Dot Product', 'Artificial Samples', 'Relevant Weight', 'Class Of Candidates', 'Semantic Space', 'Semantic Distance', 'Strong Hypothesis', 'Order Statistics', 'Visual Space']",,62,"Recognizing visual unseen classes, i.e. for which no training data is available, is known as Zero Shot Learning (ZSL). Some of the best performing methods apply the triplet loss to seen classes to learn a mapping between visual representations of images and attribute vectors that constitute class prototypes. They nevertheless make several implicit assumptions that limit their performance on real use cases, particularly with fine-grained datasets comprising a large number of classes. We identify three of these assumptions and put forward corresponding novel contributions to address them. Our approach consists in taking into account both inter-class and intra-class relations, respectively by being more permissive with confusions between similar classes, and by penalizing visual samples which are atypical to their class. The approach is tested on four datasets, including the large-scale ImageNet, and exhibits performances significantly above recent methods, even generative methods based on more restrictive hypotheses."
Moment Matching for Multi-Source Domain Adaptation,"Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang",Horizon Robotics; Boston University; Columbia University; Vector Institute & Peter Munk Cardiac Center,75.0,"Canada, usa",25.0,China,"Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at http://ai.bu.edu/M3SDA/",,http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.pdf,http://ai.bu.edu/M3SDA/,,,main,Oral,https://ieeexplore.ieee.org/document/9010750/,"['Adaptation models', 'Gallium nitride', 'Training data', 'Benchmark testing', 'Visualization', 'Painting', 'Data models']","['Domain Adaptation', 'Moment Matching', 'Multi-source Domain', 'Multi-source Domain Adaptation', 'Training Data', 'Single Domain', 'Theoretical Insights', 'Practical Scenarios', 'Target Domain', 'Source Domain', 'Domain Adaptation Methods', 'Unlabeled Target Domain', 'Labeled Source Domain', 'Neural Network', 'Deep Network', 'Interesting Observation', 'Weight Vector', 'Generative Adversarial Networks', 'Domain Shift', 'Input Space', 'Unsupervised Domain Adaptation Methods', 'Pairwise Divergence', 'Maximum Mean Discrepancy', 'Negative Transfer', 'Adversarial Domain Adaptation', 'Image Retrieval', 'Target Error', 'AlexNet', 'Domain Discriminator']",,814,"Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at http://ai.bu.edu/M3SDA/."
Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes,"Fabian Brickwedde, Steffen Abraham, Rudolf Mester","Norwegian Open AI Lab, CS Dept. (IDI), NTNU Trondheim, Norway; VSI Lab, CS Dept., Goethe University, Frankfurt, Germany; Robert Bosch GmbH, Hildesheim, Germany; Robert Bosch GmbH, Hildesheim, Germany; VSI Lab, CS Dept., Goethe University, Frankfurt, Germany",40.0,germany,60.0,Norway,"Existing 3D scene flow estimation methods provide the 3D geometry and 3D motion of a scene and gain a lot of interest, for example in the context of autonomous driving. These methods are traditionally based on a temporal series of stereo images. In this paper, we propose a novel monocular 3D scene flow estimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure and motion of the scene by combining multi-view geometry and single-view depth information. Mono-SF considers that the scene flow should be consistent in terms of warping the reference image in the consecutive image based on the principles of multi-view geometry. For integrating single-view depth in a statistical manner, a convolutional neural network, called ProbDepthNet, is proposed. ProbDepthNet estimates pixel-wise depth distributions from a single image rather than single depth values. Additionally, as part of ProbDepthNet, a novel recalibration technique for regression problems is proposed to ensure well-calibrated distributions. Our experiments show that Mono-SF outperforms state-of-the-art monocular baselines and ablation studies support the Mono-SF approach and ProbDepthNet design.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Brickwedde_Mono-SF_Multi-View_Geometry_Meets_Single-View_Depth_for_Monocular_Scene_Flow_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Brickwedde_Mono-SF_Multi-View_Geometry_Meets_Single-View_Depth_for_Monocular_Scene_Flow_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010029/,"['Three-dimensional displays', 'Estimation', 'Probabilistic logic', 'Geometry', 'Uncertainty', 'Cameras', 'Task analysis']","['Flow Estimation', 'Dynamic Scenes', 'Scene Flow', 'Scene Flow Estimation', 'Single-view Depth', 'Convolutional Neural Network', 'Single Image', 'Depth Values', '3D Geometry', 'Depth Distribution', 'Consecutive Images', '3D Motion', 'Stereo Images', 'Scene Structure', 'Mixture Model', 'Rigid Body', 'Optical Flow', 'Optimization Framework', 'Probabilistic Approach', '3D Position', 'Depth Estimation', 'Probabilistic Formulation', 'Aleatoric Uncertainty', 'Energy Minimization Problem', 'Structure From Motion', 'Scene Model', 'Image Coordinates', 'Instance Segmentation', 'Static Scenes', 'Optical Flow Estimation']",,39,"Existing 3D scene flow estimation methods provide the 3D geometry and 3D motion of a scene and gain a lot of interest, for example in the context of autonomous driving. These methods are traditionally based on a temporal series of stereo images. In this paper, we propose a novel monocular 3D scene flow estimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure and motion of the scene by combining multi-view geometry and single-view depth information. Mono-SF considers that the scene flow should be consistent in terms of warping the reference image in the consecutive image based on the principles of multi-view geometry. For integrating single-view depth in a statistical manner, a convolutional neural network, called ProbDepthNet, is proposed. ProbDepthNet estimates pixel-wise depth distributions from a single image rather than single depth values. Additionally, as part of ProbDepthNet, a novel recalibration technique for regression problems is proposed to ensure well-calibrated distributions. Our experiments show that Mono-SF outperforms state-of-the-art monocular baselines and ablation studies support the Mono-SF approach and ProbDepthNet design."
MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation,"Lorenzo Bertoni, Sven Kreiss, Alexandre Alahi","EPFL VITA lab, CH-1015 Lausanne",100.0,switzerland,0.0,,"We tackle the fundamentally ill-posed problem of 3D human localization from monocular RGB images. Driven by the limitation of neural networks outputting point estimates, we address the ambiguity in the task by predicting confidence intervals through a loss function based on the Laplace distribution. Our architecture is a light-weight feed-forward neural network that predicts 3D locations and corresponding confidence intervals given 2D human poses. The design is particularly well suited for small training data, cross-dataset generalization, and real-time applications. Our experiments show that we (i) outperform state-of-the-art results on KITTI and nuScenes datasets, (ii) even outperform a stereo-based method for far-away pedestrians, and (iii) estimate meaningful confidence intervals. We further share insights on our model of uncertainty in cases of limited observations and out-of-distribution samples.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bertoni_MonoLoco_Monocular_3D_Pedestrian_Localization_and_Uncertainty_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bertoni_MonoLoco_Monocular_3D_Pedestrian_Localization_and_Uncertainty_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010961/,"['Three-dimensional displays', 'Uncertainty', 'Task analysis', 'Two dimensional displays', 'Cameras', 'Neural networks', 'Machine learning']","['Uncertainty Estimation', 'Confidence Interval', 'Neural Network', 'Point Estimates', 'Feed-forward Network', '3D Position', 'Laplace Distribution', 'KITTI Dataset', 'Deep Learning', 'Convolutional Neural Network', 'Body Height', 'Training Time', 'Distance Function', 'Image Plane', 'Bounding Box', 'Localization Error', 'Height Variation', 'Pose Estimation', 'Depth Estimation', 'Forward Pass', 'Epistemic Uncertainty', 'Aleatoric Uncertainty', 'Errors In Task', 'Monocular Images', 'Variational Inference', '3D Object Detection', 'Fully-connected Network', 'Average Localization Error', 'Computer Vision', 'Inference Time']",,69,"We tackle the fundamentally ill-posed problem of 3D human localization from monocular RGB images. Driven by the limitation of neural networks outputting point estimates, we address the ambiguity in the task by predicting confidence intervals through a loss function based on the Laplace distribution. Our architecture is a light-weight feed-forward neural network that predicts 3D locations and corresponding confidence intervals given 2D human poses. The design is particularly well suited for small training data, cross-dataset generalization, and real-time applications. Our experiments show that we (i) outperform state-of-the-art results on KITTI and nuScenes datasets, (ii) even outperform a stereo-based method for far-away pedestrians, and (iii) estimate meaningful confidence intervals. We further share insights on our model of uncertainty in cases of limited observations and out-of-distribution samples."
Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking,"Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal, Abhishek Sharma, Arjun Jain",Indian Institute of Technology; Axogyan AI; Max Planck Institute for Informatics,66.66666666666666,"India, germany",33.33333333333334,USA,"Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose- (a) depth-ordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to state-of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at https://github.com/ssfootball04/generative_pose.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sharma_Monocular_3D_Human_Pose_Estimation_by_Generation_and_Ordinal_Ranking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sharma_Monocular_3D_Human_Pose_Estimation_by_Generation_and_Ordinal_Ranking_ICCV_2019_paper.pdf,,https://github.com/ssfootball04/generative_pose,,main,Poster,https://ieeexplore.ieee.org/document/9008113/,"['Three-dimensional displays', 'Two dimensional displays', 'Training', 'Estimation', 'Solid modeling', 'Decoding', 'Heating systems']","['Human Pose Estimation', 'Sample Set', 'Ordinal Scale', 'Benchmark Datasets', 'Variational Autoencoder', '3D Pose', 'Conditional Variational Autoencoder', 'Diverse Sample', 'Kullback-Leibler', 'RGB Images', 'Baseline Samples', 'Matrix M', 'Reprojection', 'L2 Loss', 'Anatomical Constraints', 'Input RGB Image', '2D Pose']",,107,"Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose- (a) depth-ordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to state-of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at https://github.com/ssfootball04/generative_pose."
Monocular Neural Image Based Rendering With Continuous View Control,"Xu Chen, Jie Song, Otmar Hilliges","AIT Lab, ETH Zurich",100.0,switzerland,0.0,,"We propose a method to produce a continuous stream of novel views under fine-grained (e.g., 1 degree step-size) camera control at interactive rates. A novel learning pipeline determines the output pixels directly from the source color. Injecting geometric transformations, including perspective projection, 3D rotation and translation into the network forces implicit reasoning about the underlying geometry. The latent 3D geometry representation is compact and meaningful under 3D transformation, being able to produce geometrically accurate views for both single objects and natural scenes. Our experiments show that both proposed components, the transforming encoder-decoder and depth-guided appearance mapping, lead to significantly improved generalization beyond the training views and in consequence to more accurate view synthesis under continuous 6-DoF camera control. Finally, we show that our method outperforms state-of-the-art baseline methods on public datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Monocular_Neural_Image_Based_Rendering_With_Continuous_View_Control_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Monocular_Neural_Image_Based_Rendering_With_Continuous_View_Control_ICCV_2019_paper.pdf,https://ait.ethz.ch/projects/2019/cont-view-synth/,,,main,Poster,https://ieeexplore.ieee.org/document/9008541/,"['Three-dimensional displays', 'Geometry', 'Training', 'Cameras', 'Task analysis', 'Shape', 'Rendering (computer graphics)']","['Image-based Rendering', 'Single Object', 'Visual Perspective', 'Natural Scenes', 'Latent Representation', '3D Geometry', 'Geometric Transformation', '3D Rotation', 'Fine-grained Control', 'Camera Control', 'View Synthesis', '3D Transformation', 'Decoding', 'Image Quality', 'Single Image', 'Precise Control', 'Point Cloud', 'Focal Length', 'Latent Space', 'Viewing Angle', 'Target View', 'Flow Map', 'Latent Code', 'Depth Map', 'Depth Prediction', 'Flow Prediction', 'Dense Correspondence', 'Training Pairs', '3D Shape', 'Camera Pose']",,8,"We propose a method to produce a continuous stream of novel views under fine-grained (e.g., 1 degree step-size) camera control at interactive rates. A novel learning pipeline determines the output pixels directly from the source color. Injecting geometric transformations, including perspective projection, 3D rotation and translation into the network forces implicit reasoning about the underlying geometry. The latent 3D geometry representation is compact and meaningful under 3D transformation, being able to produce geometrically accurate views for both single objects and natural scenes. Our experiments show that both proposed components, the transforming encoder-decoder and depth-guided appearance mapping, lead to significantly improved generalization beyond the training views and in consequence to more accurate view synthesis under continuous 6-DoF camera control. Finally, we show that our method outperforms state-of-the-art baseline methods on public datasets."
Mop MoirÃ© Patterns Using MopNet,"Bin He, Ce Wang, Boxin Shi, Ling-Yu Duan","National Engineering Lab for Video Technology, Peking University, Beijing, China; National Engineering Lab for Video Technology, Peking University, Beijing, China; The Peng Cheng Laboratory, Shenzhen, China",100.0,china,0.0,,"Moire pattern is a common image quality degradation caused by frequency aliasing between monitors and cameras when taking screen-shot photos. The complex frequency distribution, imbalanced magnitude in colour channels, and diverse appearance attributes of moire pattern make its removal a challenging problem. In this paper, we propose a Moire pattern Removal Neural Network (MopNet) to solve this problem. All core components of MopNet are specially designed for unique properties of moire patterns, including the multi-scale feature aggregation addressing complex frequency, the channel-wise target edge predictor to exploit imbalanced magnitude among colour channels, and the attribute-aware classifier to characterize the diverse appearance for better modelling Moire patterns. Quantitative and qualitative experimental comparison validate the state-of-the-art performance of MopNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Mop_Moire_Patterns_Using_MopNet_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Mop_Moire_Patterns_Using_MopNet_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010929/,"['Image edge detection', 'Image color analysis', 'Image restoration', 'Feature extraction', 'Shape', 'Rain', 'Degradation']","['Neural Network', 'Image Quality', 'Frequency Distribution', 'Laplace Transform', 'Multi-scale Features', 'Color Channels', 'Target Edge', 'Training Data', 'Training Dataset', 'Convolutional Neural Network', 'Frequency Band', 'Frequency Domain', 'Feature Maps', 'Benchmark Datasets', 'Visual Comparison', 'Image Pattern', 'Image Texture', 'Dominant Frequency', 'Edge Information', 'Yellow Box', 'Subtle Structure', 'Non-local Block', 'RGB Channels', 'Edge Intensity', 'Dominant Color', 'Real Curve', 'Multi-scale Strategy', 'Shape Patterns', 'Reference Image', 'Deep Learning']",,50,"Moiré pattern is a common image quality degradation caused by frequency aliasing between monitors and cameras when taking screen-shot photos. The complex frequency distribution, imbalanced magnitude in colour channels, and diverse appearance attributes of moiré pattern make its removal a challenging problem. In this paper, we propose a Moiré pattern Removal Neural Network (MopNet) to solve this problem. All core components of MopNet are specially designed for unique properties of moire patterns, including the multi-scale feature aggregation addressing complex frequency, the channel-wise target edge predictor to exploit imbalanced magnitude among colour channels, and the attribute-aware classifier to characterize the diverse appearance for better modelling Moiré patterns. Quantitative and qualitative experimental comparison validate the state-of-the-art performance of MopNet."
Motion Guided Attention for Video Salient Object Detection,"Haofeng Li, Guanqi Chen, Guanbin Li, Yizhou Yu",Sun Yat-sen University; The University of Hong Kong,100.0,"China, Hong Kong",0.0,,"Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Motion_Guided_Attention_for_Video_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Motion_Guided_Attention_for_Video_Salient_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010060/,"['Object detection', 'Optical imaging', 'Optical network units', 'Tensile stress', 'Task analysis', 'Visualization', 'Saliency detection']","['Salient Object', 'Salient Object Detection', 'Video Salient Object Detection', 'Optical Tomography', 'Attention Module', 'Optical Flow', 'Object Motion', 'Still Images', 'Saliency Detection', 'Video Object', 'Motion Cues', 'Recurrent Neural Network', 'Attention Mechanism', 'Video Frames', 'High-level Features', 'Static Images', 'Spatial Attention', 'Motion Features', 'Output Channels', 'Appearance Features', 'Saliency Map', 'Optical Flow Estimation', 'Channel-wise Attention', 'Feature Tensor', 'Conditional Random Field', 'Element-wise Multiplication', 'Residual Layer', 'Field Of Computer Vision', 'Previous Frame', 'Flow Estimation']",,140,"Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available."
Moulding Humans: Non-Parametric 3D Human Shape Estimation From Single Images,"Valentin Gabeur, Jean-SÃ©bastien Franco, Xavier Martin, Cordelia Schmid, GrÃ©gory Rogez",1Inria∗ 2Google Research; 3NAVER LABS Europe; 1Inria∗,66.66666666666666,France,33.33333333333334,USA,"In this paper, we tackle the problem of 3D human shape estimation from single RGB images. While the recent progress in convolutional neural networks has allowed impressive results for 3D human pose estimation, estimating the full 3D shape of a person is still an open issue. Model-based approaches can output precise meshes of naked under-cloth human bodies but fail to estimate details and un-modelled elements such as hair or clothing. On the other hand, non-parametric volumetric approaches can potentially estimate complete shapes but, in practice, they are limited by the resolution of the output grid and cannot produce detailed estimates. In this work, we propose a non-parametric approach that employs a double depth map to represent the 3D shape of a person: a visible depth map and a ""hidden"" depth map are estimated and combined, to reconstruct the human 3D shape as done with a ""mould"". This representation through 2D depth maps allows a higher resolution output with a much lower dimension than voxel-based volumetric representations. Additionally, our fully derivable depth-based model allows us to efficiently incorporate a discriminator in an adversarial fashion to improve the accuracy and ""humanness"" of the 3D output. We train and quantitatively validate our approach on SURREAL and on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house videos annotated with 3D ground truth surfaces.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gabeur_Moulding_Humans_Non-Parametric_3D_Human_Shape_Estimation_From_Single_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gabeur_Moulding_Humans_Non-Parametric_3D_Human_Shape_Estimation_From_Single_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008390/,"['Three-dimensional displays', 'Shape', 'Estimation', 'Image reconstruction', 'Cameras', 'Solid modeling', 'Surface reconstruction']","['Single Image', 'Shape Estimation', 'Human 3D', 'Human Shape', '3D Human Shape', 'Human Shape Estimation', '3D Human Shape Estimation', 'Humanitarian', 'Volumetric', 'Convolutional Neural Network', 'Depth Map', '3D Surface', 'Lower Dimension', 'Non-parametric Approach', '3D Shape', 'Pose Estimation', 'Visual Map', 'Human Pose Estimation', '3D Pose', '3D Mesh', 'Point Cloud', '3D Point Cloud', 'Image Appearance', 'Adversarial Training', 'Voxel Grid', 'Body Shape', 'Generative Adversarial Networks', 'Depth Prediction', '3D Point']",,95,"In this paper, we tackle the problem of 3D human shape estimation from single RGB images. While the recent progress in convolutional neural networks has allowed impressive results for 3D human pose estimation, estimating the full 3D shape of a person is still an open issue. Model-based approaches can output precise meshes of naked under-cloth human bodies but fail to estimate details and un-modelled elements such as hair or clothing. On the other hand, non-parametric volumetric approaches can potentially estimate complete shapes but, in practice, they are limited by the resolution of the output grid and cannot produce detailed estimates. In this work, we propose a non-parametric approach that employs a double depth map to represent the 3D shape of a person: a visible depth map and a ``hidden'' depth map are estimated and combined, to reconstruct the human 3D shape as done with a ``mould''. This representation through 2D depth maps allows a higher resolution output with a much lower dimension than voxel-based volumetric representations. Additionally, our fully derivable depth-based model allows us to efficiently incorporate a discriminator in an adversarial fashion to improve the accuracy and ``humanness'' of the 3D output. We train and quantitatively validate our approach on SURREAL and on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house videos annotated with 3D ground truth surfaces."
Moving Indoor: Unsupervised Video Depth Learning in Challenging Environments,"Junsheng Zhou, Yuwang Wang, Kaihuai Qin, Wenjun Zeng","Microsoft Research, Beijing, China; Tsinghua University, Beijing, China",50.0,China,50.0,USA,"Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Moving_Indoor_Unsupervised_Video_Depth_Learning_in_Challenging_Environments_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Moving_Indoor_Unsupervised_Video_Depth_Learning_in_Challenging_Environments_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009443/,"['Training', 'Optical imaging', 'Pipelines', 'Cameras', 'Optical variables control', 'Estimation', 'Indoor environments']","['Unsupervised Learning', 'Learning Disabilities', 'Unsupervised Methods', 'Indoor Environments', 'Optical Flow', 'Unsupervised Learning Methods', 'Outdoor Scenes', 'White Walls', 'Texture Regions', 'Image Pairs', 'RGB Images', 'Computationally Intractable', 'Depth Estimation', 'Camera Pose', 'Stereo Images', 'Plausible Results', 'Flow Map', 'Entire Pipeline', 'Iterative Closest Point', 'Pure Rotation', 'Supervisory Signal', 'V2 Dataset', 'Scene Depth', 'Homography Matrix', 'Target View', 'Visual Odometry', 'View Synthesis', 'Ground Truth Depth', 'Smoothness Loss']",,40,"Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets."
Multi-Adversarial Faster-RCNN for Unrestricted Object Detection,"Zhenwei He, Lei Zhang","School of Microelectronics and Communication Engineering, Chongqing University, Chongqing 400044, China",100.0,china,0.0,,"Conventional object detection methods essentially suppose that the training and testing data are collected from a restricted target domain with expensive labeling cost. For alleviating the problem of domain dependency and cumbersome labeling, this paper proposes to detect objects in unrestricted environment by leveraging domain knowledge trained from an auxiliary source domain with sufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN (MAF) framework for unrestricted object detection, which inherently addresses domain disparity minimization for domain adaptation in feature representation. The paper merits are in three-fold: 1) With the idea that object detectors often becomes domain incompatible when image distribution resulted domain disparity appears, we propose a hierarchical domain feature alignment module, in which multiple adversarial domain classifier submodules for layer-wise domain feature confusion are designed; 2) An information invariant scale reduction module (SRM) for hierarchical feature map resizing is proposed for promoting the training efficiency of adversarial domain adaptation; 3) In order to improve the domain adaptability, the aggregated proposal features with detection results are feed into a proposed weighted gradient reversal layer (WGRL) for characterizing hard confused domain samples. We evaluate our MAF on unrestricted tasks including Cityscapes, KITTI, Sim10k, etc. and the experiments show the state-of-the-art performance over the existing detectors.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Multi-Adversarial_Faster-RCNN_for_Unrestricted_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Multi-Adversarial_Faster-RCNN_for_Unrestricted_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010003/,"['Feature extraction', 'Detectors', 'Proposals', 'Object detection', 'Task analysis', 'Convolution', 'Training']","['Object Detection', 'Training Data', 'Feature Maps', 'Feature Representation', 'Detection Results', 'Domain Features', 'Target Domain', 'Submodule', 'Training Efficiency', 'Domain Adaptation', 'Domain Classifier', 'Source Domain', 'Hierarchical Features', 'Feature Alignment', 'Object Detection Methods', 'Gradient Reversal', 'Adversarial Domain Adaptation', 'Convolutional Layers', 'Transfer Learning', 'Bounding Box', 'Domain Alignment', 'Domain Transfer', 'Convolutional Feature Maps', 'Object Detection Task', 'Domain Discrepancy', 'IoU Threshold', 'Region Proposal Network', 'Generative Adversarial Networks', 'Predicted Bounding Box', 'Domain-invariant Features']",,237,"Conventional object detection methods essentially suppose that the training and testing data are collected from a restricted target domain with expensive labeling cost. For alleviating the problem of domain dependency and cumbersome labeling, this paper proposes to detect objects in unrestricted environment by leveraging domain knowledge trained from an auxiliary source domain with sufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN (MAF) framework for unrestricted object detection, which inherently addresses domain disparity minimization for domain adaptation in feature representation. The paper merits are in three-fold: 1) With the idea that object detectors often becomes domain incompatible when image distribution resulted domain disparity appears, we propose a hierarchical domain feature alignment module, in which multiple adversarial domain classifier submodules for layer-wise domain feature confusion are designed; 2) An information invariant scale reduction module (SRM) for hierarchical feature map resizing is proposed for promoting the training efficiency of adversarial domain adaptation; 3) In order to improve the domain adaptability, the aggregated proposal features with detection results are feed into a proposed weighted gradient reversal layer (WGRL) for characterizing hard confused domain samples. We evaluate our MAF on unrestricted tasks including Cityscapes, KITTI, Sim10k, etc. and the experiments show the state-of-the-art performance over the existing detectors."
Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition,"Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China; Department of Computer Vision Technology (VIS), Baidu Inc., China",100.0,china,0.0,,"Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009461/,"['Learning (artificial intelligence)', 'YouTube', 'Markov processes', 'Two dimensional displays', 'Three-dimensional displays', 'Decision making', 'Computer vision']","['Multi-agent Reinforcement Learning', 'Video Recognition', 'Untrimmed Videos', 'Contextual Information', 'Classification Network', 'Video Analysis', 'Baseline Methods', 'Markov Decision Process', 'Policy Network', 'Observation Network', 'Time Step', 'Network Parameters', 'Action Recognition', 'Optical Flow', 'Backbone Network', 'Reward Function', 'Basic Network', 'Gated Recurrent Unit', 'Challenging Dataset', 'Reinforcement Learning Framework', 'Constant Scaling Factor', 'Frame Information', 'Dynamic Datasets']",,83,"Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars."
Multi-Class Part Parsing With Joint Boundary-Semantic Awareness,"Yifan Zhao, Jia Li, Yu Zhang, Yonghong Tian","National Engineering Laboratory for Video Technology, School of EE&CS, Peking University; State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University",100.0,china,0.0,,"Object part parsing in the wild, which requires to simultaneously detect multiple object classes in the scene and accurately segments semantic parts within each class, is challenging for the joint presence of class-level and part-level ambiguities. Despite its importance, however, this problem is not sufficiently explored in existing works. In this paper, we propose a joint parsing framework with boundary and semantic awareness to address this challenging problem. To handle part-level ambiguity, a boundary awareness module is proposed to make mid-level features at multiple scales attend to part boundaries for accurate part localization, which are then fused with high-level features for effective part recognition. For class-level ambiguity, we further present a semantic awareness module that selects discriminative part features relevant to a category to prevent irrelevant features being merged together. The proposed modules are lightweight and implementation friendly, improving the performance substantially when plugged into various baseline architectures. Without bells and whistles, the full model sets new state-of-the-art results on the Pascal-Part dataset, in both multi-class and the conventional single-class setting, while running substantially faster than recent high-performance approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Multi-Class_Part_Parsing_With_Joint_Boundary-Semantic_Awareness_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Multi-Class_Part_Parsing_With_Joint_Boundary-Semantic_Awareness_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010738/,"['Semantics', 'Feature extraction', 'Fuses', 'Task analysis', 'Image resolution', 'Adaptation models', 'Decoding']","['Part Parsing', 'Ambiguity', 'Multiple Scales', 'High-level Features', 'Object Parts', 'Scene Classification', 'Baseline Architecture', 'Feature Maps', 'Semantic Information', 'Selective Modulators', 'Final Prediction', 'Semantic Segmentation', 'Low-level Features', 'Spatial Module', 'Semantic Features', 'Inference Time', 'Pose Estimation', 'Channel Selection', 'Feature Pyramid', 'Spatial Block', 'Semantic Labels', 'Part Segmentation', 'Spatial Selectivity', 'Strong Baseline', 'Human Pose', 'Semantic Objects', 'Lateral Connections', 'Weight Balance', 'Spatial Attention']",,36,"Object part parsing in the wild, which requires to simultaneously detect multiple object classes in the scene and accurately segments semantic parts within each class, is challenging for the joint presence of class-level and part-level ambiguities. Despite its importance, however, this problem is not sufficiently explored in existing works. In this paper, we propose a joint parsing framework with boundary and semantic awareness to address this challenging problem. To handle part-level ambiguity, a boundary awareness module is proposed to make mid-level features at multiple scales attend to part boundaries for accurate part localization, which are then fused with high-level features for effective part recognition. For class-level ambiguity, we further present a semantic awareness module that selects discriminative part features relevant to a category to prevent irrelevant features being merged together. The proposed modules are lightweight and implementation friendly, improving the performance substantially when plugged into various baseline architectures. Without bells and whistles, the full model sets new state-of-the-art results on the Pascal-Part dataset, in both multi-class and the conventional single-class setting, while running substantially faster than recent high-performance approaches."
Multi-Garment Net: Learning to Dress 3D People From Images,"Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, Gerard Pons-Moll","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany",100.0,germany,0.0,,"We present Multi-Garment Network (MGN), a method to predict body shape and clothing, layered on top of the SMPL model from a few frames (1-8) of a video. Several experiments demonstrate that this representation allows higher level of control when compared to single mesh or voxel representations of shape. Our model allows to predict garment geometry, relate it to the body shape, and transfer it to new body shapes and poses. To train MGN, we leverage a digital wardrobe containing 712 digital garments in correspondence, obtained with a novel method to register a set of clothing templates to a dataset of real 3D scans of people in different clothing and poses. Garments from the digital wardrobe, or predicted by MGN, can be used to dress any body shape in arbitrary poses. We will make publicly available the digital wardrobe, the MGN model, and code to dress SMPL with the garments at https://virtualhumans.mpi-inf.mpg.de/mgn",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bhatnagar_Multi-Garment_Net_Learning_to_Dress_3D_People_From_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bhatnagar_Multi-Garment_Net_Learning_to_Dress_3D_People_From_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010057/,"['Clothing', 'Shape', 'Three-dimensional displays', 'Image segmentation', 'Geometry', 'Registers', 'Two dimensional displays']","['Body Shape', '3D Scanning', 'Data Pre-processing', 'Number Of Images', 'Shape Parameter', 'Semantic Segmentation', 'Depth Camera', 'Fabric Layers', 'Single Surface', 'Person Image', 'Shape Space', '3D Pose', 'Latent Code', 'Single Template', 'Non-rigid Registration', '2D Segmentation']",,268,"We present Multi-Garment Network (MGN), a method to predict body shape and clothing, layered on top of the SMPL [40] model from a few frames (1-8) of a video. Several experiments demonstrate that this representation allows higher level of control when compared to single mesh or voxel representations of shape. Our model allows to predict garment geometry, relate it to the body shape, and transfer it to new body shapes and poses. To train MGN, we leverage a digital wardrobe containing 712 digital garments in correspondence, obtained with a novel method to register a set of clothing templates to a dataset of real 3D scans of people in different clothing and poses. Garments from the digital wardrobe, or predicted by MGN, can be used to dress any body shape in arbitrary poses. We will make publicly available the digital wardrobe, the MGN model, and code to dress SMPL with the garments at [1]."
Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting,"Vishwanath A. Sindagi, Vishal M. Patel","Department of Electrical and Computer Engineering, Johns Hopkins University, 3400 N. Charles St, Baltimore, MD 21218, USA",100.0,usa,0.0,,"Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sindagi_Multi-Level_Bottom-Top_and_Top-Bottom_Feature_Fusion_for_Crowd_Counting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sindagi_Multi-Level_Bottom-Top_and_Top-Bottom_Feature_Fusion_for_Crowd_Counting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009788/,"['Feature extraction', 'Fuses', 'Head', 'Semantics', 'Task analysis', 'Flyback transformers', 'Training']","['Feature Fusion', 'Crowd Counting', 'Deeper Layers', 'Network Layer', 'Density Map', 'Scale Variation', 'Fusion Method', 'Adjacent Layers', 'Multi-scale Features', 'Complementary Features', 'Fusion Approach', 'Conv Layer', 'Principled Way', 'Challenges In The Form', 'Multi-scale Feature Fusion', 'Feature Extraction Block', 'Convolutional Neural Network', 'Contextual Information', 'Input Image', 'Feature Maps', 'Lower Layer', 'Baseline Network', 'Fusion Techniques', 'Level Fusion', 'People Counting', 'Feature Concatenation', 'Image Dataset', 'Series Of Blocks', 'Fusion Strategy', 'Images Of People']",,137,"Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets."
Multi-Modality Latent Interaction Network for Visual Question Answering,"Peng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang Wang, Hongsheng Li","CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong; Tsinghua University; SenseTime Research",66.66666666666666,"China, Hong Kong, china",33.33333333333334,China,"Exploiting relationships between visual regions and question words have achieved great success in learning multi-modality features for Visual Question Answering (VQA). However, we argue that existing methods mostly model relations between individual visual regions and words, which are not enough to correctly answer the question. From humans' perspective, answering a visual question requires understanding the summarizations of visual and language information. In this paper, we proposed the Multi-modality Latent Interaction module (MLI) to tackle this problem. The proposed module learns the cross-modality relationships between latent visual and language summarizations, which summarize visual regions and question into a small number of latent representations to avoid modeling uninformative individual region-word relations. The cross-modality information between the latent summarizations are propagated to fuse valuable information from both modalities and are used to update the visual and word features. Such MLI modules can be stacked for several stages to model complex and latent relations between the two modalities and achieves highly competitive performance on public VQA benchmarks, VQA v2.0 and TDIUC . In addition, we show that the performance of our methods could be significantly improved by combining with pre-trained language model BERT.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010837/,"['Visualization', 'Cognition', 'Aggregates', 'Feature extraction', 'Knowledge discovery', 'Benchmark testing', 'Object detection']","['Visual Question', 'Visual Question Answering', 'Latent Interaction', 'Complex Relationship', 'Visual Features', 'Competitive Performance', 'Language Model', 'Question Wording', 'Latent Representation', 'Individual Words', 'Language Information', 'Word Features', 'Correctly Answered', 'Multimodal Features', 'Pre-trained Language Models', 'Validation Set', 'Regional Characteristics', 'Object Detection', 'Attention Mechanism', 'Linear Transformation', 'Latent Vector', 'Feature Pairs', 'Message Passing', 'Relational Reasoning', 'Information Aggregation', 'Attention Weights', 'Relational Approach', 'Image Captioning', 'Feature Aggregation', 'Non-maximum Suppression']",,28,"Exploiting relationships between visual regions and question words have achieved great success in learning multi-modality features for Visual Question Answering (VQA). However, we argue that existing methods mostly model relations between individual visual regions and words, which are not enough to correctly answer the question. From humans' perspective, answering a visual question requires understanding the summarizations of visual and language information. In this paper, we proposed the Multi-modality Latent Interaction module (MLI) to tackle this problem. The proposed module learns the cross-modality relationships between latent visual and language summarizations, which summarize visual regions and question into a small number of latent representations to avoid modeling uninformative individual region-word relations. The cross-modality information between the latent summarizations are propagated to fuse valuable information from both modalities and are used to update the visual and word features. Such MLI modules can be stacked for several stages to model complex and latent relations between the two modalities and achieves highly competitive performance on public VQA benchmarks, VQA v2.0 and TDIUC . In addition, we show that the performance of our methods could be significantly improved by combining with pre-trained language model BERT."
Multi-Stage Pathological Image Classification Using Semantic Segmentation,"Shusuke Takahama,  Yusuke Kurose,  Yusuke Mukuta,  Hiroyuki Abe,  Masashi Fukayama,  Akihiko Yoshizawa,  Masanobu Kitagawa,  Tatsuya Harada","The University of Tokyo, RIKEN, Research Center for Medical Bigdata, National Institute of Informatics; The Japanese Society of Pathology, Tokyo Medical and Dental University; The Japanese Society of Pathology, Kyoto University; The University of Tokyo, The Japanese Society of Pathology; The University of Tokyo; The University of Tokyo, RIKEN",100.0,"Japan, japan",0.0,,"Histopathological image analysis is an essential process for the discovery of diseases such as cancer. However, it is challenging to train CNN on whole slide images (WSIs) of gigapixel resolution considering the available memory capacity. Most of the previous works divide high resolution WSIs into small image patches and separately input them into the model to classify it as a tumor or a normal tissue. However, patch-based classification uses only patch-scale local information but ignores the relationship between neighboring patches. If we consider the relationship of neighboring patches and global features, we can improve the classification performance. In this paper, we propose a new model structure combining the patch-based classification model and whole slide-scale segmentation model in order to improve the prediction performance of automatic pathological diagnosis. We extract patch features from the classification model and input them into the segmentation model to obtain a whole slide tumor probability heatmap. The classification model considers patch-scale local features, and the segmentation model can take global information into account. We also propose a new optimization method that retains gradient information and trains the model partially for end-to-end learning with limited GPU memory capacity. We apply our method to the tumor/normal prediction on WSIs and the classification performance is improved compared with the conventional patch-based method.",http://openaccess.thecvf.com/content_ICCV_2019/html/Takahama_Multi-Stage_Pathological_Image_Classification_Using_Semantic_Segmentation_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Takahama_Multi-Stage_Pathological_Image_Classification_Using_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9008792/,"['Feature extraction', 'Image segmentation', 'Pathology', 'Memory management', 'Predictive models', 'Semantics', 'Image analysis']","['Semantic Segmentation', 'Pathological Images', 'Pathological Classification', 'High-resolution', 'Classification Model', 'Optimization Method', 'Classification Performance', 'Local Information', 'Local Features', 'Global Features', 'Global Information', 'Segmentation Model', 'Memory Capacity', 'Small Patches', 'Slide Images', 'Digital Pathology', 'Patch Features', 'Patch-based Methods', 'Patch Extraction', 'Training Data', 'Forward Pass', 'Feature Maps', 'Separate Learning', 'Memory Consumption', 'Prediction Map', 'Tissue Area', 'Intermediate Layer', 'Conditional Random Field', 'Pathological Analysis', 'Fully Convolutional Network']",,29,"Histopathological image analysis is an essential process for the discovery of diseases such as cancer. However, it is challenging to train CNN on whole slide images (WSIs) of gigapixel resolution considering the available memory capacity. Most of the previous works divide high resolution WSIs into small image patches and separately input them into the model to classify it as a tumor or a normal tissue. However, patch-based classification uses only patch-scale local information but ignores the relationship between neighboring patches. If we consider the relationship of neighboring patches and global features, we can improve the classification performance. In this paper, we propose a new model structure combining the patch-based classification model and whole slide-scale segmentation model in order to improve the prediction performance of automatic pathological diagnosis. We extract patch features from the classification model and input them into the segmentation model to obtain a whole slide tumor probability heatmap. The classification model considers patch-scale local features, and the segmentation model can take global information into account. We also propose a new optimization method that retains gradient information and trains the model partially for end-to-end learning with limited GPU memory capacity. We apply our method to the tumor/normal prediction on WSIs and the classification performance is improved compared with the conventional patch-based method."
Multi-View Image Fusion,"Marc Comino Trinidad, Ricardo Martin Brualla, Florian Kainz, Janne Kontkanen",Google; Polytechnic University of Catalonia,50.0,Spain,50.0,USA,"We present an end-to-end learned system for fusing multiple misaligned photographs of the same scene into a chosen target view. We demonstrate three use cases: 1) color transfer for inferring color for a monochrome view, 2) HDR fusion for merging misaligned bracketed exposures, and 3) detail transfer for reprojecting a high definition image to the point of view of an affordable VR180-camera. While the system can be trained end-to-end, it consists of three distinct steps: feature extraction, image warping and fusion. We present a novel cascaded feature extraction method that enables us to synergetically learn optical flow at different resolution levels. We show that this significantly improves the network's ability to learn large disparities. Finally, we demonstrate that our alignment architecture outperforms a state-of-the art optical flow network on the image warping task when both systems are trained in an identical manner.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Trinidad_Multi-View_Image_Fusion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Trinidad_Multi-View_Image_Fusion_ICCV_2019_paper.pdf,,,,main,Poster,http://ieeexplore.ieee.org/document/9009997/,"['Cameras', 'Image color analysis', 'Feature extraction', 'Optical imaging', 'Image resolution', 'Computer architecture', 'Image fusion']","['Image Features', 'Feature Fusion', 'Optical Flow', 'Large Disparities', 'Optical Networks', 'Warped Image', 'Smartphone', 'Dynamic Range', 'Color Images', 'Receptive Field', 'Multiple Images', 'Grayscale Images', 'Short Exposure', 'Residual Network', 'Images In Order', 'Random Cropping', 'Flow Prediction', 'Monochrome Camera', 'Flow Algorithm', 'Mobile Camera', 'Pyramid Level', 'High Dynamic Range Image', 'Finest Level', 'Interpupillary Distance', 'Finest Resolution', 'Pair Of Cameras']",,22,"We present an end-to-end learned system for fusing multiple misaligned photographs of the same scene into a chosen target view. We demonstrate three use cases: 1) color transfer for inferring color for a monochrome view, 2) HDR fusion for merging misaligned bracketed exposures, and 3) detail transfer for reprojecting a high definition image to the point of view of an affordable VR180-camera. While the system can be trained end-to-end, it consists of three distinct steps: feature extraction, image warping and fusion. We present a novel cascaded feature extraction method that enables us to synergetically learn optical flow at different resolution levels. We show that this significantly improves the network's ability to learn large disparities. Finally, we demonstrate that our alignment architecture outperforms a state-of-the art optical flow network on the image warping task when both systems are trained in an identical manner."
Multi-View Stereo by Temporal Nonparametric Fusion,"Yuxin Hou, Juho Kannala, Arno Solin","Department of Computer Science, Aalto University, Finland",100.0,finland,0.0,,"We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder-decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from nearby views. We train the encoder-decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Multi-View_Stereo_by_Temporal_Nonparametric_Fusion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Multi-View_Stereo_by_Temporal_Nonparametric_Fusion_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008539/,"['Cameras', 'Estimation', 'Three-dimensional displays', 'Computational modeling', 'Decoding', 'Gaussian processes', 'Encoding']","['Multi-view Stereo', 'Latent Space', 'Gaussian Process', 'Encoder-decoder', 'Depth Estimation', 'Bottleneck Layer', 'Encoder-decoder Model', 'Deep Neural Network', 'Kernel Function', 'Point Cloud', 'Angle Difference', 'Kriging', 'Depth Map', 'Mean Absolute Deviation', 'Posterior Mean', 'Covariance Function', 'Latent Representation', 'Error Metrics', 'Simultaneous Localization And Mapping', 'Camera Pose', 'Cost Volume', 'Relative Pose', 'Mat√©rn Covariance Function', 'Input Frames', 'Depth Planes', 'Posterior Covariance', 'Past Frames', 'Latent Variable Model', 'Exponential Kernel', 'Convolutional Layers']",,56,"We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder-decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from nearby views. We train the encoder-decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices."
"MultiSeg: Semantically Meaningful, Scale-Diverse Segmentations From Minimal User Input","Jun Hao Liew, Scott Cohen, Brian Price, Long Mai, Sim-Heng Ong, Jiashi Feng",National University of Singapore; Adobe Research,50.0,singapore,50.0,USA,"Existing deep learning-based interactive image segmentation approaches typically assume the target-of-interest is always a single object and fail to account for the potential diversity in user expectations, thus requiring excessive user input when it comes to segmenting an object part or a group of objects instead. Motivated by the observation that the object part, full object, and a collection of objects essentially differ in size, we propose a new concept called scale-diversity, which characterizes the spectrum of segmentations w.r.t. different scales. To address this, we present MultiSeg, a scale-diverse interactive image segmentation network that incorporates a set of two-dimensional scale priors into the model to generate a set of scale-varying proposals that conform to the user input. We explicitly encourage segmentation diversity during training by synthesizing diverse training samples for a given image. As a result, our method allows the user to quickly locate the closest segmentation target for further refinement if necessary. Despite its simplicity, experimental results demonstrate that our proposed model is capable of quickly producing diverse yet plausible segmentation outputs, reducing the user interaction required, especially in cases where many types of segmentations (object parts or groups) are expected.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liew_MultiSeg_Semantically_Meaningful_Scale-Diverse_Segmentations_From_Minimal_User_Input_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liew_MultiSeg_Semantically_Meaningful_Scale-Diverse_Segmentations_From_Minimal_User_Input_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010681/,"['Image segmentation', 'Training', 'Proposals', 'Feature extraction', 'Task analysis', 'Training data', 'Network architecture']","['User Input', 'Minimal User Input', 'Interaction Network', 'C=O Groups', 'Single Object', 'Object Parts', 'Diverse Training', 'Segmentation Output', 'Set Of Proposals', 'Interactive Segmentation', 'Training Data', 'Loss Of Diversity', 'Bounding Box', 'Local Parts', 'Forward Pass', 'Amount Of Interaction', 'Vertical Scale', 'Objective Scores', 'Non-maximum Suppression', 'Set Of Segments', 'Ground-truth Bounding Box', 'Combined Objective', 'Segmentation Branch', 'Backbone Architecture', 'Euclidean Distance Function', 'Diverse Definitions', 'Diversity Of Training Data', 'Quick Glance', 'Object Instances', 'Generate Training Data']",,21,"Existing deep learning-based interactive image segmentation approaches typically assume the target-of-interest is always a single object and fail to account for the potential diversity in user expectations, thus requiring excessive user input when it comes to segmenting an object part or a group of objects instead. Motivated by the observation that the object part, full object, and a collection of objects essentially differ in size, we propose a new concept called scale-diversity, which characterizes the spectrum of segmentations w.r.t. different scales. To address this, we present MultiSeg, a scale-diverse interactive image segmentation network that incorporates a set of two-dimensional scale priors into the model to generate a set of scale-varying proposals that conform to the user input. We explicitly encourage segmentation diversity during training by synthesizing diverse training samples for a given image. As a result, our method allows the user to quickly locate the closest segmentation target for further refinement if necessary. Despite its simplicity, experimental results demonstrate that our proposed model is capable of quickly producing diverse yet plausible segmentation outputs, reducing the user interaction required, especially in cases where many types of segmentations (object parts or groups) are expected."
Multimodal Style Transfer via Graph Cuts,"Yulun Zhang, Chen Fang, Yilin Wang, Zhaowen Wang, Zhe Lin, Yun Fu, Jimei Yang",Northeastern University; Adobe Research,50.0,china,50.0,USA,"An assumption widely used in recent neural style transfer methods is that image styles can be described by global statics of deep features like Gram or covariance matrices. Alternative approaches have represented styles by decomposing them into local pixel or neural patches. Despite the recent progress, most existing methods treat the semantic patterns of style image uniformly, resulting unpleasing results on complex styles. In this paper, we introduce a more flexible and general universal style transfer technique: multimodal style transfer (MST). MST explicitly considers the matching of semantic patterns in content and style images. Specifically, the style image features are clustered into sub-style components, which are matched with local content features under a graph cut formulation. A reconstruction network is trained to transfer each sub-style and render the final stylized result. We also generalize MST to improve some existing methods. Extensive experiments demonstrate the superior effectiveness, robustness, and flexibility of MST.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Multimodal_Style_Transfer_via_Graph_Cuts_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Multimodal_Style_Transfer_via_Graph_Cuts_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010372/,"['Feature extraction', 'Visualization', 'Pattern matching', 'Minimization', 'Optimization', 'Computer vision', 'Covariance matrices']","['Style Transfer', 'Covariance Matrix', 'Deep Features', 'Content Features', 'Gram Matrix', 'Style Image', 'Smoothing', 'Energy Minimization', 'Bimodal', 'Energy Function', 'Feed-forward Network', 'Transfer Characteristics', 'Loss Of Content', 'Unimodal Distribution', 'Feature Transformation', 'Markov Random Field', 'Cluster Labels', 'Style Features', 'Smooth Area', 'Multimodal Representation', 'Energy Minimization Problem', '4th Column', '1st Row', 'Adaptive Transfer', '2nd Row', '2nd Column', 'Multiple Subsets', 'Feature Space', 'Feature Points', 'User Study']",,60,"An assumption widely used in recent neural style transfer methods is that image styles can be described by global statics of deep features like Gram or covariance matrices. Alternative approaches have represented styles by decomposing them into local pixel or neural patches. Despite the recent progress, most existing methods treat the semantic patterns of style image uniformly, resulting unpleasing results on complex styles. In this paper, we introduce a more flexible and general universal style transfer technique: multimodal style transfer (MST). MST explicitly considers the matching of semantic patterns in content and style images. Specifically, the style image features are clustered into sub-style components, which are matched with local content features under a graph cut formulation. A reconstruction network is trained to transfer each sub-style and render the final stylized result. We also generalize MST to improve some existing methods. Extensive experiments demonstrate the superior effectiveness, robustness, and flexibility of MST."
NLNL: Negative Learning for Noisy Labels,"Youngdong Kim, Junho Yim, Juseung Yun, Junmo Kim","School of Electrical Engineering, KAIST, South Korea",100.0,south korea,0.0,,"Convolutional Neural Networks (CNNs) provide excellent performance when used for image classification. The classical method of training CNNs is by labeling images in a supervised manner as in ""input image belongs to this label"" (Positive Learning; PL), which is a fast and accurate method if the labels are assigned correctly to all images. However, if inaccurate labels, or noisy labels, exist, training with PL will provide wrong information, thus severely degrading performance. To address this issue, we start with an indirect learning method called Negative Learning (NL), in which the CNNs are trained using a complementary label as in ""input image does not belong to this complementary label."" Because the chances of selecting a true label as a complementary label are low, NL decreases the risk of providing incorrect information. Furthermore, to improve convergence, we extend our method by adopting PL selectively, termed as Selective Negative Learning and Positive Learning (SelNLPL). PL is used selectively to train upon expected-to-be-clean data, whose choices become possible as NL progresses, thus resulting in superior performance of filtering out noisy data. With simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classification, proving the superiority of SelNLPL's noisy data filtering ability.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009441/,,"['Noisy Labels', 'Convolutional Neural Network', 'Misinformation', 'Input Image', 'Noisy Data', 'Truth Labels', 'Convolutional Neural Network Training', 'Negative Learning', 'Poor Performance', 'Training Data', 'Clean Data', 'Precision And Recall', 'Stochastic Gradient Descent', 'Baseline Methods', 'Convolutional Neural Network Architecture', 'Ground Truth Labels', 'Amount Of Noise', 'Types Of Noise', 'Semi-supervised Learning', 'Filtering Results', 'Pseudo Labels', 'Soft Labels', 'Fashion-MNIST', 'Confidence Data', 'Noise Case']",,149,"Convolutional Neural Networks (CNNs) provide excellent performance when used for image classification. The classical method of training CNNs is by labeling images in a supervised manner as in ""input image belongs to this label'' (Positive Learning; PL), which is a fast and accurate method if the labels are assigned correctly to all images. However, if inaccurate labels, or noisy labels, exist, training with PL will provide wrong information, thus severely degrading performance. To address this issue, we start with an indirect learning method called Negative Learning (NL), in which the CNNs are trained using a complementary label as in ""input image does not belong to this complementary label.'' Because the chances of selecting a true label as a complementary label are low, NL decreases the risk of providing incorrect information. Furthermore, to improve convergence, we extend our method by adopting PL selectively, termed as Selective Negative Learning and Positive Learning (SelNLPL). PL is used selectively to train upon expected-to-be-clean data, whose choices become possible as NL progresses, thus resulting in superior performance of filtering out noisy data. With simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classification, proving the superiority of SelNLPL's noisy data filtering ability."
NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection,"Jiyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, Ram Nevatia",Stanford University; University of Southern California; Google Cloud,66.66666666666666,usa,33.33333333333334,USA,"The labeling cost of large number of bounding boxes is one of the main challenges for training modern object detectors. To reduce the dependence on expensive bounding box annotations, we propose a new semi-supervised object detection formulation, in which a few seed box level annotations and a large scale of image level annotations are used to train the detector. We adopt a training-mining framework, which is widely used in weakly supervised object detection tasks. However, the mining process inherently introduces various kinds of labelling noises: false negatives, false positives and inaccurate boundaries, which can be harmful for training the standard object detectors (e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN) object detector to handle such noisy labels. Comparing to standard Faster RCNN, it contains three highlights: an ensemble of two classification heads and a distillation head to avoid overfitting on noisy labels and improve the mining precision, masking the negative sample loss in box predictor to avoid the harm of false negative labels, and training box regression head only on seed annotations to eliminate the harm from inaccurate boundaries of mined bounding boxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we observe that the detection accuracy consistently improves as we iterate between mining and training steps, and state-of-the-art performance is achieved.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_NOTE-RCNN_NOise_Tolerant_Ensemble_RCNN_for_Semi-Supervised_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_NOTE-RCNN_NOise_Tolerant_Ensemble_RCNN_for_Semi-Supervised_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010645/,"['Detectors', 'Training', 'Object detection', 'Head', 'Proposals', 'Feature extraction', 'Standards']","['Object Detection', 'Semi-supervised Object Detection', 'False Positive', 'False Negative', 'Bounding Box', 'Training Step', 'Faster R-CNN', 'Noisy Labels', 'Bounding Box Annotations', 'Prediction Box', 'Box Annotations', 'Classification Head', 'False Labels', 'Training Data', 'Detection Performance', 'Semantic Similarity', 'Types Of Noise', 'Image Retrieval', 'Target Category', 'Source Categories', 'Region Proposal Network', 'Multiple Instance Learning', 'Source Detector', 'Image-level Labels', 'MS COCO Dataset', 'Noise Tolerance', 'Amount Of Labels', 'Object Proposals', 'Proposal Generation']",,54,"The labeling cost of large number of bounding boxes is one of the main challenges for training modern object detectors. To reduce the dependence on expensive bounding box annotations, we propose a new semi-supervised object detection formulation, in which a few seed box level annotations and a large scale of image level annotations are used to train the detector. We adopt a training-mining framework, which is widely used in weakly supervised object detection tasks. However, the mining process inherently introduces various kinds of labelling noises: false negatives, false positives and inaccurate boundaries, which can be harmful for training the standard object detectors (e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN) object detector to handle such noisy labels. Comparing to standard Faster RCNN, it contains three highlights: an ensemble of two classification heads and a distillation head to avoid overfitting on noisy labels and improve the mining precision, masking the negative sample loss in box predictor to avoid the harm of false negative labels, and training box regression head only on seed annotations to eliminate the harm from inaccurate boundaries of mined bounding boxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we observe that the detection accuracy consistently improves as we iterate between mining and training steps, and state-of-the-art performance is achieved."
Neighborhood Preserving Hashing for Scalable Video Retrieval,"Shuyan Li, Zhixiang Chen, Jiwen Lu, Xiu Li, Jie Zhou","Department of Automation, Tsinghua University, China; Graduate School at Shenzhen, Tsinghua University, China; Department of Automation, Tsinghua University, China; State Key Lab of Intelligent Technologies and Systems, China; Beijing National Research Center for Information Science and Technology, China; Department of Automation, Tsinghua University, China; State Key Lab of Intelligent Technologies and Systems, China",100.0,"China, china",0.0,,"In this paper, we propose a Neighborhood Preserving Hashing (NPH) method for scalable video retrieval in an unsupervised manner. Unlike most existing deep video hashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal neighborhood information into the encoding network such that the neighborhood-relevant visual content of a video can be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses on partial useful content of each input frame conditioned on the neighborhood information. We then integrate the neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the learned hashing functions can map similar videos to similar binary codes. Extensive experiments on three widely-used benchmark datasets validate the effectiveness of our proposed approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Neighborhood_Preserving_Hashing_for_Scalable_Video_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Neighborhood_Preserving_Hashing_for_Scalable_Video_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010279/,"['Binary codes', 'Encoding', 'Visualization', 'Feature extraction', 'Training', 'Neural networks', 'Standards']","['Neighborhood Preservation', 'Hash Function', 'Binary Code', 'Video Content', 'Unsupervised Manner', 'Visual Content', 'Neighborhood Information', 'Input Frames', 'Spatial-temporal Information', 'Validation Set', 'Structure Of Space', 'Nearest Neighbor Search', 'Memory State', 'Reconstruction Loss', 'Training Videos', 'Neighborhood Structure', 'Code Length', 'Video Dataset', 'Retrieval Performance', 'Spatial-temporal Features', 'Frame Features', 'Database Retrieval', 'Greater Margin', 'Conventional Convolutional Neural Networks', 'Unseen Classes']",,24,"In this paper, we propose a Neighborhood Preserving Hashing (NPH) method for scalable video retrieval in an unsupervised manner. Unlike most existing deep video hashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal neighborhood information into the encoding network such that the neighborhood-relevant visual content of a video can be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses on partial useful content of each input frame conditioned on the neighborhood information. We then integrate the neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the learned hashing functions can map similar videos to similar binary codes. Extensive experiments on three widely-used benchmark datasets validate the effectiveness of our proposed approach."
Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape Representation Learning and Generation,"Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Michael Bronstein, Stefanos Zafeiriou","Imperial College London, UK and Universita Svizzera Italiana, Switzerland; Imperial College London, UK; Universita Svizzera Italiana, Switzerland",100.0,"Switzerland, uk",0.0,,"Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bouritsas_Neural_3D_Morphable_Models_Spiral_Convolutional_Networks_for_3D_Shape_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bouritsas_Neural_3D_Morphable_Models_Spiral_Convolutional_Networks_for_3D_Shape_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009455/,"['Three-dimensional displays', 'Spirals', 'Shape', 'Solid modeling', 'Convolution', 'Computational modeling', 'Topology']","['Representation Learning', '3D Shape', 'Morphable Model', '3D Morphable Model', 'Computer Vision', 'Human Faces', '3D Mesh', 'Graph Neural Networks', 'Computer Graphics', 'Graph Convolution', 'Geometric Data', 'Linear Formulation', 'Common Practice', 'Convolutional Neural Network', 'Manifold', 'Facial Expressions', '3D Reconstruction', 'Point Cloud', 'Latent Space', 'High Computational Complexity', 'Mesh Vertices', 'Dilated Convolution', 'Wasserstein Generative Adversarial Networks', 'Face Identity', 'Spectral Filtering', 'Filter Weights', 'Attention Weights', 'Mean Shape', 'Latent Representation', 'Graph Topology']",,111,"Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators."
Neural Inter-Frame Compression for Video Coding,"Abdelaziz Djelouah, Joaquim Campos, Simone Schaub-Meyer, Christopher Schroers","DisneyResearch |Studios, Department of Computer Science, ETH Zurich; DisneyResearch |Studios",100.0,"USA, switzerland",0.0,,"While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Djelouah_Neural_Inter-Frame_Compression_for_Video_Coding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Djelouah_Neural_Inter-Frame_Compression_for_Video_Coding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009574/,"['Image coding', 'Interpolation', 'Video compression', 'Optical imaging', 'Codecs', 'Image reconstruction', 'Distortion']","['Video Coding', 'Neural Compression', 'Inter-frame Compression', 'Deep Learning', 'Decoding', 'Latent Space', 'Optical Flow', 'Latent Representation', 'Reconstruction Quality', 'Residual Information', 'Pixel Spacing', 'Image Compression', 'Motion Compensation', 'Neural Image', 'Intermediate Frames', 'Distortion', 'Neural Network', 'Mean Square Error', 'Image Quality', 'Reference Frame', 'Video Compression', 'Image X', 'Motion Field', 'Interpolation Results', 'Image Synthesis', 'Compression Method', 'Entropy Coding', 'Original Frame', 'Motion Information', 'Interpolation Method']",,105,"While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs."
Neural Inverse Rendering of an Indoor Scene From a Single Image,"Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, Jan Kautz","University of Maryland, College Park; NVIDIA, SenseTime; NVIDIA, University of Maryland, College Park, University of Washington; NVIDIA",50.0,usa,50.0,USA,"Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for single objects or with methods that solve for only one of the scene attributes. We propose the first learning based approach that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects (e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise. This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing the input image from the estimated components. We finetune with real data after pretraining with synthetic data. To this end, we use physically-based rendering to create a large-scale synthetic dataset, named SUNCG-PBR, which is a significant improvement over prior datasets. Experimental results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sengupta_Neural_Inverse_Rendering_of_an_Indoor_Scene_From_a_Single_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sengupta_Neural_Inverse_Rendering_of_an_Indoor_Scene_From_a_Single_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008823/,"['Lighting', 'Rendering (computer graphics)', 'Image reconstruction', 'Training', 'Geometry', 'Three-dimensional displays', 'Face']","['Single Image', '3D Reconstruction', 'Input Image', 'Large-scale Datasets', 'Self-supervised Learning', 'Reconstruction Loss', 'Cast Shadows', 'Scene Properties', 'Neural Network', 'Deep Learning', 'Image Features', 'Supervised Learning', 'High-quality Images', 'Semantic Segmentation', 'Residual Block', 'Normal Approximation', 'Appearance Features', 'Synthetic Images', 'Spherical Harmonics', 'L1 Loss', 'Weak Supervision', 'Environment Map', 'Surface Normals', 'Self-supervised Training', 'Photo-realistic Images', 'Sparse Labeling', 'Direct Illumination', 'Surface Albedo', 'Median Error', 'Learnable Parameters']",,94,"Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for single objects or with methods that solve for only one of the scene attributes. We propose the first learning based approach that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects (e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise. This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing the input image from the estimated components. We finetune with real data after pretraining with synthetic data. To this end, we use physically-based rendering to create a large-scale synthetic dataset, named SUNCG-PBR, which is a significant improvement over prior datasets. Experimental results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes."
Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses,"Eric Brachmann, Carsten Rother","Visual Learning Lab, Heidelberg University (HCI/IWR)",100.0,germany,0.0,,"We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Brachmann_Neural-Guided_RANSAC_Learning_Where_to_Sample_Model_Hypotheses_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Brachmann_Neural-Guided_RANSAC_Learning_Where_to_Sample_Model_Hypotheses_ICCV_2019_paper.pdf,http://vislearn.de,,,main,Poster,https://ieeexplore.ieee.org/document/9008398/,"['Task analysis', 'Training', 'Neural networks', 'Robustness', 'Pipelines', 'Estimation', 'Cameras']","['Random Sample Consensus', 'Neural Network', 'Axon Guidance', 'Robust Optimization', 'Training Signal', 'Task Loss', 'Error Of The Mean', 'Scoring Function', 'Image Pairs', 'Differencing', 'Set Of Observations', 'Additional Input', 'Neural Network Training', 'Feature Matching', 'Training Objective', 'Self-supervised Learning', 'Distance Ratio', 'Image Coordinates', 'Supplement For Details', 'Fundamental Matrix', 'Ground Truth Pose', 'Camera Pose', 'Relative Pose', 'Indoor Settings', 'Angular Error', 'Model Parameters']",,183,"We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones."
New Convex Relaxations for MRF Inference With Unknown Graphs,"Zhenhua Wang, Tong Liu, Qinfeng Shi, M. Pawan Kumar, Jianhua Zhang",Zhejiang University of Technology; University of Oxford; The University of Adelaide,100.0,"China, australia, uk",0.0,,"Treating graph structures of Markov random fields as unknown and estimating them jointly with labels have been shown to be useful for modeling human activity recognition and other related tasks. We propose two novel relaxations for solving this problem. The first is a linear programming (LP) relaxation, which is provably tighter than the existing LP relaxation. The second is a non-convex quadratic programming (QP) relaxation, which admits an efficient concave-convex procedure (CCCP). The CCCP algorithm is initialized by solving a convex QP relaxation of the problem, which is obtained by modifying the diagonal of the matrix that specifies the non-convex QP relaxation. We show that our convex QP relaxation is optimal in the sense that it minimizes the L1 norm of the diagonal modification vector. While the convex QP relaxation is not as tight as the existing and the new LP relaxations, when used in conjunction with the CCCP algorithm for the non-convex QP relaxation, it provides accurate solutions. We demonstrate the efficacy of our new relaxations for both synthetic data and human activity recognition.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_New_Convex_Relaxations_for_MRF_Inference_With_Unknown_Graphs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_New_Convex_Relaxations_for_MRF_Inference_With_Unknown_Graphs_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010619/,"['Linear programming', 'Upper bound', 'Markov processes', 'Quadratic programming', 'Activity recognition', 'TV']","['Markov Random Field', 'Convex Relaxation', 'Linear Programming', 'Normal Vector', 'Graph Structure', 'Action Recognition', 'Quadratic Programming', 'Synthetic Activity', 'Human Activity Recognition', 'Action Recognition Task', 'Linear Programming Relaxation', 'Objective Function', 'Linear Function', 'Running Time', 'Quadratic Function', 'Human Interaction', 'New Variables', 'Feasible Set', 'Optimization Variables', 'Problem Size', 'Linear Constraints', 'Pairwise Potential', 'Interior Point Method', 'Inference Problem', 'Maximum A Posteriori', 'Convex Approximation', 'Inference Algorithm', 'Recognition Results', 'Local Minimum Point']",,5,"Treating graph structures of Markov random fields as unknown and estimating them jointly with labels have been shown to be useful for modeling human activity recognition and other related tasks. We propose two novel relaxations for solving this problem. The first is a linear programming (LP) relaxation, which is provably tighter than the existing LP relaxation. The second is a non-convex quadratic programming (QP) relaxation, which admits an efficient concave-convex procedure (CCCP). The CCCP algorithm is initialized by solving a convex QP relaxation of the problem, which is obtained by modifying the diagonal of the matrix that specifies the non-convex QP relaxation. We show that our convex QP relaxation is optimal in the sense that it minimizes the L1 norm of the diagonal modification vector. While the convex QP relaxation is not as tight as the existing and the new LP relaxations, when used in conjunction with the CCCP algorithm for the non-convex QP relaxation, it provides accurate solutions. We demonstrate the efficacy of our new relaxations for both synthetic data and human activity recognition."
No Fear of the Dark: Image Retrieval Under Varying Illumination Conditions,"Tomas Jenicek, OndÅej Chum","Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague",100.0,Czech Republic,0.0,,"Image retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes. We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night. We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such as Oxford or Paris datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jenicek_No_Fear_of_the_Dark_Image_Retrieval_Under_Varying_Illumination_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jenicek_No_Fear_of_the_Dark_Image_Retrieval_Under_Varying_Illumination_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010812/,"['Histograms', 'Lighting', 'Feature extraction', 'Image retrieval', 'Visualization', 'Image color analysis', 'Detectors']","['Illumination Conditions', 'Image Retrieval', 'Varying Illumination Conditions', 'Fear Of The Dark', 'Convolutional Neural Network', 'Image Preprocessing', 'Image Descriptors', 'Illumination Changes', 'U-Net Architecture', 'Histogram Equalization', 'State Of The Art Methods', 'Light Conditions', 'Input Image', 'Image Dataset', 'Intersection Over Union', 'Raw Images', 'Multiple Images', 'Image Pairs', 'Target Image', 'Nearest Neighbor Search', 'Network Embedding', 'Query Image', 'Geometric Changes', 'Lab Color Space', 'Normalization Step', 'Illumination Differences', 'Contrastive Loss', 'Histogram Matching', 'View Direction', 'Gamma Correction']",,23,"Image retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes. We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night. We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such as Oxford or Paris datasets."
"No-Frills Human-Object Interaction Detection: Factorization, Layout Encodings, and Training Techniques","Tanmay Gupta, Alexander Schwing, Derek Hoiem",University of Illinois Urbana-Champaign,100.0,usa,0.0,,"We show that for human-object interaction detection a relatively simple factorized model with appearance and layout encodings constructed from pre-trained object detectors outperforms more sophisticated approaches. Our model includes factors for detection scores, human and object appearance, and coarse (box-pair configuration) and optionally fine-grained layout (human pose). We also develop training techniques that improve learning efficiency by: (1) eliminating a train-inference mismatch; (2) rejecting easy negatives during mini-batch training; and (3) using a ratio of negatives to positives that is two orders of magnitude larger than existing approaches. We conduct a thorough ablation study to understand the importance of different factors and training techniques using the challenging HICO-Det dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_No-Frills_Human-Object_Interaction_Detection_Factorization_Layout_Encodings_and_Training_Techniques_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_No-Frills_Human-Object_Interaction_Detection_Factorization_Layout_Encodings_and_Training_Techniques_ICCV_2019_paper.pdf,http://tanmaygupta.info/no_frills/,,,main,Poster,https://ieeexplore.ieee.org/document/9010934/,"['Training', 'Encoding', 'Layout', 'Detectors', 'Task analysis', 'Computational modeling', 'Feature extraction']","['Factorization', 'Technical Training', 'Human-object Interaction', 'Human-Object Interaction Detection', 'Factor Model', 'Object Detection', 'Object Appearance', 'Human Pose', 'Importance Of Different Factors', 'Training Set', 'Interaction Term', 'Attention Mechanism', 'Intersection Over Union', 'Multilayer Perceptron', 'Bounding Box', 'Batch Normalization', 'Object Classification', 'Object Location', 'Class Probabilities', 'Set Of Classes', 'Appearance Features', 'Object Boxes', 'Candidate Boxes', 'Image X', 'Object Labels', 'Semantic Role', 'Candidate Objects', 'Classification Loss', 'Hidden Layer']",,81,"We show that for human-object interaction detection a relatively simple factorized model with appearance and layout encodings constructed from pre-trained object detectors outperforms more sophisticated approaches. Our model includes factors for detection scores, human and object appearance, and coarse (box-pair configuration) and optionally fine-grained layout (human pose). We also develop training techniques that improve learning efficiency by: (1) eliminating a train-inference mismatch; (2) rejecting easy negatives during mini-batch training; and (3) using a ratio of negatives to positives that is two orders of magnitude larger than existing approaches. We conduct a thorough ablation study to understand the importance of different factors and training techniques using the challenging HICO-Det dataset."
Nocaps: Novel Object Captioning at Scale,"Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson",Oregon State University; University of Michigan; Georgia Institute of Technology; Macquarie University; Facebook AI Research,80.0,"Australia, usa",20.0,USA,"Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed 'nocaps', for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images image-level labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.pdf,https://nocaps.org,,,main,Poster,https://ieeexplore.ieee.org/document/9009481/,"['Benchmark testing', 'Visualization', 'Task analysis', 'Object detection', 'Data models', 'Training', 'Vegetation']","['Benchmark', 'Training Data', 'Validation Set', 'Object Detection', 'Image Object', 'Paired Data', 'Bounding Box', 'Object Classification', 'Open Image', 'Image Captioning', 'Visual Concepts', 'Object Detection Dataset', 'Training Set', 'Decoding', 'Visual Features', 'Language Model', 'State Machine', 'Word Embedding', 'Dolphins', 'Automatic Evaluation', 'Test Split', 'Object Instances', 'COCO Dataset', 'Ground Truth Object', 'Simple Heuristics', 'Infant-directed Speech', 'Unique Class']",,154,"Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed ‘nocaps’, for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images image-level labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work."
Noise Flow: Noise Modeling With Conditional Normalizing Flows,"Abdelrahman Abdelhamed, Marcus A. Brubaker, Michael S. Brown","Borealis AI; York University, Samsung AI Center, Toronto; York University, Borealis AI",66.66666666666666,canada,33.33333333333334,Canada,"Modeling and synthesizing image noise is an important aspect in many computer vision applications. The long-standing additive white Gaussian and heteroscedastic (signal-dependent) noise models widely used in the literature provide only a coarse approximation of real sensor noise. This paper introduces Noise Flow, a powerful and accurate noise model based on recent normalizing flow architectures. Noise Flow combines well-established basic parametric noise models (e.g., signal-dependent noise) with the flexibility and expressiveness of normalizing flow networks. The result is a single, comprehensive, compact noise model containing fewer than 2500 parameters yet able to represent multiple cameras and gain factors. Noise Flow dramatically outperforms existing noise models, with 0.42 nats/pixel improvement over the camera-calibrated noise level functions, which translates to 52% improvement in the likelihood of sampled noise. Noise Flow represents the first serious attempt to go beyond simple parametric models to one that leverages the power of deep learning and data-driven noise distributions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Abdelhamed_Noise_Flow_Noise_Modeling_With_Conditional_Normalizing_Flows_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Abdelhamed_Noise_Flow_Noise_Modeling_With_Conditional_Normalizing_Flows_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008378/,"['Mathematical model', 'Couplings', 'Adaptation models', 'Computational modeling', 'Jacobian matrices', 'Cameras', 'Noise measurement']","['Noise Model', 'Normal Flow', 'Conditional Normalizing Flow', 'Model Parameters', 'Accuracy Of Model', 'Computer Vision', 'Gaussian Noise', 'Additive Noise', 'Gaussian Model', 'Flow Model', 'Image Noise', 'Noise Distribution', 'Real Noise', 'Gain Factor', 'Likelihood Of Improvement', 'Gaussian Noise Model', 'Training Set', 'Scaling Factor', 'Log-likelihood', 'Convolutional Layers', 'Negative Log-likelihood', 'Peak Signal-to-noise Ratio', 'Noise Amplification', 'Jacobian Determinant', 'Aspects Of Model', 'Noise Samples', 'Clear Image', 'Density Estimation', 'Noise Variance', 'Noise Sources']",,110,"Modeling and synthesizing image noise is an important aspect in many computer vision applications. The long-standing additive white Gaussian and heteroscedastic (signal-dependent) noise models widely used in the literature provide only a coarse approximation of real sensor noise. This paper introduces Noise Flow, a powerful and accurate noise model based on recent normalizing flow architectures. Noise Flow combines well-established basic parametric noise models (e.g., signal-dependent noise) with the flexibility and expressiveness of normalizing flow networks. The result is a single, comprehensive, compact noise model containing fewer than 2500 parameters yet able to represent multiple cameras and gain factors. Noise Flow dramatically outperforms existing noise models, with 0.42 nats/pixel improvement over the camera-calibrated noise level functions, which translates to 52% improvement in the likelihood of sampled noise. Noise Flow represents the first serious attempt to go beyond simple parametric models to one that leverages the power of deep learning and data-driven noise distributions."
Non-Local ConvLSTM for Video Compression Artifact Reduction,"Yi Xu, Longwen Gao, Kai Tian, Shuigeng Zhou, Huyang Sun","Bilibili, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai, China",50.0,China,50.0,China,"Video compression artifact reduction aims to recover high-quality videos from low-quality compressed videos. Most existing approaches use a single neighboring frame or a pair of neighboring frames (preceding and/or following the target frame) for this task. Furthermore, as frames of high quality overall may contain low-quality patches, and high-quality patches may exist in frames of low quality overall, current methods focusing on nearby peak-quality frames (PQFs) may miss high-quality details in low-quality frames. To remedy these shortcomings, in this paper we propose a novel end-to-end deep neural network called non-local ConvLSTM (NL-ConvLSTM in short) that exploits multiple consecutive frames. An approximate non-local strategy is introduced in NL-ConvLSTM to capture global motion patterns and trace the spatiotemporal dependency in a video sequence. This approximate strategy makes the non-local module work in a fast and low space-cost way. Our method uses the preceding and following frames of the target frame to generate a residual, from which a higher quality frame is reconstructed. Experiments on two datasets show that NL-ConvLSTM outperforms the existing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Non-Local_ConvLSTM_for_Video_Compression_Artifact_Reduction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Non-Local_ConvLSTM_for_Video_Compression_Artifact_Reduction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009524/,"['Spatiotemporal phenomena', 'Video compression', 'Feature extraction', 'Image coding', 'Task analysis', 'Visualization', 'Correlation']","['Artifact Reduction', 'Video Compression', 'Compression Artifacts', 'Image Compression Artifacts', 'Compression Artifacts Reduction', 'Neural Network', 'Deep Neural Network', 'Motion Patterns', 'Consecutive Frames', 'Video Sequences', 'Multiple Frames', 'Global Motion', 'Target Frame', 'Spatio-temporal Dependencies', 'Estimation Method', 'Convolutional Layers', 'Feature Maps', 'Video Frames', 'Hidden State', 'Sequence Of Frames', 'Motion Compensation', 'Structural Similarity Index Measure', 'Non-local Operation', 'Spatiotemporal Information', 'Motion Estimation', 'I-frame', 'Compression Algorithm', 'Yellow Block', 'JPEG Compression', 'Memory Cost']",,62,"Video compression artifact reduction aims to recover high-quality videos from low-quality compressed videos. Most existing approaches use a single neighboring frame or a pair of neighboring frames (preceding and/or following the target frame) for this task. Furthermore, as frames of high quality overall may contain low-quality patches, and high-quality patches may exist in frames of low quality overall, current methods focusing on nearby peak-quality frames (PQFs) may miss high-quality details in low-quality frames. To remedy these shortcomings, in this paper we propose a novel end-to-end deep neural network called non-local ConvLSTM (NL-ConvLSTM in short) that exploits multiple consecutive frames. An approximate non-local strategy is introduced in NL-ConvLSTM to capture global motion patterns and trace the spatiotemporal dependency in a video sequence. This approximate strategy makes the non-local module work in a fast and low space-cost way. Our method uses the preceding and following frames of the target frame to generate a residual, from which a higher quality frame is reconstructed. Experiments on two datasets show that NL-ConvLSTM outperforms the existing methods."
Non-Local Intrinsic Decomposition With Near-Infrared Priors,"Ziang Cheng, Yinqiang Zheng, Shaodi You, Imari Sato","Australian National University, Australia; National Institute of Informatics, Japan; Data61, CSIRO, Australia",66.66666666666666,"Australia, japan",33.33333333333334,Unknown,"Intrinsic image decomposition is a highly under-constrained problem that has been extensively studied by computer vision researchers. Previous methods impose additional constraints by exploiting either empirical or data-driven priors. In this paper, we revisit intrinsic image decomposition with the aid of near-infrared (NIR) imagery. We show that NIR band is considerably less sensitive to textures and can be exploited to reduce ambiguity caused by reflectance variation, promoting a simple yet powerful prior for shading smoothness. With this observation, we formulate intrinsic decomposition as an energy minimisation problem. Unlike existing methods, our energy formulation decouples reflectance and shading estimation, into a convex local shading component based on NIR-RGB image pair, and a reflectance component that encourages reflectance homogeneity both locally and globally. We further show the minimisation process can be approached by a series of multi-dimensional kernel convolutions, each within linear time complexity. To validate the proposed algorithm, a NIR-RGB dataset is captured over real-world objects, where our NIR-assisted approach demonstrates clear superiority over RGB methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Non-Local_Intrinsic_Decomposition_With_Near-Infrared_Priors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Non-Local_Intrinsic_Decomposition_With_Near-Infrared_Priors_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008789/,"['Image color analysis', 'Lighting', 'Cameras', 'Computer vision', 'Minimization', 'Shape', 'Training']","['Ambiguity', 'Convolution', 'Energy Minimization', 'Formation Energy', 'Image Pairs', 'Linear Time', 'Linear Complexity', 'Reflection Component', 'Computer Vision Research', 'Linear Time Complexity', 'Energy Minimization Problem', 'Illumination', 'Input Image', 'Kernel Function', 'Near-infrared Spectroscopy', 'Visual Comparison', 'RGB Images', 'Diffuse Reflectance', 'Global Energy', 'Reflectance Values', 'Near-infrared Imaging', 'Reflectance Images', 'Tea Bags', 'Specular Reflection', 'Line Search', 'Hard Constraints', 'White Balance', 'Intrinsic Component']",,28,"Intrinsic image decomposition is a highly under-constrained problem that has been extensively studied by computer vision researchers. Previous methods impose additional constraints by exploiting either empirical or data-driven priors. In this paper, we revisit intrinsic image decomposition with the aid of near-infrared (NIR) imagery. We show that NIR band is considerably less sensitive to textures and can be exploited to reduce ambiguity caused by reflectance variation, promoting a simple yet powerful prior for shading smoothness. With this observation, we formulate intrinsic decomposition as an energy minimisation problem. Unlike existing methods, our energy formulation decouples reflectance and shading estimation, into a convex local shading component based on NIR-RGB image pair, and a reflectance component that encourages reflectance homogeneity both locally and globally. We further show the minimisation process can be approached by a series of multi-dimensional kernel convolutions, each within linear time complexity. To validate the proposed algorithm, a NIR-RGB dataset is captured over real-world objects, where our NIR-assisted approach demonstrates clear superiority over RGB methods."
Non-Local Recurrent Neural Memory for Supervised Sequence Modeling,"Canmiao Fu, Wenjie Pei, Qiong Cao, Chaopeng Zhang, Yong Zhao, Xiaoyong Shen, Yu-Wing Tai","Tencent; School of ECE, Peking University",50.0,china,50.0,China,"Typical methods for supervised sequence modeling are built upon the recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model explicitly information interactions between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It greatly limits the capability of modeling the long-range temporal dependencies since one-order interactions cannot be maintained for a long term due to information dilution and gradient vanishing. To tackle this limitation, we propose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence modeling, which performs non-local operations to learn full-order interactions within a sliding temporal block and models the global interactions between blocks in a gated recurrent manner. Consequently, our model is able to capture the long-range dependencies. Besides, the latent high-level features contained in high-order interactions can be distilled by our model. We demonstrate the merits of our NRNM approach on two different tasks: action recognition and sentiment analysis.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Non-Local_Recurrent_Neural_Memory_for_Supervised_Sequence_Modeling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_Non-Local_Recurrent_Neural_Memory_for_Supervised_Sequence_Modeling_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008250/,,"['Non-local Memory', 'Time Step', 'Recurrent Network', 'Recurrent Neural Network', 'Sequential Steps', 'High-level Features', 'Action Recognition', 'Sentiment Analysis', 'Temporal Dependencies', 'Temporal Model', 'Latent Features', 'Vanishing Gradient', 'Long-range Dependencies', 'Global Interaction', 'Non-local Operation', 'Training Set', 'Input Features', 'Block Size', 'Graphical Model', 'Model Discrimination', 'Memory State', 'Memory Block', 'Gated Recurrent Unit', 'Adjacent Blocks', 'Hidden State', 'Sliding Window Size', 'Word Embedding', 'Sequence Segments', 'Hidden Representation', 'Conditional Random Field']",,5,"Typical methods for supervised sequence modeling are built upon the recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model explicitly information interactions between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It greatly limits the capability of modeling the long-range temporal dependencies since one-order interactions cannot be maintained for a long term due to information dilution and gradient vanishing. To tackle this limitation, we propose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence modeling, which performs non-local operations to learn full-order interactions within a sliding temporal block and models the global interactions between blocks in a gated recurrent manner. Consequently, our model is able to capture the long-range dependencies. Besides, the latent high-level features contained in high-order interactions can be distilled by our model. We demonstrate the merits of our NRNM approach on two different tasks: action recognition and sentiment analysis."
Normalized Wasserstein for Mixture Distributions With Applications in Adversarial Learning and Domain Adaptation,"Yogesh Balaji, Rama Chellappa, Soheil Feizi","UMIACS, University of Maryland; Department of Computer Science, University of Maryland",100.0,usa,0.0,,"Understanding proper distance measures between distributions is at the core of several learning tasks such as generative models, domain adaptation, clustering, etc. In this work, we focus on mixture distributions that arise naturally in several application domains where the data contains different sub-populations. For mixture distributions, established distance measures such as the Wasserstein distance do not take into account imbalanced mixture proportions. Thus, even if two mixture distributions have identical mixture components but different mixture proportions, the Wasserstein distance between them will be large. This often leads to undesired results in distance-based learning methods for mixture distributions. In this paper, we resolve this issue by introducing the Normalized Wasserstein measure. The key idea is to introduce mixture proportions as optimization variables, effectively normalizing mixture proportions in the Wasserstein formulation. Using the proposed normalized Wasserstein measure leads to significant performance gains for mixture distributions with imbalanced mixture proportions compared to the vanilla Wasserstein distance. We demonstrate the effectiveness of the proposed measure in GANs, domain adaptation and adversarial clustering in several benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008549/,"['Task analysis', 'Couplings', 'Probability distribution', 'Optimization', 'Generators', 'Noise measurement', 'Adaptation models']","['Generative Adversarial Networks', 'Domain Adaptation', 'Mixture Distribution', 'Distancing Measures', 'Learning Task', 'Key Idea', 'Applicability Domain', 'Mixture Components', 'Optimization Variables', 'Mixture Proportions', 'Bimodal', 'Entire Dataset', 'Mixture Model', 'Data Modalities', 'Gaussian Mixture Model', 'Target Domain', 'Mode Of Distribution', 'Source Distribution', 'Imbalanced Datasets', 'Source Domain', 'Machine Learning Problems', 'Source Dataset', 'Statistical Problems', 'Unsupervised Tasks', 'Material For More Details', 'Target Dataset', 'Mode Components', 'MNIST Dataset', 'Divergence Measure']",,26,"Understanding proper distance measures between distributions is at the core of several learning tasks such as generative models, domain adaptation, clustering, etc. In this work, we focus on mixture distributions that arise naturally in several application domains where the data contains different sub-populations. For mixture distributions, established distance measures such as the Wasserstein distance do not take into account imbalanced mixture proportions. Thus, even if two mixture distributions have identical mixture components but different mixture proportions, the Wasserstein distance between them will be large. This often leads to undesired results in distance-based learning methods for mixture distributions. In this paper, we resolve this issue by introducing the Normalized Wasserstein measure. The key idea is to introduce mixture proportions as optimization variables, effectively normalizing mixture proportions in the Wasserstein formulation. Using the proposed normalized Wasserstein measure leads to significant performance gains for mixture distributions with imbalanced mixture proportions compared to the vanilla Wasserstein distance. We demonstrate the effectiveness of the proposed measure in GANs, domain adaptation and adversarial clustering in several benchmark datasets."
Not All Parts Are Created Equal: 3D Pose Estimation by Modeling Bi-Directional Dependencies of Body Parts,"Jue Wang, Shaoli Huang, Xinchao Wang, Dacheng Tao","Stevens Institute of Technology; UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Darlington, NSW 2008, Australia",100.0,"australia, usa",0.0,,"Not all the human body parts have the same degree of freedom (DOF) due to the physiological structure. For example, the limbs may move more flexibly and freely than the torso does. Most of the existing 3D pose estimation methods, despite the very promising results achieved, treat the body joints equally and consequently often lead to larger reconstruction errors on the limbs. In this paper, we propose a progressive approach that explicitly accounts for the distinct DOFs among the body parts. We model parts with higher DOFs like the elbows, as dependent components of the corresponding parts with lower DOFs like the torso, of which the 3D locations can be more reliably estimated. Meanwhile, the high-DOF parts may, in turn, impose a constraint on where the low-DOF ones lie. As a result, parts with different DOFs supervise one another, yielding physically constrained and plausible pose-estimation results. To further facilitate the prediction of the high-DOF parts, we introduce a pose-attribution estimation, where the relative location of a limb joint with respect to the torso, which has the least DOF of a human body, is explicitly estimated and further fed to the joint-estimation module. The proposed approach achieves very promising results, outperforming the state of the art on several benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Not_All_Parts_Are_Created_Equal_3D_Pose_Estimation_by_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Not_All_Parts_Are_Created_Equal_3D_Pose_Estimation_by_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010034/,"['Three-dimensional displays', 'Two dimensional displays', 'Pose estimation', 'Torso', 'Solid modeling', 'Bidirectional control', 'Joints']","['Body Parts', 'Pose Estimation', 'Human Pose Estimation', '3D Pose', 'Bidirectional Dependency', 'Degrees Of Freedom', 'Joint Position', 'High Degree Of Freedom', 'Limb Joint', 'Body Joints', 'Low Degrees Of Freedom', 'Learning Rate', 'Quantitative Results', 'Generalization Ability', 'Long Short-term Memory', 'Domain Shift', '3D Network', 'Domain Adaptation', '2D Pose', 'Multi-task Network', 'Proximal Joints', 'Domain Adaptation Methods', 'Distal Joint', 'Ground Truth 3D', 'One-stage Approach', 'Imaging Evidence', 'Human Pose', 'Domain Classifier']",,35,"Not all the human body parts have the same degree of freedom (DOF) due to the physiological structure. For example, the limbs may move more flexibly and freely than the torso does. Most of the existing 3D pose estimation methods, despite the very promising results achieved, treat the body joints equally and consequently often lead to larger reconstruction errors on the limbs. In this paper, we propose a progressive approach that explicitly accounts for the distinct DOFs among the body parts. We model parts with higher DOFs like the elbows, as dependent components of the corresponding parts with lower DOFs like the torso, of which the 3D locations can be more reliably estimated. Meanwhile, the high-DOF parts may, in turn, impose a constraint on where the low-DOF ones lie. As a result, parts with different DOFs supervise one another, yielding physically constrained and plausible pose-estimation results. To further facilitate the prediction of the high-DOF parts, we introduce a pose-attribution estimation, where the relative location of a limb joint with respect to the torso, which has the least DOF of a human body, is explicitly estimated and further fed to the joint-estimation module. The proposed approach achieves very promising results, outperforming the state of the art on several benchmarks."
O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks,"Jinchi Huang, Lie Qu, Rongfei Jia, Binqiang Zhao","Alibaba Group, Hangzhou, China",0.0,,100.0,China,"This paper proposes a novel noisy label detection approach, named O2U-net, for deep neural networks without human annotations. Different from prior work which requires specifically designed noise-robust loss functions or networks, O2U-net is easy to implement but effective. It only requires adjusting the hyper-parameters of the deep network to make its status transfer from overfitting to underfitting (O2U) cyclically. The losses of each sample are recorded during iterations. The higher the normalized average loss of a sample, the higher the probability of being noisy labels. O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels. We conduct sufficient experiments on multiple datasets in various settings. The experimental results prove the state-of-the-art of O2S-net.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008796/,"['Noise measurement', 'Training', 'Neural networks', 'Noise robustness', 'Industries', 'Computational modeling', 'Computer vision']","['Neural Network', 'Deep Neural Network', 'Noisy Labels', 'Noisy Label Detection', 'Deep Network', 'Active Learning', 'Sample Loss', 'Underfitting', 'Learning Rate', 'Validation Set', 'Batch Size', 'Image Classification', 'Final Classification', 'Network State', 'Cycle Length', 'Training Loss', 'Precision-recall Curve', 'Clean Samples', 'Curriculum Learning', 'Label Noise', 'Learning Rate Set', 'Constant Learning Rate', 'Synthetic Noise', 'Impact Of Labels', 'Beginning Of Training', 'Clean Dataset', 'Difficulty Of Sampling', 'Transfer Learning', 'Constant Learning']",,132,"This paper proposes a novel noisy label detection approach, named O2U-net, for deep neural networks without human annotations. Different from prior work which requires specifically designed noise-robust loss functions or networks, O2U-net is easy to implement but effective. It only requires adjusting the hyper-parameters of the deep network to make its status transfer from overfitting to underfitting (O2U) cyclically. The losses of each sample are recorded during iterations. The higher the normalized average loss of a sample, the higher the probability of being noisy labels. O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels. We conduct sufficient experiments on multiple datasets in various settings. The experimental results prove the state-of-the-art of O2S-net."
Object Guided External Memory Network for Video Object Detection,"Hanming Deng, Yang Hua, Tao Song, Zongpu Zhang, Zhengui Xue, Ruhui Ma, Neil Robertson, Haibing Guan",Queen’s University Belfast; Shanghai Jiao Tong University,100.0,"China, uk",0.0,,"Video object detection is more challenging than image object detection because of the deteriorated frame quality. To enhance the feature representation, state-of-the-art methods propagate temporal information into the deteriorated frame by aligning and aggregating entire feature maps from multiple nearby frames. However, restricted by feature map's low storage-efficiency and vulnerable content-address allocation, long-term temporal information is not fully stressed by these methods. In this work, we propose the first object guided external memory network for online video object detection. Storage-efficiency is handled by object guided hard-attention to selectively store valuable features, and long-term information is protected when stored in an addressable external data matrix. A set of read/write operations are designed to accurately propagate/allocate and delete multi-level memory feature under object guidance. We evaluate our method on the ImageNet VID dataset and achieve state-of-the-art performance as well as good speed-accuracy tradeoff. Furthermore, by visualizing the external memory, we show the detailed object-level reasoning process across frames.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Object_Guided_External_Memory_Network_for_Video_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Object_Guided_External_Memory_Network_for_Video_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9011008/,"['Feature extraction', 'Object detection', 'Detectors', 'Cognition', 'Task analysis', 'Streaming media', 'Proposals']","['Object Detection', 'External Networks', 'Object Detection Network', 'External Memory', 'Video Object Detection', 'Feature Maps', 'Temporal Information', 'Good Trade-off', 'Multiple Frames', 'Speed-accuracy Trade-off', 'Long-term Information', 'Neural Network', 'Convolutional Neural Network', 'Level Characteristics', 'Deep Convolutional Neural Network', 'Bounding Box', 'Inference Time', 'Pixel Level', 'Redundant Features', 'Current Frame', 'Different Levels Of Features', 'Read Operation', 'Memory Size', 'Key Frames', 'Convolutional Feature Maps', 'Attention Weights', 'Internal Memory', 'High-quality Features', 'Object Memory', 'Hard Examples']",,81,"Video object detection is more challenging than image object detection because of the deteriorated frame quality. To enhance the feature representation, state-of-the-art methods propagate temporal information into the deteriorated frame by aligning and aggregating entire feature maps from multiple nearby frames. However, restricted by feature map's low storage-efficiency and vulnerable content-address allocation, long-term temporal information is not fully stressed by these methods. In this work, we propose the first object guided external memory network for online video object detection. Storage-efficiency is handled by object guided hard-attention to selectively store valuable features, and long-term information is protected when stored in an addressable external data matrix. A set of read/write operations are designed to accurately propagate/allocate and delete multi-level memory feature under object guidance. We evaluate our method on the ImageNet VID dataset and achieve state-of-the-art performance as well as good speed-accuracy tradeoff. Furthermore, by visualizing the external memory, we show the detailed object-level reasoning process across frames."
Object-Aware Instance Labeling for Weakly Supervised Object Detection,"Satoshi Kosugi, Toshihiko Yamasaki, Kiyoharu Aizawa","The University of Tokyo, Japan",100.0,Japan,0.0,,"Weakly supervised object detection (WSOD), where a detector is trained with only image-level annotations, is attracting more and more attention. As a method to obtain a well-performing detector, the detector and the instance labels are updated iteratively. In this study, for more efficient iterative updating, we focus on the instance labeling problem, a problem of which label should be annotated to each region based on the last localization result. Instead of simply labeling the top-scoring region and its highly overlapping regions as positive and others as negative, we propose more effective instance labeling methods as follows. First, to solve the problem that regions covering only some parts of the object tend to be labeled as positive, we find regions covering the whole object focusing on the context classification loss. Second, considering the situation where the other objects contained in the image can be labeled as negative, we impose a spatial restriction on regions labeled as negative. Using these instance labeling methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results compared with other state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kosugi_Object-Aware_Instance_Labeling_for_Weakly_Supervised_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kosugi_Object-Aware_Instance_Labeling_for_Weakly_Supervised_Object_Detection_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010726/,"['Labeling', 'Detectors', 'Proposals', 'Training', 'Object detection', 'Iterative methods', 'Focusing']","['Object Detection', 'Instance Labels', 'Weakly Supervised Object Detection', 'Classification Loss', 'Labeling Method', 'Object Parts', 'Spatial Limitations', 'Iterative Update', 'Convolutional Neural Network', 'Intersection Over Union', 'Multiple Classes', 'Bounding Box', 'Imaging Center', 'Class Probabilities', 'Training Loss', 'Regional Context', 'Classification Output', 'Global Average Pooling', 'Mean Average Precision', 'Class Instances', 'Convolutional Neural Network Features', 'Negative Labels', 'Outside Of Regions', 'Region Proposal', 'Beginning Of Training', 'Confidence Region', 'Mean Pixel Value', 'Image-level Labels', 'Faster R-CNN', 'Local Optimum']",,41,"Weakly supervised object detection (WSOD), where a detector is trained with only image-level annotations, is attracting more and more attention. As a method to obtain a well-performing detector, the detector and the instance labels are updated iteratively. In this study, for more efficient iterative updating, we focus on the instance labeling problem, a problem of which label should be annotated to each region based on the last localization result. Instead of simply labeling the top-scoring region and its highly overlapping regions as positive and others as negative, we propose more effective instance labeling methods as follows. First, to solve the problem that regions covering only some parts of the object tend to be labeled as positive, we find regions covering the whole object focusing on the context classification loss. Second, considering the situation where the other objects contained in the image can be labeled as negative, we impose a spatial restriction on regions labeled as negative. Using these instance labeling methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results compared with other state-of-the-art approaches."
Object-Driven Multi-Layer Scene Decomposition From a Single Image,"Helisa Dhamo, Nassir Navab, Federico Tombari","Technische Universität München; Technische Universität München, Google",100.0,"germany, germany, usa",0.0,,"We present a method that tackles the challenge of predicting color and depth behind the visible content of an image. Our approach aims at building up a Layered Depth Image (LDI) from a single RGB input, which is an efficient representation that arranges the scene in layers, including originally occluded regions. Unlike previous work, we enable an adaptive scheme for the number of layers and incorporate semantic encoding for better hallucination of partly occluded objects. Additionally, our approach is object-driven, which especially boosts the accuracy for the occluded intermediate objects. The framework consists of two steps. First, we individually complete each object in terms of color and depth, while estimating the scene layout. Second, we rebuild the scene based on the regressed layers and enforce the recomposed image to resemble the structure of the original input. The learned representation enables various applications, such as 3D photography and diminished reality, all from a single RGB image.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dhamo_Object-Driven_Multi-Layer_Scene_Decomposition_From_a_Single_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dhamo_Object-Driven_Multi-Layer_Scene_Decomposition_From_a_Single_Image_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008295/,"['Three-dimensional displays', 'Layout', 'Semantics', 'Color', 'Solid modeling', 'Rendering (computer graphics)', 'Image color analysis']","['Single Image', 'Semantic', '3D Images', 'RGB Images', 'Depth Images', 'Occluded Objects', 'Occluded Regions', 'Image Object', 'Large-scale Datasets', 'Color Images', 'Depth Map', 'Classification Categories', 'Conditional Random Field', 'Visible Part', '3D Scene', 'Single View', 'Mask R-CNN', 'Inpainting', 'Representation Layer', 'Object Instances', 'View Synthesis', 'Depth Prediction', 'Scene Representation', 'Visual Point', 'Target View', 'Occlusion Level', 'Original View', 'Object Appearance', 'Object Removal', 'Root Mean Square Error']",,19,"We present a method that tackles the challenge of predicting color and depth behind the visible content of an image. Our approach aims at building up a Layered Depth Image (LDI) from a single RGB input, which is an efficient representation that arranges the scene in layers, including originally occluded regions. Unlike previous work, we enable an adaptive scheme for the number of layers and incorporate semantic encoding for better hallucination of partly occluded objects. Additionally, our approach is object-driven, which especially boosts the accuracy for the occluded intermediate objects. The framework consists of two steps. First, we individually complete each object in terms of color and depth, while estimating the scene layout. Second, we rebuild the scene based on the regressed layers and enforce the recomposed image to resemble the structure of the original input. The learned representation enables various applications, such as 3D photography and diminished reality, all from a single RGB image."
"Objects365: A Large-Scale, High-Quality Dataset for Object Detection","Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, Jian Sun",Megvii Technology,0.0,,100.0,China,"In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www.objects365.org.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf,www.objects365.org,,,main,Poster,https://ieeexplore.ieee.org/document/9009553/,"['Object detection', 'Benchmark testing', 'Pipelines', 'Training', 'Task analysis', 'Detectors', 'Clocks']","['Object Detection', 'High-quality Dataset', 'Object Detection Dataset', 'Training Time', 'Generalization Ability', 'Feature Learning', 'Bounding Box', 'Semantic Segmentation', 'Annotation Pipeline', 'ImageNet Pretraining', 'ImageNet Pre-trained Model', 'Deep Learning', 'Fine-tuned', 'Learning Rate', 'Convolutional Neural Network', 'Annotation Process', 'Classification Rules', 'Pre-trained Weights', 'Beginning Of Training', 'Baseline Algorithms', 'Pedestrian Detection', 'Object Instances', 'Consistent Rules', 'Middle Figure', 'COCO Dataset', 'Two-stage Detectors', 'Annotation Errors']",,315,"In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www.objects365.org."
Occlusion Robust Face Recognition Based on Mask Learning With Pairwise Differential Siamese Network,"Lingxue Song, Dihong Gong, Zhifeng Li, Changsong Liu, Wei Liu",Tencent AI Lab; Tsinghua University,50.0,China,50.0,China,"Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of face recognition over past years. However, existing CNN models are far less accurate when handling partially occluded faces. These general face models generalize poorly for occlusions on variable facial areas. Inspired by the fact that human visual system explicitly ignores the occlusion and only focuses on the non-occluded facial areas, we propose a mask learning strategy to find and discard corrupted feature elements from recognition. A mask dictionary is firstly established by exploiting the differences between the top conv features of occluded and occlusion-free face pairs using innovatively designed pairwise differential siamese network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements from recognition. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed algorithm significantly outperforms the state-of-the-art systems.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Song_Occlusion_Robust_Face_Recognition_Based_on_Mask_Learning_With_Pairwise_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Song_Occlusion_Robust_Face_Recognition_Based_on_Mask_Learning_With_Pairwise_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009826/,"['Face', 'Face recognition', 'Generators', 'Dictionaries', 'Frequency division multiplexing', 'Feature extraction', 'Robustness']","['Face Recognition', 'Differential Network', 'Robust Recognition', 'Pairwise Network', 'Convolutional Neural Network', 'Deep Convolutional Neural Network', 'Face Images', 'Characteristics Of Elements', 'Face Area', 'Human Visual System', 'Top Features', 'Partial Occlusion', 'Face Pairs', 'Training Set', 'Support Vector Machine', 'Facial Expressions', 'Test Phase', 'Convolutional Neural Network Model', 'Final Feature', 'Differential Signal', 'FC Layer', 'Facial Components', 'Classification Loss', 'Sunglasses', 'Deep Features', 'Gallery Set', 'Occlusion Location', 'Occluded Objects', 'Contrastive Loss', 'Facial Features']",,176,"Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of face recognition over past years. However, existing CNN models are far less accurate when handling partially occluded faces. These general face models generalize poorly for occlusions on variable facial areas. Inspired by the fact that human visual system explicitly ignores the occlusion and only focuses on the non-occluded facial areas, we propose a mask learning strategy to find and discard corrupted feature elements from recognition. A mask dictionary is firstly established by exploiting the differences between the top conv features of occluded and occlusion-free face pairs using innovatively designed pairwise differential siamese network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements from recognition. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed algorithm significantly outperforms the state-of-the-art systems."
Occlusion-Aware Networks for 3D Human Pose Estimation in Video,"Yu Cheng, Bo Yang, Bo Wang, Wending Yan, Robby T. Tan",National University of Singapore; National University of Singapore and Yale-NUS College; Tencent Game AI Research Center,66.66666666666666,singapore,33.33333333333334,China,"Occlusion is a key problem in 3D human pose estimation from a monocular video. To address this problem, we introduce an occlusion-aware deep-learning framework. By employing estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint, we filter out the unreliable estimations of occluded keypoints. When occlusion occurs, we have incomplete 2D keypoints and feed them to our 2D and 3D temporal convolutional networks (2D and 3D TCNs) that enforce temporal smoothness to produce a complete 3D pose. By using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints. Training the occlusion-aware 3D TCN requires pairs of a 3D pose and a 2D pose with occlusion labels. As no such a dataset is available, we introduce a ""Cylinder Man Model"" to approximate the occupation of body parts in 3D space. By projecting the model onto a 2D plane in different viewing angles, we obtain and label the occluded keypoints, providing us plenty of training data. In addition, we use this model to create a pose regularization constraint, preferring the 2D estimations of unreliable keypoints to be occluded. Our method outperforms state-of-the-art methods on Human 3.6M and HumanEva-I datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010921/,"['Three-dimensional displays', 'Two dimensional displays', 'Pose estimation', 'Heating systems', 'Feeds', 'Training']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Estimation In Videos', 'Video Pose Estimation', '3D Space', 'Viewing Angle', 'Optical Flow', '2D Plane', '3D Pose', 'Temporal Smoothing', 'Temporal Convolutional Network', '2D Pose', '2D Keypoints', 'Public Datasets', 'Data Augmentation', 'Bounding Box', 'Temporal Information', 'Fourth Row', 'Ground Truth 3D', '3D Joint', '3D Datasets', 'Form Of A Heatmap', 'Confidence Map', '3D Skeleton', 'Human Joint', 'Accurate 3D', '2D Space', 'Pose Estimation Methods']",,124,"Occlusion is a key problem in 3D human pose estimation from a monocular video. To address this problem, we introduce an occlusion-aware deep-learning framework. By employing estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint, we filter out the unreliable estimations of occluded keypoints. When occlusion occurs, we have incomplete 2D keypoints and feed them to our 2D and 3D temporal convolutional networks (2D and 3D TCNs) that enforce temporal smoothness to produce a complete 3D pose. By using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints. Training the occlusion-aware 3D TCN requires pairs of a 3D pose and a 2D pose with occlusion labels. As no such a dataset is available, we introduce a ``Cylinder Man Model'' to approximate the occupation of body parts in 3D space. By projecting the model onto a 2D plane in different viewing angles, we obtain and label the occluded keypoints, providing us plenty of training data. In addition, we use this model to create a pose regularization constraint, preferring the 2D estimations of unreliable keypoints to be occluded. Our method outperforms state-of-the-art methods on Human 3.6M and HumanEva-I datasets."
Occlusion-Shared and Feature-Separated Network for Occlusion Relationship Reasoning,"Rui Lu, Feng Xue, Menghan Zhou, Anlong Ming, Yu Zhou","Beijing University of Posts and Telecommunications, Beijing, China; Huazhong University of Science and Technology, Wuhan, China",100.0,"China, china",0.0,,"Occlusion relationship reasoning demands closed contour to express the object, and orientation of each contour pixel to describe the order relationship between objects. Current CNN-based methods neglect two critical issues of the task: (1) simultaneous existence of the relevance and distinction for the two elements, i.e, occlusion edge and occlusion orientation; and (2) inadequate exploration to the orientation features. For the reasons above, we propose the Occlusion-shared and Feature-separated Network (OFNet). On one hand, considering the relevance between edge and orientation, two sub-networks are designed to share the occlusion cue. On the other hand, the whole network is split into two paths to learn the high semantic features separately. Moreover, a contextual feature for orientation prediction is extracted, which represents the bilateral cue of the foreground and background areas. The bilateral cue is then fused with the occlusion cue to precisely locate the object regions. Finally, a stripe convolution is designed to further aggregate features from surrounding scenes of the occlusion edge. The proposed OFNet remarkably advances the state-of-the-art approaches on PIOD and BSDS ownership dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lu_Occlusion-Shared_and_Feature-Separated_Network_for_Occlusion_Relationship_Reasoning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Occlusion-Shared_and_Feature-Separated_Network_for_Occlusion_Relationship_Reasoning_ICCV_2019_paper.pdf,,https://github.com/buptlr/OFNet,,main,Poster,https://ieeexplore.ieee.org/document/9009468/,"['Convolution', 'Feature extraction', 'Cognition', 'Decoding', 'Image edge detection', 'Visualization', 'Task analysis']","['Occlusal Relationship', 'High-level Features', 'Object Regions', 'Specific Characteristics', 'Convolutional Neural Network', 'Horizontal Plane', 'Feature Maps', 'Vertical Direction', 'Localization Accuracy', 'Receptive Field', 'Convolution Kernel', 'Low-level Features', 'Convolutional Block', 'Dilated Convolution', 'Large Receptive Field', 'Multi-level Features', 'Foreground Objects', 'Proportion Of Features', 'Background Objects', 'Atrous Spatial Pyramid Pooling', 'Edge Path', 'Orientation Maps']",,16,"Occlusion relationship reasoning demands closed contour to express the object, and orientation of each contour pixel to describe the order relationship between objects. Current CNN-based methods neglect two critical issues of the task: (1) simultaneous existence of the relevance and distinction for the two elements, i.e, occlusion edge and occlusion orientation; and (2) inadequate exploration to the orientation features. For the reasons above, we propose the Occlusion-shared and Feature-separated Network (OFNet). On one hand, considering the relevance between edge and orientation, two sub-networks are designed to share the occlusion cue. On the other hand, the whole network is split into two paths to learn the high semantic features separately. Moreover, a contextual feature for orientation prediction is extracted, which represents the bilateral cue of the foreground and background areas. The bilateral cue is then fused with the occlusion cue to precisely locate the object regions. Finally, a stripe convolution is designed to further aggregate features from surrounding scenes of the occlusion edge. The proposed OFNet remarkably advances the state-of-the-art approaches on PIOD and BSDS ownership dataset."
Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger","Autonomous Vision Group, MPI for Intelligent Systems and University of Tübingen, ETAS GmbH, Bosch Group, Stuttgart; Autonomous Vision Group, MPI for Intelligent Systems and University of Tübingen",100.0,germany,0.0,,"Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while state-of-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008276/,"['Three-dimensional displays', 'Shape', 'Geometry', 'Task analysis', 'Image reconstruction', 'Interpolation', 'Topology']","['4D Reconstruction', 'Time And Space', 'Deep Learning', 'Time Step', 'Points In Space', 'Point Cloud', 'Vector Field', '3D Representation', '3D Geometry', 'Template Model', 'Hypervolume', 'Reconstruction Task', 'Representation Of Geometry', 'Sparse Point Cloud', 'Continuous 3D', 'Coordinate System', 'Quantitative Results', 'Qualitative Results', 'Ordinary Differential Equations', '3D Point', '3D Shape', 'Temporal Coding', 'Continuous Trajectory', 'Shape Representation', 'Spatial Encoding', 'Reconstruction Loss', 'Chamfer Distance', '3D Motion', 'Temporal Correspondence', 'Arbitrary Topology']",,167,"Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while state-of-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks."
Omni-Scale Feature Learning for Person Re-Identification,"Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang","University of Surrey; Queen Mary University of London; University of Surrey, Samsung AI Center, Cambridge",100.0,uk,0.0,,"As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We callse features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional feature streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses both pointwise and depthwise convolutions. By stacking such blocks layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, our OSNet achieves state-of-the-art performance on six person-ReID datasets. Code and models are available at: https://github.com/KaiyangZhou/deep-person-reid.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Omni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Omni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf,,https://github.com/KaiyangZhou/deep-person-reid,,main,Poster,https://ieeexplore.ieee.org/document/9011001/,"['Logic gates', 'Task analysis', 'Cameras', 'Feature extraction', 'Surveillance', 'Streaming media', 'Convolution']","['Feature Learning', 'Convolutional Neural Network', 'Building Blocks', 'Model Size', 'Deep Convolutional Neural Network', 'Residual Block', 'Multi-scale Features', 'Convolutional Features', 'Multiple Streams', 'Depthwise Convolution', 'Stream Characteristics', 'Smaller Model Size', 'Deep Learning', 'Convolutional Layers', 'Receptive Field', 'Recognition Task', 'Multilayer Perceptron', 'Training Images', 'Characteristic Scale', 'Primary Model', 'Lightweight Convolutional Neural Network', 'ImageNet Pretraining', 'Depthwise Separable Convolution', 'FC Layer', 'Standard Convolution', 'Channel Width', 'Lightweight Network', 'Feature Channels', 'Broad Range Of Tasks', 'Nice Properties']",,584,"As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We callse features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional feature streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses both pointwise and depthwise convolutions. By stacking such blocks layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, our OSNet achieves state-of-the-art performance on six person-ReID datasets. Code and models are available at: https://github.com/KaiyangZhou/deep-person-reid."
OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching,"Changhee Won, Jongbin Ryu, Jongwoo Lim","Department of Computer Science, Hanyang University, Seoul, Korea.",100.0,south korea,0.0,,"In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra wide field-of-view (FOV) cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce the omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 11K ground-truth depth maps and 45K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Won_OmniMVS_End-to-End_Learning_for_Omnidirectional_Stereo_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Won_OmniMVS_End-to-End_Learning_for_Omnidirectional_Stereo_Matching_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010863,"['Cameras', 'Feature extraction', 'Three-dimensional displays', 'Estimation', 'Neural networks', 'Machine learning', 'Computational modeling']","['Stereo Matching', 'Deep Neural Network', 'Feature Maps', 'Large-scale Datasets', 'Global Information', 'Deep Features', 'Depth Map', 'Real-world Environments', 'Depth Estimation', 'Ultra-wide', 'Global Context Information', 'Feature Volume', 'Ground Truth Depth', 'Multi-view Stereo', 'Concentric Spheres', 'Deep Learning', 'Convolutional Neural Network', 'Input Image', 'Real-world Data', 'Image Pairs', 'Cost Volume', 'Multiple Cameras', 'Number Of Spheres', 'Spherical Image', 'Matching Cost', 'Cost Aggregation', 'Network Input', 'Normalized Cross-correlation', 'Surface Reflectance', 'Inverse Index']",,37,"In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra wide field-of-view (FOV) cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce the omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 11K ground-truth depth maps and 45K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms."
On Boosting Single-Frame 3D Human Pose Estimation via Monocular Videos,"Zhi Li, Xuan Wang, Fei Wang, Peilin Jiang","School of Software Engineering, Xi’an Jiaotong University, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, China",100.0,china,0.0,,"The premise of training an accurate 3D human pose estimation network is the possession of huge amount of richly annotated training data. Nonetheless, manually obtaining rich and accurate annotations is, even not impossible, tedious and slow. In this paper, we propose to exploit monocular videos to complement the training dataset for the single-image 3D human pose estimation tasks. At the beginning, a baseline model is trained with a small set of annotations. By fixing some reliable estimations produced by the resulting model, our method automatically collects the annotations across the entire video as solving the 3D trajectory completion problem. Then, the baseline model is further trained with the collected annotations to learn the new poses. We evaluate our method on the broadly-adopted Human3.6M and MPI-INF-3DHP datasets. As illustrated in experiments, given only a small set of annotations, our method successfully makes the model to learn new poses from unlabelled monocular videos, promoting the accuracies of the baseline model by about 10%. By contrast with previous approaches, our method does not rely on either multi-view imagery or any explicit 2D keypoint annotations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_On_Boosting_Single-Frame_3D_Human_Pose_Estimation_via_Monocular_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_On_Boosting_Single-Frame_3D_Human_Pose_Estimation_via_Monocular_Videos_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010633/,"['Three-dimensional displays', 'Two dimensional displays', 'Videos', 'Training', 'Pose estimation', 'Video sequences', 'Feature extraction']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'Monocular Video', 'Training Data', 'Training Dataset', '3D Network', 'Accurate 3D', 'Annotated Training', 'Entire Video', '3D Pose', '3D Tasks', 'Annotated Training Data', '2D Keypoints', 'Network Training', 'Network Performance', 'Annotation Data', 'Manual Annotation', 'Video Sequences', 'Multi-view Images', 'Two-stage Framework', 'Multi-camera System', '2D Pose', 'Automatic Collection', '3D Prediction', 'Unannotated Sequences', 'Consistency Loss', 'Computer Graphics']",,32,"The premise of training an accurate 3D human pose estimation network is the possession of huge amount of richly annotated training data. Nonetheless, manually obtaining rich and accurate annotations is, even not impossible, tedious and slow. In this paper, we propose to exploit monocular videos to complement the training dataset for the single-image 3D human pose estimation tasks. At the beginning, a baseline model is trained with a small set of annotations. By fixing some reliable estimations produced by the resulting model, our method automatically collects the annotations across the entire video as solving the 3D trajectory completion problem. Then, the baseline model is further trained with the collected annotations to learn the new poses. We evaluate our method on the broadly-adopted Human3.6M and MPI-INF-3DHP datasets. As illustrated in experiments, given only a small set of annotations, our method successfully makes the model to learn new poses from unlabelled monocular videos, promoting the accuracies of the baseline model by about 10%. By contrast with previous approaches, our method does not rely on either multi-view imagery or any explicit 2D keypoint annotations."
On Network Design Spaces for Visual Recognition,"Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, Piotr DollÃ¡r",Facebook AI Research (FAIR),0.0,,100.0,USA,"Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Radosavovic_On_Network_Design_Spaces_for_Visual_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Radosavovic_On_Network_Design_Spaces_for_Visual_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010853/,"['Aerospace electronics', 'Complexity theory', 'Network architecture', 'Neural networks', 'Robustness', 'Visualization', 'Standards']","['Design Space', 'Neural Network', 'Point Estimates', 'Neural Architecture', 'Confounding Factors', 'Curve Estimation', 'Complex Ones', 'Neural Architecture Search', 'Empirical Studies', 'Complex Models', 'Kolmogorov-Smirnov Test', 'Recent Paper', 'Set Of Models', 'Cell Structure', 'Search Algorithm', 'Statistical Distribution', 'ImageNet', 'Minimum Error', 'Vanilla', 'Complex Distribution', 'Fractional Model', 'Random Search', 'Sanity Check', 'Continuous Progress', 'ResNet Model', 'Hyperparameter Search', 'Classical Statistics', 'Residual Connection', 'Data Generation', 'Network Depth']",,52,"Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition."
On the Design of Black-Box Adversarial Examples by Leveraging Gradient-Free Optimization and Operator Splitting Method,"Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, Xue Lin","Syracuse University; MIT-IBM Watson AI Lab, IBM Research; Department of Electrical and Computer Engineering, Northeastern University",100.0,"china, usa",0.0,,"Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_On_the_Design_of_Black-Box_Adversarial_Examples_by_Leveraging_Gradient-Free_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_On_the_Design_of_Black-Box_Adversarial_Examples_by_Leveraging_Gradient-Free_ICCV_2019_paper.pdf,,https://github.com/LinLabNEU/Blackbox_ADMM,,main,Poster,https://ieeexplore.ieee.org/document/9009069/,"['Convex functions', 'Distortion', 'Perturbation methods', 'Optimization', 'Measurement', 'Complexity theory', 'Estimation']","['Adversarial Examples', 'Operator Splitting Method', 'Black-box Adversarial Examples', 'General Framework', 'Target Model', 'Bayesian Optimization', 'Attack Success', 'Adversarial Attacks', 'Threat Model', 'Attack Methods', 'Attack Success Rate', 'Black-box Attacks', 'Loss Function', 'Alternative Models', 'Deep Neural Network', 'Gradient Descent', 'Lagrange Multiplier', 'ImageNet', 'Stochastic Gradient Descent', 'White-box Attack', 'Adversarial Perturbations', 'Gradient Approximation', 'Bregman Divergence', 'Gradient Descent Method', 'Deep Neural Network Model', 'Query Efficiency', 'Discrete Output', 'Target Class', 'Gaussian Process']",,23,"Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates."
On the Efficacy of Knowledge Distillation,"Jang Hyun Cho, Bharath Hariharan",Cornell University,100.0,usa,0.0,,"In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don't make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher's training early. Our results generalize across datasets and models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008764/,"['Training', 'Neural networks', 'Standards', 'Computer architecture', 'Knowledge engineering', 'Computational modeling', 'Probability distribution']","['Sequential Steps', 'Neural Network', 'Learning Rate', 'Hyperparameters', 'Large Networks', 'Cross-entropy Loss', 'Transfer Learning', 'Class Labels', 'Teacher Training', 'ImageNet', 'Early Stopping', 'Popular Choice', 'End Of Training', 'Student Model', 'Teacher Network', 'Teacher Capacity', 'Student Network', 'Total Number Of Epochs']",,304,"In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don't make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher's training early. Our results generalize across datasets and models."
On the Global Optima of Kernelized Adversarial Representation Learning,"Bashir Sadeghi, Runyi Yu, Vishnu Boddeti",Michigan State University; Eastern Mediterranean University,100.0,"Cyprus, usa",0.0,,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ""linear"" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ""imparting"" provable invariance to any biased pre-trained data representation, and (b) the global optima of the ""kernel"" form can provide a comparable trade-off between utility and invariance in comparison to iterative minimax optimization of existing deep neural network based approaches, but with provable guarantees.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sadeghi_On_the_Global_Optima_of_Kernelized_Adversarial_Representation_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sadeghi_On_the_Global_Optima_of_Kernelized_Adversarial_Representation_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010831/,"['Optimization', 'Task analysis', 'Kernel', 'Closed-form solutions', 'Data mining', 'Gallium nitride', 'Convergence']","['Global Optimization', 'Generative Adversarial Networks', 'Representation Learning', 'Adversarial Representation Learning', 'Optimization Problem', 'Linear Function', 'Deep Neural Network', 'Nonlinear Function', 'Numerical Experiments', 'Sensitive Attributes', 'Minimax Optimization', 'Resulting Optimization Problem', 'Training Set', 'Covariance Matrix', 'Gradient Descent', 'Identity Matrix', 'Parameter Space', 'Local Optimum', 'Stochastic Gradient Descent', 'Target Task', 'Linear Encoder', 'Global Solution', 'Optimistic Perspective', 'Polynomial Kernel', 'Empirical Validation', 'Domain Adaptation', 'Nonlinear Extension', 'Finite-dimensional', 'Null Space']",,11,"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the “linear” form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR100 datasets indicate that, (a) practically, our solution is ideal for “imparting” provable invariance to any biased pre-trained data representation, and (b) the global optima of the “kernel” form can provide a comparable trade-off between utility and invariance in comparison to iterative minimax optimization of existing deep neural network based approaches, but with provable guarantees."
On the Over-Smoothing Problem of CNN Based Disparity Estimation,"Chuangrong Chen, Xiaozhi Chen, Hui Cheng",Sun Yat-sen University; DJI,50.0,China,50.0,China,"Currently, most deep learning based disparity estimation methods have the problem of over-smoothing at boundaries, which is unfavorable for some applications such as point cloud segmentation, mapping, etc. To address this problem, we first analyze the potential causes and observe that the estimated disparity at edge boundary pixels usually follows multimodal distributions, causing over-smoothing estimation. Based on this observation, we propose a single-modal weighted average operation on the probability distribution during inference, which can alleviate the problem effectively. To integrate the constraint of this inference method into training stage, we further analyze the characteristics of different loss functions and found that using cross entropy with gaussian distribution consistently further improves the performance. For quantitative evaluation, we propose a novel metric that measures the disparity error in the local structure of edge boundaries. Experiments on various datasets using various networks show our method's effectiveness and general applicability. Code will be available at https://github.com/chenchr/otosp.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_On_the_Over-Smoothing_Problem_of_CNN_Based_Disparity_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_On_the_Over-Smoothing_Problem_of_CNN_Based_Disparity_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010261/,"['Three-dimensional displays', 'Estimation', 'Feature extraction', 'Convolution', 'Training', 'Two dimensional displays', 'Entropy']","['Convolutional Neural Network', 'Disparity Estimation', 'Over-smoothing Problem', 'Normal Distribution', 'Loss Function', 'Bimodal', 'Local Structure', 'Cross-entropy', 'Cross-entropy Loss', 'Point Cloud', 'Training Stage', 'Boundary Edges', 'Boundary Pixels', 'Point Cloud Segmentation', 'Feature Maps', 'Small Datasets', 'Network Output', 'Submodule', 'Mode Of Distribution', 'Background Regions', 'Cost Volume', 'Discontinuous Regions', 'Endpoint Error', 'Laplace Distribution', 'Distribution Of Pixels', 'Edge Region', 'Disparity Map', '3D Convolution', 'Disparity Values', 'Consistent Improvement']",,34,"Currently, most deep learning based disparity estimation methods have the problem of over-smoothing at boundaries, which is unfavorable for some applications such as point cloud segmentation, mapping, etc. To address this problem, we first analyze the potential causes and observe that the estimated disparity at edge boundary pixels usually follows multimodal distributions, causing over-smoothing estimation. Based on this observation, we propose a single-modal weighted average operation on the probability distribution during inference, which can alleviate the problem effectively. To integrate the constraint of this inference method into training stage, we further analyze the characteristics of different loss functions and found that using cross entropy with gaussian distribution consistently further improves the performance. For quantitative evaluation, we propose a novel metric that measures the disparity error in the local structure of edge boundaries. Experiments on various datasets using various networks show our method's effectiveness and general applicability. Code will be available at https://github.com/chenchr/otosp."
Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once,"Jiangfan Han, Xiaoyi Dong, Ruimao Zhang, Dongdong Chen, Weiming Zhang, Nenghai Yu, Ping Luo, Xiaogang Wang","University of Science and Technology of China, Key Laboratory of Electromagnetic Space Information, The Chinese Academy of Sciences; The University of Hong Kong; CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong",100.0,"Hong Kong, china",0.0,,"Modern deep neural networks are often vulnerable to adversarial samples. Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generation-based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds/thousands of categories. In this paper, we propose the first Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the specified category information into the intermediate features, it can attack any category of the target classification model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classification model. It can also achieve better classification accuracy than other methods when attacked by various methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Han_Once_a_MAN_Towards_Multi-Target_Attack_via_Learning_Multi-Target_Adversarial_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Once_a_MAN_Towards_Multi-Target_Attack_via_Learning_Multi-Target_Adversarial_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009819/,"['Training', 'Task analysis', 'Robustness', 'Neural networks', 'Perturbation methods', 'Predictive models', 'Decoding']","['Deep Neural Network', 'Iterative Procedure', 'Gradient-based Methods', 'Optimization-based Methods', 'Attack Methods', 'Gene-based Methods', 'Attack Performance', 'Image Features', 'Input Image', 'Feature Maps', 'Attack Rate', 'Classification Loss', 'Types Of Attacks', 'Target Model', 'Feature Integration', 'Adversarial Training', 'Target Label', 'Attack Target', 'Attack Success', 'Adversarial Attacks', 'Attack Success Rate', 'Categories In Dataset', 'White-box Attack', 'Fast Gradient Sign Method', 'Adversary Model', 'Appearance Information', 'Feature Maps Of Images', 'Black-box Attacks', 'Weighting Factor', 'Fine-tuned']",,22,"Modern deep neural networks are often vulnerable to adversarial samples. Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generation-based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds/thousands of categories. In this paper, we propose the first Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the specified category information into the intermediate features, it can attack any category of the target classification model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classification model. It can also achieve better classification accuracy than other methods when attacked by various methods."
One-Shot Neural Architecture Search via Self-Evaluated Template Network,"Xuanyi Dong, Yi Yang","ReLER, University of Technology Sydney; Baidu Research, ReLER, University of Technology Sydney",100.0,australia,0.0,,"Neural architecture search (NAS) aims to automate the search procedure of architecture instead of manual design. Even if recent NAS approaches finish the search within days, lengthy training is still required for a specific architecture candidate to get the parameters for its accurate evaluation. Recently one-shot NAS methods are proposed to largely squeeze the tedious training process by sharing parameters across candidates. In this way, the parameters for each candidate can be directly extracted from the shared parameters instead of training them from scratch. However, they have no sense of which candidate will perform better until evaluation so that the candidates to evaluate are randomly sampled and the top-1 candidate is considered the best. In this paper, we propose a Self-Evaluated Template Network (SETN) to improve the quality of the architecture candidates for evaluation so that it is more likely to cover competitive candidates. SETN consists of two components: (1) an evaluator, which learns to indicate the probability of each individual architecture being likely to have a lower validation loss. The candidates for evaluation can thus be selectively sampled according to this evaluator. (2) a template network, which shares parameters among all candidates to amortize the training cost of generated candidates. In experiments, the architecture found by SETN achieves the state-of-the-art performance on CIFAR and ImageNet benchmarks within comparable computation costs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_One-Shot_Neural_Architecture_Search_via_Self-Evaluated_Template_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_One-Shot_Neural_Architecture_Search_via_Self-Evaluated_Template_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009811/,"['Computer architecture', 'Microprocessors', 'Training', 'Tensile stress', 'Computer vision', 'Manuals']","['Neural Architecture Search', 'Network Templates', 'One-shot Neural Architecture Search', 'Random Sampling', 'Computational Cost', 'ImageNet', 'Search Procedure', 'Validation Loss', 'Evaluation Of Candidates', 'Shared Parameters', 'Training Set', 'Validation Set', 'Search Algorithm', 'Efficient Algorithm', 'High Error', 'Network Parameters', 'Search Space', 'Additional Training', 'Weight Decay', 'Validation Accuracy', 'Cell Nodes', 'Large Search Space', 'Search Costs', 'Single GPU', 'Uneven Quality', 'Matthew Effect']",,123,"Neural architecture search (NAS) aims to automate the search procedure of architecture instead of manual design. Even if recent NAS approaches finish the search within days, lengthy training is still required for a specific architecture candidate to get the parameters for its accurate evaluation. Recently one-shot NAS methods are proposed to largely squeeze the tedious training process by sharing parameters across candidates. In this way, the parameters for each candidate can be directly extracted from the shared parameters instead of training them from scratch. However, they have no sense of which candidate will perform better until evaluation so that the candidates to evaluate are randomly sampled and the top-1 candidate is considered the best. In this paper, we propose a Self-Evaluated Template Network (SETN) to improve the quality of the architecture candidates for evaluation so that it is more likely to cover competitive candidates. SETN consists of two components: (1) an evaluator, which learns to indicate the probability of each individual architecture being likely to have a lower validation loss. The candidates for evaluation can thus be selectively sampled according to this evaluator. (2) a template network, which shares parameters among all candidates to amortize the training cost of generated candidates. In experiments, the architecture found by SETN achieves the state-of-the-art performance on CIFAR and ImageNet benchmarks within comparable computation costs."
Onion-Peel Networks for Deep Video Completion,"Seoung Wug Oh, Sungho Lee, Joon-Young Lee, Seon Joo Kim",Adobe Research; Yonsei University,50.0,south korea,50.0,USA,"We propose the onion-peel networks for video completion. Given a set of reference images and a target image with holes, our network fills the hole by referring the contents in the reference images. Our onion-peel network progressively fills the hole from the hole boundary enabling it to exploit richer contextual information for the missing regions every step. Given a sufficient number of recurrences, even a large hole can be inpainted successfully. To attend to the missing information visible in the reference images, we propose an asymmetric attention block that computes similarities between the hole boundary pixels in the target and the non-hole pixels in the references in a non-local manner. With our attention block, our network can have an unlimited spatial-temporal window size and fill the holes with globally coherent contents. In addition, our framework is applicable to the image completion guided by the reference images without any modification, which is difficult to do with the previous methods. We validate that our method produces visually pleasing image and video inpainting results in realistic test cases.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Onion-Peel_Networks_for_Deep_Video_Completion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Onion-Peel_Networks_for_Deep_Video_Completion_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010390/,"['Three-dimensional displays', 'Optical imaging', 'Logic gates', 'Convolution', 'Microsoft Windows', 'Optimization', 'Task analysis']","['Target Image', 'Reference Image', 'Large Holes', 'Attention Block', 'Missing Regions', 'Number Of Recurrences', 'Image Inpainting', 'Deep Network', 'Reference Frame', 'Feature Maps', 'Attention Mechanism', 'Flow Field', 'Generative Adversarial Networks', 'Learning-based Methods', 'Video Frames', 'Temporal Window', 'Optical Flow', 'Average Rank', 'Attention Map', 'Patch Matching', 'Test Videos', 'Hole Region', 'Target Frame', 'Temporal Consistency', 'Patch-based Methods', 'Nearest Neighbor Search', 'Feed-forward Network', 'Perceptual Loss']",,74,"We propose the onion-peel networks for video completion. Given a set of reference images and a target image with holes, our network fills the hole by referring the contents in the reference images. Our onion-peel network progressively fills the hole from the hole boundary enabling it to exploit richer contextual information for the missing regions every step. Given a sufficient number of recurrences, even a large hole can be inpainted successfully. To attend to the missing information visible in the reference images, we propose an asymmetric attention block that computes similarities between the hole boundary pixels in the target and the non-hole pixels in the references in a non-local manner. With our attention block, our network can have an unlimited spatial-temporal window size and fill the holes with globally coherent contents. In addition, our framework is applicable to the image completion guided by the reference images without any modification, which is difficult to do with the previous methods. We validate that our method produces visually pleasing image and video inpainting results in realistic test cases."
Online Hyper-Parameter Learning for Auto-Augmentation Strategy,"Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei Wu, Junjie Yan, Dahua Lin, Wanli Ouyang","The Chinese University of Hong Kong; SenseTime Group Limited; The University of Sydney, SenseTime Computer Vision Research Group, Australia",100.0,"Hong Kong, australia, usa",0.0,,"Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_Online_Hyper-Parameter_Learning_for_Auto-Augmentation_Strategy_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Online_Hyper-Parameter_Learning_for_Auto-Augmentation_Strategy_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010720/,"['Training', 'Optimization', 'Biological system modeling', 'Computational modeling', 'Computer architecture', 'Probability distribution', 'Data models']","['Network Parameters', 'Data Augmentation', 'Augmentation Strategy', 'Probability Distribution Of Parameters', 'Training Set', 'Error Rate', 'Learning Rate', 'Distribution Of Parameters', 'Search Space', 'Neural Architecture', 'Hyperparameter Tuning', 'Validation Accuracy', 'Random Search', 'ImageNet Dataset', 'Multinomial Distribution', 'Number Of Trajectories', 'Search Costs', 'Sample Trajectories', 'Neural Architecture Search', 'Augmentation Operations', 'Online Manner', 'Training Pipeline', 'Augmented Samples', 'Online Optimization', 'Large Datasets', 'Gradient Descent']",,56,"Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies."
Online Model Distillation for Efficient Video Inference,"Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, Kayvon Fatahalian",Carnegie Mellon University; Stanford University,100.0,usa,0.0,,"High-quality computer vision models typically address the problem of understanding the general distribution of real-world images. However, most cameras observe only a very small fraction of this distribution. This offers the possibility of achieving more efficient inference by specializing compact, low-cost models to the specific distribution of frames observed by a single camera. In this paper, we employ the technique of model distillation (supervising a low-cost student model using the output of a high-cost teacher) to specialize accurate, low-cost semantic segmentation models to a target video stream. Rather than learn a specialized student model on offline data from the video stream, we train the student in an online fashion on the live video, intermittently running the teacher to provide a target for learning. Online model distillation yields semantic segmentation models that closely approximate their Mask R-CNN teacher with 7 to 17xlower inference runtime cost (11 to 26xin FLOPs), even when the target video's distribution is non-stationary. Our method requires no offline pretraining on the target video stream, achieves higher accuracy and lower cost than solutions based on flow or video object segmentation, and can exhibit better temporal stability than the original teacher. We also provide a new video dataset for evaluating the efficiency of inference over long running video streams.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mullapudi_Online_Model_Distillation_for_Efficient_Video_Inference_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mullapudi_Online_Model_Distillation_for_Efficient_Video_Inference_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010320/,"['Streaming media', 'Adaptation models', 'Training', 'Semantics', 'Computer architecture', 'Computational modeling', 'Cameras']","['Efficient Inference', 'Low Cost', 'Segmentation Model', 'Semantic Segmentation', 'Temporal Stability', 'Distribution Of Images', 'Student Model', 'Single Camera', 'Video Dataset', 'Mask R-CNN', 'Semantic Segmentation Models', 'Online Fashion', 'Low-cost Model', 'Gradient Descent', 'Sampling Frame', 'Training Methods', 'Online Learning', 'Bounding Box', 'Video Frames', 'Compact Model', 'Higher Learning Rate', 'Mean Intersection Over Union', 'Future Frames', 'Live Streaming', 'Accuracy Threshold', 'Compact Architecture', 'Online Training', 'Fast Variables', 'COCO Dataset']",,47,"High-quality computer vision models typically address the problem of understanding the general distribution of real-world images. However, most cameras observe only a very small fraction of this distribution. This offers the possibility of achieving more efficient inference by specializing compact, low-cost models to the specific distribution of frames observed by a single camera. In this paper, we employ the technique of model distillation (supervising a low-cost student model using the output of a high-cost teacher) to specialize accurate, low-cost semantic segmentation models to a target video stream. Rather than learn a specialized student model on offline data from the video stream, we train the student in an online fashion on the live video, intermittently running the teacher to provide a target for learning. Online model distillation yields semantic segmentation models that closely approximate their Mask R-CNN teacher with 7 to 17× lower inference runtime cost (11 to 26× in FLOPs), even when the target video's distribution is non-stationary. Our method requires no offline pretraining on the target video stream, achieves higher accuracy and lower cost than solutions based on flow or video object segmentation, and can exhibit better temporal stability than the original teacher. We also provide a new video dataset for evaluating the efficiency of inference over long running video streams."
Online Unsupervised Learning of the 3D Kinematic Structure of Arbitrary Rigid Bodies,"Urbano Miguel Nunes, Yiannis Demiris","Personal Robotics Lab, Imperial College London, UK",100.0,uk,0.0,,"This work addresses the problem of 3D kinematic structure learning of arbitrary articulated rigid bodies from RGB-D data sequences. Typically, this problem is addressed by offline methods that process a batch of frames, assuming that complete point trajectories are available. However, this approach is not feasible when considering scenarios that require continuity and fluidity, for instance, human-robot interaction. In contrast, we propose to tackle this problem in an online unsupervised fashion, by recursively maintaining the metric distance of the scene's 3D structure, while achieving real-time performance. The influence of noise is mitigated by building a similarity measure based on a linear embedding representation and incorporating this representation into the original metric distance. The kinematic structure is then estimated based on a combination of implicit motion and spatial properties. The proposed approach achieves competitive performance both quantitatively and qualitatively in terms of estimation accuracy, even compared to offline methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nunes_Online_Unsupervised_Learning_of_the_3D_Kinematic_Structure_of_Arbitrary_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nunes_Online_Unsupervised_Learning_of_the_3D_Kinematic_Structure_of_Arbitrary_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010702/,"['Three-dimensional displays', 'Kinematics', 'Motion segmentation', 'Computer vision', 'Trajectory', 'Euclidean distance', 'Tracking']","['Unsupervised Learning', 'Rigid Body', 'Kinematic Structure', '3D Structure', 'Similarity Measure', 'Learning Problem', 'Spatial Properties', 'Human-robot Interaction', 'Complete Trajectory', 'Eigenvalues', 'Body Parts', 'Distance Matrix', 'Pairwise Distances', 'Point Cloud', 'Unsupervised Methods', 'Similarity Matrix', '3D Data', 'Sequence Of Frames', 'Body Segments', 'Objective Structured', 'Number Of Motions', 'Pairwise Euclidean Distances', 'Past Information', 'Standard Deviation Of Distance', 'Online Fashion', 'Subspace Clustering', 'Spectral Clustering', 'RGB-D Sensor', 'Monocular Images', 'Difference In Distance']",,4,"This work addresses the problem of 3D kinematic structure learning of arbitrary articulated rigid bodies from RGB-D data sequences. Typically, this problem is addressed by offline methods that process a batch of frames, assuming that complete point trajectories are available. However, this approach is not feasible when considering scenarios that require continuity and fluidity, for instance, human-robot interaction. In contrast, we propose to tackle this problem in an online unsupervised fashion, by recursively maintaining the metric distance of the scene's 3D structure, while achieving real-time performance. The influence of noise is mitigated by building a similarity measure based on a linear embedding representation and incorporating this representation into the original metric distance. The kinematic structure is then estimated based on a combination of implicit motion and spatial properties. The proposed approach achieves competitive performance both quantitatively and qualitatively in terms of estimation accuracy, even compared to offline methods."
OperatorNet: Recovering 3D Shapes From Difference Operators,"Ruqi Huang, Marie-Julie Rakotosaona, Panos Achlioptas, Leonidas J. Guibas, Maks Ovsjanikov","LIX, Ecole Polytechnique; Stanford University",100.0,"france, usa",0.0,,"This paper proposes a learning-based framework for reconstructing 3D shapes from functional operators, compactly encoded as small-sized matrices. To this end we introduce a novel neural architecture, called OperatorNet, which takes as input a set of linear operators representing a shape and produces its 3D embedding. We demonstrate that this approach significantly outperforms previous purely geometric methods for the same problem. Furthermore, we introduce a novel functional operator, which encodes the extrinsic or pose-dependent shape information, and thus complements purely intrinsic pose-oblivious operators, such as the classical Laplacian. Coupled with this novel operator, our reconstruction network achieves very high reconstruction accuracy, even in the presence of incomplete information about a shape, given a soft or functional map expressed in a reduced basis. Finally, we demonstrate that the multiplicative functional algebra enjoyed by these operators can be used to synthesize entirely new unseen shapes, in the context of shape interpolation and shape analogy applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_OperatorNet_Recovering_3D_Shapes_From_Difference_Operators_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_OperatorNet_Recovering_3D_Shapes_From_Difference_Operators_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009509/,"['Shape', 'Three-dimensional displays', 'Interpolation', 'Silicon', 'Algebra', 'Image reconstruction', 'Geometry']","['3D Shape', 'Neural Architecture', 'Set Of Operations', 'Transformer', 'Point Cloud', 'Differences In Shape', 'Coefficient Vector', 'Functional Coordination', 'Rigid Transformation', 'Lie Algebra', 'Squared Euclidean Distance', 'Latent Vector', 'Convolutional Architecture', 'Triangular Mesh', 'Rotation Invariance', 'Shape Space', 'Local Distortion', 'Target Shape', 'Gram Matrix', 'Functor', 'Shape Reconstruction', 'Laplace-Beltrami Operator', 'Nearest Neighbor Interpolation', 'Linear Interpolation', 'Pose Changes', 'Autoencoder', 'Real Shape', 'Shape Representation', 'Quantitative Results', 'Point Cloud Reconstruction']",,13,"This paper proposes a learning-based framework for reconstructing 3D shapes from functional operators, compactly encoded as small-sized matrices. To this end we introduce a novel neural architecture, called OperatorNet, which takes as input a set of linear operators representing a shape and produces its 3D embedding. We demonstrate that this approach significantly outperforms previous purely geometric methods for the same problem. Furthermore, we introduce a novel functional operator, which encodes the extrinsic or pose-dependent shape information, and thus complements purely intrinsic pose-oblivious operators, such as the classical Laplacian. Coupled with this novel operator, our reconstruction network achieves very high reconstruction accuracy, even in the presence of incomplete information about a shape, given a soft or functional map expressed in a reduced basis. Finally, we demonstrate that the multiplicative functional algebra enjoyed by these operators can be used to synthesize entirely new unseen shapes, in the context of shape interpolation and shape analogy applications."
Optimizing Network Structure for 3D Human Pose Estimation,"Hai Ci, Chunyu Wang, Xiaoxuan Ma, Yizhou Wang","Computer Science Dept., Peking University, Deepwise AI Lab, Peng Cheng Laboratory; Microsoft Research, Asia; Computer Science Dept., Peking University",66.66666666666666,china,33.33333333333334,China,"A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010332/,"['Three-dimensional displays', 'Two dimensional displays', 'Joints', 'Pose estimation', 'Laplace equations', 'Solid modeling', 'Elbow']","['Pose Estimation', 'Human Pose Estimation', 'Human Pose', '3D Human Pose', 'General Form', 'Graph Convolutional Network', 'Strong Generalization', 'Representation Capability', 'Fully-connected Network', '3D Pose', '2D Pose', 'Training Set', 'Weight Matrix', 'Generalization Ability', 'Structure Of Matrix', 'Output Feature', 'Nodes In The Graph', 'Learnable Parameters', 'Dense Connections', 'Node Features', '3D Error', 'Strong Generalization Ability', 'Number Of Training Data', 'Sparse Connectivity', 'Representation Ability', 'Generalization Ability Of The Model', 'Additional Annotations', 'Order Of Expansion', 'Human Anatomy', 'Unique Parameter']",,141,"A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets."
Optimizing the F-Measure for Threshold-Free Salient Object Detection,"Kai Zhao, Shanghua Gao, Wenguan Wang, Ming-Ming Cheng","TKLNDST, CS, Nankai University; Inception Institute of Artiﬁcial Intelligence",100.0,"China, uae",0.0,,"Current CNN-based solutions to salient object detection (SOD) mainly rely on the optimization of cross-entropy loss (CELoss). Then the quality of detected saliency maps is often evaluated in terms of F-measure. In this paper, we investigate an interesting issue: can we consistently use the F-measure formulation in both training and evaluation for SOD? By reformulating the standard F-measure we propose the relaxed F-measure which is differentiable w.r.t the posterior and can be easily appended to the back of CNNs as the loss function. Compared to the conventional cross-entropy loss of which the gradients decrease dramatically in the saturated area, our loss function, named FLoss, holds considerable gradients even when the activation approaches the target. Consequently, the FLoss can continuously force the network to produce polarized activations. Comprehensive benchmarks on several popular datasets show that FLoss outperforms the state-of-the-art with a considerable margin. More specifically, due to the polarized predictions, our method is able to obtain high-quality saliency maps without carefully tuning the optimal threshold, showing significant advantages in real-world applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Optimizing_the_F-Measure_for_Threshold-Free_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Optimizing_the_F-Measure_for_Threshold-Free_Salient_Object_Detection_ICCV_2019_paper.pdf,,http://kaizhao.net/fmeasure,,main,Poster,https://ieeexplore.ieee.org/document/9010414/,"['Object detection', 'Training', 'Optimization', 'Computer architecture', 'Measurement', 'Computer vision', 'Standards']","['Salient Object', 'Salient Object Detection', 'Loss Function', 'Convolutional Neural Network', 'Cross-entropy Loss', 'Optimal Threshold', 'Saliency Map', 'Popular Datasets', 'Comprehensive Benchmark', 'Saturated Areas', 'Learning Rate', 'Mean Absolute Error', 'Data Augmentation', 'Precision And Recall', 'Exhaustive Search', 'Change Threshold', 'Edge Detection', 'Optimal Target', 'Biased Distribution', 'Unbalanced Data', 'Base Learning Rate', 'Wide Range Of Thresholds', 'Harmonic Mean Of Precision', 'Original Implementation', 'Harmonic Mean Of Recall', 'Fully Convolutional Network', 'Large Learning Rate']",,51,"Current CNN-based solutions to salient object detection (SOD) mainly rely on the optimization of cross-entropy loss (CELoss). Then the quality of detected saliency maps is often evaluated in terms of F-measure. In this paper, we investigate an interesting issue: can we consistently use the F-measure formulation in both training and evaluation for SOD? By reformulating the standard F-measure we propose the relaxed F-measure which is differentiable w.r.t the posterior and can be easily appended to the back of CNNs as the loss function. Compared to the conventional cross-entropy loss of which the gradients decrease dramatically in the saturated area, our loss function, named FLoss, holds considerable gradients even when the activation approaches the target. Consequently, the FLoss can continuously force the network to produce polarized activations. Comprehensive benchmarks on several popular datasets show that FLoss outperforms the state-of-the-art with a considerable margin. More specifically, due to the polarized predictions, our method is able to obtain high-quality saliency maps without carefully tuning the optimal threshold, showing significant advantages in real-world applications."
Order-Aware Generative Modeling Using the 3D-Craft Dataset,"Zhuoyuan Chen, Demi Guo, Tong Xiao, Saining Xie, Xinlei Chen, Haonan Yu, Jonathan Gray, Kavya Srinet, Haoqi Fan, Jerry Ma, Charles R. Qi, Shubham Tulsiani, Arthur Szlam, C. Lawrence Zitnick","Facebook AI Research, Menlo Park, CA",0.0,,100.0,USA,"In this paper, we study the problem of sequentially building houses in the game of Minecraft, and demonstrate that learning the ordering can make for more effective autoregressive models. Given a partially built house made by a human player, our system tries to place additional blocks in a human-like manner to complete the house. We introduce a new dataset, HouseCraft, for this new task. HouseCraft contains the sequential order in which 2,500 Minecraft houses were built from scratch by humans. The human action sequences enable us to learn an order-aware generative model called Voxel-CNN. In contrast to many generative models where the sequential generation ordering either does not matter (e.g. holistic generation with GANs), or is manually/arbitrarily set by simple rules (e.g. raster-scan order), our focus is on an ordered generation that imitates humans. To evaluate if a generative model can accurately predict human-like actions, we propose several novel quantitative metrics. We demonstrate that our Voxel-CNN model is simple and effective at this creative task, and can serve as a strong baseline for future research in this direction. The HouseCraft dataset and code with baseline models will be made publicly available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Order-Aware_Generative_Modeling_Using_the_3D-Craft_Dataset_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Order-Aware_Generative_Modeling_Using_the_3D-Craft_Dataset_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/VoxelCNN,,main,Poster,https://ieeexplore.ieee.org/document/9009072/,"['Three-dimensional displays', 'Solid modeling', 'Buildings', 'Games', 'Two dimensional displays', 'Task analysis', 'Computational modeling']","['Human Activities', 'Sequence Of Actions', '2D Images', '2D Model', 'Human Players', 'Convolutional Neural Network', 'Local Context', '3D Printing', '3D Reconstruction', 'Local State', 'Generative Adversarial Networks', 'Information In Order', 'Heuristic Algorithm', 'Final Objective', 'Softmax Layer', 'Natural Order', 'Block Type', 'Perplexity', '3D Datasets', 'Final Representation', 'Number Of Mistakes', '3D Convolutional Layers', 'House Building', 'Consecutive Actions', '3D Grid']",,4,"In this paper, we study the problem of sequentially building houses in the game of Minecraft, and demonstrate that learning the ordering can make for more effective autoregressive models. Given a partially built house made by a human player, our system tries to place additional blocks in a human-like manner to complete the house. We introduce a new dataset, HouseCraft, for this new task. HouseCraft contains the sequential order in which 2,500 Minecraft houses were built from scratch by humans. The human action sequences enable us to learn an order-aware generative model called Voxel-CNN. In contrast to many generative models where the sequential generation ordering either does not matter (e.g. holistic generation with GANs), or is manually/arbitrarily set by simple rules (e.g. raster-scan order), our focus is on an ordered generation that imitates humans. To evaluate if a generative model can accurately predict human-like actions, we propose several novel quantitative metrics. We demonstrate that our Voxel-CNN model is simple and effective at this creative task, and can serve as a strong baseline for future research in this direction. The HouseCraft dataset and code with baseline models will be made publicly available."
Order-Preserving Wasserstein Discriminant Analysis,"Bing Su, Jiahuan Zhou, Ying Wu","Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, 60208; Science & Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, 100190, China",100.0,"china, usa",0.0,,"Supervised dimensionality reduction for sequence data projects the observations in sequences onto a low-dimensional subspace to better separate different sequence classes. It is typically more challenging than conventional dimensionality reduction for static data, because measuring the separability of sequences involves non-linear procedures to manipulate the temporal structures. This paper presents a linear method, namely Order-preserving Wasserstein Discriminant Analysis (OWDA), which learns the projection by maximizing the inter-class distance and minimizing the intra-class scatter. For each class, OWDA extracts the order-preserving Wasserstein barycenter and constructs the intra-class scatter as the dispersion of the training sequences around the barycenter. The inter-class distance is measured as the order-preserving Wasserstein distance between the corresponding barycenters. OWDA is able to concentrate on the distinctive differences among classes by lifting the geometric relations with temporal constraints. Experiments show that OWDA achieves competitive results on three 3D action recognition datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Su_Order-Preserving_Wasserstein_Discriminant_Analysis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Su_Order-Preserving_Wasserstein_Discriminant_Analysis_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008561/,"['Training', 'Dimensionality reduction', 'Dispersion', 'Complexity theory', 'Distortion', 'Legged locomotion', 'Prototypes']","['Discriminant Analysis', 'Dimensionality Reduction', 'Temporal Structure', 'Training Sequences', 'Temporal Constraints', 'Barycenter', 'Low-dimensional Subspace', 'Performance Measures', 'Linear Discriminant Analysis', 'K-nearest Neighbor', 'Representative Sequences', 'Pairwise Distances', 'Recurrent Neural Network', 'Sequence Features', 'Kullback-Leibler', 'Joint Probability', 'Sequence Classification', 'Null Space', 'Optimal Transport', 'Dynamic Time Warping', 'Mean Average Precision', 'Average Sequence Length', 'Sequence Of Observations', 'Kinect Camera', 'Pair Of Weights', 'SVM Classifier', 'Preset Value', 'Training Set', 'Gestures']",,3,"Supervised dimensionality reduction for sequence data projects the observations in sequences onto a low-dimensional subspace to better separate different sequence classes. It is typically more challenging than conventional dimensionality reduction for static data, because measuring the separability of sequences involves non-linear procedures to manipulate the temporal structures. This paper presents a linear method, namely Order-preserving Wasserstein Discriminant Analysis (OWDA), which learns the projection by maximizing the inter-class distance and minimizing the intra-class scatter. For each class, OWDA extracts the order-preserving Wasserstein barycenter and constructs the intra-class scatter as the dispersion of the training sequences around the barycenter. The inter-class distance is measured as the order-preserving Wasserstein distance between the corresponding barycenters. OWDA is able to concentrate on the distinctive differences among classes by lifting the geometric relations with temporal constraints. Experiments show that OWDA achieves competitive results on three 3D action recognition datasets."
Orientation-Aware Semantic Segmentation on Icosahedron Spheres,"Chao Zhang, Stephan Liwicki, William Smith, Roberto Cipolla","Toshiba Research Europe Limited, Cambridge, United Kingdom; University of Cambridge, United Kingdom; University of York, United Kingdom",100.0,"United Kingdom, uk, usa",0.0,,"We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Orientation-Aware_Semantic_Segmentation_on_Icosahedron_Spheres_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Orientation-Aware_Semantic_Segmentation_on_Icosahedron_Spheres_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009101/,"['Kernel', 'Semantics', 'Standards', 'Shape', 'Convolution', 'Image segmentation', 'Task analysis']","['Semantic Segmentation', 'Icosahedral', 'Convolutional Neural Network', 'Direct Transfer', 'Convolution Kernel', 'Convolutional Neural Network Classifier', 'Rotation Invariance', 'Memory Efficiency', 'Hexagonal', 'Pedestrian', 'Regular Grid', 'Small Objects', 'Equivalency', 'Climate Patterns', '3D Shape', 'Spherical Surface', 'Bilinear Interpolation', 'North Pole', 'Filter Weights', 'Multiple Orientations', 'Standard Convolutional Neural Networks', 'Weight Transfer', 'Standard Convolution', 'South Pole', 'Spherical Image', 'Tangent Plane', 'Spherical Representation']",,46,"We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art."
Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild,"Kibok Lee, Kimin Lee, Jinwoo Shin, Honglak Lee",KAIST; University of Michigan,100.0,"south korea, usa",0.0,,"Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8% higher accuracy and 46.5% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Overcoming_Catastrophic_Forgetting_With_Unlabeled_Data_in_the_Wild_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Overcoming_Catastrophic_Forgetting_With_Unlabeled_Data_in_the_Wild_ICCV_2019_paper.pdf,,https://github.com/kibok90/iccv2019-inc,,main,Poster,https://ieeexplore.ieee.org/document/9010368/,"['Task analysis', 'Training', 'Data models', 'Sampling methods', 'Dogs', 'Predictive models', 'Neural networks']","['Unlabeled Data', 'Catastrophic Forgetting', 'Deep Neural Network', 'Learning Strategies', 'ImageNet', 'Data Streams', 'Lifelong Learning', 'External Data', 'Previous Tasks', 'Prior Methods', 'Prediction Model', 'Learning Models', 'Training Dataset', 'Cognitive Tasks', 'Small Datasets', 'Cognitive Model', 'Learning Objectives', 'Latent Space', 'Reference Model', 'Performance Gain', 'External Dataset', 'Incremental Learning', 'Current Task', 'Model-based Approach', 'Labeled Training Dataset', 'Loss Of Confidence', 'Task Data', 'Set Of Classes', 'Previous Stage', 'Non-volatile Memory']",,93,"Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8% higher accuracy and 46.5% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc."
P-MVSNet: Learning Patch-Wise Matching Confidence Aggregation for Multi-View Stereo,"Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, Yawei Luo","Huazhong University of Science and Technology, China; University of South Carolina, USA; Farsee2 Technology Ltd, China",66.66666666666666,"china, usa",33.33333333333334,China,"Learning-based methods are demonstrating their strong competitiveness in estimating depth for multi-view stereo reconstruction in recent years. Among them the approaches that generate cost volumes based on the plane-sweeping algorithm and then use them for feature matching have shown to be very prominent recently. The plane-sweep volumes are essentially anisotropic in depth and spatial directions, but they are often approximated by isotropic cost volumes in those methods, which could be detrimental. In this paper, we propose a new end-to-end deep learning network of P-MVSNet for multi-view stereo based on isotropic and anisotropic 3D convolutions. Our P-MVSNet consists of two core modules: a patch-wise aggregation module learns to aggregate the pixel-wise correspondence information of extracted features to generate a matching confidence volume, from which a hybrid 3D U-Net then infers a depth probability distribution and predicts the depth maps. We perform extensive experiments on the DTU and Tanks & Temples benchmark datasets, and the results show that the proposed P-MVSNet achieves the state-of-the-art performance over many existing methods on multi-view stereo.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Luo_P-MVSNet_Learning_Patch-Wise_Matching_Confidence_Aggregation_for_Multi-View_Stereo_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_P-MVSNet_Learning_Patch-Wise_Matching_Confidence_Aggregation_for_Multi-View_Stereo_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010001/,"['Three-dimensional displays', 'Kernel', 'Feature extraction', 'Image reconstruction', 'Surface reconstruction', 'Aggregates', 'Surface texture']","['Multi-view Stereo', 'Matching Confidence', 'Deep Learning', 'Anisotropy', 'Spatial Orientation', 'Learning-based Methods', 'Depth Map', 'Feature Matching', 'Core Module', '3D Convolution', 'Depth Direction', '3D U-Net', 'Cost Volume', 'Isotropic 3D', 'Training Set', 'Convolutional Neural Network', 'Validation Set', 'Convolutional Layers', 'Input Image', 'Feature Maps', 'Point Cloud', 'Adjacent Images', 'Reference Image', 'Image Pixels', 'Stereo Matching', 'Matching Cost', 'Deeper Layers', 'Point Cloud Reconstruction', 'Sparse Point Cloud', 'Confidence Map']",,138,"Learning-based methods are demonstrating their strong competitiveness in estimating depth for multi-view stereo reconstruction in recent years. Among them the approaches that generate cost volumes based on the plane-sweeping algorithm and then use them for feature matching have shown to be very prominent recently. The plane-sweep volumes are essentially anisotropic in depth and spatial directions, but they are often approximated by isotropic cost volumes in those methods, which could be detrimental. In this paper, we propose a new end-to-end deep learning network of P-MVSNet for multi-view stereo based on isotropic and anisotropic 3D convolutions. Our P-MVSNet consists of two core modules: a patch-wise aggregation module learns to aggregate the pixel-wise correspondence information of extracted features to generate a matching confidence volume, from which a hybrid 3D U-Net then infers a depth probability distribution and predicts the depth maps. We perform extensive experiments on the DTU and Tanks & Temples benchmark datasets, and the results show that the proposed P-MVSNet achieves the state-of-the-art performance over many existing methods on multi-view stereo."
PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data,"Zheng Tang, Milind Naphade, Stan Birchfield, Jonathan Tremblay, William Hodge, Ratnesh Kumar, Shuo Wang, Xiaodong Yang",NVIDIA,0.0,,100.0,USA,"In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009081/,"['Image color analysis', 'Shape', 'Training', 'Feature extraction', 'Heating systems', 'Pose estimation', 'Solid modeling']","['Multi-task Learning', 'Vehicle Re-identification', 'Heatmap', 'Pose Estimation', 'Semantic Properties', 'Convolutional Neural Network', 'Deep Neural Network', 'Pedestrian', 'Cross-entropy Loss', 'Large-scale Datasets', 'Generative Adversarial Networks', 'Average Precision', 'Vehicle Type', 'Additional Channels', 'Metric Learning', 'Triplet Loss', 'Human Pose Estimation', 'Color Categories', 'Car Model', 'Real Identity', 'Pose Information', 'Convolutional Neural Networks Backbone', 'Multi-task Network', 'Vehicle Identification', 'Identity Classification', 'RGB Channels', 'Bounding Box', 'Spatiotemporal Information', 'Low Resolution', 'Neural Network']",,124,"In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID."
PANet: Few-Shot Image Semantic Segmentation With Prototype Alignment,"Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, Jiashi Feng","ECE Department, National University of Singapore; NGS, National University of Singapore",100.0,singapore,0.0,,"Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010855/,"['Prototypes', 'Image segmentation', 'Feature extraction', 'Semantics', 'Training', 'Measurement', 'Silicon']","['Semantic Segmentation', 'Deep Convolutional Neural Network', 'Latent Space', 'Large Number Of Images', 'Support Set', 'Metric Learning', 'Query Image', 'Annotated Examples', 'Training Set', 'Feature Maps', 'Bounding Box', 'Segmentation Model', 'Deep Features', 'Segmentation Results', 'Segmentation Task', 'Squared Euclidean Distance', 'Dilated Convolution', 'Fully Convolutional Network', 'Scribble', 'Query Set', 'Few-shot Learning', 'Prototypical Network', 'Query Features', 'Bounding Box Annotations', 'Late Fusion', 'Decoder Structure', 'Unseen Classes', 'Class Prototypes', 'Receptive Field']",,763,"Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%."
PARN: Position-Aware Relation Networks for Few-Shot Learning,"Ziyang Wu, Yuwei Li, Lihua Guo, Kui Jia","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China",100.0,china,0.0,,"Few-shot learning presents a challenge that a classifier must quickly adapt to new classes that do not appear in the training set, given only a few labeled examples of each new class. This paper proposes a position-aware relation network (PARN) to learn a more flexible and robust metric ability for few-shot learning. Relation networks (RNs), a kind of architectures for relational reasoning, can acquire a deep metric ability for images by just being designed as a simple convolutional neural network (CNN)[23]. However, due to the inherent local connectivity of CNN, the CNN-based relation network (RN) can be sensitive to the spatial position relationship of semantic objects in two compared images. To address this problem, we introduce a deformable feature extractor (DFE) to extract more efficient features, and design a dual correlation attention mechanism (DCA) to deal with its inherent local connectivity. Successfully, our proposed approach extents the potential of RN to be position-aware of semantic objects by introducing only a small number of parameters. We evaluate our approach on two major benchmark datasets, i.e., Omniglot and Mini-Imagenet, and on both of the datasets our approach achieves state-of-the-art performance. It's worth noting that our 5-way 1-shot result on Omniglot even outperforms the previous 5-way 5-shot results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_PARN_Position-Aware_Relation_Networks_for_Few-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_PARN_Position-Aware_Relation_Networks_for_Few-Shot_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009476/,"['Feature extraction', 'Measurement', 'Semantics', 'Task analysis', 'Correlation', 'Kernel', 'Convolution']","['Network Of Relationships', 'Few-shot Learning', 'Convolutional Neural Network', 'Attention Mechanism', 'Local Connectivity', 'Efficient Feature', 'Dual Mechanism', 'Robust Ability', 'Simple Convolutional Neural Network', 'Semantic Objects', 'Dual Attention', 'Inherent Connection', 'Convolutional Layers', 'Input Features', 'Related Features', 'Receptive Field', 'Convolution Operation', 'Global Information', 'Output Feature', 'Latent Space', 'Fine-grained Features', 'Attention Module', 'Feature Extraction Network', 'Convolution Kernel', 'Query Image', 'Deformable Layer', 'Semantic Features', 'Channel Dimension', 'Ablation Experiments', 'Subsequent Comparisons']",,77,"Few-shot learning presents a challenge that a classifier must quickly adapt to new classes that do not appear in the training set, given only a few labeled examples of each new class. This paper proposes a position-aware relation network (PARN) to learn a more flexible and robust metric ability for few-shot learning. Relation networks (RNs), a kind of architectures for relational reasoning, can acquire a deep metric ability for images by just being designed as a simple convolutional neural network (CNN)[23]. However, due to the inherent local connectivity of CNN, the CNN-based relation network (RN) can be sensitive to the spatial position relationship of semantic objects in two compared images. To address this problem, we introduce a deformable feature extractor (DFE) to extract more efficient features, and design a dual correlation attention mechanism (DCA) to deal with its inherent local connectivity. Successfully, our proposed approach extents the potential of RN to be position-aware of semantic objects by introducing only a small number of parameters. We evaluate our approach on two major benchmark datasets, i.e., Omniglot and Mini-Imagenet, and on both of the datasets our approach achieves state-of-the-art performance. It's worth noting that our 5-way 1-shot result on Omniglot even outperforms the previous 5-way 5-shot results."
PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction,"Amir Rasouli, Iuliia Kotseruba, Toni Kunic, John K. Tsotsos","York University, Toronto, Ontario, Canada",100.0,canada,0.0,,"Pedestrian behavior anticipation is a key challenge in the design of assistive and autonomous driving systems suitable for urban environments. An intelligent system should be able to understand the intentions or underlying motives of pedestrians and to predict their forthcoming actions. To date, only a few public datasets were proposed for the purpose of studying pedestrian behavior prediction in the context of intelligent driving. To this end, we propose a novel large-scale dataset designed for pedestrian intention estimation (PIE). We conducted a large-scale human experiment to establish human reference data for pedestrian intention in traffic scenes. We propose models for estimating pedestrian crossing intention and predicting their future trajectory. Our intention estimation model achieves 79% accuracy and our trajectory prediction algorithm outperforms state-of-the-art by 26% on the proposed dataset. We further show that combining pedestrian intention with observed motion improves trajectory prediction. The dataset and models are available at http://data.nvision2.eecs.yorku.ca/PIE_dataset/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf,http://data.nvision2.eecs.yorku.ca/PIE_dataset/,,,main,Oral,https://ieeexplore.ieee.org/document/9008118/,"['Trajectory', 'Estimation', 'Prediction algorithms', 'Roads', 'Cameras', 'Predictive models', 'Vehicle dynamics']","['Trajectory Prediction', 'Pedestrian Trajectory', 'Trajectory Prediction Model', 'Pedestrian Trajectory Prediction', 'Intention Estimation', 'Pedestrian Intention', 'Human Experience', 'Urban Environments', 'Predictor Of Behavior', 'Large-scale Experiments', 'Future Trajectories', 'Pedestrian Behavior', 'Human Subjects', 'Local Context', 'Contextual Information', 'Human Participants', 'Weight Decay', 'Bounding Box', 'Light Signal', 'Activity Prediction', 'Bus Stop', 'Road Boundary', 'High Degree Of Agreement', 'Trajectory Estimation', 'Bounding Box Coordinates', 'Prediction Problem', 'Temporal Correspondence', 'Onboard Camera', 'Road Structure']",,215,"Pedestrian behavior anticipation is a key challenge in the design of assistive and autonomous driving systems suitable for urban environments. An intelligent system should be able to understand the intentions or underlying motives of pedestrians and to predict their forthcoming actions. To date, only a few public datasets were proposed for the purpose of studying pedestrian behavior prediction in the context of intelligent driving. To this end, we propose a novel large-scale dataset designed for pedestrian intention estimation (PIE). We conducted a large-scale human experiment to establish human reference data for pedestrian intention in traffic scenes. We propose models for estimating pedestrian crossing intention and predicting their future trajectory. Our intention estimation model achieves 79% accuracy and our trajectory prediction algorithm outperforms state-of-the-art by 26% on the proposed dataset. We further show that combining pedestrian intention with observed motion improves trajectory prediction. The dataset and models are available at http://data.nvision2.eecs.yorku.ca/PIE_dataset/."
PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li","University of Southern California, USC Institute for Creative Technologies, Pinscreen; Waseda University; University of California, Berkeley; University of Southern California, USC Institute for Creative Technologies",100.0,"japan, usa",0.0,,"We introduce Pixel-aligned Implicit Function (PIFu), an implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu produces high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Saito_PIFu_Pixel-Aligned_Implicit_Function_for_High-Resolution_Clothed_Human_Digitization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Saito_PIFu_Pixel-Aligned_Implicit_Function_for_High-Resolution_Clothed_Human_Digitization_ICCV_2019_paper.pdf,https://shunsukesaito.github.io/PIFu/,,,main,Poster,https://ieeexplore.ieee.org/document/9010814/,"['Three-dimensional displays', 'Surface reconstruction', 'Image reconstruction', 'Shape', 'Surface texture', 'Clothing', 'Topology']","['Implicit Function', 'Clothed Human', 'Pixel-aligned Implicit Function', 'Deep Learning', 'Input Image', 'Single Image', 'Multiple Images', '3D Surface', 'Real-world Images', 'Single View', 'Spatial Alignment', 'Implicit Representation', '3D Texture', 'Arbitrary Topology', 'Model Parameters', 'Deep Neural Network', 'Image Features', 'Points In Space', '3D Space', 'Multilayer Perceptron', 'Multi-view Stereo', 'Single Input Image', 'Image Encoder', 'Concave Regions', '3D Point', 'RGB Values', 'Surface Reconstruction', 'View Synthesis', 'Surface Geometry', 'RGB Color']",,781,"We introduce Pixel-aligned Implicit Function (PIFu), an implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu produces high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image."
PLMP - Point-Line Minimal Problems in Complete Multi-View Visibility,"Timothy Duff, KathlÃ©n Kohn, Anton Leykin, Tomas Pajdla","CIIRC - Czech Technical University in Prague; KTH; School of Mathematics, Georgia Tech",66.66666666666666,"Czech Republic, usa",33.33333333333334,Unknown,"We present a complete classification of all minimal problems for generic arrangements of points and lines completely observed by calibrated perspective cameras. We show that there are only 30 minimal problems in total, no problems exist for more than 6 cameras, for more than 5 points, and for more than 6 lines. We present a sequence of tests for detecting minimality starting with counting degrees of freedom and ending with full symbolic and numeric verification of representative examples. For all minimal problems discovered, we present their algebraic degrees, i.e. the number of solutions, which measure their intrinsic difficulty. It shows how exactly the difficulty of problems grows with the number of views. Importantly, several new mini- mal problems have small degrees that might be practical in image matching and 3D reconstruction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Duff_PLMP_-_Point-Line_Minimal_Problems_in_Complete_Multi-View_Visibility_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Duff_PLMP_-_Point-Line_Minimal_Problems_in_Complete_Multi-View_Visibility_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009802/,"['Cameras', 'Three-dimensional displays', 'Mathematical model', 'Tensile stress', 'Image matching', 'Geometry']","['Minimization Problem', 'Minimalist', 'Image Registration', '3D Reconstruction', 'Cardinality', 'Complex Numbers', 'Distinct Lineages', 'Image Generation', 'Relevant Work', 'Correction Algorithm', 'Distinct Points', 'Place In Space', 'Line Spacing', 'Preimage', 'Camera Pose', 'Tangent Line', 'Symbolic Processing', 'Visual Line', 'Relative Pose', 'Classical State', 'Joint Image', 'Homogeneous Coordinates', 'Generic Point']",,21,"We present a complete classification of all minimal problems for generic arrangements of points and lines completely observed by calibrated perspective cameras. We show that there are only 30 minimal problems in total, no problems exist for more than 6 cameras, for more than 5 points, and for more than 6 lines. We present a sequence of tests for detecting minimality starting with counting degrees of freedom and ending with full symbolic and numeric verification of representative examples. For all minimal problems discovered, we present their algebraic degrees, i.e. the number of solutions, which measure their intrinsic difficulty. It shows how exactly the difficulty of problems grows with the number of views. Importantly, several new mini- mal problems have small degrees that might be practical in image matching and 3D reconstruction."
POD: Practical Object Detection With Scale-Sensitive Network,"Junran Peng, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan","University of Chinese Academy of Sciences; SenseTime Group Limited; Center for Research on Intelligent Perception and Computing, CASIA",100.0,"china, usa",0.0,,"Scale-sensitive object detection remains a challenging task, where most of the existing methods not learn it explicitly and not robust to scale variance. In addition, the most existing methods are less efficient during training or slow during inference, which are not friendly to real-time application. In this paper, we propose a practical object detection with scale-sensitive network.Our method first predicts a global continuous scale ,which shared by all position, for each convolution filter of each network stage. To effectively learn the scale, we average the spatial features and distill the scale from channels. For fast-deployment, we propose a scale decomposition method that transfers the robust fractional scale into combinations of fixed integral scales for each convolution filter, which exploit the dilated convolution. We demonstrate it on one-stage and two-stage algorithm under almost different configure. For practical application, training of our method is of efficiency and simplicity which gets rid of complex data sampling or optimize strategy. During testing, the proposed method requires no extra operation and is very friendly to hardware acceleration like TensorRT and TVM.On the COCO test-dev, our model could achieve a 41.5mAP on one-stage detector and 42.1 mAP on two-stage detectors based on ResNet-101, outperforming baselines by 2.4 and 2.1 respectively without extra FLOPS.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Peng_POD_Practical_Object_Detection_With_Scale-Sensitive_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_POD_Practical_Object_Detection_With_Scale-Sensitive_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010019/,"['Convolution', 'Detectors', 'Object detection', 'Hardware', 'Proposals', 'Training', 'Optimization']","['Object Detection', 'Global Scale', 'Dilated Convolution', 'Hardware Accelerators', 'Two-stage Detectors', 'One-stage Detectors', 'Fine-tuned', 'Feature Maps', 'Receptive Field', 'Bounding Box', 'Detection Task', 'Layer Model', 'Training Epochs', 'Practical Scenarios', 'Output Channels', 'Dense Connections', 'Bilinear Interpolation', 'Detection Framework', 'Whistle', 'Faster R-CNN', 'Weight Transfer', 'Fast Deployment', 'Deformable Convolution', 'Large Receptive Field', 'Receptive Field Size', 'Region Proposal Network']",,17,"Scale-sensitive object detection remains a challenging task, where most of the existing methods not learn it explicitly and not robust to scale variance. In addition, the most existing methods are less efficient during training or slow during inference, which are not friendly to real-time application. In this paper, we propose a practical object detection with scale-sensitive network.Our method first predicts a global continuous scale ,which shared by all position, for each convolution filter of each network stage. To effectively learn the scale, we average the spatial features and distill the scale from channels. For fast-deployment, we propose a scale decomposition method that transfers the robust fractional scale into combinations of fixed integral scales for each convolution filter, which exploit the dilated convolution. We demonstrate it on one-stage and two-stage algorithm under almost different configure. For practical application, training of our method is of efficiency and simplicity which gets rid of complex data sampling or optimize strategy. During testing, the proposed method requires no extra operation and is very friendly to hardware acceleration like TensorRT and TVM.On the COCO test-dev, our model could achieve a 41.5mAP on one-stage detector and 42.1 mAP on two-stage detectors based on ResNet-101, outperforming baselines by 2.4 and 2.1 respectively without extra FLOPS."
PR Product: A Substitute for Inner Product in Neural Networks,"Zhennan Wang, Wenbin Zou, Chen Xu","College of Mathematics and Statistics, Shenzhen University; College of Electronic and Information Engineering, the Shenzhen Key Laboratory of Advanced Machine Learning and Applications, the Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen University",100.0,china,0.0,,"In this paper, we analyze the inner product of weight vector w and data vector x in neural networks from the perspective of vector orthogonal decomposition and prove that the direction gradient of w decreases with the angle between them close to 0 or p. We propose the Projection and Rejection Product (PR Product) to make the direction gradient of w independent of the angle and consistently larger than the one in standard inner product while keeping the forward propagation identical. As a reliable substitute for standard inner product, the PR Product can be applied into many existing deep learning modules, so we develop the PR Product version of fully connected layer, convolutional layer and LSTM layer. In static image classification, the experiments on CIFAR10 and CIFAR100 datasets demonstrate that the PR Product can robustly enhance the ability of various state-of-the-art classification networks. On the task of image captioning, even without any bells and whistles, our PR Product version of captioning model can compete or outperform the state-of-the-art models on MS COCO dataset. Code has been made available at: https://github.com/wzn0828/PR_Product.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_PR_Product_A_Substitute_for_Inner_Product_in_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PR_Product_A_Substitute_for_Inner_Product_in_Neural_Networks_ICCV_2019_paper.pdf,,https://github.com/wzn0828/PR_Product,,main,Oral,https://ieeexplore.ieee.org/document/9010652/,"['Neural networks', 'Standards', 'Optimization', 'Backpropagation', 'Task analysis', 'Reliability', 'Computational modeling']","['Neural Network', 'Convolutional Layers', 'Image Classification', 'Weight Vector', 'Standard Product', 'Vector Data', 'Vector Production', 'Gradient Direction', 'Scalar Product', 'Whistle', 'Image Captioning', 'Orthogonal Decomposition', 'MS COCO Dataset', 'Inner Product Of Vectors', 'Training Set', 'Convolutional Neural Network', 'Deep Network', 'Recurrent Neural Network', 'Scene Graph', 'Backbone Model', 'Neural Network Optimization', 'Image Representation', 'Backward Pass', 'Deep Convolutional Neural Network', 'Shortcut Connection', 'Ensemble Strategy', 'Optimal Network', 'Visual Attention']",,4,"In this paper, we analyze the inner product of weight vector w and data vector x in neural networks from the perspective of vector orthogonal decomposition and prove that the direction gradient of w decreases with the angle between them close to 0 or π. We propose the Projection and Rejection Product (PR Product) to make the direction gradient of w independent of the angle and consistently larger than the one in standard inner product while keeping the forward propagation identical. As a reliable substitute for standard inner product, the PR Product can be applied into many existing deep learning modules, so we develop the PR Product version of fully connected layer, convolutional layer and LSTM layer. In static image classification, the experiments on CIFAR10 and CIFAR100 datasets demonstrate that the PR Product can robustly enhance the ability of various state-of-the-art classification networks. On the task of image captioning, even without any bells and whistles, our PR Product version of captioning model can compete or outperform the state-of-the-art models on MS COCO dataset. Code has been made available at: https://github.com/wzn0828/PR_Product."
PRECOG: PREdiction Conditioned on Goals in Visual Multi-Agent Settings,"Nicholas Rhinehart, Rowan McAllister, Kris Kitani, Sergey Levine","Carnegie Mellon University; University of California, Berkeley",100.0,usa,0.0,,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rhinehart_PRECOG_PREdiction_Conditioned_on_Goals_in_Visual_Multi-Agent_Settings_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rhinehart_PRECOG_PREdiction_Conditioned_on_Goals_in_Visual_Multi-Agent_Settings_ICCV_2019_paper.pdf,https://sites.google.com/view/precog,,,main,Poster,https://ieeexplore.ieee.org/document/9009551/,"['Forecasting', 'Predictive models', 'Robots', 'Automobiles', 'Planning', 'Computational modeling', 'Zirconium']","['Multi-agent Setting', 'Autonomous Vehicles', 'Forecasting Model', 'Goal Of The Agent', 'Past Positions', 'Factorization', 'Latent Variables', 'Future Conditions', 'Multiple Agents', 'Multi-agent Systems', 'Future Decisions', 'Markov Decision Process', 'Future Position', 'Autopilot', 'Human Drivers', 'Carbonic Anhydrase II', 'Imitation Learning', 'Visual Context', 'Robot Trajectory', 'One-step Prediction', 'Joint Trajectories', 'Expert Demonstrations', 'Joint Samples', 'Qualitative Examples', 'Forecasting Results', 'Evidence Lower Bound', 'Joint Model', 'Time Step', 'Planning Approach']",,220,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions."
PU-GAN: A Point Cloud Upsampling Adversarial Network,"Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng",Tel Aviv University; The Chinese University of Hong Kong,100.0,"Hong Kong, israel",0.0,,"Point clouds acquired from range scans are often sparse, noisy, and non-uniform. This paper presents a new point cloud upsampling network called PU-GAN, which is formulated based on a generative adversarial network (GAN), to learn a rich variety of point distributions from the latent space and upsample points over patches on object surfaces. To realize a working GAN network, we construct an up-down-up expansion unit in the generator for upsampling point features with error feedback and self-correction, and formulate a self-attention unit to enhance the feature integration. Further, we design a compound loss with adversarial, uniform and reconstruction terms, to encourage the discriminator to learn more latent patterns and enhance the output point distribution uniformity. Qualitative and quantitative evaluations demonstrate the quality of our results over the state-of-the-arts in terms of distribution uniformity, proximity-to-surface, and 3D reconstruction quality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_PU-GAN_A_Point_Cloud_Upsampling_Adversarial_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_PU-GAN_A_Point_Cloud_Upsampling_Adversarial_Network_ICCV_2019_paper.pdf,https://liruihui.github.io/publication/PU-GAN/,https://github.com/liruihui,,main,Poster,https://ieeexplore.ieee.org/document/9008773/,"['Three-dimensional displays', 'Generators', 'Gallium nitride', 'Feature extraction', 'Shape', 'Training', 'Network architecture']","['Point Cloud', 'Uniform Distribution', 'Generative Adversarial Networks', 'Latent Space', 'Distribution Of Points', 'Feature Points', 'Object Surface', 'Latent Patterns', 'Error Feedback', 'Training Set', 'Deep Neural Network', 'Quantitative Results', 'Network Training', 'Multilayer Perceptron', '3D Scanning', '3D Coordinates', 'Target Distribution', 'Unit Sphere', 'Confidence Value', 'Surface Reconstruction', 'Generative Adversarial Network Framework', 'Generative Adversarial Networks Model', 'Earth Moverâ€™s Distance', 'Supplementary Material For Results', 'Sparse Set', 'Raw Point Cloud']",,254,"Point clouds acquired from range scans are often sparse, noisy, and non-uniform. This paper presents a new point cloud upsampling network called PU-GAN 
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
, which is formulated based on a generative adversarial network (GAN), to learn a rich variety of point distributions from the latent space and upsample points over patches on object surfaces. To realize a working GAN network, we construct an up-down-up expansion unit in the generator for upsampling point features with error feedback and self-correction, and formulate a self-attention unit to enhance the feature integration. Further, we design a compound loss with adversarial, uniform and reconstruction terms, to encourage the discriminator to learn more latent patterns and enhance the output point distribution uniformity. Qualitative and quantitative evaluations demonstrate the quality of our results over the state-of-the-arts in terms of distribution uniformity, proximity-to-surface, and 3D reconstruction quality."
Parametric Majorization for Data-Driven Energy Minimization Methods,"Jonas Geiping, Michael Moeller","Department of Electrical Engineering and Computer Science, University of Siegen",100.0,Germany,0.0,,"Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric energy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Geiping_Parametric_Majorization_for_Data-Driven_Energy_Minimization_Methods_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Geiping_Parametric_Majorization_for_Data-Driven_Energy_Minimization_Methods_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009002/,"['Training', 'Optimization', 'Computer vision', 'Computational modeling', 'Task analysis', 'Minimization methods', 'Machine learning']","['Energy Minimization', 'Majorization', 'Energy Minimization Method', 'Deep Learning', 'Computer Vision', 'Efficient Algorithm', 'Classification Tool', 'Computer Vision Applications', 'Strategy Framework', 'Bilevel Optimization', 'Bilevel Problem', 'Bilevel Optimization Problem', 'Loss Function', 'Convolution', 'Support Vector Machine', 'Deep Neural Network', 'Denoising', 'Large Amount Of Data', 'Parametrized', 'Feed-forward Network', 'Bregman Divergence', 'Strongly Convex', 'Fenchel Conjugate', 'Convex Analysis', 'Dual Form', 'Image X', 'Gradient Penalty', 'Log Loss', 'Surrogate Function', 'Toy Example']",,,"Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric energy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data."
Pareto Meets Huber: Efficiently Avoiding Poor Minima in Robust Estimation,"Christopher Zach, Guillaume Bourmaud","University of Bordeaux, France; Chalmers University of Technology, Sweden",100.0,"France, sweden",0.0,,"Robust cost optimization is the task of fitting parameters to data points containing outliers. In particular, we focus on large-scale computer vision problems, such as bundle adjustment, where Non-Linear Least Square (NLLS) solvers are the current workhorse. In this context, NLLS-based state of the art algorithms have been designed either to quickly improve the target objective and find a local minimum close to the initial value of the parameters, or to have a strong ability to escape poor local minima. In this paper, we propose a novel algorithm relying on multi-objective optimization which allows to match those two properties. We experimentally demonstrate that our algorithm has an ability to escape poor local minima that is on par with the best performing algorithms with a faster decrease of the target objective.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zach_Pareto_Meets_Huber_Efficiently_Avoiding_Poor_Minima_in_Robust_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zach_Pareto_Meets_Huber_Efficiently_Avoiding_Poor_Minima_in_Robust_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010256/,"['Robustness', 'Kernel', 'Estimation', 'Cost function', 'Minimization', 'Computer vision']","['Computer Vision', 'Local Minima', 'Multi-objective Optimization', 'Nonlinear Least Squares', 'Target Object', 'Large-scale Problems', 'Robust Optimization', 'Computer Vision Problems', 'Bundle Adjustment', 'Robust Cost', 'Cost Function', 'Convergence Rate', 'Objective Value', 'Stopping Criterion', 'Quadratic Model', 'Failure Cases', 'Quadratic Programming', 'Current Solution', 'Problem Instances', 'Levenberg-Marquardt Algorithm', 'Nonlinear Least Squares Problem', 'Least Squares Problem', 'Observed Data Points', 'Dense Correspondence', 'Computer Vision Applications', 'Descent Direction', 'Maximum A Posteriori']",,2,"Robust cost optimization is the task of fitting parameters to data points containing outliers. In particular, we focus on large-scale computer vision problems, such as bundle adjustment, where Non-Linear Least Square (NLLS) solvers are the current workhorse. In this context, NLLS-based state of the art algorithms have been designed either to quickly improve the target objective and find a local minimum close to the initial value of the parameters, or to have a strong ability to escape poor local minima. In this paper, we propose a novel algorithm relying on multi-objective optimization which allows to match those two properties. We experimentally demonstrate that our algorithm has an ability to escape poor local minima that is on par with the best performing algorithms with a faster decrease of the target objective."
Perceptual Deep Depth Super-Resolution,"Oleg Voynov, Alexey Artemov, Vage Egiazarian, Alexander Notchenko, Gleb Bobrovskikh, Evgeny Burnaev, Denis Zorin",New York University; Higher School of Economics; Skolkovo Institute of Science and Technology,100.0,"USA, canada, russia, usa",0.0,,"RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf,http://adase.group/3ddl/projects/perceptual-depth-sr,,,main,Poster,https://ieeexplore.ieee.org/document/9009004/,"['Image resolution', 'Visualization', 'Rendering (computer graphics)', 'Three-dimensional displays', 'Loss measurement', 'Optimization']","['Depth Super-resolution', 'Virtually', 'Deep Learning', 'Depth Map', '3D Surface', '3D Shape', 'RGB-D Images', 'Loss Function', 'Neural Network', 'Illumination', 'Mean Square Error', 'Absolute Difference', 'Convolutional Neural Network', 'Quality Measures', 'Smooth Surface', 'Square Deviation', 'Generative Adversarial Networks', 'Mean Absolute Deviation', 'Peak Signal-to-noise Ratio', 'Depth Values', 'Structural Similarity Index Measure', 'Super-resolution Problem', 'Inpainting', 'Dictionary Learning', 'Neural Network-based Methods', 'Normal Map', 'Direct Light', 'Root Mean Square Error', 'RGB Images', 'Structured Illumination']",,33,"RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques."
Permutation-Invariant Feature Restructuring for Correlation-Aware Image Set-Based Recognition,"Xiaofeng Liu, Zhenhua Guo, Site Li, Lingsheng Kong, Ping Jia, Jane You, B.V.K. Vijaya Kumar","Carnegie Mellon University; Harvard University; Carnegie Mellon University; Graduate School at Shenzhen, Tsinghua University; CIOMP, Chinese Academy of Sciences; The Hong Kong Polytechnic University; Carnegie Mellon University",100.0,"China, Hong Kong, china, usa",0.0,,"We consider the problem of comparing the similarity of image sets with variable-quantity, quality and un-ordered heterogeneous images. We use feature restructuring to exploit the correlations of both inner&inter-set images. Specifically, the residual self-attention can effectively restructure the features using the other features within a set to emphasize the discriminative images and eliminate the redundancy. Then, a sparse/collaborative learning-based dependency-guided representation scheme reconstructs the probe features conditional to the gallery features in order to adaptively align the two sets. This enables our framework to be compatible with both verification and open-set identification. We show that the parametric self-attention network and non-parametric dictionary learning can be trained end-to-end by a unified alternative optimization scheme, and that the full framework is permutation-invariant. In the numerical experiments we conducted, our method achieves top performance on competitive image set/video-based face recognition and person re-identification benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Permutation-Invariant_Feature_Restructuring_for_Correlation-Aware_Image_Set-Based_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Permutation-Invariant_Feature_Restructuring_for_Correlation-Aware_Image_Set-Based_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009464/,"['Correlation', 'Feature extraction', 'Redundancy', 'Image recognition', 'Face', 'Probes', 'Image reconstruction']","['Face Recognition', 'Dictionary Learning', 'Alternating Optimization', 'Sample Set', 'Convolutional Neural Network', 'False Positive Rate', 'Similarity Measure', 'Training Time', 'Video Frames', 'Average Pooling', 'Face Images', 'Images In Set', 'Still Images', 'Residual Term', 'Illumination Variations', 'ResNet-50 Backbone', 'Verification Task', 'Single Feature Vector', 'Gallery Set', 'Pairwise Network']",,21,"We consider the problem of comparing the similarity of image sets with variable-quantity, quality and un-ordered heterogeneous images. We use feature restructuring to exploit the correlations of both inner&inter-set images. Specifically, the residual self-attention can effectively restructure the features using the other features within a set to emphasize the discriminative images and eliminate the redundancy. Then, a sparse/collaborative learning-based dependency-guided representation scheme reconstructs the probe features conditional to the gallery features in order to adaptively align the two sets. This enables our framework to be compatible with both verification and open-set identification. We show that the parametric self-attention network and non-parametric dictionary learning can be trained end-to-end by a unified alternative optimization scheme, and that the full framework is permutation-invariant. In the numerical experiments we conducted, our method achieves top performance on competitive image set/video-based face recognition and person re-identification benchmarks."
Person Search by Text Attribute Query As Zero-Shot Learning,"Qi Dong, Shaogang Gong, Xiatian Zhu",Vision Semantics Ltd.; Queen Mary University of London,50.0,uk,50.0,UK,"Existing person search methods predominantly assume the availability of at least one-shot imagery sample of the queried person. This assumption is limited in circumstances where only a brief textual (or verbal) description of the target person is available. In this work, we present a deep learning method for attribute text description based person search without any query imagery. Whilst conventional cross-modality matching methods, such as global visual-textual embedding based zero-shot learning and local individual attribute recognition, are functionally applicable, they are limited by several assumptions invalid to person search in deployment scale, data quality, and/or category name semantics. We overcome these issues by formulating an Attribute-Image Hierarchical Matching (AIHM) model. It is able to more reliably match text attribute descriptions with noisy surveillance person images by jointly learning global category-level and local attribute-level textual-visual embedding as well as matching. Extensive evaluations demonstrate the superiority of our AIHM model over a wide variety of state-of-the-art methods on three publicly available attribute labelled surveillance person search benchmarks: Market-1501, DukeMTMC, and PA100K.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Person_Search_by_Text_Attribute_Query_As_Zero-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Person_Search_by_Text_Attribute_Query_As_Zero-Shot_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010616/,"['Visualization', 'Semantics', 'Surveillance', 'Training', 'Computational modeling', 'Videos', 'Training data']","['Zero-shot', 'Imagery', 'Textual Descriptions', 'Matching Model', 'Person Image', 'Joint Learning', 'Category Names', 'Local Embedding', 'Ambiguity', 'Training Data', 'Visual Representation', 'Challenging Problem', 'Latent Space', 'Average Pooling', 'Joint Optimization', 'Visual Properties', 'Nearest Neighbor Search', 'Multi-task Learning', 'Video Surveillance', 'Description Language', 'Query Image', 'Categories Of Persons', 'Final Embedding', 'Noisy Observations', 'Natural Language Descriptions', 'Training Pairs', 'Matching Loss', 'Complex Sentences', 'Meaningful Categories', 'Natural Language']",,27,"Existing person search methods predominantly assume the availability of at least one-shot imagery sample of the queried person. This assumption is limited in circumstances where only a brief textual (or verbal) description of the target person is available. In this work, we present a deep learning method for attribute text description based person search without any query imagery. Whilst conventional cross-modality matching methods, such as global visual-textual embedding based zero-shot learning and local individual attribute recognition, are functionally applicable, they are limited by several assumptions invalid to person search in deployment scale, data quality, and/or category name semantics. We overcome these issues by formulating an Attribute-Image Hierarchical Matching (AIHM) model. It is able to more reliably match text attribute descriptions with noisy surveillance person images by jointly learning global category-level and local attribute-level textual-visual embedding as well as matching. Extensive evaluations demonstrate the superiority of our AIHM model over a wide variety of state-of-the-art methods on three publicly available attribute labelled surveillance person search benchmarks: Market-1501, DukeMTMC, and PA100K."
Person-in-WiFi: Fine-Grained Person Perception Using WiFi,"Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, Dong Huang","†School of Cyber Science and Technology, Zhejiang University §Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies; ‡Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University; ¶The Robotics Institute, Carnegie Mellon University; †School of Cyber Science and Technology, Zhejiang University ¶The Robotics Institute, Carnegie Mellon University",100.0,"China, china, usa",0.0,,"Fine-grained person perception such as body segmentation and pose estimation has been achieved with many 2D and 3D sensors such as RGB/depth cameras, radars (e.g. RF-Pose), and LiDARs. These solutions require 2D images, depth maps or 3D point clouds of person bodies as input. In this paper, we take one step forward to show that fine-grained person perception is possible even with 1D sensors: WiFi antennas. Specifically, we used two sets of WiFi antennas to acquire signals, i.e., one transmitter set and one receiver set. Each set contains three antennas horizontally lined-up as a regular household WiFi router. The WiFi signal generated by a transmitter antenna, penetrates through and reflects on human bodies, furniture, and walls, and then superposes at a receiver antenna as 1D signal samples. We developed a deep learning approach that uses annotations on 2D images, takes the received 1D WiFi signals as input, and performs body segmentation and pose estimation in an end-to-end manner. To our knowledge, our solution is the first work based on off-the-shelf WiFi antennas and standard IEEE 802.11n WiFi signals. Demonstrating comparable results to image-based solutions, our WiFi-based person perception solution is cheaper and more ubiquitous than radars and LiDARs, while invariant to illumination and has little privacy concern comparing to cameras.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Person-in-WiFi_Fine-Grained_Person_Perception_Using_WiFi_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Person-in-WiFi_Fine-Grained_Person_Perception_Using_WiFi_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008282/,"['Wireless fidelity', 'Receiving antennas', 'Sensors', 'Two dimensional displays', 'Cameras', 'Laser radar']","['Social Perception', 'Deep Learning', '2D Images', 'Depth Map', 'Body Segments', 'Pose Estimation', '3D Point Cloud', 'Receiver Antenna', 'Transmitter Antenna', 'WiFi Signals', 'Feature Maps', 'Electromagnetic Wave', 'Video Frames', 'Antenna Array', 'Angular Resolution', 'Orthogonal Frequency Division Multiplexing', 'Faster R-CNN', 'Multiple Cameras', 'Simultaneous Localization And Mapping', 'Indoor Localization', 'Mask R-CNN', 'Body Joints', 'Frequency Modulated Continuous Wave', 'Wi-Fi Devices', 'High-quality Annotations', 'Human Pose Estimation', 'Electromagnetic Frequency', 'Input Tensor', 'L2 Loss', 'Output Tensor']",,117,"Fine-grained person perception such as body segmentation and pose estimation has been achieved with many 2D and 3D sensors such as RGB/depth cameras, radars (e.g. RF-Pose), and LiDARs. These solutions require 2D images, depth maps or 3D point clouds of person bodies as input. In this paper, we take one step forward to show that fine-grained person perception is possible even with 1D sensors: WiFi antennas. Specifically, we used two sets of WiFi antennas to acquire signals, i.e., one transmitter set and one receiver set. Each set contains three antennas horizontally lined-up as a regular household WiFi router. The WiFi signal generated by a transmitter antenna, penetrates through and reflects on human bodies, furniture, and walls, and then superposes at a receiver antenna as 1D signal samples. We developed a deep learning approach that uses annotations on 2D images, takes the received 1D WiFi signals as input, and performs body segmentation and pose estimation in an end-to-end manner. To our knowledge, our solution is the first work based on off-the-shelf WiFi antennas and standard IEEE 802.11n WiFi signals. Demonstrating comparable results to image-based solutions, our WiFi-based person perception solution is cheaper and more ubiquitous than radars and LiDARs, while invariant to illumination and has little privacy concern comparing to cameras."
Personalized Fashion Design,"Cong Yu, Yang Hu, Yan Chen, Bing Zeng","School of Information and Communication Engineering, University of Electronic Science and Technology of China",100.0,china,0.0,,"Fashion recommendation is the task of suggesting a fashion item that fits well with a given item. In this work, we propose to automatically synthesis new items for recommendation. We jointly consider the two key issues for the task, i.e., compatibility and personalization. We propose a personalized fashion design framework with the help of generative adversarial training. A convolutional network is first used to map the query image into a latent vector representation. This latent representation, together with another vector which characterizes user's style preference, are taken as the input to the generator network to generate the target item image. Two discriminator networks are built to guide the generation process. One is the classic real/fake discriminator. The other is a matching network which simultaneously models the compatibility between fashion items and learns users' preference representations. The performance of the proposed method is evaluated on thousands of outfits composited by online users. The experiments show that the items generated by our model are quite realistic. They have better visual quality and higher matching degree than those generated by alternative methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Personalized_Fashion_Design_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Personalized_Fashion_Design_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010857/,"['Clothing', 'Gallium nitride', 'Task analysis', 'Computer vision', 'Ions', 'Training', 'Generators']","['Fashion Design', 'Visual Quality', 'Latent Representation', 'Online Users', 'Latent Vector', 'Matching Network', 'Discriminator Network', 'Query Image', 'Item Generation', 'Representation Of Orientation', 'Style Preferences', 'Fashion Items', 'Generative Adversarial Networks', 'Image Generation', 'Baseline Methods', 'User Preferences', 'Siamese Network', 'Conditional Generative Adversarial Network', 'Translation Problems', 'Fake Data', 'Discrete Labels', 'Fr√©chet Inception Distance']",,35,"Fashion recommendation is the task of suggesting a fashion item that fits well with a given item. In this work, we propose to automatically synthesis new items for recommendation. We jointly consider the two key issues for the task, i.e., compatibility and personalization. We propose a personalized fashion design framework with the help of generative adversarial training. A convolutional network is first used to map the query image into a latent vector representation. This latent representation, together with another vector which characterizes user's style preference, are taken as the input to the generator network to generate the target item image. Two discriminator networks are built to guide the generation process. One is the classic real/fake discriminator. The other is a matching network which simultaneously models the compatibility between fashion items and learns users' preference representations. The performance of the proposed method is evaluated on thousands of outfits composited by online users. The experiments show that the items generated by our model are quite realistic. They have better visual quality and higher matching degree than those generated by alternative methods."
Perspective-Guided Convolution Networks for Crowd Counting,"Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, Errui Ding","Harbin Institute of Technology; Department of Computer Vision Technology (VIS), Baidu Inc.",100.0,china,0.0,,"In this paper, we propose a novel perspective-guided convolution (PGC) for convolutional neural network (CNN) based crowd counting (i.e. PGCNet), which aims to overcome the dramatic intra-scene scale variations of people due to the perspective effect. While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. PGCNet, on the other hand, utilizes perspective information to guide the spatially variant smoothing of feature maps before feeding them to the successive convolutions. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained. Our PGCNet is single-column with moderate increase in computation, and extensive experimental results on four benchmark datasets show the improvements of our method against the state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a large scale dataset for crowd counting that contains 13,000+ high-resolution images with challenging scenarios. Code is available at https://github.com/Zhaoyi-Yan/PGCNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Perspective-Guided_Convolution_Networks_for_Crowd_Counting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Perspective-Guided_Convolution_Networks_for_Crowd_Counting_ICCV_2019_paper.pdf,,https://github.com/Zhaoyi-Yan/PGCNet,,main,Poster,https://ieeexplore.ieee.org/document/9010874/,"['Convolution', 'Estimation', 'Smoothing methods', 'Kernel', 'Computer architecture', 'Computer vision', 'Benchmark testing']","['Crowd Counting', 'Convolutional Neural Network', 'High-resolution Images', 'Feature Maps', 'Large-scale Datasets', 'Scale Variation', 'Challenging Scenarios', 'Discrete Scale', 'Computational Increase', 'Large Scale', 'Appended', 'Density Map', 'Receptive Field', 'Training Strategy', 'Stochastic Gradient Descent', 'Encoder-decoder', 'Posterior Mode', 'CNN-based Methods', 'Dilated Convolution', 'Pre-trained Weights', 'International Exhibition', 'Crowd Density', 'Features Of Different Scales', 'Latent Code', 'Complicated Background', 'ResNet-101 Backbone']",,135,"In this paper, we propose a novel perspective-guided convolution (PGC) for convolutional neural network (CNN) based crowd counting (i.e. PGCNet), which aims to overcome the dramatic intra-scene scale variations of people due to the perspective effect. While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. PGCNet, on the other hand, utilizes perspective information to guide the spatially variant smoothing of feature maps before feeding them to the successive convolutions. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained. Our PGCNet is single-column with moderate increase in computation, and extensive experimental results on four benchmark datasets show the improvements of our method against the state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a large scale dataset for crowd counting that contains 13,000+ high-resolution images with challenging scenarios. Code is available at https://github.com/Zhaoyi-Yan/PGCNet."
Photo-Realistic Facial Details Synthesis From Single Image,"Anpei Chen, Zhang Chen, Guli Zhang, Kenny Mitchell, Jingyi Yu",Edinburgh Napier University; ShanghaiTech University,100.0,"United Kingdom, china",0.0,,"We present a single-image 3D face synthesis technique that can handle challenging facial expressions while recovering fine geometric details. Our technique employs expression analysis for proxy face geometry generation and combines supervised and unsupervised learning for facial detail synthesis. On proxy generation, we conduct emotion prediction to determine a new expression-informed proxy. On detail synthesis, we present a Deep Facial Detail Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs both geometry and appearance loss functions. For geometry, we capture 366 high-quality 3D scans from 122 different subjects under 3 facial expressions. For appearance, we use additional 163K in-the-wild face images and apply image-based rendering to accommodate lighting variations. Comprehensive experiments demonstrate that our framework can produce high-quality 3D faces with realistic details under challenging facial expressions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Photo-Realistic_Facial_Details_Synthesis_From_Single_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Photo-Realistic_Facial_Details_Synthesis_From_Single_Image_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008263/,"['Three-dimensional displays', 'Face', 'Geometry', 'Shape', 'Cameras', 'Image reconstruction', 'Two dimensional displays']","['Single Image', 'Facial Details', 'Loss Function', 'Facial Expressions', 'Unsupervised Learning', 'Fine Details', 'Face Images', '3D Scanning', 'Detailed Synthesis', 'Geometric Details', 'High-quality 3D', '3D Face', 'Convolutional Neural Network', 'Input Image', 'Wrinkles', '2D Images', 'Surface Reflectance', 'Human Faces', 'Light Environment', 'Appearance Features', 'Displacement Maps', 'Nasolabial Fold', 'Multi-view Stereo', 'Face Model', 'Principal Component Analysis Space', 'HSV Color', 'Light Stage', 'Emotional Features', 'Spherical Harmonics', 'Normal Map']",,84,"We present a single-image 3D face synthesis technique that can handle challenging facial expressions while recovering fine geometric details. Our technique employs expression analysis for proxy face geometry generation and combines supervised and unsupervised learning for facial detail synthesis. On proxy generation, we conduct emotion prediction to determine a new expression-informed proxy. On detail synthesis, we present a Deep Facial Detail Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs both geometry and appearance loss functions. For geometry, we capture 366 high-quality 3D scans from 122 different subjects under 3 facial expressions. For appearance, we use additional 163K in-the-wild face images and apply image-based rendering to accommodate lighting variations. Comprehensive experiments demonstrate that our framework can produce high-quality 3D faces with realistic details under challenging facial expressions."
Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks,"Zhe He, Adrian Spurr, Xucong Zhang, Otmar Hilliges","AIT Lab, ETH Zürich & Institute of Neuroinformatics, ETH Zürich & University of Zürich; AIT Lab, ETH Zürich",100.0,Switzerland,0.0,,"Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008804/,"['Task analysis', 'Gallium nitride', 'Estimation', 'Training', 'Generators', 'Training data', 'Three-dimensional displays']","['Generative Adversarial Networks', 'Gaze Redirection', 'Monocular Gaze', 'Training Data', 'Image Quality', 'Videoconferencing', 'Perceptual Similarity', 'Gaze Direction', 'Adversarial Training', 'Eye Images', 'Eye Patch', 'Photo-realistic Images', 'Cycle Consistency Loss', 'Appearance Of Areas', 'Convolutional Neural Network', 'Deep Network', 'Input Image', 'Quantitative Evaluation', 'User Study', 'Eye Contact', 'Ground Truth Image', 'Pitch Angle', 'Yaw Angle', 'Perceptual Loss', 'Angular Error', 'Blurry Images', 'Generative Adversarial Network Framework', 'Head Pose', 'Loss Term', 'Quantitative Experiments']",,37,"Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data."
Photorealistic Style Transfer via Wavelet Transforms,"Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, Jung-Woo Ha","Clova AI Research, NAVER Corp.",0.0,,100.0,China,"Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT2) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize a 1024x1024 resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. Our code, generated images, pre-trained models and supplementary documents are all available at https://github.com/ClovaAI/WCT2.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.pdf,,https://github.com/ClovaAI/WCT2,,main,Poster,https://ieeexplore.ieee.org/document/9010913/,"['Decoding', 'Wavelet transforms', 'Distortion', 'Network architecture', 'Image resolution', 'Image reconstruction']","['Wavelet Transform', 'Style Transfer', 'Pleased', 'Spatial Distortion', 'Reference Style', 'Decoding', 'Low-pass', 'User Study', 'Singular Value Decomposition', 'Semantic Segmentation', 'Fine Details', 'Optical Flow', 'Inference Time', 'Post-processing Step', 'Pre-trained Network', 'Single Pass', 'High Frequency Components', 'Semantic Map', 'Signal Reconstruction', 'Noise Amplification', 'Multilevel Strategy', 'Haar Wavelet', 'Material For More Details', 'Style Image', 'VGG Network', 'Frequency Components', 'Supplementary Material For Results', 'Average Pooling']",,227,"Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT2) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize a 1024x1024 resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. Our code, generated images, pre-trained models and supplementary documents are all available at https://github.com/ClovaAI/WCT2."
Phrase Localization Without Paired Training Examples,"Josiah Wang, Lucia Specia",Imperial College London,100.0,uk,0.0,,"Localizing phrases in images is an important part of image understanding and can be useful in many applications that require mappings between textual and visual information. Existing work attempts to learn these mappings from examples of phrase-image region correspondences (strong supervision) or from phrase-image pairs (weak supervision). We postulate that such paired annotations are unnecessary, and propose the first method for the phrase localization problem where neither training procedure nor paired, task-specific data is required. Our method is simple but effective: we use off-the-shelf approaches to detect objects, scenes and colours in images, and explore different approaches to measure semantic similarity between the categories of detected visual elements and words in phrases. Experiments on two well-known phrase localization datasets show that this approach surpasses all weakly supervised methods by a large margin and performs very competitively to strongly supervised methods, and can thus be considered a strong baseline to the task. The non-paired nature of our method makes it applicable to any domain and where no paired phrase localization annotation is available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Phrase_Localization_Without_Paired_Training_Examples_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Phrase_Localization_Without_Paired_Training_Examples_ICCV_2019_paper.pdf,http://www.josiahwang.com,,,main,Oral,https://ieeexplore.ieee.org/document/9009480/,"['Detectors', 'Training', 'Task analysis', 'Visualization', 'Image color analysis', 'Semantics', 'Proposals']","['Training Examples', 'Training Pairs', 'Paired Training Examples', 'Phrase Localization', 'Semantic Similarity', 'Strong Baseline', 'Weak Supervision', 'Word Phrase', 'Strong Supervision', 'Upper Bound', 'Convolutional Neural Network', 'Training Time', 'State Of The Art', 'Localization Accuracy', 'Object Detection', 'Deep Convolutional Neural Network', 'Bounding Box', 'Local Strategies', 'Local Image', 'Word Embedding', 'Combined Detection', 'Minimum Bounding Box', 'Semantic Similarity Measures', 'Color Word', 'Region Proposal', 'Image Categories', 'Image Captioning', 'Validation Split', 'Noun Phrase', 'Object Instances']",,23,"Localizing phrases in images is an important part of image understanding and can be useful in many applications that require mappings between textual and visual information. Existing work attempts to learn these mappings from examples of phrase-image region correspondences (strong supervision) or from phrase-image pairs (weak supervision). We postulate that such paired annotations are unnecessary, and propose the first method for the phrase localization problem where neither training procedure nor paired, task-specific data is required. Our method is simple but effective: we use off-the-shelf approaches to detect objects, scenes and colours in images, and explore different approaches to measure semantic similarity between the categories of detected visual elements and words in phrases. Experiments on two well-known phrase localization datasets show that this approach surpasses all weakly supervised methods by a large margin and performs very competitively to strongly supervised methods, and can thus be considered a strong baseline to the task. The non-paired nature of our method makes it applicable to any domain and where no paired phrase localization annotation is available."
Physical Adversarial Textures That Fool Visual Object Tracking,"Rey Reza Wiyatno, Anqi Xu","Element AI, Montreal, Canada",0.0,,100.0,Canada,"We present a method for creating inconspicuous-looking textures that, when displayed as posters in the physical world, cause visual object tracking systems to become confused. As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade. This adversarial attack evaluates several optimization strategies for fooling seldom-targeted regression models: non-targeted, targeted, and a newly-coined family of guided adversarial losses. Also, while we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different scene variables to find practical attack setups with high resulting adversarial strength and convergence speed. We further showcase that textures optimized using simulated scenes can confuse real-world tracking systems for cameras and robots.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wiyatno_Physical_Adversarial_Textures_That_Fool_Visual_Object_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wiyatno_Physical_Adversarial_Textures_That_Fool_Visual_Object_Tracking_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009841/,"['Target tracking', 'Task analysis', 'Cameras', 'Visualization', 'Object tracking', 'Convergence']","['Object Tracking', 'Visual Object Tracking', 'Adversarial Textures', 'Tracking System', 'Physical World', 'Track Model', 'Adversarial Attacks', 'Simulated Scene', 'Illumination', 'Imitation', 'Step Size', 'Target Location', 'Fully-connected Layer', 'Source Images', 'Training Loss', 'Perceptual Similarity', 'Current Frame', 'Image Synthesis', 'Previous Frame', 'Camera Frame', 'Adversarial Examples', 'Humanoid Robot', 'Search Area', 'Visual Servoing', 'Adversarial Perturbations', 'Camera Pose', 'Ground Truth Values', 'Steering Angle']",,37,"We present a method for creating inconspicuous-looking textures that, when displayed as posters in the physical world, cause visual object tracking systems to become confused. As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade. This adversarial attack evaluates several optimization strategies for fooling seldom-targeted regression models: non-targeted, targeted, and a newly-coined family of guided adversarial losses. Also, while we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different scene variables to find practical attack setups with high resulting adversarial strength and convergence speed. We further showcase that textures optimized using simulated scenes can confuse real-world tracking systems for cameras and robots."
Physics-Based Rendering for Improving Robustness to Rain,"Shirsendu Sukanta Halder, Jean-FranÃ§ois Lalonde, Raoul de Charette","Inria, Paris, France; Université Laval, Québec, Canada",100.0,"France, canada",0.0,,"To improve the robustness to rain, we present a physically-based rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection and 60% for semantic segmentation. Furthermore, we show refining existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15% for object detection and 35% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Halder_Physics-Based_Rendering_for_Improving_Robustness_to_Rain_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Halder_Physics-Based_Rendering_for_Improving_Robustness_to_Rain_ICCV_2019_paper.pdf,https://team.inria.fr/rits/computer-vision/weather-augment/,,,main,Poster,https://ieeexplore.ieee.org/document/9010641/,"['Rain', 'Rendering (computer graphics)', 'Cameras', 'Object detection', 'Image segmentation', 'Databases']","['Illumination', 'User Study', 'Object Detection', 'Semantic Segmentation', 'Robust Algorithm', 'Physical Simulation', 'Project Page', 'Amount Of Rain', 'Clear Weather', 'Semantic Segmentation Algorithms', 'Field Of View', 'Fine-tuned', 'Computer Vision', 'Qualitative Results', 'Rain Events', 'Faster R-CNN', 'Curriculum Learning', 'Environment Map', 'Computer Vision Algorithms', 'High Processing Cost', 'Mean Opinion Score', 'Alpha Matte']",,72,"To improve the robustness to rain, we present a physically-based rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection and 60% for semantic segmentation. Furthermore, we show refining existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15% for object detection and 35% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page."
Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation,"Kiru Park, Timothy Patten, Markus Vincze","Vision for Robotics Laboratory, Automation and Control Institute, TU Wien, Austria",100.0,Austria,0.0,,"Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008819/,"['Three-dimensional displays', 'Solid modeling', 'Training', 'Pose estimation', 'Robustness', 'Two dimensional displays']","['Pose Estimation', '6D Pose', 'Coordinate Regression', '6D Pose Estimation', 'Loss Function', 'RGB Images', '3D Coordinates', 'Pixel Coordinates', 'Random Sample Consensus', 'Object Pose', 'Object Pixels', 'Pose Estimation Methods', 'Symmetric Objects', 'Transformer Loss', 'Texture Model', 'Convolutional Neural Network', 'Prediction Error', 'Object Detection', 'Color Images', 'Training Images', 'Generative Adversarial Networks', 'Human Pose Estimation', 'Bounding Box', 'Target Image', 'Network Output', 'Pose Prediction', 'Ground Truth Pose', 'Faster R-CNN', 'CAD Model', 'Objective Estimates']",,306,"Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images."
Pix2Vox: Context-Aware 3D Reconstruction From Single and Multi-View Images,"Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Shengping Zhang","Harbin Institute of Technology; Harbin Institute of Technology, Peng Cheng Laboratory; SenseTime Research",66.66666666666666,china,33.33333333333334,China,"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.pdf,https://haozhexie.com/project/pix2vox,,,main,Poster,https://ieeexplore.ieee.org/document/9009507/,"['Three-dimensional displays', 'Image reconstruction', 'Shape', 'Feature extraction', 'Decoding', 'Solid modeling', 'Kernel']","['3D Reconstruction', 'Multi-view Images', 'Deep Neural Network', 'Input Image', 'Feature Maps', 'Recurrent Neural Network', 'Large Margin', 'RGB Images', '3D Volume', 'Reconstruction Results', 'Long-term Loss', 'Refiner', 'Reconstruction Framework', 'Convolutional Layers', 'Quantitative Evaluation', 'Intersection Over Union', 'Point Cloud', 'Multiple Images', 'Generative Adversarial Networks', 'Unseen Objects', 'Simultaneous Localization And Mapping', 'Real-world Images', 'Synthetic Images', 'Batch Normalization Layer', 'Structure From Motion', '3D Shape', 'Leaky ReLU Activation', 'Variational Autoencoder']",,191,"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method."
Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation,"Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu","Nuro, Inc.; Google LLC; Fudan University",33.33333333333333,China,66.66666666666667,USA,"We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wen_Pixel2Mesh_Multi-View_3D_Mesh_Generation_via_Deformation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wen_Pixel2Mesh_Multi-View_3D_Mesh_Generation_via_Deformation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010697/,"['Shape', 'Three-dimensional displays', 'Strain', 'Solid modeling', 'Color', 'Cameras', 'Geometry']","['3D Mesh', 'Input Image', 'Color Images', 'Multiple Images', 'Generalization Capability', '3D Shape', 'Nearby Areas', 'Graph Convolutional Network', 'Camera Pose', 'Accurate Shape', 'Mesh Representation', 'Vertex Locations', 'Contralateral', 'Deep Learning', 'Deep Models', 'Single Image', 'Point Cloud', 'Statistical Features', '3D Volume', 'Iterative Refinement', 'Shape Priors', 'Feature Pooling', 'Chamfer Distance', 'Multi-view Stereo', 'Standard Evaluation Metrics', 'Multi-view Feature', 'Detailed Surface', 'Camera Intrinsics', 'Large Baseline', 'Mesh Model']",,160,"We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization."
Point-Based Multi-View Stereo Network,"Rui Chen, Songfang Han, Jing Xu, Hao Su","University of California, San Diego; Tsinghua University; The Hong Kong University of Science and Technology",100.0,"China, Hong Kong, usa",0.0,,"We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. Our source code and trained models are available at https://github.com/callmeray/PointMVSNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Point-Based_Multi-View_Stereo_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Point-Based_Multi-View_Stereo_Network_ICCV_2019_paper.pdf,,https://github.com/callmeray/PointMVSNet,,main,Oral,https://ieeexplore.ieee.org/document/9010428/,"['Three-dimensional displays', 'Cameras', 'Two dimensional displays', 'Geometry', 'Image reconstruction', 'Task analysis', 'Feature extraction']","['Multi-view Stereo', 'Point Cloud', 'Depth Map', 'Reconstruction Quality', '3D Geometry', '3D Flow', 'Cost Volume', 'Low Resolution', 'Image Features', 'Input Image', 'Feature Maps', '3D Space', '3D Volume', '3D Point', 'Neighboring Points', 'Depth Estimation', 'Iterative Refinement', 'Feature Pyramid', 'Spatial Partitioning', 'Flow Prediction', 'Earth Moverâ€™s Distance', 'Multi-view Images', 'Point Cloud Reconstruction', 'Depth Prediction', 'Perspective Transformation', 'Multi-scale Convolutional Neural Network', 'Image Appearance', 'Feature Points', 'Feature Aggregation']",,201,"We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. Our source code and trained models are available at https://github.com/callmeray/PointMVSNet."
Point-to-Point Video Generation,"Tsun-Hsuan Wang, Yen-Chi Cheng, Chieh Hubert Lin, Hwann-Tzong Chen, Min Sun",National Tsing Hua University,100.0,taiwan,0.0,,"While image synthesis achieves tremendous breakthroughs (e.g., generating realistic faces), video generation is less explored and harder to control, which limits its applications in the real world. For instance, video editing requires temporal coherence across multiple clips and thus poses both start and end constraints within a video sequence. We introduce point-to-point video generation that controls the generation process with two control points: the targeted start- and end-frames. The task is challenging since the model not only generates a smooth transition of frames but also plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of various lengths. We propose to maximize the modified variational lower bound of conditional data likelihood under a skip-frame training strategy. Our model can generate end-frame-consistent sequences without loss of quality and diversity. We evaluate our method through extensive experiments on Stochastic Moving MNIST, Weizmann Action, Human3.6M, and BAIR Robot Pushing under a series of scenarios. The qualitative results showcase the effectiveness and merits of point-to-point generation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Point-to-Point_Video_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Point-to-Point_Video_Generation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010312/,"['Training', 'Controllability', 'Decoding', 'Skeleton', 'Process control', 'Image generation', 'Interpolation']","['Video Generation', 'Generation Process', 'Qualitative Results', 'Control Points', 'Video Sequences', 'Image Synthesis', 'Conditional Likelihood', 'Temporal Coherence', 'Training Set', 'Maximum Likelihood Estimation', 'Latent Variables', 'Kullback-Leibler', 'Latent Space', 'Velocity Vector', 'Peak Signal-to-noise Ratio', 'Robotic Arm', 'Final Objective', 'Counting Time', 'Variational Autoencoder', 'Decoding Process', 'Alignment Loss', '3D Skeleton', 'Background Clutter', 'Time Budget', 'Global Descriptors']",,15,"While image synthesis achieves tremendous breakthroughs (e.g., generating realistic faces), video generation is less explored and harder to control, which limits its applications in the real world. For instance, video editing requires temporal coherence across multiple clips and thus poses both start and end constraints within a video sequence. We introduce point-to-point video generation that controls the generation process with two control points: the targeted start- and end-frames. The task is challenging since the model not only generates a smooth transition of frames but also plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of various lengths. We propose to maximize the modified variational lower bound of conditional data likelihood under a skip-frame training strategy. Our model can generate end-frame-consistent sequences without loss of quality and diversity. We evaluate our method through extensive experiments on Stochastic Moving MNIST, Weizmann Action, Human3.6M, and BAIR Robot Pushing under a series of scenarios. The qualitative results showcase the effectiveness and merits of point-to-point generation."
PointAE: Point Auto-Encoder for 3D Statistical Shape and Texture Modelling,"Hang Dai, Ling Shao","Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE",100.0,uae,0.0,,"The outcome of standard statistical shape modelling is a vector space representation of objects. Any convex combination of vectors of a set of object class examples generates a real and valid example. In this paper, we propose a Point Auto-Encoder (PointAE) with skip-connection, attention blocks for 3D statistical shape modelling directly on 3D points. The proposed PointAE is able to refine the correspondence with a correspondence refinement block. The data with refined correspondence can be fed to the PointAE again and bootstrap the constructed statistical models. Instead of two seperate models, PointAE can simultaneously model the shape and texture variation. The extensive evaluation in three open-sourced datasets demonstrates that the proposed method achieves better performance in representation ability of the shape variations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010679/,"['Shape', 'Three-dimensional displays', 'Solid modeling', 'Data models', 'Principal component analysis', 'Training', 'Adaptation models']","['Shape Model', 'Statistical Shape Model', 'Texture Model', '3D Texture', 'Statistical Models', 'Shape Variation', '3D Point', 'Representation Ability', 'Attention Block', 'Root Mean Square Error', 'Facial Expressions', 'Global Model', 'Point Cloud', 'Shape Parameter', 'Latent Space', 'Gaussian Process', '3D Scanning', '3D Coordinates', 'Skip Connections', '3D Shape', 'Face Model', 'Dense Correspondence', 'Latent Representation', '3D Face', 'Nonlinear Variation', 'Facial Shape', 'Latent Dimensions', 'Generalized Procrustes Analysis', 'Principal Component Analysis Model', 'Face Dataset']",,7,"The outcome of standard statistical shape modelling is a vector space representation of objects. Any convex combination of vectors of a set of object class examples generates a real and valid example. In this paper, we propose a Point Auto-Encoder (PointAE) with skip-connection, attention blocks for 3D statistical shape modelling directly on 3D points. The proposed PointAE is able to refine the correspondence with a correspondence refinement block. The data with refined correspondence can be fed to the PointAE again and bootstrap the constructed statistical models. Instead of two seperate models, PointAE can simultaneously model the shape and texture variation. The extensive evaluation in three open-sourced datasets demonstrates that the proposed method achieves better performance in representation ability of the shape variations."
PointCloud Saliency Maps,"Tianhang Zheng, Changyou Chen, Junsong Yuan, Bo Li, Kui Ren",State University of New York at Buffalo; Zhejiang University,100.0,"China, usa",0.0,,"3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate point-wise importance w.r.t.  classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on https://github.com/tianzheng4/PointCloud-Saliency-Maps",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf,,https://github.com/tianzheng4/PointCloud-Saliency-Maps,,main,Oral,https://ieeexplore.ieee.org/document/9010640/,"['Three-dimensional displays', 'Data models', 'Loss measurement', 'Robustness', 'Image recognition', 'Predictive models']","['Point Cloud', 'Saliency Map', '3D Point Cloud', 'Point Cloud Data', 'Gradient Loss', 'Experimental Code', 'Point Cloud Model', 'Model Performance', 'Important Point', 'Deep Neural Network', 'Local Structure', 'Negative Scores', 'Max-pooling', 'Recognition Performance', 'Max-pooling Layer', '3D Point', 'Deep Neural Network Model', 'Accurate Way', 'Graph Convolutional Network', 'Recognition Results', 'Raw Point Cloud', 'Spherical Coordinate System', 'After Dropping', 'Point Cloud Processing', 'Original Point Cloud']",,120,"3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate pointwise importance w.r.t. classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on https://github.com/tianzheng4/ Point-Cloud-Saliency-Maps."
PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows,"Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, Bharath Hariharan","Cornell University, Cornell Tech; Cornell University; NVIDIA",66.66666666666666,usa,33.33333333333334,USA,"As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code is available at https://github.com/stevenygd/PointFlow.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_PointFlow_3D_Point_Cloud_Generation_With_Continuous_Normalizing_Flows_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_PointFlow_3D_Point_Cloud_Generation_With_Continuous_Normalizing_Flows_ICCV_2019_paper.pdf,https://www.guandaoyang.com/PointFlow/,https://github.com/stevenygd/PointFlow,,main,Oral,https://ieeexplore.ieee.org/document/9010395/,"['Three-dimensional displays', 'Shape', 'Computational modeling', 'Training', 'Task analysis', 'Gallium nitride', 'Solid modeling']","['Point Cloud', '3D Point', 'Normal Flow', '3D Point Cloud', 'Point Cloud Generation', '3D Point Cloud Generation', 'Deep Learning', 'Invertible', 'Continuous Flow', 'Shape Of Distribution', 'Distribution Of Points', 'Unsupervised Manner', 'Variational Inference', 'Point Cloud Reconstruction', 'Neural Network', 'Training Set', 'Latent Variables', 'Autoregressive Model', 'Parametrized', 'Ordinary Differential Equations', 'Earth Moverâ€™s Distance', 'Variational Autoencoder', 'Generative Adversarial Networks', 'Evidence Lower Bound', 'Shape Representation', 'Jensen-Shannon Divergence', 'Chamfer Distance', 'Latent Space', 'Earth Mover', 'Marginal Distribution']",,342,"As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code is available at https://github.com/stevenygd/PointFlow."
Polarimetric Relative Pose Estimation,"Zhaopeng Cui, Viktor Larsson, Marc Pollefeys","Department of Computer Science, ETH Zurich2Microsoft; Department of Computer Science, ETH Zurich",100.0,switzerland,0.0,,"In this paper we consider the problem of relative pose estimation from two images with per-pixel polarimetric information. Using these additional measurements we derive a simple minimal solver for the essential matrix which only requires two point correspondences. The polarization constraints allow us to pointwise recover the 3D surface normal up to a two-fold ambiguity for the diffuse reflection. Since this ambiguity exists per point, there is a combinatorial explosion of possibilities. However, since our solver only requires two point correspondences, we only need to consider 16 configurations when solving for the relative pose. Once the relative orientation is recovered, we show that it is trivial to resolve the ambiguity for the remaining points. For robustness, we also propose a joint optimization between the relative pose and the refractive index to handle the refractive distortion. In experiments, on both synthetic and real data, we demonstrate that by leveraging the additional information available from polarization cameras, we can improve over classical methods which only rely on the 2D-point locations to estimate the geometry. Finally, we demonstrate the practical applicability of our approach by integrating it into a state-of-the-art global Structure-from-Motion pipeline.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cui_Polarimetric_Relative_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cui_Polarimetric_Relative_Pose_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009074/,"['Azimuth', 'Pose estimation', 'Three-dimensional displays', 'Refractive index', 'Cameras', 'Optimization']","['Pose Estimation', 'Relative Pose', 'Relative Pose Estimation', 'Refractive Index', 'Diffuse Reflectance', 'Corresponding Points', 'Joint Optimization', 'Surface Normals', 'Polarimetric Information', 'Linearly Polarized', 'Measurement Noise', 'Azimuth Angle', 'Polar Angle', 'Geometric Information', 'Specular Reflection', 'Zenith Angle', 'Image Coordinates', 'Degree Of Polarization', 'Local Coordinate System', 'Translation Error', 'Polarization Imaging', 'Bundle Adjustment', 'Angular Error', 'Polarimetric Images', '2D Point', 'Direction Of Gravity', 'Root Mean Square Error Of Cross-validation', 'Synthetic Experiments', 'Polarization Measurements']",,16,"In this paper we consider the problem of relative pose estimation from two images with per-pixel polarimetric information. Using these additional measurements we derive a simple minimal solver for the essential matrix which only requires two point correspondences. The polarization constraints allow us to pointwise recover the 3D surface normal up to a two-fold ambiguity for the diffuse reflection. Since this ambiguity exists per point, there is a combinatorial explosion of possibilities. However, since our solver only requires two point correspondences, we only need to consider 16 configurations when solving for the relative pose. Once the relative orientation is recovered, we show that it is trivial to resolve the ambiguity for the remaining points. For robustness, we also propose a joint optimization between the relative pose and the refractive index to handle the refractive distortion. In experiments, on both synthetic and real data, we demonstrate that by leveraging the additional information available from polarization cameras, we can improve over classical methods which only rely on the 2D-point locations to estimate the geometry. Finally, we demonstrate the practical applicability of our approach by integrating it into a state-of-the-art global Structure-from-Motion pipeline."
Pose-Aware Multi-Level Feature Network for Human Object Interaction Detection,"Bo Wan, Desen Zhou, Yongfei Liu, Rongjie Li, Xuming He","ShanghaiTech University, Shanghai, China",100.0,china,0.0,,"Reasoning human object interactions is a core problem in human-centric scene understanding and detecting such relations poses a unique challenge to vision systems due to large variations in human-object configurations, multiple co-occurring relation instances and subtle visual difference between relation categories. To address those challenges, we propose a multi-level relation detection strategy that utilizes human pose cues to capture global spatial configurations of relations and as an attention mechanism to dynamically zoom into relevant regions at human part level. We develop a multi-branch deep network to learn a pose-augmented relation representation at three semantic levels, incorporating interaction context, object features and detailed semantic part cues. As a result, our approach is capable of generating robust predictions on fine-grained human object interactions with interpretable outputs. Extensive experimental evaluations on public benchmarks show that our model outperforms prior methods by a considerable margin, demonstrating its efficacy in handling complex scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wan_Pose-Aware_Multi-Level_Feature_Network_for_Human_Object_Interaction_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wan_Pose-Aware_Multi-Level_Feature_Network_for_Human_Object_Interaction_Detection_ICCV_2019_paper.pdf,,https://github.com/bobwan1995/PMFNet,,main,Oral,https://ieeexplore.ieee.org/document/9010950/,"['Proposals', 'Visualization', 'Feature extraction', 'Cognition', 'Task analysis', 'Semantics', 'Neural networks']","['Human-object Interaction', 'Human-Object Interaction Detection', 'Deep Network', 'Attention Mechanism', 'Object Features', 'Spatial Configuration', 'Semantic Level', 'Human Pose', 'Semantic Cues', 'Extensive Experimental Evaluation', 'Spatial Information', 'Local Features', 'Feature Maps', 'Object Detection', 'Bounding Box', 'Fully-connected Layer', 'Target Object', 'Action Recognition', 'Affinity Interaction', 'Pose Estimation', 'Object Proposals', 'Convolutional Feature Maps', 'Faster R-CNN', 'Fine-grained Features', 'Image Instance', 'Spatial Cues', 'Relational Reasoning', 'Set Of Proposals', 'Irrelevant Ones', 'Affinity Score']",,158,"Reasoning human object interactions is a core problem in human-centric scene understanding and detecting such relations poses a unique challenge to vision systems due to large variations in human-object configurations, multiple co-occurring relation instances and subtle visual difference between relation categories. To address those challenges, we propose a multi-level relation detection strategy that utilizes human pose cues to capture global spatial configurations of relations and as an attention mechanism to dynamically zoom into relevant regions at human part level. We develop a multi-branch deep network to learn a pose-augmented relation representation at three semantic levels, incorporating interaction context, object features and detailed semantic part cues. As a result, our approach is capable of generating robust predictions on fine-grained human object interactions with interpretable outputs. Extensive experimental evaluations on public benchmarks show that our model outperforms prior methods by a considerable margin, demonstrating its efficacy in handling complex scenes."
Pose-Guided Feature Alignment for Occluded Person Re-Identification,"Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, Yi Yang","Baidu Research; ReLER, University of Technology Sydney; Baidu Research, ReLER, University of Technology Sydney",66.66666666666666,australia,33.33333333333334,China,"Persons are often occluded by various obstacles in person retrieval scenarios. Previous person re-identification (re-id) methods, either overlook this issue or resolve it based on an extreme assumption. To alleviate the occlusion problem, we propose to detect the occluded regions, and explicitly exclude those regions during feature generation and matching. In this paper, we introduce a novel method named Pose-Guided Feature Alignment (PGFA), exploiting pose landmarks to disentangle the useful information from the occlusion noise. During the feature constructing stage, our method utilizes human landmarks to generate attention maps. The generated attention maps indicate if a specific body part is occluded and guide our model to attend to the non-occluded regions. During matching, we explicitly partition the global feature into parts and use the pose landmarks to indicate which partial features belonging to the target person. Only the visible regions are utilized for the retrieval. Besides, we construct a large-scale dataset for the Occluded Person Re-ID problem, namely Occluded-DukeMTMC, which is by far the largest dataset for the Occlusion Person Re-ID. Extensive experiments are conducted on our constructed occluded re-id dataset, two partial re-id datasets, and two commonly used holistic re-id datasets. Our method largely outperforms existing person re-id methods on three occlusion datasets, while remains top performance on two holistic datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010704/,"['Probes', 'Feature extraction', 'Automobiles', 'Training', 'Image reconstruction', 'Vegetation', 'Agriculture']","['Occluded Person', 'Occluded Person Re-identification', 'Global Features', 'Large-scale Datasets', 'Part Features', 'Target Person', 'Occlusion Problem', 'Occluded Regions', 'Training Set', 'Feature Maps', 'Cross-entropy Loss', 'Confidence Score', 'Imaging Probes', 'Advantage Of Information', 'Pose Estimation', 'Previous Datasets', 'Average Pooling Layer', 'Visible Part', 'Person Image', 'Query Set', 'Query Image', 'Gallery Images', 'Feature Branch', 'Matching Stage', 'Gallery Set', 'Construction Of Representations', 'COCO Dataset', 'Base Learning Rate', 'Query Features', 'Real Scenarios']",,368,"Persons are often occluded by various obstacles in person retrieval scenarios. Previous person re-identification (re-id) methods, either overlook this issue or resolve it based on an extreme assumption. To alleviate the occlusion problem, we propose to detect the occluded regions, and explicitly exclude those regions during feature generation and matching. In this paper, we introduce a novel method named Pose-Guided Feature Alignment (PGFA), exploiting pose landmarks to disentangle the useful information from the occlusion noise. During the feature constructing stage, our method utilizes human landmarks to generate attention maps. The generated attention maps indicate if a specific body part is occluded and guide our model to attend to the non-occluded regions. During matching, we explicitly partition the global feature into parts and use the pose landmarks to indicate which partial features belonging to the target person. Only the visible regions are utilized for the retrieval. Besides, we construct a large-scale dataset for the Occluded Person Re-ID problem, namely Occluded-DukeMTMC, which is by far the largest dataset for the Occlusion Person Re-ID. Extensive experiments are conducted on our constructed occluded re-id dataset, two partial re-id datasets, and two commonly used holistic re-id datasets. Our method largely outperforms existing person re-id methods on three occlusion datasets, while remains top performance on two holistic datasets."
Predicting 3D Human Dynamics From Video,"Jason Y. Zhang, Panna Felsen, Angjoo Kanazawa, Jitendra Malik","University of California, Berkeley",100.0,usa,0.0,,"Given a video of a person in action, we can easily guess the 3D future motion of the person. In this work, we present perhaps the first approach for predicting a future 3D mesh model sequence of a person from past video input. We do this for periodic motions such as walking and also actions like bowling and squatting seen in sports or workout videos. While there has been a surge of future prediction problems in computer vision, most approaches predict 3D future from 3D past or 2D future from 2D past inputs. In this work, we focus on the problem of predicting 3D future motion from past image sequences, which has a plethora of practical applications in autonomous systems that must operate safely around people from visual inputs. Inspired by the success of autoregressive models in language modeling tasks, we learn an intermediate latent space on which we predict the future. This effectively facilitates autoregressive predictions when the input differs from the output domain. Our approach can be trained on video sequences obtained in-the-wild without 3D ground truth labels. The project website with videos can be found at https://jasonyzhang.com/phd.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Predicting_3D_Human_Dynamics_From_Video_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Predicting_3D_Human_Dynamics_From_Video_ICCV_2019_paper.pdf,https://jasonyzhang.com/phd,,,main,Poster,https://ieeexplore.ieee.org/document/9009824/,"['Three-dimensional displays', 'Two dimensional displays', 'Predictive models', 'Solid modeling', 'Dynamics', 'Motion pictures', 'Shape']","['Dynamic 3D', 'Autoregressive Model', 'Latent Space', 'Visual Input', '3D Mesh', 'Video Sequences', '3D Motion', 'Computer Vision Problems', 'Past Inputs', 'Ground Truth 3D', 'Future Motion', 'Video Of A Person', 'Convolutional Layers', 'Single Image', 'Motion Capture', 'Latent Representation', 'Human Motion', 'Human-robot Interaction', 'Type Of Motion', 'Motor Representations', '3D Pose', 'Temporal Coding', 'Dynamic Time Warping', '2D Keypoints', 'Pose Parameters', '3D Skeleton', '1D Convolution', '3D Prediction', 'Deep Learning-based Approaches', 'Human Pose']",,66,"Given a video of a person in action, we can easily guess the 3D future motion of the person. In this work, we present perhaps the first approach for predicting a future 3D mesh model sequence of a person from past video input. We do this for periodic motions such as walking and also actions like bowling and squatting seen in sports or workout videos. While there has been a surge of future prediction problems in computer vision, most approaches predict 3D future from 3D past or 2D future from 2D past inputs. In this work, we focus on the problem of predicting 3D future motion from past image sequences, which has a plethora of practical applications in autonomous systems that must operate safely around people from visual inputs. Inspired by the success of autoregressive models in language modeling tasks, we learn an intermediate latent space on which we predict the future. This effectively facilitates autoregressive predictions when the input differs from the output domain. Our approach can be trained on video sequences obtained in-the-wild without 3D ground truth labels. The project website with videos can be found at https://jasonyzhang.com/phd."
Predicting the Future: A Jointly Learnt Model for Action Anticipation,"Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes","Image and Video Research Lab, SAIVT, Queensland University of Technology (QUT), Australia",100.0,australia,0.0,,"Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gammulle_Predicting_the_Future_A_Jointly_Learnt_Model_for_Action_Anticipation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gammulle_Predicting_the_Future_A_Jointly_Learnt_Model_for_Action_Anticipation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009844/,"['Visualization', 'Gallium nitride', 'Task analysis', 'Feature extraction', 'Predictive models', 'Semantics', 'Generative adversarial networks']","['Activity Prediction', 'Semantic', 'Visual Representation', 'Future Actions', 'Generative Adversarial Networks', 'Joint Learning', 'Temporal Representation', 'Generative Adversarial Network Framework', 'Extensive Experimental Evaluation', 'Loss Function', 'Visual Features', 'Attention Mechanism', 'Visual Input', 'Action Recognition', 'Optical Flow', 'Classification Loss', 'Action Classes', 'Joint Training', 'Distant Future', 'Visual Stream', 'Generative Adversarial Networks Model', 'Input Stream', 'Future Frames', 'Multiple Benchmarks', 'Human Activity Recognition', 'Ablation Model', 'Future Classification', 'Temporal Frame', 'Scene Representation', 'Discrete Action']",,56,"Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction."
Presence-Only Geographical Priors for Fine-Grained Image Classification,"Oisin Mac Aodha, Elijah Cole, Pietro Perona",Caltech,100.0,usa,0.0,,"Appearance information alone is often not sufficient to accurately differentiate between fine-grained visual categories. Human experts make use of additional cues such as where, and when, a given image was taken in order to inform their final decision. This contextual information is readily available in many online image collections but has been underutilized by existing image classifiers that focus solely on making predictions based on the image contents. We propose an efficient spatio-temporal prior, that when conditioned on a geographical location and time, estimates the probability that a given object category occurs at that location. Our prior is trained from presence-only observation data and jointly models object categories, their spatio-temporal distributions, and photographer biases. Experiments performed on multiple challenging image classification datasets show that combining our prior with the predictions from image classifiers results in a large improvement in final classification performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Aodha_Presence-Only_Geographical_Priors_for_Fine-Grained_Image_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Aodha_Presence-Only_Geographical_Priors_for_Fine-Grained_Image_Classification_ICCV_2019_paper.pdf,http://www.vision.caltech.edu/~macaodha/projects/geopriors,,,main,Poster,https://ieeexplore.ieee.org/document/9008116/,"['Data models', 'Training', 'Visualization', 'Predictive models', 'Neural networks', 'Europe', 'Standards']","['Image Classification', 'Fine-grained Image', 'Fine-grained Image Classification', 'Classification Performance', 'Spatiotemporal Distribution', 'Presence-only Data', 'Neural Network', 'Discretion', 'Training Set', 'Local Information', 'Training Time', 'Distribution Models', 'Image Object', 'Dimensional Vector', 'Time Information', 'Large Image', 'Uniform Prior', 'Species Distribution Models', 'Citizen Science', 'Spatiotemporal Model', 'Local Embedding', 'Presence-absence Data', 'Embedding Matrix', 'Set Of Categories', 'Fully-connected Network', 'Longitude', 'Images Of Species', 'Test Locations']",,54,"Appearance information alone is often not sufficient to accurately differentiate between fine-grained visual categories. Human experts make use of additional cues such as where, and when, a given image was taken in order to inform their final decision. This contextual information is readily available in many online image collections but has been underutilized by existing image classifiers that focus solely on making predictions based on the image contents. We propose an efficient spatio-temporal prior, that when conditioned on a geographical location and time, estimates the probability that a given object category occurs at that location. Our prior is trained from presence-only observation data and jointly models object categories, their spatio-temporal distributions, and photographer biases. Experiments performed on multiple challenging image classification datasets show that combining our prior with the predictions from image classifiers results in a large improvement in final classification performance."
Prior Guided Dropout for Robust Visual Localization in Dynamic Environments,"Zhaoyang Huang, Yan Xu, Jianping Shi, Xiaowei Zhou, Hujun Bao, Guofeng Zhang","State Key Lab of CAD&CG, Zhejiang University†; SenseTime Research",50.0,China,50.0,China,"Camera localization from monocular images has been a long-standing problem, but its robustness in dynamic environments is still not adequately addressed. Compared with classic geometric approaches, modern CNN-based methods (e.g. PoseNet) have manifested the reliability against illumination or viewpoint variations, but they still have the following limitations. First, foreground moving objects are not explicitly handled, which results in poor performance and instability in dynamic environments. Second, the output for each image is a point estimate without uncertainty quantification. In this paper, we propose a framework which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. The key idea is a prior guided dropout module coupled with a self-attention module which can guide CNNs to ignore foreground objects during both training and inference. Additionally, the dropout module enables the pose regressor to output multiple hypotheses from which the uncertainty of pose estimates can be quantified and leveraged in the following uncertainty-aware pose-graph optimization to improve the robustness further. We achieve an average accuracy of 9.98m/3.63deg on RobotCar dataset, which outperforms the state-of-the-art method by 62.97%/47.08%. The source code of our implementation is available at https://github.com/zju3dv/RVL-dynamic.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Prior_Guided_Dropout_for_Robust_Visual_Localization_in_Dynamic_Environments_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Prior_Guided_Dropout_for_Robust_Visual_Localization_in_Dynamic_Environments_ICCV_2019_paper.pdf,,https://github.com/zju3dv/RVL-Dynamic,,main,Poster,https://ieeexplore.ieee.org/document/9008288/,"['Feature extraction', 'Robustness', 'Cameras', 'Visualization', 'Training', 'Neural networks', 'Uncertainty']","['Dynamic Environment', 'Visual Localization', 'Robust Localization', 'Robust Visual Localization', 'Convolutional Neural Network', 'Multiple Hypothesis', 'Output Image', 'Uncertainty Quantification', 'Foreground Objects', 'Self-attention Module', 'Neural Network', 'Feature Maps', 'Long Short-term Memory', 'Pedestrian', 'Object Recognition', 'Training Images', 'Image Pairs', 'Asymptotic Distribution', 'Large Margin', 'Global Pooling', 'Camera Pose', 'Dynamic Objects', 'Visual Odometry', 'Relative Pose', 'Query Image', 'Object Motion', 'Video Sequences', 'Absolute Position', 'Place Recognition', 'Mask R-CNN']",,35,"Camera localization from monocular images has been a long-standing problem, but its robustness in dynamic environments is still not adequately addressed. Compared with classic geometric approaches, modern CNN-based methods (e.g. PoseNet) have manifested the reliability against illumination or viewpoint variations, but they still have the following limitations. First, foreground moving objects are not explicitly handled, which results in poor performance and instability in dynamic environments. Second, the output for each image is a point estimate without uncertainty quantification. In this paper, we propose a framework which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. The key idea is a prior guided dropout module coupled with a self-attention module which can guide CNNs to ignore foreground objects during both training and inference. Additionally, the dropout module enables the pose regressor to output multiple hypotheses from which the uncertainty of pose estimates can be quantified and leveraged in the following uncertainty-aware pose graph optimization to improve the robustness further. We achieve an average accuracy of 9.98m/3.63° on RobotCar dataset, which outperforms the state-of-the-art method by 62.97%/47.08%. The source code of our implementation is available at https://github.com/zju3dv/RVL-Dynamic."
Prior-Aware Neural Network for Partially-Supervised Multi-Organ Segmentation,"Yuyin Zhou,  Zhe Li,  Song Bai,  Chong Wang,  Xinlei Chen,  Mei Han,  Elliot Fishman,  Alan L. Yuille",Johns Hopkins University; The Johns Hopkins Medical Institute; PAII Inc.; Facebook; ByteDance Inc.; University of Oxford; Google Research,42.857142857142854,"uk, usa",57.142857142857146,USA,"Accurate multi-organ abdominal CT segmentation is essential to many clinical applications such as computer-aided intervention. As data annotation requires massive human labor from experienced radiologists, it is common that training data is usually partially-labeled. However, these background labels can be misleading in multi-organ segmentation since the ""background"" usually contains some other organs of interest. To address the background ambiguity in these partially-labeled datasets, we propose Prior-aware Neural Network (PaNN) via explicitly incorporating anatomical priors on abdominal organ sizes, guiding the training process with domain-specific knowledge. More specifically, PaNN assumes that the average organ size distributions in the abdomen should approximate their empirical distributions, a prior statistics obtained from the fully-labeled dataset. As our objective is difficult to be directly optimized using stochastic gradient descent, it is reformulated as a min-max form and optimized via the stochastic primal-dual gradient algorithm. PaNN achieves state-of-the-art performance on the MICCAI2015 challenge ""Multi-Atlas Labeling Beyond the Cranial Vault"", a competition on organ segmentation in the abdomen. We report an average Dice score of 84.97%, surpassing the prior art by a large margin of 3.27%. Code and models will be made publicly available.",http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Prior-Aware_Neural_Network_for_Partially-Supervised_Multi-Organ_Segmentation_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Prior-Aware_Neural_Network_for_Partially-Supervised_Multi-Organ_Segmentation_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9009566/,"['Biomedical imaging', 'Image segmentation', 'Training', 'Pancreas', 'Computed tomography', 'Liver', 'Neural networks']","['Multi-organ Segmentation', 'Training Data', 'Stochastic Gradient Descent', 'Stochastic Gradient', 'Average Distribution', 'Abdominal Computed Tomography', 'Interest Organizations', 'Training Objective', 'Domain-specific Knowledge', 'Dice Score', 'Cranial Vault', 'Partial Labels', 'Primal-dual Algorithm', 'Computed Tomography', 'Medical Imaging', 'Learning Rate', 'Convolutional Neural Network', 'Adrenal', 'Training Procedure', 'Kullback-Leibler', '2D Model', 'Splenic Vein', 'Soft Constraints', 'Medical Image Analysis', 'Fully Convolutional Network', 'Backbone Model', 'Abdominal Region', 'Minimax Optimization', 'Segmentation Model', 'Gallbladder']",,114,"Accurate multi-organ abdominal CT segmentation is essential to many clinical applications such as computer-aided intervention. As data annotation requires massive human labor from experienced radiologists, it is common that training data is usually partially-labeled. However, these background labels can be misleading in multi-organ segmentation since the ``background'' usually contains some other organs of interest. To address the background ambiguity in these partially-labeled datasets, we propose Prior-aware Neural Network (PaNN) via explicitly incorporating anatomical priors on abdominal organ sizes, guiding the training process with domain-specific knowledge. More specifically, PaNN assumes that the average organ size distributions in the abdomen should approximate their empirical distributions, a prior statistics obtained from the fully-labeled dataset. As our objective is difficult to be directly optimized using stochastic gradient descent, it is reformulated as a min-max form and optimized via the stochastic primal-dual gradient algorithm. PaNN achieves state-of-the-art performance on the MICCAI2015 challenge ``Multi-Atlas Labeling Beyond the Cranial Vault'', a competition on organ segmentation in the abdomen. We report an average Dice score of 84.97%, surpassing the prior art by a large margin of 3.27%. Code and models will be made publicly available."
Privacy Preserving Image Queries for Camera Localization,"Pablo Speciale, Johannes L. SchÃ¶nberger, Sudipta N. Sinha, Marc Pollefeys","ETH Zurich, Microsoft; Microsoft",50.0,switzerland,50.0,USA,"Augmented/mixed reality and robotic applications are increasingly relying on cloud-based localization services, which require users to upload query images to perform camera pose estimation on a server. This raises significant privacy concerns when consumers use such services in their homes or in confidential industrial settings. Even if only image features are uploaded, the privacy concerns remain as the images can be reconstructed fairly well from feature locations and descriptors. We propose to conceal the content of the query images from an adversary on the server or a man-in-the-middle intruder. The key insight is to replace the 2D image feature points in the query image with randomly oriented 2D lines passing through their original 2D positions. It will be shown that this feature representation hides the image contents, and thereby protects user privacy, yet still provides sufficient geometric constraints to enable robust and accurate 6-DOF camera pose estimation from feature correspondences. Our proposed method can handle single- and multi-image queries as well as exploit additional information about known structure, gravity, and scale. Numerous experiments demonstrate the high practical relevance of our approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Speciale_Privacy_Preserving_Image_Queries_for_Camera_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Speciale_Privacy_Preserving_Image_Queries_for_Camera_Localization_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009058/,"['Three-dimensional displays', 'Two dimensional displays', 'Privacy', 'Cameras', 'Pose estimation', 'Servers', 'Visualization']","['Privacy Preservation', 'Query Image', 'Gravity', 'Image Features', 'Descriptive Characteristics', '2D Images', 'Privacy Issues', 'Feature Points', 'Image Point', 'Pose Estimation', 'Geometric Constraints', 'Camera Pose', 'Cloud-based Services', 'Camera Pose Estimation', '2D Line', 'Original 2D', 'Confidential Information', 'Original Point', 'Image Space', '3D Point', '3D Line', '3D Plane', '3D Point Cloud', 'Geometric Problem', '2D Feature', 'Image Lines', 'Unknown Scale', 'Privacy Risks', 'Map Points', '3D Cloud']",,24,"Augmented/mixed reality and robotic applications are increasingly relying on cloud-based localization services, which require users to upload query images to perform camera pose estimation on a server. This raises significant privacy concerns when consumers use such services in their homes or in confidential industrial settings. Even if only image features are uploaded, the privacy concerns remain as the images can be reconstructed fairly well from feature locations and descriptors. We propose to conceal the content of the query images from an adversary on the server or a man-in-the-middle intruder. The key insight is to replace the 2D image feature points in the query image with randomly oriented 2D lines passing through their original 2D positions. It will be shown that this feature representation hides the image contents, and thereby protects user privacy, yet still provides sufficient geometric constraints to enable robust and accurate 6-DOF camera pose estimation from feature correspondences. Our proposed method can handle single- and multi-image queries as well as exploit additional information about known structure, gravity, and scale. Numerous experiments demonstrate the high practical relevance of our approach."
Pro-Cam SSfM: Projector-Camera System for Structure and Spectral Reflectance From Motion,"Chunyu Li, Yusuke Monno, Hironori Hidaka, Masatoshi Okutomi","Tokyo Institute of Technology, Tokyo, Japan",100.0,japan,0.0,,"In this paper, we propose a novel projector-camera system for practical and low-cost acquisition of a dense object 3D model with the spectral reflectance property. In our system, we use a standard RGB camera and leverage an off-the-shelf projector as active illumination for both the 3D reconstruction and the spectral reflectance estimation. We first reconstruct the 3D points while estimating the poses of the camera and the projector, which are alternately moved around the object, by combining multi-view structured light and structure-from-motion (SfM) techniques. We then exploit the projector for multispectral imaging and estimate the spectral reflectance of each 3D point based on a novel spectral reflectance estimation model considering the geometric relationship between the reconstructed 3D points and the estimated projector positions. Experimental results on several real objects demonstrate that our system can precisely acquire a dense 3D model with the full spectral reflectance property using off-the-shelf devices.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Pro-Cam_SSfM_Projector-Camera_System_for_Structure_and_Spectral_Reflectance_From_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Pro-Cam_SSfM_Projector-Camera_System_for_Structure_and_Spectral_Reflectance_From_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009842/,"['Three-dimensional displays', 'Cameras', 'Solid modeling', 'Lighting', 'Image reconstruction', 'Estimation', 'Data acquisition']","['Spectral Reflectance', 'Projector-camera System', '3D Reconstruction', 'Spectral Properties', 'Multispectral Images', 'Structured Illumination', '3D Point', 'Density Model', 'Geometric Relationship', 'Spectral Estimation', 'Spectral Model', 'RGB Camera', 'Camera Pose', '3D Density', 'Standard Camera', 'Diagonal Matrix', 'Point Cloud', 'Geometric Properties', 'Camera Images', 'Pose Estimation', 'Spectral Distribution', 'Multi-view Images', 'Camera Position', 'Spectral Acquisition', 'Bundle Adjustment', 'Reprojection Error', 'Object Surface', 'Multispectral Camera', 'Geometric Information', 'Gray Code']",,12,"In this paper, we propose a novel projector-camera system for practical and low-cost acquisition of a dense object 3D model with the spectral reflectance property. In our system, we use a standard RGB camera and leverage an off-the-shelf projector as active illumination for both the 3D reconstruction and the spectral reflectance estimation. We first reconstruct the 3D points while estimating the poses of the camera and the projector, which are alternately moved around the object, by combining multi-view structured light and structure-from-motion (SfM) techniques. We then exploit the projector for multispectral imaging and estimate the spectral reflectance of each 3D point based on a novel spectral reflectance estimation model considering the geometric relationship between the reconstructed 3D points and the estimated projector positions. Experimental results on several real objects demonstrate that our system can precisely acquire a dense 3D model with the full spectral reflectance property using off-the-shelf devices."
Probabilistic Face Embeddings,"Yichun Shi, Anil K. Jain","Michigan State University, East Lansing, MI",100.0,usa,0.0,,"Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Probabilistic_Face_Embeddings_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Probabilistic_Face_Embeddings_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008376/,"['Face', 'Uncertainty', 'Face recognition', 'Probabilistic logic', 'Estimation', 'Facial features', 'Training data']","['Face Embedding', 'Probabilistic Face Embeddings', 'Uncertainty Estimation', 'Face Recognition', 'Latent Space', 'Facial Features', 'Face Images', 'Probable Solution', 'Loss Function', 'Training Data', 'Deep Neural Network', 'Similarity Score', 'Feature Dimension', 'Types Of Errors', 'Image Pairs', 'High-quality Images', 'Large Improvement', 'Dirac Delta', 'Performance In Cases', 'Confidence Estimation', 'False Acceptance Rate', 'Original Representation', 'Squared Euclidean Distance', 'Term In Brackets', 'Face Representation', 'Latent Code', 'Video Surveillance', 'Inverse Mapping', 'Posterior Probability', 'False Recognition']",,220,"Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system."
Program-Guided Image Manipulators,"Jiayuan Mao, Xiuming Zhang, Yikai Li, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu","MIT CSAIL; MIT CSAIL, Google Research; MIT CSAIL, Shanghai Jiao Tong University",100.0,"China, usa",0.0,,"Humans are capable of building holistic representations for images at various levels, from local objects, to pairwise relations, to global structures. The interpretation of structures involves reasoning over repetition and symmetry of the objects in the image. In this paper, we present the Program-Guided Image Manipulator (PG-IM), inducing neuro-symbolic program-like representations to represent and manipulate images. Given an image, PG-IM detects repeated patterns, induces symbolic programs, and manipulates the image using a neural network that is guided by the program. PG-IM learns from a single image, exploiting its internal statistics. Despite trained only on image inpainting, PG-IM is directly capable of extrapolation and regularity editing in a unified framework. Extensive experiments show that PG-IM achieves superior performance on all the tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mao_Program-Guided_Image_Manipulators_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Program-Guided_Image_Manipulators_ICCV_2019_paper.pdf,http://pgim.csail.mit.edu,,,main,Poster,https://ieeexplore.ieee.org/document/9008534/,"['Lattices', 'Manipulators', 'Semantics', 'Extrapolation', 'Task analysis', 'Visualization', 'Neural networks']","['Neural Network', 'Use Of Imaging', 'Image Object', 'Global Structure', 'Image Inpainting', 'Deep Network', 'Input Image', 'Feature Maps', 'Image Pixels', 'Large-scale Datasets', 'Natural Images', 'Convex Hull', 'Object Properties', 'Voronoi Diagram', 'Ground Truth Image', 'Computer Graphics', 'Interest In Images', 'Training Paradigm', 'Object Appearance', 'Neural Modulation', 'Patch Matching', 'Domain-specific Languages']",,1,"Humans are capable of building holistic representations for images at various levels, from local objects, to pairwise relations, to global structures. The interpretation of structures involves reasoning over repetition and symmetry of the objects in the image. In this paper, we present the Program-Guided Image Manipulator (PG-IM), inducing neuro-symbolic program-like representations to represent and manipulate images. Given an image, PG-IM detects repeated patterns, induces symbolic programs, and manipulates the image using a neural network that is guided by the program. PG-IM learns from a single image, exploiting its internal statistics. Despite trained only on image inpainting, PG-IM is directly capable of extrapolation and regularity editing in a unified framework. Extensive experiments show that PG-IM achieves superior performance on all the tasks."
Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation,"Xin Chen, Lingxi Xie, Jun Wu, Qi Tian",Tongji University; Huawei Noah’s Ark Lab,50.0,china,50.0,China,"Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time ( 7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Progressive_Differentiable_Architecture_Search_Bridging_the_Depth_Gap_Between_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Progressive_Differentiable_Architecture_Search_Bridging_the_Depth_Gap_Between_Search_ICCV_2019_paper.pdf,,https://github.com/chenxin061/pdarts,,main,Oral,https://ieeexplore.ieee.org/document/9010638/,"['Computer architecture', 'Task analysis', 'Training', 'Image recognition', 'Graphics processing units', 'Approximation algorithms', 'Computational modeling']","['Differentiable Architecture Search', 'Search Space', 'Computational Overhead', 'Target Dataset', 'Single GPU', 'Search Costs', 'Evaluation Scenarios', 'Neural Architecture Search', 'Learning Rate', 'Convolutional Neural Network', 'Deep Network', 'Classification Performance', 'Network Parameters', 'End Stage', 'Image Recognition', 'Previous Stage', 'Evaluation Stage', 'Test Error', 'Operational Space', 'Shallow Network', 'Architecture Parameters', 'Search Stage', 'Powerful Architecture', 'Target Task', 'Regularization Effect', 'GPU Memory', 'Memory Efficiency', 'NVIDIA Tesla', 'Stage 2']",,366,"Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time (~7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts"
Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations,"Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, Jiayi Ma","Wuhan University, Wuhan, China; Harbin Institute of Technology, Harbin, China",100.0,china,0.0,,"Most previous fusion strategies either fail to fully utilize temporal information or cost too much time, and how to effectively fuse temporal information from consecutive frames plays an important role in video super-resolution (SR). In this study, we propose a novel progressive fusion network for video SR, which is designed to make better use of spatio-temporal information and is proved to be more efficient and effective than the existing direct fusion, slow fusion or 3D convolution strategies. Under this progressive fusion framework, we further introduce an improved non-local operation to avoid the complex motion estimation and motion compensation (ME&MC) procedures as in previous video SR approaches. Extensive experiments on public datasets demonstrate that our method surpasses state-of-the-art with 0.96 dB in average, and runs about 3 times faster, while requires only about half of the parameters.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yi_Progressive_Fusion_Video_Super-Resolution_Network_via_Exploiting_Non-Local_Spatio-Temporal_Correlations_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yi_Progressive_Fusion_Video_Super-Resolution_Network_via_Exploiting_Non-Local_Spatio-Temporal_Correlations_ICCV_2019_paper.pdf,,https://github.com/psychopa4/PFNL,,main,Oral,https://ieeexplore.ieee.org/document/9009484/,"['Correlation', 'Convolution', 'Three-dimensional displays', 'Feature extraction', 'Spatial resolution', 'Fuses']","['Super-resolution Network', 'Video Super-resolution', 'Temporal Information', 'Consecutive Frames', 'Fusion Strategy', 'Motion Estimation', '3D Convolution', 'Direct Fusion', 'Motion Compensation', 'Non-local Operation', 'Convolutional Neural Network', 'Convolutional Layers', 'Spatial Information', 'Feature Maps', 'Temporal Dimension', 'Temporal Correlation', 'Residual Block', 'Gaussian Blur', 'Kind Of Approach', 'Multiple Frames', 'Extract Spatial Information', 'Input Frames', 'Single Image Super-resolution', 'Convolutional Long Short-term Memory', 'Number Of Convolutional Filters', 'Merge Module', 'Residual Learning', 'Number Of Filters', 'Extra Parameters']",,176,"Most previous fusion strategies either fail to fully utilize temporal information or cost too much time, and how to effectively fuse temporal information from consecutive frames plays an important role in video super-resolution (SR). In this study, we propose a novel progressive fusion network for video SR, which is designed to make better use of spatio-temporal information and is proved to be more efficient and effective than the existing direct fusion, slow fusion or 3D convolution strategies. Under this progressive fusion framework, we further introduce an improved non-local operation to avoid the complex motion estimation and motion compensation (ME&MC) procedures as in previous video SR approaches. Extensive experiments on public datasets demonstrate that our method surpasses state-of-the-art with 0.96 dB in average, and runs about 3 times faster, while requires only about half of the parameters."
Progressive Reconstruction of Visual Structure for Image Inpainting,"Jingyuan Li, Fengxiang He, Lefei Zhang, Bo Du, Dacheng Tao","School of Computer Science, Wuhan University, Wuhan, China; UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Australia",100.0,"australia, china",0.0,,"Inpainting methods aim to restore missing parts of corrupted images and play a critical role in many computer vision applications, such as object removal and image restoration. Although existing methods perform well on images with small holes, restoring large holes remains elusive. To address this issue, this paper proposes a Progressive Reconstruction of Visual Structure (PRVS) network that progressively reconstructs the structures and the associated visual feature. Specifically, we design a novel Visual Structure Reconstruction (VSR) layer to entangle reconstructions of the visual structure and visual feature, which benefits each other by sharing parameters. We repeatedly stack four VSR layers in both encoding and decoding stages of a U-Net like architecture to form the generator of a generative adversarial network (GAN) for restoring images with either small or large holes. We prove the generalization error upper bound of the PRVS network is O(1sqrt(N)), which theoretically guarantees its performance. Extensive empirical evaluations and comparisons on Places2, Paris Street View and CelebA datasets validate the strengths of the proposed approach and demonstrate that the model outperforms current state-of-the-art methods. The source code package is available at https://github.com/jingyuanli001/PRVS-Image-Inpainting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Progressive_Reconstruction_of_Visual_Structure_for_Image_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Progressive_Reconstruction_of_Visual_Structure_for_Image_Inpainting_ICCV_2019_paper.pdf,,https://github.com/jingyuanli001/PRVS-Image-Inpainting,,main,Poster,https://ieeexplore.ieee.org/document/9010670/,"['Convolution', 'Visualization', 'Image reconstruction', 'Generators', 'Image restoration', 'Image edge detection', 'Shape']","['Reconstruction Of Structure', 'Image Inpainting', 'Visual Features', 'Generative Adversarial Networks', 'Small Holes', 'Generalization Error', 'Large Holes', 'Decoding Stage', 'Model Testing', 'Convolutional Layers', 'Feature Maps', 'Weight Matrix', 'Generalization Ability', 'Recent Results', 'ImageNet', 'Image Generation', 'Convolution Kernel', 'Residual Block', 'Peak Signal-to-noise Ratio', 'Structure Learning', 'Pre-trained VGG16', 'Perceptual Loss', 'Spectral Normalization', 'Feature Maps Of Images', 'Encoding Stage', 'Masked Area', 'Spectral Norm', 'Layer In Stage', 'Theoretical Analysis', 'Quantitative Results']",,113,"Inpainting methods aim to restore missing parts of corrupted images and play a critical role in many computer vision applications, such as object removal and image restoration. Although existing methods perform well on images with small holes, restoring large holes remains elusive. To address this issue, this paper proposes a Progressive Reconstruction of Visual Structure (PRVS) network that progressively reconstructs the structures and the associated visual feature. Specifically, we design a novel Visual Structure Reconstruction (VSR) layer to entangle reconstructions of the visual structure and visual feature, which benefits each other by sharing parameters. We repeatedly stack four VSR layers in both encoding and decoding stages of a U-Net like architecture to form the generator of a generative adversarial network (GAN) for restoring images with either small or large holes. We prove the generalization error upper bound of the PRVS network is O(1\sqrt(N)), which theoretically guarantees its performance. Extensive empirical evaluations and comparisons on Places2, Paris Street View and CelebA datasets validate the strengths of the proposed approach and demonstrate that the model outperforms current state-of-the-art methods. The source code package is available at https://github.com/jingyuanli001/PRVS-Image-Inpainting."
Progressive Sparse Local Attention for Video Object Detection,"Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, VÃ©ronique Prinet, Chunhong Pan","Horizon Robotics; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",50.0,china,50.0,China,"Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Guo_Progressive_Sparse_Local_Attention_for_Video_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Guo_Progressive_Sparse_Local_Attention_for_Video_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008253/,"['Feature extraction', 'Optical imaging', 'Object detection', 'Optical propagation', 'Integrated optics', 'Detectors', 'Optical detectors']","['Object Detection', 'Sparse Attention', 'Video Object Detection', 'Model Size', 'High-level Features', 'Optical Flow', 'Detection Framework', 'Spatial Correspondence', 'Extra Model', 'Object Detection Framework', 'Smaller Model Size', 'Convolutional Neural Network', 'Convolutional Layers', 'Contextual Information', 'Feature Maps', 'Temporal Features', 'Bounding Box', 'Temporal Information', 'Semantic Segmentation', 'Low-level Features', 'Key Frames', 'Frame Features', 'Propagation Characteristics', 'Conduct Ablation Studies', 'High-level Feature Maps', 'Feature Alignment', 'Image Object Detection', 'Temporal Context', 'Video Information', 'Semantic Features']",,58,"Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed."
"Progressive-X: Efficient, Anytime, Multi-Model Fitting Algorithm","DÃ¡niel BarÃ¡th, JiÅÃ­ Matas","VRG, Department of Cybernetics, Czech Technical University in Prague, Czech Republic",100.0,Czech Republic,0.0,,"The Progressive-X algorithm, Prog-X in short, is proposed for geometric multi-model fitting. The method interleaves sampling and consolidation of the current data interpretation via repetitive hypothesis proposal, fast rejection, and integration of the new hypothesis into the kept instance set by labeling energy minimization. Due to exploring the data progressively, the method has several beneficial properties compared with the state-of-the-art. First, a clear criterion, adopted from RANSAC, controls the termination and stops the algorithm when the probability of finding a new model with a reasonable number of inliers falls below a threshold. Second, Prog-X is an any-time algorithm. Thus, whenever is interrupted, e.g. due to a time limit, the returned instances cover real and, likely, the most dominant ones. The method is superior to the state-of-the-art in terms of accuracy in both synthetic experiments and on publicly available real-world datasets for homography, two-view motion, and motion segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Barath_Progressive-X_Efficient_Anytime_Multi-Model_Fitting_Algorithm_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Barath_Progressive-X_Efficient_Anytime_Multi-Model_Fitting_Algorithm_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010674/,"['Compounds', 'Proposals', 'Engines', 'Optimization', 'Labeling', 'Image color analysis', 'Minimization']","['Multi-model Fitting', 'Energy Minimization', 'Inliers', 'Homography', 'Synthetic Experiments', 'Dominant Ones', 'Processing Time', 'Intersection Over Union', 'Point Cloud', 'Real-world Problems', 'Worst Results', 'Termination Criterion', 'Misclassification Error', 'Millisecond Time', 'Label Space', 'Plane Fitting', 'Synthetic Environment']",,48,"The Progressive-X algorithm, Prog-X in short, is proposed for geometric multi-model fitting. The method interleaves sampling and consolidation of the current data interpretation via repetitive hypothesis proposal, fast rejection, and integration of the new hypothesis into the kept instance set by labeling energy minimization. Due to exploring the data progressively, the method has several beneficial properties compared with the state-of-the-art. First, a clear criterion, adopted from RANSAC, controls the termination and stops the algorithm when the probability of finding a new model with a reasonable number of inliers falls below a threshold. Second, Prog-X is an any-time algorithm. Thus, whenever is interrupted, e.g. due to a time limit, the returned instances cover real and, likely, the most dominant ones. The method is superior to the state-of-the-art in terms of accuracy in both synthetic experiments and on publicly available real-world datasets for homography, two-view motion, and motion segmentation."
Proximal Mean-Field for Neural Network Quantization,"Thalaiyasingam Ajanthan, Puneet K. Dokania, Richard Hartley, Philip H. S. Torr",Australian National University; University of Oxford,100.0,"Australia, uk",0.0,,"Compressing large Neural Networks (NN) by quantizing the parameters, while maintaining the performance is highly desirable due to reduced memory and time complexity. In this work, we cast NN quantization as a discrete labelling problem, and by examining relaxations, we design an efficient iterative optimization procedure that involves stochastic gradient descent followed by a projection. We prove that our simple projected gradient descent approach is, in fact, equivalent to a proximal version of the well-known mean-field method. These findings would allow the decades-old and theoretically grounded research on MRF optimization to be used to design better network quantization schemes. Our experiments on standard classification datasets (MNIST, CIFAR10/100, TinyImageNet) with convolutional and residual architectures show that our algorithm obtains fully-quantized networks with accuracies very close to the floating-point reference networks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ajanthan_Proximal_Mean-Field_for_Neural_Network_Quantization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ajanthan_Proximal_Mean-Field_for_Neural_Network_Quantization_ICCV_2019_paper.pdf,,https://github.com/tajanthan/pmf,,main,Poster,https://ieeexplore.ieee.org/document/9010338/,"['Quantization (signal)', 'Optimization', 'Artificial neural networks', 'Standards', 'Approximation algorithms', 'Labeling']","['Neural Network', 'Network Quantization', 'Neural Network Quantization', 'Gradient Descent', 'Large Networks', 'Adaptive Method', 'Stochastic Gradient Descent', 'Stochastic Gradient', 'Markov Random Field', 'Discrete Problem', 'Projected Gradient Descent', 'Loss Function', 'Leisure', 'Optimization Problem', 'Objective Function', 'Bayesian Inference', 'Softmax', 'Fixed Point', 'Network Parameters', 'Energy Function', 'Quantization Levels', 'Learnable Parameters', 'Stochastic Setting', 'Gradient Descent Method', 'Solution Space', 'Fully-connected Layer', 'Feasible Point', 'Entropy Term', 'Discrete Set', 'Convex Polytope']",,9,"Compressing large Neural Networks (NN) by quantizing the parameters, while maintaining the performance is highly desirable due to reduced memory and time complexity. In this work, we cast NN quantization as a discrete labelling problem, and by examining relaxations, we design an efficient iterative optimization procedure that involves stochastic gradient descent followed by a projection. We prove that our simple projected gradient descent approach is, in fact, equivalent to a proximal version of the well-known mean-field method. These findings would allow the decades-old and theoretically grounded research on MRF optimization to be used to design better network quantization schemes. Our experiments on standard classification datasets (MNIST, CIFAR10/100, TinyImageNet) with convolutional and residual architectures show that our algorithm obtains fully-quantized networks with accuracies very close to the floating-point reference networks."
PuppetGAN: Cross-Domain Image Manipulation by Demonstration,"Ben Usman, Nick Dufour, Kate Saenko, Chris Bregler","Google AI; Google AI, Boston University; Boston University",66.66666666666666,usa,33.33333333333334,USA,"In this work we propose a model that can manipulate individual visual attributes of objects in a real scene using examples of how respective attribute manipulations affect the output of a simulation. As an example, we train our model to manipulate the expression of a human face using nonphotorealistic 3D renders of a face with varied expression. Our model manages to preserve all other visual attributes of a real face, such as head orientation, even though this and other attributes are not labeled in either real or synthetic domain. Since our model learns to manipulate a specific property in isolation using only ""synthetic demonstrations"" of such manipulations without explicitly provided labels, it can be applied to shape, texture, lighting, and other properties that are difficult to measure or represent as real-valued vectors. We measure the degree to which our model preserves other attributes of a real image when a single specific attribute is manipulated. We use digit datasets to analyze how discrepancy in attribute distributions affects the performance of our model, and demonstrate results in a far more difficult setting: learning to manipulate real human faces using nonphotorealistic 3D renders.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Usman_PuppetGAN_Cross-Domain_Image_Manipulation_by_Demonstration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Usman_PuppetGAN_Cross-Domain_Image_Manipulation_by_Demonstration_ICCV_2019_paper.pdf,http://bit.ly/iccv19_pupgan,,,main,Oral,https://ieeexplore.ieee.org/document/9010675/,"['Face', 'Lighting', 'Mouth', 'Data models', 'Visualization', 'Decoding']","['Use Of Imaging', 'Specific Properties', 'Facial Expressions', 'Individual Attributes', 'Human Faces', 'Head Orientation', 'Real Domain', 'Real Faces', 'Model Parameters', 'Semantic', 'Decoding', 'Training Time', 'Class Labels', 'Single Domain', 'Domain Shift', 'Representation Learning', 'Precise Model', 'Synthetic Images', 'Image Properties', 'Data Constraints', 'Explicit Labels', 'Synthetic Inputs', 'Real Identity', 'Face Orientation', 'Image Embedding', 'Disentangled Representation', 'Real Input', 'Input Label', 'Translational Model', 'Quantitative Evaluation']",,9,"In this work we propose a model that can manipulate individual visual attributes of objects in a real scene using examples of how respective attribute manipulations affect the output of a simulation. As an example, we train our model to manipulate the expression of a human face using nonphotorealistic 3D renders of a face with varied expression. Our model manages to preserve all other visual attributes of a real face, such as head orientation, even though this and other attributes are not labeled in either real or synthetic domain. Since our model learns to manipulate a specific property in isolation using only ""synthetic demonstrations"" of such manipulations without explicitly provided labels, it can be applied to shape, texture, lighting, and other properties that are difficult to measure or represent as real-valued vectors. We measure the degree to which our model preserves other attributes of a real image when a single specific attribute is manipulated. We use digit datasets to analyze how discrepancy in attribute distributions affects the performance of our model, and demonstrate results in a far more difficult setting: learning to manipulate real human faces using nonphotorealistic 3D renders."
Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method,"Vishwanath A. Sindagi, Rajeev Yasarla, Vishal M. Patel","Department of Electrical and Computer Engineering, Johns Hopkins University, 3400 N. Charles St, Baltimore, MD 21218, USA",100.0,usa,0.0,,"In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is  2.8 larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sindagi_Pushing_the_Frontiers_of_Unconstrained_Crowd_Counting_New_Dataset_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sindagi_Pushing_the_Frontiers_of_Unconstrained_Crowd_Counting_New_Dataset_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009496/,"['Image resolution', 'Task analysis', 'Training', 'Head', 'Estimation', 'Benchmark testing', 'Error analysis']","['Crowd Counting', 'Number Of Images', 'Density Map', 'Residual Learning', 'Improvement In Error', 'Crowd Density', 'Training Set', 'Results Of Experiments', 'Convolutional Neural Network', 'Adverse Conditions', 'Image Dataset', 'Recent Approaches', 'Scale Variation', 'Marathon', 'Large-scale Variation', 'Recent Algorithms', 'Images Of People', 'Dataset Bias', 'Conv Layer', 'International Exhibition', 'Image-level Labels', 'Confidence Map', 'Efficacy Of Learning', 'Occlusion Level', 'VGG-16 Network', 'Learning Bias', 'Deep Network', 'Bounding Box', 'Large-scale Datasets', 'Input Image']",,56,"In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is ~2.8 larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset."
Pyramid Graph Networks With Connection Attentions for Region-Based One-Shot Semantic Segmentation,"Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo, Qingyao Wu, Rui Yao",Stanford University; Nanyang Technological University; South China University of Technology; China University of Mining and Technology; Institute for Infocomm Research A*STAR,100.0,"Singapore, china, singapore, usa",0.0,,"One-shot image segmentation aims to undertake the segmentation task of a novel class with only one training image available. The difficulty lies in that image segmentation has structured data representations, which yields a many-to-many message passing problem. Previous methods often simplify it to a one-to-many problem by squeezing support data to a global descriptor. However, a mixed global representation drops the data structure and information of individual elements. In this paper, we propose to model structured segmentation data with graphs and apply attentive graph reasoning to propagate label information from support data to query data. The graph attention mechanism could establish the element-to-element correspondence across structured data by learning attention weights between connected graph nodes. To capture correspondence at different semantic levels, we further propose a pyramid-like structure that models different sizes of image regions as graph nodes and undertakes graph reasoning at different levels. Experiments on PASCAL VOC 2012 dataset demonstrate that our proposed network significantly outperforms the baseline method and leads to new state-of-the-art performance on 1-shot and 5-shot segmentation benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010760/,"['Task analysis', 'Image segmentation', 'Training', 'Cognition', 'Adaptation models', 'Feature extraction', 'Computer vision']","['Semantic Segmentation', 'Pyramid Graph', 'Data Structure', 'Image Regions', 'Attention Mechanism', 'Size Of Region', 'Nodes In The Graph', 'Baseline Methods', 'Attention Weights', 'Semantic Level', 'Global Descriptors', 'Graph Attention', 'Training Time', 'Feature Maps', 'Intersection Over Union', 'Residual Block', 'Dilated Convolution', 'Support Set', 'Query Set', 'Global Vector', 'Query Image', 'Few-shot Classification', 'Graph Attention Network', 'Atrous Spatial Pyramid Pooling', 'Original Feature Map', 'Query Features', 'Original Query', 'Mean Intersection Over Union', 'Meta Learning', 'Node Representations']",,199,"One-shot image segmentation aims to undertake the segmentation task of a novel class with only one training image available. The difficulty lies in that image segmentation has structured data representations, which yields a many-to-many message passing problem. Previous methods often simplify it to a one-to-many problem by squeezing support data to a global descriptor. However, a mixed global representation drops the data structure and information of individual elements. In this paper, we propose to model structured segmentation data with graphs and apply attentive graph reasoning to propagate label information from support data to query data. The graph attention mechanism could establish the element-to-element correspondence across structured data by learning attention weights between connected graph nodes. To capture correspondence at different semantic levels, we further propose a pyramid-like structure that models different sizes of image regions as graph nodes and undertakes graph reasoning at different levels. Experiments on PASCAL VOC 2012 dataset demonstrate that our proposed network significantly outperforms the baseline method and leads to new state-of-the-art performance on 1-shot and 5-shot segmentation benchmarks."
QUARCH: A New Quasi-Affine Reconstruction Stratum From Vague Relative Camera Orientation Knowledge,"Devesh Adlakha, Adlane Habed, Fabio Morbidi, CÃ©dric Demonceaux, Michel de Mathelin","ICube laboratory, CNRS, University of Strasbourg; MIS laboratory, University of Picardie Jules Verne; ImViA laboratory, VIBOT ERL CNRS, University of Burgundy - Franche-Comté",100.0,France,0.0,,"We present a new quasi-affine reconstruction of a scene and its application to camera self-calibration. We refer to this reconstruction as QUARCH (QUasi-Affine Reconstruction with respect to Camera centers and the Hodographs of horopters). A QUARCH can be obtained by solving a semidefinite programming problem when, (i) the images have been captured by a moving camera with constant intrinsic parameters, and (ii) a vague knowledge of the relative orientation (under or over 120 degrees) between camera pairs is available. The resulting reconstruction comes close enough to an affine one allowing thus an easy upgrade of the QUARCH to its affine and metric counterparts. We also present a constrained Levenberg-Marquardt method for nonlinear optimization subject to Linear Matrix Inequality (LMI) constraints so as to ensure that the QUARCH LMIs are satisfied during optimization. Experiments with synthetic and real data show the benefits of QUARCH in reliably obtaining a metric reconstruction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Adlakha_QUARCH_A_New_Quasi-Affine_Reconstruction_Stratum_From_Vague_Relative_Camera_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Adlakha_QUARCH_A_New_Quasi-Affine_Reconstruction_Stratum_From_Vague_Relative_Camera_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010300/,"['Cameras', 'Image reconstruction', 'Optimization', 'Reliability', 'Calibration', 'Linear matrix inequalities']","['Nonlinear Programming', 'Relative Orientation', 'Linear Inequalities', 'Levenberg-Marquardt Algorithm', 'Linear Matrix Inequalities', 'Semidefinite Programming', 'Camera Center', 'Constrained Method', 'Semidefinite Programming Problem', 'Pair Of Cameras', 'Cost Function', 'Local Optimum', 'Focal Length', 'Linear Problem', 'Presence Of Noise', 'Projection Matrix', 'Convex Hull', 'Constrained Optimization', 'Feature Matching', 'Relative Angle', 'Unconstrained Optimization', 'Scene Point', '3D Error', 'Median Error', 'Real Point', 'Optimal Cost Function', 'Mild Assumptions', 'Local Optimization Methods']",,,"We present a new quasi-affine reconstruction of a scene and its application to camera self-calibration. We refer to this reconstruction as QUARCH (QUasi-Affine Reconstruction with respect to Camera centers and the Hodographs of horopters). A QUARCH can be obtained by solving a semidefinite programming problem when, (i) the images have been captured by a moving camera with constant intrinsic parameters, and (ii) a vague knowledge of the relative orientation (under or over 120 degrees) between camera pairs is available. The resulting reconstruction comes close enough to an affine one allowing thus an easy upgrade of the QUARCH to its affine and metric counterparts. We also present a constrained Levenberg-Marquardt method for nonlinear optimization subject to Linear Matrix Inequality (LMI) constraints so as to ensure that the QUARCH LMIs are satisfied during optimization. Experiments with synthetic and real data show the benefits of QUARCH in reliably obtaining a metric reconstruction."
Quasi-Globally Optimal and Efficient Vanishing Point Estimation in Manhattan World,"Haoang Li, Ji Zhao, Jean-Charles Bazin, Wen Chen, Zhe Liu, Yun-Hui Liu","KAIST, South Korea; TuSimple, China; The Chinese University of Hong Kong, China",100.0,"Hong Kong, south korea, usa",0.0,,"The image lines projected from parallel 3D lines intersect at a common point called the vanishing point (VP). Manhattan world holds for the scenes with three orthogonal VPs. In Manhattan world, given several lines in a calibrated image, we aim at clustering them by three unknown-but-sought VPs. The VP estimation can be reformulated as computing the rotation between the Manhattan frame and the camera frame. To compute this rotation, state-of-the-art methods are based on either data sampling or parameter search, and they fail to guarantee the accuracy and efficiency simultaneously. In contrast, we propose to hybridize these two strategies. We first compute two degrees of freedom (DOF) of the above rotation by two sampled image lines, and then search for the optimal third DOF based on the branch-and-bound. Our sampling accelerates our search by reducing the search space and simplifying the bound computation. Our search is not sensitive to noise and achieves quasi-global optimality in terms of maximizing the number of inliers. Experiments on synthetic and real-world images showed that our method outperforms state-of-the-art approaches in terms of accuracy and/or efficiency.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Quasi-Globally_Optimal_and_Efficient_Vanishing_Point_Estimation_in_Manhattan_World_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Quasi-Globally_Optimal_and_Efficient_Vanishing_Point_Estimation_in_Manhattan_World_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008387/,"['Three-dimensional displays', 'Estimation', 'Cameras', 'Acceleration', 'Art', 'Two dimensional displays', 'Uncertainty']","['Vanishing Point', 'Manhattan World', 'Degrees Of Freedom', 'Search Space', 'Synthetic Images', 'Camera Frame', 'Real-world Images', 'Inliers', '3D Line', 'Upper Bound', 'Loss Of Generality', 'Cost Function', 'Local Minima', 'Local Maxima', 'Global Optimization', 'Parametrized', 'Precision And Recall', 'Set Of Lines', 'Validity Of Hypothesis', 'Line Positions', 'Rotation In Space', 'Green Tree', 'Sampling-based Methods', '2D Vector', 'Division Operation', 'Time Budget', 'Axis Angle', 'Search Scope', 'Rotation Parameters']",,22,"The image lines projected from parallel 3D lines intersect at a common point called the vanishing point (VP). Manhattan world holds for the scenes with three orthogonal VPs. In Manhattan world, given several lines in a calibrated image, we aim at clustering them by three unknown-but-sought VPs. The VP estimation can be reformulated as computing the rotation between the Manhattan frame and the camera frame. To compute this rotation, state-of-the-art methods are based on either data sampling or parameter search, and they fail to guarantee the accuracy and efficiency simultaneously. In contrast, we propose to hybridize these two strategies. We first compute two degrees of freedom (DOF) of the above rotation by two sampled image lines, and then search for the optimal third DOF based on the branch-and-bound. Our sampling accelerates our search by reducing the search space and simplifying the bound computation. Our search is not sensitive to noise and achieves quasi-global optimality in terms of maximizing the number of inliers. Experiments on synthetic and real-world images showed that our method outperforms state-of-the-art approaches in terms of accuracy and/or efficiency."
RANet: Ranking Attention Network for Fast Video Object Segmentation,"Ziqin Wang, Jun Xu, Li Liu, Fan Zhu, Ling Shao","Inception Institute of Artiﬁcial Intelligence (IIAI), Abu Dhabi, UAE; The University of Sydney, Sydney, Australia",100.0,"australia, uae",0.0,,"Despite online learning (OL) techniques have boosted the performance of semi-supervised video object segmentation (VOS) methods, the huge time costs of OL greatly restricts their practicality. Matching based and propagation based methods run at a faster speed by avoiding OL techniques. However, they are limited by sub-optimal accuracy, due to mismatching and drifting problems. In this paper, we develop a real-time yet very accurate Ranking Attention Network (RANet) for VOS. Specifically, to integrate the insights of matching based and propagation based methods, we employ an encoder-decoder framework to learn pixel-level similarity and segmentation in an end-to-end manner. To better utilize the similarity maps, we propose a novel ranking attention module, which automatically ranks and selects these maps for fine-grained VOS performance. Experiments on DAVIS16 and DAVIS17 datasets show that our RANet achieves the best speed-accuracy trade-off, e.g., with 33 milliseconds per frame and J&F=85.5% on DAVIS16. With OL, our RANet reaches J&F=87.1% on DAVIS16, exceeding state-of-the-art VOS methods. The code can be found at https://github.com/Storife/RANet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_RANet_Ranking_Attention_Network_for_Fast_Video_Object_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_RANet_Ranking_Attention_Network_for_Fast_Video_Object_Segmentation_ICCV_2019_paper.pdf,,https://github.com/Storife/RANet,,main,Poster,https://ieeexplore.ieee.org/document/9010635/,"['Feature extraction', 'Correlation', 'Random access memory', 'Decoding', 'Image segmentation', 'Task analysis', 'Real-time systems']","['Video Object Segmentation', 'Online Learning', 'Similarity Map', 'Training Set', 'Validation Set', 'Convolutional Layers', 'Qualitative Results', 'Data Augmentation', 'Static Images', 'Video Data', 'Optical Flow', 'Multi-scale Features', 'Ranking Score', 'Object Tracking', 'Current Frame', 'Channel Size', 'Final Segmentation', 'Matching Technique', 'Previous Frame', 'Siamese Network', 'Template Feature', 'Merge Module', 'Pixel In Frame', 'Conformable', 'Fast Motion', 'Challenging Scenarios', 'Feature Matching']",,163,"Despite online learning (OL) techniques have boosted the performance of semi-supervised video object segmentation (VOS) methods, the huge time costs of OL greatly restricts their practicality. Matching based and propagation based methods run at a faster speed by avoiding OL techniques. However, they are limited by sub-optimal accuracy, due to mismatching and drifting problems. In this paper, we develop a real-time yet very accurate Ranking Attention Network (RANet) for VOS. Specifically, to integrate the insights of matching based and propagation based methods, we employ an encoder-decoder framework to learn pixel-level similarity and segmentation in an end-to-end manner. To better utilize the similarity maps, we propose a novel ranking attention module, which automatically ranks and selects these maps for fine-grained VOS performance. Experiments on DAVIS16 and DAVIS17 datasets show that our RANet achieves the best speed-accuracy trade-off, e.g., with 33 milliseconds per frame and J&F=85.5% on DAVIS16. With OL, our RANet reaches J&F=87.1% on DAVIS16, exceeding state-of-the-art VOS methods. The code can be found at https://github.com/Storife/RANet."
RIO: 3D Object Instance Re-Localization in Changing Indoor Environments,"Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias NieÃner","Technical University of Munich; Technical University of Munich, Google",100.0,germany,0.0,,"In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wald_RIO_3D_Object_Instance_Re-Localization_in_Changing_Indoor_Environments_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wald_RIO_3D_Object_Instance_Re-Localization_in_Changing_Indoor_Environments_ICCV_2019_paper.pdf,https://waldjohannau.github.io/RIO/,,,main,Oral,https://ieeexplore.ieee.org/document/9010673/,"['Three-dimensional displays', 'Semantics', 'Task analysis', 'Benchmark testing', 'Simultaneous localization and mapping', 'Indoor environments', 'Layout']","['Indoor Environments', 'Object Instances', 'Benchmark', 'Multiple Objects', '3D Scanning', 'Feature Matching', '3D Scene', 'Ground Truth Annotations', 'Object Pose', 'Multiple Time Steps', 'Temporal Changes', 'F1 Score', 'Large-scale Datasets', 'Singular Value Decomposition', 'Latent Space', 'Semantic Segmentation', 'Large Margin', 'Different Points In Time', 'Pose Estimation', 'Global Alignment', 'Semantic Annotation', 'Multi-scale Network', 'Scene Changes', 'Scene Understanding', '3D Transformation', 'Target Scene', 'RGB-D Dataset', 'Camera Pose', 'Scan Pattern', 'Symmetry Properties']",,65,"In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58%."
Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network,"Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, Yaohai Huang","Beijing University of Posts and Telecommunications; Canon Information Technology (Beijing) Co., Ltd",50.0,China,50.0,China,"Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Racial_Faces_in_the_Wild_Reducing_Racial_Bias_by_Information_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Racial_Faces_in_the_Wild_Reducing_Racial_Bias_by_Information_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010843/,"['Databases', 'Face recognition', 'Mutual information', 'Training', 'Task analysis', 'Clustering algorithms', 'Testing']","['Racial Faces', 'Mutual Information', 'Biometric', 'Face Recognition', 'Target Domain', 'Domain Adaptation', 'Label Information', 'Source Domain', 'Unsupervised Adaptation', 'Convolutional Neural Network', 'Feature Space', 'Visual Comparison', 'Target Image', 'Large Margin', 'Fully-connected Layer', 'Unlabeled Data', 'Target Data', 'Existence Of Bias', 'Entropy Term', 'Maximum Mean Discrepancy', 'Unsupervised Domain Adaptation Methods', 'Pseudo Labels', 'Algorithmic Bias', 'Domain Gap', 'Domain Discrepancy', 'Unlabeled Target Data', 'Deep Learning Era', 'Unsupervised Way', 'Pre-training Stage']",,200,"Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases."
RainFlow: Optical Flow Under Rain Streaks and Rain Veiling Effect,"Ruoteng Li, Robby T. Tan, Loong-Fah Cheong, Angelica I. Aviles-Rivero, Qingnan Fan, Carola-Bibiane SchÃ¶nlieb",Yale-NUS College; National University of Singapore; Stanford University; University of Cambridge,100.0,"singapore, uk, usa",0.0,,"Optical flow in heavy rainy scenes is challenging due to the presence of both rain steaks and rain veiling effect, which break the existing optical flow constraints. Concerning this, we propose a deep-learning based optical flow method designed to handle heavy rain. We introduce a feature multiplier in our network that transforms the features of an image affected by the rain veiling effect into features that are less affected by it, which we call veiling-invariant features. We establish a new mapping operation in the feature space to produce streak-invariant features. The operation is based on a feature pyramid structure of the input images, and the basic idea is to preserve the chromatic features of the background scenes while canceling the rain-streak patterns. Both the veiling-invariant and streak-invariant features are computed and optimized automatically based on the the accuracy of our optical flow estimation. Our network is end-to-end, and handles both rain streaks and the veiling effect in an integrated framework. Extensive experiments show the effectiveness of our method, which outperforms the state of the art method and other baseline methods. We also show that our network can robustly maintain good performance on clean (no rain) images even though it is trained under rain image data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_RainFlow_Optical_Flow_Under_Rain_Streaks_and_Rain_Veiling_Effect_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_RainFlow_Optical_Flow_Under_Rain_Streaks_and_Rain_Veiling_Effect_ICCV_2019_paper.pdf,,https://github.com/liruoteng/RainFlow,,main,Poster,https://ieeexplore.ieee.org/document/9010417/,"['Rain', 'Optical imaging', 'Optical scattering', 'Robustness', 'Optical computing', 'Nonlinear optics', 'Optical sensors']","['Optical Flow', 'Rain Streaks', 'Veiling Effect', 'Image Features', 'Input Image', 'Baseline Methods', 'Clear Image', 'Heavy Rain', 'Feature Pyramid', 'Optical Flow Estimation', 'Optical Flow Method', 'Maximum And Minimum', 'Robust Method', 'Feature Representation', 'Data Augmentation', 'Benchmark Datasets', 'Flow Field', 'Image Pairs', 'Domain Features', 'Depth Information', 'Cost Volume', 'Clean Dataset', 'CNN-based Methods', 'Color Channels', 'Pyramid Level', 'Atmospheric Light', 'Transmission Map', 'Light Attenuation', 'Minimum Feature', 'Backbone Network']",,25,"Optical flow in heavy rainy scenes is challenging due to the presence of both rain steaks and rain veiling effect, which break the existing optical flow constraints. Concerning this, we propose a deep-learning based optical flow method designed to handle heavy rain. We introduce a feature multiplier in our network that transforms the features of an image affected by the rain veiling effect into features that are less affected by it, which we call veiling-invariant features. We establish a new mapping operation in the feature space to produce streak-invariant features. The operation is based on a feature pyramid structure of the input images, and the basic idea is to preserve the chromatic features of the background scenes while canceling the rain-streak patterns. Both the veiling-invariant and streak-invariant features are computed and optimized automatically based on the the accuracy of our optical flow estimation. Our network is end-to-end, and handles both rain streaks and the veiling effect in an integrated framework. Extensive experiments show the effectiveness of our method, which outperforms the state of the art method and other baseline methods. We also show that our network can robustly maintain good performance on clean (no rain) images even though it is trained under rain image data."
Re-ID Driven Localization Refinement for Person Search,"Chuchu Han, Jiacheng Ye, Yunshan Zhong, Xin Tan, Chi Zhang, Changxin Gao, Nong Sang","Peking University; Megvii Technology; Key Laboratory of Ministry of Education for Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology; Shanghai Jiao Tong University",75.0,"China, china",25.0,China,"Person search aims at localizing and identifying a query person from a gallery of uncropped scene images. Different from person re-identification (re-ID), its performance also depends on the localization accuracy of a pedestrian detector. The state-of-the-art methods train the detector individually, and the detected bounding boxes may be sub-optimal for the following re-ID task. To alleviate this issue, we propose a re-ID driven localization refinement framework for providing the refined detection boxes for person search. Specifically, we develop a differentiable ROI transform layer to effectively transform the bounding boxes from the original images. Thus, the box coordinates can be supervised by the re-ID training other than the original detection task. With this supervision, the detector can generate more reliable bounding boxes, and the downstream re-ID model can produce more discriminative embeddings based on the refined person localizations. Extensive experimental results on the widely used benchmarks demonstrate that our proposed method performs favorably against the state-of-the-art person search methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Han_Re-ID_Driven_Localization_Refinement_for_Person_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Re-ID_Driven_Localization_Refinement_for_Person_Search_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010724/,"['Task analysis', 'Detectors', 'Transforms', 'Feature extraction', 'Reliability', 'Training', 'Search problems']","['Local Refinement', 'Bounding Box', 'Detection Task', 'Pedestrian Detection', 'Box Coordinates', 'Re-identification Task', 'Large Datasets', 'Deep Learning', 'Object Detection', 'Raw Images', 'Stochastic Gradient Descent', 'Kullback-Leibler', 'Backbone Network', 'Affine Transformation', 'Classification Loss', 'Faster R-CNN', 'Detection Stage', 'Cropped Images', 'Feature Pyramid Network', 'Triplet Loss', 'Detection Training', 'Regression Loss', 'Softmax Loss', 'Bounding Box Coordinates', 'Target Coordinates', 'Region Proposal Network', 'Detection Model']",,87,"Person search aims at localizing and identifying a query person from a gallery of uncropped scene images. Different from person re-identification (re-ID), its performance also depends on the localization accuracy of a pedestrian detector. The state-of-the-art methods train the detector individually, and the detected bounding boxes may be sub-optimal for the following re-ID task. To alleviate this issue, we propose a re-ID driven localization refinement framework for providing the refined detection boxes for person search. Specifically, we develop a differentiable ROI transform layer to effectively transform the bounding boxes from the original images. Thus, the box coordinates can be supervised by the re-ID training other than the original detection task. With this supervision, the detector can generate more reliable bounding boxes, and the downstream re-ID model can produce more discriminative embeddings based on the refined person localizations. Extensive experimental results on the widely used benchmarks demonstrate that our proposed method performs favorably against the state-of-the-art person search methods."
Real Image Denoising With Feature Attention,"Saeed Anwar, Nick Barnes","Data61, CSIRO and The Australian National University, Australia",100.0,Australia,0.0,,"Deep convolutional neural networks perform better on images containing spatially invariant noise (synthetic noise); however, its performance is limited on real-noisy photographs and requires multiple stage network modeling. To advance the practicability of the denoising algorithms, this paper proposes a novel single-stage blind real image denoising network (RIDNet) by employing a modular architecture. We use residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality on three synthetic and four real noisy datasets against 19 state-of-the-art algorithms demonstrate the superiority of our RIDNet.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Anwar_Real_Image_Denoising_With_Feature_Attention_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Anwar_Real_Image_Denoising_With_Feature_Attention_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010324,"['Noise reduction', 'Noise measurement', 'Feature extraction', 'Image denoising', 'Convolution', 'Image reconstruction', 'Computational modeling']","['Attention Feature', 'Convolutional Neural Network', 'Visual Quality', 'Synthetic Noise', 'Model Performance', 'Deep Network', 'Convolutional Layers', 'Feature Maps', 'Additive Noise', 'Receptive Field', 'Convolutional Neural Network Model', 'Clear Image', 'Residual Block', 'Peak Signal-to-noise Ratio', 'Skip Connections', 'Local Connectivity', 'Noisy Images', 'Real Noise', 'Conv Layer', 'Peak Signal-to-noise Ratio Values', 'Residual Module', 'Competitive Algorithm', 'Average Peak Signal-to-noise Ratio', 'Feature Channels', 'Perceptual Loss', 'Noisy Input', 'Super-resolution', 'Global Pooling', 'Online System']",,365,"Deep convolutional neural networks perform better on images containing spatially invariant noise (synthetic noise); however, its performance is limited on real-noisy photographs and requires multiple stage network modeling. To advance the practicability of the denoising algorithms, this paper proposes a novel single-stage blind real image denoising network (RIDNet) by employing a modular architecture. We use residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality on three synthetic and four real noisy datasets against 19 state-of-the-art algorithms demonstrate the superiority of our RIDNet."
Reasoning About Human-Object Interactions Through Dual Attention Networks,"Tete Xiao, Quanfu Fan, Dan Gutfreund, Mathew Monfort, Aude Oliva, Bolei Zhou","Massachusetts Institute of Technology; The Chinese University of Hong Kong; MIT-IBM Watson AI Lab, IBM Research; University of California, Berkeley",100.0,"Hong Kong, usa",0.0,,"Objects are entities we act upon, where the functionality of an object is determined by how we interact with it. In this work we propose a Dual Attention Network model which reasons about human-object interactions. The dual-attentional framework weights the important features for objects and actions respectively. As a result, the recognition of objects and actions mutually benefit each other. The proposed model shows competitive classification performance on the human-object interaction dataset Something-Something. Besides, it can perform weak spatiotemporal localization and affordance segmentation, despite being trained only with video-level labels. The model not only finds when an action is happening and which object is being manipulated, but also identifies which part of the object is being interacted with.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiao_Reasoning_About_Human-Object_Interactions_Through_Dual_Attention_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiao_Reasoning_About_Human-Object_Interactions_Through_Dual_Attention_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009830/,"['Feature extraction', 'Object recognition', 'Head', 'Cognition', 'Visualization', 'Task analysis', 'Two dimensional displays']","['Dual Network', 'Dual Attention', 'Human-object Interaction', 'Dual Attention Network', 'Object Recognition', 'Competitive Performance', 'Action Recognition', 'Object Parts', 'Dual Model', 'Convolutional Neural Network', 'Spatial Dimensions', 'Multilayer Perceptron', 'Kullback-Leibler', 'Attention Module', 'Optical Flow', 'Dual Mode', 'Attention Map', 'Temporal Localization', 'Conditional Random Field', 'Validation Subset', 'Frame Features', 'Object Labels', 'Separate Heading', '3D Filter', 'CNN Features', 'Raw Video', 'Visual Question Answering', 'Classification Head', 'Video Understanding', 'Temporal Domain']",,25,"Objects are entities we act upon, where the functionality of an object is determined by how we interact with it. In this work we propose a Dual Attention Network model which reasons about human-object interactions. The dual-attentional framework weights the important features for objects and actions respectively. As a result, the recognition of objects and actions mutually benefit each other. The proposed model shows competitive classification performance on the human-object interaction dataset Something-Something. Besides, it can perform weak spatiotemporal localization and affordance segmentation, despite being trained only with video-level labels. The model not only finds when an action is happening and which object is being manipulated, but also identifies which part of the object is being interacted with."
Reciprocal Multi-Layer Subspace Learning for Multi-View Clustering,"Ruihuang Li, Changqing Zhang, Huazhu Fu, Xi Peng, Tianyi Zhou, Qinghua Hu","Tianjin University; Sichuan University; Institute of High Performance Computing, A*STAR; Inception Institute of Artiﬁcial Intelligence",100.0,"china, singapore, uae",0.0,,"Multi-view clustering is a long-standing important research topic, however, remains challenging when handling high-dimensional data and simultaneously exploring the consistency and complementarity of different views. In this work, we present a novel Reciprocal Multi-layer Subspace Learning (RMSL) algorithm for multi-view clustering, which is composed of two main components: Hierarchical Self-Representative Layers (HSRL), and Backward Encoding Networks (BEN). Specifically, HSRL constructs reciprocal multi-layer subspace representations linked with a latent representation to hierarchically recover the underlying low-dimensional subspaces in which the high-dimensional data lie; BEN explores complex relationships among different views and implicitly enforces the subspaces of all views to be consistent with each other and more separable. The latent representation flexibly encodes complementary information from multiple views and depicts data more comprehensively. Our model can be efficiently optimized by an alternating optimization scheme. Extensive experiments on benchmark datasets show the superiority of RMSL over other state-of-the-art clustering methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Reciprocal_Multi-Layer_Subspace_Learning_for_Multi-View_Clustering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Reciprocal_Multi-Layer_Subspace_Learning_for_Multi-View_Clustering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010687/,"['Correlation', 'Linear programming', 'Matrix decomposition', 'Clustering algorithms', 'Encoding', 'Data models', 'Computer vision']","['Reciprocal Learning', 'Multi-view Clustering', 'Learning Algorithms', 'Clustering Method', 'Latent Representation', 'Consistent View', 'Hierarchical Layers', 'Subspace Representation', 'Data Structure', 'Typical Features', 'Objective Function', 'Comprehensive Information', 'Second Category', 'Original Features', 'Cluster Structure', 'Common Space', 'Data Partitioning', 'Canonical Correlation', 'Canonical Correlation Analysis', 'Clustering Performance', 'Common Representation', 'Subspace Clustering', 'Spectral Clustering', 'Affinity Matrix', 'Multi-view Learning', 'Linear Subspace', 'Raw Features', 'Single View', 'Weight Matrix', 'Representation Learning']",,77,"Multi-view clustering is a long-standing important research topic, however, remains challenging when handling high-dimensional data and simultaneously exploring the consistency and complementarity of different views. In this work, we present a novel Reciprocal Multi-layer Subspace Learning (RMSL) algorithm for multi-view clustering, which is composed of two main components: Hierarchical Self-Representative Layers (HSRL), and Backward Encoding Networks (BEN). Specifically, HSRL constructs reciprocal multi-layer subspace representations linked with a latent representation to hierarchically recover the underlying low-dimensional subspaces in which the high-dimensional data lie; BEN explores complex relationships among different views and implicitly enforces the subspaces of all views to be consistent with each other and more separable. The latent representation flexibly encodes complementary information from multiple views and depicts data more comprehensively. Our model can be efficiently optimized by an alternating optimization scheme. Extensive experiments on benchmark datasets show the superiority of RMSL over other state-of-the-art clustering methods."
Recognizing Part Attributes With Insufficient Data,"Xiangyun Zhao, Yi Yang, Feng Zhou, Xiao Tan, Yuchen Yuan, Yingze Bao, Ying Wu",Baidu Research; Northwestern University; Baidu Inc.,33.33333333333333,usa,66.66666666666667,China,"Recognizing the attributes of objects and their parts is central to many computer vision applications. Although great progress has been made to apply object-level recognition, recognizing the attributes of parts remains less applicable since the training data for part attributes recognition is usually scarce especially for internet-scale applications. Furthermore, most existing part attribute recognition methods rely on the part annotations which are more expensive to obtain. In order to solve the data insufficiency problem and get rid of dependence on the part annotation, we introduce a novel Concept Sharing Network (CSN) for part attribute recognition. A great advantage of CSN is its capability of recognizing the part attribute (a combination of part location and appearance pattern) that has insufficient or zero training data, by learning the part location and appearance pattern respectively from the training data that usually mix them in a single label. Extensive experiments on CUB, Celeb A, and a newly proposed human attribute dataset demonstrate the effectiveness of CSN and its advantages over other methods, especially for the attributes with few training samples. Further experiments show that CSN can also perform zero-shot part attribute recognition.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Recognizing_Part_Attributes_With_Insufficient_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Recognizing_Part_Attributes_With_Insufficient_Data_ICCV_2019_paper.pdf,,https://github.com/Zhaoxiangyun/Concept-Sharing-Network,,main,Poster,https://ieeexplore.ieee.org/document/9009781/,"['Pattern recognition', 'Training', 'Image recognition', 'Training data', 'Birds', 'Visualization', 'Feature extraction']","['Training Data', 'Computer Vision', 'Local Patterns', 'Human Attributes', 'Training Set', 'Pattern Recognition', 'Positive Samples', 'Feature Maps', 'Attention Mechanism', 'Visual Attention', 'Local Module', 'Attention Map', 'Few-shot Learning', 'Number Of Training Images', 'Recognition Module', 'Visual Question Answering']",,12,"Recognizing the attributes of objects and their parts is central to many computer vision applications. Although great progress has been made to apply object-level recognition, recognizing the attributes of parts remains less applicable since the training data for part attributes recognition is usually scarce especially for internet-scale applications. Furthermore, most existing part attribute recognition methods rely on the part annotations which are more expensive to obtain. In order to solve the data insufficiency problem and get rid of dependence on the part annotation, we introduce a novel Concept Sharing Network (CSN) for part attribute recognition. A great advantage of CSN is its capability of recognizing the part attribute (a combination of part location and appearance pattern) that has insufficient or zero training data, by learning the part location and appearance pattern respectively from the training data that usually mix them in a single label. Extensive experiments on CUB, Celeb A, and a newly proposed human attribute dataset demonstrate the effectiveness of CSN and its advantages over other methods, especially for the attributes with few training samples. Further experiments show that CSN can also perform zero-shot part attribute recognition."
Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification,"Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, Yu-Chiang Frank Wang",National Taiwan University; Academia Sinica; Umbo Computer Vision,33.33333333333333,taiwan,66.66666666666667,USA,"Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Recover_and_Identify_A_Generative_Dual_Model_for_Cross-Resolution_Person_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Recover_and_Identify_A_Generative_Dual_Model_for_Cross-Resolution_Person_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008310/,"['Image resolution', 'Decoding', 'Training', 'Cameras', 'Task analysis', 'Gallium nitride', 'Image reconstruction']","['Input Image', 'Generative Adversarial Networks', 'Low-resolution Images', 'Camera View', 'Query Image', 'Low-resolution Input', 'Training Set', 'Convolutional Neural Network', 'High-resolution Images', 'Image Resolution', 'Feature Maps', 'Attention Mechanism', 'Latent Space', 'Classification Loss', 'Loss Of Identity', 'Realistic Images', 'Unique Color', 'Triplet Loss', 'Background Clutter', 'Viewpoint Changes', 'Super-resolution Model', 'Joint Representation', 'Gallery Set', 'Gallery Images']",,51,"Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training."
Recurrent U-Net for Resource-Constrained Segmentation,"Wei Wang, Kaicheng Yu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann","CVLab, EPFL, 1015 Lausanne",100.0,switzerland,0.0,,"State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010910/,"['Image segmentation', 'Training', 'Standards', 'Computer architecture', 'Encoding', 'Semantics', 'Tensile stress']","['Deep Network', 'State Of The Art', 'Retinal Vessels', 'Road Segments', 'U-Net Architecture', 'Recurrent Architecture', 'Input Image', 'Internal State', 'ImageNet', 'Semantic Segmentation', 'Hidden State', 'Urban Landscape', 'Amount Of Training Data', 'Recurrent Unit', 'Gated Recurrent Unit', 'Iterative Refinement', 'Standard Benchmark', 'Encoder Layer', 'Encoder-decoder Network', 'Decoder Layer', 'Heavy Ones', 'Resource-constrained Environments', 'Signs Of Diabetic Retinopathy', 'Previous Hidden State']",,64,"State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation."
Recursive Cascaded Networks for Unsupervised Medical Image Registration,"Shengyu Zhao, Yue Dong, Eric I-Chao Chang, Yan Xu","School of Biological Science and Medical Engineering and Beijing Advanced Innovation Centre for Biomedical Engineering, Beihang University; Microsoft Research; IIIS, Tsinghua University",66.66666666666666,"China, china",33.33333333333334,USA,"We present recursive cascaded networks, a general architecture that enables learning deep cascades, for deformable image registration. The proposed architecture is simple in design and can be built on any base network. The moving image is warped successively by each cascade and finally aligned to the fixed image; this procedure is recursive in a way that every cascade learns to perform a progressive deformation for the current warped image. The entire system is end-to-end and jointly trained in an unsupervised manner. In addition, enabled by the recursive architecture, one cascade can be iteratively applied for multiple times during testing, which approaches a better fit between each of the image pairs. We evaluate our method on 3D medical images, where deformable registration is most commonly applied. We demonstrate that recursive cascaded networks achieve consistent, significant gains and outperform state-of-the-art methods. The performance reveals an increasing trend as long as more cascades are trained, while the limit is not observed. Code is available at https://github.com/zsyzzsoft/Recursive-Cascaded-Networks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Recursive_Cascaded_Networks_for_Unsupervised_Medical_Image_Registration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Recursive_Cascaded_Networks_for_Unsupervised_Medical_Image_Registration_ICCV_2019_paper.pdf,,https://github.com/zsyzzsoft/Recursive-Cascaded-Networks,,main,Poster,https://ieeexplore.ieee.org/document/9010680/,"['Image registration', 'Strain', 'Computer architecture', 'Three-dimensional displays', 'Biomedical imaging', 'Training', 'Liver']","['Medical Imaging', 'Image Registration', 'Cascade Network', 'Medical Image Registration', '3D Images', 'Image Pairs', 'Warped Image', 'Deformable Registration', 'Convolutional Neural Network', 'Brain Magnetic Resonance Imaging', 'Object Detection', 'Super-resolution', 'Flow Field', 'Unsupervised Methods', 'Learning-based Methods', 'Target Object', 'Affine Transformation', 'Rigid Transformation', 'Large Displacement', 'Regularization Loss', 'Number Of Cascades', 'Dice Score', 'Optical Flow Estimation', 'Progressive Alignment', 'Linear Increment', 'Atlas Analysis', 'NVIDIA Titan Xp GPU', 'Xeon E5', 'Baseline Methods']",,174,"We present recursive cascaded networks, a general architecture that enables learning deep cascades, for deformable image registration. The proposed architecture is simple in design and can be built on any base network. The moving image is warped successively by each cascade and finally aligned to the fixed image; this procedure is recursive in a way that every cascade learns to perform a progressive deformation for the current warped image. The entire system is end-to-end and jointly trained in an unsupervised manner. In addition, enabled by the recursive architecture, one cascade can be iteratively applied for multiple times during testing, which approaches a better fit between each of the image pairs. We evaluate our method on 3D medical images, where deformable registration is most commonly applied. We demonstrate that recursive cascaded networks achieve consistent, significant gains and outperform state-of-the-art methods. The performance reveals an increasing trend as long as more cascades are trained, while the limit is not observed."
Recursive Visual Sound Separation Using Minus-Plus Net,"Xudong Xu, Bo Dai, Dahua Lin","CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong",100.0,"Hong Kong, china",0.0,,"Sounds provide rich semantics, complementary to visual data, for many tasks. However, in practice, sounds from multiple sources are often mixed together. In this paper we propose a novel framework, referred to as MinusPlus Network (MP-Net), for the task of visual sound separation. MP-Net separates sounds recursively in the order of average energy, removing the separated sound from the mixture at the end of each prediction, until the mixture becomes empty or contains only noise. In this way, MP-Net could be applied to sound mixtures with arbitrary numbers and types of sounds. Moreover, while MP-Net keeps removing sounds with large energy from the mixture, sounds with small energy could emerge and become clearer, so that the separation is more accurate. Compared to previous methods, MP-Net obtains state-of-the-art results on two large scale datasets, across mixtures with different types and numbers of sounds.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Recursive_Visual_Sound_Separation_Using_Minus-Plus_Net_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Recursive_Visual_Sound_Separation_Using_Minus-Plus_Net_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008385/,"['Visualization', 'Videos', 'Spectrogram', 'Task analysis', 'Semantics', 'Predictive models', 'Computer vision']","['Sound Separation', 'Spectroscopic', 'Data Visualization', 'Large-scale Datasets', 'Average Energy', 'Large Energy', 'Small Energy', 'Validation Set', 'Video Clips', 'Large Margin', 'Strong Assumptions', 'Non-negative Matrix Factorization', 'Sound Source', 'Visual Content', 'Audio Data', 'Energy In Order', 'Signal-to-interference Ratio', 'Recursive Step', 'Number Of Sounds', 'Set Of Sounds']",,59,"Sounds provide rich semantics, complementary to visual data, for many tasks. However, in practice, sounds from multiple sources are often mixed together. In this paper we propose a novel framework, referred to as MinusPlus Network (MP-Net), for the task of visual sound separation. MP-Net separates sounds recursively in the order of average energy, removing the separated sound from the mixture at the end of each prediction, until the mixture becomes empty or contains only noise. In this way, MP-Net could be applied to sound mixtures with arbitrary numbers and types of sounds. Moreover, while MP-Net keeps removing sounds with large energy from the mixture, sounds with small energy could emerge and become clearer, so that the separation is more accurate. Compared to previous methods, MP-Net obtains state-of-the-art results on two large scale datasets, across mixtures with different types and numbers of sounds."
Reflective Decoding Network for Image Captioning,"Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, Yu-Wing Tai",Tencent; The Hong Kong University of Science and Technology,50.0,Hong Kong,50.0,China,"State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to utilizing the inherent properties of language to boost captioning performance. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the long-sequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for hard cases with complex scenes to describe by captions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009778/,"['Decoding', 'Visualization', 'Feature extraction', 'Syntactics', 'Task analysis', 'Rivers', 'Random access memory']","['Image Captioning', 'Vocabulary', 'Visual Features', 'Encoder-decoder Framework', 'Sentence Position', 'Model Performance', 'Time Step', 'Decoding', 'Image Features', 'Natural Language', 'Regional Characteristics', 'Attention Mechanism', 'Sequential Model', 'Visual Attention', 'Hidden State', 'Syntactic Structure', 'Faster R-CNN', 'Attention Layer', 'Embedding Matrix', 'Word Information', 'Position Embedding', 'Beam Search', 'Position Modulation', 'Object Annotations', 'Inference Stage', 'Visual Representation', 'Semantic', 'Image Descriptors', 'Word Pairs']",,73,"State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to utilizing the inherent properties of language to boost captioning performance. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the long-sequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for hard cases with complex scenes to describe by captions."
RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes,"Po-Wei Wu, Yu-Jing Lin, Che-Han Chang, Edward Y. Chang, Shih-Wei Liao",Stanford University; National Taiwan University; HTC Research & Healthcare,66.66666666666666,"taiwan, usa",33.33333333333334,Taiwan,"Multi-domain image-to-image translation has gained increasing attention recently. Previous methods take an image and some target attributes as inputs and generate an output image with the desired attributes. However, such methods have two limitations. First, these methods assume binary-valued attributes and thus cannot yield satisfactory results for fine-grained control. Second, these methods require specifying the entire set of target attributes, even if most of the attributes would not be changed. To address these limitations, we propose RelGAN, a new method for multi-domain image-to-image translation. The key idea is to use relative attributes, which describes the desired change on selected attributes. Our method is capable of modifying images by changing particular attributes of interest in a continuous manner while preserving the other attributes. Experimental results demonstrate both the quantitative and qualitative effectiveness of our method on the tasks of facial attribute transfer and interpolation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_RelGAN_Multi-Domain_Image-to-Image_Translation_via_Relative_Attributes_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_RelGAN_Multi-Domain_Image-to-Image_Translation_via_Relative_Attributes_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010872,"['Interpolation', 'Generators', 'Gallium nitride', 'Hair', 'Image color analysis', 'Image reconstruction', 'Training']","['Interesting Properties', 'Output Image', 'Facial Attributes', 'Convolutional Layers', 'Input Image', 'Qualitative Results', 'User Study', 'Generative Adversarial Networks', 'Visual Comparison', 'Face Images', 'Visual Quality', 'Target Domain', 'Hair Color', 'Reconstruction Loss', 'Zero Vector', 'Oral And Maxillofacial Surgery', 'Image X', 'Attribute Values', 'Conditional Generative Adversarial Network', 'Blond Hair', 'Fréchet Inception Distance']",,27,"Multi-domain image-to-image translation has gained increasing attention recently. Previous methods take an image and some target attributes as inputs and generate an output image with the desired attributes. However, such methods have two limitations. First, these methods assume binary-valued attributes and thus cannot yield satisfactory results for fine-grained control. Second, these methods require specifying the entire set of target attributes, even if most of the attributes would not be changed. To address these limitations, we propose RelGAN, a new method for multi-domain image-to-image translation. The key idea is to use relative attributes, which describes the desired change on selected attributes. Our method is capable of modifying images by changing particular attributes of interest in a continuous manner while preserving the other attributes. Experimental results demonstrate both the quantitative and qualitative effectiveness of our method on the tasks of facial attribute transfer and interpolation."
Relation Distillation Networks for Video Object Detection,"Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, Tao Mei","CAS Key Laboratory of GIPAS, University of Science and Technology of China, Hefei, China; JD AI Research, Beijing, China",50.0,china,50.0,China,"It has been well recognized that modeling object-to-object relations would be helpful for object detection. Nevertheless, the problem is not trivial especially when exploring the interactions between objects to boost video object detectors. The difficulty originates from the aspect that reliable object relations in a video should depend on not only the objects in the present frame but also all the supportive objects extracted over a long range span of the video. In this paper, we introduce a new design to capture the interactions across the objects in spatio-temporal context. Specifically, we present Relation Distillation Networks (RDN) --- a new architecture that novelly aggregates and propagates object relation to augment object features for detection. Technically, object proposals are first generated via Region Proposal Networks (RPN). RDN then, on one hand, models object relation via multi-stage reasoning, and on the other, progressively distills relation through refining supportive object proposals with high objectness scores in a cascaded manner. The learnt relation verifies the efficacy on both improving object detection in each frame and box linking across frames. Extensive experiments are conducted on ImageNet VID dataset, and superior results are reported when comparing to state-of-the-art methods. More remarkably, our RDN achieves 81.8% and 83.2% mAP with ResNet-101 and ResNeXt-101, respectively. When further equipped with linking and rescoring, we obtain to-date the best reported mAP of 83.8% and 84.7%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Relation_Distillation_Networks_for_Video_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Relation_Distillation_Networks_for_Video_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008824/,"['Proposals', 'Object detection', 'Feature extraction', 'Task analysis', 'Context modeling', 'Detectors', 'Aggregates']","['Object Detection', 'Video Object Detection', 'Object Relations', 'Region Proposal', 'Objective Scores', 'Region Proposal Network', 'Object In Frame', 'Objects In Context', 'Spatiotemporal Context', 'Object Proposals', 'Frame Detection', 'Advanced Stage', 'Convolutional Neural Network', 'Running Time', 'Reference Frame', 'Related Features', 'Ratio Of Samples', 'Detection In Images', 'Appearance Features', 'Faster R-CNN', 'Proposal Features', 'Basic Stages', 'Support Frame', 'Related Modules', 'Detection Boxes', 'Image Object Detection', 'Relational Reasoning', 'Detection In Videos', 'Geometry Information', 'Adjacent Frames']",,163,"It has been well recognized that modeling object-to-object relations would be helpful for object detection. Nevertheless, the problem is not trivial especially when exploring the interactions between objects to boost video object detectors. The difficulty originates from the aspect that reliable object relations in a video should depend on not only the objects in the present frame but also all the supportive objects extracted over a long range span of the video. In this paper, we introduce a new design to capture the interactions across the objects in spatio-temporal context. Specifically, we present Relation Distillation Networks (RDN) - a new architecture that novelly aggregates and propagates object relation to augment object features for detection. Technically, object proposals are first generated via Region Proposal Networks (RPN). RDN then, on one hand, models object relation via multi-stage reasoning, and on the other, progressively distills relation through refining supportive object proposals with high objectness scores in a cascaded manner. The learnt relation verifies the efficacy on both improving object detection in each frame and box linking across frames. Extensive experiments are conducted on ImageNet VID dataset, and superior results are reported when comparing to state-of-the-art methods. More remarkably, our RDN achieves 81.8% and 83.2% mAP with ResNet-101 and ResNeXt-101, respectively. When further equipped with linking and rescoring, we obtain to-date the best reported mAP of 83.8% and 84.7%."
Relation Parsing Neural Network for Human-Object Interaction Detection,"Penghao Zhou, Mingmin Chi","Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China",100.0,China,0.0,,"Human-Object Interaction Detection devotes to infer a triplet < human, verb, object > between human and objects. In this paper, we propose a novel model, i.e., Relation Parsing Neural Network (RPNN), to detect human-object interactions. Specifically, the network is represented by two graphs, i.e., Object-Bodypart Graph and Human-Bodypart Graph. Here, the Object-Bodypart Graph dynamically captures the relationship between body parts and the surrounding objects. The Human-Bodypart Graph infers the relationship between human and body parts, and assembles body part contexts to predict actions. These two graphs are associated through an action passing mechanism. The proposed RPNN model is able to implicitly parse a pairwise relation in two graphs without supervised labels. Experiments conducted on V-COCO and HICO-DET datasets confirm the effectiveness of the proposed RPNN network which significantly outperforms state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008535/,"['Feature extraction', 'Neural networks', 'Task analysis', 'Estimation', 'Object detection', 'Legged locomotion', 'Visualization']","['Neural Network', 'Parsing', 'Neural Network For Detection', 'Human-object Interaction', 'Human-Object Interaction Detection', 'Body Parts', 'Deep Neural Network', 'Object Detection', 'Human Interaction', 'Attention Mechanism', 'Intersection Over Union', 'Multilayer Perceptron', 'Bounding Box', 'Detection Task', 'Object Features', 'Node Features', 'Graph Neural Networks', 'Detection Dataset', 'Bottom-up Methods', 'Message Passing', 'Mask R-CNN', 'Scene Features', 'Object Boxes', 'Top-down Methods', 'Linear Combination Of Features']",,99,"Human-Object Interaction Detection devotes to infer a triplet <; human, verb, object > between human and objects. In this paper, we propose a novel model, i.e., Relation Parsing Neural Network (RPNN), to detect human-object interactions. Specifically, the network is represented by two graphs, i.e., Object-Bodypart Graph and Human-Bodypart Graph. Here, the Object-Bodypart Graph dynamically captures the relationship between body parts and the surrounding objects. The Human-Bodypart Graph infers the relationship between human and body parts, and assembles body part contexts to predict actions. These two graphs are associated through an action passing mechanism. The proposed RPNN model is able to implicitly parse a pairwise relation in two graphs without supervised labels. Experiments conducted on V-COCO and HICO-DET datasets confirm the effectiveness of the proposed RPNN network which significantly outperforms state-of-the-art methods."
Relation-Aware Graph Attention Network for Visual Question Answering,"Linjie Li, Zhe Gan, Yu Cheng, Jingjing Liu",Microsoft Dynamics 365 AI Research,0.0,,100.0,USA,"In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Relation-Aware_Graph_Attention_Network_for_Visual_Question_Answering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Relation-Aware_Graph_Attention_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010056/,"['Visualization', 'Semantics', 'Feature extraction', 'Image edge detection', 'Task analysis', 'Knowledge discovery', 'Computational modeling']","['Graph Attention', 'Graph Attention Network', 'Visual Question Answering', 'Types Of Relationships', 'Image Regions', 'Attention Mechanism', 'Scene Images', 'Object Relations', 'Visual Scene', 'Explicit Relationship', 'Implicit Relations', 'Training Set', 'Spatial Relationship', 'Related Features', 'Visual Features', 'Recurrent Neural Network', 'Image Object', 'Bounding Box', 'Semantic Similarity', 'Types Of Questions', 'Different Types Of Relationships', 'Visual Relationship', 'Gated Recurrent Unit', 'Graph Convolutional Network', 'Semantic Graph', 'High-order Method', 'Black Pixels', 'White Pixels', 'Relation Graph', 'Relational Reasoning']",,207,"In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA."
Relational Attention Network for Crowd Counting,"Anran Zhang, Jiayi Shen, Zehao Xiao, Fan Zhu, Xiantong Zhen, Xianbin Cao, Ling Shao","School of Electronic and Information Engineering, Beihang University, Beijing, China; Key Laboratory of Advanced Technology of Near Space Information System (Beihang University), Ministry of Industry and Information Technology of China, Beijing, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beijing, China; Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE; School of Electronic and Information Engineering, Beihang University, Beijing, China",80.0,"china, uae",20.0,China,"Crowd counting is receiving rapidly growing research interests due to its potential application value in numerous real-world scenarios. However, due to various challenges such as occlusion, insufficient resolution and dynamic backgrounds, crowd counting remains an unsolved problem in computer vision. Density estimation is a popular strategy for crowd counting, where conventional density estimation methods perform pixel-wise regression without explicitly accounting the interdependence of pixels. As a result, independent pixel-wise predictions can be noisy and inconsistent. In order to address such an issue, we propose a Relational Attention Network (RANet) with a self-attention mechanism for capturing interdependence of pixels. The RANet enhances the self-attention mechanism by accounting both short-range and long-range interdependence of pixels, where we respectively denote these implementations as local self-attention (LSA) and global self-attention (GSA). We further introduce a relation module to fuse LSA and GSA to achieve more informative aggregated feature representations. We conduct extensive experiments on four public datasets, including ShanghaiTech A, ShanghaiTech B, UCF-CC-50 and UCF-QNRF. Experimental results on all datasets suggest RANet consistently reduces estimation errors and surpasses the state-of-the-art approaches by large margins.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Relational_Attention_Network_for_Crowd_Counting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Relational_Attention_Network_for_Crowd_Counting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010829/,"['Feature extraction', 'Computational modeling', 'Task analysis', 'Estimation', 'Fuses', 'Correlation', 'Image reconstruction']","['Network Of Relationships', 'Crowd Counting', 'Interdependence', 'Computer Vision', 'Feature Representation', 'Public Datasets', 'Density Estimation', 'Feature Aggregation', 'Self-attention Mechanism', 'Computer Vision Problems', 'Related Modules', 'Root Mean Square Error', 'Convolutional Neural Network', 'Feature Maps', 'Number Of Images', 'Attention Mechanism', 'Density Map', 'Max-pooling', 'Representation Of Information', 'Attention Module', 'Attention Feature', 'Images Of People', 'Intermediate Representation', 'Diverse Tasks', 'Crowd Density', 'Position Of Region', 'Perspective Distortion', 'Long-range Dependencies', 'Self-attention Layer']",,115,"Crowd counting is receiving rapidly growing research interests due to its potential application value in numerous real-world scenarios. However, due to various challenges such as occlusion, insufficient resolution and dynamic backgrounds, crowd counting remains an unsolved problem in computer vision. Density estimation is a popular strategy for crowd counting, where conventional density estimation methods perform pixel-wise regression without explicitly accounting the interdependence of pixels. As a result, independent pixel-wise predictions can be noisy and inconsistent. In order to address such an issue, we propose a Relational Attention Network (RANet) with a self-attention mechanism for capturing interdependence of pixels. The RANet enhances the self-attention mechanism by accounting both short-range and long-range interdependence of pixels, where we respectively denote these implementations as local self-attention (LSA) and global self-attention (GSA). We further introduce a relation module to fuse LSA and GSA to achieve more informative aggregated feature representations. We conduct extensive experiments on four public datasets, including ShanghaiTech A, ShanghaiTech B, UCF-CC-50 and UCF-QNRF. Experimental results on all datasets suggest RANet consistently reduces estimation errors and surpasses the state-of-the-art approaches by large margins."
Remote Heart Rate Measurement From Highly Compressed Facial Videos: An End-to-End Deep Learning Solution With Video Enhancement,"Zitong Yu, Wei Peng, Xiaobai Li, Xiaopeng Hong, Guoying Zhao","Center for Machine Vision and Signal Analysis, University of Oulu, Finland; School of Information and Technology, Northwest University, PRC; Center for Machine Vision and Signal Analysis, University of Oulu, Finland; Faculty of Electronic and Information Engineering, Xi’an Jiaotong University, PRC; Peng Cheng Laborotory, China",80.0,"China, china, finland",20.0,China,"Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing rPPG approaches rely on analyzing very fine details of facial videos, which are prone to be affected by video compression. Here we propose a two-stage, end-to-end method using hidden rPPG information enhancement and attention networks, which is the first attempt to counter video compression loss and recover rPPG signals from highly compressed videos. The method includes two parts: 1) a Spatio-Temporal Video Enhancement Network (STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal recovery. The rPPGNet can work on its own for robust rPPG measurement, and the STVEN network can be added and jointly trained to further boost the performance especially on highly compressed videos. Comprehensive experiments are performed on two benchmark datasets to show that, 1) the proposed method not only achieves superior performance on compressed videos with high-quality videos pair, 2) it also generalizes well on novel data with only compressed videos available, which implies the promising potential for real-world applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Remote_Heart_Rate_Measurement_From_Highly_Compressed_Facial_Videos_An_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Remote_Heart_Rate_Measurement_From_Highly_Compressed_Facial_Videos_An_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010965,"['Videos', 'Video compression', 'Bit rate', 'Skin', 'Convolution', 'Machine learning', 'Standards']","['Deep Learning', 'Heart Rate Measurements', 'High Compression', 'Remote Measurement', 'Deep Learning Solutions', 'Attention Network', 'Hidden Information', 'Signal Recovery', 'Joint Training', 'High-quality Video', 'Spatiotemporal Network', 'Video Compression', 'Root Mean Square Error', 'Convolutional Neural Network', 'Feature Maps', 'Subtle Changes', 'Heart Rate Variability', 'Peak Location', 'Face Images', 'Multi-task Learning', 'Original Video', 'Video Quality', 'Average Heart Rate', 'Compression Artifacts', 'Skin Regions', 'Average Bit', 'Quality Enhancement', 'Electrocardiography Signals', 'File Size', 'Least Significant Bit']",,182,"Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing rPPG approaches rely on analyzing very fine details of facial videos, which are prone to be affected by video compression. Here we propose a two-stage, end-to-end method using hidden rPPG information enhancement and attention networks, which is the first attempt to counter video compression loss and recover rPPG signals from highly compressed videos. The method includes two parts: 1) a Spatio-Temporal Video Enhancement Network (STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal recovery. The rPPGNet can work on its own for robust rPPG measurement, and the STVEN network can be added and jointly trained to further boost the performance especially on highly compressed videos. Comprehensive experiments are performed on two benchmark datasets to show that, 1) the proposed method not only achieves superior performance on compressed videos with high-quality videos pair, 2) it also generalizes well on novel data with only compressed videos available, which implies the promising potential for real-world applications."
RepPoints: Point Set Representation for Object Detection,"Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, Stephen Lin",Peking University; Tsinghua University; Microsoft Research Asia,66.66666666666666,"China, china",33.33333333333334,USA,"Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present RepPoints (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 AP_ 50  on the COCO test-dev detection benchmark, using ResNet-101 model. Code is available at  https://github.com/microsoft/RepPoints  \color cyan  https://github.com/microsoft/RepPoints  .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.pdf,,https://github.com/microsoft/RepPoints,,main,Poster,https://ieeexplore.ieee.org/document/9009032/,"['Detectors', 'Feature extraction', 'Object detection', 'Proposals', 'Target recognition', 'Benchmark testing', 'Training']","['Object Detection', 'Semantic', 'Bounding Box', 'Object Features', 'Object Location', 'Rectangular Box', 'Recognition Stage', 'Anchor-based Methods', 'Deep Network', 'Feature Maps', 'Central Point', 'Localization Accuracy', 'Object Recognition', 'Extreme Points', 'Local Loss', 'Final Target', 'Object Point', 'Object Detection Methods', 'Local Stage', 'Basic Representation', 'Bounding Box Regression', 'Ground-truth Bounding Box', 'Ground Truth Object', 'Deformable Convolution', 'Hypothesis Space', 'Deformable Layer', 'Flexible Representation', 'Object Bounding Boxes', 'Bottom-up Manner', 'Conv Layer']",,714,"Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present \textbf{RepPoints} (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 $AP_{50}$ on the COCO test-dev detection benchmark, using ResNet-101 model. Code is available at \href{https://github.com/microsoft/RepPoints}{\color{cyan}{https://github.com/microsoft/RepPoints}}."
Rescan: Inductive Instance Segmentation for Indoor RGBD Scans,"Maciej Halber, Yifei Shi, Kai Xu, Thomas Funkhouser",Princeton University,100.0,usa,0.0,,"In depth-sensing applications ranging from home robotics to AR/VR, it will be common to acquire 3D scans of interior spaces repeatedly at sparse time intervals (e.g., as part of regular daily use). We propose an algorithm that analyzes these ""rescans"" to infer a temporal model of a scene with semantic instance information. Our algorithm operates inductively by using the temporal model resulting from past observations to infer an instance segmentation of a new scan, which is then used to update the temporal model. The model contains object instance associations across time and thus can be used to track individual objects, even though there are only sparse observations. During experiments with a new benchmark for the new task, our algorithm outperforms alternate approaches based on state-of-the-art networks for semantic instance segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Halber_Rescan_Inductive_Instance_Segmentation_for_Indoor_RGBD_Scans_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Halber_Rescan_Inductive_Instance_Segmentation_for_Indoor_RGBD_Scans_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009033/,"['Silicon', 'Solid modeling', 'Semantics', 'Optimization', 'Geometry', 'Three-dimensional displays', 'Cameras']","['Instance Segmentation', 'RGB-D Scans', 'Semantic Segmentation', '3D Scanning', 'Temporal Model', 'Object Instances', 'Sparse Observations', 'Average Score', 'Objective Function', 'Computer Vision', 'Inset Of Fig', 'Point Cloud', 'Benchmark Datasets', 'Simulated Annealing', 'Object Motion', 'Clusters Of Points', 'Surface Reconstruction', 'Matching Score', 'Semantic Labels', 'Object Pairs', 'Instance Segmentation Methods', 'Semantic Segmentation Task', 'Part Of The Scene', 'Instance Labels', 'Building Information Modelling', 'Multiple Time Steps', 'Object Placement', 'Deep Neural Network', 'Depth Camera', 'Mahalanobis Distance']",,10,"In depth-sensing applications ranging from home robotics to AR/VR, it will be common to acquire 3D scans of interior spaces repeatedly at sparse time intervals (e.g., as part of regular daily use). We propose an algorithm that analyzes these ``rescans'' to infer a temporal model of a scene with semantic instance information. Our algorithm operates inductively by using the temporal model resulting from past observations to infer an instance segmentation of a new scan, which is then used to update the temporal model. The model contains object instance associations across time and thus can be used to track individual objects, even though there are only sparse observations. During experiments with a new benchmark for the new task, our algorithm outperforms alternate approaches based on state-of-the-art networks for semantic instance segmentation."
Resolving 3D Human Pose Ambiguities With 3D Scene Constraints,"Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, Michael J. Black",Max Planck Institute for Intelligent Systems,100.0,Germany,0.0,,"To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The inter-penetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hassan_Resolving_3D_Human_Pose_Ambiguities_With_3D_Scene_Constraints_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hassan_Resolving_3D_Human_Pose_Ambiguities_With_3D_Scene_Constraints_ICCV_2019_paper.pdf,https://prox.is.tue.mpg.de,,,main,Poster,https://ieeexplore.ieee.org/document/9010321/,"['Three-dimensional displays', 'Solid modeling', 'Pose estimation', 'Cameras', 'Image reconstruction', 'Shape', 'Two dimensional displays']","['3D Scene', 'Human Pose', '3D Pose', '3D Human Pose', 'Scene Constraints', 'Human Model', 'Quantitative Evaluation', 'Pose Estimation', 'Body Model', 'Human Pose Estimation', 'Static Scenes', '3D Body', 'Human Body Model', 'Pose Estimation Methods', 'Monocular Images', 'Body Pose', 'Single Image', 'Facial Expressions', 'Differentiable Function', '3D Reconstruction', 'RGB Images', 'Human Body Shape', 'Body Shape', '3D Scanning', '3D Mesh', 'Qualitative Dataset', 'Quantitative Datasets', 'Point Cloud', 'Pose Parameters', 'Triangular Face']",,175,"To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The inter-penetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de."
Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?,"Yunyang Xiong, Ronak Mehta, Vikas Singh",University of Wisconsin Madison,100.0,usa,0.0,,"The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_Resource_Constrained_Neural_Network_Architecture_Search_Will_a_Submodularity_Assumption_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_Resource_Constrained_Neural_Network_Architecture_Search_Will_a_Submodularity_Assumption_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009490/,"['Computer architecture', 'Optimization', 'Computational modeling', 'Search problems', 'Neural networks', 'Heuristic algorithms']","['Neural Architecture', 'Neural Architecture Search', 'Resource Constrained', 'Neural Network Architecture Search', 'Resource Constraints', 'Search Method', 'Fewer Parameters', 'Human Experts', 'Gain Margin', 'Derivative-free Optimization', 'Training Set', 'Evaluation Of Function', 'Computer Vision', 'Learning Curve', 'Search Space', 'Mobile Network', 'Early Stopping', 'Convex Hull', 'Marginal Utility', 'Set Of Blocks', 'Block Type', 'Priority Queue', 'Top Element', 'CNN Model', 'Learning Rate Schedule', 'Group Convolution', 'ImageNet Dataset', 'Horizontal Flip', 'Black-box Optimization']",,16,"The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster."
Restoration of Non-Rigidly Distorted Underwater Images Using a Combination of Compressive Sensing and Local Polynomial Image Representations,"Jerin Geo James, Pranay Agrawal, Ajit Rajwade",IIT Bombay,100.0,India,0.0,,"Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage. The source code, datasets and supplemental material can be accessed at [??], [??].",,http://openaccess.thecvf.com/content_ICCV_2019/html/James_Restoration_of_Non-Rigidly_Distorted_Underwater_Images_Using_a_Combination_of_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/James_Restoration_of_Non-Rigidly_Distorted_Underwater_Images_Using_a_Combination_of_ICCV_2019_paper.pdf,1,,,main,Oral,https://ieeexplore.ieee.org/document/9009582/,"['Tracking', 'Image restoration', 'Video sequences', 'Optical surface waves', 'Trajectory', 'Optical distortion', 'Optical imaging']","['Distortion', 'Underwater Image', 'Supplemental Material', 'Feature Points', 'Optical Flow', 'Discrete Fourier Transform', 'Video Sequences', 'Sparse Representation', 'Motion Estimation', 'Video Quality', 'Field Contributions', 'Salient Points', 'Temporal Period', 'Static Scenes', 'Motion Field', 'Fourier Basis', 'Polynomial Expansion', 'Optical Flow Algorithm', 'Non-rigid Motion', 'Deep Learning', 'Small Patches', 'Feature Point Detection', 'State Of The Art', 'Video Frames', 'Optical Flow Method', 'Difference Of Gaussian', 'Measurement Vector', 'Vector Field', 'Siamese Network', 'Real Videos']",,12,"Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage. The source code, datasets and supplemental material can be accessed at \cite{GitRepo}, \cite{ProjectPage}."
Rethinking ImageNet Pre-Training,"Kaiming He, Ross Girshick, Piotr DollÃ¡r",Facebook AI Research (FAIR),0.0,,100.0,USA,"We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate  50.9  AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010930/,"['Training', 'Task analysis', 'Detectors', 'Schedules', 'Data models', 'Computer vision', 'Object detection']","['ImageNet Pretraining', 'Training Data', 'Computer Vision', 'Object Detection', 'Training Iterations', 'Random Initialization', 'Instance Segmentation', 'Target Task', 'Fine-tuned Model', 'Mask R-CNN', 'COCO Dataset', 'Baseline System', 'Overfitting', 'Learning Rate', 'Batch Size', 'Large-scale Data', 'Data Augmentation', 'Number Of Images', 'Target Data', 'Training Schedule', 'Group Normalization', 'Pre-training Tasks', 'Keypoint Detection', 'Feature Pyramid Network', 'Faster R-CNN', 'Pre-training Data', 'Target Dataset', 'Small Batch', 'Region Proposal Network']",,539,"We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data-a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pretraining and fine-tuning' in computer vision."
Rethinking Zero-Shot Learning: A Conditional Visual Classification Perspective,"Kai Li, Martin Renqiang Min, Yun Fu","Khoury College of Computer Science, Northeastern University, Boston, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, USA; NEC Laboratories America",66.66666666666666,china,33.33333333333334,USA,"Zero-shot learning (ZSL) aims to recognize instances of unseen classes solely based on the semantic descriptions of the classes. Existing algorithms usually formulate it as a semantic-visual correspondence problem, by learning mappings from one feature space to the other. Despite being reasonable, previous approaches essentially discard the highly precious discriminative power of visual features in an implicit way, and thus produce undesirable results. We instead reformulate ZSL as a conditioned visual classification problem, i.e., classifying visual features based on the classifiers learned from the semantic descriptions. With this reformulation, we develop algorithms targeting various ZSL settings: For the conventional setting, we propose to train a deep neural network that directly generates visual feature classifiers from the semantic attributes with an episode-based training scheme; For the generalized setting, we concatenate the learned highly discriminative classifiers for seen classes and the generated classifiers for unseen classes to classify visual features of all classes; For the transductive setting, we exploit unlabeled data to effectively calibrate the classifier generator using a novel learning-without-forgetting self-training mechanism and guide the process by a robust generalized cross-entropy loss. Extensive experiments show that our proposed algorithms significantly outperform state-of-the-art methods by large margins on most benchmark datasets in all the ZSL settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Rethinking_Zero-Shot_Learning_A_Conditional_Visual_Classification_Perspective_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Rethinking_Zero-Shot_Learning_A_Conditional_Visual_Classification_Perspective_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008374/,"['Visualization', 'Semantics', 'Training', 'Task analysis', 'Neural networks', 'Generators', 'Computer vision']","['Visual Classification', 'Zero-shot', 'Deep Neural Network', 'Classification Problem', 'Feature Classification', 'Visual Features', 'Cross-entropy Loss', 'Visual Problems', 'High Discrimination', 'Semantic Properties', 'Conventional Settings', 'Implicit Way', 'Semantic Description', 'Unseen Classes', 'Training Set', 'Generative Adversarial Networks', 'Discriminative Features', 'Latent Space', 'Dot Product', 'Classification Score', 'Pseudo Labels', 'Visual Space', 'Intermediate Space', 'Semantic Vectors', 'Class Weights', 'Semantic Space', 'Class Prototypes', 'Local Label', 'Semantic Features', 'Incorrect Labels']",,79,"Zero-shot learning (ZSL) aims to recognize instances of unseen classes solely based on the semantic descriptions of the classes. Existing algorithms usually formulate it as a semantic-visual correspondence problem, by learning mappings from one feature space to the other. Despite being reasonable, previous approaches essentially discard the highly precious discriminative power of visual features in an implicit way, and thus produce undesirable results. We instead reformulate ZSL as a conditioned visual classification problem, i.e., classifying visual features based on the classifiers learned from the semantic descriptions. With this reformulation, we develop algorithms targeting various ZSL settings: For the conventional setting, we propose to train a deep neural network that directly generates visual feature classifiers from the semantic attributes with an episode-based training scheme; For the generalized setting, we concatenate the learned highly discriminative classifiers for seen classes and the generated classifiers for unseen classes to classify visual features of all classes; For the transductive setting, we exploit unlabeled data to effectively calibrate the classifier generator using a novel learning-without-forgetting self-training mechanism and guide the process by a robust generalized cross-entropy loss. Extensive experiments show that our proposed algorithms significantly outperform state-of-the-art methods by large margins on most benchmark datasets in all the ZSL settings."
Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data,"Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, Sai-Kit Yeung",The University of Tokyo; Hong Kong University of Science and Technology; Deakin University; Singapore University of Technology and Design,100.0,"Australia, Hong Kong, Japan, singapore",0.0,,"Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy ( 92%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Uy_Revisiting_Point_Cloud_Classification_A_New_Benchmark_Dataset_and_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Uy_Revisiting_Point_Cloud_Classification_A_New_Benchmark_Dataset_and_Classification_ICCV_2019_paper.pdf,https://hkust-vgd.github.io/scanobjectnn/,,,main,Oral,https://ieeexplore.ieee.org/document/9009007/,"['Three-dimensional displays', 'Solid modeling', 'Benchmark testing', 'Data models', 'Task analysis', 'Training', 'Market research']","['Benchmark', 'Real-world Data', 'Point Cloud', 'Point Cloud Classification', 'Deep Learning', 'Classification Methods', 'Real-world Setting', 'Object Classification', 'Real-world Datasets', '3D Classification', 'Point Cloud Data', 'CAD Model', 'Real-world Objects', 'Object Dataset', 'Object Point Cloud', 'Classification Task', 'Object Detection', 'Bounding Box', 'Classification Network', 'Local Coordinate', 'Background Objects', 'Background Elements', 'Presence Of Background', 'Part Segmentation', 'Segmentation Task', 'Joint Learning', 'Real Scans', 'Real-world Scenes', 'Background Points', 'Real-world Scenarios']",,400,"Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy (~92\%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/."
Revisiting Radial Distortion Absolute Pose,"Viktor Larsson, Torsten Sattler, Zuzana Kukelova, Marc Pollefeys","Department of Computer Science, ETH Zurich and Microsoft; Chalmers University of Technology; Department of Computer Science, ETH Zurich; VRG, Department of Cybernetics, Czech Technical University in Prague",100.0,"Czech Republic, sweden, switzerland",0.0,,"To model radial distortion there are two main approaches; either the image points are undistorted such that they correspond to pinhole projections, or the pinhole projections are distorted such that they align with the image measurements. Depending on the application, either of the two approaches can be more suitable. For example, distortion models are commonly used in Structure-from-Motion since they simplify measuring the reprojection error in images. Surprisingly, all previous minimal solvers for pose estimation with radial distortion use undistortion models. In this paper we aim to fill this gap in the literature by proposing the first minimal solvers which can jointly estimate distortion models together with camera pose. We present a general approach which can handle rational models of arbitrary degree for both distortion and undistortion.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Larsson_Revisiting_Radial_Distortion_Absolute_Pose_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Larsson_Revisiting_Radial_Distortion_Absolute_Pose_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008300/,"['Nonlinear distortion', 'Cameras', 'Optical distortion', 'Lenses', 'Mathematical model', 'Calibration']","['Image Distortion', 'Absolute Pose', 'Gap In The Literature', 'Image Point', 'Pose Estimation', 'Rational Model', 'Joint Estimation', 'Camera Pose', 'Degree Model', 'Reprojection Error', 'Distortion Model', 'Focal Length', 'Corresponding Points', '3D Point', 'Polynomial Model', 'Eigenvalue Problem', 'Checkers', 'Intrinsic Parameters', 'Radial Component', 'Tangential Components', 'Distortion Parameters', 'Rational Function', 'Pose Parameters', 'Ground Truth Pose', 'Generalized Eigenvalue Problem', 'Inliers', 'Least-squares Sense', 'Synthetic Experiments', 'Pixel Error', 'Camera Model']",,26,"To model radial distortion there are two main approaches; either the image points are undistorted such that they correspond to pinhole projections, or the pinhole projections are distorted such that they align with the image measurements. Depending on the application, either of the two approaches can be more suitable. For example, distortion models are commonly used in Structure-from-Motion since they simplify measuring the reprojection error in images. Surprisingly, all previous minimal solvers for pose estimation with radial distortion use undistortion models. In this paper we aim to fill this gap in the literature by proposing the first minimal solvers which can jointly estimate distortion models together with camera pose. We present a general approach which can handle rational models of arbitrary degree for both distortion and undistortion."
Robust Change Captioning,"Dong Huk Park, Trevor Darrell, Anna Rohrbach","University of California, Berkeley",100.0,usa,0.0,,"Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over ""before"" and ""after"" images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. ""before"" or ""after"" image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Park_Robust_Change_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008523/,"['Visualization', 'Task analysis', 'Robustness', 'Semantics', 'Natural languages', 'Monitoring', 'Predictive models']","['Natural Language', 'Types Of Changes', 'Visual Input', 'Dual Model', 'Baseline Number', 'Scene Changes', 'Viewpoint Changes', 'Semantic Change', 'Dual Attention', 'Dynamic Attention', 'Convolutional Layers', 'Change Detection', 'Visual Representation', 'Visual Features', 'Attention Mechanism', 'Bounding Box', 'Image Pairs', 'Aerial Images', 'Spatial Attention', 'Object Motion', 'Presence Of Distractors', 'Objects In The Scene', 'Image Captioning', 'Attention Weights', 'Entropy Regularization', 'Street Scenes', 'Inductive Bias', 'Series Of Convolutions', 'Extract Visual Features', 'Amount Of Shift']",,77,"Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over “before” and “after” images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. “before” or “after” image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors."
Robust Motion Segmentation From Pairwise Matches,"Federica Arrigoni, Tomas Pajdla",CIIRC – Czech Technical University in Prague,100.0,Czech Republic,0.0,,"In this paper we consider the problem of motion segmentation, where only pairwise correspondences are assumed as input without prior knowledge about tracks. The problem is formulated as a two-step process. First, motion segmentation is performed on image pairs independently. Secondly, we combine independent pairwise segmentation results in a robust way into the final globally consistent segmentation. Our approach is inspired by the success of averaging methods. We demonstrate in simulated as well as in real experiments that our method is very effective in reducing the errors in the pairwise motion segmentation and can cope with large number of mismatches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Arrigoni_Robust_Motion_Segmentation_From_Pairwise_Matches_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Arrigoni_Robust_Motion_Segmentation_From_Pairwise_Matches_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008798/,"['Motion segmentation', 'Image segmentation', 'Computer vision', 'Task analysis', 'Tracking', 'Cameras', 'Structure from motion']","['Pairwise Matching', 'Image Pairs', 'Permutation', 'General Framework', 'Image Segmentation', 'Multiple Images', 'Image Point', 'Part Segmentation', 'Spectral Clustering', 'Misclassification Error', 'Subset Of Images', 'Structure From Motion', 'Camera Model', 'Computer Vision Problems', 'Outdoor Scenes', 'Homography', 'Input Matching', 'Presence Of Mismatch', 'Point Labels', '3D Registration', 'Number Of Motions', 'Subspace Clustering', 'Low-rank Representation']",,10,"In this paper we consider the problem of motion segmentation, where only pairwise correspondences are assumed as input without prior knowledge about tracks. The problem is formulated as a two-step process. First, motion segmentation is performed on image pairs independently. Secondly, we combine independent pairwise segmentation results in a robust way into the final globally consistent segmentation. Our approach is inspired by the success of averaging methods. We demonstrate in simulated as well as in real experiments that our method is very effective in reducing the errors in the pairwise motion segmentation and can cope with large number of mismatches."
Robust Multi-Modality Multi-Object Tracking,"Wenwei Zhang, Hui Zhou, Shuyang Sun, Zhe Wang, Jianping Shi, Chen Change Loy",University of Oxford; Nanyang Technological University; SenseTime Research,66.66666666666666,"Singapore, uk",33.33333333333334,China,"Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and could further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Robust_Multi-Modality_Multi-Object_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Robust_Multi-Modality_Multi-Object_Tracking_ICCV_2019_paper.pdf,,https://github.com/ZwwWayne/mmMOT,,main,Poster,https://ieeexplore.ieee.org/document/9010755/,"['Feature extraction', 'Three-dimensional displays', 'Laser radar', 'Detectors', 'Robustness']","['Multi-object Tracking', 'Point Cloud', 'Deep Representation', 'Object Detection', 'Linear Programming', 'Intersection Over Union', 'Bounding Box', 'Deep Features', 'Correlated Features', 'Linear Constraints', 'Markov Decision Process', 'Single Sensor', 'Previous Frame', '3D Detection', 'LiDAR Point Clouds', 'Frustum', 'Frame Detection', 'Pointwise Convolution', 'Point Cloud Features', 'Tracking Framework', '3D Bounding Box', 'Robust Modulation', 'Minimum Cost Flow', 'Online Setting', 'Hard Examples', 'Image Patches', 'Element-wise Multiplication', 'Handcrafted Features', 'Consecutive Frames', '3D Object Detection']",,153,"Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and could further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT."
Robust Person Re-Identification by Modelling Feature Uncertainty,"Tianyuan Yu, Da Li, Yongxin Yang, Timothy M. Hospedales, Tao Xiang","University of Surrey, Samsung AI Centre, Cambridge; University of Surrey; Samsung AI Centre, Cambridge, The University of Edinburgh",100.0,uk,0.0,,"We aim to learn deep person re-identification (ReID) models that are robust against noisy training data. Two types of noise are prevalent in practice: (1) label noise caused by human annotator errors and (2) data outliers caused by person detector errors or occlusion. Both types of noise pose serious problems for training ReID models, yet have been largely ignored so far. In this paper, we propose a novel deep network termed DistributionNet for robust ReID. Instead of representing each person image as a feature vector, DistributionNet models it as a Gaussian distribution with its variance representing the uncertainty of the extracted features. A carefully designed loss is formulated in DistributionNet to unevenly allocate uncertainty across training samples. Consequently, noisy samples are assigned large variance/uncertainty, which effectively alleviates their negative impacts on model fitting. Extensive experiments demonstrate that our model is more effective than alternative noise-robust deep models. The source code is available at: https://github.com/TianyuanYu/DistributionNet",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.pdf,,https://github.com/TianyuanYu/DistributionNet,,main,Poster,https://ieeexplore.ieee.org/document/9008369/,"['Noise measurement', 'Training', 'Robustness', 'Uncertainty', 'Feature extraction', 'Data models', 'Cameras']","['Deep Models', 'Noisy Data', 'Human Error', 'Types Of Noise', 'Person Image', 'Label Noise', 'Person Detection', 'Noisy Training', 'Large Variation', 'Training Set', 'Convolutional Neural Network', 'Deep Neural Network', 'Input Image', 'Random Noise', 'Feature Learning', 'Generative Adversarial Networks', 'Target Image', 'Latent Space', 'Classification Loss', 'Similar Appearance', 'Noise Patterns', 'Noisy Labels', 'Gallery Set', 'Identity Labels', 'Reparameterization Trick', 'Additional Annotations', 'Inliers', 'Camera Network', 'Random Sampling', 'Class Center']",,89,"We aim to learn deep person re-identification (ReID) models that are robust against noisy training data. Two types of noise are prevalent in practice: (1) label noise caused by human annotator errors and (2) data outliers caused by person detector errors or occlusion. Both types of noise pose serious problems for training ReID models, yet have been largely ignored so far. In this paper, we propose a novel deep network termed DistributionNet for robust ReID. Instead of representing each person image as a feature vector, DistributionNet models it as a Gaussian distribution with its variance representing the uncertainty of the extracted features. A carefully designed loss is formulated in DistributionNet to unevenly allocate uncertainty across training samples. Consequently, noisy samples are assigned large variance/uncertainty, which effectively alleviates their negative impacts on model fitting. Extensive experiments demonstrate that our model is more effective than alternative noise-robust deep models. The source code is available at: https://github.com/TianyuanYu/DistributionNet"
Robust Variational Bayesian Point Set Registration,"Jie Zhou, Xinke Ma, Li Liang, Yang Yang, Shijin Xu, Yuhe Liu, Sim-Heng Ong","School of Information Science and Technology, Yunnan Normal University; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University; Department of Electrical and Computer Engineering, National University of Singapore",100.0,"China, singapore",0.0,,"In this work, we propose a hierarchical Bayesian network based point set registration method to solve missing correspondences and various massive outliers. We construct this network first using the finite Student s t latent mixture model (TLMM), in which distributions of latent variables are estimated by a tree-structured variational inference (VI) so that to obtain a tighter lower bound under the Bayesian framework. We then divide the TLMM into two different mixtures with isotropic and anisotropic covariances for correspondences recovering and outliers identification, respectively. Finally, the parameters of mixing proportion and covariances are both taken as latent variables, which benefits explaining of missing correspondences and heteroscedastic outliers. In addition, a cooling schedule is adopted to anneal prior on covariances and scale variables within designed two phases of transformation, it anneal priors on global and local variables to perform a coarse-to- fine registration. In experiments, our method outperforms five state-of-the-art methods in synthetic point set and realistic imaging registrations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008278/,"['Bayes methods', 'Robustness', 'Maximum likelihood estimation', 'Annealing', 'Optimization', 'Mixture models', 'Linear programming']","['Point Cloud Registration', 'Variational Bayesian', 'Studentâ€™s T', 'Distribution Of Variables', 'Latent Variables', 'Mixture Model', 'Scale Variation', 'Bayesian Framework', 'Variational Inference', 'Transformation Phase', 'Latent Variable Distribution', 'Root Mean Square Error', 'Degrees Of Freedom', 'Factorization', 'Maximum Likelihood Estimation', 'Point Estimates', 'Biased Estimates', 'Image Registration', 'Gamma Distribution', 'Assessment Criteria', 'Evidence Lower Bound', '3D Motion', 'Scene Point', 'Optimal Objective Function', 'Gaussian Mixture Model', 'Number Of Outliers', 'Conjugate Prior', 'Gaussian Components', 'Heavy-tailed', 'Gaussian Radial Basis Function']",,11,"In this work, we propose a hierarchical Bayesian network based point set registration method to solve missing correspondences and various massive outliers. We construct this network first using the finite Student s t latent mixture model (TLMM), in which distributions of latent variables are estimated by a tree-structured variational inference (VI) so that to obtain a tighter lower bound under the Bayesian framework. We then divide the TLMM into two different mixtures with isotropic and anisotropic covariances for correspondences recovering and outliers identification, respectively. Finally, the parameters of mixing proportion and covariances are both taken as latent variables, which benefits explaining of missing correspondences and heteroscedastic outliers. In addition, a cooling schedule is adopted to anneal prior on covariances and scale variables within designed two phases of transformation, it anneal priors on global and local variables to perform a coarse-to- fine registration. In experiments, our method outperforms five state-of-the-art methods in synthetic point set and realistic imaging registrations."
S2GAN: Share Aging Factors Across Ages and Share Aging Trends Among Individuals,"Zhenliang He, Meina Kan, Shiguang Shan, Xilin Chen","Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China; Peng Cheng Laboratory, Shenzhen, 518055, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China",100.0,china,0.0,,"Generally, we human follow the roughly common aging trends, e.g., the wrinkles only tend to be more, longer or deeper. However, the aging process of each individual is more dominated by his/her personalized factors, including the invariant factors such as identity and mole, as well as the personalized aging patterns, e.g., one may age by graying hair while another may age by receding hairline. Following this biological principle, in this work, we propose an effective and efficient method to simulate natural aging. Specifically, a personalized aging basis is established for each individual to depict his/her own aging factors. Then different ages share this basis, being derived through age-specific transforms. The age-specific transforms represent the aging trends which are shared among all individuals. The proposed method can achieve continuous face aging with favorable aging accuracy, identity preservation, and fidelity. Furthermore, befitted from the effective design, a unique model is capable of all ages and the prediction time is significantly saved.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_S2GAN_Share_Aging_Factors_Across_Ages_and_Share_Aging_Trends_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_S2GAN_Share_Aging_Factors_Across_Ages_and_Share_Aging_Trends_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009536/,"['Aging', 'Face', 'Transforms', 'Generative adversarial networks', 'Gallium nitride', 'Market research', 'Prototypes']","['Age Trends', 'Aging Process', 'Wrinkles', 'Prediction Time', 'Natural Aging', 'Graying', 'Facial Age', 'Identity Preservation', 'Age Groups', 'Personality', 'Prototype', 'Decoding', 'Face Recognition', 'Generative Adversarial Networks', 'Deep Learning Approaches', 'Face Images', 'Residual Block', 'Low Computational Cost', 'Adversarial Training', 'Target Age', 'Personal Basis', 'Transition Patterns', 'Target Age Group', 'Simple Distribution', 'Nasolabial Fold', 'Computational Consumption', 'Average Face', 'Discrete Groups', 'Natural Aging Process', 'New Perspective']",,36,"Generally, we human follow the roughly common aging trends, e.g., the wrinkles only tend to be more, longer or deeper. However, the aging process of each individual is more dominated by his/her personalized factors, including the invariant factors such as identity and mole, as well as the personalized aging patterns, e.g., one may age by graying hair while another may age by receding hairline. Following this biological principle, in this work, we propose an effective and efficient method to simulate natural aging. Specifically, a personalized aging basis is established for each individual to depict his/her own aging factors. Then different ages share this basis, being derived through age-specific transforms. The age-specific transforms represent the aging trends which are shared among all individuals. The proposed method can achieve continuous face aging with favorable aging accuracy, identity preservation, and fidelity. Furthermore, befitted from the effective design, a unique model is capable of all ages and the prediction time is significantly saved."
S4L: Self-Supervised Semi-Supervised Learning,"Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer","Google Research, Brain Team",0.0,,100.0,USA,"This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_S4L_Self-Supervised_Semi-Supervised_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_S4L_Self-Supervised_Semi-Supervised_Learning_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010283/,"['Semisupervised learning', 'Visualization', 'Task analysis', 'Standards', 'Training', 'Computer vision', 'Benchmark testing']","['Semi-supervised Learning', 'Self-supervised Learning', 'Visual Representation', 'Representation Learning', 'Semi-supervised Methods', 'Training Set', 'Learning Rate', 'Hyperparameters', 'Deep Neural Network', 'Validation Set', 'Computer Vision', 'Image Dataset', 'Cross-entropy Loss', 'Weight Decay', 'Stochastic Gradient Descent', 'Semantic Segmentation', 'Representation Of Space', 'Unlabeled Data', 'Pseudo Labels', 'Semi-supervised Model', 'Top-5 Accuracy', 'Pretext Task', 'Semi-supervised Learning Algorithm', 'Image Instance', 'Unlabeled Examples', 'Image Recognition', 'Minimum Entropy', 'Model Architecture', 'Visual Classification', 'Public Settings']",,396,"This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels."
SANet: Scene Agnostic Network for Camera Localization,"Luwei Yang, Ziqian Bai, Chengzhou Tang, Honghua Li, Yasutaka Furukawa, Ping Tan",Alibaba A.I Labs; Simon Fraser University,50.0,canada,50.0,China,"This paper presents a scene agnostic neural architecture for camera localization, where model parameters and scenes are independent from each other.Despite recent advancement in learning based methods, most approaches require training for each scene one by one, not applicable for online applications such as SLAM and robotic navigation, where a model must be built on-the-fly.Our approach learns to build a hierarchical scene representation and predicts a dense scene coordinate map of a query RGB image on-the-fly given an arbitrary scene. The 6D camera pose of the query image can be estimated with the predicted scene coordinate map. Additionally, the dense prediction can be used for other online robotic and AR applications such as obstacle avoidance. We demonstrate the effectiveness and efficiency of our method on both indoor and outdoor benchmarks, achieving state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SANet_Scene_Agnostic_Network_for_Camera_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SANet_Scene_Agnostic_Network_for_Camera_Localization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009066/,"['Cameras', 'Three-dimensional displays', 'Feature extraction', 'Image resolution', 'Pipelines', 'Robot vision systems']","['Camera Localization', 'Density Map', 'RGB Images', 'Obstacle Avoidance', 'Camera Pose', 'Hierarchical Representation', 'Robot Navigation', 'Query Image', 'Scene Representation', 'Dense Prediction', 'Convolutional Neural Network', 'Random Forest', 'Feature Maps', 'Localization Accuracy', 'Point Cloud', 'Red Dots', '3D Space', 'Blue Dots', 'Handcrafted Features', 'Scene Images', 'Scene Point', 'Outdoor Scenes', 'Camera Pose Estimation', 'Pose Estimation', '3D Scene', 'Error Threshold', '3D Point Cloud', '3D Point', 'Feature Matching', 'Specific Scene']",,59,"This paper presents a scene agnostic neural architecture for camera localization, where model parameters and scenes are independent from each other.Despite recent advancement in learning based methods, most approaches require training for each scene one by one, not applicable for online applications such as SLAM and robotic navigation, where a model must be built on-the-fly.Our approach learns to build a hierarchical scene representation and predicts a dense scene coordinate map of a query RGB image on-the-fly given an arbitrary scene. The 6D camera pose of the query image can be estimated with the predicted scene coordinate map. Additionally, the dense prediction can be used for other online robotic and AR applications such as obstacle avoidance. We demonstrate the effectiveness and efficiency of our method on both indoor and outdoor benchmarks, achieving state-of-the-art performance."
SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification,"Yan Huang, Qiang Wu, JingSong Xu, Yi Zhong","School of Electrical and Data Engineering, University of Technology Sydney, Australia; School of Information and Electronics, Beijing Institute of Technology, China",100.0,"China, australia",0.0,,"Cross-domain person re-identification (re-ID) is challenging due to the bias between training and testing domains. We observe that if backgrounds in the training and testing datasets are very different, it dramatically introduces difficulties to extract robust pedestrian features, and thus compromises the cross-domain person re-ID performance. In this paper, we formulate such problems as a background shift problem. A Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to generate images with suppressed backgrounds. Unlike simply removing backgrounds using binary masks, SBSGAN allows the generator to decide whether pixels should be preserved or suppressed to reduce segmentation errors caused by noisy foreground masks. Additionally, we take ID-related cues, such as vehicles and companions into consideration. With high-quality generated images, a Densely Associated 2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection (ISDC) modules to strengthen the complementarity of the generated data and ID-related cues. The experiments show that the proposed method achieves competitive performance on three re-ID datasets, i.e., Market-1501, DukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_SBSGAN_Suppression_of_Inter-Domain_Background_Shift_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_SBSGAN_Suppression_of_Inter-Domain_Background_Shift_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010990/,"['Training', 'Streaming media', 'Task analysis', 'Image color analysis', 'Testing', 'Generative adversarial networks', 'Gallium nitride']","['Generative Adversarial Networks', 'Segmentation Errors', 'Training Domain', 'Loss Function', 'Objective Function', 'Input Image', 'Quantitative Evaluation', 'Data Generation', 'Training Images', 'Image Generation', 'Residual Block', 'Target Domain', 'Backbone Network', 'Source Domain', 'Reconstruction Loss', 'Person Image', 'Dense Block', 'Query Image', 'Style Transfer', 'Style Image', 'Element-wise Summation', 'Fake Images', 'Individual Streams', 'Convolutional Layers', 'Style Changes', 'Fully-connected Layer', 'Types Of Images']",,83,"Cross-domain person re-identification (re-ID) is challenging due to the bias between training and testing domains. We observe that if backgrounds in the training and testing datasets are very different, it dramatically introduces difficulties to extract robust pedestrian features, and thus compromises the cross-domain person re-ID performance. In this paper, we formulate such problems as a background shift problem. A Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to generate images with suppressed backgrounds. Unlike simply removing backgrounds using binary masks, SBSGAN allows the generator to decide whether pixels should be preserved or suppressed to reduce segmentation errors caused by noisy foreground masks. Additionally, we take ID-related cues, such as vehicles and companions into consideration. With high-quality generated images, a Densely Associated 2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection (ISDC) modules to strengthen the complementarity of the generated data and ID-related cues. The experiments show that the proposed method achieves competitive performance on three re-ID datasets, i.e., Market-1501, DukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario."
SC-FEGAN: Face Editing Generative Adversarial Network With Userâs Sketch and Color,"Youngjoo Jo, Jongyoul Park","ETRI, South Korea",0.0,,100.0,South Korea,"We present a novel image editing system that generates images as the user provides free-form masks, sketches and color as inputs. Our system consists of an end-to-end trainable convolutional network. In contrast to the existing methods, our system utilizes entirely free-form user input in terms of color and shape. This allows the system to respond to the user's sketch and color inputs, using them as guidelines to generate an image. In this work, we trained the network with an additional style loss, which made it possible to generate realistic results despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited for generating high-quality synthetic images using intuitive user inputs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jo_SC-FEGAN_Face_Editing_Generative_Adversarial_Network_With_Users_Sketch_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jo_SC-FEGAN_Face_Editing_Generative_Adversarial_Network_With_Users_Sketch_and_ICCV_2019_paper.pdf,,,,main,Poster,,,,,,
"SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects","Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Xian Sun, Kun Fu","University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University; NIST, Institute of Electronics, Chinese Academy of Sciences, Beijing (Suzhou), China",100.0,"China, china",0.0,,"Object detection has been a building block in computer vision. Though considerable progress has been made, there still exist challenges for objects with small size, arbitrary direction, and dense distribution. Apart from natural images, such issues are especially pronounced for aerial images of great importance. This paper presents a novel multi-category rotation detector for small, cluttered and rotated objects, namely SCRDet. Specifically, a sampling fusion network is devised which fuses multi-layer feature with effective anchor sampling, to improve the sensitivity to small objects. Meanwhile, the supervised pixel attention network and the channel attention network are jointly explored for small and cluttered object detection by suppressing the noise and highlighting the objects feature. For more accurate rotation estimation, the IoU constant factor is added to the smooth L1 loss to address the boundary problem for the rotating bounding box. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as natural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the state-of-the-art performance of our detector. The code and models will be available at https://github.com/DetectionTeamUCAS.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf,,https://github.com/DetectionTeamUCAS,,main,Poster,https://ieeexplore.ieee.org/document/9008772/,"['Detectors', 'Feature extraction', 'Object detection', 'Remote sensing', 'Training', 'Convolution', 'Semantics']","['Object Rotation', 'Object Detection', 'Intersection Over Union', 'Bounding Box', 'Natural Images', 'Aerial Images', 'Attention Network', 'Small Objects', 'Fusion Network', 'Boundary Problem', 'Arbitrary Direction', 'Feature Maps', 'Softmax Function', 'Insufficient Sample', 'Feature Fusion', 'Faster R-CNN', 'Optical Character Recognition', 'Saliency Map', 'Large Aspect Ratio', 'Feature Pyramid Network', 'Small Object Detection', 'Low-level Feature Maps', 'Dense Objects', 'Arbitrary Orientation', 'Arbitrary Rotation', 'Image Pyramid', 'Feature Fusion Network', 'High-level Feature Maps', 'Multidimensional Network', 'Dense Arrangement']",,598,"Object detection has been a building block in computer vision. Though considerable progress has been made, there still exist challenges for objects with small size, arbitrary direction, and dense distribution. Apart from natural images, such issues are especially pronounced for aerial images of great importance. This paper presents a novel multi-category rotation detector for small, cluttered and rotated objects, namely SCRDet. Specifically, a sampling fusion network is devised which fuses multi-layer feature with effective anchor sampling, to improve the sensitivity to small objects. Meanwhile, the supervised pixel attention network and the channel attention network are jointly explored for small and cluttered object detection by suppressing the noise and highlighting the objects feature. For more accurate rotation estimation, the IoU constant factor is added to the smooth L1 loss to address the boundary problem for the rotating bounding box. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as natural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the state-of-the-art performance of our detector. The code and models will be available at https://github.com/DetectionTeamUCAS."
SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition,"Bruno Korbar, Du Tran, Lorenzo Torresani",Facebook AI,0.0,,100.0,USA,"While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive. Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight ""clip-sampling"" model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009073/,"['Computational modeling', 'Training', 'Proposals', 'Computational efficiency', 'Face recognition', 'Runtime', 'Task analysis']","['Action Recognition', 'Efficient Action Recognition', 'Computational Cost', 'Recognition Accuracy', 'Action Classes', 'Training Set', 'Typical Features', 'Random Sampling', 'Video Clips', 'Video Analysis', 'Design Requirements', 'Convex Combination', 'Training Videos', 'Adjacent Frames', 'Action Labels', 'Gain In Accuracy', 'Test Videos', 'Entire Video', 'Dynamic Datasets', 'Ranking Loss', 'Video Compression', 'Saliency Models', 'Audio Segments', 'Action Recognition Model', 'Number Of Clips', 'Critical Requirement']",,140,"While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive. Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight ``clip-sampling'' model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost."
SENSE: A Shared Encoder Network for Scene-Flow Estimation,"Huaizu Jiang, Deqing Sun, Varun Jampani, Zhaoyang Lv, Erik Learned-Miller, Jan Kautz",Georgia Tech; UMass Amherst; NVIDIA,66.66666666666666,usa,33.33333333333334,USA,"We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_SENSE_A_Shared_Encoder_Network_for_Scene-Flow_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_SENSE_A_Shared_Encoder_Network_for_Scene-Flow_Estimation_ICCV_2019_paper.pdf,http://jianghz.me/projects/sense,,,main,Oral,https://ieeexplore.ieee.org/document/9010018/,"['Task analysis', 'Optical imaging', 'Estimation', 'Optical sensors', 'Semantics', 'Motion segmentation', 'Decoding']","['Estimation Network', 'Shared Encoder', 'Scene Flow Estimation', 'State Of The Art', 'Feature Representation', 'Real-world Data', 'Semantic Segmentation', 'Optical Flow', 'Inference Time', 'Flow Estimation', 'Modular Design', 'Compact Model', 'Optical Flow Estimation', 'Disparity Estimation', 'Convolutional Neural Network', 'Feature Maps', 'Differentiable Function', 'Network Training', 'Convolutional Neural Network Model', 'Loss Term', 'Distillation Loss', 'Left Image', 'Ground Truth Annotations', '3D Motion', 'Pyramid Level', 'Cost Volume', 'Learning Rate Schedule', 'Stereo Images', 'Optical Flow Method', 'CNN-based Approaches']",,56,"We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory."
SID4VAM: A Benchmark Dataset With Synthetic Images for Visual Attention Modeling,"David Berga, XosÃ© R. Fdez-Vidal, Xavier Otazu, XosÃ© M. Pardo","Computer Vision Center, Universitat Aut `onoma de Barcelona, Spain; CiTIUS, Universidade de Santiago de Compostela, Spain",100.0,"Spain, spain",0.0,,"A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Berga_SID4VAM_A_Benchmark_Dataset_With_Synthetic_Images_for_Visual_Attention_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Berga_SID4VAM_A_Benchmark_Dataset_With_Synthetic_Images_for_Visual_Attention_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008799/,"['Visualization', 'Task analysis', 'Computational modeling', 'Measurement', 'Gaze tracking', 'Predictive models', 'Benchmark testing']","['Visual Attention', 'Synthetic Images', 'Model Performance', 'Deep Learning', 'Typical Pattern', 'Deep Models', 'Eye-tracking', 'Low-level Features', 'Visual Search', 'Psychophysics', 'Visual Search Task', 'Salient Regions', 'Saliency Models', 'Computational Model', 'Eye Movements', 'Feature Maps', 'Human Performance', 'Deep Learning Models', 'Object Detection', 'Parametrized', 'Eye-tracking Data', 'Saliency Map', 'Human Visual System', 'High-level Features', 'Center Bias', 'High Contrast', 'Bottom-up Attention', 'Salient Object', 'Real Scenes', 'Fixation Location']",,6,"A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets."
"SILCO: Show a Few Images, Localize the Common Object","Tao Hu, Pascal Mettes, Jia-Hong Huang, Cees G. M. Snoek",University of Amsterdam,100.0,Netherlands,0.0,,"Few-shot learning is a nascent research topic, motivated by the fact that traditional deep learning requires tremendous amounts of data. In this work, we propose a new task along this research direction, we call few-shot common-localization. Given a few weakly-supervised support images, we aim to localize the common object in the query image without any box annotation. This task differs from standard few-shot settings, since we aim to address the localization problem, rather than the global classification problem. To tackle this new problem, we propose a network that aims to get the most out of the support and query images. To that end, we introduce a spatial similarity module that searches the spatial commonality among the given images. We furthermore introduce a feature reweighting module to balance the influence of different support images through graph convolutional networks. To evaluate few-shot common-localization, we repurpose and reorganize the well-known Pascal VOC and MS-COCO datasets, as well as a video dataset from ImageNet VID. Experiments on the new settings for few-shot common-localization shows the importance of searching for spatial similarity and feature reweighting, outperforming baselines from related tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_SILCO_Show_a_Few_Images_Localize_the_Common_Object_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_SILCO_Show_a_Few_Images_Localize_the_Common_Object_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9011006/,"['Task analysis', 'Silicon', 'Object detection', 'Detectors', 'Training', 'Shape', 'Machine learning']","['Common Objects', 'Spatial Features', 'Image Object', 'Spatial Module', 'Graph Convolutional Network', 'Spatial Similarity', 'Query Image', 'Few-shot Learning', 'PASCAL VOC', 'Box Annotations', 'Deep Network', 'Validation Set', 'Active Learning', 'Object Detection', 'Bounding Box', 'Common Categories', 'Video Frames', 'Global Average Pooling', 'Center Of Box', 'Query Features', 'Single Shot Detector', 'Region Proposal Network', 'Global Similarity', 'Siamese Network', 'Disjoint Groups', 'Two-stage Detectors', 'Sigmoid Layer']",,28,"Few-shot learning is a nascent research topic, motivated by the fact that traditional deep learning requires tremendous amounts of data. In this work, we propose a new task along this research direction, we call few-shot common-localization. Given a few weakly-supervised support images, we aim to localize the common object in the query image without any box annotation. This task differs from standard few-shot settings, since we aim to address the localization problem, rather than the global classification problem. To tackle this new problem, we propose a network that aims to get the most out of the support and query images. To that end, we introduce a spatial similarity module that searches the spatial commonality among the given images. We furthermore introduce a feature reweighting module to balance the influence of different support images through graph convolutional networks. To evaluate few-shot common-localization, we repurpose and reorganize the well-known Pascal VOC and MS-COCO datasets, as well as a video dataset from ImageNet VID. Experiments on the new settings for few-shot common-localization shows the importance of searching for spatial similarity and feature reweighting, outperforming baselines from related tasks."
SME-Net: Sparse Motion Estimation for Parametric Video Prediction Through Reinforcement Learning,"Yung-Han Ho, Chuan-Yuan Cho, Wen-Hsiao Peng, Guo-Lun Jin","Department of Computer Science, National Chiao Tung University, Taiwan",100.0,taiwan,0.0,,"This paper leverages a classic prediction technique, known as parametric overlapped block motion compensation (POBMC), in a reinforcement learning framework for video prediction. Learning-based prediction methods with explicit motion models often suffer from having to estimate large numbers of motion parameters with artificial regularization. Inspired by the success of sparse motion-based prediction for video compression, we propose a parametric video prediction on a sparse motion field composed of few critical pixels and their motion vectors. The prediction is achieved by gradually refining the estimate of a future frame in iterative, discrete steps. Along the way, the identification of critical pixels and their motion estimation are addressed by two neural networks trained under a reinforcement learning setting. Our model achieves the state-of-the-art performance on CaltchPed, UCF101 and CIF datasets in one-step and multi-step prediction tests. It shows good generalization results and is able to learn well on small training data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ho_SME-Net_Sparse_Motion_Estimation_for_Parametric_Video_Prediction_Through_Reinforcement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ho_SME-Net_Sparse_Motion_Estimation_for_Parametric_Video_Prediction_Through_Reinforcement_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010948/,"['Predictive models', 'Kernel', 'Motion estimation', 'Task analysis', 'Dynamics', 'Convolution', 'Learning (artificial intelligence)']","['Motion Estimation', 'Video Prediction', 'Sparse Motion', 'Learning Framework', 'Classification Techniques', 'Small Data', 'Motion Model', 'Prediction Framework', 'Prediction Techniques', 'Motion Vector', 'Motion Compensation', 'Reinforcement Learning Framework', 'Motion Field', 'Video Compression', 'Future Frames', 'One-step Prediction', 'Multi-step Prediction', 'Small Training Data', 'Mean Square Error', 'Convolutional Neural Network', 'Contextual Framing', 'Past Frames', 'Peak Signal-to-noise Ratio', 'Large Motion', 'Objective Quality', 'Kernel-based Methods', 'Optical Flow', 'Convolution Kernel', 'Long Short-term Memory', 'Subjective Quality']",,7,"This paper leverages a classic prediction technique, known as parametric overlapped block motion compensation (POBMC), in a reinforcement learning framework for video prediction. Learning-based prediction methods with explicit motion models often suffer from having to estimate large numbers of motion parameters with artificial regularization. Inspired by the success of sparse motion-based prediction for video compression, we propose a parametric video prediction on a sparse motion field composed of few critical pixels and their motion vectors. The prediction is achieved by gradually refining the estimate of a future frame in iterative, discrete steps. Along the way, the identification of critical pixels and their motion estimation are addressed by two neural networks trained under a reinforcement learning setting. Our model achieves the state-of-the-art performance on CaltchPed, UCF101 and CIF datasets in one-step and multi-step prediction tests. It shows good generalization results and is able to learn well on small training data."
SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation With Semi-Supervised Learning,"Yujin Chen, Zhigang Tu, Liuhao Ge, Dejun Zhang, Ruizhi Chen, Junsong Yuan","China University of Geosciences, Wuhan, China; State University of New York at Buffalo, Buffalo, NY, USA; Wuhan University, Wuhan, China; Nanyang Technological University, Singapore",100.0,"China, Singapore, china, usa",0.0,,"3D hand pose estimation has made significant progress recently, where Convolutional Neural Networks (CNNs) play a critical role. However, most of the existing CNN-based hand pose estimation methods depend much on the training set, while labeling 3D hand pose on training data is laborious and time-consuming. Inspired by the point cloud autoencoder presented in self-organizing network (SO-Net), our proposed SO-HandNet aims at making use of the unannotated data to obtain accurate 3D hand pose estimation in a semi-supervised manner. We exploit hand feature encoder (HFE) to extract multi-level features from hand point cloud and then fuse them to regress 3D hand pose by a hand pose estimator (HPE). We design a hand feature decoder (HFD) to recover the input point cloud from the encoded feature. Since the HFE and the HFD can be trained without 3D hand pose annotation, the proposed method is able to make the best of unannotated data during the training phase. Experiments on four challenging benchmark datasets validate that our proposed SO-HandNet can achieve superior performance for 3D hand pose estimation via semi-supervised learning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_SO-HandNet_Self-Organizing_Network_for_3D_Hand_Pose_Estimation_With_Semi-Supervised_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_SO-HandNet_Self-Organizing_Network_for_3D_Hand_Pose_Estimation_With_Semi-Supervised_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008521/,"['Three-dimensional displays', 'Pose estimation', 'Feature extraction', 'Self-organizing feature maps', 'Training', 'Solid modeling', 'Decoding']","['Pose Estimation', 'Semi-supervised Learning', 'Self-organizing Network', 'Pose Estimation Network', 'Hand Pose', 'Hand Pose Estimation', '3D Hand Pose Estimation', 'Training Set', 'Convolutional Neural Network', 'Training Phase', 'Point Cloud', 'Feature Encoder', 'Input Point', 'Multi-level Features', 'Human Pose Estimation', '3D Pose', 'Input Point Cloud', 'Deep Network', 'Deep Neural Network', 'K-nearest Neighbor', 'Node Features', 'Original Point Cloud', 'Global Features', 'Annotation Data', 'Self-organizing Map', 'Depth Images', 'Unlabeled Data', 'Nearest Neighbor Search', 'Feature Points', 'Hierarchical Feature Extraction']",,60,"3D hand pose estimation has made significant progress recently, where Convolutional Neural Networks (CNNs) play a critical role. However, most of the existing CNN-based hand pose estimation methods depend much on the training set, while labeling 3D hand pose on training data is laborious and time-consuming. Inspired by the point cloud autoencoder presented in self-organizing network (SO-Net), our proposed SO-HandNet aims at making use of the unannotated data to obtain accurate 3D hand pose estimation in a semi-supervised manner. We exploit hand feature encoder (HFE) to extract multi-level features from hand point cloud and then fuse them to regress 3D hand pose by a hand pose estimator (HPE). We design a hand feature decoder (HFD) to recover the input point cloud from the encoded feature. Since the HFE and the HFD can be trained without 3D hand pose annotation, the proposed method is able to make the best of unannotated data during the training phase. Experiments on four challenging benchmark datasets validate that our proposed SO-HandNet can achieve superior performance for 3D hand pose estimation via semi-supervised learning."
SPGNet: Semantic Prediction Guidance for Scene Parsing,"Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun Zhu, Zilong Huang, Jinjun Xiong, Thomas S. Huang, Wen-Mei Hwu, Honghui Shi","; IBM Research, UIUC, University of Oregon; UIUC; IBM Research; UIUC, ReLER, UTS",75.0,"australia, usa",25.0,USA,"Multi-scale context module and single-stage encoder-decoder structure are commonly employed for semantic segmentation. The multi-scale context module refers to the operations to aggregate feature responses from a large spatial extent, while the single-stage encoder-decoder structure encodes the high-level semantic information in the encoder path and recovers the boundary information in the decoder path. In contrast, multi-stage encoder-decoder networks have been widely used in human pose estimation and show superior performance than their single-stage counterpart. However, few efforts have been attempted to bring this effective design to semantic segmentation. In this work, we propose a Semantic Prediction Guidance (SPG) module which learns to re-weight the local features through the guidance from pixel-wise semantic prediction. We find that by carefully re-weighting features across stages, a two-stage encoder-decoder network coupled with our proposed SPG module can significantly outperform its one-stage counterpart with similar parameters and computations. Finally, we report experimental results on the semantic segmentation benchmark Cityscapes, in which our SPGNet attains 81.1% on the test set using only 'fine' annotations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_SPGNet_Semantic_Prediction_Guidance_for_Scene_Parsing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_SPGNet_Semantic_Prediction_Guidance_for_Scene_Parsing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008568/,"['Semantics', 'Decoding', 'Task analysis', 'Periodic structures', 'Pose estimation', 'Computational modeling', 'Feature extraction']","['Semantic Prediction', 'Local Features', 'Semantic Segmentation', 'Pose Estimation', 'Similarity Calculation', 'Encoder-decoder Network', 'Encoder-decoder Structure', 'Human Pose Estimation', 'Boundary Information', 'Large Spatial Extent', 'Decoding Path', 'Feature Maps', 'Deep Convolutional Neural Network', 'Classification Datasets', 'Attention Module', 'Previous Stage', 'Residual Block', 'Backbone Network', 'Identity Mapping', 'Decoder Features', 'Decoder Output', 'Output Stage', 'Encoder Module', 'Output Of The Previous Layer', 'Hard Examples', 'Simple Convolution', 'Atrous Convolution', 'Grid Scale', 'Feature Pyramid Network']",,98,"Multi-scale context module and single-stage encoder-decoder structure are commonly employed for semantic segmentation. The multi-scale context module refers to the operations to aggregate feature responses from a large spatial extent, while the single-stage encoder-decoder structure encodes the high-level semantic information in the encoder path and recovers the boundary information in the decoder path. In contrast, multi-stage encoder-decoder networks have been widely used in human pose estimation and show superior performance than their single-stage counterpart. However, few efforts have been attempted to bring this effective design to semantic segmentation. In this work, we propose a Semantic Prediction Guidance (SPG) module which learns to re-weight the local features through the guidance from pixel-wise semantic prediction. We find that by carefully re-weighting features across stages, a two-stage encoder-decoder network coupled with our proposed SPG module can significantly outperform its one-stage counterpart with similar parameters and computations. Finally, we report experimental results on the semantic segmentation benchmark Cityscapes, in which our SPGNet attains 81.1% on the test set using only 'fine' annotations."
SPLINE-Net: Sparse Photometric Stereo Through Lighting Interpolation and Normal Estimation Networks,"Qian Zheng, Yiming Jia, Boxin Shi, Xudong Jiang, Ling-Yu Duan, Alex C. Kot","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; National Engineering Laboratory for Video Technology, Department of CS, Peking University, Beijing, China; Department of Precision Instrument, Tsinghua University, Beijing, China",100.0,"China, Singapore, china",0.0,,This paper solves the Sparse Photometric stereo through Lighting Interpolation and Normal Estimation using a generative Network (SPLINE-Net). SPLINE-Net contains a lighting interpolation network to generate dense lighting observations given a sparse set of lights as inputs followed by a normal estimation network to estimate surface normals. Both networks are jointly constrained by the proposed symmetric and asymmetric loss functions to enforce isotropic constrain and perform outlier rejection of global illumination effects. SPLINE-Net is verified to outperform existing methods for photometric stereo of general BRDFs by using only ten images of different lights instead of using nearly one hundred images.,,http://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_SPLINE-Net_Sparse_Photometric_Stereo_Through_Lighting_Interpolation_and_Normal_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_SPLINE-Net_Sparse_Photometric_Stereo_Through_Lighting_Interpolation_and_Normal_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009024/,"['Lighting', 'Estimation', 'Splines (mathematics)', 'Interpolation', 'Analytical models', 'Mathematical model', 'Surface reconstruction']","['Normal Approximation', 'Photometric Stereo', 'Interpolation Approximation', 'Global Effect', 'Asymmetric Loss', 'Surface Normals', 'Outlier Rejection', 'Symmetric Loss', 'Model Analysis', 'Neural Network', 'Training Data', 'Regression Equation', 'Quantitative Results', 'Empirical Model', 'General Properties', 'Visual Comparison', 'Image Formation', 'Direct Light', 'Reconstruction Loss', 'Surface Diffusion', 'Cast Shadows', 'Observation Density', 'Symmetric Pattern', 'Angular Error', 'Irradiance Values', 'Number Of Lights', 'Image Formation Process', 'Symmetric Function']",,50,This paper solves the Sparse Photometric stereo through Lighting Interpolation and Normal Estimation using a generative Network (SPLINE-Net). SPLINE-Net contains a lighting interpolation network to generate dense lighting observations given a sparse set of lights as inputs followed by a normal estimation network to estimate surface normals. Both networks are jointly constrained by the proposed symmetric and asymmetric loss functions to enforce isotropic constrain and perform outlier rejection of global illumination effects. SPLINE-Net is verified to outperform existing methods for photometric stereo of general BRDFs by using only ten images of different lights instead of using nearly one hundred images.
SRM: A Style-Based Recalibration Module for Convolutional Neural Networks,"HyunJae Lee, Hyo-Eun Kim, Hyeonseob Nam",Lunit Inc.,0.0,,100.0,South Korea,"Following the advance of style transfer with Convolutional Neural Networks (CNNs), the role of styles in CNNs has drawn growing attention from a broader perspective. In this paper, we aim to fully leverage the potential of styles to improve the performance of CNNs in general vision tasks. We propose a Style-based Recalibration Module (SRM), a simple yet effective architectural unit, which adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM effectively enhances the representational ability of a CNN. The proposed module is directly fed into existing CNN architectures with negligible overhead. We conduct comprehensive experiments on general image recognition as well as tasks related to styles, which verify the benefit of SRM over recent approaches such as Squeeze-and-Excitation (SE). To explain the inherent difference between SRM and SE, we provide an in-depth comparison of their representational properties.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_SRM_A_Style-Based_Recalibration_Module_for_Convolutional_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_SRM_A_Style-Based_Recalibration_Module_for_Convolutional_Neural_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008782/,"['Task analysis', 'Feature extraction', 'Standards', 'Decision making', 'Tensile stress', 'Convolution', 'Training']","['Convolutional Neural Network', 'Recalibration Module', 'Feature Maps', 'Recent Approaches', 'Convolutional Neural Network Architecture', 'Style Transfer', 'Intermediate Feature Maps', 'Input Image', 'Spatial Dimensions', 'Multilayer Perceptron', 'Batch Normalization', 'Max-pooling', 'Generative Adversarial Networks', 'Object Classification', 'Average Pooling', 'Residual Block', '5-fold Cross-validation', 'Validation Accuracy', 'Batch Normalization Layer', 'Global Average Pooling', 'Style Features', 'Standard Convolutional Neural Networks', 'Style Image', 'Texture Classification', 'Training Policy', 'Training Curves']",,162,"Following the advance of style transfer with Convolutional Neural Networks (CNNs), the role of styles in CNNs has drawn growing attention from a broader perspective. In this paper, we aim to fully leverage the potential of styles to improve the performance of CNNs in general vision tasks. We propose a Style-based Recalibration Module (SRM), a simple yet effective architectural unit, which adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM effectively enhances the representational ability of a CNN. The proposed module is directly fed into existing CNN architectures with negligible overhead. We conduct comprehensive experiments on general image recognition as well as tasks related to styles, which verify the benefit of SRM over recent approaches such as Squeeze-and-Excitation (SE). To explain the inherent difference between SRM and SE, we provide an in-depth comparison of their representational properties."
SROBB: Targeted Perceptual Loss for Single Image Super-Resolution,"Mohammad Saeed Rad, Behzad Bozorgtabar, Urs-Viktor Marti, Max Basler, Hazim Kemal Ekenel, Jean-Philippe Thiran","AI Lab, Swisscom AG, Switzerland; LTS5, EPFL, Switzerland; LTS5, EPFL, Switzerland; SiMiT Lab, ITU, Turkey",75.0,"switzerland, usa",25.0,Switzerland,"By benefiting from perceptual losses, recent studies have improved significantly the performance of the super-resolution task, where a high-resolution image is resolved from its low-resolution counterpart. Although such objective functions generate near-photorealistic results, their capability is limited, since they estimate the reconstruction error for an entire image in the same way, without considering any semantic information. In this paper, we propose a novel method to benefit from perceptual loss in a more objective way. We optimize a deep network-based decoder with a targeted objective function that penalizes images at different semantic levels using the corresponding terms. In particular, the proposed method leverages our proposed OBB (Object, Background and Boundary) labels, generated from segmentation labels, to estimate a suitable perceptual loss for boundaries, while considering texture similarity for backgrounds. We show that our proposed approach results in more realistic textures and sharper edges, and outperforms other state-of-the-art algorithms in terms of both qualitative results on standard benchmarks and results of extensive user studies.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rad_SROBB_Targeted_Perceptual_Loss_for_Single_Image_Super-Resolution_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rad_SROBB_Targeted_Perceptual_Loss_for_Single_Image_Super-Resolution_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010819/,"['Image edge detection', 'Image resolution', 'Semantics', 'Image reconstruction', 'Image segmentation', 'Linear programming', 'Training']","['Super-resolution', 'Perceptual Loss', 'Single Image Super-resolution', 'Objective Function', 'High-resolution Images', 'Qualitative Results', 'User Study', 'Semantic Information', 'Sharp Edges', 'Objective Way', 'Segmentation Labels', 'Loss Function', 'Convolutional Neural Network', 'Deep Network', 'Convolutional Layers', 'Feature Space', 'Feature Maps', 'Image Regions', 'Receptive Field', 'High-level Features', 'Mean Square Error Loss', 'Perceptual Similarity', 'Perceptual Performance', 'Photo-realistic Images', 'Term Weight', 'Loss Term', 'Residual Block', 'Convolutional Neural Network Layers', 'Distance In Feature Space', 'Layers Of VGG16']",,102,"By benefiting from perceptual losses, recent studies have improved significantly the performance of the superresolution task, where a high-resolution image is resolved from its low-resolution counterpart. Although such objective functions generate near-photorealistic results, their capability is limited, since they estimate the reconstruction error for an entire image in the same way, without considering any semantic information. In this paper, we propose a novel method to benefit from perceptual loss in a more objective way. We optimize a deep network-based decoder with a targeted objective function that penalizes images at different semantic levels using the corresponding terms. In particular, the proposed method leverages our proposed OBB (Object, Background and Boundary) labels, generated from segmentation labels, to estimate a suitable perceptual loss for boundaries, while considering texture similarity for backgrounds. We show that our proposed approach results in more realistic textures and sharper edges, and outperforms other state-of-the-art algorithms in terms of both qualitative results on standard benchmarks and results of extensive user studies."
SSAP: Single-Shot Instance Segmentation With Affinity Pyramid,"Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu, Ming Yang, Kaiqi Huang","CRISE, Institute of Automation, Chinese Academy of Sciences and University of Chinese Academy of Sciences and CAS Center for Excellence in Brain Science and Intelligence Technology; CRISE, Institute of Automation, Chinese Academy of Sciences and University of Chinese Academy of Sciences; Horizon Robotics, Inc",66.66666666666666,china,33.33333333333334,China,"Recently, proposal-free instance segmentation has received increasing attention due to its concise and efficient pipeline. Generally, proposal-free methods generate instance-agnostic semantic segmentation labels and instance-aware features to group pixels into different object instances. However, previous methods mostly employ separate modules for these two sub-tasks and require multiple passes for inference. We argue that treating these two sub-tasks separately is suboptimal. In fact, employing multiple separate modules significantly reduces the potential for application. The mutual benefits between the two complementary sub-tasks are also unexplored. To this end, this work proposes a single-shot proposal-free instance segmentation method that requires only one single pass for prediction. Our method is based on a pixel-pair affinity pyramid, which computes the probability that two pixels belong to the same instance in a hierarchical manner. The affinity pyramid can also be jointly learned with the semantic class labeling and achieve mutual benefits. Moreover, incorporating with the learned affinity pyramid, a novel cascaded graph partition module is presented to sequentially generate instances from coarse to fine. Unlike previous time-consuming graph partition methods, this module achieves 5x speedup and 9% relative improvement on Average-Precision (AP). Our approach achieves new state of the art on the challenging Cityscapes dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_SSAP_Single-Shot_Instance_Segmentation_With_Affinity_Pyramid_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_SSAP_Single-Shot_Instance_Segmentation_With_Affinity_Pyramid_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9056852/,"['Semantics', 'Image segmentation', 'Training', 'Proposals', 'Task analysis', 'Automation', 'Predictive models']","['Instance Segmentation', 'Affinity Pyramid', 'Semantic Segmentation', 'Training Examples', 'Semantic Labels', 'Graph Partitioning', 'COCO Dataset', 'Object Instances', 'Large Scale', 'Performance Of Method', 'Validation Set', 'Image Dataset', 'Bounding Box', 'Average Loss', 'Segmentation Results', 'Small Window', 'Mutual Benefit', 'Pixel Level', 'Agglomerative Clustering', 'Feature Pyramid', 'Joint Learning', 'Edge Score', 'Mask R-CNN', 'Semantic Segmentation Results', 'Annotation Quality', 'Segmentation Prediction', 'Affinity Prediction', 'Semantic Scores', 'Intersection Over Union', 'Small Scale']","['Instance segmentation', 'affinity pyramid', 'feature pyramid', 'single-shot', 'graph partition']",26,"Proposal-free instance segmentation methods mainly generate instance-agnostic semantic segmentation labels and instance-aware features to group pixels into different object instances. However, previous methods mostly employ separate modules for these two sub-tasks and require multiple passes for inference. In addition to the lack of efficiency, previous methods also failed to perform as well as proposal-based approaches. To this end, this work proposes a single-shot proposal-free instance segmentation method that requires only one single pass for prediction. Our method is based on learning an affinity pyramid, which computes the probability that two pixels belong to the same instance in a hierarchical manner. Moreover, incorporating with the learned affinity pyramid, a novel cascaded graph partition (CGP) module is presented to fuse the two predictions and segment instances efficiently. As an additional contribution, we conduct an experiment to demonstrate the benefits of proposal-free methods in capturing detailed structures from finely annotated training examples. Our approach is evaluated on the Cityscapes and COCO datasets and achieves state-of-the-art performance."
STD: Sparse-to-Dense 3D Object Detector for Point Cloud,"Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, Jiaya Jia","The Chinese University of Hong Kong; YouTu Lab, Tencent",100.0,"Hong Kong, china",0.0,,"We propose a two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point clouds as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a higher recall with less computation compared with prior works. Then, PointsPool is applied for proposal feature generation by transforming interior point features from sparse expression to compact representation, which saves even more computation. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method on 3D object and Bird's Eye View (BEV) detection. Our method outperforms other methods by a large margin, especially on the hard set, with 10+ FPS inference speed.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008777/,"['Three-dimensional displays', 'Proposals', 'Object detection', 'Semantics', 'Laser radar', 'Feature extraction', 'Training']","['Object Detection', 'Point Cloud', '3D Object Detection', 'Interior Point', 'High Recall', 'Compact Representation', 'KITTI Dataset', 'Two-stage Framework', 'Prediction Box', 'Proposal Generation', 'Hard Set', 'Receptive Field', 'Bounding Box', 'Final Prediction', 'Semantic Segmentation', 'Fully-connected Layer', 'Classification Score', 'Semantic Features', 'Inference Time', 'Prediction Loss', 'Ground-truth Box', '3D Detection', 'Non-maximum Suppression', '3D Segmentation', 'Proposal Features', 'Raw Point Cloud', 'Point-based Methods', 'Intersection Over Union Threshold', 'Canonical Transformation', 'Smoothness Loss']",,574,"We propose a two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point clouds as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a higher recall with less computation compared with prior works. Then, PointsPool is applied for proposal feature generation by transforming interior point features from sparse expression to compact representation, which saves even more computation. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method on 3D object and Bird’s Eye View (BEV) detection. Our method outperforms other methods by a large margin, especially on the hard set, with 10+ FPS inference speed."
STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction,"Yingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, Zhaoqi Wang","Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, CAS, Beijing 100190, China",100.0,China,0.0,,"Human trajectory prediction is challenging and critical in various applications (e.g., autonomous vehicles and social robots). Because of the continuity and foresight of the pedestrian movements, the moving pedestrians in crowded spaces will consider both spatial and temporal interactions to avoid future collisions. However, most of the existing methods ignore the temporal correlations of interactions with other pedestrians involved in a scene. In this work, we propose a Spatial-Temporal Graph Attention network (STGAT), based on a sequence-to-sequence architecture to predict future trajectories of pedestrians. Besides the spatial interactions captured by the graph attention mechanism at each time-step, we adopt an extra LSTM to encode the temporal correlations of interactions. Through comparisons with state-of-the-art methods, our model achieves superior performance on two publicly available crowd datasets (ETH and UCY) and produces more ""socially"" plausible trajectories for pedestrians.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010834/,"['Trajectory', 'Predictive models', 'Correlation', 'Task analysis', 'Neural networks', 'Computational modeling', 'Dynamics']","['Trajectory Prediction', 'Human Trajectory Prediction', 'Attention Mechanism', 'Autonomous Vehicles', 'Spatial Interaction', 'Future Trajectories', 'Social Robots', 'Graph Attention', 'Temporal Interactions', 'Graph Attention Network', 'Crowded Spaces', 'Pedestrian Trajectory', 'Neural Network', 'Complex Interactions', 'Long Short-term Memory', 'Recurrent Neural Network', 'Speech Recognition', 'Social Forces', 'Hidden State', 'Motion Patterns', 'Crowded Scenes', 'Graph Neural Networks', 'Learned Weights', 'Existence Of Interactions', 'Graph Convolutional Network', 'Human-human Interaction', 'Encoder Module', 'Seq2seq Model', 'Average Error Rate', 'Motion State']",,353,"Human trajectory prediction is challenging and critical in various applications (e.g., autonomous vehicles and social robots). Because of the continuity and foresight of the pedestrian movements, the moving pedestrians in crowded spaces will consider both spatial and temporal interactions to avoid future collisions. However, most of the existing methods ignore the temporal correlations of interactions with other pedestrians involved in a scene. In this work, we propose a Spatial-Temporal Graph Attention network (STGAT), based on a sequence-to-sequence architecture to predict future trajectories of pedestrians. Besides the spatial interactions captured by the graph attention mechanism at each time-step, we adopt an extra LSTM to encode the temporal correlations of interactions. Through comparisons with state-of-the-art methods, our model achieves superior performance on two publicly available crowd datasets (ETH and UCY) and produces more ""socially"" plausible trajectories for pedestrians."
STM: SpatioTemporal and Motion Encoding for Action Recognition,"Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, Junjie Yan",SenseTime Group Limited; Zhejiang University,100.0,"China, usa",0.0,,"Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose a STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010925/,"['Spatiotemporal phenomena', 'Two dimensional displays', 'Three-dimensional displays', 'Convolution', 'Feature extraction', 'Kernel', 'Computer architecture']","['Action Recognition', 'Computational Cost', 'Residual Block', 'Motion Features', 'Spatiotemporal Characteristics', 'ResNet Architecture', 'Original Block', 'Video Action Recognition', 'Time And Space', 'Stage 2', 'Feature Maps', 'Small Datasets', 'Temporal Features', 'Large-scale Datasets', 'Video Clips', 'Baseline Methods', 'Optical Flow', 'Temporal Model', 'Motion Information', 'CNN-based Methods', 'Temporal Convolution', '3D Convolution', 'Motor Representations', 'ImageNet Pre-trained Model', 'Video Dataset', 'Element-wise Summation', '3D Flow', 'Distinct Edges', 'Temporal Dimension', 'Contralateral']",,303,"Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose a STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together."
SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval,"Qing-Yuan Jiang, Yi He, Gen Li, Jian Lin, Lei Li, Wu-Jun Li","ByteDance AI Lab, Beijing, China; National Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, China",50.0,china,50.0,China,"With the explosive growth of video data in real applications, near-duplicate video retrieval (NDVR) has become indispensable and challenging, especially for short videos. However, all existing NDVR datasets are introduced for long videos. Furthermore, most of them are small-scale and lack of diversity due to the high cost of collecting and labeling near-duplicate videos. In this paper, we introduce a large-scale short video dataset, called SVD, for the NDVR task. SVD contains over 500,000 short videos and over 30,000 labeled videos of near-duplicates. We use multiple video mining techniques to construct positive/negative pairs. Furthermore, we design temporal and spatial transformations to mimic user-attack behavior in real applications for constructing more difficult variants of SVD. Experiments show that existing state-of-the-art NDVR methods, including real-value based and hashing based methods, fail to achieve satisfactory performance on this challenging dataset. The release of SVD dataset will foster research and system engineering in the NDVR area. The SVD dataset is available at https://svdbase.github.io.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_SVD_A_Large-Scale_Short_Video_Dataset_for_Near-Duplicate_Video_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_SVD_A_Large-Scale_Short_Video_Dataset_for_Near-Duplicate_Video_Retrieval_ICCV_2019_paper.pdf,https://svdbase.github.io,https://github.com/svdbase,,main,Poster,https://ieeexplore.ieee.org/document/9008275/,"['Task analysis', 'Muscles', 'YouTube', 'Explosives', 'Convolutional neural networks', 'Feature extraction']","['Large-scale Datasets', 'Short Video', 'Video Dataset', 'Real Applications', 'Lack Of Diversity', 'Challenging Dataset', 'Explosive Growth Of Data', 'Convolutional Neural Network', 'Deep Features', 'Storage Cost', 'Hamming Distance', 'Mean Average Precision', 'Metric Learning', 'Query Set', 'Retrieval Accuracy', 'Original Video', 'Television Screen', 'Number Of Videos', 'Deep Metric Learning', 'Locality Sensitive Hashing', 'Positive Candidates', 'Retrieval Procedure', 'Frame Features', 'YouTube']",,29,"With the explosive growth of video data in real applications, near-duplicate video retrieval (NDVR) has become indispensable and challenging, especially for short videos. However, all existing NDVR datasets are introduced for long videos. Furthermore, most of them are small-scale and lack of diversity due to the high cost of collecting and labeling near-duplicate videos. In this paper, we introduce a large-scale short video dataset, called SVD, for the NDVR task. SVD contains over 500,000 short videos and over 30,000 labeled videos of near-duplicates. We use multiple video mining techniques to construct positive/negative pairs. Furthermore, we design temporal and spatial transformations to mimic user-attack behavior in real applications for constructing more difficult variants of SVD. Experiments show that existing state-of-the-art NDVR methods, including real-value based and hashing based methods, fail to achieve satisfactory performance on this challenging dataset. The release of SVD dataset will foster research and system engineering in the NDVR area. The SVD dataset is available at https://svdbase.github.io."
Saliency-Guided Attention Network for Image-Sentence Matching,"Zhong Ji, Haoran Wang, Jungong Han, Yanwei Pang","WMG Data Science, University of Warwick, Coventry, UK; School of Electrical and Information Engineering, Tianjin University, Tianjin, China",100.0,"china, uk",0.0,,"This paper studies the task of matching image and sentence, where learning appropriate representations to bridge the semantic gap between image contents and language appears to be the main challenge. Unlike previous approaches that predominantly deploy symmetrical architecture to represent both modalities, we introduce a Saliency-guided Attention Network (SAN) that is characterized by building an asymmetrical link between vision and language to efficiently learn a fine-grained cross-modal correlation. The proposed SAN mainly includes three components: saliency detector, Saliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual Attention (STA) module. Concretely, the saliency detector provides the visual saliency information to drive both two attention modules. Taking advantage of the saliency information, SVA is able to learn more discriminative visual features. By fusing the visual information from SVA and intra-modal information as a multi-modal guidance, STA affords us powerful textual representations that are synchronized with visual clues. Extensive experiments demonstrate SAN can improve the state-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ji_Saliency-Guided_Attention_Network_for_Image-Sentence_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ji_Saliency-Guided_Attention_Network_for_Image-Sentence_Matching_ICCV_2019_paper.pdf,,https://github.com/HabbakukWang1103/SAN,,main,Poster,https://ieeexplore.ieee.org/document/9009576/,"['Visualization', 'Semantics', 'Saliency detection', 'Task analysis', 'Correlation', 'Hafnium', 'Computer architecture']","['Attention Network', 'Image-sentence Matching', 'Visual Information', 'Visual Features', 'Visual Attention', 'Attention Module', 'Salient Information', 'Text Representation', 'Saliency Detection', 'Visual Saliency', 'Visual Clues', 'MS COCO Dataset', 'Convolutional Neural Network', 'Convolutional Layers', 'Feature Maps', 'Long Short-term Memory', 'Global Features', 'High-level Features', 'Fully-connected Layer', 'Average Pooling', 'Image Retrieval', 'Low-level Features', 'Textual Features', 'Visual Modality', 'Image Encoder', 'Saliency Map', 'Salient Regions', 'Words In Sentences', 'Text Modality', 'Joint Space']",,75,"This paper studies the task of matching image and sentence, where learning appropriate representations to bridge the semantic gap between image contents and language appears to be the main challenge. Unlike previous approaches that predominantly deploy symmetrical architecture to represent both modalities, we introduce a Saliency-guided Attention Network (SAN) that is characterized by building an asymmetrical link between vision and language to efficiently learn a fine-grained cross-modal correlation. The proposed SAN mainly includes three components: saliency detector, Saliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual Attention (STA) module. Concretely, the saliency detector provides the visual saliency information to drive both two attention modules. Taking advantage of the saliency information, SVA is able to learn more discriminative visual features. By fusing the visual information from SVA and intra-modal information as a multi-modal guidance, STA affords us powerful textual representations that are synchronized with visual clues. Extensive experiments demonstrate SAN can improve the state-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a large margin."
Sampling Wisely: Deep Image Embedding by Top-K Precision Optimization,"Jing Lu, Chaofan Xu, Wei Zhang, Ling-Yu Duan, Tao Mei","JD AI Research, Harbin Institute of Technology; Peking University; Business Growth BU, JD; JD AI Research",50.0,china,50.0,China,"Deep image embedding aims at learning a convolutional neural network (CNN) based mapping function that maps an image to a feature vector. The embedding quality is usually evaluated by the performance in image search tasks. Since very few users bother to open the second page search results, top-k precision mostly dominates the user experience and thus is one of the crucial evaluation metrics for the embedding quality. Despite being extensively studied, existing algorithms are usually based on heuristic observation without theoretical guarantee. Consequently, gradient descent direction on the training loss is mostly inconsistent with the direction of optimizing the concerned evaluation metric. This inconsistency certainly misleads the training direction and degrades the performance. In contrast to existing works, in this paper, we propose a novel deep image embedding algorithm with end-to-end optimization to top-k precision, the evaluation metric that is closely related to user experience. Specially, our loss function is constructed with wisely selected ""misplaced"" images along the top k nearest neighbor decision boundary, so that the gradient descent update directly promotes the concerned metric, top-k precision. Further more, our theoretical analysis on the upper bounding and consistency properties of the proposed loss supports that minimizing our proposed loss is equivalent to maximizing top-k precision. Experiments show that our proposed algorithm outperforms all compared state-of-the-art deep image embedding algorithms on three benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lu_Sampling_Wisely_Deep_Image_Embedding_by_Top-K_Precision_Optimization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Sampling_Wisely_Deep_Image_Embedding_by_Top-K_Precision_Optimization_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010844/,"['Measurement', 'Training', 'Optimization', 'Task analysis', 'Visualization', 'Sampling methods', 'Toy manufacturing industry']","['Image Embedding', 'Deep Embedding', 'Top-k Precision', 'Loss Function', 'Upper Bound', 'Convolutional Neural Network', 'Gradient Descent', 'Theoretical Analysis', 'User Experience', 'Benchmark Datasets', 'Training Loss', 'Decision Boundary', 'Image Retrieval', 'Theoretical Guarantees', 'Embedding Algorithm', 'Sampling Method', 'Similarity Score', 'K-nearest Neighbor', 'Number Of Images', 'Class Labels', 'Triplet Loss', 'Positive Image', 'Visual Search Task', 'High Similarity Score', 'Positive Candidates', 'Negative Images', 'Ideal Solution', 'Training Images', 'Clustering Task', 'Large Margin']",,18,"Deep image embedding aims at learning a convolutional neural network (CNN) based mapping function that maps an image to a feature vector. The embedding quality is usually evaluated by the performance in image search tasks. Since very few users bother to open the second page search results, top-k precision mostly dominates the user experience and thus is one of the crucial evaluation metrics for the embedding quality. Despite being extensively studied, existing algorithms are usually based on heuristic observation without theoretical guarantee. Consequently, gradient descent direction on the training loss is mostly inconsistent with the direction of optimizing the concerned evaluation metric. This inconsistency certainly misleads the training direction and degrades the performance. In contrast to existing works, in this paper, we propose a novel deep image embedding algorithm with end-to-end optimization to top-k precision, the evaluation metric that is closely related to user experience. Specially, our loss function is constructed with wisely selected ``misplaced"" images along the top k nearest neighbor decision boundary, so that the gradient descent update directly promotes the concerned metric, top-k precision. Further more, our theoretical analysis on the upper bounding and consistency properties of the proposed loss supports that minimizing our proposed loss is equivalent to maximizing top-k precision. Experiments show that our proposed algorithm outperforms all compared state-of-the-art deep image embedding algorithms on three benchmark datasets."
Sampling-Free Epistemic Uncertainty Estimation Using Approximated Variance Propagation,"Janis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, Federico Tombari",Technical University Munich and Google; Technical University Munich; Autonomous Intelligent Driving GmbH,66.66666666666666,germany,33.33333333333334,USA,"We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Postels_Sampling-Free_Epistemic_Uncertainty_Estimation_Using_Approximated_Variance_Propagation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Postels_Sampling-Free_Epistemic_Uncertainty_Estimation_Using_Approximated_Variance_Propagation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010684/,"['Uncertainty', 'Neural networks', 'Covariance matrices', 'Training', 'Jacobian matrices', 'Estimation', 'Real-time systems']","['Uncertainty Estimation', 'Epistemic Uncertainty', 'Variance Propagation', 'Epistemic Uncertainty Estimation', 'Neural Network', 'Monte Carlo Simulation', 'Semantic Segmentation', 'Computational Overhead', 'Inference Time', 'Safety-critical Applications', 'Noise Injection', 'Sampling-based Approach', 'Root Mean Square Error', 'Covariance Matrix', 'Convolutional Layers', 'Hidden Layer', 'Diagonal Matrix', 'Training Time', 'Error Propagation', 'Random Vector', 'Mean Absolute Deviation', 'Large Neural Networks', 'Distribution Of Training Data', 'Log-likelihood Test', 'Dropout Layer', 'Diagonal Covariance Matrix', 'Autonomous Vehicles', 'Aleatoric Uncertainty', 'Uncertainty Values', 'ReLU Activation Function']",,76,"We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (\ie, semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead."
Scalable Place Recognition Under Appearance Change for Autonomous Driving,"Anh-Dzung Doan, Yasir Latif, Tat-Jun Chin, Yu Liu, Thanh-Toan Do, Ian Reid","Department of Computer Science, University of Liverpool; School of Computer Science, The University of Adelaide",100.0,"australia, uk",0.0,,"A major challenge in place recognition for autonomous driving is to be robust against appearance changes due to short-term (e.g., weather, lighting) and long-term (seasons, vegetation growth, etc.) environmental variations. A promising solution is to continuously accumulate images to maintain an adequate sample of the conditions and incorporate new changes into the place recognition decision. However, this demands a place recognition technique that is scalable on an ever growing dataset. To this end, we propose a novel place recognition technique that can be efficiently retrained and compressed, such that the recognition of new queries can exploit all available data (including recent changes) without suffering from visible growth in computational cost. Underpinning our method is a novel temporal image matching technique based on Hidden Markov Models. Our experiments show that, compared to state-of-the-art techniques, our method has much greater potential for large-scale place recognition for autonomous driving.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Doan_Scalable_Place_Recognition_Under_Appearance_Change_for_Autonomous_Driving_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Doan_Scalable_Place_Recognition_Under_Appearance_Change_for_Autonomous_Driving_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009095/,"['Hidden Markov models', 'Videos', 'Autonomous vehicles', 'Robustness', 'Visualization', 'Cameras', 'Roads']","['Autonomous Vehicles', 'Changes In Appearance', 'Place Recognition', 'Hidden Markov Model', 'Error Of The Mean', 'Road Network', 'Edge Weights', 'Query Sequence', 'Leaf Node', 'Inference Time', 'Image Retrieval', 'Maximum A Posteriori', 'Map Representation', 'Median Error', 'Update Function', 'Appearance Variations', 'Camera Pose', 'Query Image', 'Rotation Data', 'Visual Odometry', 'State Transition Model', 'Definition Of Probability']",,49,"A major challenge in place recognition for autonomous driving is to be robust against appearance changes due to short-term (e.g., weather, lighting) and long-term (seasons, vegetation growth, etc.) environmental variations. A promising solution is to continuously accumulate images to maintain an adequate sample of the conditions and incorporate new changes into the place recognition decision. However, this demands a place recognition technique that is scalable on an ever growing dataset. To this end, we propose a novel place recognition technique that can be efficiently retrained and compressed, such that the recognition of new queries can exploit all available data (including recent changes) without suffering from visible growth in computational cost. Underpinning our method is a novel temporal image matching technique based on Hidden Markov Models. Our experiments show that, compared to state-of-the-art techniques, our method has much greater potential for large-scale place recognition for autonomous driving."
Scalable Verified Training for Provably Robust Image Classification,"Sven Gowal, Krishnamurthy (Dj) Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja ArandjeloviÄ, Timothy Mann, Pushmeet Kohli",DeepMind,0.0,,100.0,UK,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of IMAGENET.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010971/,"['Robustness', 'Training', 'Neural networks', 'Perturbation methods', 'Upper bound', 'Adaptation models', 'Optimization']","['Image Classification', 'Neural Network', 'Upper Bound', 'Downscaling', 'Robust Network', 'Adversarial Perturbations', 'Robustness Of Neural Networks', 'Error Rate', 'Optimization Problem', 'Large Model', 'Stochastic Gradient Descent', 'Model Architecture', 'Matrix Multiplication', 'Specific Procedures', 'Standard Training', 'Mixed-integer Programming', 'Linear Layer', 'Adversarial Training', 'Forward Pass', 'Range Of Radii', 'Projected Gradient Descent', 'Optimal Value Of Problem', 'Hinge Loss', 'Adversarial Examples', 'Nominal Input', 'Dual Solution']",,42,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of IMAGENET."
Scale-Aware Trident Networks for Object Detection,"Yanghao Li, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang",TuSimple; University of Chinese Academy of Sciences,100.0,"china, usa",0.0,,"Scale variation is one of the key challenges in object detection. In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection. Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.pdf,,https://git.io/fj5vR,,main,Oral,https://ieeexplore.ieee.org/document/9010716/,"['Feature extraction', 'Training', 'Detectors', 'Object detection', 'Proposals', 'Semantics', 'Computer architecture']","['Object Detection', 'Object Detection Network', 'Additional Costs', 'Feature Maps', 'Scale Variation', 'Receptive Field', 'Training Strategy', 'Variant Detection', 'Transformation Parameters', 'Fast Estimation', 'COCO Dataset', 'Object Instances', 'ResNet-101 Backbone', 'Uniform Power', 'Convolutional Neural Network', 'Detection Performance', 'Deep Convolutional Neural Network', 'Small Objects', 'Low-level Features', 'Residual Block', 'Dilation Rate', 'Image Pyramid', 'Large Objects', 'Dilated Convolution', 'Single Branch', 'Faster R-CNN', 'Two-stage Method', 'Region Proposal Network', 'Deformable Convolution', 'Backbone Network']",,696,"Scale variation is one of the key challenges in object detection. In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection. Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR."
Scaling Object Detection by Transferring Classification Weights,"Jason Kuen, Federico Perazzi, Zhe Lin, Jianming Zhang, Yap-Peng Tan","Adobe Research; Nanyang Technological University, Singapore",50.0,Singapore,50.0,USA,"Large scale object detection datasets are constantly increasing their size in terms of the number of classes and annotations count. Yet, the number of object-level categories annotated in detection datasets is an order of magnitude smaller than image-level classification labels. State-of-the art object detection models are trained in a supervised fashion and this limits the number of object classes they can detect. In this paper, we propose a novel weight transfer network (WTN) to effectively and efficiently transfer knowledge from classification network's weights to detection network's weights to allow detection of novel classes without box supervision. We first introduce input and feature normalization schemes to curb the under-fitting during training of a vanilla WTN. We then propose autoencoder-WTN (AE-WTN) which uses reconstruction loss to preserve classification network's information over all classes in the target latent space to ensure generalization to novel classes. Compared to vanilla WTN, AE-WTN obtains absolute performance gains of 6% on two Open Images evaluation sets with 500 seen and 57 novel classes respectively, and 25% on a Visual Genome evaluation set with 200 novel classes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kuen_Scaling_Object_Detection_by_Transferring_Classification_Weights_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kuen_Scaling_Object_Detection_by_Transferring_Classification_Weights_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009444/,"['Training', 'Detectors', 'Object detection', 'Task analysis', 'Semantics', 'Optical wavelength conversion', 'Knowledge engineering']","['Object Detection', 'Class Weights', 'Knowledge Transfer', 'Classification Network', 'Rate Set', 'Object Classification', 'Latent Space', 'Vanilla', 'Reconstruction Loss', 'Open Image', 'Object Detection Dataset', 'Detection Performance', 'Large-scale Datasets', 'Multilayer Perceptron', 'Bounding Box', 'Detection Task', 'Normalization Layer', 'Classification Datasets', 'Training Loss', 'Class Information', 'Encoder Network', 'Faster R-CNN', 'Intermediate Activity', 'Image-level Labels', 'Classification Loss', 'Feature Pyramid Network', 'Group Normalization', 'Bounding Box Annotations', 'Decoder Network', 'Source Task']",,13,"Large scale object detection datasets are constantly increasing their size in terms of the number of classes and annotations count. Yet, the number of object-level categories annotated in detection datasets is an order of magnitude smaller than image-level classification labels. State-of-the art object detection models are trained in a supervised fashion and this limits the number of object classes they can detect. In this paper, we propose a novel weight transfer network (WTN) to effectively and efficiently transfer knowledge from classification network's weights to detection network's weights to allow detection of novel classes without box supervision. We first introduce input and feature normalization schemes to curb the under-fitting during training of a vanilla WTN. We then propose autoencoder-WTN (AE-WTN) which uses reconstruction loss to preserve classification network's information over all classes in the target latent space to ensure generalization to novel classes. Compared to vanilla WTN, AE-WTN obtains absolute performance gains of 6% on two Open Images evaluation sets with 500 seen and 57 novel classes respectively, and 25% on a Visual Genome evaluation set with 200 novel classes."
Scaling Recurrent Models via Orthogonal Approximations in Tensor Trains,"Ronak Mehta, Rudrasis Chakraborty, Yunyang Xiong, Vikas Singh",University of Wisconsin Madison; University of California Berkeley,100.0,"USA, usa",0.0,,"Modern deep networks have proven to be very effective for analyzing real world images. However, their application in medical imaging is still in its early stages, primarily due to the large size of three-dimensional images, requiring enormous convolutional or fully connected layers - if we treat an image (and not image patches) as a sample. These issues only compound when the focus moves towards longitudinal analysis of 3D image volumes through recurrent structures, and when a point estimate of model parameters is insufficient in scientific applications where a reliability measure is necessary. Using insights from differential geometry, we adapt the tensor train decomposition to construct networks with significantly fewer parameters, allowing us to train powerful recurrent networks on whole brain image volume sequences. We describe the ""orthogonal"" tensor train, and demonstrate its ability to express a standard network layer both theoretically and empirically. We show its ability to effectively reconstruct whole brain volumes with faster convergence and stronger confidence intervals compared to the standard tensor train decomposition. We provide code and show experiments on the ADNI dataset using image sequences to regress on a cognition related outcome.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mehta_Scaling_Recurrent_Models_via_Orthogonal_Approximations_in_Tensor_Trains_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mehta_Scaling_Recurrent_Models_via_Orthogonal_Approximations_in_Tensor_Trains_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010323/,"['Tensile stress', 'Manifolds', 'Computational modeling', 'Brain modeling', 'Data models', 'Solid modeling', 'Three-dimensional displays']","['Recurrent Model', 'Tensor Train', 'Medical Imaging', '3D Images', 'Image Size', 'Recurrent Network', 'Image Volumes', 'Differential Geometry', 'Medical Imaging Applications', 'Recurrent Structure', 'Standard Decomposition', 'Sequencing Data', 'Neural Network', 'Deep Learning', 'Convolutional Network', 'Convolutional Neural Network', 'Gray Matter', 'Recurrent Neural Network', 'Model Uncertainty', 'Model Size', 'Mean Average Precision', 'Sequential Model', 'Tangent Space', 'QR Decomposition', 'Tensor Decomposition', 'Arbitrary Matrix', 'Lower Rank', 'Tensor Operations', 'Exponential Map', 'Ambient Space']",,2,"Modern deep networks have proven to be very effective for analyzing real world images. However, their application in medical imaging is still in its early stages, primarily due to the large size of three-dimensional images, requiring enormous convolutional or fully connected layers – if we treat an image (and not image patches) as a sample. These issues only compound when the focus moves towards longitudinal analysis of 3D image volumes through recurrent structures, and when a point estimate of model parameters is insufficient in scientific applications where a reliability measure is necessary. Using insights from differential geometry, we adapt the tensor train decomposition to construct networks with significantly fewer parameters, allowing us to train powerful recurrent networks on whole brain image volume sequences. We describe the “orthogonal” tensor train, and demonstrate its ability to express a standard network layer both theoretically and empirically. We show its ability to effectively reconstruct whole brain volumes with faster convergence and stronger confidence intervals compared to the standard tensor train decomposition. We provide code and show experiments on the ADNI dataset using image sequences to regress on a cognition related outcome."
Scaling and Benchmarking Self-Supervised Visual Representation Learning,"Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra",Facebook AI Research,0.0,,100.0,USA,"Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/fair_self_supervision_benchmark,,main,Poster,https://ieeexplore.ieee.org/document/9010709/,"['Task analysis', 'Complexity theory', 'Visualization', 'Benchmark testing', 'Data models', 'Image color analysis', 'Navigation']","['Visual Representation', 'Representation Learning', 'Self-supervised Learning', 'Visual Representation Learning', 'Semantic', 'Hardness', 'Object Detection', 'Popular Approach', 'Machine Vision', 'Self-supervised Approach', 'Convolutional Neural Network', 'Complex Problems', 'Classification Task', 'Image Classification', 'Feature Representation', 'Network Layer', 'Transfer Learning', 'Linear Classifier', 'AlexNet', 'High Reward', 'Pretext Task', 'Quality Of Representations', 'Pre-training Dataset', 'Transfer Task', 'Benchmark Suite', 'Discriminative Learning', 'Pre-training Data', 'Image Classification Tasks', 'Limited Supervision', 'Number Of Permutations']",,195,"Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark."
Scene Graph Prediction With Limited Labels,"Vincent S. Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein, Christopher RÃ©, Li Fei-Fei",Stanford University,100.0,usa,0.0,,"Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R^2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Scene_Graph_Prediction_With_Limited_Labels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Scene_Graph_Prediction_With_Limited_Labels_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9022298/,"['Visualization', 'Complexity theory', 'Feature extraction', 'Genomics', 'Bioinformatics', 'Training', 'Data models']","['Scene Graph', 'Scene Graph Prediction', 'Heuristic', 'Transfer Learning', 'Training Labels', 'Computer Vision Applications', 'Semi-supervised Methods', 'Visual Question Answering', 'Visual Relationship', 'Complex Relationship', 'Decision Tree', 'F1 Score', 'Spatial Features', 'Feature Classification', 'Object Detection', 'Bounding Box', 'Majority Voting', 'Target Domain', 'Unlabeled Data', 'Semi-supervised Learning', 'Source Domain', 'Label Probability', 'Unlabeled Set', 'Number Of Subtypes', 'Label Propagation', 'Subject And Object', 'Object Pairs', 'External Knowledge', 'Relative Spatial Position']","['weak supervision', 'knowledge base', 'visual relationship detection', 'scene graph', 'semi supervised', 'visual knowledge base']",33,"Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R^2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels."
Scene Text Visual Question Answering,"Ali Furkan Biten, RubÃ¨n Tito, AndrÃ©s Mafla, Lluis Gomez, MarÃ§al RusiÃ±ol, Ernest Valveny, C.V. Jawahar, Dimosthenis Karatzas","Computer Vision Center, UAB, Spain; CVIT, IIIT Hyderabad, India",100.0,"Spain, india, spain",0.0,,"Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the Visual Question Answering process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9011031/,"['Visualization', 'Task analysis', 'Knowledge discovery', 'Text recognition', 'Cognition', 'Computer vision', 'Semantics']","['Question Answering', 'Visual Question Answering', 'Scene Text', 'Visual Information', 'Optical Character Recognition', 'High-level Semantic Information', 'Upper Bound', 'Convolutional Neural Network', 'Standard Model', 'Image Features', 'Computer Vision', 'Long Short-term Memory', 'Bounding Box', 'Exact Match', 'Single Word', 'Textual Information', 'Open Challenges', 'Image Texture', 'Attention Map', 'Common Answer', 'License Plate', 'Levenshtein Distance', 'Text Retrieval', 'Edit Distance', 'Class Vector']",,142,"Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the Visual Question Answering process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research."
SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation,"Yang Zhou, Zachary While, Evangelos Kalogerakis",University of Massachusetts Amherst,100.0,usa,0.0,,"In this paper we propose a neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings. Given an input, potentially incomplete, 3D scene and a query location, our method predicts a probability distribution over object types that fit well in that location. Our distribution is predicted though passing learned messages in a dense graph whose nodes represent objects in the input scene and edges represent spatial and structural relationships. By weighting messages through an attention mechanism, our method learns to focus on the most relevant surrounding scene context to predict new scene objects. We found that our method significantly outperforms state-of-the-art approaches in terms of correctly predicting objects missing in a scene based on our experiments in the SUNCG dataset. We also demonstrate other applications of our method, including context-based 3D object recognition and iterative scene generation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_SceneGraphNet_Neural_Message_Passing_for_3D_Indoor_Scene_Augmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_SceneGraphNet_Neural_Message_Passing_for_3D_Indoor_Scene_Augmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008822/,"['Three-dimensional displays', 'Message passing', 'Neural networks', 'Shape', 'Solid modeling', 'Probability distribution', 'Task analysis']","['Message Passing', '3D Indoor Scene', 'Attention Mechanism', 'Object Recognition', 'Objects In The Scene', '3D Scene', 'Scene Context', 'Types Of Relationships', 'Bounding Box', 'Learnable Parameters', 'Graph Structure', 'Object Size', 'Latent Representation', 'Graph Neural Networks', 'Specific Node', 'Different Types Of Relationships', 'Shape Representation', 'Sequence Of Units', 'Node Representations', 'Scene Graph', 'Recursive Network', 'Room Type', 'Spatial Query', 'Hold-out Validation', 'Scene Model', '2D Location', 'Sparse Graph', 'Predictive Distribution', 'Side Of The Room']",,48,"In this paper we propose a neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings. Given an input, potentially incomplete, 3D scene and a query location, our method predicts a probability distribution over object types that fit well in that location. Our distribution is predicted though passing learned messages in a dense graph whose nodes represent objects in the input scene and edges represent spatial and structural relationships. By weighting messages through an attention mechanism, our method learns to focus on the most relevant surrounding scene context to predict new scene objects. We found that our method significantly outperforms state-of-the-art approaches in terms of correctly predicting objects missing in a scene based on our experiments in the SUNCG dataset. We also demonstrate other applications of our method, including context-based 3D object recognition and iterative scene generation."
Scoot: A Perceptual Metric for Facial Sketches,"Deng-Ping Fan, ShengChuan Zhang, Yu-Huan Wu, Yun Liu, Ming-Ming Cheng, Bo Ren, Paul L. Rosin, Rongrong Ji","Department of Artiﬁcial Intelligence, School of Informatics, Xiamen University; Peng Cheng Lab; TKLNDST, CS, Nankai University; Cardiff University; Department of Artiﬁcial Intelligence, School of Informatics, Xiamen University; TKLNDST, CS, Nankai University; Inception Institute of Artiﬁcial Intelligence (IIAI)",85.71428571428571,"China, china, uae, uk",14.285714285714292,China,"While it is trivial for humans to quickly assess the perceptual similarity between two images, the underlying mechanism are thought to be quite complex. Despite this, the most widely adopted perceptual metrics today, such as SSIM and FSIM, are simple, shallow functions, and fail to consider many factors of human perception. Recently, the facial modeling community has observed that the inclusion of both structure and texture has a significant positive benefit for face sketch synthesis (FSS). But how perceptual are these so-called ""perceptual features""? Which elements are critical for their success? In this paper, we design a perceptual metric, called Structure Co-Occurrence Texture (Scoot), which simultaneously considers the block-level spatial structure and co-occurrence texture statistics. To test the quality of metrics, we propose three novel meta-measures based on various reliable properties. Extensive experiments verify that our Scoot metric exceeds the performance of prior work. Besides, we built the first largest scale (152k judgments) human-perception-based sketch database that can evaluate how well a metric consistent with human perception. Our results suggest that ""spatial structure"" and ""co-occurrence texture"" are two generally applicable perceptual features in face sketch synthesis.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.pdf,http://mmcheng.net/scoot/,,,main,Poster,https://ieeexplore.ieee.org/document/9009510/,"['Measurement', 'Frequency selective surfaces', 'Distortion', 'Image quality', 'Graphite', 'Databases', 'Task analysis']","['Perceptual Metrics', 'Spatial Structure', 'Perceptual Similarity', 'Human Visual System', 'Reliability Properties', 'Image Quality', 'Grayscale', 'Texture Features', 'Source Images', 'Co-occurrence Matrix', 'Human Vision', 'Quantization Parameter', 'Distance Perception', 'Distortion Types', 'Slight Rotation', 'Classical Metrics', 'Current Metrics']",,34,"While it is trivial for humans to quickly assess the perceptual similarity between two images, the underlying mechanism are thought to be quite complex. Despite this, the most widely adopted perceptual metrics today, such as SSIM and FSIM, are simple, shallow functions, and fail to consider many factors of human perception. Recently, the facial modeling community has observed that the inclusion of both structure and texture has a significant positive benefit for face sketch synthesis (FSS). But how perceptual are these so-called “perceptual features”? Which elements are critical for their success? In this paper, we design a perceptual metric, called Structure Co-Occurrence Texture (Scoot), which simultaneously considers the block-level spatial structure and co-occurrence texture statistics. To test the quality of metrics, we propose three novel meta-measures based on various reliable properties. Extensive experiments verify that our Scoot metric exceeds the performance of prior work. Besides, we built the first largest scale (152k judgments) human-perception-based sketch database that can evaluate how well a metric consistent with human perception. Our results suggest that “spatial structure” and “co-occurrence texture” are two generally applicable perceptual features in face sketch synthesis."
Searching for MobileNetV3,"Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam",Google AI; Google Brain,0.0,,100.0,USA,"We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008835/,"['Computer architecture', 'Proposals', 'Computational modeling', 'Image segmentation', 'Neural networks', 'Next generation networking', 'Mobile handsets']","['State Of The Art', 'Object Detection', 'Semantic Segmentation', 'Semantic Segmentation Task', 'Atrous Spatial Pyramid Pooling', 'ImageNet Classification', 'Higher Resource Use', 'Neural Network', 'Factorization', 'Mobile Devices', 'Neural Architecture', 'Backbone Network', 'Spatial Filter', 'Mobility Model', 'Feature Extraction Layer', 'Projection Layer', 'Network Search', 'Global Network Structure', 'Channel Reduction']",,4647,"We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation."
Second-Order Non-Local Attention Networks for Person Re-Identification,"Bryan (Ning) Xia, Yuan Gong, Yizhe Zhang, Christian Poellabauer",University of Notre Dame,100.0,usa,0.0,,"Recent efforts have shown promising results for person re-identification by designing part-based architectures to allow a neural network to learn discriminative representations from semantically coherent parts. Some efforts use soft attention to reallocate distant outliers to their most similar parts, while others adjust part granularity to incorporate more distant positions for learning the relationships. Others seek to generalize part-based methods by introducing a dropout mechanism on consecutive regions of the feature map to enhance distant region relationships. However, only few prior efforts model the distant or non-local positions of the feature map directly for the person re-ID task. In this paper, we propose a novel attention mechanism to directly model long-range relationships via second-order feature statistics. When combined with a generalized DropBlock module, our method performs equally to or better than state-of-the-art results for mainstream person re-identification datasets, including Market1501, CUHK03, and DukeMTMC-reID.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008774/,"['Correlation', 'Convolution', 'Neural networks', 'Task analysis', 'Feature extraction', 'Tensile stress', 'Computer architecture']","['Non-local Attention', 'Neural Network', 'Feature Maps', 'Relative Distance', 'Second-order Statistics', 'Convolutional Neural Network', 'Deep Network', 'Covariance Matrix', 'Deep Neural Network', 'Local Information', 'Convolutional Layers', 'Stage 2', 'Image Area', 'Global Features', 'Backbone Network', 'Batch Normalization Layer', 'Robust Representation', 'Dilated Convolution', 'Triplet Loss', 'Query Image', 'Baseline Network', 'Non-local Operation', 'Perform Ablation Studies', 'Nonexpansive Mapping', 'Adjacent Parts', 'Bounding Box', 'Training Images', 'Attention Module', 'Training Procedure', 'Deep Learning Models']",,53,"Recent efforts have shown promising results for person re-identification by designing part-based architectures to allow a neural network to learn discriminative representations from semantically coherent parts. Some efforts use soft attention to reallocate distant outliers to their most similar parts, while others adjust part granularity to incorporate more distant positions for learning the relationships. Others seek to generalize part-based methods by introducing a dropout mechanism on consecutive regions of the feature map to enhance distant region relationships. However, only few prior efforts model the distant or non-local positions of the feature map directly for the person re-ID task. In this paper, we propose a novel attention mechanism to directly model long-range relationships via second-order feature statistics. When combined with a generalized DropBlock module, our method performs equally to or better than state-of-the-art results for mainstream person re-identification datasets, including Market1501, CUHK03, and DukeMTMC-reID."
See-Through-Text Grouping for Referring Image Segmentation,"Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu","Institute of Information Science, Academia Sinica, Taiwan; Department of Computer Science, National Tsing Hua University, Taiwan",100.0,taiwan,0.0,,"Motivated by the conventional grouping techniques to image segmentation, we develop their DNN counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network (ConvRNN) that iteratively carries out top-down processing of bottom-up segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise (STEP) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The ConvRNN performs a top-down approximation by converting the STEP heatmap into a refined one, whereas the improvement is expected from training the network with a classification loss from the ground truth. With the refined heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new STEP heatmap as the next input to the ConvRNN. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other DNN models, while achieving state-of-the-art performance in all of the four datasets in the experiments.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_See-Through-Text_Grouping_for_Referring_Image_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_See-Through-Text_Grouping_for_Referring_Image_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009843/,"['Image segmentation', 'Visualization', 'Heating systems', 'Natural languages', 'Task analysis', 'Feature extraction', 'Convolution']","['Image Segmentation', 'Referring Image Segmentation', 'Neural Network', 'Deep Neural Network', 'Natural Language', 'Top-down Processes', 'Deep Neural Network Model', 'Pixel Level', 'Text Representation', 'Convolutional Recurrent Neural Network', 'Convolutional Layers', 'Contextual Information', 'Feature Maps', 'Visual Representation', 'Visual Features', 'Recurrent Neural Network', 'Receptive Field', 'Semantic Segmentation', 'Hidden State', 'Top-down And Bottom-up', 'Visual Question Answering', 'Interactive Segmentation', 'Word Embedding', 'Recurrent Unit', 'Multimodal Features', 'Common Space', 'Feature Concatenation', 'Joint Embedding', 'Bidirectional Long Short-term Memory', 'Coherent Regions']",,77,"Motivated by the conventional grouping techniques to image segmentation, we develop their DNN counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network (ConvRNN) that iteratively carries out top-down processing of bottom-up segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise (STEP) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The ConvRNN performs a top-down approximation by converting the STEP heatmap into a refined one, whereas the improvement is expected from training the network with a classification loss from the ground truth. With the refined heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new STEP heatmap as the next input to the ConvRNN. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other DNN models, while achieving state-of-the-art performance in all of the four datasets in the experiments."
Seeing Motion in the Dark,"Chen Chen, Qifeng Chen, Minh N. Do, Vladlen Koltun",Intel Labs; HKUST; UIUC,66.66666666666666,"hong kong, usa",33.33333333333334,USA,"Deep learning has recently been applied with impressive results to extreme low-light imaging. Despite the success of single-image processing, extreme low-light video processing is still intractable due to the difficulty of collecting raw video data with corresponding ground truth. Collecting long-exposure ground truth, as was done for single-image processing, is not feasible for dynamic scenes. In this paper, we present deep processing of very dark raw videos: on the order of one lux of illuminance. To support this line of work, we collect a new dataset of raw low-light videos, in which high-resolution raw data is captured at video rate. At this level of darkness, the signal-to-noise ratio is extremely low (negative if measured in dB) and the traditional image processing pipeline generally breaks down. A new method is presented to address this challenging problem. By carefully designing a learning-based pipeline and introducing a new loss function to encourage temporal stability, we train a siamese network on static raw videos, for which ground truth is available, such that the network generalizes to videos of dynamic scenes at test time. Experimental results demonstrate that the presented approach outperforms state-of-the-art models for burst processing, per-frame processing, and blind temporal consistency.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Seeing_Motion_in_the_Dark_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Seeing_Motion_in_the_Dark_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009494/,"['Streaming media', 'Pipelines', 'Image processing', 'Cameras', 'Noise reduction', 'Training', 'Noise measurement']","['Raw Data', 'Signal-to-noise', 'Image Processing', 'Video Dataset', 'Siamese Network', 'Temporal Consistency', 'Dynamic Scenes', 'Video Rate', 'Raw Video', 'Traditional Pipeline', 'Low-light Image', 'Deep Network', 'Image Dataset', 'Raw Images', 'Temporal Correlation', 'Optical Flow', 'Shot Noise', 'Video Quality', 'White Balance', 'Raw Frames', 'Dynamic Video', 'Temporal Error', 'Input Frames', 'Dense Correspondence', 'Video Output', 'Temporal Instability']",,133,"Deep learning has recently been applied with impressive results to extreme low-light imaging. Despite the success of single-image processing, extreme low-light video processing is still intractable due to the difficulty of collecting raw video data with corresponding ground truth. Collecting long-exposure ground truth, as was done for single-image processing, is not feasible for dynamic scenes. In this paper, we present deep processing of very dark raw videos: on the order of one lux of illuminance. To support this line of work, we collect a new dataset of raw low-light videos, in which high-resolution raw data is captured at video rate. At this level of darkness, the signal-to-noise ratio is extremely low (negative if measured in dB) and the traditional image processing pipeline generally breaks down. A new method is presented to address this challenging problem. By carefully designing a learning-based pipeline and introducing a new loss function to encourage temporal stability, we train a siamese network on static raw videos, for which ground truth is available, such that the network generalizes to videos of dynamic scenes at test time. Experimental results demonstrate that the presented approach outperforms state-of-the-art models for burst processing, per-frame processing, and blind temporal consistency."
Seeing What a GAN Cannot Generate,"David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba","The Chinese University of Hong Kong; MIT CSAIL; MIT-IBM Watson AI Lab; MIT CSAIL, MIT-IBM Watson AI Lab",100.0,"Hong Kong, usa",0.0,,"Despite the success of Generative Adversarial Networks (GANs), mode collapse remains a serious issue during GAN training. To date, little work has focused on understanding and quantifying which modes have been dropped by a model. In this work, we visualize mode collapse at both the distribution level and the instance level. First, we deploy a semantic segmentation network to compare the distribution of segmented objects in the generated images with the target distribution in the training set. Differences in statistics reveal object classes that are omitted by a GAN. Second, given the identified omitted object classes, we visualize the GAN's omissions directly. In particular, we compare specific differences between individual photos and their approximate inversions by a GAN. To this end, we relax the problem of inversion and solve the tractable problem of inverting a GAN layer instead of the entire generator. Finally, we use this framework to analyze several recent GANs trained on multiple datasets and identify their typical failure cases.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bau_Seeing_What_a_GAN_Cannot_Generate_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bau_Seeing_What_a_GAN_Cannot_Generate_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009809/,"['Gallium nitride', 'Generative adversarial networks', 'Image segmentation', 'Generators', 'Visualization', 'Image reconstruction', 'Training']","['Generative Adversarial Networks', 'Semantic', 'Training Set', 'Object Classification', 'Inverse Problem', 'Failure Cases', 'Target Distribution', 'Generative Adversarial Networks Training', 'Neural Network', 'Images Of Samples', 'Qualitative Results', 'Target Image', 'Inverse Method', 'Full Method', 'Reconstruction Loss', 'Intermediate Representation', 'Segment Classification', 'Generative Adversarial Networks Model', 'Generation Layer', 'Inception Distance', 'Fr√©chet Inception Distance', 'StyleGAN']",,195,"Despite the success of Generative Adversarial Networks (GANs), mode collapse remains a serious issue during GAN training. To date, little work has focused on understanding and quantifying which modes have been dropped by a model. In this work, we visualize mode collapse at both the distribution level and the instance level. First, we deploy a semantic segmentation network to compare the distribution of segmented objects in the generated images with the target distribution in the training set. Differences in statistics reveal object classes that are omitted by a GAN. Second, given the identified omitted object classes, we visualize the GAN's omissions directly. In particular, we compare specific differences between individual photos and their approximate inversions by a GAN. To this end, we relax the problem of inversion and solve the tractable problem of inverting a GAN layer instead of the entire generator. Finally, we use this framework to analyze several recent GANs trained on multiple datasets and identify their typical failure cases."
SegSort: Segmentation by Discriminative Sorting of Segments,"Jyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D. Collins, Tien-Ju Yang, Xiao Zhang, Liang-Chieh Chen",University of Pennsylvania; MIT; UC Berkeley / ICSI; Google Research,75.0,usa,25.0,USA,"Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set. As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving 76% performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hwang_SegSort_Segmentation_by_Discriminative_Sorting_of_Segments_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hwang_SegSort_Segmentation_by_Discriminative_Sorting_of_Segments_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010893/,"['Image segmentation', 'Semantics', 'Sorting', 'Visualization', 'Training', 'Measurement', 'Machine learning']","['Deep Learning', 'Semantic Segmentation', 'Semantic Labels', 'Metric Learning', 'Precise Boundaries', 'Training Set', 'Validation Set', 'K-means', 'Image Segmentation', 'Unsupervised Learning', 'Points In Space', 'Latent Space', 'Ground Truth Labels', 'Segmentation Approach', 'Image Retrieval', 'Contrastive Loss', 'Hidden Variables', 'Hypersphere', 'Probability Of Pixel', 'Architecture For Segmentation', 'Contour Detection', 'Learning For Segmentation', 'Memory Bank', 'Spherical Clusters', 'Perceptual Organization', 'Training Objective', 'K-nearest Neighbor']",,77,"Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set. As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving 76% performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments."
Selective Sparse Sampling for Fine-Grained Image Recognition,"Yao Ding, Yanzhao Zhou, Yi Zhu, Qixiang Ye, Jianbin Jiao","University of Chinese Academy of Sciences, Beijing, China",100.0,china,0.0,,"Fine-grained recognition poses the unique challenge of capturing subtle inter-class differences under considerable intra-class variances (e.g., beaks for bird species). Conventional approaches crop local regions and learn detailed representation from those regions, but suffer from the fixed number of parts and missing of surrounding context. In this paper, we propose a simple yet effective framework, called Selective Sparse Sampling, to capture diverse and fine-grained details. The framework is implemented using Convolutional Neural Networks, referred to as Selective Sparse Sampling Networks (S3Ns). With image-level supervision, S3Ns collect peaks, i.e., local maximums, from class response maps to estimate informative, receptive fields and learn a set of sparse attention for capturing fine-detailed visual evidence as well as preserving context. The evidence is selectively sampled to extract discriminative and complementary features, which significantly enrich the learned representation and guide the network to discover more subtle cues. Extensive experiments and ablation studies show that the proposed method consistently outperforms the state-of-the-art methods on challenging benchmarks including CUB-200-2011, FGVC-Aircraft, and Stanford Cars.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ding_Selective_Sparse_Sampling_for_Fine-Grained_Image_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Selective_Sparse_Sampling_for_Fine-Grained_Image_Recognition_ICCV_2019_paper.pdf,,https://github.com/Yao-DD/S3N.git,,main,Poster,https://ieeexplore.ieee.org/document/9008286/,"['Visualization', 'Image recognition', 'Feature extraction', 'Birds', 'Task analysis', 'Agriculture', 'Automobiles']","['Image Recognition', 'Sparse Sampling', 'Fine-grained Image', 'Fine-grained Image Recognition', 'Convolutional Network', 'Convolutional Neural Network', 'Local Maxima', 'Receptive Field', 'Bird Species', 'Discriminative Features', 'Visual Evidence', 'Complementary Features', 'Extract Discriminative Features', 'Strong Evidence', 'Classification Accuracy', 'Gaussian Kernel', 'Local Features', 'Contextual Information', 'Input Image', 'Global Features', 'Image Resampling', 'Object Parts', 'Discriminative Regions', 'Image X', 'Weak Evidence', 'Bounding Box', 'Human Visual System', 'Feature Learning', 'Feature Maps']",,170,"Fine-grained recognition poses the unique challenge of capturing subtle inter-class differences under considerable intra-class variances (e.g., beaks for bird species). Conventional approaches crop local regions and learn detailed representation from those regions, but suffer from the fixed number of parts and missing of surrounding context. In this paper, we propose a simple yet effective framework, called Selective Sparse Sampling, to capture diverse and fine-grained details. The framework is implemented using Convolutional Neural Networks, referred to as Selective Sparse Sampling Networks (S3Ns). With image-level supervision, S3Ns collect peaks, i.e., local maximums, from class response maps to estimate informative, receptive fields and learn a set of sparse attention for capturing fine-detailed visual evidence as well as preserving context. The evidence is selectively sampled to extract discriminative and complementary features, which significantly enrich the learned representation and guide the network to discover more subtle cues. Extensive experiments and ablation studies show that the proposed method consistently outperforms the state-of-the-art methods on challenging benchmarks including CUB-200-2011, FGVC-Aircraft, and Stanford Cars."
Selectivity or Invariance: Boundary-Aware Salient Object Detection,"Jinming Su, Jia Li, Yu Zhang, Changqun Xia, Yonghong Tian","Peng Cheng Laboratory, Shenzhen, China; National Engineering Laboratory for Video Technology, School of EE&CS, Peking University; State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University",100.0,china,0.0,,"Typically, a salient object detection (SOD) model faces opposite requirements in processing object interiors and boundaries. The features of interiors should be invariant to strong appearance change so as to pop-out the salient object as a whole, while the features of boundaries should be selective to slight appearance change to distinguish salient objects and background. To address this selectivity-invariance dilemma, we propose a novel boundary-aware network with successive dilation for image-based SOD. In this network, the feature selectivity at boundaries is enhanced by incorporating a boundary localization stream, while the feature invariance at interiors is guaranteed with a complex interior perception stream. Moreover, a transition compensation stream is adopted to amend the probable failures in transitional regions between interiors and boundaries. In particular, an integrated successive dilation module is proposed to enhance the feature invariance at interiors and transitional regions. Extensive experiments on six datasets show that the proposed approach outperforms 16 state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Su_Selectivity_or_Invariance_Boundary-Aware_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Su_Selectivity_or_Invariance_Boundary-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf,http://cvteam.net,,,main,Poster,https://ieeexplore.ieee.org/document/9011003/,"['Feature extraction', 'Streaming media', 'Task analysis', 'Kernel', 'Object detection', 'Visualization', 'Saliency detection']","['Salient Object', 'Salient Object Detection', 'Transition Region', 'Changes In Appearance', 'Invariant Features', 'Object Boundaries', 'Boundary Features', 'Large Changes', 'High Selectivity', 'Local Information', 'Convolutional Layers', 'Contextual Information', 'Feature Maps', 'Receptive Field', 'Benchmark Datasets', 'Precision-recall Curve', 'Binary Map', 'Transition Characteristics', 'Saliency Map', 'Multi-level Features', 'Atrous Spatial Pyramid Pooling', 'Integration Of Objectives', 'Dilation Rate', 'Confidence Map', 'Separate Streams', 'Saliency Detection', 'Ground Truth Map', 'Learning Rate']",,124,"Typically, a salient object detection (SOD) model faces opposite requirements in processing object interiors and boundaries. The features of interiors should be invariant to strong appearance change so as to pop-out the salient object as a whole, while the features of boundaries should be selective to slight appearance change to distinguish salient objects and background. To address this selectivity-invariance dilemma, we propose a novel boundary-aware network with successive dilation for image-based SOD. In this network, the feature selectivity at boundaries is enhanced by incorporating a boundary localization stream, while the feature invariance at interiors is guaranteed with a complex interior perception stream. Moreover, a transition compensation stream is adopted to amend the probable failures in transitional regions between interiors and boundaries. In particular, an integrated successive dilation module is proposed to enhance the feature invariance at interiors and transitional regions. Extensive experiments on six datasets show that the proposed approach outperforms 16 state-of-the-art methods."
Self-Critical Attention Learning for Person Re-Identification,"Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, Jie Zhou","Department of Automation, Tsinghua University, China; State Key Lab of Intelligent Technologies and Systems, China; Beijing National Research Center for Information Science and Technology, China",100.0,"China, china",0.0,,"In this paper, we propose a self-critical attention learning method for person re-identification. Unlike most existing methods which train the attention mechanism in a weakly-supervised manner and ignore the attention confidence level, we learn the attention with a critic which measures the attention quality and provides a powerful supervisory signal to guide the learning process. Moreover, the critic model facilitates the interpretation of the effectiveness of the attention mechanism during the learning process, by estimating the quality of the attention maps. Specifically, we jointly train our attention agent and critic in a reinforcement learning manner, where the agent produces the visual attention while the critic analyzes the gain from the attention and guides the agent to maximize this gain. We design spatial- and channel-wise attention models with our critic module and evaluate them on three popular benchmarks including Market-1501, DukeMTMC-ReID, and CUHK03. The experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin of 5.9%/2.1%, 6.3%/3.0%, and 10.5%/9.5% on mAP/Rank-1, respectively.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009089/,"['Training', 'Learning systems', 'Visualization', 'Predictive models', 'Measurement', 'Convolution', 'Learning (artificial intelligence)']","['Attention Learning', 'Learning Process', 'Attention Mechanism', 'Visual Attention', 'Large Margin', 'Attention Model', 'Attention Map', 'Critical Modulator', 'Channel-wise Attention', 'Supervisory Signal', 'Deep Learning', 'Performance Of Method', 'Convolutional Layers', 'Feature Maps', 'Fully-connected Layer', 'Attention Module', 'Spatial Attention', 'Backbone Network', 'Classification Loss', 'Effects Of Attention', 'Triplet Loss', 'Metric Learning', 'Basic Network', 'Metric Learning Methods', 'Background Clutter', 'Horizontal Flip', 'Attention-based Methods', 'Weak Supervision', 'Critic Network', 'Score Map']",,111,"In this paper, we propose a self-critical attention learning method for person re-identification. Unlike most existing methods which train the attention mechanism in a weakly-supervised manner and ignore the attention confidence level, we learn the attention with a critic which measures the attention quality and provides a powerful supervisory signal to guide the learning process. Moreover, the critic model facilitates the interpretation of the effectiveness of the attention mechanism during the learning process, by estimating the quality of the attention maps. Specifically, we jointly train our attention agent and critic in a reinforcement learning manner, where the agent produces the visual attention while the critic analyzes the gain from the attention and guides the agent to maximize this gain. We design spatial- and channel-wise attention models with our critic module and evaluate them on three popular benchmarks including Market-1501, DukeMTMC-ReID, and CUHK03. The experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin of 5.9%/2.1%, 6.3%/3.0%, and 10.5%/9.5% on mAP/Rank-1, respectively."
Self-Ensembling With GAN-Based Data Augmentation for Domain Adaptation in Semantic Segmentation,"Jaehoon Choi, Taekyung Kim, Changick Kim",KAIST,100.0,south korea,0.0,,"Deep learning-based semantic segmentation methods have an intrinsic limitation that training a model requires a large amount of data with pixel-level annotations. To address this challenging issue, many researchers give attention to unsupervised domain adaptation for semantic segmentation. Unsupervised domain adaptation seeks to adapt the model trained on the source domain to the target domain. In this paper, we introduce a self-ensembling technique, one of the successful methods for domain adaptation in classification. However, applying self-ensembling to semantic segmentation is very difficult because heavily-tuned manual data augmentation used in self-ensembling is not useful to reduce the large domain gap in the semantic segmentation. To overcome this limitation, we propose a novel framework consisting of two components, which are complementary to each other. First, we present a data augmentation method based on Generative Adversarial Networks (GANs), which is computationally efficient and effective to facilitate domain alignment. Given those augmented images, we apply self-ensembling to enhance the performance of the segmentation network on the target domain. The proposed method outperforms state-of-the-art semantic segmentation methods on unsupervised domain adaptation benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Self-Ensembling_With_GAN-Based_Data_Augmentation_for_Domain_Adaptation_in_Semantic_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Self-Ensembling_With_GAN-Based_Data_Augmentation_for_Domain_Adaptation_in_Semantic_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008277/,"['Semantics', 'Generators', 'Image segmentation', 'Training', 'Gallium nitride', 'Adaptation models', 'Feature extraction']","['Data Augmentation', 'Semantic Segmentation', 'Domain Adaptation', 'GAN-based Data Augmentation', 'Generative Adversarial Networks', 'Target Domain', 'Source Domain', 'Data Augmentation Methods', 'Image Augmentation', 'Domain Alignment', 'Domain Adaptation Methods', 'Semantic Segmentation Methods', 'Pixel-level Annotations', 'Data Sources', 'Local Structure', 'Target Sample', 'Domain Shift', 'Source Images', 'Target Data', 'Pixel Level', 'Student Network', 'Teacher Network', 'Semantic Constraints', 'Consistency Loss', 'Adversarial Training', 'Semantic Consistency', 'Object Boundaries', 'Geometric Transformation', 'Pseudo Labels', 'Minority Class']",,161,"Deep learning-based semantic segmentation methods have an intrinsic limitation that training a model requires a large amount of data with pixel-level annotations. To address this challenging issue, many researchers give attention to unsupervised domain adaptation for semantic segmentation. Unsupervised domain adaptation seeks to adapt the model trained on the source domain to the target domain. In this paper, we introduce a self-ensembling technique, one of the successful methods for domain adaptation in classification. However, applying self-ensembling to semantic segmentation is very difficult because heavily-tuned manual data augmentation used in self-ensembling is not useful to reduce the large domain gap in the semantic segmentation. To overcome this limitation, we propose a novel framework consisting of two components, which are complementary to each other. First, we present a data augmentation method based on Generative Adversarial Networks (GANs), which is computationally efficient and effective to facilitate domain alignment. Given those augmented images, we apply self-ensembling to enhance the performance of the segmentation network on the target domain. The proposed method outperforms state-of-the-art semantic segmentation methods on unsupervised domain adaptation benchmarks."
Self-Guided Network for Fast Image Denoising,"Shuhang Gu, Yawei Li, Luc Van Gool, Radu Timofte","Computer Vision Lab, ETH Zurich, Switzerland; KU Leuven, Belgium; Computer Vision Lab, ETH Zurich, Switzerland",100.0,"belgium, switzerland",0.0,,"During the past years, tremendous advances in image restoration tasks have been achieved using highly complex neural networks. Despite their good restoration performance, the heavy computational burden hinders the deployment of these networks on constrained devices, e.g. smart phones and consumer electronic products. To tackle this problem, we propose a self-guided network (SGN), which adopts a top-down self-guidance architecture to better exploit image multi-scale information. SGN directly generates multi-resolution inputs with the shuffling operation. Large-scale contextual information extracted at low resolution is gradually propagated into the higher resolution sub-networks to guide the feature extraction processes at these scales. Such a self-guidance strategy enables SGN to efficiently incorporate multi-scale information and extract good local features to recover noisy images. We validate the effectiveness of SGN through extensive experiments. The experimental results demonstrate that SGN greatly improves the memory and runtime efficiency over state-of-the-art efficient methods, without trading off PSNR accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Self-Guided_Network_for_Fast_Image_Denoising_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Self-Guided_Network_for_Fast_Image_Denoising_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010817/,"['Spatial resolution', 'Feature extraction', 'Noise reduction', 'Image restoration', 'Convolution', 'Image denoising']","['Neural Network', 'Smartphone', 'Complex Network', 'Low Resolution', 'Contextual Information', 'Memory Efficiency', 'Multi-scale Information', 'Large-scale Information', 'Restoration Performance', 'Spatial Resolution', 'Convolutional Neural Network', 'Deep Network', 'Running Time', 'Convolutional Layers', 'Input Image', 'Feature Maps', 'Additive Noise', 'Recurrent Neural Network', 'Color Images', 'Receptive Field', 'Denoising Algorithm', 'Residual Block', 'Large Receptive Field', 'Guidance Information', 'Semantic Segmentation', 'Grayscale Images', 'PSNR Values', 'Multilayer Perceptron', 'Dilated Convolution', 'ReLU Layer']",,127,"During the past years, tremendous advances in image restoration tasks have been achieved using highly complex neural networks. Despite their good restoration performance, the heavy computational burden hinders the deployment of these networks on constrained devices, \eg smart phones and consumer electronic products. To tackle this problem, we propose a self-guided network (SGN), which adopts a top-down self-guidance architecture to better exploit image multi-scale information. SGN directly generates multi-resolution inputs with the shuffling operation. Large-scale contextual information extracted at low resolution is gradually propagated into the higher resolution sub-networks to guide the feature extraction processes at these scales. Such a self-guidance strategy enables SGN to efficiently incorporate multi-scale information and extract good local features to recover noisy images. We validate the effectiveness of SGN through extensive experiments. The experimental results demonstrate that SGN greatly improves the memory and runtime efficiency over state-of-the-art efficient methods, without trading off PSNR accuracy."
Self-Similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-Identification,"Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, Thomas S. Huang","University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign, ReLER, University of Technology Sydney; University of Illinois at Urbana-Champaign, IBM Research, University of Oregon; Shanghai Jiao Tong University",100.0,"China, australia, usa",0.0,,"Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC-Market1501) and 4.4% (Market1501-DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Self-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_Adaptation_Approach_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_Self-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_Adaptation_Approach_for_ICCV_2019_paper.pdf,,https://github.com/OasisYang/SSG,,main,Oral,https://ieeexplore.ieee.org/document/9008293/,"['Training', 'Cameras', 'Task analysis', 'Feature extraction', 'Adaptation models', 'Neural networks', 'Machine learning']","['Domain Adaptation', 'Unsupervised Domain Adaptation Approaches', 'Self-similarity Grouping', 'Open Set', 'Target Domain', 'Unsupervised Manner', 'Random Sampling', 'Lower Body', 'Large-scale Datasets', 'Training Strategy', 'Upper Body', 'Generative Adversarial Networks', 'Unsupervised Methods', 'Accurate Mapping', 'Fully-connected Layer', 'Handcrafted Features', 'Source Domain', 'Target Dataset', 'Source Dataset', 'Person Image', 'Pseudo Labels', 'Triplet Loss', 'Unsupervised Domain Adaptation Methods', 'Target Domain Data', 'Metric Learning', 'Unlabeled Images', 'Joint Training', 'Representation Of A Person', 'Large Margin', 'Bounding Box']",,353,"Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC→Market1501) and 4.4% (Market1501→DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG ."
Self-Supervised Deep Depth Denoising,"Vladimiros Sterzentsenko, Leonidas Saroglou, Anargyros Chatzitofis, Spyridon Thermos, Nikolaos Zioulis, Alexandros Doumanoglou, Dimitrios Zarpalas, Petros Daras","Information Technologies Institute (ITI), Centre for Research and Technology Hellas (CERTH), Greece",100.0,Greece,0.0,,"Depth perception is considered an invaluable source of information for various vision tasks. However, depth maps acquired using consumer-level sensors still suffer from non-negligible noise. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned non-uniform noise, while preserving geometric details. Despite the effort, deep depth denoising is still an open challenge mainly due to the lack of clean data that could be used as ground truth. In this paper, we propose a fully convolutional deep autoencoder that learns to denoise depth maps, surpassing the lack of ground truth data. Specifically, the proposed autoencoder exploits multiple views of the same scene from different points of view in order to learn to suppress noise in a self-supervised end-to-end manner using depth and color information during training, yet only depth during inference. To enforce self-supervision, we leverage a differentiable rendering technique to exploit photometric supervision, which is further regularized using geometric and surface priors. As the proposed approach relies on raw data acquisition, a large RGB-D corpus is collected using Intel RealSense sensors. Complementary to a quantitative evaluation, we demonstrate the effectiveness of the proposed self-supervised denoising approach on established 3D reconstruction applications. Code is avalable at https://github.com/VCL3D/DeepDepthDenoising",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sterzentsenko_Self-Supervised_Deep_Depth_Denoising_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sterzentsenko_Self-Supervised_Deep_Depth_Denoising_ICCV_2019_paper.pdf,,https://github.com/VCL3D/DeepDepthDenoising,,main,Poster,https://ieeexplore.ieee.org/document/9008557/,"['Sensors', 'Noise reduction', 'Image color analysis', 'Task analysis', 'Three-dimensional displays', 'Noise measurement', 'Color']","['Lack Of Data', 'Quantitative Evaluation', '3D Reconstruction', 'Depth Map', 'Depth Information', 'Color Information', 'Depth Perception', 'Deep Autoencoder', 'View In Order', 'Intel RealSense', 'Convolutional Neural Network', 'Convolutional Layers', 'Total Loss', 'Color Images', 'Point Cloud', 'Image Intensity', 'Noise Model', 'Skip Connections', 'Depth Camera', 'Image Domain', 'Bilateral Filter', 'View Synthesis', 'Normal Priors', 'Direct Supervision', 'Depth Data', 'Depth Estimation', 'Common Coordinate System', 'Target View', 'Self-supervised Approach', 'Downsampling Layer']",,28,"Depth perception is considered an invaluable source of information for various vision tasks. However, depth maps acquired using consumer-level sensors still suffer from non-negligible noise. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned non-uniform noise, while preserving geometric details. Despite the effort, deep depth denoising is still an open challenge mainly due to the lack of clean data that could be used as ground truth. In this paper, we propose a fully convolutional deep autoencoder that learns to denoise depth maps, surpassing the lack of ground truth data. Specifically, the proposed autoencoder exploits multiple views of the same scene from different points of view in order to learn to suppress noise in a self-supervised end-to-end manner using depth and color information during training, yet only depth during inference. To enforce self-supervision, we leverage a differentiable rendering technique to exploit photometric supervision, which is further regularized using geometric and surface priors. As the proposed approach relies on raw data acquisition, a large RGB-D corpus is collected using Intel RealSense sensors. Complementary to a quantitative evaluation, we demonstrate the effectiveness of the proposed self-supervised denoising approach on established 3D reconstruction applications. Code is avalable at https://github.com/VCL3D/DeepDepthDenoising."
Self-Supervised Difference Detection for Weakly-Supervised Semantic Segmentation,"Wataru Shimoda, Keiji Yanai","Artificial Intelligence eXploration Research Center, The University of Electro Communications, Tokyo",100.0,Japan,0.0,,"To minimize the annotation costs associated with the training of semantic segmentation models, researchers have extensively investigated weakly-supervised segmentation approaches. In the current weakly-supervised segmentation methods, the most widely adopted approach is based on visualization. However, the visualization results are not generally equal to semantic segmentation. Therefore, to perform accurate semantic segmentation under the weakly supervised condition, it is necessary to consider the mapping functions that convert the visualization results into semantic segmentation. For such mapping functions, the conditional random field and iterative re-training using the outputs of a segmentation model are usually used. However, these methods do not always guarantee improvements in accuracy; therefore, if we apply these mapping functions iteratively multiple times, eventually the accuracy will not improve or will decrease. In this paper, to make the most of such mapping functions, we assume that the results of the mapping function include noise, and we improve the accuracy by removing noise. To achieve our aim, we propose the self-supervised difference detection module, which estimates noise from the results of the mapping functions by predicting the difference between the segmentation masks before and after the mapping. We verified the effectiveness of the proposed method by performing experiments on the PASCAL Visual Object Classes 2012 dataset, and we achieved 64.9% in the val set and 65.5% in the test set. Both of the results become new state-of-the-art under the same setting of weakly supervised semantic segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shimoda_Self-Supervised_Difference_Detection_for_Weakly-Supervised_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shimoda_Self-Supervised_Difference_Detection_for_Weakly-Supervised_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009092/,"['Image segmentation', 'Semantics', 'Training', 'Visualization', 'Frequency selective surfaces', 'Task analysis', 'Predictive models']","['Semantic Segmentation', 'Weakly Supervised Semantic Segmentation', 'Improvement In Accuracy', 'Segmentation Method', 'Segmentation Model', 'Conditional Random Field', 'Semantic Segmentation Models', 'Convolutional Neural Network', 'Input Image', 'Intersection Over Union', 'Low-level Features', 'Segmentation Results', 'Augmentation Techniques', 'Self-supervised Learning', 'Saliency Map', 'Inference Results', 'Object Regions', 'Differences In Difficulty', 'Pseudo Labels', 'Class Activation Maps', 'Pixel-level Labels', 'Image-level Labels', 'Pixel-level Annotations', 'Confidence Map', 'Mean Intersection Over Union', 'Global Max Pooling', 'True Value', 'Hyperparameters']",,91,"To minimize the annotation costs associated with the training of semantic segmentation models, researchers have extensively investigated weakly-supervised segmentation approaches. In the current weakly-supervised segmentation methods, the most widely adopted approach is based on visualization. However, the visualization results are not generally equal to semantic segmentation. Therefore, to perform accurate semantic segmentation under the weakly supervised condition, it is necessary to consider the mapping functions that convert the visualization results into semantic segmentation. For such mapping functions, the conditional random field and iterative re-training using the outputs of a segmentation model are usually used. However, these methods do not always guarantee improvements in accuracy; therefore, if we apply these mapping functions iteratively multiple times, eventually the accuracy will not improve or will decrease. In this paper, to make the most of such mapping functions, we assume that the results of the mapping function include noise, and we improve the accuracy by removing noise. To achieve our aim, we propose the self-supervised difference detection module, which estimates noise from the results of the mapping functions by predicting the difference between the segmentation masks before and after the mapping. We verified the effectiveness of the proposed method by performing experiments on the PASCAL Visual Object Classes 2012 dataset, and we achieved 64.9% in the val set and 65.5% in the test set. Both of the results become new state-of-the-art under the same setting of weakly supervised semantic segmentation."
"Self-Supervised Learning With Geometric Constraints in Monocular Video: Connecting Flow, Depth, and Camera","Yuhua Chen, Cordelia Schmid, Cristian Sminchisescu","Google Research; Google Research, ETH Zurich",50.0,switzerland,50.0,USA,"We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video -- addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Supervised_Learning_With_Geometric_Constraints_in_Monocular_Video_Connecting_Flow_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Self-Supervised_Learning_With_Geometric_Constraints_in_Monocular_Video_Connecting_Flow_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010956/,"['Cameras', 'Three-dimensional displays', 'Optical variables control', 'Optical imaging', 'Task analysis', 'Optical losses', 'Geometrical optics']","['Geometric Constraints', 'Self-supervised Learning', 'Monocular Video', 'Transfer Learning', 'Multiple Tasks', 'Optical Flow', 'Intrinsic Parameters', 'Camera Pose', 'Bundle Adjustment', 'Refinement Strategy', 'Camera Intrinsics', 'Training Set', '3D Reconstruction', 'Target Image', 'Source Images', 'Ground Truth Labels', 'Pose Estimation', 'Rigid Transformation', 'Depth Estimation', 'Skew-symmetric', 'Camera Pose Estimation', 'Scene Geometry', 'Adjacent Frames', 'Geometric Loss', 'Structure From Motion', 'Fine-tuning Of Parameters', 'Homogeneous Coordinates', 'Camera Motion', 'Dense Correspondence', 'Odometry']",,173,"We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video - addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning."
Self-Supervised Monocular Depth Hints,"Jamie Watson, Michael Firman, Gabriel J. Brostow, Daniyar Turmukhambetov","Niantic, UCL; Niantic",50.0,"uk, usa, uk",50.0,USA,"Monocular depth estimators can be trained with various forms of self-supervision from binocular-stereo data to circumvent the need for high-quality laser-scans or other ground-truth data. The disadvantage, however, is that the photometric reprojection losses used with self-supervised learning typically have multiple local minima. These plausible-looking alternatives to ground-truth can restrict what a regression network learns, causing it to predict depth maps of limited quality. As one prominent example, depth discontinuities around thin structures are often incorrectly estimated by current state-of-the-art methods. Here, we study the problem of ambiguous reprojections in depth-prediction from stereo-based self-supervision, and introduce Depth Hints to alleviate their effects. Depth Hints are complementary depth-suggestions obtained from simple off-the-shelf stereo algorithms. These hints enhance an existing photometric loss function, and are used to guide a network to learn better weights. They require no additional data, and are assumed to be right only sometimes. We show that using our Depth Hints gives a substantial boost when training several leading self-supervised-from-stereo models, not just our own. Further, combined with other good practices, we produce state-of-the-art depth predictions on the KITTI benchmark.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Watson_Self-Supervised_Monocular_Depth_Hints_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Watson_Self-Supervised_Monocular_Depth_Hints_ICCV_2019_paper.pdf,,https://www.github.com/nianticlabs/depth-hints,,main,Poster,https://ieeexplore.ieee.org/document/9010315/,"['Training', 'Cameras', 'Videos', 'Estimation', 'Prediction algorithms', 'Image color analysis', 'Laser radar']","['Loss Function', 'Local Minima', 'Depth Map', 'Depth Estimation', 'Thin Structures', 'Reprojection', 'Depth Prediction', 'Multiple Local Minima', 'Single Image', 'Reference Image', 'Prediction Network', 'Ground Truth Labels', 'Optical Flow', 'Depth Values', 'Pose Estimation', 'Lidar Data', 'Camera Pose', 'Stereo Images', 'Relative Pose', 'KITTI Dataset', 'Monocular Depth Estimation', 'Stereo Pairs', 'Ground Truth Depth', 'Stereo Matching', 'LiDAR Point Clouds', 'Past Frames', 'Self-supervised Training', 'Supervisory Signal', 'Semantic Labels', 'Current Frame']",,170,"Monocular depth estimators can be trained with various forms of self-supervision from binocular-stereo data to circumvent the need for high-quality laser-scans or other ground-truth data. The disadvantage, however, is that the photometric reprojection losses used with self-supervised learning typically have multiple local minima. These plausible-looking alternatives to ground-truth can restrict what a regression network learns, causing it to predict depth maps of limited quality. As one prominent example, depth discontinuities around thin structures are often incorrectly estimated by current state-of-the-art methods. Here, we study the problem of ambiguous reprojections in depth-prediction from stereo-based self-supervision, and introduce Depth Hints to alleviate their effects. Depth Hints are complementary depth-suggestions obtained from simple off-the-shelf stereo algorithms. These hints enhance an existing photometric loss function, and are used to guide a network to learn better weights. They require no additional data, and are assumed to be right only sometimes. We show that using our Depth Hints gives a substantial boost when training several leading self-supervised-from-stereo models, not just our own. Further, combined with other good practices, we produce state-of-the-art depth predictions on the KITTI benchmark."
Self-Supervised Moving Vehicle Tracking With Stereo Sound,"Chuang Gan, Hang Zhao, Peihao Chen, David Cox, Antonio Torralba","IBM Research AI; MIT CSAIL; MIT-IBM Watson AI Lab, IBM Research AI",66.66666666666666,usa,33.33333333333334,USA,"Humans are able to localize objects in the environment using both visual and auditory cues, integrating information from multiple modalities into a common reference frame. We introduce a system that can leverage unlabeled audiovisual data to learn to localize objects (moving vehicles) in a visual reference frame, purely using stereo sound at inference time. Since it is labor-intensive to manually annotate the correspondences between audio and object bounding boxes, we achieve this goal by using the co-occurrence of visual and audio streams in unlabeled videos as a form of self-supervision, without resorting to the collection of ground truth annotations. In particular, we propose a framework that consists of a vision ""teacher"" network and a stereo-sound ""student"" network. During training, knowledge embodied in a well-established visual vehicle detection model is transferred to the audio domain using unlabeled videos as a bridge. At test time, the stereo-sound student network can work independently to perform object localization using just stereo audio and camera meta-data, without any visual input. Experimental results on a newly collected Auditory Vehicles Tracking dataset verify that our proposed approach outperforms several baseline approaches. We also demonstrate that our cross-modal auditory localization approach can assist in the visual localization of moving vehicles under poor lighting conditions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009049/,"['Visualization', 'Videos', 'Cameras', 'Training', 'Object detection', 'Microphones', 'Convolution']","['Stereo Sound', 'Poor Conditions', 'Bounding Box', 'Object Location', 'Visual Input', 'Sound Localization', 'Visual Stream', 'Student Network', 'Object Bounding Boxes', 'Vehicle Detection', 'Audio Stream', 'Common Reference Frame', 'Poor Lighting Conditions', 'Spectroscopic', 'Time Difference', 'Convolutional Layers', 'Feature Maps', 'Feature Representation', 'Object Detection', 'Intersection Over Union', 'Teacher Network', 'Camera Angle', 'Constraint Loss', 'Pitch Angle', 'Sound Source', 'Feature Alignment', 'Video Clips', 'Self-supervised Learning', 'Random Assignment']",,109,"Humans are able to localize objects in the environment using both visual and auditory cues, integrating information from multiple modalities into a common reference frame. We introduce a system that can leverage unlabeled audiovisual data to learn to localize objects (moving vehicles) in a visual reference frame, purely using stereo sound at inference time. Since it is labor-intensive to manually annotate the correspondences between audio and object bounding boxes, we achieve this goal by using the co-occurrence of visual and audio streams in unlabeled videos as a form of self-supervision, without resorting to the collection of ground truth annotations. In particular, we propose a framework that consists of a vision ``teacher'' network and a stereo-sound ``student'' network. During training, knowledge embodied in a well-established visual vehicle detection model is transferred to the audio domain using unlabeled videos as a bridge. At test time, the stereo-sound student network can work independently to perform object localization using just stereo audio and camera meta-data, without any visual input. Experimental results on a newly collected Auditory Vehicles Tracking dataset verify that our proposed approach outperforms several baseline approaches. We also demonstrate that our cross-modal auditory localization approach can assist in the visual localization of moving vehicles under poor lighting conditions."
Self-Supervised Representation Learning From Multi-Domain Data,"Zeyu Feng, Chang Xu, Dacheng Tao","UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia",100.0,australia,0.0,,"We present an information-theoretically motivated constraint for self-supervised representation learning from multiple related domains. In contrast to previous self-supervised learning methods, our approach learns from multiple domains, which has the benefit of decreasing the build-in bias of individual domain, as well as leveraging information and allowing knowledge transfer across multiple domains. The proposed mutual information constraints encourage neural network to extract common invariant information across domains and to preserve peculiar information of each domain simultaneously. We adopt tractable upper and lower bounds of mutual information to make the proposed constraints solvable. The learned representation is more unbiased and robust toward the input images. Extensive experimental results on both multi-domain and large-scale datasets demonstrate the necessity and advantage of multi-domain self-supervised learning with mutual information constraints. Representations learned in our framework on state-of-the-art methods achieve improved performance than those learned on a single domain.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Feng_Self-Supervised_Representation_Learning_From_Multi-Domain_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_Self-Supervised_Representation_Learning_From_Multi-Domain_Data_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008251/,"['Task analysis', 'Training', 'Mutual information', 'Semantics', 'Supervised learning', 'Neural networks', 'Picture archiving and communication systems']","['Representation Learning', 'Self-supervised Learning', 'Self-supervised Representation Learning', 'Multi-domain Data', 'Neural Network', 'Input Image', 'Knowledge Transfer', 'Mutual Information', 'Large-scale Datasets', 'Single Domain', 'Related Domains', 'Self-supervised Learning Methods', 'Convolutional Neural Network', 'Deep Neural Network', 'Convolutional Layers', 'Unsupervised Learning', 'Number Of Images', 'Multiple Datasets', 'Empirical Distribution', 'Fully-connected Layer', 'Pretext Task', 'Domain-specific Information', 'Linear Classifier', 'Style Image', 'Image X', 'Domain-invariant Representations', 'Image Domain', 'Softplus', 'Domain Generalization', 'Real-world Images']",,25,"We present an information-theoretically motivated constraint for self-supervised representation learning from multiple related domains. In contrast to previous self-supervised learning methods, our approach learns from multiple domains, which has the benefit of decreasing the build-in bias of individual domain, as well as leveraging information and allowing knowledge transfer across multiple domains. The proposed mutual information constraints encourage neural network to extract common invariant information across domains and to preserve peculiar information of each domain simultaneously. We adopt tractable upper and lower bounds of mutual information to make the proposed constraints solvable. The learned representation is more unbiased and robust toward the input images. Extensive experimental results on both multi-domain and large-scale datasets demonstrate the necessity and advantage of multi-domain self-supervised learning with mutual information constraints. Representations learned in our framework on state-of-the-art methods achieve improved performance than those learned on a single domain."
Self-Supervised Representation Learning via Neighborhood-Relational Encoding,"Mohammad Sabokrou, Mohammad Khalooei, Ehsan Adeli",Stanford University; Institute for Research in Fundamental Sciences; Amirkabir University of Tech.,100.0,"Iran, usa",0.0,,"In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhood-relational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classification, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results confirm the performance of our method is better or at least comparable with the state-of-the-art for each specific application, but with a generic and self-supervised approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sabokrou_Self-Supervised_Representation_Learning_via_Neighborhood-Relational_Encoding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sabokrou_Self-Supervised_Representation_Learning_via_Neighborhood-Relational_Encoding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010354/,"['Image reconstruction', 'Task analysis', 'Training', 'Encoding', 'Manifolds', 'Anomaly detection', 'Feature extraction']","['Representation Learning', 'Self-supervised Learning', 'Self-supervised Representation', 'Self-supervised Representation Learning', 'Training Data', 'Deep Network', 'Computer Vision', 'Unsupervised Learning', 'Latent Space', 'Anomaly Detection', 'Neighborhood Information', 'Adversarial Attacks', 'Adversarial Examples', 'Unsupervised Representation', 'Unsupervised Representation Learning', 'Data Manifold', 'Objective Function', 'Hyperparameters', 'Support Vector Machine', 'Classification Task', 'Encoder-decoder Network', 'Pretext Task', 'Image Analysis Tasks', 'Similar Samples', 'Image Classification', 'Neighborhood Relationship', 'Types Of Tasks', 'Variety Of Tasks', 'Unsupervised Feature Learning', 'MNIST Dataset']",,27,"In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhood-relational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classification, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results confirm the performance of our method is better or at least comparable with the state-of-the-art for each specific application, but with a generic and self-supervised approach."
Self-Training With Progressive Augmentation for Unsupervised Cross-Domain Person Re-Identification,"Xinyu Zhang, Jiewei Cao, Chunhua Shen, Mingyu You","The University of Adelaide, Australia; Tongji University, China",100.0,"australia, china",0.0,,"Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function based on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: tinyurl.com/PASTReID",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.pdf,,https://tinyurl.com/PASTReID,,main,Poster,https://ieeexplore.ieee.org/document/9009546/,"['Training', 'Feature extraction', 'Data models', 'Task analysis', 'Clustering methods', 'Training data', 'Adaptation models']","['Progressive Augmentation', 'Loss Function', 'Training Data', 'Objective Function', 'Local Structure', 'Feature Representation', 'Capability Of Model', 'Target Domain', 'Unlabeled Data', 'Classification Layer', 'Source Domain', 'Target Dataset', 'Triplet Loss', 'Local Structure Of Data', 'Training Set', 'Learning Rate', 'Convolutional Neural Network', 'Clustering Method', 'Pedestrian', 'Large-scale Datasets', 'Pseudo Labels', 'Quality Labels', 'Unsupervised Methods', 'Alternative Process', 'Clustering Quality', 'Domain Adaptation', 'Classification Loss', 'Updated Set', 'Structure Distribution', 'Domain Shift']",,157,"Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function based on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: tinyurl.com/PASTReID"
Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection,"Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, Changick Kim",KAIST,100.0,south korea,0.0,,"Deep learning-based object detectors have shown remarkable improvements. However, supervised learning-based methods perform poorly when the train data and the test data have different distributions. To address the issue, domain adaptation transfers knowledge from the label-sufficient domain (source domain) to the label-scarce domain (target domain). Self-training is one of the powerful ways to achieve domain adaptation since it helps class-wise domain adaptation. Unfortunately, a naive approach that utilizes pseudo-labels as ground-truth degenerates the performance due to incorrect pseudo-labels. In this paper, we introduce a weak self-training (WST) method and adversarial background score regularization (BSR) for domain adaptive one-stage object detection. WST diminishes the adverse effects of inaccurate pseudo-labels to stabilize the learning procedure. BSR helps the network extract discriminative features for target backgrounds to reduce the domain shift. Two components are complementary to each other as BSR enhances discrimination between foregrounds and backgrounds, whereas WST strengthen class-wise discrimination. Experimental results show that our approach effectively improves the performance of the one-stage object detection in unsupervised domain adaptation setting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Self-Training_and_Adversarial_Background_Regularization_for_Unsupervised_Domain_Adaptive_One-Stage_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Self-Training_and_Adversarial_Background_Regularization_for_Unsupervised_Domain_Adaptive_One-Stage_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010241/,"['Detectors', 'Object detection', 'Feature extraction', 'Training', 'Reliability', 'Proposals', 'Semantics']","['Object Detection', 'Domain Adaptation', 'One-stage Object Detection', 'Adversarial Regularization', 'Domain Shift', 'Target Domain', 'Remarkable Improvement', 'Source Domain', 'Naive Approach', 'Deep Object Detection', 'Data Sources', 'False Positive', 'False Negative', 'Learning Rate', 'Bounding Box', 'Generative Adversarial Networks', 'Semantic Segmentation', 'Target Data', 'Global Alignment', 'Faster R-CNN', 'Pseudo Labels', 'Image-level Labels', 'Region Proposal Network', 'One-stage Detectors', 'Two-stage Detectors', 'Style Transfer', 'PASCAL VOC', 'Maximum Mean Discrepancy', 'Region Proposal', 'Target Dataset']",,143,"Deep learning-based object detectors have shown remarkable improvements. However, supervised learning-based methods perform poorly when the train data and the test data have different distributions. To address the issue, domain adaptation transfers knowledge from the label-sufficient domain (source domain) to the label-scarce domain (target domain). Self-training is one of the powerful ways to achieve domain adaptation since it helps class-wise domain adaptation. Unfortunately, a naive approach that utilizes pseudo-labels as ground-truth degenerates the performance due to incorrect pseudo-labels. In this paper, we introduce a weak self-training (WST) method and adversarial background score regularization (BSR) for domain adaptive one-stage object detection. WST diminishes the adverse effects of inaccurate pseudo-labels to stabilize the learning procedure. BSR helps the network extract discriminative features for target backgrounds to reduce the domain shift. Two components are complementary to each other as BSR enhances discrimination between foregrounds and backgrounds, whereas WST strengthen class-wise discrimination. Experimental results show that our approach effectively improves the performance of the one-stage object detection in unsupervised domain adaptation setting."
Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers,"Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, Chinmay Hegde",Iowa State University,100.0,USA,0.0,,"Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, fine-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter specific attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier. We propose a novel approach to generate such ""semantic"" adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010394/,"['Semantics', 'Face', 'Manifolds', 'Optimization', 'Perturbation methods', 'Gallium nitride', 'Neural networks']","['Adversarial Attacks', 'Deep Classification', 'Model Parameters', 'Deep Neural Network', 'Binary Classification', 'Input Image', 'Face Images', 'Imperceptible', 'Pixel Spacing', 'Adversarial Examples', 'Adversary Model', 'Attack Methods', 'Optimization Problem', 'Dimensional Space', 'Parameter Space', 'Mixture Model', 'Parameter Vector', 'Generative Adversarial Networks', 'Latent Space', 'Transformation Function', 'Fast Gradient Sign Method', 'Target Model', 'Semantic Properties', 'Input Transformation', 'Neural Classifier', 'Hair Color', 'Types Of Attacks', 'Semantic Constraints', 'Transformer Model', 'Linear Classifier']",,48,"Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, fine-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter specific attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier. We propose a novel approach to generate such ``semantic'' adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples."
Semantic Part Detection via Matching: Learning to Generalize to Novel Viewpoints From Limited Training Data,"Yutong Bai, Qing Liu, Lingxi Xie, Weichao Qiu, Yan Zheng, Alan L. Yuille","Johns Hopkins University; Johns Hopkins University, Huawei Noah’s Ark Lab; University of Texas at Austin",100.0,usa,0.0,,"Detecting semantic parts of an object is a challenging task, particularly because it is hard to annotate semantic parts and construct large datasets. In this paper, we present an approach which can learn from a small annotated dataset containing a limited range of viewpoints and generalize to detect semantic parts for a much larger range of viewpoints. The approach is based on our matching algorithm, which is used for finding accurate spatial correspondence between two images and transplanting semantic parts annotated on one image to the other. Images in the training set are matched to synthetic images rendered from a 3D CAD model, following which a clustering algorithm is used to automatically annotate semantic parts of the CAD model. During the testing period, this CAD model can synthesize annotated images under every viewpoint. These synthesized images are matched to images in the testing set to detect semantic parts in novel viewpoints. Our algorithm is simple, intuitive, and contains very few parameters. Experiments show our method outperforms standard deep learning approaches and, in particular, performs much better on novel viewpoints. For facilitating the future research, code is available: https://github.com/ytongbai/SemanticPartDetection",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bai_Semantic_Part_Detection_via_Matching_Learning_to_Generalize_to_Novel_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bai_Semantic_Part_Detection_via_Matching_Learning_to_Generalize_to_Novel_ICCV_2019_paper.pdf,,https://github.com/ytongbai/SemanticPartDetection,,main,Poster,https://ieeexplore.ieee.org/document/9009062/,"['Semantics', 'Training', 'Testing', 'Solid modeling', 'Three-dimensional displays', 'Machine learning', 'Feature extraction']","['Training Data', 'Semantic Parts', 'Training Set', 'Deep Learning', 'Matching Algorithm', 'Synthetic Images', 'CAD Model', 'Range Of Viewpoints', '3D CAD Models', 'Deep Network', 'Parsing', 'Object Detection', 'Training Images', 'Bounding Box', 'Goal Of This Work', 'Azimuth Angle', 'Testing Stage', 'Amount Of Training Data', 'Feature Matching', 'Semantic Matching', 'Faster R-CNN', 'Matching Quality', 'Similar Viewpoints', 'Pre-trained Network', 'Occlusion Level', 'Pre-trained Deep Network', 'Vehicle Type', 'Virtual Data', 'Objective Components']",,1,"Detecting semantic parts of an object is a challenging task, particularly because it is hard to annotate semantic parts and construct large datasets. In this paper, we present an approach which can learn from a small annotated dataset containing a limited range of viewpoints and generalize to detect semantic parts for a much larger range of viewpoints. The approach is based on our matching algorithm, which is used for finding accurate spatial correspondence between two images and transplanting semantic parts annotated on one image to the other. Images in the training set are matched to synthetic images rendered from a 3D CAD model, following which a clustering algorithm is used to automatically annotate semantic parts of the CAD model. During the testing period, this CAD model can synthesize annotated images under every viewpoint. These synthesized images are matched to images in the testing set to detect semantic parts in novel viewpoints. Our algorithm is simple, intuitive, and contains very few parameters. Experiments show our method outperforms standard deep learning approaches and, in particular, performs much better on novel viewpoints. For facilitating the future research, code is available: https://github.com/ytongbai/SemanticPartDetection."
Semantic Stereo Matching With Pyramid Cost Volumes,"Zhenyao Wu, Xinyi Wu, Xiaoping Zhang, Song Wang, Lili Ju","Wuhan University, China; University of South Carolina, USA; Farsee2 Technology Ltd, China; University of South Carolina, USA",75.0,"china, usa",25.0,China,"The accuracy of stereo matching has been greatly improved by using deep learning with convolutional neural networks. To further capture the details of disparity maps, in this paper, we propose a novel semantic stereo network named SSPCV-Net, which includes newly designed pyramid cost volumes for describing semantic and spatial information on multiple levels. The semantic features are inferred by a semantic segmentation subnetwork while the spatial features are derived by hierarchical spatial pooling. In the end, we design a 3D multi-cost aggregation module to integrate the extracted multilevel features and perform regression for accurate disparity maps. We conduct comprehensive experiments and comparisons with some recent stereo matching networks on Scene Flow, KITTI 2015 and 2012, and Cityscapes benchmark datasets, and the results show that the proposed SSPCV-Net significantly promotes the state-of-the-art stereo-matching performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Semantic_Stereo_Matching_With_Pyramid_Cost_Volumes_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Semantic_Stereo_Matching_With_Pyramid_Cost_Volumes_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010648/,"['Semantics', 'Three-dimensional displays', 'Feature extraction', 'Estimation', 'Image segmentation', 'Computer vision', 'Optical imaging']","['Stereo Matching', 'Cost Volume', 'Convolutional Neural Network', 'Spatial Information', 'Spatial Features', 'Semantic Information', 'Benchmark Datasets', 'Semantic Segmentation', 'Semantic Features', 'Semantic Network', 'Matching Accuracy', '3D Aggregates', 'Disparity Map', '3D Module', 'Image Features', 'Feature Maps', 'Level Of Information', 'Weight Vector', 'Image Pairs', 'Stereo Image Pairs', 'Different Levels Of Information', 'Disparity Estimation', 'Semantic Labels', 'Optical Flow', 'Spatial Intensity', 'Stereo Images', 'Error Metrics', 'Feature Fusion Module', 'Spatial Contextual Information']",,89,"The accuracy of stereo matching has been greatly improved by using deep learning with convolutional neural networks. To further capture the details of disparity maps, in this paper, we propose a novel semantic stereo network named SSPCV-Net, which includes newly designed pyramid cost volumes for describing semantic and spatial information on multiple levels. The semantic features are inferred by a semantic segmentation subnetwork while the spatial features are derived by hierarchical spatial pooling. In the end, we design a 3D multi-cost aggregation module to integrate the extracted multilevel features and perform regression for accurate disparity maps. We conduct comprehensive experiments and comparisons with some recent stereo matching networks on Scene Flow, KITTI 2015 and 2012, and Cityscapes benchmark datasets, and the results show that the proposed SSPCV-Net significantly promotes the state-of-the-art stereo-matching performance."
Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image Retrieval,"Qing Liu, Lingxi Xie, Huiyu Wang, Alan L. Yuille","Johns Hopkins University, Noah’s Ark Lab, Huawei Inc.; Johns Hopkins University",100.0,usa,0.0,,"Sketch-based image retrieval (SBIR) is widely recognized as an important vision problem which implies a wide range of real-world applications. Recently, research interests arise in solving this problem under the more realistic and challenging setting of zero-shot learning. In this paper, we investigate this problem from the viewpoint of domain adaptation which we show is critical in improving feature embedding in the zero-shot scenario. Based on a framework which starts with a pre-trained model on ImageNet and fine-tunes it on the training set of SBIR benchmark, we advocate the importance of preserving previously acquired knowledge, e.g., the rich discriminative features learned from ImageNet, to improve the model's transfer ability. For this purpose, we design an approach named Semantic-Aware Knowledge prEservation (SAKE), which fine-tunes the pre-trained model in an economical way and leverages semantic information, e.g., inter-class relationship, to achieve the goal of knowledge preservation. Zero-shot experiments on two extended SBIR datasets, TU-Berlin and Sketchy, verify the superior performance of our approach. Extensive diagnostic experiments validate that knowledge preserved benefits SBIR in zero-shot settings, as a large fraction of the performance gain is from the more properly structured feature embedding for photo images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Semantic-Aware_Knowledge_Preservation_for_Zero-Shot_Sketch-Based_Image_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Semantic-Aware_Knowledge_Preservation_for_Zero-Shot_Sketch-Based_Image_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010875/,"['Training', 'Task analysis', 'Adaptation models', 'Feature extraction', 'Testing', 'Semantics', 'Knowledge engineering']","['Image Retrieval', 'Sketch-based Image Retrieval', 'Zero-shot Sketch-based Image Retrieval', 'Benchmark', 'Training Set', 'Photographic Images', 'Domain Adaptation', 'Rich Features', 'Deep Network', 'Large Capacity', 'Generative Adversarial Networks', 'Hash Function', 'Ground Truth Labels', 'Target Domain', 'Incremental Learning', 'Binary Code', 'Loss Term', 'Variational Autoencoder', 'Source Domain', 'Catastrophic Forgetting', 'Original Domain', 'Semantic Constraints', 'Teacher Network', 'ImageNet Pre-trained Model', 'WordNet', 'Bias Term', 'Fine-tuning Process', 'Discrete Distribution', 'Training Stage']",,65,"Sketch-based image retrieval (SBIR) is widely recognized as an important vision problem which implies a wide range of real-world applications. Recently, research interests arise in solving this problem under the more realistic and challenging setting of zero-shot learning. In this paper, we investigate this problem from the viewpoint of domain adaptation which we show is critical in improving feature embedding in the zero-shot scenario. Based on a framework which starts with a pre-trained model on ImageNet and fine-tunes it on the training set of SBIR benchmark, we advocate the importance of preserving previously acquired knowledge, e.g., the rich discriminative features learned from ImageNet, to improve the model's transfer ability. For this purpose, we design an approach named Semantic-Aware Knowledge prEservation (SAKE), which fine-tunes the pre-trained model in an economical way and leverages semantic information, e.g., inter-class relationship, to achieve the goal of knowledge preservation. Zero-shot experiments on two extended SBIR datasets, TU-Berlin and Sketchy, verify the superior performance of our approach. Extensive diagnostic experiments validate that knowledge preserved benefits SBIR in zero-shot settings, as a large fraction of the performance gain is from the more properly structured feature embedding for photo images."
Semantic-Transferable Weakly-Supervised Endoscopic Lesions Segmentation,"Jiahua Dong,  Yang Cong,  Gan Sun,  Dongdong Hou","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China",100.0,china,0.0,,"Weakly-supervised learning under image-level labels supervision has been widely applied to semantic segmentation of medical lesions regions. However, 1) most existing models rely on effective constraints to explore the internal representation of lesions, which only produces inaccurate and coarse lesions regions; 2) they ignore the strong probabilistic dependencies between target lesions dataset (e.g., enteroscopy images) and well-to-annotated source diseases dataset (e.g., gastroscope images). To better utilize these dependencies, we present a new semantic lesions representation transfer model for weakly-supervised endoscopic lesions segmentation, which can exploit useful knowledge from relevant fully-labeled diseases segmentation task to enhance the performance of target weakly-labeled lesions segmentation task. More specifically, a pseudo label generator is proposed to leverage seed information to generate highly-confident pseudo pixel labels by incorporating class balance and super-pixel spatial prior. It can iteratively include more hard-to-transfer samples from weakly-labeled target dataset into training set. Afterwards, dynamically-searched feature centroids for same class among different datasets are aligned by accumulating previously-learned features. Meanwhile, adversarial learning is also employed in this paper, to narrow the gap between the lesions among different datasets in output space. Finally, we build a new medical endoscopic dataset with 3659 images collected from more than 1100 volunteers. Extensive experiments on our collected dataset and several benchmark datasets validate the effectiveness of our model.",http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Semantic-Transferable_Weakly-Supervised_Endoscopic_Lesions_Segmentation_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Semantic-Transferable_Weakly-Supervised_Endoscopic_Lesions_Segmentation_ICCV_2019_paper.pdf,,,,,,https://ieeexplore.ieee.org/document/9008842/,"['Semantics', 'Lesions', 'Image segmentation', 'Task analysis', 'Training', 'Biomedical imaging', 'Solid modeling']","['Lesion Segmentation', 'Training Set', 'Centroid', 'Extensive Experiments', 'Benchmark Datasets', 'Generative Adversarial Networks', 'Segmentation Model', 'Semantic Segmentation', 'Esophagogastroduodenoscopy', 'Segmentation Task', 'Semantic Representations', 'Target Lesion', 'Target Dataset', 'Semantic Model', 'Class Balance', 'Medical Datasets', 'Pseudo Labels', 'Label Of Pixel', 'Label Generation', 'Image-level Labels', 'Source Dataset', 'Semantic Knowledge', 'Training Procedure', 'Image Annotation', 'Segmentation Output', 'Knowledge Transfer', 'Softmax Output', 'Intersection Over Union', 'Training Subsets', 'Pixel-level Annotations']",,32,"Weakly-supervised learning under image-level labels supervision has been widely applied to semantic segmentation of medical lesions regions. However, 1) most existing models rely on effective constraints to explore the internal representation of lesions, which only produces inaccurate and coarse lesions regions; 2) they ignore the strong probabilistic dependencies between target lesions dataset (e.g., enteroscopy images) and well-to-annotated source diseases dataset (e.g., gastroscope images). To better utilize these dependencies, we present a new semantic lesions representation transfer model for weakly-supervised endoscopic lesions segmentation, which can exploit useful knowledge from relevant fully-labeled diseases segmentation task to enhance the performance of target weakly-labeled lesions segmentation task. More specifically, a pseudo label generator is proposed to leverage seed information to generate highly-confident pseudo pixel labels by incorporating class balance and super-pixel spatial prior. It can iteratively include more hard-to-transfer samples from weakly-labeled target dataset into training set. Afterwards, dynamically-searched feature centroids for same class among different datasets are aligned by accumulating previously-learned features. Meanwhile, adversarial learning is also employed in this paper, to narrow the gap between the lesions among different datasets in output space. Finally, we build a new medical endoscopic dataset with 3659 images collected from more than 1100 volunteers. Extensive experiments on our collected dataset and several benchmark datasets validate the effectiveness of our model."
SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences,"Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, JÃ¼rgen Gall","University of Bonn, Germany",100.0,germany,0.0,,"Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete 360-degree field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Behley_SemanticKITTI_A_Dataset_for_Semantic_Scene_Understanding_of_LiDAR_Sequences_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Behley_SemanticKITTI_A_Dataset_for_Semantic_Scene_Understanding_of_LiDAR_Sequences_ICCV_2019_paper.pdf,http://www.semantic-kitti.org,,,main,Oral,https://ieeexplore.ieee.org/document/9010727/,"['Three-dimensional displays', 'Semantics', 'Benchmark testing', 'Laser radar', 'Labeling', 'Autonomous automobiles', 'Lasers']","['Light Detection And Ranging', 'Semantic Scene Understanding', 'Point Cloud', 'Semantic Segmentation', 'Self-driving', 'Multiple Scans', 'Odometry', 'Semantic Understanding', 'Point Cloud Segmentation', 'Training Set', 'Laser Scanning', 'Convolutional Neural Network', 'State Of The Art', 'Sparsity', 'Far-field', 'Sequence Annotation', 'RGB Images', 'Depth Information', 'Conditional Random Field', 'Point Cloud Data', 'Voxel Grid', 'KITTI Dataset', 'Labeling Effort', 'Inertial Navigation', 'Point Labels', 'Raw Point Cloud']",,1101,"Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete 360-degree field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions."
Semantics-Enhanced Adversarial Nets for Text-to-Image Synthesis,"Hongchen Tan, Xiuping Liu, Xin Li, Yi Zhang, Baocai Yin","Dalian University of Technology; Dalian University of Technology, Peng Cheng Laboratory; Louisiana State University",100.0,"USA, china",0.0,,"This paper presents a new model, Semantics-enhanced Generative Adversarial Network (SEGAN), for fine-grained text-to-image generation. We introduce two modules, a Semantic Consistency Module (SCM) and an Attention Competition Module (ACM), to our SEGAN. The SCM incorporates image-level semantic consistency into the training of the Generative Adversarial Network (GAN), and can diversify the generated images and improve their structural coherence. A Siamese network and two types of semantic similarities are designed to map the synthesized image and the groundtruth image to nearby points in the latent semantic feature space. The ACM constructs adaptive attention weights to differentiate keywords from unimportant words, and improves the stability and accuracy of SEGAN. Extensive experiments demonstrate that our SEGAN significantly outperforms existing state-of-the-art methods in generating photo-realistic images. All source codes and models will be released for comparative study.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tan_Semantics-Enhanced_Adversarial_Nets_for_Text-to-Image_Synthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Semantics-Enhanced_Adversarial_Nets_for_Text-to-Image_Synthesis_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010053/,"['Semantics', 'Training', 'Generators', 'Synthesizers', 'Generative adversarial networks', 'Gallium nitride', 'Stability analysis']","['Generative Adversarial Networks', 'Semantic Similarity', 'Ground Truth Image', 'Attention Weights', 'Siamese Network', 'Semantic Space', 'Semantic Consistency', 'Image Features', 'Attention Mechanism', 'Challenging Problem', 'Key Words', 'Semantic Information', 'Textual Descriptions', 'Words In Sentences', 'Textual Features', 'Image Synthesis', 'Realistic Images', 'Word Level', 'Contrastive Loss', 'Semantic Structure', 'Text Encoder', 'Fréchet Inception Distance', 'Generative Adversarial Networks Model', 'COCO Dataset', 'Image Encoder', 'Visual Attention Mechanism', 'MS COCO Dataset', 'Noun Phrase', 'Discriminator Loss', 'Semantic Ambiguity']",,60,"This paper presents a new model, Semantics-enhanced Generative Adversarial Network (SEGAN), for fine-grained text-to-image generation. We introduce two modules, a Semantic Consistency Module (SCM) and an Attention Competition Module (ACM), to our SEGAN. The SCM incorporates image-level semantic consistency into the training of the Generative Adversarial Network (GAN), and can diversify the generated images and improve their structural coherence. A Siamese network and two types of semantic similarities are designed to map the synthesized image and the groundtruth image to nearby points in the latent semantic feature space. The ACM constructs adaptive attention weights to differentiate keywords from unimportant words, and improves the stability and accuracy of SEGAN. Extensive experiments demonstrate that our SEGAN significantly outperforms existing state-of-the-art methods in generating photo-realistic images. All source codes and models will be released for comparative study."
Semi-Supervised Domain Adaptation via Minimax Entropy,"Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, Kate Saenko","Boston University; University of California, Berkeley",100.0,usa,0.0,,"Contemporary domain adaptation methods are very effective at aligning feature distributions of source and target domains without any target supervision. However, we show that these techniques perform poorly when even a few labeled examples are available in the target domain. To address this semi-supervised domain adaptation (SSDA) setting, we propose a novel Minimax Entropy (MME) approach that adversarially optimizes an adaptive few-shot model. Our base model consists of a feature encoding network, followed by a classification layer that computes the features' similarity to estimated prototypes (representatives of each class). Adaptation is achieved by alternately maximizing the conditional entropy of unlabeled target data with respect to the classifier and minimizing it with respect to the feature encoder. We empirically demonstrate the superiority of our method over many baselines, including conventional feature alignment and few-shot methods, setting a new state of the art for SSDA. Our code is available at http://cs-people.bu.edu/keisaito/research/MME.html.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Saito_Semi-Supervised_Domain_Adaptation_via_Minimax_Entropy_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Saito_Semi-Supervised_Domain_Adaptation_via_Minimax_Entropy_ICCV_2019_paper.pdf,,http://cs-people.bu.edu/keisaito/research/MME.html,,main,Poster,https://ieeexplore.ieee.org/document/9010425/,"['Feature extraction', 'Prototypes', 'Entropy', 'Task analysis', 'Adaptation models', 'Data mining', 'Computational modeling']","['Domain Adaptation', 'Minimax Entropy', 'Distribution Characteristics', 'Target Domain', 'Unlabeled Data', 'Representative Class', 'Alignment Method', 'Classification Layer', 'Source Domain', 'Feature Encoder', 'Feature Alignment', 'Conditional Entropy', 'Domain Adaptation Methods', 'Unlabeled Target Data', 'Divergence', 'Deep Convolutional Neural Network', 'Weight Vector', 'Generative Adversarial Networks', 'Discriminative Features', 'Maximum Entropy', 'Unlabeled Examples', 'Unsupervised Domain Adaptation Methods', 'Minimum Entropy', 'Linear Layer', 'Target Features', 'Few-shot Learning', 'Adaptation Scenarios', 'Semi-supervised Learning', 'Domain Classifier', 'Extract Discriminative Features']",,360,"Contemporary domain adaptation methods are very effective at aligning feature distributions of source and target domains without any target supervision. However, we show that these techniques perform poorly when even a few labeled examples are available in the target domain. To address this semi-supervised domain adaptation (SSDA) setting, we propose a novel Minimax Entropy (MME) approach that adversarially optimizes an adaptive few-shot model. Our base model consists of a feature encoding network, followed by a classification layer that computes the features' similarity to estimated prototypes (representatives of each class). Adaptation is achieved by alternately maximizing the conditional entropy of unlabeled target data with respect to the classifier and minimizing it with respect to the feature encoder. We empirically demonstrate the superiority of our method over many baselines, including conventional feature alignment and few-shot methods, setting a new state of the art for SSDA. Our code is available at \url{http://cs-people.bu.edu/keisaito/research/MME.html}."
Semi-Supervised Learning by Augmented Distribution Alignment,"Qin Wang, Wen Li, Luc Van Gool",ETH Zurich; ETH Zurich and KU Leuven,100.0,"belgium, switzerland",0.0,,"In this work, we propose a simple yet effective semi-supervised learning approach called Augmented Distribution Alignment. We reveal that an essential sampling bias exists in semi-supervised learning due to the limited number of labeled samples, which often leads to a considerable empirical distribution mismatch between labeled data and unlabeled data. To this end, we propose to align the empirical distributions of labeled and unlabeled data to alleviate the bias. On one hand, we adopt an adversarial training strategy to minimize the distribution distance between labeled and unlabeled data as inspired by domain adaptation works. On the other hand, to deal with the small sample size issue of labeled data, we also propose a simple interpolation strategy to generate pseudo training samples. Those two strategies can be easily implemented into existing deep neural networks. We demonstrate the effectiveness of our proposed approach on the benchmark SVHN and CIFAR10 datasets. Our code is available at https://github.com/qinenergy/adanet .",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Semi-Supervised_Learning_by_Augmented_Distribution_Alignment_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Semi-Supervised_Learning_by_Augmented_Distribution_Alignment_ICCV_2019_paper.pdf,,https://github.com/qinenergy/adanet,,main,Oral,https://ieeexplore.ieee.org/document/9008404/,"['Training', 'Semisupervised learning', 'Supervised learning', 'Interpolation', 'Neural networks', 'Data models', 'Benchmark testing']","['Semi-supervised Learning', 'Distribution Alignment', 'Neural Network', 'Interpolation', 'Deep Neural Network', 'Sampling Bias', 'Empirical Distribution', 'Unlabeled Data', 'Domain Adaptation', 'Adversarial Training', 'Divergence', 'Training Data', 'Deep Learning', 'Error Rate', 'New Perspective', 'Data Augmentation', 'Latent Space', 'Identical Distribution', 'Augmentation Strategy', 'Augmentation Approach', 'Maximum Mean Discrepancy', 'Regular Graphs', 'Intermediate Distribution', 'Classification Error Rate']",,49,"In this work, we propose a simple yet effective semi-supervised learning approach called Augmented Distribution Alignment. We reveal that an essential sampling bias exists in semi-supervised learning due to the limited number of labeled samples, which often leads to a considerable empirical distribution mismatch between labeled data and unlabeled data. To this end, we propose to align the empirical distributions of labeled and unlabeled data to alleviate the bias. On one hand, we adopt an adversarial training strategy to minimize the distribution distance between labeled and unlabeled data as inspired by domain adaptation works. On the other hand, to deal with the small sample size issue of labeled data, we also propose a simple interpolation strategy to generate pseudo training samples. Those two strategies can be easily implemented into existing deep neural networks. We demonstrate the effectiveness of our proposed approach on the benchmark SVHN and CIFAR10 datasets. Our code is available at https://github.com/qinenergy/adanet ."
Semi-Supervised Monocular 3D Face Reconstruction With End-to-End Shape-Preserved Domain Transfer,"Jingtan Piao, Chen Qian, Hongsheng Li","CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong Sensetime Research",100.0,"Hong Kong, china",0.0,,"Monocular face reconstruction is a challenging task in computer vision, which aims to recover 3D face geometry from a single RGB face image. Recently, deep learning based methods have achieved great improvements on monocular face reconstruction. However, for deep learning-based methods to reach optimal performance, it is paramount to have large-scale training images with ground-truth 3D face geometry, which is generally difficult for human to annotate. To tackle this problem, we propose a semi-supervised monocular reconstruction method, which jointly optimizes a shape-preserved domain-transfer CycleGAN and a shape estimation network. The framework is semi-supervised trained with 3D rendered images with ground-truth shapes and in-the-wild face images without any extra annotation. The CycleGAN network transforms all realistic images to have the rendered style and is end-to-end trained within the overall framework. This is the key difference compared with existing CycleGAN-based learning methods, which just used CycleGAN as a separate training sample generator. Novel landmark consistency loss and edge-aware shape estimation loss are proposed for our two networks to jointly solve the challenging face reconstruction problem. Extensive experiments on public face reconstruction datasets demonstrate the effectiveness of our overall method as well as the individual components.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Piao_Semi-Supervised_Monocular_3D_Face_Reconstruction_With_End-to-End_Shape-Preserved_Domain_Transfer_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Piao_Semi-Supervised_Monocular_3D_Face_Reconstruction_With_End-to-End_Shape-Preserved_Domain_Transfer_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008260/,"['Face', 'Shape', 'Three-dimensional displays', 'Image reconstruction', 'Geometry', 'Training', 'Estimation']","['Oral And Maxillofacial Surgery', 'Domain Transfer', '3D Face', '3D Face Reconstruction', 'Monocular 3D Face Reconstruction', 'Separate Samples', 'Face Images', 'Joint Optimization', 'Estimation Network', '3D Geometry', 'Realistic Images', 'Consistency Loss', 'Semi-supervised Methods', 'Shape Estimation', 'Ground Truth 3D', 'Training Data', 'Input Image', 'Generative Adversarial Networks', 'Image Generation', '3D Shape', 'Synthetic Images', 'Facial Shape', 'Real Domain', 'Yaw Angle', 'Edge Points', '3D Landmarks', 'Style Image', 'Optimization-based Methods', '3D Point']",,25,"Monocular face reconstruction is a challenging task in computer vision, which aims to recover 3D face geometry from a single RGB face image. Recently, deep learning based methods have achieved great improvements on monocular face reconstruction. However, for deep learning-based methods to reach optimal performance, it is paramount to have large-scale training images with ground-truth 3D face geometry, which is generally difficult for human to annotate. To tackle this problem, we propose a semi-supervised monocular reconstruction method, which jointly optimizes a shape-preserved domain-transfer CycleGAN and a shape estimation network. The framework is semi-supervised trained with 3D rendered images with ground-truth shapes and in-the-wild face images without any extra annotation. The CycleGAN network transforms all realistic images to have the rendered style and is end-to-end trained within the overall framework. This is the key difference compared with existing CycleGAN-based learning methods, which just used CycleGAN as a separate training sample generator. Novel landmark consistency loss and edge-aware shape estimation loss are proposed for our two networks to jointly solve the challenging face reconstruction problem. Extensive experiments on public face reconstruction datasets demonstrate the effectiveness of our overall method as well as the individual components."
Semi-Supervised Pedestrian Instance Synthesis and Detection With Mutual Reinforcement,"Si Wu, Sihao Lin, Wenhao Wu, Mohamed Azzam, Hau-San Wong","Department of Computer Science, City University of Hong Kong; School of Computer Science and Engineering, South China University of Technology",100.0,"Hong Kong, china",0.0,,"We propose a GAN-based scene-specific instance synthesis and classification model for semi-supervised pedestrian detection. Instead of collecting unreliable detections from unlabeled data, we adopt a class-conditional GAN for synthesizing pedestrian instances to alleviate the problem of insufficient labeled data. With the help of a base detector, we integrate pedestrian instance synthesis and detection by including a post-refinement classifier (PRC) into a minimax game. A generator and the PRC can mutually reinforce each other by synthesizing high-fidelity pedestrian instances and providing more accurate categorical information. Both of them compete with a class-conditional discriminator and a class-specific discriminator, such that the four fundamental networks in our model can be jointly trained. In our experiments, we validate that the proposed model significantly improves the performance of the base detector and achieves state-of-the-art results on multiple benchmarks. As shown in Figure 1, the result indicates the possibility of using inexpensively synthesized instances for improving semi-supervised detection models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Semi-Supervised_Pedestrian_Instance_Synthesis_and_Detection_With_Mutual_Reinforcement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Semi-Supervised_Pedestrian_Instance_Synthesis_and_Detection_With_Mutual_Reinforcement_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009099/,"['Detectors', 'Generators', 'Feature extraction', 'Task analysis', 'Training', 'Proposals', 'Games']","['Pedestrian Detection', 'Pedestrian Instances', 'Detection Model', 'Generative Adversarial Networks', 'Unlabeled Data', 'Basic Detection', 'Semi-supervised Model', 'Minimax Game', 'Fundamental Network', 'Loss Function', 'Convolutional Neural Network', 'Deep Neural Network', 'Percentage Points', 'Feature Maps', 'Object Detection', 'Weighting Factor', 'Class Labels', 'Bounding Box', 'Latent Space', 'Semantic Segmentation', 'Faster R-CNN', 'Fr√©chet Inception Distance', 'Unlabeled Instances', 'Adversarial Training', 'Improve Detection Performance', 'Optimal Formulation', 'Backbone Network', 'Synthesis Quality', 'Region Proposal Network', 'Test Dataset']",,6,"We propose a GAN-based scene-specific instance synthesis and classification model for semi-supervised pedestrian detection. Instead of collecting unreliable detections from unlabeled data, we adopt a class-conditional GAN for synthesizing pedestrian instances to alleviate the problem of insufficient labeled data. With the help of a base detector, we integrate pedestrian instance synthesis and detection by including a post-refinement classifier (PRC) into a minimax game. A generator and the PRC can mutually reinforce each other by synthesizing high-fidelity pedestrian instances and providing more accurate categorical information. Both of them compete with a class-conditional discriminator and a class-specific discriminator, such that the four fundamental networks in our model can be jointly trained. In our experiments, we validate that the proposed model significantly improves the performance of the base detector and achieves state-of-the-art results on multiple benchmarks. As shown in Figure 1, the result indicates the possibility of using inexpensively synthesized instances for improving semi-supervised detection models."
Semi-Supervised Skin Detection by Network With Mutual Guidance,"Yi He, Jiayuan Shi, Chuan Wang, Haibin Huang, Jiaming Liu, Guanbin Li, Risheng Liu, Jue Wang",Dalian University of Technology; Sun Yat-sen University; Megvii Technology,66.66666666666666,"China, china",33.33333333333334,China,"We present a new data-driven method for robust skin detection from a single human portrait image. Unlike previous methods, we incorporate human body as a weak semantic guidance into this task, considering acquiring large-scale of human labeled skin data is commonly expensive and time-consuming. To be specific, we propose a dual-task neural network for joint detection of skin and body via a semi-supervised learning strategy. The dual-task network contains a shared encoder but two decoders for skin and body separately. For each decoder, its output also serves as a guidance for its counterpart, making both decoders mutually guided. Extensive experiments were conducted to demonstrate the effectiveness of our network with mutual guidance, and experimental results show our network outperforms the state-of-the-art in skin detection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_Semi-Supervised_Skin_Detection_by_Network_With_Mutual_Guidance_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Semi-Supervised_Skin_Detection_by_Network_With_Mutual_Guidance_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010854/,"['Skin', 'Task analysis', 'Decoding', 'Image color analysis', 'Feature extraction', 'Neural networks', 'Training']","['Mutual Guidance', 'Neural Network', 'Neural Network For Detection', 'New Method For Detection', 'Training Data', 'Stage 2', 'Feature Maps', 'Skin Color', 'Cross-entropy Loss', 'Training Strategy', 'Detection Task', 'Threshold Method', 'RGB Images', 'Color Space', 'Gaussian Mixture Model', 'Multi-task Learning', 'Unbalanced Dataset', 'Joint Learning', 'Guidance Information', 'Image Segmentation Tasks', 'Acceptance Of The Paper', 'Strong Supervision', 'Label Consistency', 'Traditional Machine Learning Techniques', 'Deep Neural Network', 'Multilayer Perceptron', 'Background Pixels', 'Learning Algorithms', 'Similar Color', 'Training Details']",,16,"We present a new data-driven method for robust skin detection from a single human portrait image. Unlike previous methods, we incorporate human body as a weak semantic guidance into this task, considering acquiring large-scale of human labeled skin data is commonly expensive and time-consuming. To be specific, we propose a dual-task neural network for joint detection of skin and body via a semi-supervised learning strategy. The dual-task network contains a shared encoder but two decoders for skin and body separately. For each decoder, its output also serves as a guidance for its counterpart, making both decoders mutually guided. Extensive experiments were conducted to demonstrate the effectiveness of our network with mutual guidance, and experimental results show our network outperforms the state-of-the-art in skin detection."
Seq-SG2SL: Inferring Semantic Layout From Scene Graph Through Sequence to Sequence Learning,"Boren Li, Boyu Zhuang, Mingyang Li, Jian Gu",Alibaba AI Labs; Unknown,0.0,,100.0,China,"Generating semantic layout from scene graph is a crucial intermediate task connecting text to image. We present a conceptually simple, flexible and general framework using sequence to sequence (seq-to-seq) learning for this task. The framework, called Seq-SG2SL, derives sequence proxies for the two modality and a Transformer-based seq-to-seq model learns to transduce one into the other. A scene graph is decomposed into a sequence of semantic fragments (SF), one for each relationship. A semantic layout is represented as the consequence from a series of brick-action code segments (BACS), dictating the position and scale of each object bounding box in the layout. Viewing the two building blocks, SF and BACS, as corresponding terms in two different vocabularies, a seq-to-seq model is fittingly used to translate. A new metric, semantic layout evaluation understudy (SLEU), is devised to evaluate the task of semantic layout prediction inspired by BLEU. SLEU defines relationships within a layout as unigrams and looks at the spatial distribution for n-grams. Unlike the binary precision of BLEU, SLEU allows for some tolerances spatially through thresholding the Jaccard Index and is consequently more adapted to the task. Experimental results on the challenging Visual Genome dataset show improvement over a non-sequential approach based on graph convolution.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Seq-SG2SL_Inferring_Semantic_Layout_From_Scene_Graph_Through_Sequence_to_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Seq-SG2SL_Inferring_Semantic_Layout_From_Scene_Graph_Through_Sequence_to_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008234/,"['Layout', 'Semantics', 'Task analysis', 'Visualization', 'Measurement', 'Image generation', 'Graphical models']","['Sequence Learning', 'Scene Graph', 'Semantic Layout', 'Intersection Over Union', 'Bounding Box', 'Graph Convolution', 'Unigram', 'Object Bounding Boxes', 'Training Set', 'Blueprint', 'Attention Mechanism', 'Challenging Problem', 'Sequence Pairs', 'Object Classification', 'Transformer Model', 'Visual Object', 'Subject And Object', 'Machine Translation', 'Conditional Random Field', 'Classification Index', 'Beam Search', 'Visual Relationship', 'Positional Encoding', 'Absolute Coordinates', 'Subset Of Pairs', 'Corpus Size', 'Sequence Of Nodes']",,7,"Generating semantic layout from scene graph is a crucial intermediate task connecting text to image. We present a conceptually simple, flexible and general framework using sequence to sequence (seq-to-seq) learning for this task. The framework, called Seq-SG2SL, derives sequence proxies for the two modality and a Transformer-based seq-to-seq model learns to transduce one into the other. A scene graph is decomposed into a sequence of semantic fragments (SF), one for each relationship. A semantic layout is represented as the consequence from a series of brick-action code segments (BACS), dictating the position and scale of each object bounding box in the layout. Viewing the two building blocks, SF and BACS, as corresponding terms in two different vocabularies, a seq-to-seq model is fittingly used to translate. A new metric, semantic layout evaluation understudy (SLEU), is devised to evaluate the task of semantic layout prediction inspired by BLEU. SLEU defines relationships within a layout as unigrams and looks at the spatial distribution for n-grams. Unlike the binary precision of BLEU, SLEU allows for some tolerances spatially through thresholding the Jaccard Index and is consequently more adapted to the task. Experimental results on the challenging Visual Genome dataset show improvement over a non-sequential approach based on graph convolution."
Sequence Level Semantics Aggregation for Video Object Detection,"Haiping Wu, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang",TuSimple; University of Chinese Academy of Sciences; McGill University,100.0,"canada, china, usa",0.0,,"Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Sequence_Level_Semantics_Aggregation_for_Video_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Sequence_Level_Semantics_Aggregation_for_Video_Object_Detection_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009547/,"['Proposals', 'Feature extraction', 'Semantics', 'Object detection', 'Optical imaging', 'Detectors', 'Adaptive optics']","['Object Detection', 'Sequence Level', 'Video Object Detection', 'Neural Network', 'Recurrent Neural Network', 'Video Frames', 'Optical Flow', 'Feature Aggregation', 'Fast Motion', 'Spectral Clustering', 'Post-processing Methods', 'Aggregation Module', 'Deep Neural Network', 'Data Augmentation', 'Transition Probabilities', 'Deep Convolutional Neural Network', 'Bounding Box', 'Temporal Information', 'Detection In Images', 'Semantic Similarity', 'Still Images', 'Backbone Network', 'Post-processing Techniques', 'Detection In Videos', 'Faster R-CNN', 'Transition Probability Matrix', 'Object Appearance', 'Optical Flow Estimation', 'Region Proposal Network', 'Bounding Box Regression']",,155,"Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean."
Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry,"Shunkai Li, Fei Xue, Xin Wang, Zike Yan, Hongbin Zha","PKU-SenseTime Machine Vision Joint Lab; Key Laboratory of Machine Perception (MOE), School of EECS, Peking University",50.0,china,50.0,China,"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Sequential_Adversarial_Learning_for_Self-Supervised_Deep_Visual_Odometry_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Sequential_Adversarial_Learning_for_Self-Supervised_Deep_Visual_Odometry_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008265/,"['Feature extraction', 'Correlation', 'Pose estimation', 'Three-dimensional displays', 'Optical imaging', 'Optical variables control']","['Generative Adversarial Networks', 'Visual Odometry', 'Sequence Information', 'Single Image', 'Consecutive Frames', 'Depth Estimation', 'Self-supervised Learning', 'Structure From Motion', 'Relative Pose', 'Accurate Depth', 'KITTI Dataset', 'Warped Image', 'Self-supervised Task', 'Convolutional Neural Network', 'Convolutional Layers', 'Short-term Memory', 'Long Short-term Memory', 'Recurrent Neural Network', 'Depth Map', 'Pose Estimation', 'Simultaneous Localization And Mapping', 'Optical Flow', 'Accuracy Of Pose Estimation', 'View Synthesis', 'Bundle Adjustment', 'Self-supervised Manner', 'Loop Closure', 'Extract High-level Features', 'Accurate Pose']",,47,"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly."
Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning,"Jyoti Aneja, Harsh Agrawal, Dhruv Batra, Alexander Schwing","University of Illinois, Urbana-Champaign; Georgia Institute of Technology",100.0,usa,0.0,,"Diverse and accurate vision+language modeling is an important goal to retain creative freedom and maintain user engagement. However, adequately capturing the intricacies of diversity in language models is challenging. Recent works commonly resort to latent variable models augmented with more or less supervision from object detectors or part-of-speech tags. In common to all those methods is the fact that the latent variable either only initializes the sentence generation process or is identical across the steps of generation. Both methods offer no fine-grained control. To address this concern, we propose Seq-CVAE which learns a latent space for every word. We encourage this temporal latent space to capture the 'intention' about how to complete the sentence by mimicking a representation which summarizes the future. We illustrate the efficacy of the proposed approach on the challenging MSCOCO dataset, significantly improving diversity metrics compared to baselines while performing on par w.r.t. sentence quality.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010960/,"['Decoding', 'Measurement', 'Training', 'Aerospace electronics', 'Probability distribution', 'Gallium nitride', 'Controllability']","['Latent Space', 'Image Captioning', 'Latent Variables', 'Object Detection', 'Diversity Metrics', 'Language Model', 'Postage', 'Word Position', 'Text Generation', 'Time Step', 'Recurrent Neural Network', 'Multilayer Perceptron', 'Generative Adversarial Networks', 'Image Representation', 'Word Embedding', 'Accuracy Metrics', 'Variational Autoencoder', 'ImageNet Dataset', 'Individual Words', 'Representation Of Individuals', 'Beam Search', 'Latent Vector', 'Decoder Network', 'Image Embedding', 'Backward Pass', 'Entire Sentence', 'Latent Distribution', 'Hidden Representation', 'Part Of The Sentence']",,35,"Diverse and accurate vision+language modeling is an important goal to retain creative freedom and maintain user engagement. However, adequately capturing the intricacies of diversity in language models is challenging. Recent works commonly resort to latent variable models augmented with more or less supervision from object detectors or part-of-speech tags. In common to all those methods is the fact that the latent variable either only initializes the sentence generation process or is identical across the steps of generation. Both methods offer no fine-grained control. To address this concern, we propose Seq-CVAE which learns a latent space for every word. We encourage this temporal latent space to capture the 'intention' about how to complete the sentence by mimicking a representation which summarizes the future. We illustrate the efficacy of the proposed approach on the challenging MSCOCO dataset, significantly improving diversity metrics compared to baselines while performing on par w.r.t. sentence quality."
Shadow Removal via Shadow Image Decomposition,"Hieu Le, Dimitris Samaras","Stony Brook University, New York, 11794, USA",100.0,usa,0.0,,"We propose a novel deep learning method for shadow removal. Inspired by physical models of shadow formation, we use a linear illumination transformation to model the shadow effects in the image that allows the shadow image to be expressed as a combination of the shadow-free image, the shadow parameters, and a matte layer. We use two deep networks, namely SP-Net and M-Net, to predict the shadow parameters and the shadow matte respectively. This system allows us to remove the shadow effects on the images. We train and test our framework on the most challenging shadow removal dataset (ISTD). Compared to the state-of-the-art method, our model achieves a 40% error reduction in terms of root mean square error (RMSE) for the shadow area, reducing RMSE from 13.3 to 7.9. Moreover, we create an augmented ISTD dataset based on an image decomposition system by modifying the shadow parameters to generate new synthetic shadow images. Training our model on this new augmented ISTD dataset further lowers the RMSE on the shadow area to 7.4.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008769/,"['Lighting', 'Image color analysis', 'Image decomposition', 'Machine learning', 'Computational modeling', 'Training', 'Image reconstruction']","['Image Decomposition', 'Shadow Images', 'Shadow Removal', 'Root Mean Square Error', 'Deep Learning', 'Deep Network', 'Effective Imaging', 'Removal Method', 'Shadowing Effect', 'Linear Model', 'Model Parameters', 'Scaling Factor', 'Single Image', 'Early Work', 'Linear System', 'Color Channels', 'Penumbra', 'Set Of Datasets', 'Illumination Changes', 'Additive Constant', 'Direct Illumination']",,107,"We propose a novel deep learning method for shadow removal. Inspired by physical models of shadow formation, we use a linear illumination transformation to model the shadow effects in the image that allows the shadow image to be expressed as a combination of the shadow-free image, the shadow parameters, and a matte layer. We use two deep networks, namely SP-Net and M-Net, to predict the shadow parameters and the shadow matte respectively. This system allows us to remove the shadow effects on the images. We train and test our framework on the most challenging shadow removal dataset (ISTD). Compared to the state-of-the-art method, our model achieves a 40% error reduction in terms of root mean square error (RMSE) for the shadow area, reducing RMSE from 13.3 to 7.9. Moreover, we create an augmented ISTD dataset based on an image decomposition system by modifying the shadow parameters to generate new synthetic shadow images. Training our model on this new augmented ISTD dataset further lowers the RMSE on the shadow area to 7.4."
Shape Reconstruction Using Differentiable Projections and Deep Priors,"Matheus Gadelha, Rui Wang, Subhransu Maji","University of Massachusetts, Amherst",100.0,usa,0.0,,"We investigate the problem of reconstructing shapes from noisy and incomplete projections in the presence of viewpoint uncertainities. The problem is cast as an optimization over the shape given measurements obtained by a projection operator and a prior. We present differentiable projection operators for a number of reconstruction problems which when combined with the deep image prior or shape prior allows efficient inference through gradient descent. We apply our method on a variety of reconstruction problems, such as tomographic reconstruction from a few samples, visual hull reconstruction incorporating view uncertainties, and 3D shape reconstruction from noisy depth maps. Experimental results show that our approach is effective for such shape reconstruction problems, without requiring any task-specific training.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gadelha_Shape_Reconstruction_Using_Differentiable_Projections_and_Deep_Priors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gadelha_Shape_Reconstruction_Using_Differentiable_Projections_and_Deep_Priors_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010698/,"['Shape', 'Image reconstruction', 'Three-dimensional displays', 'Noise measurement', 'Task analysis', 'Transforms', 'Bayes methods']","['Shape Reconstruction', 'Deep Prior', 'Gradient Descent', 'Image Reconstruction', 'Depth Map', '3D Shape', 'Projection Operator', 'Shape Priors', 'Neural Network', 'Denoising', 'Markov Chain Monte Carlo', 'Stochastic Gradient Descent', 'Line-of-sight', 'Natural Images', 'Binary Image', 'Inverse Problem', 'Depth Images', 'Maximum A Posteriori', 'Noisy Measurements', '3D Convolution', 'Voxel Grid', 'Occupancy Grid', 'Radon Transform', 'Filtered Back Projection', 'Volumetric Reconstruction', 'Reconstruction Task', 'PSNR Values', 'Projective Measurements', 'Convolutional Network', 'Bayesian Inference']",,26,"We investigate the problem of reconstructing shapes from noisy and incomplete projections in the presence of viewpoint uncertainities. The problem is cast as an optimization over the shape given measurements obtained by a projection operator and a prior. We present differentiable projection operators for a number of reconstruction problems which when combined with the deep image prior or shape prior allows efficient inference through gradient descent. We apply our method on a variety of reconstruction problems, such as tomographic reconstruction from a few samples, visual hull reconstruction incorporating view uncertainties, and 3D shape reconstruction from noisy depth maps. Experimental results show that our approach is effective for such shape reconstruction problems, without requiring any task-specific training."
Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images,"Junbang Liang, Ming C. Lin","University of Maryland, College Park",100.0,usa,0.0,,"We propose a scalable neural network framework to reconstruct the 3D mesh of a human body from multi-view images, in the subspace of the SMPL model. Use of multi-view images can significantly reduce the projection ambiguity of the problem, increasing the reconstruction accuracy of the 3D human body under clothing. Our experiments show that this method benefits from the synthetic dataset generated from our pipeline since it has good flexibility of variable control and can provide ground-truth for validation. Our method outperforms existing methods on real-world images, especially on shape estimations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liang_Shape-Aware_Human_Pose_and_Shape_Reconstruction_Using_Multi-View_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_Shape-Aware_Human_Pose_and_Shape_Reconstruction_Using_Multi-View_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010281/,"['Shape', 'Clothing', 'Biological system modeling', 'Estimation', 'Training', 'Three-dimensional displays', 'Data models']","['Human Pose', 'Multi-view Images', 'Neural Network', 'Real-world Images', 'Shape Estimation', '3D Body', 'Convolutional Neural Network', 'Deep Neural Network', 'Image Features', 'Input Image', 'Number Of Images', 'Shape Parameter', 'Body Shape', 'Viewing Angle', 'Joint Position', 'Pose Estimation', 'Human Motion', 'Hausdorff Distance', '3D Pose', 'Global Rotation', 'Human Body Shape', 'Pose Parameters', 'Mesh Representation', 'Body Pose', 'Ground Truth Pose', 'Computer Animation', 'Real-world Datasets', 'Online Image', 'Camera Calibration', 'Skin Color']",,49,"We propose a scalable neural network framework to reconstruct the 3D mesh of a human body from multi-view images, in the subspace of the SMPL model. Use of multi-view images can significantly reduce the projection ambiguity of the problem, increasing the reconstruction accuracy of the 3D human body under clothing. Our experiments show that this method benefits from the synthetic dataset generated from our pipeline since it has good flexibility of variable control and can provide ground-truth for validation. Our method outperforms existing methods on real-world images, especially on shape estimations."
ShapeMask: Learning to Segment Novel Objects by Refining Shape Priors,"Weicheng Kuo, Anelia Angelova, Jitendra Malik, Tsung-Yi Lin","Google Brain; University of California, Berkeley",50.0,usa,50.0,USA,"Instance segmentation aims to detect and segment individual objects in a scene. Most existing methods rely on precise mask annotations of every category. However, it is difficult and costly to segment objects in novel categories because a large number of mask annotations is required. We introduce ShapeMask, which learns the intermediate concept of object shape to address the problem of generalization in instance segmentation to novel categories. ShapeMask starts with a bounding box detection and gradually refines it by first estimating the shape of the detected object through a collection of shape priors. Next, ShapeMask refines the coarse shape into an instance level mask by learning instance embeddings. The shape priors provide a strong cue for object-like prediction, and the instance embeddings model the instance specific appearance information. ShapeMask significantly outperforms the state-of-the-art by 6.4 and 3.8 AP when learning across categories, and obtains competitive performance in the fully supervised setting. It is also robust to inaccurate detections, decreased model capacity, and small training data. Moreover, it runs efficiently with 150ms inference time on a GPU and trains within 11 hours on TPUs. With a larger backbone model, ShapeMask increases the gap with state-of-the-art to 9.4 and 6.2 AP across categories. Code will be publicly available at: https://sites.google.com/view/shapemask/home.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kuo_ShapeMask_Learning_to_Segment_Novel_Objects_by_Refining_Shape_Priors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kuo_ShapeMask_Learning_to_Segment_Novel_Objects_by_Refining_Shape_Priors_ICCV_2019_paper.pdf,https://sites.google.com/view/shapemask/home,,,main,Oral,https://ieeexplore.ieee.org/document/9009070/,"['Shape', 'Image segmentation', 'Training', 'Feature extraction', 'Predictive models', 'Task analysis', 'Robots']","['Shape Priors', 'Training Data', 'Bounding Box', 'Object Shape', 'Inference Time', 'Objects In The Scene', 'Instance Segmentation', 'Detection Boxes', 'Small Training Data', 'Training Set', 'Image Features', 'Training Time', 'Similar Shape', 'Transfer Learning', 'Target Object', 'Feature Pyramid', 'Shape Estimation', 'Mask R-CNN', 'COCO Dataset', 'Object Instances', 'Bounding Box Annotations', 'Subset Of Categories']",,81,"Instance segmentation aims to detect and segment individual objects in a scene. Most existing methods rely on precise mask annotations of every category. However, it is difficult and costly to segment objects in novel categories because a large number of mask annotations is required. We introduce ShapeMask, which learns the intermediate concept of object shape to address the problem of generalization in instance segmentation to novel categories. ShapeMask starts with a bounding box detection and gradually refines it by first estimating the shape of the detected object through a collection of shape priors. Next, ShapeMask refines the coarse shape into an instance level mask by learning instance embeddings. The shape priors provide a strong cue for object-like prediction, and the instance embeddings model the instance specific appearance information. ShapeMask significantly outperforms the state-of-the-art by 6.4 and 3.8 AP when learning across categories, and obtains competitive performance in the fully supervised setting. It is also robust to inaccurate detections, decreased model capacity, and small training data. Moreover, it runs efficiently with 150ms inference time on a GPU and trains within 11 hours on TPUs. With a larger backbone model, ShapeMask increases the gap with state-of-the-art to 9.4 and 6.2 AP across categories. Code will be publicly available at: https://sites.google.com/view/shapemask/home."
Shapeglot: Learning Language for Shape Differentiation,"Panos Achlioptas, Judy Fan, Robert Hawkins, Noah Goodman, Leonidas J. Guibas",Stanford University,100.0,usa,0.0,,"In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on 2D and/or 3D object representations. We first build a large scale, carefully controlled dataset of human utterances each of which refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are capable of zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit supervision on such parts), and that transfer to novel classes is most successful when known part-related words are available. This work illustrates a practical approach to language grounding, and provides a novel case study in the relationship between object shape and linguistic structure when it comes to object differentiation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Achlioptas_Shapeglot_Learning_Language_for_Shape_Differentiation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Achlioptas_Shapeglot_Learning_Language_for_Shape_Differentiation_ICCV_2019_paper.pdf,https://ai.stanford.edu/~optas/shapeglot,,,main,Poster,https://ieeexplore.ieee.org/document/9010248/,"['Three-dimensional displays', 'Solid modeling', 'Shape', 'Task analysis', 'Games', 'Two dimensional displays', 'Pragmatics']","['Listening', 'Transfer Learning', 'Object Classification', 'Object Shape', 'Object Parts', 'Human Partner', '3D CAD Models', 'Attention Mechanism', 'Image Object', '3D Point Cloud', 'Test Split', 'Communicative Context', 'Single Part', 'Image Captioning', 'Training Domain', 'Latent Code', 'Human Listeners', 'Code Vector', 'Geometry And Topology', 'Performance Of Listeners']",,42,"In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on 2D and/or 3D object representations. We first build a large scale, carefully controlled dataset of human utterances each of which refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are capable of zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit supervision on such parts), and that transfer to novel classes is most successful when known part-related words are available. This work illustrates a practical approach to language grounding, and provides a novel case study in the relationship between object shape and linguistic structure when it comes to object differentiation."
Sharpen Focus: Learning With Attention Separability and Consistency,"Lezi Wang, Ziyan Wu, Srikrishna Karanam, Kuan-Chuan Peng, Rajat Vikram Singh, Bo Liu, Dimitris N. Metaxas","Siemens Corporate Technology, Princeton NJ; Rutgers University, New Brunswick NJ",50.0,usa,50.0,USA,"Recent developments in gradient-based attention modeling have seen attention maps emerge as a powerful tool for interpreting convolutional neural networks. Despite good localization for an individual class of interest, these techniques produce attention maps with substantially overlapping responses among different classes, leading to the problem of visual confusion and the need for discriminative attention. In this paper, we address this problem by means of a new framework that makes class-discriminative attention a principled part of the learning process. Our key innovations include new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual confusion. Extensive experiments on image classification benchmarks show the effectiveness of our approach in terms of improved classification accuracy, including CIFAR-100 (+3.33%), Caltech-256 (+1.64%), ImageNet (+0.92%), CUB-200-2011 (+4.8%) and PASCAL VOC2012 (+5.73%).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Sharpen_Focus_Learning_With_Attention_Separability_and_Consistency_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Sharpen_Focus_Learning_With_Attention_Separability_and_Consistency_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9011002/,"['Visualization', 'Training', 'Computational modeling', 'Predictive models', 'Tools', 'Convolutional neural networks', 'Benchmark testing']","['Learning Process', 'Convolutional Neural Network', 'Image Classification', 'Learning Objectives', 'Attention Map', 'Need For Attention', 'Convolutional Layers', 'Feature Maps', 'Target Region', 'Inner Layer', 'Qualitative Results', 'Attention Mechanism', 'Multi-label', 'Semantic Information', 'Class Probabilities', 'Target Object', 'Classism', 'Background Pixels', 'Improve Classification Performance', 'Positive Gradient', 'Ground-truth Class', 'Supervisory Signal', 'Higher-order Derivatives', 'Fine-grained Image']",,19,"Recent developments in gradient-based attention modeling have seen attention maps emerge as a powerful tool for interpreting convolutional neural networks. Despite good localization for an individual class of interest, these techniques produce attention maps with substantially overlapping responses among different classes, leading to the problem of visual confusion and the need for discriminative attention. In this paper, we address this problem by means of a new framework that makes class-discriminative attention a principled part of the learning process. Our key innovations include new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual confusion. Extensive experiments on image classification benchmarks show the effectiveness of our approach in terms of improved classification accuracy, including CIFAR-100 (+3.33%), Caltech-256 (+1.64%), ImageNet (+0.92%), CUB-200-2011 (+4.8%) and PASCAL VOC2012 (+5.73%)."
ShellNet: Efficient Point Cloud Convolutional Neural Networks Using Concentric Shells Statistics,"Zhiyuan Zhang, Binh-Son Hua, Sai-Kit Yeung",Hong Kong University of Science and Technology; The University of Tokyo; Singapore University of Technology and Design,100.0,"Hong Kong, Japan, singapore",0.0,,"Deep learning with 3D data has progressed significantly since the introduction of convolutional neural networks that can handle point order ambiguity in point cloud data. While being able to achieve good accuracies in various scene understanding tasks, previous methods often have low training speed and complex network architecture. In this paper, we address these problems by proposing an efficient end-to-end permutation invariant convolution for point cloud deep learning. Our simple yet effective convolution operator named ShellConv uses statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform on such features. Based on ShellConv we further build an efficient neural network named ShellNet to directly consume the point clouds with larger receptive fields while maintaining less layers. We demonstrate the efficacy of ShellNet by producing state-of-the-art results on object classification, object part segmentation, and semantic scene segmentation while keeping the network very fast to train.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.pdf,https://hkust-vgd.github.io/shellnet/,,,main,Oral,https://ieeexplore.ieee.org/document/9010996/,"['Three-dimensional displays', 'Convolution', 'Neural networks', 'Machine learning', 'Task analysis', 'Image segmentation', 'Semantics']","['Neural Network', 'Convolutional Neural Network', 'Point Cloud', 'Network Efficiency', 'Deep Learning', 'Receptive Field', 'Convolution Operation', 'Object Classification', 'Semantic Segmentation', '3D Data', 'Training Speed', 'Object Parts', 'Part Segmentation', 'Point Cloud Data', 'Spherical Shell', 'Scene Understanding', 'Efficient Neural Network', 'Hierarchical Structure', 'Local Features', 'Feature Learning', 'Shell Size', 'Feature Points', '3D Point Cloud', 'Representative Points', 'Neighboring Points', 'Input Point', 'Feature Channels', 'Global Features', 'Concentric Spheres', '3D Scene']",,223,"Deep learning with 3D data has progressed significantly since the introduction of convolutional neural networks that can handle point order ambiguity in point cloud data. While being able to achieve good accuracies in various scene understanding tasks, previous methods often have low training speed and complex network architecture. In this paper, we address these problems by proposing an efficient end-to-end permutation invariant convolution for point cloud deep learning. Our simple yet effective convolution operator named ShellConv uses statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform on such features. Based on ShellConv we further build an efficient neural network named ShellNet to directly consume the point clouds with larger receptive fields while maintaining less layers. We demonstrate the efficacy of ShellNet by producing state-of-the-art results on object classification, object part segmentation, and semantic scene segmentation while keeping the network very fast to train."
Siamese Networks: The Tale of Two Manifolds,"Soumava Kumar Roy, Mehrtash Harandi, Richard Nock, Richard Hartley","Monash University; DATA61-CSIRO, Australia; The Australian National University",66.66666666666666,"Australia, australia",33.33333333333334,Australia,"Siamese networks are non-linear deep models that have found their ways into a broad set of problems in learning theory, thanks to their embedding capabilities. In this paper, we study Siamese networks from a new perspective and question the validity of their training procedure. We show that in the majority of cases, the objective of a Siamese network is endowed with an invariance property. Neglecting the invariance property leads to a hindrance in training the Siamese networks. To alleviate this issue, we propose two Riemannian structures and generalize a well-established accelerated stochastic gradient descent method to take into account the proposed Riemannian structures. Our empirical evaluations suggest that by making use of the Riemannian geometry, we achieve state-of-the-art results against several algorithms for the challenging problem of fine-grained image classification.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010055/,"['Training', 'Manifolds', 'Geometry', 'Measurement', 'Face', 'Computer vision', 'Optimization']","['Siamese Network', 'Stochastic Gradient Descent', 'Riemannian Manifold', 'Invariance Property', 'Use Of Geometry', 'Deep Learning', 'Factorization', 'Convolutional Layers', 'Singular Value Decomposition', 'Latent Space', 'Dense Clusters', 'Classification Loss', 'Positive Semidefinite Matrix', 'Positive Semidefinite', 'Metric Learning', 'Tangent Space', 'Horizontal Spacing', 'Vertical Space', 'QR Decomposition', 'Backpropagation Algorithm', 'Deep Metric Learning', 'Sylvester Equation', 'Triplet Formation', 'Geometry Of Space', 'Orthogonal Group', 'Neural Network', 'Spectral Clustering', 'MNIST Dataset', 'Contrastive Loss']",,28,"Siamese networks are non-linear deep models that have found their ways into a broad set of problems in learning theory, thanks to their embedding capabilities. In this paper, we study Siamese networks from a new perspective and question the validity of their training procedure. We show that in the majority of cases, the objective of a Siamese network is endowed with an invariance property. Neglecting the invariance property leads to a hindrance in training the Siamese networks. To alleviate this issue, we propose two Riemannian structures and generalize a well-established accelerated stochastic gradient descent method to take into account the proposed Riemannian structures. Our empirical evaluations suggest that by making use of the Riemannian geometry, we achieve state-of-the-art results against several algorithms for the challenging problem of fine-grained image classification."
Significance-Aware Information Bottleneck for Domain Adaptive Semantic Segmentation,"Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang","School of Computer Science & Technology, Huazhong University of Science & Technology; ReLER, University of Technology Sydney; Baidu Research; Center of Network and Computation, Huazhong University of Science & Technology",75.0,"China, australia",25.0,China,"For unsupervised domain adaptation problems, the strategy of aligning the two domains in latent feature space through adversarial learning has achieved much progress in image classification, but usually fails in semantic segmentation tasks in which the latent representations are overcomplex. In this work, we equip the adversarial network with a ""significance-aware information bottleneck (SIB)"", to address the above problem. The new network structure, called SIBAN, enables a significance-aware feature purification before the adversarial adaptation, which eases the feature alignment and stabilizes the adversarial training course. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method can yield leading results compared with other feature-space alternatives. Moreover, SIBAN can even match the state-of-the-art output-space methods in segmentation accuracy, while the latter are often considered to be better choices for domain adaptive segmentation task.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Significance-Aware_Information_Bottleneck_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Significance-Aware_Information_Bottleneck_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010846/,"['Task analysis', 'Semantics', 'Feature extraction', 'Image segmentation', 'Training', 'Mutual information', 'Data mining']","['Semantic Segmentation', 'Domain Adaptation', 'Adaptive Segmentation', 'Information Bottleneck', 'Domain Adaptation For Semantic Segmentation', 'Domain Adaptive Segmentation', 'Generative Adversarial Networks', 'Latent Space', 'Version Of Task', 'Segmentation Accuracy', 'Segmentation Task', 'Latent Representation', 'Latent Features', 'Adversarial Training', 'Feature Alignment', 'Semantic Segmentation Task', 'Convolutional Neural Network', 'Convolutional Layers', 'Mutual Information', 'Segmentation Method', 'Source Domain', 'Adversarial Domain Adaptation', 'Domain Adaptation Methods', 'Target Domain', 'Pixel Spacing', 'Information Constraints', 'Variational Autoencoder', 'Domain-invariant Features', 'Domain Shift', 'Line Of Work']",,128,"For unsupervised domain adaptation problems, the strategy of aligning the two domains in latent feature space through adversarial learning has achieved much progress in image classification, but usually fails in semantic segmentation tasks in which the latent representations are overcomplex. In this work, we equip the adversarial network with a “significance-aware information bottleneck (SIB)”, to address the above problem. The new network structure, called SIBAN, enables a significance-aware feature purification before the adversarial adaptation, which eases the feature alignment and stabilizes the adversarial training course. In two domain adaptation tasks, i.e., GTA5 → Cityscapes and SYNTHIA → Cityscapes, we validate that the proposed method can yield leading results compared with other feature-space alternatives. Moreover, SIBAN can even match the state-of-the-art output-space methods in segmentation accuracy, while the latter are often considered to be better choices for domain adaptive segmentation task."
Similarity-Preserving Knowledge Distillation,"Frederick Tung, Greg Mori","Simon Fraser University, Borealis AI",100.0,canada,0.0,,"Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010328/,"['Knowledge engineering', 'Training', 'Neural networks', 'Machine learning', 'Semantics', 'Computer vision', 'Task analysis']","['Similarity-preserving Knowledge Distillation', 'Neural Network', 'Similar Activity', 'Network Training', 'Representation Of Space', 'Pairwise Similarity', 'Teacher Network', 'Input Pair', 'Student Network', 'Network Compression', 'Distillation Loss', 'Neural Compression', 'Deep Learning', 'Deep Network', 'Data Augmentation', 'Transfer Learning', 'ImageNet', 'Noisy Data', 'Activation Maps', 'Spatial Attention', 'Source Domain', 'Semi-supervised Learning', 'Incremental Learning', 'Spatial Attention Map', 'Nesterov Momentum', 'Limited Training Data', 'Specific Hardware', 'Supervisory Signal', 'Catastrophic Forgetting', 'Spatial Transfer']",,568,"Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."
Simultaneous Multi-View Instance Detection With Learned Geometric Soft-Constraints,"Ahmed Samy Nassar, SÃ©bastien LefÃ¨vre, Jan Dirk Wegner","IRISA, Université Bretagne Sud; EcoVision Lab, Photogrammetry and Remote Sensing group, ETH Zurich",100.0,"France, switzerland",0.0,,"We propose to jointly learn multi-view geometry and warping between views of the same object instances for robust cross-view object detection. What makes multi-view object instance detection difficult are strong changes in viewpoint, lighting conditions, high similarity of neighbouring objects, and strong variability in scale. By turning object detection and instance re-identification in different views into a joint learning task, we are able to incorporate both image appearance and geometric soft constraints into a single, multi-view detection process that is learnable end-to-end. We validate our method on a new, large data set of street-level panoramas of urban objects and show superior performance compared to various baselines. Our contribution is threefold: a large-scale, publicly available data set for multi-view instance detection and re-identification; an annotation tool custom-tailored for multi-view instance detection; and a novel, holistic multi-view instance detection and re-identification method that jointly models geometry and appearance across views.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nassar_Simultaneous_Multi-View_Instance_Detection_With_Learned_Geometric_Soft-Constraints_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nassar_Simultaneous_Multi-View_Instance_Detection_With_Learned_Geometric_Soft-Constraints_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009529/,"['Cameras', 'Object detection', 'Task analysis', 'Detectors', 'Vegetation', 'Geometry', 'Pose estimation']","['Simultaneous Detection', 'Soft Constraints', 'Object Detection', 'Scale Variation', 'Strong Changes', 'Image Appearance', 'Object Instances', 'Viewpoint Changes', 'Re-identification Methods', 'Bounding Box', 'Scale Changes', 'Pose Estimation', 'Individual Objects', 'Imaging Evidence', 'Camera Pose', 'Object Appearance', 'Relative Pose', 'Predicted Bounding Box', 'Panoramic Images', 'Single Shot Detector', 'Candidate Boxes', 'Pose Information', 'Google Street View', 'Labeling Tool', 'Instance Labels', 'Radius Of The Earth', 'Tree Detection', 'Re-identification Task', 'Warping Function']",,18,"We propose to jointly learn multi-view geometry and warping between views of the same object instances for robust cross-view object detection. What makes multi-view object instance detection difficult are strong changes in viewpoint, lighting conditions, high similarity of neighbouring objects, and strong variability in scale. By turning object detection and instance re-identification in different views into a joint learning task, we are able to incorporate both image appearance and geometric soft constraints into a single, multi-view detection process that is learnable end-to-end. We validate our method on a new, large data set of street-level panoramas of urban objects and show superior performance compared to various baselines. Our contribution is threefold: a large-scale, publicly available data set for multi-view instance detection and re-identification; an annotation tool custom-tailored for multi-view instance detection; and a novel, holistic multi-view instance detection and re-identification method that jointly models geometry and appearance across views."
SinGAN: Learning a Generative Model From a Single Natural Image,"Tamar Rott Shaham, Tali Dekel, Tomer Michaeli",Technion; Google Research,50.0,israel,50.0,USA,"We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008787/,"['Training', 'Gallium nitride', 'Task analysis', 'Generators', 'Image resolution', 'Image reconstruction', 'Computational modeling']","['Single Image', 'Natural Images', 'Single Natural Image', 'Aspect Ratio', 'Diverse Sample', 'User Study', 'Training Images', 'Global Structure', 'Image Scale', 'Wide Range Of Tasks', 'Fine Texture', 'Distribution Of Patches', 'Root Mean Square Error', 'Super-resolution', 'Image Object', 'Receptive Field', 'Scale Effect', 'Image Patches', 'Fr√©chet Inception Distance', 'Noise Map', 'Image Editing', 'Fake Images', 'Multi-scale Architecture', 'Inception Distance', 'Similar Architecture', 'Image X', 'Number Of Scales', 'Reconstruction Loss']",,515,"We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks."
Single-Network Whole-Body Pose Estimation,"Gines Hidalgo, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, Hanbyul Joo, Tomas Simon, Yaser Sheikh",Carnegie Mellon University; RetailNext; Facebook AI Research,33.33333333333333,usa,66.66666666666667,Canada,"We present the first single-network approach for 2D whole-body pose estimation, which entails simultaneous localization of body, face, hands, and feet keypoints. Due to the bottom-up formulation, our method maintains constant real-time performance regardless of the number of people in the image. The network is trained in a single stage using multi-task learning, through an improved architecture which can handle scale differences between body/foot and face/hand keypoints. Our approach considerably improves upon OpenPose [??], the only work so far capable of whole-body pose estimation, both in terms of speed and global accuracy. Unlike OpenPose, our method does not need to run an additional network for each hand and face candidate, making it substantially faster for multi-person scenarios. This work directly results in a reduction of computational complexity for applications that require 2D whole-body information (e.g., VR/AR, re-targeting). In addition, it yields higher accuracy, especially for occluded, blurry, and low resolution faces and hands. For code, trained models, and validation benchmarks, visit our project page: https://github.com/CMU-Perceptual-Computing-Lab/openpose_train.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hidalgo_Single-Network_Whole-Body_Pose_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hidalgo_Single-Network_Whole-Body_Pose_Estimation_ICCV_2019_paper.pdf,,https://github.com/CMU-Perceptual-Computing-Lab/openpose_train,,main,Poster,https://ieeexplore.ieee.org/document/9010752/,"['Face', 'Pose estimation', 'Task analysis', 'Detectors', 'Two dimensional displays', 'Three-dimensional displays']","['Pose Estimation', 'Whole-body Pose', 'Whole-body Pose Estimation', 'Number Of People', 'Low Resolution', 'Estimation Approach', 'Single Stage', 'Multi-task Learning', 'Images Of People', 'Global Accuracy', 'Convolutional Neural Network', '3D Reconstruction', 'Receptive Field', 'Bounding Box', 'Test Speed', 'Manual Annotation', 'Single Person', '3D Mesh', 'Mean Average Precision', 'Probability Ratio', 'Face Dataset', 'Face Detection', 'Keypoint Detection', 'Confidence Map', 'Bipartite Matching', 'Input Resolution', 'Face Alignment', 'Controlled Laboratory Environment', 'Subset Of Images', 'Non-maximum Suppression']",,25,"We present the first single-network approach for 2D~whole-body pose estimation, which entails simultaneous localization of body, face, hands, and feet keypoints. Due to the bottom-up formulation, our method maintains constant real-time performance regardless of the number of people in the image. The network is trained in a single stage using multi-task learning, through an improved architecture which can handle scale differences between body/foot and face/hand keypoints. Our approach considerably improves upon OpenPose~\cite{cao2018openpose}, the only work so far capable of whole-body pose estimation, both in terms of speed and global accuracy. Unlike OpenPose, our method does not need to run an additional network for each hand and face candidate, making it substantially faster for multi-person scenarios. This work directly results in a reduction of computational complexity for applications that require 2D whole-body information (e.g., VR/AR, re-targeting). In addition, it yields higher accuracy, especially for occluded, blurry, and low resolution faces and hands. For code, trained models, and validation benchmarks, visit our project page: https://github.com/CMU-Perceptual-Computing-Lab/openpose_train."
Single-Stage Multi-Person Pose Machines,"Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, Shuicheng Yan","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Yitu Technology; Department of Electrical and Computer Engineering, National University of Singapore, Singapore",100.0,"china, singapore",0.0,,"Multi-person pose estimation is a challenging problem. Existing methods are mostly two-stage based-one stage for proposal generation and the other for allocating poses to corresponding persons. However, such two-stage methods generally suffer low efficiency. In this work, we present the first single-stage model, Single-stage multi-person Pose Machine (SPM), to simplify the pipeline and lift the efficiency for multi-person pose estimation. To achieve this, we propose a novel Structured Pose Representation (SPR) that unifies person instance and body joint position representations. Based on SPR, we develop the SPM model that can directly predict structured poses for multiple persons in a single stage, and thus offer a more compact pipeline and attractive efficiency advantage over two-stage methods. In particular, SPR introduces the root joints to indicate different person instances and human body joint positions are encoded into their displacements w.r.t. the roots. To better predict long-range displacements for some joints, SPR is further extended to hierarchical representations. Based on SPR, SPM can efficiently perform multi-person poses estimation by simultaneously predicting root joints (location of instances) and body joint displacements via CNNs. Moreover, to demonstrate the generality of SPM, we also apply it to multi-person 3D pose estimation. Comprehensive experiments on benchmarks MPII, extended PASCAL-Person-Part, MSCOCO and CMU Panoptic clearly demonstrate the state-of-the-art efficiency of SPM for multi-person 2D/3D pose estimation, together with outstanding accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010416/,"['Pose estimation', 'Three-dimensional displays', 'Two dimensional displays', 'Pipelines', 'Periodic structures', 'Detectors', 'Wrist']","['Convolutional Neural Network', 'Joint Position', 'Two-stage Method', 'Pose Estimation', 'Single Stage', 'Antisocial Personality Disorder', 'Body Joints', 'Human Pose Estimation', '3D Pose', 'Single-stage Model', 'Joint Displacement', 'Learning Rate', 'Factorization', 'Scaling Factor', 'Feature Maps', 'Qualitative Results', 'Data Augmentation', 'Average Precision', 'Local Image', '3D Coordinates', 'Confidence Map', 'Human Pose', 'MS COCO Dataset', 'Bottom-up Model', 'Top-down Strategies', 'Conventional Representation', '2D Case', 'Displacement Maps', 'Joint Information']",,160,"Multi-person pose estimation is a challenging problem. Existing methods are mostly two-stage based-one stage for proposal generation and the other for allocating poses to corresponding persons. However, such two-stage methods generally suffer low efficiency. In this work, we present the first single-stage model, Single-stage multi-person Pose Machine (SPM), to simplify the pipeline and lift the efficiency for multi-person pose estimation. To achieve this, we propose a novel Structured Pose Representation (SPR) that unifies person instance and body joint position representations. Based on SPR, we develop the SPM model that can directly predict structuredposesfor multiple persons in a single stage, and thus offer a more compact pipeline and attractive efficiency advantage over two-stage methods. In particular, SPR introduces the root joints to indicate different person instances and human body joint positions are encoded into their displacements w.r.t. the roots. To better predict long-range displacements for some joints, SPR is further extended to hierarchical representations. Based on SPR, SPM can efficiently perform multi-person poses estimation by simultaneously predicting root joints (location of instances) and body joint displacements via CNNs. Moreover, to demonstrate the generality of SPM, we also apply it to multi-person 3D pose estimation. Comprehensive experiments on benchmarks MPII, extended PASCAL-PersonPart, MSCOCO and CMU Panoptic clearly demonstrate the state-of-the-art efficiency of SPM for multi-person 2D/3D pose estimation, together with outstanding accuracy."
Situational Fusion of Visual Representation for Visual Navigation,"William B. Shen, Danfei Xu, Yuke Zhu, Leonidas J. Guibas, Li Fei-Fei, Silvio Savarese","Stanford University, Facebook AI Research; Stanford University",100.0,usa,0.0,,"A complex visual navigation task puts an agent in different situations which call for a diverse range of visual perception abilities. For example, to ""go to the nearest chair"", the agent might need to identify a chair in a living room using semantics, follow along a hallway using vanishing point cues, and avoid obstacles using depth. Therefore, utilizing the appropriate visual perception abilities based on a situational understanding of the visual environment can empower these navigation models in unseen visual environments. We propose to train an agent to fuse a large set of visual representations that correspond to diverse visual perception abilities. To fully utilize each representation, we develop an action-level representation fusion scheme, which predicts an action candidate from each representation and adaptively consolidate these action candidates into the final action. Furthermore, we employ a data-driven inter-task affinity regularization to reduce redundancies and improve generalization. Our approach leads to a significantly improved performance in novel environments over ImageNet-pretrained baseline and other fusion methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Situational_Fusion_of_Visual_Representation_for_Visual_Navigation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Situational_Fusion_of_Visual_Representation_for_Visual_Navigation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009052/,"['Task analysis', 'Visualization', 'Navigation', 'Adaptation models', 'Visual perception', 'Robots', 'Robustness']","['Visual Representation', 'Machine Vision', 'Visual Perception', 'Visual Task', 'Fusion Method', 'Fusion Strategy', 'Visual Ability', 'Set Of Representations', 'Navigation Task', 'Final Action', 'Navigation Model', 'Unseen Environments', 'Activity Levels', 'Deep Learning', 'Deep Network', 'Individual Performance', 'Simulation Environment', 'Representation Learning', 'Test Environment', 'Semantic Segmentation', 'Feature-level Fusion', 'Fusion Weights', 'Deep Reinforcement Learning', 'Vision Tasks', 'Lack Of Robustness', 'Use Of Representations', 'Semantic Task', 'RGB Images', 'Beginning Of This Section', 'Majority Voting']",,40,"A complex visual navigation task puts an agent in different situations which call for a diverse range of visual perception abilities. For example, to ""go to the nearest chair'', the agent might need to identify a chair in a living room using semantics, follow along a hallway using vanishing point cues, and avoid obstacles using depth. Therefore, utilizing the appropriate visual perception abilities based on a situational understanding of the visual environment can empower these navigation models in unseen visual environments. We propose to train an agent to fuse a large set of visual representations that correspond to diverse visual perception abilities. To fully utilize each representation, we develop an action-level representation fusion scheme, which predicts an action candidate from each representation and adaptively consolidate these action candidates into the final action. Furthermore, we employ a data-driven inter-task affinity regularization to reduce redundancies and improve generalization. Our approach leads to a significantly improved performance in novel environments over ImageNet-pretrained baseline and other fusion methods."
Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds,"Haiyong Jiang, Jianfei Cai, Jianmin Zheng","Faculty of Information Technology, Monash University; Nanyang Technological University, Singapore",100.0,"Singapore, australia",0.0,,"This work addresses the problem of 3D human shape reconstruction from point clouds. Considering that human shapes are of high dimensions and with large articulations, we adopt the state-of-the-art parametric human body model, SMPL, to reduce the dimension of learning space and generate smooth and valid reconstruction. However, SMPL parameters, especially pose parameters, are not easy to learn because of ambiguity and locality of the pose representation. Thus, we propose to incorporate skeleton awareness into the deep learning based regression of SMPL parameters for 3D human shape reconstruction. Our basic idea is to use the state-of-the-art technique PointNet++ to extract point features, and then map point features to skeleton joint features and finally to SMPL parameters for the reconstruction from point clouds. Particularly, we develop an end-to-end framework, where we propose a graph aggregation module to augment PointNet++ by extracting better point features, an attention module to better map unordered point features into ordered skeleton joint features, and a skeleton graph module to extract better joint features for SMPL parameter regression. The entire framework network is first trained in an end-to-end manner on synthesized dataset, and then online fine-tuned on unseen dataset with unsupervised loss to bridges gaps between training and testing. The experiments on multiple datasets show that our method is on par with the state-of-the-art solution.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008280/,"['Three-dimensional displays', 'Feature extraction', 'Shape', 'Image reconstruction', 'Skeleton', 'Convolution', 'Solid modeling']","['Point Cloud', 'Human Shape', '3D Human Shape', 'Deep Learning', 'Human Model', '3D Reconstruction', 'Entire Network', 'Attention Module', 'Feature Points', '3D Shape', 'Joint Feature', 'Reconstruction Parameters', 'Aggregation Module', 'Human Body Model', 'Pose Parameters', 'Neural Network', 'Feature Learning', 'Shape Parameter', '3D Analysis', 'Powerful Feature', 'Spectral Convolution', 'Graph Convolution', 'Multilayer Perception', '3D Mesh', 'Pose Estimation', 'Powerful Feature Extraction', 'Human Pose Estimation', 'Input Point Cloud', 'Graph Convolutional Network', 'Human Pose']",,50,"This work addresses the problem of 3D human shape reconstruction from point clouds. Considering that human shapes are of high dimensions and with large articulations, we adopt the state-of-the-art parametric human body model, SMPL, to reduce the dimension of learning space and generate smooth and valid reconstruction. However, SMPL parameters, especially pose parameters, are not easy to learn because of ambiguity and locality of the pose representation. Thus, we propose to incorporate skeleton awareness into the deep learning based regression of SMPL parameters for 3D human shape reconstruction. Our basic idea is to use the state-of-the-art technique PointNet++ to extract point features, and then map point features to skeleton joint features and finally to SMPL parameters for the reconstruction from point clouds. Particularly, we develop an end-to-end framework, where we propose a graph aggregation module to augment PointNet++ by extracting better point features, an attention module to better map unordered point features into ordered skeleton joint features, and a skeleton graph module to extract better joint features for SMPL parameter regression. The entire framework network is first trained in an end-to-end manner on synthesized dataset, and then online fine-tuned on unseen dataset with unsupervised loss to bridges gaps between training and testing. The experiments on multiple datasets show that our method is on par with the state-of-the-art solution."
SkyScapes Â­ Fine-Grained Semantic Understanding of Aerial Scenes,"Seyed Majid Azimi, Corentin Henry, Lars Sommer, Arne Schumann, Eleonora Vig","German Aerospace Center (DLR), Wessling, Germany; Fraunhofer IOSB, Karlsruhe, Germany",50.0,germany,50.0,Germany,"Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring, and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications. To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the baselines in region outlines and level of detail on both tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Azimi_SkyScapes__Fine-Grained_Semantic_Understanding_of_Aerial_Scenes_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Azimi_SkyScapes__Fine-Grained_Semantic_Understanding_of_Aerial_Scenes_ICCV_2019_paper.pdf,https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760,,,main,Poster,https://ieeexplore.ieee.org/document/9010920/,"['Semantics', 'Roads', 'Task analysis', 'Buildings', 'Benchmark testing', 'Image edge detection', 'Vegetation']","['Aerial Scene', 'Segmentation Method', 'Aerial Images', 'Urban Management', 'Semantic Segmentation', 'Edge Detection', 'Urban Infrastructure', 'Wide Range Of Scales', 'Multi-task Model', 'Infrastructure Monitoring', 'Lane Markings', 'Wide Range Of Classes', 'Aerial Image Dataset', 'Validation Set', 'Fine Structure', 'State Of The Art', 'Urban Planning', 'Intersection Over Union', 'Receptive Field', 'Small Objects', 'Auxiliary Task', 'Advanced Driver Assistance Systems', 'Mean Intersection Over Union', 'Impervious Surface', 'Low Vegetation', 'Dangerous Areas', 'Separate Layers', 'PASCAL VOC', 'Qualitative Examples']",,34,"Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring, and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications. To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the baselines in region outlines and level of detail on both tasks."
SlowFast Networks for Video Recognition,"Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He",Facebook AI Research (FAIR),0.0,,100.0,USA,"We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/SlowFast,,main,Oral,https://ieeexplore.ieee.org/document/9008780/,"['Spatiotemporal phenomena', 'Spatial resolution', 'Semantics', 'Image color analysis', 'Optical imaging', 'Biomedical optical imaging', 'Channel capacity']","['Video Recognition', 'SlowFast Network', 'Frame Rate', 'Action Classes', 'Channel Capacity', 'High Frame Rate', 'Lower Frame', 'Fast Pathway', 'High-resolution', 'Spatiotemporal', 'Temporal Dimension', 'Spatial Dimensions', 'Optical Flow', 'Ablation Experiments', 'Mean Average Precision', 'Spatial Details', 'Convolutional Model', 'Fast Motion', 'Training Videos', 'Lateral Connections', 'Action Detection', 'Validation Videos', 'Temporal Axis', 'Ground-truth Box', 'ImageNet Pretraining', 'Temporal Convolution', 'Video Modeling', 'Spatial Resolution', 'Random Initialization']",,1995,"We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast."
Small Steps and Giant Leaps: Minimal Newton Solvers for Deep Learning,"JoÃ£o F. Henriques, Sebastien Ehrhardt, Samuel Albanie, Andrea Vedaldi","Visual Geometry Group, University of Oxford",100.0,uk,0.0,,"We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement. Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network. This estimate has the same size and is similar to the momentum variable that is commonly used in  SGD . No estimate of the Hessian is maintained. We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle. We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. We also show our optimiser's generality by testing on a large set of randomly generated architectures.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Henriques_Small_Steps_and_Giant_Leaps_Minimal_Newton_Solvers_for_Deep_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Henriques_Small_Steps_and_Giant_Leaps_Minimal_Newton_Solvers_for_Deep_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008232/,"['Machine learning', 'Optimization', 'Jacobian matrices', 'Computational modeling', 'Linear programming', 'Stochastic processes', 'Convergence']","['Deep Learning', 'Gradient Descent', 'Stochastic Gradient Descent', 'Hyperparameter Tuning', 'Conjugate Gradient', 'Hessian Matrix', 'Second-order Method', 'Automatic Differentiation', 'Learning Rate', 'Convolutional Neural Network', 'Deep Network', 'Taylor Expansion', 'Regularization Term', 'Newton Method', 'Stochastic Optimization', 'Training Error', 'Global Convergence', 'Line Search', 'Validation Error', 'RMSprop', 'First-order Method', 'Fisher Information Matrix', 'Descent Direction', 'Wall-clock Time', 'Logistic Loss', 'AdaGrad', 'Natural Gradient', 'Basic Convolutional Neural Network', 'Neural Network', 'Deep Neural Network']",,3,"We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement. Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network. This estimate has the same size and is similar to the momentum variable that is commonly used in {SGD}. No estimate of the Hessian is maintained. We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle. We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. We also show our optimiser's generality by testing on a large set of randomly generated architectures."
Soft Rasterizer: A Differentiable Renderer for Image-Based 3D Reasoning,"Shichen Liu, Tianye Li, Weikai Chen, Hao Li",USC Institute for Creative Technologies; University of Southern California; Pinscreen; USC Institute for Creative Technologies; University of Southern California; USC Institute for Creative Technologies,83.33333333333334,usa,16.666666666666657,USA,"Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers. Code is available at https://github.com/ShichenLiu/SoftRas.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.pdf,,https://github.com/ShichenLiu/SoftRas,,main,Oral,https://ieeexplore.ieee.org/document/9008817/,"['Three-dimensional displays', 'Rendering (computer graphics)', 'Two dimensional displays', 'Cognition', 'Standards', 'Image reconstruction', 'Task analysis']","['Differentiable Rendering', 'Soft Rasterizer', 'Differentiable Function', '3D Reconstruction', '2D Images', 'Gradient Flow', 'Aggregation Function', 'Triangular Mesh', 'Supervision Signal', 'Mesh Vertices', 'Shape Fitting', 'Neural Network', 'Input Image', 'Single Image', 'Image Plane', 'Parametrized', 'Intersection Over Union', 'Probability Function', 'Target Image', 'Differencing', 'Color Map', '3D Shape', 'Extrinsic Variables', 'Mesh Generation', 'Screen Space', 'Pose Estimation', 'Z Coordinates', '3D Properties', 'Forward Pass', 'Relative Depth']",,397,"Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers. Code is available at https://github.com/ShichenLiu/SoftRas."
SoftTriple Loss: Deep Metric Learning Without Triplet Sampling,"Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, Rong Jin","School of Engineering and Technology, University of Washington, Tacoma, WA, 98402, USA; Alibaba Group, Hangzhou, China; Alibaba Group, Bellevue, WA, 98004, USA",33.33333333333333,usa,66.66666666666667,China,"Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008816/,"['Measurement', 'Task analysis', 'Neural networks', 'Principal component analysis', 'Optimization', 'Benchmark testing', 'Data models']","['Deep Learning', 'Metric Learning', 'Deep Metric Learning', 'Deep Neural Network', 'Single Center', 'Real-world Data', 'Local Clustering', 'Multiple Centers', 'Embedding Learning', 'Triplet Loss', 'Fine-grained Data', 'Class Center', 'Softmax Loss', 'Conventional Deep Learning', 'Learning Performance', 'Stochastic Gradient Descent', 'Benchmark Datasets', 'Unit Length', 'Number Of Centers', 'Handcrafted Features', 'Number Of Triplets', 'Central Set', 'Embedding Dimension', 'Rest For Testing', 'Original Space', 'Similar Definition', 'Dimensionality Of The Input Space', 'Intra-class Variance', 'Positive Semidefinite', 'Image Retrieval']",,238,"Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function."
Solving Vision Problems via Filtering,"Sean I. Young, Aous T. Naman, Bernd Girod, David Taubman",Stanford University; University of New South Wales,100.0,"australia, usa",0.0,,"We propose a new, filtering approach for solving a large number of regularized inverse problems commonly found in computer vision. Traditionally, such problems are solved by finding the solution to the system of equations that expresses the first-order optimality conditions of the problem. This can be slow if the system of equations is dense due to the use of nonlocal regularization, necessitating iterative solvers such as successive over-relaxation or conjugate gradients. In this paper, we show that similar solutions can be obtained more easily via filtering, obviating the need to solve a potentially dense system of equations using slow iterative methods. Our filtered solutions are very similar to the true ones, but often up to 10 times faster to compute.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Young_Solving_Vision_Problems_via_Filtering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Young_Solving_Vision_Problems_via_Filtering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009478/,['Optimized production technology'],"['Visual Impairment', 'System Of Equations', 'Computer Vision', 'Conjugate Gradient', 'Inverse Problem', 'Successful Conjugation', 'First-order Optimality Conditions', 'Denoising', 'General Case', 'Unique Solution', 'Filtering Method', 'Optical Flow', 'Laplacian Matrix', 'Visual Problems', 'Problem Instances', 'Flow Estimation', 'Least Squares Problem', 'Graph Laplacian', 'Imaging Problem', 'Optical Flow Estimation', 'Disparity Estimation', 'Image Inpainting', 'Image Deblurring', 'Bilateral Filter', 'Blur Kernel', 'Inverse Covariance Matrix', 'Depth Map', 'Estimation Problem', 'Shortest Path']",,2,"We propose a new, filtering approach for solving a large number of regularized inverse problems commonly found in computer vision. Traditionally, such problems are solved by finding the solution to the system of equations that expresses the first-order optimality conditions of the problem. This can be slow if the system of equations is dense due to the use of nonlocal regularization, necessitating iterative solvers such as successive over-relaxation or conjugate gradients. In this paper, we show that similar solutions can be obtained more easily via filtering, obviating the need to solve a potentially dense system of equations using slow iterative methods. Our filtered solutions are very similar to the true ones, but often up to 10 times faster to compute."
SpaceNet MVOI: A Multi-View Overhead Imagery Dataset,"Nicholas Weir, David Lindenbaum, Alexei Bastidas, Adam Van Etten, Sean McPherson, Jacob Shermeyer, Varun Kumar, Hanlin Tang",In-Q-Tel CosmiQ Works; Intel AI Lab; Accenture Federal Services,33.33333333333333,USA,66.66666666666667,USA,"Detection and segmentation of objects in overheard imagery is a challenging task. The variable density, random orientation, small size, and instance-to-instance heterogeneity of objects in overhead imagery calls for approaches distinct from existing models designed for natural scene datasets. Though new overhead imagery datasets are being developed, they almost universally comprise a single view taken from directly overhead (""at nadir""), failing to address a critical variable: look angle. By contrast, views vary in real-world overhead imagery, particularly in dynamic scenarios such as natural disasters where first looks are often over 40 degrees off-nadir. This represents an important challenge to computer vision methods, as changing view angle adds distortions, alters resolution, and changes lighting. At present, the impact of these perturbations for algorithmic detection and segmentation of objects is untested. To address this problem, we present an open source Multi-View Overhead Imagery dataset, termed SpaceNet MVOI, with 27 unique looks from a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of these images cover the same 665 square km geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance. We benchmark multiple leading segmentation and object detection models on: (1) building detection, (2) generalization to unseen viewing angles and resolutions, and (3) sensitivity of building footprint extraction to changes in resolution. We find that state of the art segmentation and object detection models struggle to identify buildings in off-nadir imagery and generalize poorly to unseen views, presenting an important benchmark to explore the broadly relevant challenge of detecting small, heterogeneous target objects in visually dynamic contexts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Weir_SpaceNet_MVOI_A_Multi-View_Overhead_Imagery_Dataset_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Weir_SpaceNet_MVOI_A_Multi-View_Overhead_Imagery_Dataset_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010896/,"['Buildings', 'Image segmentation', 'Task analysis', 'Image resolution', 'Object detection', 'Satellites', 'Computer vision']","['Imagery Dataset', 'Distortion', 'Benchmark', 'Computer Vision', 'Object Detection', 'Segmentation Model', 'Viewing Angle', 'Target Object', 'Natural Scenes', 'Object Segmentation', 'Segmentation Detection', 'Resolution Of Changes', 'Object Detection Model', 'Building Footprints', 'Impact Of Perturbations', 'Training Set', 'Low Resolution', 'F1 Score', 'Intersection Over Union', 'Bounding Box', 'Semantic Segmentation', 'Natural Images', 'Natural Scene Images', 'Change In Angle', 'Multispectral Images', 'Original Resolution', 'Binary Cross-entropy Loss', 'Mask R-CNN']",,55,"Detection and segmentation of objects in overheard imagery is a challenging task. The variable density, random orientation, small size, and instance-to-instance heterogeneity of objects in overhead imagery calls for approaches distinct from existing models designed for natural scene datasets. Though new overhead imagery datasets are being developed, they almost universally comprise a single view taken from directly overhead (“at nadir”), failing to address a critical variable: look angle. By contrast, views vary in real-world overhead imagery, particularly in dynamic scenarios such as natural disasters where first looks are often over 40° off-nadir. This represents an important challenge to computer vision methods, as changing view angle adds distortions, alters resolution, and changes lighting. At present, the impact of these perturbations for algorithmic detection and segmentation of objects is untested. To address this problem, we present an open source Multi-View Overhead Imagery dataset, termed SpaceNet MVOI, with 27 unique looks from a broad range of viewing angles (-32.5° to 54.0°). Each of these images cover the same 665 km
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance. We benchmark multiple leading segmentation and object detection models on: (1) building detection, (2) generalization to unseen viewing angles and resolutions, and (3) sensitivity of building footprint extraction to changes in resolution. We find that state of the art segmentation and object detection models struggle to identify buildings in off-nadir imagery and generalize poorly to unseen views, presenting an important benchmark to explore the broadly relevant challenge of detecting small, heterogeneous target objects in visually dynamic contexts."
Sparse and Imperceivable Adversarial Attacks,"Francesco Croce, Matthias Hein",University of Tübingen,100.0,germany,0.0,,"Neural networks have been proven to be vulnerable to a variety of adversarial attacks. From a safety perspective, highly sparse adversarial attacks are particularly dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected. We propose a new black-box technique to craft adversarial examples aiming at minimizing l_0-distance to the original image. Extensive experiments show that our attack is better or competitive to the state of the art. Moreover, we can integrate additional bounds on the componentwise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis-aligned edges makes our adversarial examples almost non-perceivable. Moreover, we adapt the Projected Gradient Descent attack to the l_0-norm integrating componentwise constraints. This allows us to do adversarial training to enhance the robustness of classifiers against sparse and imperceivable adversarial manipulations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010626/,"['Image color analysis', 'Perturbation methods', 'Training', 'Gray-scale', 'Robustness', 'Neural networks', 'Image edge detection']","['Adversarial Attacks', 'Neural Network', 'Adversarial Training', 'Adversarial Examples', 'Projected Gradient Descent', 'Color Change', 'Large Changes', 'Distance Function', 'Color Images', 'Grayscale Images', 'Color Space', 'Practical Scenarios', 'Classification Output', 'Iterative Scheme', 'Types Of Attacks', 'Color Channels', 'Safety-critical', 'Color Saturation', 'Coordinate Axis', 'Image X', 'Black-box Attacks', 'White-box Attack', 'Adversarial Perturbations', 'Attack Success Rate', 'HSV Color']",,100,"Neural networks have been proven to be vulnerable to a variety of adversarial attacks. From a safety perspective, highly sparse adversarial attacks are particularly dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected. We propose a new black-box technique to craft adversarial examples aiming at minimizing l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-distance to the original image. Extensive experiments show that our attack is better or competitive to the state of the art. Moreover, we can integrate additional bounds on the componentwise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis-aligned edges makes our adversarial examples almost non-perceivable. Moreover, we adapt the Projected Gradient Descent attack to the l
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub>
-norm integrating componentwise constraints. This allows us to do adversarial training to enhance the robustness of classifiers against sparse and imperceivable adversarial manipulations."
SparseMask: Differentiable Connectivity Learning for Dense Image Prediction,"Huikai Wu, Junge Zhang, Kaiqi Huang","Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; CAS Center for Excellence in Brain Science and Intelligence Technology; Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences",100.0,china,0.0,,"In this paper, we aim at automatically searching an efficient network architecture for dense image prediction. Particularly, we follow the encoder-decoder style and focus on designing a connectivity structure for the decoder. To achieve that, we design a densely connected network with learnable connections, named Fully Dense Network, which contains a large set of possible final connectivity structures. We then employ gradient descent to search the optimal connectivity from the dense connections. The search process is guided by a novel loss function, which pushes the weight of each connection to be binary and the connections to be sparse. The discovered connectivity achieves competitive results on two segmentation datasets, while runs more than three times faster and requires less than half parameters compared to the state-of-the-art methods. An extensive experiment shows that the discovered connectivity is compatible with various backbones and generalizes well to other dense image prediction tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_SparseMask_Differentiable_Connectivity_Learning_for_Dense_Image_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_SparseMask_Differentiable_Connectivity_Learning_for_Dense_Image_Prediction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009522/,"['Decoding', 'Task analysis', 'Computer architecture', 'Search problems', 'Feature extraction', 'Transforms', 'Network architecture']","['Dense Image Prediction', 'Loss Function', 'Gradient Descent', 'Structural Connectivity', 'Dense Connections', 'Connection Weights', 'Architecture For Prediction', 'Learning Rate', 'Input Features', 'Search Space', 'Spatial Dimensions', 'Semantic Segmentation', 'High-level Features', 'Low-level Features', 'Stage Characteristics', 'L1 Loss', 'Feature Encoder', 'Fully Convolutional Network', 'Multi-level Features', 'Global Average Pooling Layer', 'Decoding Stage', 'Final Architecture', 'NVIDIA Titan Xp GPU', 'Neural Architecture Search', 'Sparse Connectivity', 'Number Of Input Features', 'Saliency Detection', 'Edge Detection', 'Deep Supervision', 'Image Classification']",,10,"In this paper, we aim at automatically searching an efficient network architecture for dense image prediction. Particularly, we follow the encoder-decoder style and focus on designing a connectivity structure for the decoder. To achieve that, we design a densely connected network with learnable connections, named Fully Dense Network, which contains a large set of possible final connectivity structures. We then employ gradient descent to search the optimal connectivity from the dense connections. The search process is guided by a novel loss function, which pushes the weight of each connection to be binary and the connections to be sparse. The discovered connectivity achieves competitive results on two segmentation datasets, while runs more than three times faster and requires less than half parameters compared to the state-of-the-art methods. An extensive experiment shows that the discovered connectivity is compatible with various backbones and generalizes well to other dense image prediction tasks."
Spatial Correspondence With Generative Adversarial Network: Learning Depth From Monocular Videos,"Zhenyao Wu, Xinyi Wu, Xiaoping Zhang, Song Wang, Lili Ju","Wuhan University, China; University of South Carolina, USA; Farsee2 Technology Ltd, China; University of South Carolina, USA",75.0,"china, usa",25.0,China,"Depth estimation from monocular videos has important applications in many areas such as autonomous driving and robot navigation. It is a very challenging problem without knowing the camera pose since errors in camera-pose estimation can significantly affect the video-based depth estimation accuracy. In this paper, we present a novel SC-GAN network with end-to-end adversarial training for depth estimation from monocular videos without estimating the camera pose and pose change over time. To exploit cross-frame relations, SC-GAN includes a spatial correspondence module which uses Smolyak sparse grids to efficiently match the features across adjacent frames, and an attention mechanism to learn the importance of features in different directions. Furthermore, the generator in SC-GAN learns to estimate depth from the input frames, while the discriminator learns to distinguish between the ground-truth and estimated depth map for the reference frame. Experiments on the KITTI and Cityscapes datasets show that the proposed SC-GAN can achieve much more accurate depth maps than many existing state-of-the-art methods on monocular videos.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Spatial_Correspondence_With_Generative_Adversarial_Network_Learning_Depth_From_Monocular_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Spatial_Correspondence_With_Generative_Adversarial_Network_Learning_Depth_From_Monocular_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009012/,"['Estimation', 'Videos', 'Cameras', 'Correlation', 'Generators', 'Gallium nitride', 'Chebyshev approximation']","['Generative Adversarial Networks', 'Spatial Correspondence', 'Monocular Video', 'Reference Frame', 'Attention Mechanism', 'Depth Map', 'Spatial Module', 'Depth Estimation', 'Adjacent Frames', 'Camera Pose', 'Robot Navigation', 'Input Frames', 'KITTI Dataset', 'Pose Changes', 'Camera Pose Estimation', 'Sparse Grid', 'Convolutional Neural Network', 'Feature Maps', 'Spatial Features', 'Error Metrics', 'Video Frames', 'Original Frame', 'Stereo Images', 'Stereo Matching', 'Correlated Features', 'Optical Flow', 'Key Frames', 'Corresponding Points', 'Interest In Recent Years']",,13,"Depth estimation from monocular videos has important applications in many areas such as autonomous driving and robot navigation. It is a very challenging problem without knowing the camera pose since errors in camera-pose estimation can significantly affect the video-based depth estimation accuracy. In this paper, we present a novel SC-GAN network with end-to-end adversarial training for depth estimation from monocular videos without estimating the camera pose and pose change over time. To exploit cross-frame relations, SC-GAN includes a spatial correspondence module which uses Smolyak sparse grids to efficiently match the features across adjacent frames, and an attention mechanism to learn the importance of features in different directions. Furthermore, the generator in SC-GAN learns to estimate depth from the input frames, while the discriminator learns to distinguish between the ground-truth and estimated depth map for the reference frame. Experiments on the KITTI and Cityscapes datasets show that the proposed SC-GAN can achieve much more accurate depth maps than many existing state-of-the-art methods on monocular videos."
Spatial-Temporal Relation Networks for Multi-Object Tracking,"Jiarui Xu, Yue Cao, Zheng Zhang, Han Hu","Microsoft Research Asia; Hong Kong University of Science and Technology; School of Software, Tsinghua University",66.66666666666666,"China, Hong Kong",33.33333333333334,USA,"Recent progress in multiple object tracking (MOT) has shown that a robust similarity score is a key to the success of trackers. A good similarity score is expected to reflect multiple cues, e.g. appearance, location, and topology, over a long period of time. However, these cues are heterogeneous, making them hard to be combined in a unified network. As a result, existing methods usually encode them in separate networks or require a complex training approach. In this paper, we present a unified framework for similarity measurement based on spatial-temporal relation network which could simultaneously encode various cues and perform reasoning across both spatial and temporal domains. We also study the feature representation of a tracklet-object pair in depth, showing a proper design of the pair features can well empower the trackers. The resulting approach is named spatial-temporal relation networks (STRN). It runs in a feed-forward way and can be trained in an end-to-end manner. The state-of-the-art accuracy was achieved on all of the MOT15~17 benchmarks using public detection and online settings.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Spatial-Temporal_Relation_Networks_for_Multi-Object_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Spatial-Temporal_Relation_Networks_for_Multi-Object_Tracking_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010993/,"['Network topology', 'Topology', 'Feature extraction', 'Benchmark testing', 'Robustness', 'Object tracking', 'Training']","['Network Of Relationships', 'Multi-object Tracking', 'Spatial-temporal Relationships', 'Similarity Score', 'Object Features', 'Multiple Objects', 'Online Setting', 'Related Modules', 'Robust Score', 'Multiple Object Tracking', 'Local Features', 'Spatial Relationship', 'Related Features', 'Visuospatial', 'Temporal Relationship', 'Bounding Box', 'Motion Features', 'Appearance Features', 'Current Frame', 'Multiple Frames', 'Object In Frame', 'Object Trajectory', 'Attention Weights', 'Cosine Value', 'Relational Reasoning', 'Information Aggregation', 'Field Of Object Detection', 'Topological Information', 'Occupancy Grid', 'Previous Frame']",,197,"Recent progress in multiple object tracking (MOT) has shown that a robust similarity score is a key to the success of trackers. A good similarity score is expected to reflect multiple cues, e.g. appearance, location, and topology, over a long period of time. However, these cues are heterogeneous, making them hard to be combined in a unified network. As a result, existing methods usually encode them in separate networks or require a complex training approach. In this paper, we present a unified framework for similarity measurement based on spatial-temporal relation network which could simultaneously encode various cues and perform reasoning across both spatial and temporal domains. We also study the feature representation of a tracklet-object pair in depth, showing a proper design of the pair features can well empower the trackers. The resulting approach is named spatial-temporal relation networks (STRN). It runs in a feed-forward way and can be trained in an end-to-end manner. The state-of-the-art accuracy was achieved on all of the MOT15$\sim$17 benchmarks using public detection and online settings."
SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition,"Kaiyu Yang, Olga Russakovsky, Jia Deng",Princeton University,100.0,usa,0.0,,"Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be ""behind"" a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be ""next to"" each other if there is a third student between them. We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at https://github.com/princeton-vl/SpatialSense.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SpatialSense_An_Adversarially_Crowdsourced_Benchmark_for_Spatial_Relation_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SpatialSense_An_Adversarially_Crowdsourced_Benchmark_for_Spatial_Relation_Recognition_ICCV_2019_paper.pdf,,https://github.com/princeton-vl/SpatialSense,,main,Poster,https://ieeexplore.ieee.org/document/9010884/,"['Visualization', 'Task analysis', 'Crowdsourcing', 'Benchmark testing', 'Robots', 'Cognition', 'Genomics']","['Benchmark', 'Spatial Relationship', 'Spatial Recognition', 'Visuospatial', 'Long Tail', 'Person Image', 'Dataset Bias', 'Simple Baseline', 'Negative Relationship', 'Object Detection', 'Positive Relation', 'Bounding Box', 'Model Architecture', 'Visual Detection', 'Word Embedding', 'Linear Layer', 'Gated Recurrent Unit', 'Subject And Object', 'Object Naming', 'Visual Question Answering', 'Language Bias', 'Object Bounding Boxes', 'Simple 2D', '2D Location', 'Spatial Understanding', 'Accuracy Drop', 'Visual Reasoning', 'Center Of The Bounding Box', 'Flowering']",,14,"Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be ""behind"" a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be ""next to"" each other if there is a third student between them. We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at https://github.com/princeton-vl/SpatialSense."
Spatio-Temporal Filter Adaptive Network for Video Deblurring,"Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie, Wangmeng Zuo, Jimmy Ren","Harbin Institute of Technology, Harbin, China; Nanjing University of Science and Technology, Nanjing, China; SenseTime Research",66.66666666666666,china,33.33333333333334,China,"Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Spatio-Temporal_Filter_Adaptive_Network_for_Video_Deblurring_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Spatio-Temporal_Filter_Adaptive_Network_for_Video_Deblurring_ICCV_2019_paper.pdf,https://shangchenzhou.com/projects/stfan,,,main,Poster,https://ieeexplore.ieee.org/document/9010007/,"['Kernel', 'Optical imaging', 'Image restoration', 'Optical filters', 'Adaptive systems', 'Task analysis', 'Feature extraction']","['Spatiotemporal Network', 'Video Deblurring', 'Convolutional Layers', 'Quantitative Evaluation', 'Model Size', 'Optical Flow', 'Consecutive Frames', 'Network Reconstruction', 'Current Frame', 'Adaptive Filter', 'Kernel Estimation', 'Blurry Images', 'Optical Flow Estimation', 'Blur Kernel', 'Neural Network', 'Convolutional Neural Network', 'Recurrent Network', 'Recurrent Neural Network', 'Receptive Field', 'Domain Features', 'Image Deblurring', 'Dynamic Scenes', 'CNN-based Methods', 'Convolutional Neural Network Method', 'Residual Block', 'Motion Field', 'Stereo Images', 'Frame Alignment', 'Smaller Model Size', 'Feature Transformation']",,143,"Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size."
Spatio-Temporal Fusion Based Convolutional Sequence Learning for Lip Reading,"Xingxuan Zhang, Feng Cheng, Shilin Wang","Shanghai Jiao Tong University, Shanghai, China",100.0,China,0.0,,"Current state-of-the-art approaches for lip reading are based on sequence-to-sequence architectures that are designed for natural machine translation and audio speech recognition. Hence, these methods do not fully exploit the characteristics of the lip dynamics, causing two main drawbacks. First, the short-range temporal dependencies, which are critical to the mapping from lip images to visemes, receives no extra attention. Second, local spatial information is discarded in the existing sequence models due to the use of global average pooling (GAP). To well solve these drawbacks, we propose a Temporal Focal block to sufficiently describe short-range dependencies and a Spatio-Temporal Fusion Module (STFM) to maintain the local spatial information and to reduce the feature dimensions as well. From the experiment results, it is demonstrated that our method achieves comparable performance with the state-of-the-art approach using much less training data and much lighter Convolutional Feature Extractor. The training time is reduced by 12 days due to the convolutional structure and the local self-attention mechanism.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Spatio-Temporal_Fusion_Based_Convolutional_Sequence_Learning_for_Lip_Reading_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Spatio-Temporal_Fusion_Based_Convolutional_Sequence_Learning_for_Lip_Reading_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009091/,"['Lips', 'Feature extraction', 'Hidden Markov models', 'Training', 'Task analysis', 'Convolution', 'Decoding']","['Spatiotemporal Fusion', 'Training Data', 'Local Information', 'Speech Recognition', 'Global Pooling', 'Temporal Dependencies', 'Global Average Pooling', 'Local Spatial Information', 'Spatiotemporal Modulation', 'Convolutional Neural Network', 'Convolutional Layers', 'Utterances', 'Spatial Features', 'Visual Features', 'Attention Mechanism', 'Temporal Features', 'Changes In Appearance', 'Transformer Model', 'Attention Weights', 'Long-range Dependencies', 'Gridded Datasets', 'Word Error Rate', 'Seq2seq Model', 'Dynamic Bayesian Network', 'Temporal Convolution', 'Memory Cost', 'Lip Movements', 'Hidden Size', 'Accuracy Drop', 'Time Step']",,48,"Current state-of-the-art approaches for lip reading are based on sequence-to-sequence architectures that are designed for natural machine translation and audio speech recognition. Hence, these methods do not fully exploit the characteristics of the lip dynamics, causing two main drawbacks. First, the short-range temporal dependencies, which are critical to the mapping from lip images to visemes, receives no extra attention. Second, local spatial information is discarded in the existing sequence models due to the use of global average pooling (GAP). To well solve these drawbacks, we propose a Temporal Focal block to sufficiently describe short-range dependencies and a Spatio-Temporal Fusion Module (STFM) to maintain the local spatial information and to reduce the feature dimensions as well. From the experiment results, it is demonstrated that our method achieves comparable performance with the state-of-the-art approach using much less training data and much lighter Convolutional Feature Extractor. The training time is reduced by 12 days due to the convolutional structure and the local self-attention mechanism."
Spatiotemporal Feature Residual Propagation for Action Prediction,"He Zhao, Richard P. Wildes","York University, Toronto",100.0,canada,0.0,,"Recognizing actions from limited preliminary video observations has seen considerable recent progress. Typically, however, such progress has been had without explicitly modeling fine-grained motion evolution as a potentially valuable information source. In this study, we address this task by investigating how action patterns evolve over time in a spatial feature space. There are three key components to our system. First, we work with intermediate-layer ConvNet features, which allow for abstraction from raw data, while retaining spatial layout, which is sacrificed in approaches that rely on vectorized global representations. Second, instead of propagating features per se, we propagate their residuals across time, which allows for a compact representation that reduces redundancy while retaining essential information about evolution over time. Third, we employ a Kalman filter to combat error build-up and unify across prediction start times. Extensive experimental results on the JHMDB21, UCF101 and BIT datasets show that our approach leads to a new state-of-the-art in action prediction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Spatiotemporal_Feature_Residual_Propagation_for_Action_Prediction_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Spatiotemporal_Feature_Residual_Propagation_for_Action_Prediction_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010936/,"['Feature extraction', 'Kernel', 'Kalman filters', 'Generators', 'Computational modeling', 'Layout', 'Training']","['Activity Prediction', 'Raw Data', 'Feature Space', 'Spatial Features', 'Kalman Filter', 'Compact Representation', 'Deep Learning', 'Objective Function', 'Feature Maps', 'Spatial Structure', 'Training Strategy', 'Action Recognition', 'Intermediate Layer', 'Residual Network', 'Optical Flow', 'Action Classes', 'Temporal Model', 'Intermediate Features', 'Prior Estimates', 'Partial Observation', 'Kalman Gain', 'Action Labels', 'Backpropagation Through Time', 'Optical Flow Estimation']",,21,"Recognizing actions from limited preliminary video observations has seen considerable recent progress. Typically, however, such progress has been had without explicitly modeling fine-grained motion evolution as a potentially valuable information source. In this study, we address this task by investigating how action patterns evolve over time in a spatial feature space. There are three key components to our system. First, we work with intermediate-layer ConvNet features, which allow for abstraction from raw data, while retaining spatial layout, which is sacrificed in approaches that rely on vectorized global representations. Second, instead of propagating features per se, we propagate their residuals across time, which allows for a compact representation that reduces redundancy while retaining essential information about evolution over time. Third, we employ a Kalman filter to combat error build-up and unify across prediction start times. Extensive experimental results on the JHMDB21, UCF101 and BIT datasets show that our approach leads to a new state-of-the-art in action prediction."
Specifying Object Attributes and Relations in Interactive Scene Generation,"Oron Ashual, Lior Wolf",Tel Aviv University; Tel Aviv University and Facebook AI Research,100.0,israel,0.0,,"We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space by selecting an appearance archetype. Our code is publicly available at https://www.github.com/ashual/scene_generation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ashual_Specifying_Object_Attributes_and_Relations_in_Interactive_Scene_Generation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ashual_Specifying_Object_Attributes_and_Relations_in_Interactive_Scene_Generation_ICCV_2019_paper.pdf,,https://www.github.com/ashual/scene_generation,,main,Oral,https://ieeexplore.ieee.org/document/9010748/,"['Tensile stress', 'Layout', 'Training', 'Image generation', 'Visualization', 'Semantics', 'Tools']","['Image Generation', 'Visual Quality', 'Output Image', 'Scene Graph', 'High Visual Quality', 'Semantic', 'Learning Rate', 'User Study', 'Image Object', 'Bounding Box', 'Counterfactual', 'Local Properties', 'Random Vector', 'Baseline Methods', 'Local Vector', 'Graph Convolutional Network', 'Loss Term', 'Ground Truth Image', 'Objects In The Scene', 'Embedding Vectors', 'Ground-truth Bounding Box', 'Encoder-decoder Network', 'Object Appearance', 'Perceptual Loss']",,109,"We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space by selecting an appearance archetype. Our code is publicly available at https://www.github.com/ashual/scene_generation."
Spectral Feature Transformation for Person Re-Identification,"Chuanchen Luo, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang","TuSimple; University of Chinese Academy of Sciences, Center for Research on Intelligent Perception and Computing, CASIA, Center for Excellence in Brain Science and Intelligence Technology, CAS",100.0,"china, usa",0.0,,"With the surge of deep learning techniques, the field of person re-identification has witnessed rapid progress in recent years. Deep learning based methods focus on learning a discriminative feature space where data points are clustered compactly according to their corresponding identities. Most existing methods process data points individually or only involves a fraction of samples while building a similarity structure. They ignore dense informative connections among samples more or less. The lack of holistic observation eventually leads to inferior performance. To relieve the issue, we propose to formulate the whole data batch as a similarity graph. Inspired by spectral clustering, a novel module termed Spectral Feature Transformation is developed to facilitate the optimization of group-wise similarities. It adds no burden to the inference and can be applied to various scenarios. As a natural extension, we further derive a lightweight re-ranking method named Local Blurring Re-ranking which makes the underlying clustering structure around the probe set more compact. Empirical studies on four public benchmarks show the superiority of the proposed method. Code is available at https://github.com/LuckyDC/SFT_REID.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Spectral_Feature_Transformation_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Spectral_Feature_Transformation_for_Person_Re-Identification_ICCV_2019_paper.pdf,,https://github.com/LuckyDC/SFT_REID,,main,Poster,https://ieeexplore.ieee.org/document/9011035/,"['Machine learning', 'Measurement', 'Probes', 'Training', 'Task analysis', 'Buildings', 'Benchmark testing']","['Feature Transformation', 'Spectral Transformation', 'Deep Learning', 'Spectral Clustering', 'Similarity Graph', 'Progress In Recent Years', 'Diffusion Process', 'Transition Probabilities', 'Stochastic Gradient Descent', 'Vanilla', 'Imaging Probes', 'Classification Loss', 'Graph Convolutional Network', 'Ranked List', 'Transition Probability Matrix', 'Contrastive Loss', 'Affinity Matrix', 'Metric Learning', 'Hard Constraints', 'Triplet Loss', 'Re-identification Task', 'Classification Branch', 'Post-processing Stage', 'Gallery Images', 'Strong Baseline', 'Training Batch', 'Feature Aggregation']",,83,"With the surge of deep learning techniques, the field of person re-identification has witnessed rapid progress in recent years. Deep learning based methods focus on learning a discriminative feature space where data points are clustered compactly according to their corresponding identities. Most existing methods process data points individually or only involves a fraction of samples while building a similarity structure. They ignore dense informative connections among samples more or less. The lack of holistic observation eventually leads to inferior performance. To relieve the issue, we propose to formulate the whole data batch as a similarity graph. Inspired by spectral clustering, a novel module termed Spectral Feature Transformation is developed to facilitate the optimization of group-wise similarities. It adds no burden to the inference and can be applied to various scenarios. As a natural extension, we further derive a lightweight re-ranking method named Local Blurring Re-ranking which makes the underlying clustering structure around the probe set more compact. Empirical studies on four public benchmarks show the superiority of the proposed method. Code is available at https://github.com/LuckyDC/SFT_REID."
Spectral Regularization for Combating Mode Collapse in GANs,"Kanglin Liu, Wenming Tang, Fei Zhou, Guoping Qiu","Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen, China; Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society, Shenzhen, China; University of Nottingham, Nottingham, United Kingdom; Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Intelligent Information Processing, Shenzhen, China; Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society, Shenzhen, China",71.42857142857143,"China, china, uk",28.57142857142857,China,"Despite excellent progress in recent years, mode collapse remains a major unsolved problem in generative adversarial networks (GANs). In this paper, we present spectral regularization for GANs (SR-GANs), a new and robust method for combating the mode collapse problem in GANs. Theoretical analysis shows that the optimal solution to the discriminator has a strong relationship to the spectral distributions of the weight matrix. Therefore, we monitor the spectral distribution in the discriminator of spectral normalized GANs (SN-GANs), and discover a phenomenon which we refer to as spectral collapse, where a large number of singular values of the weight matrices drop dramatically when mode collapse occurs. We show that there are strong evidence linking mode collapse to spectral collapse; and based on this link, we set out to tackle spectral collapse as a surrogate of mode collapse. We have developed a spectral regularization method where we compensate the spectral distributions of the weight matrices to prevent them from collapsing, which in turn successfully prevents mode collapse in GANs. We provide theoretical explanations for why SR-GANs are more stable and can provide better performances than SN-GANs. We also present extensive experimental results and analysis to show that SR-GANs not only always outperform SN-GANs but also always succeed in combating mode collapse where SN-GANs fail.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Spectral_Regularization_for_Combating_Mode_Collapse_in_GANs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Spectral_Regularization_for_Combating_Mode_Collapse_in_GANs_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010938/,"['Gallium nitride', 'Training', 'Robustness', 'Generative adversarial networks', 'Generators', 'Monitoring', 'Optimization']","['Generative Adversarial Networks', 'Spectral Regularization', 'Robust Method', 'Number Of Values', 'Weight Matrix', 'Extensive Experiments', 'Singular Value', 'Spectral Distribution', 'Singular Values Of Matrix', 'Progress In Recent Years', 'Training Set', 'Training Data', 'Linear Function', 'Series Of Experiments', 'Batch Size', 'Linear Discriminant Analysis', 'Image Generation', 'Channel Size', 'Fourth Term', 'Normalization Techniques', 'Fr√©chet Inception Distance', 'Spectral Normalization', 'Inception Distance', 'Generative Adversarial Networks Model', 'Generative Adversarial Networks Training', 'Discriminator Network', 'Gradient Analysis', 'Largest Singular Value']",,34,"Despite excellent progress in recent years, mode collapse remains a major unsolved problem in generative adversarial networks (GANs). In this paper, we present spectral regularization for GANs (SR-GANs), a new and robust method for combating the mode collapse problem in GANs. Theoretical analysis shows that the optimal solution to the discriminator has a strong relationship to the spectral distributions of the weight matrix. Therefore, we monitor the spectral distribution in the discriminator of spectral normalized GANs (SN-GANs), and discover a phenomenon which we refer to as spectral collapse, where a large number of singular values of the weight matrices drop dramatically when mode collapse occurs. We show that there are strong evidence linking mode collapse to spectral collapse; and based on this link, we set out to tackle spectral collapse as a surrogate of mode collapse. We have developed a spectral regularization method where we compensate the spectral distributions of the weight matrices to prevent them from collapsing, which in turn successfully prevents mode collapse in GANs. We provide theoretical explanations for why SR-GANs are more stable and can provide better performances than SN-GANs. We also present extensive experimental results and analysis to show that SR-GANs not only always outperform SN-GANs but also always succeed in combating mode collapse where SN-GANs fail."
SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation,"Daniel Gordon, Abhishek Kadian, Devi Parikh, Judy Hoffman, Dhruv Batra","Paul G. Allen School of Computer Science, University of Washington; Facebook AI Research; Facebook AI Research, Georgia Institute of Technology",66.66666666666666,usa,33.33333333333334,USA,"We propose SplitNet, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, SplitNet generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, SplitNet can match the performance of traditional end-to-end pipelines which receive the entire dataset",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gordon_SplitNet_Sim2Sim_and_Task2Task_Transfer_for_Embodied_Visual_Navigation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_SplitNet_Sim2Sim_and_Task2Task_Transfer_for_Embodied_Visual_Navigation_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/splitnet,,main,Poster,https://ieeexplore.ieee.org/document/9009082/,"['Task analysis', 'Visualization', 'Navigation', 'Adaptation models', 'Training', 'Robots', 'Data mining']","['Machine Vision', 'Learning Objectives', 'Target Domain', 'Policy Learning', 'Navigation Task', 'Auxiliary Task', 'Deep Learning', 'Visual Representation', 'Shortest Path', 'Visual Task', 'ImageNet', 'Representation Learning', 'Encoder-decoder', 'Visual Input', 'Transfer Characteristics', 'Related Tasks', 'Domain Adaptation', 'Deep Reinforcement Learning', 'Depth Estimation', 'Visual Domain', 'Visual Encoding', 'Start Location', 'Policy Transfer', 'Surface Normals', 'Geodesic Distance', 'Visual Environment', 'Motor Task', 'Semantic Segmentation', 'Visual Learning', 'Visual Adaptation']",,34,"We propose SplitNet, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, SplitNet generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, SplitNet can match the performance of traditional end-to-end pipelines which receive the entire dataset"
Stacked Cross Refinement Network for Edge-Aware Salient Object Detection,"Zhe Wu, Li Su, Qingming Huang","School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS), Beijing, China; Key Lab of Big Data Mining and Knowledge Management, UCAS, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS), Beijing, China; Key Lab of Big Data Mining and Knowledge Management, UCAS, Beijing, China; Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China; Peng Cheng Laboratory, ShenZhen, China; School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS), Beijing, China; Key Lab of Big Data Mining and Knowledge Management, UCAS, Beijing, China; Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China",100.0,china,0.0,,"Salient object detection is a fundamental computer vision task. The majority of existing algorithms focus on aggregating multi-level features of pre-trained convolutional neural networks. Moreover, some researchers attempt to utilize edge information for auxiliary training. However, existing edge-aware models design unidirectional frameworks which only use edge features to improve the segmentation features. Motivated by the logical interrelations between binary segmentation and edge maps, we propose a novel Stacked Cross Refinement Network (SCRN) for salient object detection in this paper. Our framework aims to simultaneously refine multi-level features of salient object detection and edge detection by stacking Cross Refinement Unit (CRU). According to the logical interrelations, the CRU designs two direction-specific integration operations, and bidirectionally passes messages between the two tasks. Incorporating the refined edge-preserving features with the typical U-Net, our model detects salient objects accurately. Extensive experiments conducted on six benchmark datasets demonstrate that our method outperforms existing state-of-the-art algorithms in both accuracy and efficiency. Besides, the attribute-based performance on the SOC dataset show that the proposed model ranks first in the majority of challenging scenes. Code can be found at https://github.com/wuzhe71/SCAN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf,,https://github.com/wuzhe71/SCAN,,main,Poster,https://ieeexplore.ieee.org/document/9010954/,"['Image edge detection', 'Task analysis', 'Feature extraction', 'Object detection', 'Image segmentation', 'Silicon', 'Computational modeling']","['Salient Object', 'Object Detection Network', 'Salient Object Detection', 'Convolutional Neural Network', 'Benchmark Datasets', 'Edge Detection', 'Segmentation Map', 'Binary Map', 'Edge Features', 'Edge Information', 'Segmentation Feature', 'Binary Segmentation', 'Multi-level Features', 'Edges Of Objects', 'Scaling Factor', 'Convolutional Layers', 'Deep Models', 'Input Image', 'Discriminative Features', 'Saliency Map', 'Feature Refinement', 'Fully Convolutional Network', 'High-level Features', 'OR Operation', 'Concatenation Operation', 'Edge Labels', 'Low-level Features', 'Object Detection Framework', 'Bidirectional Model']",,286,"Salient object detection is a fundamental computer vision task. The majority of existing algorithms focus on aggregating multi-level features of pre-trained convolutional neural networks. Moreover, some researchers attempt to utilize edge information for auxiliary training. However, existing edge-aware models design unidirectional frameworks which only use edge features to improve the segmentation features. Motivated by the logical interrelations between binary segmentation and edge maps, we propose a novel Stacked Cross Refinement Network (SCRN) for salient object detection in this paper. Our framework aims to simultaneously refine multi-level features of salient object detection and edge detection by stacking Cross Refinement Unit (CRU). According to the logical interrelations, the CRU designs two direction-specific integration operations, and bidirectionally passes messages between the two tasks. Incorporating the refined edge-preserving features with the typical U-Net, our model detects salient objects accurately. Extensive experiments conducted on six benchmark datasets demonstrate that our method outperforms existing state-of-the-art algorithms in both accuracy and efficiency. Besides, the attribute-based performance on the SOC dataset show that the proposed model ranks first in the majority of challenging scenes. Code can be found at https://github.com/wuzhe71/SCAN."
StartNet: Online Detection of Action Start in Untrimmed Videos,"Mingfei Gao, Mingze Xu, Larry S. Davis, Richard Socher, Caiming Xiong",Salesforce Research; Indiana University; University of Maryland,66.66666666666666,usa,33.33333333333334,USA,"We propose StartNet to address Online Detection of Action Start (ODAS) where action starts and their associated categories are detected in untrimmed, streaming videos. Previous methods aim to localize action starts by learning feature representations that can directly separate the start point from its preceding background. It is challenging due to the subtle appearance difference near the action starts and the lack of training data. Instead, StartNet decomposes ODAS into two stages: action classification (using ClsNet) and start point localization (using LocNet). ClsNet focuses on per-frame labeling and predicts action score distributions online. Based on the predicted action scores of the past and current frames, LocNet conducts class-agnostic start detection by optimizing long-term localization rewards using policy gradient methods. The proposed framework is validated on two large-scale datasets, THUMOS'14 and ActivityNet. The experimental results show that StartNet significantly outperforms the state-of-the-art by 15%-30% p-mAP under the offset tolerance of 1-10 seconds on THUMOS'14, and achieves comparable performance on ActivityNet with 10 times smaller time offset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gao_StartNet_Online_Detection_of_Action_Start_in_Untrimmed_Videos_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_StartNet_Online_Detection_of_Action_Start_in_Untrimmed_Videos_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008394/,"['Videos', 'Labeling', 'Proposals', 'Task analysis', 'Training', 'Learning (artificial intelligence)', 'Training data']","['Untrimmed Videos', 'Large-scale Datasets', 'Action Classes', 'Current Frame', 'Policy Gradient', 'Policy Gradient Method', 'Lack Of Training Data', 'Time Step', 'Video Frames', 'Hidden State', 'Consequences Of Decisions', 'Motion Features', 'Consecutive Frames', 'Reward Function', 'Appearance Features', 'Markov Decision Process', 'Positive Sequence', 'Auxiliary Task', 'Two-stage Framework', 'RGB Features', 'Action Detection', 'Action Instances', 'Long-term Reward', 'Late Fusion', 'Proposed Classification', 'Deep Q-learning', 'Proposal Generation', 'Object Detection', 'Ground Truth Points']",,32,"We propose StartNet to address Online Detection of Action Start (ODAS) where action starts and their associated categories are detected in untrimmed, streaming videos. Previous methods aim to localize action starts by learning feature representations that can directly separate the start point from its preceding background. It is challenging due to the subtle appearance difference near the action starts and the lack of training data. Instead, StartNet decomposes ODAS into two stages: action classification (using ClsNet) and start point localization (using LocNet). ClsNet focuses on per-frame labeling and predicts action score distributions online. Based on the predicted action scores of the past and current frames, LocNet conducts class-agnostic start detection by optimizing long-term localization rewards using policy gradient methods. The proposed framework is validated on two large-scale datasets, THUMOS'14 and ActivityNet. The experimental results show that StartNet significantly outperforms the state-of-the-art by 15%-30% p-mAP under the offset tolerance of 1-10 seconds on THUMOS'14, and achieves comparable performance on ActivityNet with 10 times smaller time offset."
Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization,"Liu Liu, Hongdong Li, Yuchao Dai","School of Electronics and Information, Northwestern Polytechnical University; Australian National University, Australian Centre for Robotic Vision",100.0,"Australia, china",0.0,,"This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Stochastic_Attraction-Repulsion_Embedding_for_Large_Scale_Image_Localization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Stochastic_Attraction-Repulsion_Embedding_for_Large_Scale_Image_Localization_ICCV_2019_paper.pdf,,https://github.com/Liumouliu/deepIBL,,main,Poster,https://ieeexplore.ieee.org/document/9010658/,"['Task analysis', 'Kernel', 'Feature extraction', 'Probability distribution', 'Image representation', 'Training', 'Image databases']","['Convolutional Neural Network', 'Large Database', 'Kullback-Leibler', 'Similar Images', 'Image Representation', 'Image Database', 'Standard Benchmark', 'Contrastive Loss', 'Query Image', 'Training Dataset', 'Gaussian Kernel', 'Classification Task', 'Conditional Probability', 'Multiple Images', 'Image Pairs', 'Latent Space', 'Positive Image', 'Convolutional Neural Network Architecture', 'Negative Images', 'Image Retrieval', 'Convolutional Neural Network Training', 'Improvement In Recall', 'Exponential Kernel', 'Convolution Neural Networks Network', 'Nearest Neighbor Search', 'Competitive Learning', 'Saturation Region', 'Probabilistic Framework', 'Metric Learning Methods', 'Original Vector']",,69,"This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL."
Stochastic Exposure Coding for Handling Multi-ToF-Camera Interference,"Jongho Lee, Mohit Gupta",University of Wisconsin-Madison,100.0,usa,0.0,,"As continuous-wave time-of-flight (C-ToF) cameras become popular in 3D imaging applications, they need to contend with the problem of multi-camera interference (MCI). In a multi-camera environment, a ToF camera may receive light from the sources of other cameras, resulting in large depth errors. In this paper, we propose stochastic exposure coding (SEC), a novel approach for mitigating. SEC involves dividing a camera's integration time into multiple slots, and switching the camera off and on stochastically during each slot. This approach has two benefits. First, by appropriately choosing the on probability for each slot, the camera can effectively filter out both the AC and DC components of interfering signals, thereby mitigating depth errors while also maintaining high signal-to-noise ratio. This enables high accuracy depth recovery with low power consumption. Second, this approach can be implemented without modifying the C-ToF camera's coding functions, and thus, can be used with a wide range of cameras with minimal changes. We demonstrate the performance benefits of SEC with theoretical analysis, simulations and real experiments, across a wide range of imaging scenarios.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Stochastic_Exposure_Coding_for_Handling_Multi-ToF-Camera_Interference_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Stochastic_Exposure_Coding_for_Handling_Multi-ToF-Camera_Interference_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010906/,"['Cameras', 'Interference', 'Encoding', 'Signal to noise ratio', 'Demodulation', 'Photonics']","['Low Power', 'Integration Time', 'Depth Error', 'Multiple Slots', 'Time-of-flight Sensors', 'Energy Consumption', 'Light Source', 'Systematic Errors', 'Intensity Measurements', 'Modulation Frequency', 'Practical Limitations', 'Peak Power', 'Root Mean Square Error Values', 'Ambient Light', 'Depth Values', 'Periodic Function', 'Depth Estimation', 'Shot Noise', 'Orthogonal Function', 'Capture Time', 'Time Division Multiple Access', 'Hardware Experiments', 'Optimal Probability', 'Nanosecond Timescale', 'Inliers', 'Multiple Cameras', 'Power Source', 'Reduction Approach', 'Base Frequencies', 'Technical Report']",,7,"As continuous-wave time-of-flight (C-ToF) cameras become popular in 3D imaging applications, they need to contend with the problem of multi-camera interference (MCI). In a multi-camera environment, a ToF camera may receive light from the sources of other cameras, resulting in large depth errors. In this paper, we propose stochastic exposure coding (SEC), a novel approach for mitigating. SEC involves dividing a camera's integration time into multiple slots, and switching the camera off and on stochastically during each slot. This approach has two benefits. First, by appropriately choosing the on probability for each slot, the camera can effectively filter out both the AC and DC components of interfering signals, thereby mitigating depth errors while also maintaining high signal-to-noise ratio. This enables high accuracy depth recovery with low power consumption. Second, this approach can be implemented without modifying the C-ToF camera's coding functions, and thus, can be used with a wide range of cameras with minimal changes. We demonstrate the performance benefits of SEC with theoretical analysis, simulations and real experiments, across a wide range of imaging scenarios."
Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels,"Felix J.S. Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C. Alexander, Jorge Cardoso","University College London, UK; Kings College London; University College London",100.0,"United Kingdom, uk",0.0,,"The performance of multi-task learning in Convolutional Neural Networks (CNNs) hinges on the design of feature sharing between tasks within the architecture. The number of possible sharing patterns are combinatorial in the depth of the network and the number of tasks, and thus hand-crafting an architecture, purely based on the human intuitions of task relationships can be time-consuming and suboptimal. In this paper, we present a probabilistic approach to learning task-specific and shared representations in CNNs for multi-task learning. Specifically, we propose ""stochastic filter groups"" (SFG), a mechanism to assign convolution kernels in each layer to ""specialist"" and ""generalist"" groups, which are specific to and shared across different tasks, respectively. The SFG modules determine the connectivity between layers and the structures of task-specific and shared representations in the network. We employ variational inference to learn the posterior distribution over the possible grouping of kernels and network parameters. Experiments demonstrate the proposed method generalises across multiple tasks and shows improved performance over baseline methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bragman_Stochastic_Filter_Groups_for_Multi-Task_CNNs_Learning_Specialist_and_Generalist_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bragman_Stochastic_Filter_Groups_for_Multi-Task_CNNs_Learning_Specialist_and_Generalist_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009037/,"['Task analysis', 'Convolution', 'Kernel', 'Computer architecture', 'Routing', 'Stochastic processes', 'Cats']","['Convolutional Neural Network', 'Convolution Kernel', 'Multitask Convolutional Neural Network', 'Stochastic Filter', 'Learning Specialist', 'Neural Network', 'Posterior Probability', 'Multiple Tasks', 'Multi-task Learning', 'Network Depth', 'Variational Inference', 'Shared Representation', 'Kernel Of Layer', 'Computed Tomography', 'Convolutional Layers', 'Feature Maps', 'Network Layer', 'Max-pooling', 'Semantic Segmentation', 'Gender Binary', 'Task-specific Features', 'Medical Image Datasets', 'Convolutional Neural Network Architecture', 'Age Prediction', 'Subsequent Layers', 'Learning Architecture', 'Femoral Head', 'Residual Block', 'Organ Segmentation', 'Merge Operation']",,39,"The performance of multi-task learning in Convolutional Neural Networks (CNNs) hinges on the design of feature sharing between tasks within the architecture. The number of possible sharing patterns are combinatorial in the depth of the network and the number of tasks, and thus hand-crafting an architecture, purely based on the human intuitions of task relationships can be time-consuming and suboptimal. In this paper, we present a probabilistic approach to learning task-specific and shared representations in CNNs for multi-task learning. Specifically, we propose ""stochastic filter groups"" (SFG), a mechanism to assign convolution kernels in each layer to ""specialist"" and ""generalist"" groups, which are specific to and shared across different tasks, respectively. The SFG modules determine the connectivity between layers and the structures of task-specific and shared representations in the network. We employ variational inference to learn the posterior distribution over the possible grouping of kernels and network parameters. Experiments demonstrate the proposed method generalises across multiple tasks and shows improved performance over baseline methods."
Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection,"Yingyue Xu, Dan Xu, Xiaopeng Hong, Wanli Ouyang, Rongrong Ji, Min Xu, Guoying Zhao","University of Oulu; Xiamen University; University of Technology Sydney; SenseTime Computer Vision Group, The University of Sydney; Xi’an Jiaotong University; University of Oxford",100.0,"australia, china, finland, uk",0.0,,"Recent saliency models extensively explore to incorporate multi-scale contextual information from Convolutional Neural Networks (CNNs). Besides direct fusion strategies, many approaches introduce message-passing to enhance CNN features or predictions. However, the messages are mainly transmitted in two ways, by feature-to-feature passing, and by prediction-to-prediction passing. In this paper, we add message-passing between features and predictions and propose a deep unified CRF saliency model . We design a novel cascade CRFs architecture with CNN to jointly refine deep features and predictions at each scale and progressively compute a final refined saliency map. We formulate the CRF graphical model that involves message-passing of feature-feature, feature-prediction, and prediction-prediction, from the coarse scale to the finer scale, to update the features and the corresponding predictions. Also, we formulate the mean-field updates for joint end-to-end model training with CNN through back propagation. The proposed deep unified CRF saliency model is evaluated over six datasets and shows highly competitive performance among the state of the arts.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Structured_Modeling_of_Joint_Deep_Feature_and_Prediction_Refinement_for_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Structured_Modeling_of_Joint_Deep_Feature_and_Prediction_Refinement_for_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009087/,"['Predictive models', 'Computational modeling', 'Training', 'Kernel', 'Context modeling', 'Feature extraction', 'Computer vision']","['Deep Features', 'Salient Object', 'Feature Refinement', 'Salient Object Detection', 'Joint Prediction', 'Neural Network', 'Convolutional Neural Network', 'Deep Models', 'State Of The Art', 'Backpropagation', 'Unified Model', 'Graphical Model', 'Conditional Random Field', 'Saliency Map', 'Convolutional Neural Network Features', 'Convolutional Neural Network Prediction', 'Saliency Models', 'Learning Rate', 'Gaussian Kernel', 'Feature Maps', 'Prediction Map', 'Multi-scale Features', 'Output Side', 'Multi-scale Convolutional Neural Network', 'Map Scale', 'Convolutional Neural Networks Backbone', 'L Scale', 'Joint Training', 'Pre-training Stage', 'Kernel Similarity']",,37,"Recent saliency models extensively explore to incorporate multi-scale contextual information from Convolutional Neural Networks (CNNs). Besides direct fusion strategies, many approaches introduce message-passing to enhance CNN features or predictions. However, the messages are mainly transmitted in two ways, by feature-to-feature passing, and by prediction-to-prediction passing. In this paper, we add message-passing between features and predictions and propose a deep unified CRF saliency model . We design a novel cascade CRFs architecture with CNN to jointly refine deep features and predictions at each scale and progressively compute a final refined saliency map. We formulate the CRF graphical model that involves message-passing of feature-feature, feature-prediction, and prediction-prediction, from the coarse scale to the finer scale, to update the features and the corresponding predictions. Also, we formulate the mean-field updates for joint end-to-end model training with CNN through back propagation. The proposed deep unified CRF saliency model is evaluated over six datasets and shows highly competitive performance among the state of the arts."
Structured Prediction Helps 3D Human Motion Modelling,"Emre Aksan, Manuel Kaufmann, Otmar Hilliges","Department of Computer Science, ETH Zürich",100.0,Switzerland,0.0,,"Human motion prediction is a challenging and important task in many computer vision application domains. Existing work only implicitly models the spatial structure of the human skeleton. In this paper, we propose a novel approach that decomposes the prediction into individual joints by means of a structured prediction layer that explicitly models the joint dependencies. This is implemented via a hierarchy of small-sized neural networks connected analogously to the kinematic chains in the human body as well as a joint-wise decomposition in the loss function. The proposed layer is agnostic to the underlying network and can be used with existing architectures for motion modelling. Prior work typically leverages the H3.6M dataset. We show that some state-of-the-art techniques do not perform well when trained and tested on AMASS, a recently released dataset 14 times the size of H3.6M. Our experiments indicate that the proposed layer increases the performance of motion forecasting irrespective of the base network, joint-angle representation, and prediction horizon. We furthermore show that the layer also improves motion predictions qualitatively. We make code and models publicly available at https://ait.ethz.ch/projects/2019/spl.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Aksan_Structured_Prediction_Helps_3D_Human_Motion_Modelling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Aksan_Structured_Prediction_Helps_3D_Human_Motion_Modelling_ICCV_2019_paper.pdf,https://ait.ethz.ch/projects/2019/spl,,,main,Poster,https://ieeexplore.ieee.org/document/9009821/,"['Computer architecture', 'Computational modeling', 'Predictive models', 'Skeleton', 'Task analysis', 'Three-dimensional displays', 'Kinematics']","['Structure Prediction', 'Motion Model', 'Human Motion', 'Loss Function', 'Neural Network', 'Human Bone', 'Explicit Model', 'Joint Angles', 'Prediction Horizon', 'Motion Prediction', 'Individual Joint', 'Kinematic Chain', 'Network Hierarchy', 'Time Step', 'Hidden Layer', 'Recurrent Neural Network', 'Dense Layer', 'Latent Space', 'Prediction Task', 'Spatial Dependence', 'Seq2seq Model', 'Euler Angles', 'Recurrent Neural Network Model', 'Motion Sequences', 'Pose Estimation', 'Residual Connection', 'Exponential Map', 'Short-term Prediction', 'Variational Autoencoder', 'Hidden Layer Units']",,134,"Human motion prediction is a challenging and important task in many computer vision application domains. Existing work only implicitly models the spatial structure of the human skeleton. In this paper, we propose a novel approach that decomposes the prediction into individual joints by means of a structured prediction layer that explicitly models the joint dependencies. This is implemented via a hierarchy of small-sized neural networks connected analogously to the kinematic chains in the human body as well as a joint-wise decomposition in the loss function. The proposed layer is agnostic to the underlying network and can be used with existing architectures for motion modelling. Prior work typically leverages the H3.6M dataset. We show that some state-of-the-art techniques do not perform well when trained and tested on AMASS, a recently released dataset 14 times the size of H3.6M. Our experiments indicate that the proposed layer increases the performance of motion forecasting irrespective of the base network, joint-angle representation, and prediction horizon. We furthermore show that the layer also improves motion predictions qualitatively. We make code and models publicly available at https://ait.ethz.ch/projects/2019/spl."
Subspace Structure-Aware Spectral Clustering for Robust Subspace Clustering,"Masataka Yamaguchi, Go Irie, Takahito Kawanishi, Kunio Kashino","NTT Communication Science Laboratories, NTT Corporation, Japan",0.0,,100.0,Japan,"Subspace clustering is the problem of partitioning data drawn from a union of multiple subspaces. The most popular subspace clustering framework in recent years is the graph clustering-based approach, which performs subspace clustering in two steps: graph construction and graph clustering. Although both steps are equally important for accurate clustering, the vast majority of work has focused on improving the graph construction step rather than the graph clustering step. In this paper, we propose a novel graph clustering framework for robust subspace clustering. By incorporating a geometry-aware term with the spectral clustering objective, we encourage our framework to be robust to noise and outliers in given affinity matrices. We also develop an efficient expectation-maximization-based algorithm for optimization. Through extensive experiments on four real-world datasets, we demonstrate that the proposed method outperforms existing methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008831/,"['Clustering methods', 'Robustness', 'Clustering algorithms', 'Standards', 'Probabilistic logic', 'Optimization', 'Sparse matrices']","['Spectral Clustering', 'Subspace Clustering', 'Extensive Experiments', 'Real-world Datasets', 'Graph Construction', 'Affinity Matrix', 'Construction Steps', 'Clustering Step', 'Clustering Accuracy', 'Clustering Framework', 'Vast Majority Of Work', 'Simplex', 'Mixture Model', 'Expectation Maximization', 'Data Clustering', 'Clustering Results', 'Estimation Problem', 'Geometric Structure', 'Structure Of Space', 'Matrix M', 'Low-dimensional Subspace', 'Ranking Function', 'Maximum A Posteriori', 'Gaussian Mixture Model', 'Ambient Space', 'Noise Intensity', 'Single Optimization Problem', 'Depth-first', 'Nuclear Norm', 'Clusters In Space']",,2,"Subspace clustering is the problem of partitioning data drawn from a union of multiple subspaces. The most popular subspace clustering framework in recent years is the graph clustering-based approach, which performs subspace clustering in two steps: graph construction and graph clustering. Although both steps are equally important for accurate clustering, the vast majority of work has focused on improving the graph construction step rather than the graph clustering step. In this paper, we propose a novel graph clustering framework for robust subspace clustering. By incorporating a geometry-aware term with the spectral clustering objective, we encourage our framework to be robust to noise and outliers in given affinity matrices. We also develop an efficient expectation-maximization-based algorithm for optimization. Through extensive experiments on four real-world datasets, we demonstrate that the proposed method outperforms existing methods."
Surface Networks via General Covers,"Niv Haim, Nimrod Segol, Heli Ben-Hamu, Haggai Maron, Yaron Lipman","Weizmann Institute of Science, Rehovot, Israel",100.0,israel,0.0,,"Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning. We have used the surface-to-image representation to apply standard CNN architectures to 3D models including spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Haim_Surface_Networks_via_General_Covers_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Haim_Surface_Networks_via_General_Covers_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008813/,"['Convolution', 'Distortion', 'Shape', 'Two dimensional displays', 'Image segmentation', 'Task analysis', 'Topology']","['Deep Learning', 'Convolutional Neural Network', 'State Of The Art', 'Single Image', 'Convolutional Neural Network Model', 'Learning Problem', 'Semantic Segmentation', 'Part Of Surface', 'Image Domain', 'Geometric Data', 'Low Distortion', 'Shape Classification', 'Neural Network', 'Learning Rate', 'Learning Task', 'Point Cloud', 'Generative Adversarial Networks', 'Branch Points', 'Equivalency', 'Image Representation', 'Tangent Plane', 'Global Parameters', 'Triangular Mesh', 'Spherical Projection', 'Segment Boundaries']",,27,"Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning. We have used the surface-to-image representation to apply standard CNN architectures to 3D models including spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation."
Surface Normals and Shape From Water,"Satoshi Murai, Meng-Yu Jennifer Kuo, Ryo Kawahara, Shohei Nobuhara, Ko Nishino","Kyoto University, Kyoto, Japan",100.0,japan,0.0,,"In this paper, we introduce a novel method for reconstructing surface normals and depth of dynamic objects in water. Past shape recovery methods have leveraged various visual cues for estimating shape (e.g., depth) or surface normals. Methods that estimate both compute one from the other. We show that these two geometric surface properties can be simultaneously recovered for each pixel when the object is observed underwater. Our key idea is to leverage multi-wavelength near-infrared light absorption along different underwater light paths in conjunction with surface shading. We derive a principled theory for this surface normals and shape from water method and a practical calibration method for determining its imaging parameters values. By construction, the method can be implemented as a one-shot imaging system. We prototype both an off-line and a video-rate imaging system and demonstrate the effectiveness of the method on a number of real-world static and dynamic objects. The results show that the method can recover intricate surface features that are otherwise inaccessible.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Murai_Surface_Normals_and_Shape_From_Water_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Murai_Surface_Normals_and_Shape_From_Water_ICCV_2019_paper.pdf,http://vision.ist.i.kyoto-u.ac.jp/,,,main,Oral,https://ieeexplore.ieee.org/document/9583771/,"['Surface reconstruction', 'Shape', 'Three-dimensional displays', 'Absorption', 'Light sources', 'Surface waves', 'Water resources']","['Surface Shape', 'Surface Normals', 'Light Absorption', 'Near-infrared Light', 'Recovery Method', 'Static Objects', 'Dynamic Objects', 'Surface Depth', 'Water Objective', 'Near-infrared Absorption', 'Object Depth', 'Absorption Of Near-infrared Light', 'Illumination', 'Light Source', 'Absorption Coefficient', '3D Reconstruction', 'Reconstruction Method', 'Global Solution', 'Side Of Eq', 'Normal Approximation', 'Direct Light', 'Intensity Of The Light Source', 'Auxiliary Source', 'Wavelength Combinations', 'Depth Estimation', 'Simultaneous Estimation', 'Object Surface', 'Hand Side Of Eq', 'Shape Estimation', 'Dynamic Surface']","['Computational photography', 'underwater reconstruction', 'near-infrared light', 'absorption']",2,"In this paper, we introduce a novel method for reconstructing surface normals and depth of dynamic objects in water. Past shape recovery methods have leveraged various visual cues for estimating shape (e.g., depth) or surface normals. Methods that estimate both compute one from the other. We show that these two geometric surface properties can be simultaneously recovered for each pixel when the object is observed underwater. Our key idea is to leverage multi-wavelength near-infrared light absorption along different underwater light paths in conjunction with surface shading. Our method can handle both Lambertian and non-Lambertian surfaces. We derive a principled theory for this surface normals and shape from water method and a practical calibration method for determining its imaging parameters values. By construction, the method can be implemented as a one-shot imaging system. We prototype both an off-line and a video-rate imaging system and demonstrate the effectiveness of the method on a number of real-world static and dynamic objects. The results show that the method can recover intricate surface features that are otherwise inaccessible."
Switchable Whitening for Deep Representation Learning,"Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, Ping Luo","CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong; The University of Hong Kong; SenseTime Group Limited",100.0,"Hong Kong, china, usa",0.0,,"Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010963/,"['Standardization', 'Switches', 'Task analysis', 'Decorrelation', 'Covariance matrices', 'Training', 'Semantics']","['Deep Learning', 'Neural Network', 'Convolutional Neural Network', 'Standard Techniques', 'General Form', 'Image Classification', 'Normalization Method', 'ImageNet', 'Semantic Segmentation', 'Image Domain', 'Domain Adaptation', 'Normalization Techniques', 'Style Transfer', 'Style Image', 'Challenging Benchmark', 'Adaptive Transfer', 'Training Set', 'Covariance Matrix', 'Deep Neural Network', 'Time Complexity', 'Maximum Mean Discrepancy', 'Normalization Layer', 'Importance Weights', 'Domain Discrepancy', 'Instance Normalization', 'Convolutional Neural Network Features', 'Image Appearance', 'Eigendecomposition', 'Batch Normalization', 'Appearance Information']",,82,"Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset."
Sym-Parameterized Dynamic Inference for Mixed-Domain Image Translation,"Simyung Chang, SeongUk Park, John Yang, Nojun Kwak","Seoul National University, Seoul, Korea; Samsung Electronics, Suwon, Korea; Seoul National University, Seoul, Korea",66.66666666666666,south korea,33.33333333333334,South Korea,"Recent advances in image-to-image translation have led to some ways to generate multiple domain images through a single network. However, there is still a limit in creating an image of a target domain without a dataset on it. We propose a method to expand the concept of `multi-domain' from data to the loss area, and to combine the characteristics of each domain to create an image. First, we introduce a sym-parameter and its learning method that can mix various losses and can synchronize them with input conditions. Then, we propose Sym-parameterized Generative Network (SGN) using it. Through experiments, we confirmed that SGN could mix the characteristics of various data and losses, and it is possible to translate images to any mixed-domain without ground truths, such as 30% Van Gogh and 20% Monet and 40% snowy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chang_Sym-Parameterized_Dynamic_Inference_for_Mixed-Domain_Image_Translation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Sym-Parameterized_Dynamic_Inference_for_Mixed-Domain_Image_Translation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008803/,"['Generators', 'Training', 'Gallium nitride', 'Image reconstruction', 'Image generation', 'Generative adversarial networks', 'Task analysis']","['Single Network', 'Van Gogh', 'Loss Function', 'Weight Loss', 'Objective Function', 'Input Image', 'Dimensional Vector', 'Loss Value', 'Generative Adversarial Networks', 'Image Generation', 'Residual Block', 'Spatial Size', 'Injection Method', 'Reconstruction Loss', 'Channel Attention', 'Multiple Losses', 'Perceptual Loss', 'Dirichlet Distribution', 'Linear Combination Of Functions', 'Combined Loss Function', 'Generative Adversarial Networks Loss', 'Discrete Conditions', 'Combination Of Functions', 'Continuous Values']",,7,"Recent advances in image-to-image translation have led to some ways to generate multiple domain images through a single network. However, there is still a limit in creating an image of a target domain without a dataset on it. We propose a method to expand the concept of `multi-domain' from data to the loss area, and to combine the characteristics of each domain to create an image. First, we introduce a sym-parameter and its learning method that can mix various losses and can synchronize them with input conditions. Then, we propose Sym-parameterized Generative Network (SGN) using it. Through experiments, we confirmed that SGN could mix the characteristics of various data and losses, and it is possible to translate images to any mixed-domain without ground truths, such as 30% Van Gogh and 20% Monet and 40% snowy."
Symmetric Cross Entropy for Robust Learning With Noisy Labels,"Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, James Bailey",Cainiao AI; The University of Melbourne; JD AI; Shanghai Jiao Tong University,50.0,"China, australia",50.0,USA,"Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (""easy"" classes), but more surprisingly, it also suffers from significant under learning on some other classes (""hard"" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Symmetric_Cross_Entropy_for_Robust_Learning_With_Noisy_Labels_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Symmetric_Cross_Entropy_for_Robust_Learning_With_Noisy_Labels_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010653/,"['Noise measurement', 'Training', 'Entropy', 'Neural networks', 'Robustness', 'Artificial intelligence', 'Task analysis']","['Cross-entropy', 'Noisy Labels', 'Symmetric Cross Entropy', 'Neural Network', 'Deep Neural Network', 'Real-world Datasets', 'Loss Function', 'Overfitting', 'Convolutional Neural Network', 'Clean Data', 'Cross-entropy Loss', 'Large-scale Datasets', 'Weight Decay', 'Learning Problem', 'Predictive Distribution', 'Learning Stage', 'Correct Label', 'Single Label', 'Noise Rate', 'Label Noise', 'Incorrect Labels', 'Standard Cross-entropy Loss', 'True Positive Samples', 'Soft Labels', 'Noise Set', 'Previous Beliefs', 'Accuracy Drop', 'Clear Set', 'Learning Process', 'Entropy Loss']",,483,"Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (""easy"" classes), but more surprisingly, it also suffers from significant under learning on some other classes (""hard"" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance."
Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning,"Jiwoong Park, Minsik Lee, Hyung Jin Chang, Kyuewang Lee, Jin Young Choi","ASRI, Dept. of ECE., Seoul National University; School of Computer Science, University of Birmingham; Div. of EE., Hanyang University",100.0,"south korea, uk",0.0,,"We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Park_Symmetric_Graph_Convolutional_Autoencoder_for_Unsupervised_Graph_Representation_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Symmetric_Graph_Convolutional_Autoencoder_for_Unsupervised_Graph_Representation_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010004/,"['Laplace equations', 'Decoding', 'Smoothing methods', 'Chebyshev approximation', 'Image reconstruction', 'Convolution', 'Task analysis']","['Graph Convolution', 'Symmetric Graph', 'Graph Convolutional Autoencoder', 'Cost Function', 'Graph Structure', 'Numerical Stability', 'Numerical Instability', 'Latent Representation', 'Node Features', 'Affinity Matrix', 'Link Prediction', 'Clustering Task', 'Decoder Part', 'Autoencoder Architecture', 'Decoding', 'Image Dataset', 'Singular Value Decomposition', 'Feature Matrix', 'Graph Convolutional Network', 'Cluster Nodes', 'Spectral Convolution', 'Subspace Clustering', 'Node Representations', 'Network Datasets', 'Least Squares Regression Model', 'Adjusted Rand Index', 'Graph Laplacian', 'Autoencoder Framework', 'Chebyshev Polynomials', 'Spectral Radius']",,129,"We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms."
Symmetry-Constrained Rectification Network for Scene Text Recognition,"Mingkun Yang, Yushuo Guan, Minghui Liao, Xin He, Kaigui Bian, Song Bai, Cong Yao, Xiang Bai",Peking University; Megvii (Face++) Inc.; Huazhong University of Science and Technology; University of Oxford,75.0,"china, uk",25.0,China,"Reading text in the wild is a very challenging task due to the diversity of text instances and the complexity of natural scenes. Recently, the community has paid increasing attention to the problem of recognizing text instances with irregular shapes. One intuitive and effective way to handle this problem is to rectify irregular text to a canonical form before recognition. However, these methods might struggle when dealing with highly curved or distorted text instances. To tackle this issue, we propose in this paper a Symmetry-constrained Rectification Network (ScRN) based on local attributes of text instances, such as center line, scale and orientation. Such constraints with an accurate description of text shape enable ScRN to generate better rectification results than existing methods and thus lead to higher recognition accuracy. Our method achieves state-of-the-art performance on text with both regular and irregular shapes. Specifically, the system outperforms existing algorithms by a large margin on datasets that contain quite a proportion of irregular text instances, e.g., ICDAR 2015, SVT-Perspective and CUTE80.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Symmetry-Constrained_Rectification_Network_for_Scene_Text_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Symmetry-Constrained_Rectification_Network_for_Scene_Text_Recognition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008843/,,"['Optical Character Recognition', 'Recognition Network', 'Correction Network', 'Scene Text', 'Scene Text Recognition', 'Irregular Shape', 'Central Line', 'Recognition Accuracy', 'Shape Descriptors', 'Convolutional Layers', 'Input Image', 'Feature Maps', 'Central Point', 'Sequence Features', 'Training Strategy', 'Geometric Properties', 'Bounding Box', 'Control Points', 'Shared Features', 'Nearest Point', 'Correction Module', 'Recognition Module', 'Midpoint Of Edge', 'Fiducial Points', 'Regular Ones', 'Loss Of Recognition', 'Sequence Of Characters', 'Street View', 'Recognition Results', 'Image Texture']",,109,"Reading text in the wild is a very challenging task due to the diversity of text instances and the complexity of natural scenes. Recently, the community has paid increasing attention to the problem of recognizing text instances with irregular shapes. One intuitive and effective way to handle this problem is to rectify irregular text to a canonical form before recognition. However, these methods might struggle when dealing with highly curved or distorted text instances. To tackle this issue, we propose in this paper a Symmetry-constrained Rectification Network (ScRN) based on local attributes of text instances, such as center line, scale and orientation. Such constraints with an accurate description of text shape enable ScRN to generate better rectification results than existing methods and thus lead to higher recognition accuracy. Our method achieves state-of-the-art performance on text with both regular and irregular shapes. Specifically, the system outperforms existing algorithms by a large margin on datasets that contain quite a proportion of irregular text instances, e.g., ICDAR 2015, SVT-Perspective and CUTE80."
SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion,"Behzad Bozorgtabar, Mohammad Saeed Rad, Dwarikanath Mahapatra, Jean-Philippe Thiran",; Ecole Polytechnique Fédérale de Lausanne (EPFL),100.0,"france, switzerland",0.0,,"Despite well-established baselines, learning of scene depth and ego-motion from monocular video remains an ongoing challenge, specifically when handling scaling ambiguity issues and depth inconsistencies in image sequences. Much prior work uses either a supervised mode of learning or stereo images. The former is limited by the amount of labeled data, as it requires expensive sensors, while the latter is not always readily available as monocular sequences. In this work, we demonstrate the benefit of using geometric information from synthetic images, coupled with scene depth information, to recover the scale in depth and ego-motion estimation from monocular videos. We developed our framework using synthetic image-depth pairs and unlabeled real monocular images. We had three training objectives: first, to use deep feature alignment to reduce the domain gap between synthetic and monocular images to yield more accurate depth estimation when presented with only real monocular images at test time. Second, we learn scene specific representation by exploiting self-supervision coming from multi-view synthetic images without the need for depth labels. Third, our method uses single-view depth and pose networks, which are capable of jointly training and supervising one another mutually, yielding consistent depth and ego-motion estimates. Extensive experiments demonstrate that our depth and ego-motion models surpass the state-of-the-art, unsupervised methods and compare favorably to early supervised deep models for geometric understanding. We validate the effectiveness of our training objectives against standard benchmarks thorough an ablation study.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bozorgtabar_SynDeMo_Synergistic_Deep_Feature_Alignment_for_Joint_Learning_of_Depth_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bozorgtabar_SynDeMo_Synergistic_Deep_Feature_Alignment_for_Joint_Learning_of_Depth_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010303/,"['Geometry', 'Streaming media', 'Training', 'Estimation', 'Decoding', 'Task analysis', 'Three-dimensional displays']","['Unsupervised Methods', 'Synthetic Images', 'Geometric Information', 'Training Objective', 'Depth Estimation', 'Stereo Images', 'Domain Gap', 'Multi-view Images', 'Monocular Images', 'Scene Depth', 'Scale Ambiguity', 'Convolutional Neural Network', 'Unsupervised Learning', 'Latent Space', 'Data Streams', 'Depth Map', 'Loss Term', 'Latent Representation', 'Self-supervised Learning', 'Scene Geometry', 'Unsupervised Learning Methods', 'Structure From Motion', 'Smoothness Loss', 'Stereo Image Pairs', 'Ground Truth Depth', 'Task Loss', 'Visual Odometry', 'Depth Prediction', 'KITTI Dataset']",,20,"Despite well-established baselines, learning of scene depth and ego-motion from monocular video remains an ongoing challenge, specifically when handling scaling ambiguity issues and depth inconsistencies in image sequences. Much prior work uses either a supervised mode of learning or stereo images. The former is limited by the amount of labeled data, as it requires expensive sensors, while the latter is not always readily available as monocular sequences. In this work, we demonstrate the benefit of using geometric information from synthetic images, coupled with scene depth information, to recover the scale in depth and ego-motion estimation from monocular videos. We developed our framework using synthetic image-depth pairs and unlabeled real monocular images. We had three training objectives: first, to use deep feature alignment to reduce the domain gap between synthetic and monocular images to yield more accurate depth estimation when presented with only real monocular images at test time. Second, we learn scene specific representation by exploiting self-supervision coming from multi-view synthetic images without the need for depth labels. Third, our method uses single-view depth and pose networks, which are capable of jointly training and supervising one another mutually, yielding consistent depth and ego-motion estimates. Extensive experiments demonstrate that our depth and ego-motion models surpass the state-of-the-art, unsupervised methods and compare favorably to early supervised deep models for geometric understanding. We validate the effectiveness of our training objectives against standard benchmarks thorough an ablation study."
TAPA-MVS: Textureless-Aware PAtchMatch Multi-View Stereo,"Andrea Romanoni, Matteo Matteucci","Politecnico di Milano, Italy",0.0,,100.0,Canada,"One of the most successful approaches in Multi-View Stereo estimates a depth map and a normal map for each view via PatchMatch-based optimization and fuses them into a consistent 3D points cloud. This approach relies on photo-consistency to evaluate the goodness of a depth estimate. It generally produces very accurate results; however, the reconstructed model often lacks completeness, especially in correspondence of broad untextured areas where the photo-consistency metrics are unreliable. Assuming the untextured areas piecewise planar, in this paper we generate novel PatchMatch hypotheses so to expand reliable depth estimates in neighboring untextured regions. At the same time, we modify the photo-consistency measure such to favor standard or novel PatchMatch depth hypotheses depending on the textureness of the considered area. We also propose a depth refinement step to filter wrong estimates and to fill the gaps on both the depth maps and normal maps while preserving the discontinuities. The effectiveness of our new methods has been tested against several state of the art algorithms in the publicly available ETH3D dataset containing a wide variety of high and low-resolution images.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Romanoni_TAPA-MVS_Textureless-Aware_PAtchMatch_Multi-View_Stereo_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Romanoni_TAPA-MVS_Textureless-Aware_PAtchMatch_Multi-View_Stereo_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010665/,"['Three-dimensional displays', 'Estimation', 'Cameras', 'Optimization', 'Measurement', 'Reliability', 'Image edge detection']","['Multi-view Stereo', 'Patch Matching', 'Point Cloud', 'Depth Map', '3D Point', 'Depth Estimation', 'Wrong Estimation', 'Normal Map', 'Reference Image', 'Source Images', 'Optimization Framework', 'Normal Approximation', 'Maximum A Posteriori', 'Training Sequences', 'Refinement Method', 'Mesh Refinement', 'Percentage Of Pixels', 'Parallax', 'Sum Of Squared Differences', 'Subset Of Pairs', 'Tower Of London', 'Stereo Matching', 'Reprojection Error', 'Geometric Consistency', 'Matching Cost']",,52,"One of the most successful approaches in Multi-View Stereo estimates a depth map and a normal map for each view via PatchMatch-based optimization and fuses them into a consistent 3D points cloud. This approach relies on photo-consistency to evaluate the goodness of a depth estimate. It generally produces very accurate results; however, the reconstructed model often lacks completeness, especially in correspondence of broad untextured areas where the photo-consistency metrics are unreliable. Assuming the untextured areas piecewise planar, in this paper we generate novel PatchMatch hypotheses so to expand reliable depth estimates in neighboring untextured regions. At the same time, we modify the photo-consistency measure such to favor standard or novel PatchMatch depth hypotheses depending on the textureness of the considered area. We also propose a depth refinement step to filter wrong estimates and to fill the gaps on both the depth maps and normal maps while preserving the discontinuities. The effectiveness of our new methods has been tested against several state of the art algorithms in the publicly available ETH3D dataset containing a wide variety of high and low-resolution images."
TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for Video Saliency Detection,"Kyle Min, Jason J. Corso","University of Michigan, Ann Arbor, MI 48109",100.0,usa,0.0,,"TASED-Net is a 3D fully-convolutional network architecture for video saliency detection. It consists of two building blocks: first, the encoder network extracts low-resolution spatiotemporal features from an input clip of several consecutive frames, and then the following prediction network decodes the encoded features spatially while aggregating all the temporal information. As a result, a single prediction map is produced from an input clip of multiple frames. Frame-wise saliency maps can be predicted by applying TASED-Net in a sliding-window fashion to a video. The proposed approach assumes that the saliency map of any frame can be predicted by considering a limited number of past frames. The results of our extensive experiments on video saliency detection validate this assumption and demonstrate that our fully-convolutional model with temporal aggregation method is effective. TASED-Net significantly outperforms previous state-of-the-art approaches on all three major large-scale datasets of video saliency detection: DHF1K, Hollywood2, and UCFSports. After analyzing the results qualitatively, we observe that our model is especially better at attending to salient moving objects.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Min_TASED-Net_Temporally-Aggregating_Spatial_Encoder-Decoder_Network_for_Video_Saliency_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Min_TASED-Net_Temporally-Aggregating_Spatial_Encoder-Decoder_Network_for_Video_Saliency_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010000/,"['Feature extraction', 'Three-dimensional displays', 'Saliency detection', 'Decoding', 'Spatiotemporal phenomena', 'Two dimensional displays', 'Convolution']","['Encoder-decoder Network', 'Saliency Detection', 'Video Saliency', 'Video Saliency Detection', 'Large-scale Datasets', 'Temporal Information', 'Prediction Network', 'Spatiotemporal Characteristics', 'Single Map', 'Prediction Map', 'Saliency Map', 'Encoder Network', 'Architecture For Detection', 'Temporal Aggregation', 'Validation Set', 'Convolutional Layers', 'Spatial Information', 'Feature Maps', 'Spatial Features', 'Fixed Point', 'Temporal Convolution', 'Transposed Convolution Layers', 'Video Frames', 'Dynamic Datasets', 'Convolutional Block', 'Optical Flow', 'Saliency Models', 'Temporal Dimension', 'Image Saliency', 'Up-sampling Operation']",,113,"TASED-Net is a 3D fully-convolutional network architecture for video saliency detection. It consists of two building blocks: first, the encoder network extracts low-resolution spatiotemporal features from an input clip of several consecutive frames, and then the following prediction network decodes the encoded features spatially while aggregating all the temporal information. As a result, a single prediction map is produced from an input clip of multiple frames. Frame-wise saliency maps can be predicted by applying TASED-Net in a sliding-window fashion to a video. The proposed approach assumes that the saliency map of any frame can be predicted by considering a limited number of past frames. The results of our extensive experiments on video saliency detection validate this assumption and demonstrate that our fully-convolutional model with temporal aggregation method is effective. TASED-Net significantly outperforms previous state-of-the-art approaches on all three major large-scale datasets of video saliency detection: DHF1K, Hollywood2, and UCFSports. After analyzing the results qualitatively, we observe that our model is especially better at attending to salient moving objects."
TRB: A Novel Triplet Representation for Understanding 2D Human Body,"Haodong Duan, Kwan-Yee Lin, Sheng Jin, Wentao Liu, Chen Qian, Wanli Ouyang","CUHK-Sensetime Joint Lab; SenseTime Group Limited; The University of Sydney, SenseTime Computer Vision Research Group, Australia",100.0,"australia, china, usa",0.0,,"Human pose and shape are two important components of 2D human body. However, how to efficiently represent both of them in images is still an open question. In this paper, we propose the Triplet Representation for Body (TRB) --- a compact 2D human body representation, with skeleton keypoints capturing human pose information and contour keypoints containing human shape information. TRB not only preserves the flexibility of skeleton keypoint representation, but also contains rich pose and human shape information. Therefore, it promises broader application areas, such as human shape editing and conditional image generation. We further introduce the challenging problem of TRB estimation, where joint learning of human pose and shape is required. We construct several large-scale TRB estimation datasets, based on the popular 2D pose datasets LSP, MPII and COCO. To effectively solve TRB estimation, we propose a two-branch network (TRB-net) with three novel techniques, namely X-structure (Xs), Directional Convolution (DC) and Pairwise mapping (PM), to enforce multi-level message passing for joint feature learning. We evaluate our proposed TRB-net and several leading approaches on our proposed TRB datasets, and demonstrate the superiority of our method through extensive evaluations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Duan_TRB_A_Novel_Triplet_Representation_for_Understanding_2D_Human_Body_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_TRB_A_Novel_Triplet_Representation_for_Understanding_2D_Human_Body_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010030/,"['Skeleton', 'Shape', 'Two dimensional displays', 'Estimation', 'Task analysis', 'Message passing', 'Convolution']","['2D Human Body', 'Feature Learning', 'Body Representation', 'Compact Representation', 'Joint Learning', 'Human Pose', 'Pairwise Function', 'Message Passing', 'Pose Information', 'Human Shape', '2D Pose', 'Local Information', 'Feature Maps', 'Localization Accuracy', 'Convolution Operation', 'Pose Estimation', 'Multi-task Learning', 'Semantic Labels', 'Human Pose Estimation', '2D Shape', 'Accurate Location Information', 'Contour Points', 'Keypoint Locations', 'Transformation Module']",,12,"Human pose and shape are two important components of 2D human body. However, how to efficiently represent both of them in images is still an open question. In this paper, we propose the Triplet Representation for Body (TRB) --- a compact 2D human body representation, with skeleton keypoints capturing human pose information and contour keypoints containing human shape information. TRB not only preserves the flexibility of skeleton keypoint representation, but also contains rich pose and human shape information. Therefore, it promises broader application areas, such as human shape editing and conditional image generation. We further introduce the challenging problem of TRB estimation, where joint learning of human pose and shape is required. We construct several large-scale TRB estimation datasets, based on the popular 2D pose datasets LSP, MPII and COCO. To effectively solve TRB estimation, we propose a two-branch network (TRB-net) with three novel techniques, namely X-structure (Xs), Directional Convolution (DC) and Pairwise mapping (PM), to enforce multi-level message passing for joint feature learning. We evaluate our proposed TRB-net and several leading approaches on our proposed TRB datasets, and demonstrate the superiority of our method through extensive evaluations."
TSM: Temporal Shift Module for Efficient Video Understanding,"Ji Lin, Chuang Gan, Song Han",MIT; MIT-IBM Watson AI Lab,100.0,usa,0.0,,"The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf,,https://github.com/mit-han-lab/temporal-shift-module,,main,Poster,https://ieeexplore.ieee.org/document/9008827/,"['Two dimensional displays', 'Computational modeling', 'Streaming media', 'Three-dimensional displays', 'Convolution', 'Solid modeling', 'Real-time systems']","['Temporal Modulation', 'Temporal Shift', 'Video Understanding', 'Temporal Shift Module', 'Computational Cost', 'Temporal Dimension', 'Object Detection', 'Temporal Relationship', 'Low Latency', 'Online Video', 'Temporal Model', 'Real-time Video', 'Real-time Online', 'Video Recognition', 'High Efficiency Performance', 'Feature Maps', 'Spatial Memory', 'Efficient Model', 'Early Recognition', '3D Network', 'Optical Flow', 'Residual Shift', 'Load Data', 'Edge Devices', 'Future Frames', 'Shift Operator', 'Current Frame', 'Region Proposal Network', '3D Convolution', 'Efficient Framework']",,1141,"The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN’s complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module."
Tag2Pix: Line Art Colorization Using Text Tag With SECat and Changing Loss,"Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, Sungjoo Yoo",Seoul National University,100.0,south korea,0.0,,"Line art colorization is expensive and challenging to automate. A GAN approach is proposed, called Tag2Pix, of line art colorization which takes as input a grayscale line art and color tag information and produces a quality colored image. First, we present the Tag2Pix line art colorization dataset. A generator network is proposed which consists of convolutional layers to transform the input line art, a pre-trained semantic extraction network, and an encoder for input color information. The discriminator is based on an auxiliary classifier GAN to classify the tag information as well as genuineness. In addition, we propose a novel network structure called SECat, which makes the generator properly colorize even small features such as eyes, and also suggest a novel two-step training method where the generator and discriminator first learn the notion of object and shape and then, based on the learned notion, learn colorization, such as where and how to place which color. We present both quantitative and qualitative evaluations which prove the effectiveness of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Tag2Pix_Line_Art_Colorization_Using_Text_Tag_With_SECat_and_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Tag2Pix_Line_Art_Colorization_Using_Text_Tag_With_SECat_and_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009840/,"['Art', 'Image color analysis', 'Feature extraction', 'Generators', 'Gallium nitride', 'Decoding', 'Color']","['Line Art', 'Convolutional Layers', 'Color Images', 'Generative Adversarial Networks', 'Pre-trained Network', 'Small Features', 'Tag Information', 'Amount Of Information', 'Feature Maps', 'User Study', 'Grayscale Images', 'Image Generation', 'Semantic Segmentation', 'Fully-connected Layer', 'Affine Transformation', 'Reconstruction Loss', 'Vanishing Gradient Problem', 'Eye Color', 'RGB Values', 'Red Hair', 'Fr√©chet Inception Distance', 'Decoder Block', 'Intermediate Feature Maps', 'StyleGAN', 'Manjunatha', 'Color Mixing', 'Block Generation', 'Embedding Methods', 'Weighting Factor', 'Input Image']",,68,"Line art colorization is expensive and challenging to automate. A GAN approach is proposed, called Tag2Pix, of line art colorization which takes as input a grayscale line art and color tag information and produces a quality colored image. First, we present the Tag2Pix line art colorization dataset. A generator network is proposed which consists of convolutional layers to transform the input line art, a pre-trained semantic extraction network, and an encoder for input color information. The discriminator is based on an auxiliary classifier GAN to classify the tag information as well as genuineness. In addition, we propose a novel network structure called SECat, which makes the generator properly colorize even small features such as eyes, and also suggest a novel two-step training method where the generator and discriminator first learn the notion of object and shape and then, based on the learned notion, learn colorization, such as where and how to place which color. We present both quantitative and qualitative evaluations which prove the effectiveness of the proposed method."
Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded,"Ramprasaath R. Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, Devi Parikh","Georgia Institute of Technology; Georgia Institute of Technology, Facebook AI Research; Samsung Research; Georgia Institute of Technology, Oregon State University",75.0,usa,25.0,USA,"Many vision and language models suffer from poor visual grounding -- often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances -- ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009041/,"['Visualization', 'Grounding', 'Task analysis', 'Proposals', 'Training', 'Tuning', 'Correlation']","['Language Model', 'Visual Model', 'Deep Network', 'Attention Map', 'Human Attention', 'Input Regions', 'Image Captioning', 'Concept Of Image', 'Visual Concepts', 'Visual Question Answering', 'Rank Correlation', 'Number Of Workers', 'Long Short-term Memory', 'Image Regions', 'Attention Mechanism', 'Top-down Approach', 'Importance Scores', 'Set Of Scores', 'Language Tasks', 'Top-down Attention', 'Ranking Loss', 'Qualitative Examples', 'Language Bias', 'Language Components', 'Bottom-up Model', 'Visual Explanation', 'Human Supervision', 'Task Loss', 'Adversary Model']",,141,"Many vision and language models suffer from poor visual grounding -- often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances -- ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data."
Talking With Hands 16.2M: A Large-Scale Dataset of Synchronized Body-Finger Motion and Audio for Conversational Motion Analysis and Synthesis,"Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S. Srinivasa, Yaser Sheikh",University of Washington; Facebook Reality Labs; Simon Fraser University,66.66666666666666,"canada, usa",33.33333333333334,USA,"We present a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. Our dataset features synchronized body and finger motion as well as audio data. To the best of our knowledge, it represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis. As an illustration, we propose a novel real-time finger motion synthesis method: a temporal neural network innovatively trained with an inverse kinematics (IK) loss, which adds skeletal structural information to the generative model. Our qualitative user study shows that the finger motion generated by our method is perceived as natural and conversation enhancing, while the quantitative ablation study demonstrates the effectiveness of IK loss.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Talking_With_Hands_16.2M_A_Large-Scale_Dataset_of_Synchronized_Body-Finger_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Talking_With_Hands_16.2M_A_Large-Scale_Dataset_of_Synchronized_Body-Finger_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010909/,"['Hidden Markov models', 'Kinematics', 'Cameras', 'Neural networks', 'Three-dimensional displays', 'Task analysis', 'Synchronization']","['Neural Network', 'User Study', 'Motion Capture', 'Body Motion', 'Audio Data', 'Inverse Kinematics', 'Multimodal Dataset', 'Real-time Motion', 'Finger Motion', 'Strong Covariation', 'Social Interaction', 'Hidden Markov Model', 'Long Short-term Memory', 'Large-scale Datasets', 'Joint Angles', 'Depth Camera', 'Acoustic Features', 'Human Motion', 'Temporal Model', 'Variational Autoencoder', 'Arm Joints', 'Kinematic Constraints', 'Temporal Convolutional Network', 'Optical Motion Capture System', 'Conversation Task', 'Motor Scores', 'Arm Motion', 'Directional Microphone', 'Historical Observations', 'Kinematic Chain']",,27,"We present a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. Our dataset features synchronized body and finger motion as well as audio data. To the best of our knowledge, it represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis. As an illustration, we propose a novel real-time finger motion synthesis method: a temporal neural network innovatively trained with an inverse kinematics (IK) loss, which adds skeletal structural information to the generative model. Our qualitative user study shows that the finger motion generated by our method is perceived as natural and conversation enhancing, while the quantitative ablation study demonstrates the effectiveness of IK loss."
Targeted Mismatch Adversarial Attack: Query With a Flower to Retrieve the Tower,"Giorgos Tolias, Filip Radenovic, OndÅej Chum","Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague",100.0,Czech Republic,0.0,,"Access to online visual search engines implies sharing of private user content -- the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tolias_Targeted_Mismatch_Adversarial_Attack_Query_With_a_Flower_to_Retrieve_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tolias_Targeted_Mismatch_Adversarial_Attack_Query_With_a_Flower_to_Retrieve_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008294/,"['Visualization', 'Image retrieval', 'Image resolution', 'Optimization', 'Tensile stress', 'Search engines', 'Neural networks']","['Adversarial Attacks', 'Loss Function', 'Search Engine', 'Global Pooling', 'Pooling Operation', 'Retrieval System', 'Query Image', 'Global Pooling Operation', 'Neural Network', 'High-dimensional', 'Convolutional Neural Network', 'Image Resolution', 'Image Classification', 'Target Image', 'Performance Loss', 'Target Class', 'Types Of Attacks', 'Image Retrieval', 'Nearest Neighbor Search', 'Original Resolution', 'Fully Convolutional Network', 'Image X', 'Projected Gradient Descent', 'Global Descriptors', 'Minimum Of The Loss Function', 'Central Bin', 'Box Constraints', 'Retrieval Performance', 'Histogram', 'Changes In Variables']",,41,"Access to online visual search engines implies sharing of private user content - the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image."
Task-Driven Modular Networks for Zero-Shot Compositional Learning,"Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, Marc'Aurelio Ranzato",Carnegie Mellon University; Facebook AI Research,50.0,usa,50.0,USA,"One of the hallmarks of human intelligence is the ability to compose learned knowledge into novel concepts which can be recognized without a single training example. In contrast, current state-of-the-art methods require hundreds of training examples for each possible category to build reliable and accurate classifiers. To alleviate this striking difference in efficiency, we propose a task-driven modular architecture for compositional reasoning and sample efficient learning. Our architecture consists of a set of neural network modules, which are small fully connected layers operating in semantic concept space. These modules are configured through a gating function conditioned on the task to produce features representing the compatibility between the input image and the concept under consideration. This enables us to express tasks as a combination of sub-tasks and to generalize to unseen categories by reweighting a set of small modules. Furthermore, the network can be trained efficiently as it is fully differentiable and its modules operate on small sub-spaces. We focus our study on the problem of compositional zero-shot classification of object-attribute categories. We show in our experiments that current evaluation metrics are flawed as they only consider unseen object-attribute pairs. When extending the evaluation to the generalized setting which accounts also for pairs seen during training, we discover that naive baseline methods perform similarly or better than current approaches. However, our modular network is able to outperform all existing approaches on two widely-used benchmark datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Purushwalkam_Task-Driven_Modular_Networks_for_Zero-Shot_Compositional_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Purushwalkam_Task-Driven_Modular_Networks_for_Zero-Shot_Compositional_Learning_ICCV_2019_paper.pdf,http://www.cs.cmu.edu/~spurushw/projects/compositional.html,,,main,Poster,https://ieeexplore.ieee.org/document/9010265/,"['Task analysis', 'Training', 'Feature extraction', 'Visualization', 'Dogs', 'Cognition', 'Semantics']","['Network Modularity', 'Zero-shot', 'Neural Network', 'Input Image', 'Fully-connected Layer', 'Sampling Efficiency', 'Training Examples', 'Semantic Space', 'Training Set', 'Validation Set', 'Feature Space', 'Wrinkles', 'Modularity', 'Recent Approaches', 'Evaluation Protocol', 'Linear Projection', 'Recognition Network', 'Visual Similarity', 'Unseen Classes', 'Visual Concepts', 'Gate Model']",,76,"One of the hallmarks of human intelligence is the ability to compose learned knowledge into novel concepts which can be recognized without a single training example. In contrast, current state-of-the-art methods require hundreds of training examples for each possible category to build reliable and accurate classifiers. To alleviate this striking difference in efficiency, we propose a task-driven modular architecture for compositional reasoning and sample efficient learning. Our architecture consists of a set of neural network modules, which are small fully connected layers operating in semantic concept space. These modules are configured through a gating function conditioned on the task to produce features representing the compatibility between the input image and the concept under consideration. This enables us to express tasks as a combination of sub-tasks and to generalize to unseen categories by reweighting a set of small modules. Furthermore, the network can be trained efficiently as it is fully differentiable and its modules operate on small sub-spaces. We focus our study on the problem of compositional zero-shot classification of object-attribute categories. We show in our experiments that current evaluation metrics are flawed as they only consider unseen object-attribute pairs. When extending the evaluation to the generalized setting which accounts also for pairs seen during training, we discover that naive baseline methods perform similarly or better than current approaches. However, our modular network is able to outperform all existing approaches on two widely-used benchmark datasets."
Task2Vec: Task Embedding for Meta-Learning,"Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, Pietro Perona","Caltech; University of California, Irvine; University of Massachusetts, Amherst; University of California, Los Angeles; Amazon; AWS, University of California, Los Angeles",83.33333333333334,"USA, usa",16.666666666666657,USA,"We introduce a method to generate vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function, we process images through a ""probe network"" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and requires no understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks. We demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a novel task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well on which task. Selecting a feature extractor with task embedding yields performance close to the best available feature extractor, with substantially less computational effort than exhaustively training and evaluating all available models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Achille_Task2Vec_Task_Embedding_for_Meta-Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Achille_Task2Vec_Task_Embedding_for_Meta-Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008292/,"['Task analysis', 'Feature extraction', 'Visualization', 'Probes', 'Measurement', 'Semantics', 'Training']","['Task Embedding', 'Classification Task', 'Visual Task', 'Semantic Similarity', 'Ground Truth Labels', 'Fisher Information', 'Fisher Information Matrix', 'Task Representations', 'Neural Network', 'Model Selection', 'Deep Network', 'Deep Neural Network', 'Image Dataset', 'Cross-entropy Loss', 'Variety Of Tasks', 'Supplementary Materials For Details', 'Image Representation', 'Related Tasks', 'Decision Boundary', 'Set Of Categories', 'iNaturalist', 'Symmetric Distance', 'Taxonomic Distance', 'Transfer Distance', 'Task Space', 'Categorical Attributes', 'Variational Inference', 'Choice Of Network', 'Knowledge Transfer', 'ImageNet Pre-trained Model']",,83,"We introduce a method to generate vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function, we process images through a ""probe network"" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and requires no understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks. We demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a novel task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well on which task. Selecting a feature extractor with task embedding yields performance close to the best available feature extractor, with substantially less computational effort than exhaustively training and evaluating all available models."
Teacher Guided Architecture Search,"Pouya Bashivan, Mark Tensen, James J. DiCarlo","University of Amsterdam; Department of Brain and Cognitive Sciences and McGovern Institute for Brain Research, MIT; Department of Brain and Cognitive Sciences, McGovern Institute for Brain Research, MIT",100.0,"Netherlands, usa",0.0,,"Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efficiency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (fixed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efficiency over performance-guided methods. Our approach finds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only  300 neurons from primate visual system provides enough signal to find a network with an Imagenet top-1 error that is significantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain's sensory processing networks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bashivan_Teacher_Guided_Architecture_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bashivan_Teacher_Guided_Architecture_Search_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010650/,"['Computer architecture', 'Training', 'Biological neural networks', 'Computational efficiency', 'Task analysis', 'Network architecture']","['Neural Network', 'High Performance', 'Computational Cost', 'Total Cost', 'Cell Structure', 'Mental Representations', 'Neural Architecture', 'Previously Found', 'Search Efficiency', 'Representational Similarity', 'Teacher Network', 'Neural Architecture Search', 'Representational Similarity Analysis', 'Total Computational Cost', 'Candidate Network', 'Convolutional Neural Network', 'Artificial Neural Network', 'Deep Neural Network', 'Experimental Section', 'Validation Set', 'Search Costs', 'Ventral Stream', 'Architectural Space', 'Neural Measures', 'Student Network', 'Neural Responses', 'Search Space', 'Network Layer', 'Ventral Visual Pathway', 'Long Short-term Memory']",,7,"Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efficiency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (fixed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efficiency over performance-guided methods. Our approach finds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only ∼300 neurons from primate visual system provides enough signal to find a network with an Imagenet top-1 error that is significantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain’s sensory processing networks."
Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection,"Xuanyi Dong, Yi Yang","SUSTech-UTS Joint Centre of CIS, Southern China University of Science and Technology; ReLER, University of Technology Sydney",100.0,australia,0.0,,"Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data and become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Teacher_Supervises_Students_How_to_Learn_From_Partially_Labeled_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Teacher_Supervises_Students_How_to_Learn_From_Partially_Labeled_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009475/,"['Detectors', 'Heating systems', 'Training', 'Semisupervised learning', 'Robustness', 'Tuning']","['Landmark Detection', 'Facial Landmark Detection', 'Face Images', 'Unlabeled Data', 'Pseudo Labels', 'Unlabeled Images', 'Training Set', 'Validation Set', 'Structure Prediction', 'Detection Performance', 'Data Augmentation', 'Generative Adversarial Networks', 'Ground Truth Labels', 'Semi-supervised Learning', 'Teacher Network', 'Random Rotation', 'Student Network', 'Landmark Coordinates', 'Head Pose', 'Semi-supervised Learning Algorithm']",,50,"Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data and become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance."
"Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction","Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, Graham W. Taylor","University of Montreal, Montreal Institute for Learning Algorithms, Canadian Institute for Advanced Research; University of Guelph, Vector Institute for Artiﬁcial Intelligence, Canadian Institute for Advanced Research; University of Guelph, Vector Institute for Artiﬁcial Intelligence; Microsoft Research; Microsoft Research, University of Montreal, Montreal Institute for Learning Algorithms",80.0,"Canada, canada",20.0,USA,"Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step. One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/El-Nouby_Tell_Draw_and_Repeat_Generating_and_Modifying_Images_Based_on_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/El-Nouby_Tell_Draw_and_Repeat_Generating_and_Modifying_Images_Based_on_ICCV_2019_paper.pdf,https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/,,,main,Poster,https://ieeexplore.ieee.org/document/9008134/,"['Task analysis', 'Linguistics', 'Image generation', 'Generators', 'Visualization', 'History', 'Gallium nitride']","['Linguistic Instructions', 'Single Image', 'Image Generation', 'Current Step', 'Contralateral', 'Time Step', 'Convolutional Neural Network', 'Feature Maps', 'Object Detection', 'Recurrent Neural Network', 'Generative Adversarial Networks', 'Word Embedding', 'Gated Recurrent Unit', 'Ground Truth Image', 'Objects In The Scene', 'Current Time Step', 'Discriminator Loss', 'Sequence Of Instructions', 'Fr√©chet Inception Distance', 'Photo-realistic Images', 'Scene Graph', 'Text Encoder', 'G-prior', 'Visual Question Answering', 'Localizer', 'Natural Images', 'Final Image', 'Generative Adversarial Networks Model', 'Bounding Box', 'Object Pose']",,47,"Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step. One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/."
Temporal Attentive Alignment for Large-Scale Video Domain Adaptation,"Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen, Jian Zheng",Sony Interactive Entertainment LLC; Georgia Institute of Technology; Binghamton University,66.66666666666666,usa,33.33333333333334,Japan,"Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over ""Source only"" from 73.9% to 81.8% on ""HMDB --> UCF"", and 10.3% gain on ""Kinetics --> Gameplay""). The code and data are released at http://github.com/cmhungsteve/TA3N.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.pdf,,http://github.com/cmhungsteve/TA3N,,main,Oral,https://ieeexplore.ieee.org/document/9008391/,"['Computer architecture', 'Task analysis', 'Feature extraction', 'Encoding', 'Generators', 'Benchmark testing', 'Dynamics']","['Domain Adaptation', 'Temporal Alignment', 'Large-scale Video', 'Temporal Dynamics', 'Large-scale Datasets', 'Domain Shift', 'Attention Network', 'Domain Adaptation Methods', 'Small-scale Datasets', 'Domain Discrepancy', 'Video Methods', 'Convolutional Neural Network', 'Attention Mechanism', 'Temporal Features', 'Multilayer Perceptron', 'Deep Convolutional Neural Network', 'Video Analysis', 'Attention Module', 'Target Domain', 'Source Domain', 'Temporal Modulation', 'Maximum Mean Discrepancy', 'Loss Of Domain', 'Video Features', 'Baseline Architecture', 'Backbone Architecture', 'Domain Discriminator', 'Video Dataset', 'Domains Of Attention']",,100,"Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup>
N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over “Source only” from 73.9% to 81.8% on “HMDB → UCF”, and 10.3% gain on “Kinetics → Gameplay”). The code and data are released at http://github.com/cmhungsteve/TA3N."
Temporal Knowledge Propagation for Image-to-Video Person Re-Identification,"Xinqian Gu, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen","Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, 200031, China; University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",100.0,china,0.0,,"In many scenarios of Person Re-identification (Re-ID), the gallery set consists of lots of surveillance videos and the query is just an image, thus Re-ID has to be conducted between image and videos. Compared with videos, still person images lack temporal information. Besides, the information asymmetry between image and video features increases the difficulty in matching images and videos. To solve this problem, we propose a novel Temporal Knowledge Propagation (TKP) method which propagates the temporal knowledge learned by the video representation network to the image representation network. Specifically, given the input videos, we enforce the image representation network to fit the outputs of video representation network in a shared feature space. With back propagation, temporal knowledge can be transferred to enhance the image features and the information asymmetry problem can be alleviated. With additional classification and integrated triplet losses, our model can learn expressive and discriminative image and video features for image-to-video re-identification. Extensive experiments demonstrate the effectiveness of our method and the overall results on two widely used datasets surpass the state-of-the-art methods by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Temporal_Knowledge_Propagation_for_Image-to-Video_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Temporal_Knowledge_Propagation_for_Image-to-Video_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010981/,"['Videos', 'Image representation', 'Feature extraction', 'Knowledge engineering', 'Visualization', 'Robustness', 'Training']","['Temporal Knowledge', 'Temporal Propagation', 'Image Features', 'Backpropagation', 'Temporal Information', 'Information Asymmetry', 'Network Output', 'Large Margin', 'Network Representation', 'Image Representation', 'Video Features', 'Triplet Loss', 'Information Asymmetry Problems', 'Gallery Set', 'Deep Learning', 'Visual Information', 'Recurrent Neural Network', 'Temporal Relationship', 'Learning Network', 'Video Clips', 'Non-local Block', 'Video Frames', 'Representation Learning', 'Original Video', 'Still Images', 'Classification Loss', 'Student Network', 'Final Objective Function', 'Video Images', 'Robust Features']",,44,"In many scenarios of Person Re-identification (Re-ID), the gallery set consists of lots of surveillance videos and the query is just an image, thus Re-ID has to be conducted between image and videos. Compared with videos, still person images lack temporal information. Besides, the information asymmetry between image and video features increases the difficulty in matching images and videos. To solve this problem, we propose a novel Temporal Knowledge Propagation (TKP) method which propagates the temporal knowledge learned by the video representation network to the image representation network. Specifically, given the input videos, we enforce the image representation network to fit the outputs of video representation network in a shared feature space. With back propagation, temporal knowledge can be transferred to enhance the image features and the information asymmetry problem can be alleviated. With additional classification and integrated triplet losses, our model can learn expressive and discriminative image and video features for image-to-video re-identification. Extensive experiments demonstrate the effectiveness of our method and the overall results on two widely used datasets surpass the state-of-the-art methods by a large margin."
Temporal Recurrent Networks for Online Action Detection,"Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S. Davis, David J. Crandall","Honda Research Institute, USA; Indiana University; University of Maryland",100.0,"USA, usa",0.0,,"Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, the Temporal Recurrent Network (TRN), to model greater temporal context of each frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Temporal_Recurrent_Networks_for_Online_Action_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Temporal_Recurrent_Networks_for_Online_Action_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009797/,"['Computer architecture', 'Decoding', 'Feature extraction', 'Microprocessors', 'Streaming media', 'Logic gates', 'Predictive models']","['Online Activities', 'Action Detection', 'Online Action Detection', 'Video Frames', 'Temporal Context', 'Popular Datasets', 'Future Information', 'Advanced Driver Assistance Systems', 'Entire Video', 'Visual Features', 'Long Short-term Memory', 'Recurrent Neural Network', 'Temporal Information', 'Fully-connected Layer', 'Hidden State', 'Action Recognition', 'Optical Flow', 'Appearance Features', 'Temporal Model', 'Gated Recurrent Unit', 'Controller Area Network', 'Current Frame', 'Hours Of Video', 'Feature Concatenation', 'Future Context', 'Decoding Step', 'Live Streaming', 'Previous Hidden State', 'Left Turn', 'Input Sequence Length']",,102,"Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, the Temporal Recurrent Network (TRN), to model greater temporal context of each frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art."
Temporal Structure Mining for Weakly Supervised Action Detection,"Tan Yu, Zhou Ren, Yuncheng Li, Enxu Yan, Ning Xu, Junsong Yuan","Snap Inc.; Amazon; Cognitive Computing Lab, Baidu Research; Wormpex AI Research; State University of New York at Buffalo",20.0,usa,80.0,USA,"Different from the fully-supervised action detection problem that is dependent on expensive frame-level annotations, weakly supervised action detection (WSAD) only needs video-level annotations, making it more practical for real-world applications. Existing WSAD methods detect action instances by scoring each video segment (a stack of frames) individually. Most of them fail to model the temporal relations among video segments and cannot effectively characterize action instances possessing latent temporal structure. To alleviate this problem in WSAD, we propose the temporal structure mining (TSM) approach. In TSM, each action instance is modeled as a multi-phase process and phase evolving within an action instance, i.e., the temporal structure, is exploited. Meanwhile, the video background is modeled by a background phase, which separates different action instances in an untrimmed video. In this framework, phase filters are used to calculate the confidence scores of the presence of an action's phases in each segment. Since in the WSAD task, frame-level annotations are not available and thus phase filters cannot be trained directly. To tackle the challenge, we treat each segment's phase as a hidden variable. We use segments' confidence scores from each phase filter to construct a table and determine hidden variables, i.e., phases of segments, by a maximal circulant path discovery along the table. Experiments conducted on three benchmark datasets demonstrate the state-of-the-art performance of the proposed TSM.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Temporal_Structure_Mining_for_Weakly_Supervised_Action_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Temporal_Structure_Mining_for_Weakly_Supervised_Action_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010849/,"['Training', 'Hidden Markov models', 'Indexes', 'Task analysis', 'Testing', 'Periodic structures', 'Visualization']","['Temporal Structure', 'Action Detection', 'Temporal Relationship', 'Confidence Score', 'Pathfinding', 'Circulator', 'Hidden Variables', 'Video Segments', 'Action Instances', 'Phase Filter', 'Multi-phase Process', 'Active Phase', 'Hidden Markov Model', 'Detection Results', 'Intersection Over Union', 'Training Stage', 'Dynamic Programming', 'Exhaustive Search', 'Action Recognition', 'Ground Truth Labels', 'Single Instance', 'Detection In Videos', 'Background Phase', 'Backbone Network', 'Optical Flow', 'Context-free Grammar', 'Filter Weights', 'Multiple Segments', 'Classification Loss', 'Standard Cross-entropy Loss']",,67,"Different from the fully-supervised action detection problem that is dependent on expensive frame-level annotations, weakly supervised action detection (WSAD) only needs video-level annotations, making it more practical for real-world applications. Existing WSAD methods detect action instances by scoring each video segment (a stack of frames) individually. Most of them fail to model the temporal relations among video segments and cannot effectively characterize action instances possessing latent temporal structure. To alleviate this problem in WSAD, we propose the temporal structure mining (TSM) approach. In TSM, each action instance is modeled as a multi-phase process and phase evolving within an action instance, \emph{i.e.}, the temporal structure, is exploited. Meanwhile, the video background is modeled by a background phase, which separates different action instances in an untrimmed video. In this framework, phase filters are used to calculate the confidence scores of the presence of an action's phases in each segment. Since in the WSAD task, frame-level annotations are not available and thus phase filters cannot be trained directly. To tackle the challenge, we treat each segment's phase as a hidden variable. We use segments' confidence scores from each phase filter to construct a table and determine hidden variables, i.e., phases of segments, by a maximal circulant path discovery along the table. Experiments conducted on three benchmark datasets demonstrate the state-of-the-art performance of the proposed TSM."
TensorMask: A Foundation for Dense Object Segmentation,"Xinlei Chen, Ross Girshick, Kaiming He, Piotr DollÃ¡r",Facebook AI Research (FAIR),0.0,,100.0,USA,"Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chen_TensorMask_A_Foundation_for_Dense_Object_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_TensorMask_A_Foundation_for_Dense_Object_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010024/,"['Tensile stress', 'Task analysis', 'Windows', 'Shape', 'Proposals', 'Two dimensional displays', 'Image segmentation']","['Object Detection', 'Semantic Segmentation', 'Instance Segmentation', 'Mask R-CNN', 'Aspect Ratio', 'Window Size', 'Input Image', 'Feature Maps', 'Image Pixels', 'Unit Length', 'Coordinate Transformation', 'Output Channels', 'Bilinear Interpolation', 'Axis Length', 'Nature Of Representations', 'Nearest Neighbor Interpolation', 'Ratio Of Units', 'Center Of Window', 'Detection Boxes', 'Classification Head']",,263,"Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available."
Tex2Shape: Detailed Full Human Body Geometry From a Single Image,"Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, Marcus Magnor","Computer Graphics Lab, TU Braunschweig, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany",100.0,germany,0.0,,"We present a simple yet effective method to infer detailed full human body shape from only a single photograph. Our model can infer full-body shape including face, hair, and clothing including wrinkles at interactive frame-rates. Results feature details even on parts that are occluded in the input image. Our main idea is to turn shape regression into an aligned image-to-image translation problem. The input to our method is a partial texture map of the visible region obtained from off-the-shelf methods. From a partial texture, we estimate detailed normal and vector displacement maps, which can be applied to a low-resolution smooth body model to add detail and clothing. Despite being trained purely with synthetic data, our model generalizes well to real-world photographs. Numerous results demonstrate the versatility and robustness of our method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Alldieck_Tex2Shape_Detailed_Full_Human_Body_Geometry_From_a_Single_Image_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Alldieck_Tex2Shape_Detailed_Full_Human_Body_Geometry_From_a_Single_Image_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010995/,"['Shape', 'Clothing', 'Image reconstruction', 'Three-dimensional displays', 'Geometry', 'Face', 'Hair']","['Single Image', 'Input Image', 'Wrinkles', 'Body Shape', 'Displacement Vector', 'Body Model', 'Part Of The Map', 'Detailed Shape', 'Translation Problems', 'Normal Displacement', 'Normal Map', 'Single Photograph', 'Displacement Maps', 'Human Body Shape', 'Neural Network', 'Shape Parameter', '3D Mesh', 'Binding Pose', 'Spherical Harmonics', '3D Pose', 'Shape Reconstruction', 'Human Pose Estimation', 'Non-rigid Deformation', 'Facial Details', 'Monocular Images', 'Surface Normals', 'Long Hair', 'Human Pose', 'Non-rigid Registration']",,223,"We present a simple yet effective method to infer detailed full human body shape from only a single photograph. Our model can infer full-body shape including face, hair, and clothing including wrinkles at interactive frame-rates. Results feature details even on parts that are occluded in the input image. Our main idea is to turn shape regression into an aligned image-to-image translation problem. The input to our method is a partial texture map of the visible region obtained from off-the-shelf methods. From a partial texture, we estimate detailed normal and vector displacement maps, which can be applied to a low-resolution smooth body model to add detail and clothing. Despite being trained purely with synthetic data, our model generalizes well to real-world photographs. Numerous results demonstrate the versatility and robustness of our method."
TextDragon: An End-to-End Framework for Arbitrary Shaped Text Spotting,"Wei Feng, Wenhao He, Fei Yin, Xu-Yao Zhang, Cheng-Lin Liu","National Laboratory of Pattern Recognition (NLPR), Institute of Automation of Chinese Academy of Sciences, Beijing 100190, China; School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation of Chinese Academy of Sciences, Beijing 100190, China; School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China; CAS Center for Excellence of Brain Science and Intelligence Technology, Beijing 100190, China",100.0,china,0.0,,"Most existing text spotting methods either focus on horizontal/oriented texts or perform arbitrary shaped text spotting with character-level annotations. In this paper, we propose a novel text spotting framework to detect and recognize text of arbitrary shapes in an end-to-end manner, using only word/line-level annotations for training. Motivated from the name of TextSnake, which is only a detection model, we call the proposed text spotting framework TextDragon. In TextDragon, a text detector is designed to describe the shape of text with a series of quadrangles, which can handle text of arbitrary shapes. To extract arbitrary text regions from feature maps, we propose a new differentiable operator named RoISlide, which is the key to connect arbitrary shaped text detection and recognition. Based on the extracted features through RoISlide, a CNN and CTC based text recognizer is introduced to make the framework free from labeling the location of characters. The proposed method achieves state-of-the-art performance on two curved text benchmarks CTW1500 and Total-Text, and competitive results on the ICDAR 2015 Dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Feng_TextDragon_An_End-to-End_Framework_for_Arbitrary_Shaped_Text_Spotting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_TextDragon_An_End-to-End_Framework_for_Arbitrary_Shaped_Text_Spotting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009034/,"['Text recognition', 'Shape', 'Feature extraction', 'Task analysis', 'Character recognition', 'Detectors', 'Training']","['Arbitrary Shape', 'Text Spotting', 'Arbitrary-shaped Text', 'Convolutional Neural Network', 'Feature Maps', 'Optical Character Recognition', 'Annotated Training', 'Arbitrary Region', 'Final Results', 'Decoding', 'Convolutional Layers', 'Input Image', 'Recurrent Neural Network', 'Detection Results', 'Bounding Box', 'Fully-connected Layer', 'Recognition Performance', 'Affine Transformation', 'Max-pooling Layer', 'Textual Features', 'Recognition Module', 'Box Location', 'Transformation Parameters', 'Standard Benchmark', 'Inference Stage', 'Recognition Results', 'Words In Set', 'Series Of Units']",,148,"Most existing text spotting methods either focus on horizontal/oriented texts or perform arbitrary shaped text spotting with character-level annotations. In this paper, we propose a novel text spotting framework to detect and recognize text of arbitrary shapes in an end-to-end manner, using only word/line-level annotations for training. Motivated from the name of TextSnake, which is only a detection model, we call the proposed text spotting framework TextDragon. In TextDragon, a text detector is designed to describe the shape of text with a series of quadrangles, which can handle text of arbitrary shapes. To extract arbitrary text regions from feature maps, we propose a new differentiable operator named RoISlide, which is the key to connect arbitrary shaped text detection and recognition. Based on the extracted features through RoISlide, a CNN and CTC based text recognizer is introduced to make the framework free from labeling the location of characters. The proposed method achieves state-of-the-art performance on two curved text benchmarks CTW1500 and Total-Text, and competitive results on the ICDAR 2015 Dataset."
TextPlace: Visual Place Recognition and Topological Localization Through Reading Scene Texts,"Ziyang Hong, Yvan Petillot, David Lane, Yishu Miao, Sen Wang","Edinburgh Centre for Robotics, Heriot-Watt University; University of Oxford",100.0,"United Kingdom, uk",0.0,,"Visual place recognition is a fundamental problem for many vision based applications. Sparse feature and deep learning based methods have been successful and dominant over the decade. However, most of them do not explicitly leverage high-level semantic information to deal with challenging scenarios where they may fail. This paper proposes a novel visual place recognition algorithm, termed TextPlace, based on scene texts in the wild. Since scene texts are high-level information invariant to illumination changes and very distinct for different places when considering spatial correlation, it is beneficial for visual place recognition tasks under extreme appearance changes and perceptual aliasing. It also takes spatial-temporal dependence between scene texts into account for topological localization. Extensive experiments show that TextPlace achieves state-of-the-art performance, verifying the effectiveness of using high-level scene texts for robust visual place recognition in urban areas.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hong_TextPlace_Visual_Place_Recognition_and_Topological_Localization_Through_Reading_Scene_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hong_TextPlace_Visual_Place_Recognition_and_Topological_Localization_Through_Reading_Scene_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010336/,"['Text recognition', 'Visualization', 'Image recognition', 'Dictionaries', 'Measurement', 'Semantics', 'Microsoft Windows']","['Visual Recognition', 'Place Recognition', 'Scene Text', 'Topological Location', 'Visual Place Recognition', 'Urban Areas', 'Deep Learning', 'Feature Learning', 'Extreme Changes', 'Illumination Changes', 'Challenging Scenarios', 'Sparse Feature', 'High-level Semantic Information', 'High-level Semantics', 'Bounding Box', 'Query Sequence', 'Sequence Mapping', 'Textual Descriptions', 'Optical Character Recognition', 'Graph Topology', 'Topological Map', 'Levenshtein Distance', 'Node Mapping', 'Mapping Stage', 'Visual Odometry', 'Nonexpansive Mapping', 'Query Image', 'Search Window', 'Bag Of Visual Words', 'String Length']",,36,"Visual place recognition is a fundamental problem for many vision based applications. Sparse feature and deep learning based methods have been successful and dominant over the decade. However, most of them do not explicitly leverage high-level semantic information to deal with challenging scenarios where they may fail. This paper proposes a novel visual place recognition algorithm, termed TextPlace, based on scene texts in the wild. Since scene texts are high-level information invariant to illumination changes and very distinct for different places when considering spatial correlation, it is beneficial for visual place recognition tasks under extreme appearance changes and perceptual aliasing. It also takes spatial-temporal dependence between scene texts into account for topological localization. Extensive experiments show that TextPlace achieves state-of-the-art performance, verifying the effectiveness of using high-level scene texts for robust visual place recognition in urban areas."
Texture Fields: Learning Texture Representations in Function Space,"Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger","ETAS GmbH, Bosch Group, Stuttgart; Autonomous Vision Group, MPI for Intelligent Systems and University of Tübingen",100.0,germany,0.0,,"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Oechsle_Texture_Fields_Learning_Texture_Representations_in_Function_Space_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Oechsle_Texture_Fields_Learning_Texture_Representations_in_Function_Space_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008380/,"['Three-dimensional displays', 'Shape', 'Solid modeling', 'Image reconstruction', 'Image color analysis', 'Two dimensional displays', 'Neural networks']","['Texture Representation', 'Neural Network', '3D Reconstruction', 'Shape Parameter', 'Shape Representation', 'Object Reconstruction', 'Probabilistic Generative Model', 'Input Image', 'Single Image', '2D Images', 'Point Cloud', 'Generative Adversarial Networks', 'Latent Space', 'Object Shape', '3D Point', '3D Shape', 'Variational Autoencoder', 'Single View', 'Objective View', 'Generative Adversarial Networks Model', 'Latent Code', 'Fréchet Inception Distance', 'Variational Autoencoder Model', 'Texture Of Objects', 'Image Encoder', 'Texture Model', 'View Synthesis', 'Appearance Information', 'Input Shape', 'Image X']",,166,"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models."
TexturePose: Supervising Human Mesh Estimation With Texture Consistency,"Georgios Pavlakos, Nikos Kolotouros, Kostas Daniilidis",University of Pennsylvania,100.0,usa,0.0,,"This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https://seas.upenn.edu/ pavlakos/projects/texturepose.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pavlakos_TexturePose_Supervising_Human_Mesh_Estimation_With_Texture_Consistency_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pavlakos_TexturePose_Supervising_Human_Mesh_Estimation_With_Texture_Consistency_ICCV_2019_paper.pdf,https://seas.upenn.edu/~pavlakos/projects/texturepose,,,main,Poster,https://ieeexplore.ieee.org/document/9010287/,"['Three-dimensional displays', 'Shape', 'Pose estimation', 'Two dimensional displays', 'Biological system modeling', 'Cameras', 'Parametric statistics']","['Texture Consistency', 'Model Parameters', 'Natural Images', '3D Shape', 'Pose Estimation', 'Strong Baseline', 'Model-based Estimates', 'Human Pose Estimation', 'Human Pose', 'Multi-view Images', 'Form Of Supervision', 'Texture Map', 'Smoothness Assumption', 'Single Image', 'Shape Parameter', 'Video Data', 'Optical Flow', 'Unlabeled Data', '3D Mesh', '3D Pose', 'Pose Parameters', '2D Keypoints', 'Supervision Signal', 'Multi-view Data', 'Personal Appearance', 'Additional Annotations', 'Lack Of Images', '3D Joint', 'Weak Supervision']",,77,"This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https://seas.upenn.edu/~pavlakos/projects/texturepose."
The LogBarrier Adversarial Attack: Making Effective Use of Decision Boundary Information,"Chris Finlay, Aram-Alexandre Pooladian, Adam Oberman",McGill University,100.0,canada,0.0,,"Adversarial attacks for image classification are small perturbations to images that are designed to cause misclassification by a model. Adversarial attacks formally correspond to an optimization problem: find a minimum norm image perturbation, constrained to cause misclassification. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the well-regarded logarithmic barrier method. On average, our attack distance is similar or better than all state-of-the-art attacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our method performs significantly better on the most challenging images, those which normally require larger perturbations for misclassification. We employ the LogBarrier attack on several adversarially defended models, and show that it adversarially perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-of-the-art attacks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010707/,"['Perturbation methods', 'Optimization', 'Predictive models', 'Robustness', 'Upper bound', 'Best practices', 'Benchmark testing']","['Decision Boundary', 'Adversarial Attacks', 'Optimization Problem', 'Interior Point Method', 'Logarithmic Method', 'Loss Function', 'Gradient Descent', 'Small Distance', 'Face Recognition', 'Inequality Constraints', 'Euclidean Norm', 'Previous Iteration', 'Penalty Term', 'Adversarial Training', 'Class Boundaries', 'Random Perturbations', 'Image X', 'Adversarial Examples', 'Attack Methods', 'Fast Gradient Sign Method', 'Adversarial Perturbations', 'Projected Gradient Descent', 'Defense Methods', 'Adversarial Robustness', 'Objective Function', 'Gradient Descent Step']",,14,"Adversarial attacks for image classification are small perturbations to images that are designed to cause misclassification by a model. Adversarial attacks formally correspond to an optimization problem: find a minimum norm image perturbation, constrained to cause misclassification. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the well-regarded logarithmic barrier method. On average, our attack distance is similar or better than all state-of-the-art attacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our method performs significantly better on the most challenging images, those which normally require larger perturbations for misclassification. We employ the LogBarrier attack on several adversarially defended models, and show that it adversarially perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-of-the-art attacks."
The Sound of Motions,"Hang Zhao, Chuang Gan, Wei-Chiu Ma, Antonio Torralba",MIT; MIT-IBM Watson AI Lab,100.0,usa,0.0,,"Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_The_Sound_of_Motions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_The_Sound_of_Motions_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010962/,"['Videos', 'Trajectory', 'Visualization', 'Task analysis', 'Source separation', 'Training', 'Integrated optics']","['Visual Cues', 'Challenging Problem', 'Musical Instruments', 'Sound Localization', 'Curriculum Learning', 'Cueing Task', 'Motion Cues', 'Task Performance', 'Visual Features', 'Temporal Information', 'Video Frames', 'Action Recognition', 'Optical Flow', 'Appearance Features', 'Non-negative Matrix Factorization', 'Source Separation', 'Self-supervised Learning', 'Motion Information', 'Short-time Fourier Transform', 'Kinds Of Instruments', 'Signal-to-interference Ratio', 'Trajectory Features', 'Separation Performance', 'Sound Features', 'Video Information', 'Motor Representations', 'Network Flow', 'Feature Maps', 'Motion Features']",,148,"Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before."
The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs,"Boris Ivanovic, Marco Pavone",Stanford University,100.0,usa,0.0,,"Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009454/,"['Trajectory', 'Predictive models', 'Forecasting', 'Graphical models', 'Spatiotemporal phenomena', 'Standards', 'Autonomous systems']","['Probabilistic Model', 'Trajectory Model', 'Dynamic Graph', 'Number Of Agents', 'Future Trajectories', 'Autonomous Agents', 'Deep Generative Models', 'Distribution Of Trajectories', 'Trajectories Of Agents', 'Long Short-term Memory', 'Recurrent Neural Network', 'Pedestrian', 'Generative Adversarial Networks', 'Graphical Model', 'Kriging', 'Theory Of Mind', 'Gaussian Mixture Model', 'Network Graph', 'Long Short-term Memory Network', 'Prediction Horizon', 'Types Of Edges', 'Inverse Reinforcement Learning', 'Standard Benchmark', 'Human Trajectory', 'Multimodal Model', 'Conditional Variational Autoencoder', 'Probabilistic Graphical Models', 'Types Of Nodes', 'Negative Log-likelihood', 'Hidden Dimension']",,269,"Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions."
"Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture From Images âIn the Wildâ","Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, Michael J. Black","Max Planck Institute for Intelligent Systems, Tübingen, Germany; IMATI-CNR, Milan, Italy; University of California, Berkeley; University of Illinois at Chicago",75.0,"Germany, germany, usa",25.0,Italy,"We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy's zebras from a collection of images. The Grevy's zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other. To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. Learning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision. Moreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. We show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zuffi_Three-D_Safari_Learning_to_Estimate_Zebra_Pose_Shape_and_Texture_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zuffi_Three-D_Safari_Learning_to_Estimate_Zebra_Pose_Shape_and_Texture_ICCV_2019_paper.pdf,,https://github.com/silviazuffi/smalst,,main,Poster,,,,,,
Through-Wall Human Mesh Recovery Using Radio Signals,"Mingmin Zhao, Yingcheng Liu, Aniruddh Raghu, Tianhong Li, Hang Zhao, Antonio Torralba, Dina Katabi",MIT CSAIL,100.0,usa,0.0,,"This paper presents RF-Avatar, a neural network model that can estimate 3D meshes of the human body in the presence of occlusions, baggy clothes, and bad lighting conditions. We leverage that radio frequency (RF) signals in the WiFi range traverse clothes and occlusions and bounce off the human body. Our model parses such radio signals and recovers 3D body meshes. Our meshes are dynamic and smoothly track the movements of the corresponding people. Further, our model works both in single and multi-person scenarios. Inferring body meshes from radio signals is a highly under-constrained problem. Our model deals with this challenge using: 1) a combination of strong and weak supervision, 2) a multi-headed self-attention mechanism that attends differently to temporal information in the radio signal, and 3) an adversarially trained temporal discriminator that imposes a prior on the dynamics of human motion. Our results show that RF-Avatar accurately recovers dynamic 3D meshes in the presence of occlusions, baggy clothes, bad lighting conditions, and even through walls.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Through-Wall_Human_Mesh_Recovery_Using_Radio_Signals_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Through-Wall_Human_Mesh_Recovery_Using_Radio_Signals_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009491/,"['Three-dimensional displays', 'Radio frequency', 'RF signals', 'Shape', 'Dynamics', 'Two dimensional displays', 'Cameras']","['Radio Waves', 'Human Mesh', 'Mesh Recovery', 'Neural Network', 'Artificial Neural Network', 'Radiofrequency', 'Range Of Signals', '3D Mesh', 'Dynamic Motion', 'Light Pollution', 'Human Motion', 'Adversarial Training', 'Radio Frequency Signal', 'Weak Supervision', 'Presence Of Occlusion', 'Temporal Discrimination', 'Strong Supervision', 'Body Parts', '2D Images', '3D Space', 'Shape Estimation', 'Body Shape', 'Shape Vectors', 'Human Pose', 'Joint Angles', 'Pose Estimation', 'Image-based Methods', 'Self-attention Module', '3D Joint', 'Human Shape']",,75,"This paper presents RF-Avatar, a neural network model that can estimate 3D meshes of the human body in the presence of occlusions, baggy clothes, and bad lighting conditions. We leverage that radio frequency (RF) signals in the WiFi range traverse clothes and occlusions and bounce off the human body. Our model parses such radio signals and recovers 3D body meshes. Our meshes are dynamic and smoothly track the movements of the corresponding people. Further, our model works both in single and multi-person scenarios. Inferring body meshes from radio signals is a highly under-constrained problem. Our model deals with this challenge using: 1) a combination of strong and weak supervision, 2) a multi-headed self-attention mechanism that attends differently to temporal information in the radio signal, and 3) an adversarially trained temporal discriminator that imposes a prior on the dynamics of human motion. Our results show that RF-Avatar accurately recovers dynamic 3D meshes in the presence of occlusions, baggy clothes, bad lighting conditions, and even through walls."
ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices,"Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, Gang Yu, Yuxing Peng, Jian Sun",National University of Defense Technology; Megvii Inc. (Face++),50.0,China,50.0,China,"Real-time generic object detection on mobile platforms is a crucial but challenging computer vision task. Prior lightweight CNN-based detectors are inclined to use one-stage pipeline. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context Enhancement Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Benefit from the highly efficient backbone and detection part design, ThunderNet surpasses previous lightweight one-stage detectors with only 40% of the computational cost on PASCAL VOC and COCO benchmarks. Without bells and whistles, ThunderNet runs at 24.1 fps on an ARM-based device with 19.2 AP on COCO. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Code will be released for paper reproduction.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010015/,"['Detectors', 'Real-time systems', 'Object detection', 'Head', 'Computer architecture', 'Convolutional codes', 'Computational efficiency']","['Mobile Devices', 'Real-time Detection', 'Object Detection', 'General Detection', 'Real-time Object Detection', 'Computational Cost', 'Challenging Task', 'Spatial Module', 'Mobile Platform', 'Whistle', 'Region Proposal Network', 'Input Resolution', 'Detection Head', 'Two-stage Detectors', 'Spatial Attention Module', 'One-stage Detectors', 'Contextual Information', 'Input Image', 'Image Classification', 'Feature Maps', 'Large Detector', 'Depthwise Convolution', 'Low-level Features', 'Receptive Field', 'Feature Pyramid Network', 'Large Image', 'Backbone Network', 'Inference Speed', 'Small Imaging', 'High-level Features']",,189,"Real-time generic object detection on mobile platforms is a crucial but challenging computer vision task. Prior lightweight CNN-based detectors are inclined to use one-stage pipeline. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context Enhancement Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Benefit from the highly efficient backbone and detection part design, ThunderNet surpasses previous lightweight one-stage detectors with only 40% of the computational cost on PASCAL VOC and COCO benchmarks. Without bells and whistles, ThunderNet runs at 24.1 fps on an ARM-based device with 19.2 AP on COCO. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Code will be released for paper reproduction."
Topological Map Extraction From Overhead Images,"Zuoyue Li, Jan Dirk Wegner, AurÃ©lien Lucchi","ETH Zürich, Switzerland",100.0,Switzerland,0.0,,"We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-arts also show that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Topological_Map_Extraction_From_Overhead_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Topological_Map_Extraction_From_Overhead_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008272/,"['Roads', 'Buildings', 'Image segmentation', 'Feature extraction', 'Shape', 'Image edge detection', 'Machine learning']","['Topological Map', 'Overhead Images', 'Image Segmentation', 'Road Network', 'Aerial Images', 'Graph Structure', 'Vector Representation', 'Building Footprints', 'Different Types Of Objects', 'Deep Learning', 'Shortest Path', 'Precision And Recall', 'Bounding Box', 'Semantic Segmentation', 'Object Of Interest', 'Image Patches', 'Image Edge', 'Graph Neural Networks', 'OpenStreetMap', 'Feature Pyramid Network', 'Convolutional Long Short-term Memory', 'Average Recall', 'Instance Segmentation', 'Mask R-CNN', 'Beam Search', 'Road Segments', 'IoU Threshold', 'Google Maps', 'Active Contour']",,106,"We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-arts also show that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints."
Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning,"Pedro Hermosilla, Tobias Ritschel, Timo Ropinski","University College London; Ulm University, Linköping University; Ulm University",100.0,"Sweden, germany, uk",0.0,,"We show that denoising of 3D point clouds can be learned unsupervised, directly from noisy 3D point cloud data only. This is achieved by extending recent ideas from learning of unsupervised image denoisers to unstructured 3D point clouds. Unsupervised image denoisers operate under the assumption that a noisy pixel observation is a random realization of a distribution around a clean pixel value, which allows appropriate learning on this distribution to eventually converge to the correct value. Regrettably, this assumption is not valid for unstructured points: 3D point clouds are subject to total noise, i.e. deviations in all coordinates, with no reliable pixel grid. Thus, an observation can be the realization of an entire manifold of clean 3D points, which makes the quality of a naive extension of unsupervised image denoisers to 3D point clouds unfortunately only little better than mean filtering. To overcome this, and to enable effective and unsupervised 3D point cloud denoising, we introduce a spatial prior term, that steers converges to the unique closest out of the many possible modes on the manifold. Our results demonstrate unsupervised denoising performance similar to that of supervised learning with clean data when given enough training examples - whereby we do not need any pairs of noisy and clean training data.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008289/,"['Three-dimensional displays', 'Noise measurement', 'Noise reduction', 'Surface cleaning', 'Manifolds', 'Unsupervised learning', 'Training']","['Unsupervised Learning', 'Point Cloud', '3D Point Cloud', 'Training Data', 'Clean Data', 'Noisy Data', 'Training Examples', 'Noisy Observations', 'Total Noise', 'Pixel Grid', 'Noisy Points', 'Pairing', 'Convolutional Neural Network', 'Receptive Field', 'Unsupervised Methods', 'Test Points', 'Blind Spot', 'Clean Surface', 'Noise Model', 'Unsupervised Training', 'Mode Of Distribution', 'Amount Of Training Data', 'Real Point', 'Real Noise', 'Types Of Noise', 'Scanner Noise', 'Noisy Images', 'Local Frame', 'Linear Filter']",,30,"We show that denoising of 3D point clouds can be learned unsupervised, directly from noisy 3D point cloud data only. This is achieved by extending recent ideas from learning of unsupervised image denoisers to unstructured 3D point clouds. Unsupervised image denoisers operate under the assumption that a noisy pixel observation is a random realization of a distribution around a clean pixel value, which allows appropriate learning on this distribution to eventually converge to the correct value. Regrettably, this assumption is not valid for unstructured points: 3D point clouds are subject to total noise, i.e. deviations in all coordinates, with no reliable pixel grid. Thus, an observation can be the realization of an entire manifold of clean 3D points, which makes the quality of a naïve extension of unsupervised image denoisers to 3D point clouds unfortunately only little better than mean filtering. To overcome this, and to enable effective and unsupervised 3D point cloud denoising, we introduce a spatial prior term, that steers converges to the unique closest out of the many possible modes on the manifold. Our results demonstrate unsupervised denoising performance similar to that of supervised learning with clean data when given enough training examples - whereby we do not need any pairs of noisy and clean training data."
Toward Real-World Single Image Super-Resolution: A New Benchmark and a New Model,"Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, Lei Zhang","DJI Co., Ltd; The Hong Kong Polytechnic University, DAMO Academy, Alibaba Group; The Hong Kong Polytechnic University",66.66666666666666,"Hong Kong, china",33.33333333333334,China,"Most of the existing learning-based single image super-resolution (SISR) methods are trained and evaluated on simulated datasets, where the low-resolution (LR) images are generated by applying a simple and uniform degradation (i.e., bicubic downsampling) to their high-resolution (HR) counterparts. However, the degradations in real-world LR images are far more complicated. As a consequence, the SISR models trained on simulated data become less effective when applied to practical scenarios. In this paper, we build a real-world super-resolution (RealSR) dataset where paired LR-HR images on the same scene are captured by adjusting the focal length of a digital camera. An image registration algorithm is developed to progressively align the image pairs at different resolutions. Considering that the degradation kernels are naturally non-uniform in our dataset, we present a Laplacian pyramid based kernel prediction network (LP-KPN), which efficiently learns per-pixel kernels to recover the HR image. Our extensive experiments demonstrate that SISR models trained on our RealSR dataset deliver better visual quality with sharper edges and finer textures on real-world scenes than those trained on simulated datasets. Though our RealSR dataset is built by using only two cameras (Canon 5D3 and Nikon D810), the trained model generalizes well to other camera devices such as Sony a7II and mobile phones.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Toward_Real-World_Single_Image_Super-Resolution_A_New_Benchmark_and_a_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Toward_Real-World_Single_Image_Super-Resolution_A_New_Benchmark_and_a_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009805/,"['Cameras', 'Degradation', 'Kernel', 'Lenses', 'Image resolution', 'Training', 'Benchmark testing']","['Real-world Images', 'Single Image Super-resolution', 'Simulated Data', 'High-resolution Images', 'Simulated Datasets', 'Image Pairs', 'Focal Length', 'Visual Quality', 'Low-resolution Images', 'Real-world Scenes', 'Laplacian Pyramid', 'Image Registration Algorithm', 'Training Set', 'Objective Function', 'Computational Cost', 'Convolutional Neural Network', 'Scaling Factor', 'Supplementary File', 'Image Sensor', 'Generalization Capability', 'Multiple Degradation', 'Image Pyramid', 'Memory Cost', 'Thin Lens', 'Larger Kernel Size', 'Image Distortion', 'Principal Plane', 'Blur Kernel', 'Simulated Images']",,300,"Most of the existing learning-based single image super-resolution (SISR) methods are trained and evaluated on simulated datasets, where the low-resolution (LR) images are generated by applying a simple and uniform degradation (i.e., bicubic downsampling) to their high-resolution (HR) counterparts. However, the degradations in real-world LR images are far more complicated. As a consequence, the SISR models trained on simulated data become less effective when applied to practical scenarios. In this paper, we build a real-world super-resolution (RealSR) dataset where paired LR-HR images on the same scene are captured by adjusting the focal length of a digital camera. An image registration algorithm is developed to progressively align the image pairs at different resolutions. Considering that the degradation kernels are naturally non-uniform in our dataset, we present a Laplacian pyramid based kernel prediction network (LP-KPN), which efficiently learns per-pixel kernels to recover the HR image. Our extensive experiments demonstrate that SISR models trained on our RealSR dataset deliver better visual quality with sharper edges and finer textures on real-world scenes than those trained on simulated datasets. Though our RealSR dataset is built by using only two cameras (Canon 5D3 and Nikon D810), the trained model generalizes well to other camera devices such as Sony a7II and mobile phones."
Towards Adversarially Robust Object Detection,"Haichao Zhang, Jianyu Wang","Baidu Research, Sunnyvale USA",0.0,,100.0,China,"Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Towards_Adversarially_Robust_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Towards_Adversarially_Robust_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009990/,"['Task analysis', 'Detectors', 'Robustness', 'Object detection', 'Training', 'Standards', 'Analytical models']","['Object Detection', 'Robust Detection', 'Robust Object Detection', 'Role In Loss', 'Multi-task Learning', 'Adversarial Training', 'Indispensable Component', 'Adversarial Attacks', 'Task Loss', 'Model Performance', 'Standard Model', 'Classification Task', 'Number Of Steps', 'Single Domain', 'Bounding Box', 'Clear Image', 'Individual Tasks', 'Classification Loss', 'Types Of Attacks', 'Local Loss', 'Adversarial Examples', 'Single Shot Multibox Detector', 'Task Domain', 'Fast Gradient Sign Method', 'Architecture For Detection', 'Robust Levels', 'Conventional Training', 'Robust Training', 'Attack Methods', 'Classification Performance']",,84,"Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach."
Towards Bridging Semantic Gap to Improve Semantic Segmentation,"Yanwei Pang, Yazhao Li, Jianbing Shen, Ling Shao","Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE; Tianjin University, Tianjin, China",100.0,"china, uae",0.0,,"Aggregating multi-level features is essential for capturing multi-scale context information for precise scene semantic segmentation. However, the improvement by directly fusing shallow features and deep features becomes limited as the semantic gap between them increases. To solve this problem, we explore two strategies for robust feature fusion. One is enhancing shallow features using a semantic enhancement module (SeEM) to alleviate the semantic gap between shallow features and deep features. The other strategy is feature attention, which involves discovering complementary information (i.e., boundary information) from low-level features to enhance high-level features for precise segmentation. By embedding these two strategies, we construct a parallel feature pyramid towards improving multi-level feature fusion. A Semantic Enhanced Network called SeENet is constructed with the parallel pyramid to implement precise segmentation. Experiments on three benchmark datasets demonstrate the effectiveness of our method for robust multi-level feature aggregation. As a result, our SeENet has achieved better performance than other state-of-the-art methods for semantic segmentation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Pang_Towards_Bridging_Semantic_Gap_to_Improve_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pang_Towards_Bridging_Semantic_Gap_to_Improve_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009558/,"['Semantics', 'Feature extraction', 'Robustness', 'Decoding', 'Data mining', 'Aggregates', 'Convolution']","['Semantic Segmentation', 'Semantic Gap', 'Contextual Information', 'Benchmark Datasets', 'High-level Features', 'Robust Features', 'Deep Features', 'Feature Fusion', 'Feature Aggregation', 'Feature Pyramid', 'Segmentation Feature', 'Multi-level Features', 'Shallow Features', 'Boundary Information', 'Semantic Segmentation Methods', 'Precise Segmentation', 'Convolutional Layers', 'Deeper Layers', 'Multi-scale Features', 'Skip Connections', 'Atrous Spatial Pyramid Pooling', 'Shallow Layers', 'Feature Representation Ability', 'Dilation Rate', 'Atrous Convolution', 'Ability Of Features', 'Bottom-up Manner', 'Inception Module', 'Image Pyramid', 'Stage Of Network']",,86,"Aggregating multi-level features is essential for capturing multi-scale context information for precise scene semantic segmentation. However, the improvement by directly fusing shallow features and deep features becomes limited as the semantic gap between them increases. To solve this problem, we explore two strategies for robust feature fusion. One is enhancing shallow features using a semantic enhancement module (SeEM) to alleviate the semantic gap between shallow features and deep features. The other strategy is feature attention, which involves discovering complementary information (i.e., boundary information) from low-level features to enhance high-level features for precise segmentation. By embedding these two strategies, we construct a parallel feature pyramid towards improving multi-level feature fusion. A Semantic Enhanced Network called SeENet is constructed with the parallel pyramid to implement precise segmentation. Experiments on three benchmark datasets demonstrate the effectiveness of our method for robust multi-level feature aggregation. As a result, our SeENet has achieved better performance than other state-of-the-art methods for semantic segmentation."
Towards High-Resolution Salient Object Detection,"Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, Huchuan Lu","Adobe Research, USA; Dalian University of Technology, China",50.0,china,50.0,USA,"Deep neural network based methods have made a significant breakthrough in salient object detection. However, they are typically limited to input images with low resolutions (400x400 pixels or less). Little effort has been made to train neural networks to directly handle salient object segmentation in high-resolution images. This paper pushes forward high-resolution saliency detection, and contributes a new dataset, named High-Resolution Salient Object Detection (HRSOD) dataset. To our best knowledge, HRSOD is the first high-resolution saliency detection dataset to date. As another contribution, we also propose a novel approach, which incorporates both global semantic information and local high-resolution details, to address this challenging task. More specifically, our approach consists of a Global Semantic Network (GSN), a Local Refinement Network (LRN) and a Global-Local Fusion Network (GLFN). The GSN extracts the global semantic information based on downsampled entire image. Guided by the results of GSN, the LRN focuses on some local regions and progressively produces high-resolution predictions. The GLFN is further proposed to enforce spatial consistency and boost performance. Experiments illustrate that our method outperforms existing state-of-the-art methods on high-resolution saliency datasets by a large margin, and achieves comparable or even better performance than them on some widely used saliency benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Towards_High-Resolution_Salient_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Towards_High-Resolution_Salient_Object_Detection_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008818/,"['Object detection', 'Saliency detection', 'Semantics', 'Task analysis', 'Training', 'Neural networks', 'Image resolution']","['Object Detection', 'Salient Object', 'High-resolution Detection', 'Salient Object Detection', 'High-resolution Object', 'Neural Network', 'Deep Network', 'High-resolution Images', 'Deep Neural Network', 'Semantic Information', 'Large Margin', 'High-resolution Dataset', 'Saliency Detection', 'High-resolution Details', 'Running Time', 'Convolutional Layers', 'Image Dataset', 'Stochastic Gradient Descent', 'Low-level Features', 'Saliency Map', 'Dilated Convolution Layers', 'Patch-based Methods', 'Input Size', 'Precision-recall Curve', 'Limited GPU Memory', 'Rich Contextual Information', 'Parameters Of The Convolutional Layer', 'Boundary Shape', 'Ablation Analysis']",,139,"Deep neural network based methods have made a significant breakthrough in salient object detection. However, they are typically limited to input images with low resolutions (400×400 pixels or less). Little effort has been made to train neural networks to directly handle salient object segmentation in high-resolution images. This paper pushes forward high-resolution saliency detection, and contributes a new dataset, named High-Resolution Salient Object Detection (HRSOD) dataset. To our best knowledge, HRSOD is the first high-resolution saliency detection dataset to date. As another contribution, we also propose a novel approach, which incorporates both global semantic information and local high-resolution details, to address this challenging task. More specifically, our approach consists of a Global Semantic Network (GSN), a Local Refinement Network (LRN) and a Global-Local Fusion Network (GLFN). The GSN extracts the global semantic information based on downsampled entire image. Guided by the results of GSN, the LRN focuses on some local regions and progressively produces high-resolution predictions. The GLFN is further proposed to enforce spatial consistency and boost performance. Experiments illustrate that our method outperforms existing state-of-the-art methods on high-resolution saliency datasets by a large margin, and achieves comparable or even better performance than them on some widely used saliency benchmarks."
Towards Interpretable Face Recognition,"Bangjie Yin, Luan Tran, Haoxiang Li, Xiaohui Shen, Xiaoming Liu",Michigan State University; Wormpex AI Research; ByteDance AI Lab,33.33333333333333,usa,66.66666666666667,China,"Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yin_Towards_Interpretable_Face_Recognition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Towards_Interpretable_Face_Recognition_ICCV_2019_paper.pdf,http://cvlab.cse.msu.edu/project-interpret-FR,,,main,Oral,https://ieeexplore.ieee.org/document/9008301/,"['Face', 'Face recognition', 'Visualization', 'Training', 'Robustness', 'Semantics', 'Feature extraction']","['Face Recognition', 'Spatial Variation', 'Loss Of Diversity', 'Deep Convolutional Neural Network', 'Recognition Accuracy', 'Meaningful Representation', 'Strong Demand', 'Face Representation', 'Interpretable Representation', 'Negative Responses', 'Local Structure', 'Feature Maps', 'Feature Representation', 'Object Detection', 'Image Pairs', 'Peak Location', 'Average Pooling', 'Recognition Performance', 'Face Area', 'Global Average Pooling', 'Filter Response', 'Response Map', 'Local Parts', 'Learned Filters', 'Face Parts', 'Global Max Pooling', 'Loss Function Design', 'Deep Learning Era', 'Final Representation', 'Face Recognition Performance']",,63,"Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations."
Towards Interpretable Object Detection by Unfolding Latent Structures,"Tianfu Wu, Xi Song","Department of ECE and the Visual Narrative Initiative, NC State University; Independent Researcher",50.0,usa,50.0,Unknown,"This paper first proposes a method of formulating model interpretability in visual understanding tasks based on the idea of unfolding latent structures. It then presents a case study in object detection using popular two-stage region-based convolutional network (i.e., R-CNN) detection systems. The proposed method focuses on weakly-supervised extractive rationale generation, that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations. It utilizes a top-down hierarchical and compositional grammar model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold the space of latent part configurations of regions of interest (RoIs). It presents an AOGParsing operator that seamlessly integrates with the RoIPooling/RoIAlign operator widely used in R-CNN and is trained end-to-end. In object detection, a bounding box is interpreted by the best parse tree derived from the AOG on-the-fly, which is treated as the qualitatively extractive rationale generated for interpreting detection. In experiments, Faster R-CNN is used to test the proposed method on the PASCAL VOC 2007 and the COCO 2017 object detection datasets. The experimental results show that the proposed method can compute promising latent structures without hurting the performance. The code and pretrained models are available at https://github.com/iVMCL/iRCNN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Towards_Interpretable_Object_Detection_by_Unfolding_Latent_Structures_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Towards_Interpretable_Object_Detection_by_Unfolding_Latent_Structures_ICCV_2019_paper.pdf,,https://github.com/iVMCL/iRCNN,,main,Oral,https://ieeexplore.ieee.org/document/9008311/,"['Neural networks', 'Visualization', 'Grammar', 'Object detection', 'Task analysis', 'Training', 'Predictive models']","['Object Detection', 'Latent Structure', 'Convolutional Network', 'Hierarchical Model', 'Bounding Box', 'Model Interpretation', 'Configuration Space', 'Faster R-CNN', 'Unfolded Structure', 'Parse Tree', 'Semantic', 'Deep Learning', 'Deep Network', 'Deep Neural Network', 'Image Classification', 'Feature Maps', 'Discriminatory Power', 'Latent Space', 'Directed Acyclic Graph', 'Child Nodes', 'Visual Question Answering', 'Adversarial Attacks', 'Breadth-first Search', 'FC Layer', 'Baseline System', 'Intuitive Idea', 'Principled Way', 'Region Proposal Network', 'Depth-first', 'Feature Pyramid Network']",,17,"This paper first proposes a method of formulating model interpretability in visual understanding tasks based on the idea of unfolding latent structures. It then presents a case study in object detection using popular two-stage region-based convolutional network (i.e., R-CNN) detection systems. The proposed method focuses on weakly-supervised extractive rationale generation, that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations. It utilizes a top-down hierarchical and compositional grammar model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold the space of latent part configurations of regions of interest (RoIs). It presents an AOGParsing operator that seamlessly integrates with the RoIPooling/RoIAlign operator widely used in R-CNN and is trained end-to-end. In object detection, a bounding box is interpreted by the best parse tree derived from the AOG on-the-fly, which is treated as the qualitatively extractive rationale generated for interpreting detection. In experiments, Faster R-CNN is used to test the proposed method on the PASCAL VOC 2007 and the COCO 2017 object detection datasets. The experimental results show that the proposed method can compute promising latent structures without hurting the performance. The code and pretrained models are available at https://github.com/iVMCL/iRCNN."
Towards Latent Attribute Discovery From Triplet Similarities,"Ishan Nigam, Pavel Tokmakov, Deva Ramanan","Robotics Institute, Carnegie Mellon University",100.0,usa,0.0,,"This paper addresses the task of learning latent attributes from triplet similarity comparisons. Consider, for instance, the three shoes in Fig. 1(a). They can be compared according to color, comfort, size, or shape resulting in different rankings. Most approaches for embedding learning either make a simplifying assumption - that all inputs are comparable under a single criterion, or require expensive attribute supervision. We introduce Latent Similarity Networks (LSNs): a simple and effective technique to discover the underlying latent notions of similarity in data without any explicit attribute supervision. LSNs can be trained with standard triplet supervision and learn several latent embeddings that can be used to compare images under multiple notions of similarity. LSNs achieve state-of-the-art performance on UT-Zappos-50k Shoes and Celeb-A Faces datasets and also demonstrate the ability to uncover meaningful latent attributes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nigam_Towards_Latent_Attribute_Discovery_From_Triplet_Similarities_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nigam_Towards_Latent_Attribute_Discovery_From_Triplet_Similarities_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008771/,"['Footwear', 'Image color analysis', 'Training', 'Task analysis', 'Computer vision', 'Shape', 'Semantics']","['Latent Attributes', 'Similar Data', 'Similarity Network', 'Similar Note', 'Embedding Learning', 'Latent Embedding', 'Qualitative Analysis', 'Learning Algorithms', 'Latent Variables', 'Unsupervised Learning', 'ImageNet', 'Stochastic Gradient Descent', 'Random Assignment', 'Unsupervised Methods', 'Latent Space', 'Natural Properties', 'Training Examples', 'User Preferences', 'Visual Properties', 'Linear Subspace', 'Latent Concept', 'Random Baseline', 'Triplet Loss', 'Image Embedding']",,7,"This paper addresses the task of learning latent attributes from triplet similarity comparisons. Consider, for instance, the three shoes in Fig. 1(a). They can be compared according to color, comfort, size, or shape resulting in different rankings. Most approaches for embedding learning either make a simplifying assumption - that all inputs are comparable under a single criterion, or require expensive attribute supervision. We introduce Latent Similarity Networks (LSNs): a simple and effective technique to discover the underlying latent notions of similarity in data without any explicit attribute supervision. LSNs can be trained with standard triplet supervision and learn several latent embeddings that can be used to compare images under multiple notions of similarity. LSNs achieve state-of-the-art performance on UT-Zappos-50k Shoes and Celeb-A Faces datasets and also demonstrate the ability to uncover meaningful latent attributes."
Towards Multi-Pose Guided Virtual Try-On Network,"Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, Jian Yin","ByteDance AI Lab.; School of Computer Science, South China Normal University; Guangzhou Key Laboratory of Big Data and Intelligent Education; School of Intelligent Systems Engineering, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou 510006, P.R.China; School of Data and Computer Science, Sun Yat-sen University; Carnegie Mellon University",75.0,"China, usa",25.0,China,"Virtual try-on systems under arbitrary human poses have significant application potential, yet also raise extensive challenges, such as self-occlusions, heavy misalignment among different poses, and complex clothes textures. Existing virtual try-on methods can only transfer clothes given a fixed human pose, and still show unsatisfactory performances, often failing to preserve person identity or texture details, and with limited pose diversity. This paper makes the first attempt towards a multi-pose guided virtual try-on system, which enables clothes to transfer onto a person with diverse poses. Given an input person image, a desired clothes image, and a desired pose, the proposed Multi-pose Guided Virtual Try-On Network (MG-VTON) generates a new person image after fitting the desired clothes into the person and manipulating the pose. MG-VTON is constructed with three stages: 1) a conditional human parsing network is proposed that matches both the desired pose and the desired clothes shape; 2) a deep Warping Generative Adversarial Network (Warp-GAN) that warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and the desired one; 3) a refinement render network recovers the texture details of clothes and removes artifacts, based on multi-pose composition masks. Extensive experiments on commonly-used datasets and our newly-collected largest virtual try-on benchmark demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively, showing promising virtual try-on performances.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Towards_Multi-Pose_Guided_Virtual_Try-On_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Towards_Multi-Pose_Guided_Virtual_Try-On_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008237/,"['Shape', 'Three-dimensional displays', 'Generators', 'Hair', 'Task analysis', 'Face', 'Gallium nitride']","['Virtual Try-on', 'Virtual Try-on Network', 'Deep Network', 'Generative Adversarial Networks', 'Promising Performance', 'Person Image', 'Texture Details', 'Human Pose', 'Significant Potential For Applications', 'Human Studies', 'Convolutional Network', 'Quantitative Results', 'Feature Maps', 'Body Shape', 'High-quality Images', 'Reference Image', 'Baseline Methods', 'Ground Truth Image', 'Image Synthesis', 'Target Pose', 'Perceptual Loss', 'Thin-plate Spline', 'Transformation Parameters', 'Refinement Network', 'Feature Map Of Layer', 'Downsampling Layer', 'Shape Context', 'L1 Loss', 'Warped Image']",,121,"Virtual try-on systems under arbitrary human poses have significant application potential, yet also raise extensive challenges, such as self-occlusions, heavy misalignment among different poses, and complex clothes textures. Existing virtual try-on methods can only transfer clothes given a fixed human pose, and still show unsatisfactory performances, often failing to preserve person identity or texture details, and with limited pose diversity. This paper makes the first attempt towards a multi-pose guided virtual try-on system, which enables clothes to transfer onto a person with diverse poses. Given an input person image, a desired clothes image, and a desired pose, the proposed Multi-pose Guided Virtual Try-On Network (MG-VTON) generates a new person image after fitting the desired clothes into the person and manipulating the pose. MG-VTON is constructed with three stages: 1) a conditional human parsing network is proposed that matches both the desired pose and the desired clothes shape; 2) a deep Warping Generative Adversarial Network (Warp-GAN) that warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and the desired one; 3) a refinement render network recovers the texture details of clothes and removes artifacts, based on multi-pose composition masks. Extensive experiments on commonly-used datasets and our newly-collected largest virtual try-on benchmark demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively, showing promising virtual try-on performances."
Towards Photorealistic Reconstruction of Highly Multiplexed Lensless Images,"Salman S. Khan, Adarsh V. R., Vivek Boominathan, Jasper Tan, Ashok Veeraraghavan, Kaushik Mitra","IIT Madras, India; Rice University, USA",100.0,"india, usa",0.0,,"Recent advancements in fields like Internet of Things (IoT), augmented reality, etc. have led to an unprecedented demand for miniature cameras with low cost that can be integrated anywhere and can be used for distributed monitoring. Mask-based lensless imaging systems make such inexpensive and compact models realizable. However, reduction in the size and cost of these imagers comes at the expense of their image quality due to the high degree of multiplexing inherent in their design. In this paper, we present a method to obtain image reconstructions from mask-based lensless measurements that are more photorealistic than those currently available in the literature. We particularly focus on FlatCam, a lensless imager consisting of a coded mask placed over a bare CMOS sensor. Existing techniques for reconstructing FlatCam measurements suffer from several drawbacks including lower resolution and dynamic range than lens-based cameras. Our approach overcomes these drawbacks using a fully trainable non-iterative deep learning based model. Our approach is based on two stages: an inversion stage that maps the measurement into the space of intermediate reconstruction and a perceptual enhancement stage that improves this intermediate reconstruction based on perceptual and signal distortion metrics. Our proposed method is fast and produces photo-realistic reconstruction as demonstrated on many real and challenging scenes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Khan_Towards_Photorealistic_Reconstruction_of_Highly_Multiplexed_Lensless_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Khan_Towards_Photorealistic_Reconstruction_of_Highly_Multiplexed_Lensless_Images_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009794/,"['Image reconstruction', 'Cameras', 'Reconstruction algorithms', 'Calibration', 'Extraterrestrial measurements', 'Lenses']","['Lensless Imaging', 'Photorealistic Reconstruction', 'Imaging System', 'Deep Learning', 'Reduction In Size', 'Dynamic Range', 'Internet Of Things', 'Image Reconstruction', 'Signal Distortion', 'Convolutional Neural Network', 'Deep Network', 'Weight Matrix', 'Performance Degradation', 'Original Structure', 'Weight Decay', 'Localization Performance', 'Reconstruction Algorithm', 'Forward Model', 'Residual Block', 'Point Spread Function', 'Output Stage', 'Regularized Least Squares', 'Low Dynamic Range', 'Random Initialization', 'Severe Artifacts', 'Calibration Matrix', 'Perceptual Loss', 'Computer Image', 'Hyperacusis', 'Perception Of Quality']",,22,"Recent advancements in fields like Internet of Things (IoT), augmented reality, etc. have led to an unprecedented demand for miniature cameras with low cost that can be integrated anywhere and can be used for distributed monitoring. Mask-based lensless imaging systems make such inexpensive and compact models realizable. However, reduction in the size and cost of these imagers comes at the expense of their image quality due to the high degree of multiplexing inherent in their design. In this paper, we present a method to obtain image reconstructions from mask-based lensless measurements that are more photorealistic than those currently available in the literature. We particularly focus on FlatCam, a lensless imager consisting of a coded mask placed over a bare CMOS sensor. Existing techniques for reconstructing FlatCam measurements suffer from several drawbacks including lower resolution and dynamic range than lens-based cameras. Our approach overcomes these drawbacks using a fully trainable non-iterative deep learning based model. Our approach is based on two stages: an inversion stage that maps the measurement into the space of intermediate reconstruction and a perceptual enhancement stage that improves this intermediate reconstruction based on perceptual and signal distortion metrics. Our proposed method is fast and produces photo-realistic reconstruction as demonstrated on many real and challenging scenes."
Towards Precise End-to-End Weakly Supervised Object Detection Network,"Ke Yang, Dongsheng Li, Yong Dou",National University of Defense Technology,100.0,China,0.0,,"It is challenging for weakly supervised object detection network to precisely predict the positions of the objects, since there are no instance-level category annotations. Most existing methods tend to solve this problem by using a two-phase learning procedure, i.e., multiple instance learning detector followed by a fully supervised learning detector with bounding-box regression. Based on our observation, this procedure may lead to local minima for some object categories. In this paper, we propose to jointly train the two phases in an end-to-end manner to tackle this problem. Specifically, we design a single network with both multiple instance learning and bounding-box regression branches that share the same backbone. Meanwhile, a guided attention module using classification loss is added to the backbone for effectively extracting the implicit location information in the features. Experimental results on public datasets show that our method achieves state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Towards_Precise_End-to-End_Weakly_Supervised_Object_Detection_Network_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Towards_Precise_End-to-End_Weakly_Supervised_Object_Detection_Network_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010677/,"['Detectors', 'Feature extraction', 'Object detection', 'Proposals', 'Training', 'Task analysis', 'Testing']","['Object Detection', 'Weakly Supervised Object Detection', 'Local Minima', 'Attention Module', 'Classification Loss', 'Bounding Box Regression', 'Multiple Instance Learning', 'Regression Branch', 'Convolutional Neural Network', 'Convolutional Layers', 'Multi-label', 'Object Classification', 'Object Location', 'Spatial Attention', 'Joint Optimization', 'Faster R-CNN', 'Attention Map', 'Positive Instances', 'Object Instances', 'Box Regression', 'Discriminative Parts', 'Multiphase Approach', 'Image-level Labels', 'Candidate Objects', 'Online Manner', 'Channel-wise Attention', 'Object Proposals', 'Proposed Classification', 'Attention Weights', 'Region Proposal']",,87,"It is challenging for weakly supervised object detection network to precisely predict the positions of the objects, since there are no instance-level category annotations. Most existing methods tend to solve this problem by using a two-phase learning procedure, i.e., multiple instance learning detector followed by a fully supervised learning detector with bounding-box regression. Based on our observation, this procedure may lead to local minima for some object categories. In this paper, we propose to jointly train the two phases in an end-to-end manner to tackle this problem. Specifically, we design a single network with both multiple instance learning and bounding-box regression branches that share the same backbone. Meanwhile, a guided attention module using classification loss is added to the backbone for effectively extracting the implicit location information in the features. Experimental results on public datasets show that our method achieves state-of-the-art performance."
Towards Unconstrained End-to-End Text Spotting,"Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, Ying Xiao",Google AI,0.0,,100.0,USA,"We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectification. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to significant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6%, and on the Total-Text (curved) benchmark by more than 16%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010994/,"['Feature extraction', 'Optical character recognition software', 'Text recognition', 'Detectors', 'Decoding', 'Shape', 'Training']","['Text Spotting', 'Benchmark', 'Training Data', 'Recognition Task', 'Attention Model', 'Instance Segmentation', 'Optical Character Recognition', 'Arbitrary Shape', 'Image Features', 'Recognizable', 'Aspect Ratio', 'Detection Results', 'Receptive Field', 'Bounding Box', 'Detection Task', 'Ablation Experiments', 'Inference Time', 'Image Texture', 'Rectangular Box', 'Mask R-CNN', 'Average Precision Score', 'Prediction Head', 'ResNet-50 Backbone', 'Seq2seq Model', 'Receptive Field Size', 'CNN Backbone', 'Reading Direction', 'Non-maximum Suppression', 'Weight Vector']",,83,"We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectification. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to significant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6%, and on the Total-Text (curved) benchmark by more than 16%."
Towards Unsupervised Image Captioning With Shared Multimodal Embeddings,"Iro Laina, Christian Rupprecht, Nassir Navab",Technische Universit ¨at M ¨unchen; University of Oxford,100.0,"germany, uk",0.0,,"Understanding images without explicit supervision has become an important problem in computer vision. In this paper, we address image captioning by generating language descriptions of scenes without learning from annotated pairs of images and their captions. The core component of our approach is a shared latent space that is structured by visual concepts. In this space, the two modalities should be indistinguishable. A language model is first trained to encode sentences into semantically structured embeddings. Image features that are translated into this embedding space can be decoded into descriptions through the same language model, similarly to sentence embeddings. This translation is learned from weakly paired images and text using a loss robust to noisy assignments and a conditional adversarial component. Our approach allows to exploit large text corpora outside the annotated distributions of image/caption data. Our experiments show that the proposed domain alignment learns a semantically meaningful representation which outperforms previous work.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010396/,"['Visualization', 'Task analysis', 'Training', 'Decoding', 'Dogs', 'Detectors', 'Semantics']","['Image Captioning', 'Multi-modal Embedding', 'Semantic', 'Image Features', 'Latent Space', 'Language Model', 'Computer Vision Problems', 'Domain Alignment', 'Visual Concepts', 'Sentence Embedding', 'Convolutional Neural Network', 'State Of The Art', 'Object Detection', 'Recurrent Neural Network', 'Image Regions', 'Paired Data', 'Similar Concepts', 'Source Images', 'Image Domain', 'Latent Representation', 'Language Domains', 'Adversarial Training', 'Source Text', 'Concept Of Image', 'Image Descriptors', 'Triplet Loss', 'Gated Recurrent Unit', 'Source Domain', 'Independent Images', 'Structural Embeddedness']",,53,"Understanding images without explicit supervision has become an important problem in computer vision. In this paper, we address image captioning by generating language descriptions of scenes without learning from annotated pairs of images and their captions. The core component of our approach is a shared latent space that is structured by visual concepts. In this space, the two modalities should be indistinguishable. A language model is first trained to encode sentences into semantically structured embeddings. Image features that are translated into this embedding space can be decoded into descriptions through the same language model, similarly to sentence embeddings. This translation is learned from weakly paired images and text using a loss robust to noisy assignments and a conditional adversarial component. Our approach allows to exploit large text corpora outside the annotated distributions of image/caption data. Our experiments show that the proposed domain alignment learns a semantically meaningful representation which outperforms previous work."
Toyota Smarthome: Real-World Activities of Daily Living,"Srijan Das, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond, Gianpiero Francesca","Universit ´e Cˆote d’Azur, Inria; Toyota Motor Europe",50.0,France,50.0,Belgium,"The performance of deep neural networks is strongly influenced by the quantity and quality of annotated data. Most of the large activity recognition datasets consist of data sourced from the web, which does not reflect challenges that exist in activities of daily living. In this paper, we introduce a large real-world video dataset for activities of daily living: Toyota Smarthome. The dataset consists of 16K RGB+D clips of 31 activity classes, performed by seniors in a smarthome. Unlike previous datasets, videos were fully unscripted. As a result, the dataset poses several challenges: high intra-class variation, high class imbalance, simple and composite activities, and activities with similar motion and variable duration. Activities were annotated with both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome from other datasets for activity recognition. As recent activity recognition approaches fail to address the challenges posed by Toyota Smarthome, we present a novel activity recognition method with attention mechanism. We propose a pose driven spatio-temporal attention mechanism through 3D ConvNets. We show that our novel method outperforms state-of-the-art methods on benchmark datasets, as well as on the Toyota Smarthome dataset. We release the dataset for research use.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf,https://project.inria.fr/toyotasmarthome,,,main,Poster,https://ieeexplore.ieee.org/document/9008135/,"['Cameras', 'Three-dimensional displays', 'Activity recognition', 'Monitoring', 'Kinetic theory', 'Skeleton', 'Feature extraction']","['Smart Home', 'Attention Mechanism', 'Action Recognition', 'Activity Classification', 'Similar Motion', 'Action Recognition Datasets', 'Spatiotemporal Mechanisms', 'Feature Maps', 'Attention Network', 'Spatial Attention', '3D Information', 'Camera View', 'Attention Weights', 'Camera Frame', 'Saliency Map', 'Joint Training', 'High Motion', 'Video Samples', '3D Pose', 'Key Frames', 'Temporal Attention', '3D Skeleton', '3D Joint', 'Real-world Challenges', 'Low Motion', 'Sequence Network', 'Spatiotemporal Representation', 'Attention Model', 'Spatial Attention Mechanism']",,87,"The performance of deep neural networks is strongly influenced by the quantity and quality of annotated data. Most of the large activity recognition datasets consist of data sourced from the web, which does not reflect challenges that exist in activities of daily living. In this paper, we introduce a large real-world video dataset for activities of daily living: Toyota Smarthome. The dataset consists of 16K RGB+D clips of 31 activity classes, performed by seniors in a smarthome. Unlike previous datasets, videos were fully unscripted. As a result, the dataset poses several challenges: high intra-class variation, high class imbalance, simple and composite activities, and activities with similar motion and variable duration. Activities were annotated with both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome from other datasets for activity recognition. As recent activity recognition approaches fail to address the challenges posed by Toyota Smarthome, we present a novel activity recognition method with attention mechanism. We propose a pose driven spatio-temporal attention mechanism through 3D ConvNets. We show that our novel method outperforms state-of-the-art methods on benchmark datasets, as well as on the Toyota Smarthome dataset. We release the dataset for research use."
Tracking Without Bells and Whistles,"Philipp Bergmann, Tim Meinhardt, Laura Leal-TaixÃ©",Technical University of Munich,100.0,germany,0.0,,"The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bergmann_Tracking_Without_Bells_and_Whistles_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bergmann_Tracking_Without_Bells_and_Whistles_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010033/,"['Detectors', 'Trajectory', 'Target tracking', 'Object detection', 'Task analysis', 'Proposals']","['Whistle', 'Superior Performance', 'Paradigm Shift', 'Object Detection', 'Bounding Box', 'Small Objects', 'Tracking Performance', 'Object Position', 'Promising Research Direction', 'Motion Prediction', 'Camera Motion', 'Bounding Box Regression', 'Object In Frame', 'Promising Direction For Future Research', 'Multi-object Tracking', 'Tracking Scenarios', 'Neural Network', 'Frame Rate', 'Object Size', 'Action Recognition', 'Motion Model', 'Detector Set', 'Faster R-CNN', 'Identity Preservation', 'Lower Frame', 'Multiple Object Tracking', 'Feature Pyramid Network', 'Tracking Problem', 'Siamese Network', 'Object Bounding Boxes']",,665,"The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions."
Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning,"Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang, Yonghong Tian","Center for Data Science, AAIS, Peking University; National Engineering Laboratory for Video Technology, School of EE&CS, Peking University; Peng Cheng Laborotory, Shenzhen, China; State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University",75.0,china,25.0,China,"Few-shot learning, which aims at extracting new concepts rapidly from extremely few examples of novel classes, has been featured into the meta-learning paradigm recently. Yet, the key challenge of how to learn a generalizable classifier with the capability of adapting to specific tasks with severely limited data still remains in this domain. To this end, we propose a Transductive Episodic-wise Adaptive Metric (TEAM) framework for few-shot learning, by integrating the meta-learning paradigm with both deep metric learning and transductive inference. With exploring the pairwise constraints and regularization prior within each task, we explicitly formulate the adaptation procedure into a standard semi-definite programming problem. By solving the problem with its closed-form solution on the fly with the setup of transduction, our approach efficiently tailors an episodic-wise metric for each task to adapt all features from a shared task-agnostic embedding space into a more discriminative task-specific metric space. Moreover, we further leverage an attention-based bi-directional similarity strategy for extracting the more robust relationship between queries and prototypes. Extensive experiments on three benchmark datasets show that our framework is superior to other existing approaches and achieves the state-of-the-art performance in the few-shot literature.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010009/,"['Task analysis', 'Training', 'Feature extraction', 'Extraterrestrial measurements', 'Adaptation models', 'Prototypes']","['Few-shot Learning', 'Benchmark Datasets', 'Latent Space', 'Robust Relationship', 'Metric Learning', 'Semidefinite Programming Problem', 'Deep Metric Learning', 'Deep Neural Network', 'K-nearest Neighbor', 'Batch Normalization', 'Off-diagonal', 'Positive Definite Matrix', 'Backbone Network', 'Unlabeled Data', 'Batch Normalization Layer', 'Semi-supervised Learning', 'Convex Combination', 'Series Of Tasks', 'Target Task', 'Support Set', 'Few-shot Classification', 'Query Set', 'Source Task', 'Class Prototypes', 'Unseen Classes', 'Distinct Information', 'Evaluation Of Classes', 'Loss Function']",,113,"Few-shot learning, which aims at extracting new concepts rapidly from extremely few examples of novel classes, has been featured into the meta-learning paradigm recently. Yet, the key challenge of how to learn a generalizable classifier with the capability of adapting to specific tasks with severely limited data still remains in this domain. To this end, we propose a Transductive Episodic-wise Adaptive Metric (TEAM) framework for few-shot learning, by integrating the meta-learning paradigm with both deep metric learning and transductive inference. With exploring the pairwise constraints and regularization prior within each task, we explicitly formulate the adaptation procedure into a standard semi-definite programming problem. By solving the problem with its closed-form solution on the fly with the setup of transduction, our approach efficiently tailors an episodic-wise metric for each task to adapt all features from a shared task-agnostic embedding space into a more discriminative task-specific metric space. Moreover, we further leverage an attention-based bi-directional similarity strategy for extracting the more robust relationship between queries and prototypes. Extensive experiments on three benchmark datasets show that our framework is superior to other existing approaches and achieves the state-of-the-art performance in the few-shot literature."
Transductive Learning for Zero-Shot Object Detection,"Shafin Rahman, Salman Khan, Nick Barnes",Australian National University; Data61-CSIRO; Inception Institute of AI,66.66666666666666,"Australia, uae",33.33333333333334,China,"Zero-shot object detection (ZSD) is a relatively unexplored research problem as compared to the conventional zero-shot recognition task. ZSD aims to detect previously unseen objects during inference. Existing ZSD works suffer from two critical issues: (a) large domain-shift between the source (seen) and target (unseen) domains since the two distributions are highly mismatched. (b) the learned model is biased against unseen classes, therefore in generalized ZSD settings, where both seen and unseen objects co-occur during inference, the learned model tends to misclassify unseen to seen categories. This brings up an important question: How effectively can a transductive setting address the aforementioned problems? To the best of our knowledge, we are the first to propose a transductive zero-shot object detection approach that convincingly reduces the domain-shift and model-bias against unseen classes. Our approach is based on a self-learning mechanism that uses a novel hybrid pseudo-labeling technique. It progressively updates learned model parameters by associating unlabeled data samples to their corresponding classes. During this process, our technique makes sure that knowledge that was previously acquired on the source domain is not forgotten. We report significant 'relative' improvements of 34.9% and 77.1% in terms of mAP and recall rates over the previous best inductive models on MSCOCO dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rahman_Transductive_Learning_for_Zero-Shot_Object_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Transductive_Learning_for_Zero-Shot_Object_Detection_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009482/,"['Training', 'Data models', 'Visualization', 'Task analysis', 'Semantics', 'Detectors', 'Object detection']","['Object Detection', 'Zero-shot', 'Transductive Learning', 'Zero-shot Object Detection', 'Learning Models', 'Recognition Task', 'Target Domain', 'Unlabeled Data', 'Recall Rate', 'Source Domain', 'Unseen Objects', 'Unseen Classes', 'Visual Features', 'Prediction Score', 'Bounding Box', 'Detection Task', 'Inductive Reasoning', 'Word Embedding', 'Domain Shift Problem', 'Transduction Methods', 'Unlabeled Set', 'Anchor Boxes', 'Split Set', 'Unlabeled Target Domain', 'Semantic Vectors', 'Focal Loss', 'Classification Branch', 'Unseen Data']",,57,"Zero-shot object detection (ZSD) is a relatively unexplored research problem as compared to the conventional zero-shot recognition task. ZSD aims to detect previously unseen objects during inference. Existing ZSD works suffer from two critical issues: (a) large domain-shift between the source (seen) and target (unseen) domains since the two distributions are highly mismatched. (b) the learned model is biased against unseen classes, therefore in generalized ZSD settings, where both seen and unseen objects co-occur during inference, the learned model tends to misclassify unseen to seen categories. This brings up an important question: How effectively can a transductive setting address the aforementioned problems? To the best of our knowledge, we are the first to propose a transductive zero-shot object detection approach that convincingly reduces the domain-shift and model-bias against unseen classes. Our approach is based on a self-learning mechanism that uses a novel hybrid pseudo-labeling technique. It progressively updates learned model parameters by associating unlabeled data samples to their corresponding classes. During this process, our technique makes sure that knowledge that was previously acquired on the source domain is not forgotten. We report significant 'relative' improvements of 34.9% and 77.1% in terms of mAP and recall rates over the previous best inductive models on MSCOCO dataset."
Transferability and Hardness of Supervised Classification Tasks,"Anh T. Tran, Cuong V. Nguyen, Tal Hassner",Amazon Web Services; VinAI Research; Facebook AI,0.0,,100.0,USA,"We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets---CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks)---together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for highly transferable attributes.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tran_Transferability_and_Hardness_of_Supervised_Classification_Tasks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tran_Transferability_and_Hardness_of_Supervised_Classification_Tasks_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009545/,"['Task analysis', 'Training', 'Entropy', 'Data models', 'Random variables', 'Machine learning', 'Computational modeling']","['Hardness', 'Classification Task', 'Supervised Classification Task', 'Random Variables', 'Learning Models', 'Large-scale Datasets', 'Face Recognition', 'Target Task', 'Training Labels', 'Conditional Entropy', 'Categorical Attributes', 'Face Recognition Model', 'Machine Learning', 'Training Set', 'Training Data', 'Deep Network', 'Support Vector Machine', 'State Of The Art', 'Transfer Learning', 'Multiple Tasks', 'Source Task', 'Input Domain', 'Sequence Labeling', 'Trivial Task', 'Multi-task Learning', 'Multiple Labels', 'Practical Use Cases', 'Transfer Task', 'Facial Attributes', 'Labeled Data Set']",,50,"We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets - CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks) - together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for tasks estimated to be highly transferable."
Transferable Contrastive Network for Generalized Zero-Shot Learning,"Huajie Jiang, Ruiping Wang, Shiguang Shan, Xilin Chen","1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; 2University of Chinese Academy of Sciences, Beijing, 100049, China; 1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; 2University of Chinese Academy of Sciences, Beijing, 100049, China; 3Shanghai Institute of Microsystem and Information Technology, CAS, Shanghai, 200050, China; 4School of Information Science and Technology, ShanghaiTech University, Shanghai, 200031, China",100.0,china,0.0,,"Zero-shot learning (ZSL) is a challenging problem that aims to recognize the target categories without seen data, where semantic information is leveraged to transfer knowledge from some source classes. Although ZSL has made great progress in recent years, most existing approaches are easy to overfit the sources classes in generalized zero-shot learning (GZSL) task, which indicates that they learn little knowledge about target classes. To tackle such problem, we propose a novel Transferable Contrastive Network (TCN) that explicitly transfers knowledge from the source classes to the target classes. It automatically contrasts one image with different classes to judge whether they are consistent or not. By exploiting the class similarities to make knowledge transfer from source images to similar target classes, our approach is more robust to recognize the target images. Experiments on five benchmark datasets show the superiority of our approach for GZSL.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Transferable_Contrastive_Network_for_Generalized_Zero-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Transferable_Contrastive_Network_for_Generalized_Zero-Shot_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009572/,"['Semantics', 'Visualization', 'Image recognition', 'Robustness', 'Target recognition', 'Task analysis', 'Fuses']","['Zero-shot', 'Generalized Zero-shot Learning', 'Knowledge Transfer', 'Semantic Information', 'Target Image', 'Target Class', 'Similar Classification', 'Source Images', 'Target Category', 'Progress In Recent Years', 'Source Class', 'Neural Network', 'Convolutional Neural Network', 'Feature Space', 'Multilayer Perceptron', 'Latent Space', 'Image Recognition', 'Class Information', 'Word Embedding', 'Common Space', 'Self-supervised Learning', 'Visual Space', 'Discriminative Properties', 'Semantic Space', 'Transfer Properties', 'Hidden Dimension', 'Metric Learning', 'Source Categories', 'Information Fusion', 'Domain Shift Problem']",,128,"Zero-shot learning (ZSL) is a challenging problem that aims to recognize the target categories without seen data, where semantic information is leveraged to transfer knowledge from some source classes. Although ZSL has made great progress in recent years, most existing approaches are easy to overfit the sources classes in generalized zero-shot learning (GZSL) task, which indicates that they learn little knowledge about target classes. To tackle such problem, we propose a novel Transferable Contrastive Network (TCN) that explicitly transfers knowledge from the source classes to the target classes. It automatically contrasts one image with different classes to judge whether they are consistent or not. By exploiting the class similarities to make knowledge transfer from source images to similar target classes, our approach is more robust to recognize the target images. Experiments on five benchmark datasets show the superiority of our approach for GZSL."
Transferable Representation Learning in Vision-and-Language Navigation,"Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, Eugene Ie",Google Research,0.0,,100.0,USA,"Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Transferable_Representation_Learning_in_Vision-and-Language_Navigation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Transferable_Representation_Learning_in_Vision-and-Language_Navigation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010345/,"['Task analysis', 'Visualization', 'Navigation', 'Training', 'Adaptation models', 'Predictive models', 'Length measurement']","['Transfer Learning', 'Representation Learning', 'Transferable Representations', 'Path Length', 'Language Teaching', 'Latent Space', 'Navigation Task', 'Alignment Task', 'Training Set', 'Validation Set', 'Visual Representation', 'Random Walk', 'Semantic Similarity', 'Visual Input', 'Absolute Measures', 'Visual Scene', 'Area Under Receiver Operating Characteristic Curve', 'Policy Gradient', 'Auxiliary Task', 'Path Nodes', 'Visual Encoding', 'Reference Path', 'Behavior Policy', 'Beam Search', 'Improve Success Rates', 'Original Path', 'Trained Agent', 'Undirected', 'Viewing Angle']",,43,"Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric."
Transferable Semi-Supervised 3D Object Detection From RGB-D Data,"Yew Siang Tang, Gim Hee Lee","Department of Computer Science, National University of Singapore",100.0,singapore,0.0,,"We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semi-supervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Box-to-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tang_Transferable_Semi-Supervised_3D_Object_Detection_From_RGB-D_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_Transferable_Semi-Supervised_3D_Object_Detection_From_RGB-D_Data_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008127/,"['Three-dimensional displays', 'Object detection', 'Two dimensional displays', 'Detectors', 'Training', 'Solid modeling', 'Estimation']","['Object Detection', '3D Object Detection', 'RGB-D Data', 'Class Labels', 'Bounding Box', 'Object Classification', 'Set Of Classes', '3D Information', 'Reprojection', 'KITTI Dataset', '3D Detection', 'Semi-supervised Model', '3D Bounding Box', 'Upper Bound', 'Pedestrian', 'Image Plane', 'Point Cloud', 'Semantic Segmentation', '3D Point Cloud', 'Set Of Perturbations', 'Strong Labeling', 'Object Boxes', 'Semi-supervised Learning', 'Regression Loss', '3D Point', 'Weak Labels', 'Prediction Box', 'Mean Average Precision']",,27,"We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semi-supervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Box-to-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets."
Transformable Bottleneck Networks,"Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li, Linjie Luo",ByteDance Inc.; University of Southern California; Snap Inc.,33.33333333333333,usa,66.66666666666667,China,"We propose a novel approach to performing fine-grained 3D manipulation of image content via a convolutional neural network, which we call the Transformable Bottleneck Network (TBN). It applies given spatial transformations directly to a volumetric bottleneck within our encoder-bottleneck-decoder architecture. Multi-view supervision encourages the network to learn to spatially disentangle the feature space within the bottleneck. The resulting spatial structure can be manipulated with arbitrary spatial transformations. We demonstrate the efficacy of TBNs for novel view synthesis, achieving state-of-the-art results on a challenging benchmark. We demonstrate that the bottlenecks produced by networks trained for this task contain meaningful spatial structure that allows us to intuitively perform a variety of image manipulations in 3D, well beyond the rigid transformations seen during training. These manipulations include non-uniform scaling, non-rigid warping, and combining content from different images. Finally, we extract explicit 3D structure from the bottleneck, performing impressive 3D reconstruction from a single input image.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Olszewski_Transformable_Bottleneck_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Olszewski_Transformable_Bottleneck_Networks_ICCV_2019_paper.pdf,,https://github.com/kyleolsz/TB-Networks,,main,Oral,https://ieeexplore.ieee.org/document/9010362/,"['Three-dimensional displays', 'Decoding', 'Training', 'Two dimensional displays', 'Computer architecture', 'Shape', 'Task analysis']","['Convolutional Neural Network', '3D Structure', 'Feature Space', 'Input Image', 'Single Image', 'Network Training', '3D Reconstruction', 'Use Of Imaging', 'Rigid Transformation', 'View Synthesis', '3D Manipulation', 'Single Input Image', '3D Printing', 'Grid Cells', 'Training Methods', 'Hallucinations', 'Color Images', 'Flow Field', 'Target Image', 'Encoder-decoder', 'Image Synthesis', 'Objective View', 'Non-rigid Deformation', 'Output Image', 'Single View', 'Encoder Network', 'Volumetric Reconstruction', 'Qualitative Examples', 'Multiple Inputs', 'Source Images']",,45,"We propose a novel approach to performing fine-grained 3D manipulation of image content via a convolutional neural network, which we call the Transformable Bottleneck Network (TBN). It applies given spatial transformations directly to a volumetric bottleneck within our encoder-bottleneck-decoder architecture. Multi-view supervision encourages the network to learn to spatially disentangle the feature space within the bottleneck. The resulting spatial structure can be manipulated with arbitrary spatial transformations. We demonstrate the efficacy of TBNs for novel view synthesis, achieving state-of-the-art results on a challenging benchmark. We demonstrate that the bottlenecks produced by networks trained for this task contain meaningful spatial structure that allows us to intuitively perform a variety of image manipulations in 3D, well beyond the rigid transformations seen during training. These manipulations include non-uniform scaling, non-rigid warping, and combining content from different images. Finally, we extract explicit 3D structure from the bottleneck, performing impressive 3D reconstruction from a single input image."
Two-Stream Action Recognition-Oriented Video Super-Resolution,"Haochen Zhang, Dong Liu, Zhiwei Xiong","CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230027, China",100.0,china,0.0,,"We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Two-Stream_Action_Recognition-Oriented_Video_Super-Resolution_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Two-Stream_Action_Recognition-Oriented_Video_Super-Resolution_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008393/,"['Streaming media', 'Visualization', 'Optical imaging', 'Image recognition', 'Spatial resolution', 'Task analysis']","['Super-resolution', 'Video Super-resolution', 'Convolutional Network', 'Recognition Accuracy', 'Visual Quality', 'Action Recognition', 'Optical Flow', 'Consecutive Frames', 'Recognition Network', 'Siamese Network', 'Temporal Continuity', 'Super-resolution Network', 'Two-stream Network', 'Loss Function', 'Convolutional Neural Network', 'Image Classification', 'Highest Accuracy', 'Temporal Information', 'Generative Adversarial Networks', 'Temporal Profile', 'Bicubic Interpolation', 'Temporal Consistency', 'Perceptual Loss', 'Adjacent Frames', 'Motion Compensation', 'Mean Square Error Loss', 'Convolutional Neural Network Architecture', 'Single Image Super-resolution', 'Blue Box', '3D Convolution']",,32,"We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy."
U-CAM: Visual Explanation Using Uncertainty Based Class Activation Maps,"Badri N. Patro, Mayank Lunayach, Shivansh Patel, Vinay P. Namboodiri","Indian Institute of Technology, Kanpur",100.0,India,0.0,,"Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009044/,"['Uncertainty', 'Task analysis', 'Visualization', 'Predictive models', 'Machine learning', 'Mathematical model', 'Data models']","['Class Activation Maps', 'Visual Explanation', 'Deep Learning', 'Deep Models', 'Deep Learning Models', 'Attention Map', 'Human Attention', 'Visual Question Answering', 'Attention Regions', 'Convolutional Neural Network', 'Rank Correlation', 'Image Features', 'State Of The Art', 'Cross-entropy', 'Uncertainty Estimation', 'Cross-entropy Loss', 'Model Uncertainty', 'Class Probabilities', 'Classification Loss', 'Prediction Uncertainty', 'Standard Cross-entropy Loss', 'Earth Moverâ€™s Distance', 'Aleatoric Uncertainty', 'Epistemic Uncertainty', 'Variance Of Gaussian Noise', 'Output Logits', 'Attention Feature', 'Logit Values', 'Ablation Analysis', 'Cost Function']",,47,"Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods."
U4D: Unsupervised 4D Dynamic Scene Understanding,"Armin Mustafa, Chris Russell, Adrian Hilton","CVSSP, University of Surrey, United Kingdom",100.0,uk,0.0,,"We introduce the first approach to solve the challenging problem of unsupervised 4D visual scene understanding for complex dynamic scenes with multiple interacting people from multi-view video. Our approach simultaneously estimates a detailed model that includes a per-pixel semantically and temporally coherent reconstruction, together with instance-level segmentation exploiting photo-consistency, semantic and motion information. We further leverage recent advances in 3D pose estimation to constrain the joint semantic instance segmentation and 4D temporally coherent reconstruction. This enables per person semantic instance segmentation of multiple interacting people in complex dynamic scenes. Extensive evaluation of the joint visual scene understanding framework against state-of-the-art methods on challenging indoor and outdoor sequences demonstrates a significant (approx 40%) improvement in semantic segmentation, reconstruction and scene flow accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mustafa_U4D_Unsupervised_4D_Dynamic_Scene_Understanding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mustafa_U4D_Unsupervised_4D_Dynamic_Scene_Understanding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010646/,"['Three-dimensional displays', 'Semantics', 'Motion segmentation', 'Image reconstruction', 'Two dimensional displays', 'Image segmentation', 'Estimation']","['Scene Understanding', 'Dynamic Scenes', 'Improvement In Accuracy', 'Semantic Information', 'Semantic Segmentation', 'Extensive Evaluation', 'Pose Estimation', 'Motion Information', 'Instance Segmentation', 'Human Pose Estimation', 'Temporal Coherence', '3D Pose', 'Segmented Flow', 'Time And Space', 'Classification Methods', 'Depth Information', '3D Point', 'Depth Values', 'Joint Optimization', 'Flow Estimation', 'Motion Constraints', 'Temporal Correspondence', 'Motion In Space', 'Joint Estimation', 'Semantic Constraints', 'Motion Estimation', 'Objects In The Scene', 'Sparse Feature', 'Pre-trained Parameters', 'Large Motion']",,8,"We introduce the first approach to solve the challenging problem of unsupervised 4D visual scene understanding for complex dynamic scenes with multiple interacting people from multi-view video. Our approach simultaneously estimates a detailed model that includes a per-pixel semantically and temporally coherent reconstruction, together with instance-level segmentation exploiting photo-consistency, semantic and motion information. We further leverage recent advances in 3D pose estimation to constrain the joint semantic instance segmentation and 4D temporally coherent reconstruction. This enables per person semantic instance segmentation of multiple interacting people in complex dynamic scenes. Extensive evaluation of the joint visual scene understanding framework against state-of-the-art methods on challenging indoor and outdoor sequences demonstrates a significant (approx 40%) improvement in semantic segmentation, reconstruction and scene flow accuracy."
UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation,"Jogendra Nath Kundu, Nishank Lakkakula, R. Venkatesh Babu","Video Analytics Lab, Indian Institute of Science, Bangalore, India",100.0,India,0.0,,"Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010723/,"['Task analysis', 'Training', 'Decoding', 'Learning systems', 'Computer vision', 'Predictive models', 'Adaptation models']","['Unsupervised Adaptation', 'Transfer Learning', 'Representation Learning', 'Prediction Task', 'Balance Performance', 'Individual Tasks', 'Domain Adaptation', 'Multi-task Learning', 'Regularization Scheme', 'Domain Discrepancy', 'ImageNet Classification', 'Distillation Loss', 'Visual Representation', 'Section Of The Paper', 'Generative Adversarial Networks', 'Joint Distribution', 'Semantic Segmentation', 'Adaptive Approach', 'Target Domain', 'Latent Representation', 'Source Domain', 'Depth Estimation', 'Outdoor Scenes', 'Ground Truth Map', 'Prior Art', 'Spatial Regularization', 'Multi-task Framework', 'Source Domain Samples', 'Individual Task Performance']",,33,"Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset."
USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds,"Jiaxin Li, Gim Hee Lee","Department of Computer Science, National University of Singapore",100.0,singapore,0.0,,"In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. https://github.com/lijx10/USIP",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.pdf,,https://github.com/lijx10/USIP,,main,Poster,https://ieeexplore.ieee.org/document/9010005/,"['Three-dimensional displays', 'Detectors', 'Training', 'Proposals', 'Probabilistic logic', 'Uncertainty', 'Solid modeling']","['Point Cloud', '3D Point Cloud', 'Feature Point Detection', 'Localization Accuracy', 'High Repeatability', 'CAD Model', 'Keypoint Detection', 'Centroid', 'Computational Efficiency', 'Gaussian Noise', 'K-nearest Neighbor', 'Receptive Field', 'Multilayer Perceptron', 'Transformation Matrix', 'Singular Value Decomposition', 'Quantization Error', 'Redwood', 'Input Point', 'Non-maximum Suppression', 'Translation Vector', 'Input Point Cloud', 'KITTI Dataset', 'Point Cloud Registration', '3D Detection', '3D Rotation']",,122,"In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. https://github.com/lijx10/USIP."
Uncertainty Modeling of Contextual-Connections Between Tracklets for Unconstrained Video-Based Face Recognition,"Jingxiao Zheng, Ruichi Yu, Jun-Cheng Chen, Boyu Lu, Carlos D. Castillo, Rama Chellappa","CITI, Academia Sinica, Taiwan; UMIACS, University of Maryland, College Park",50.0,usa,50.0,Unknown,"Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_Uncertainty_Modeling_of_Contextual-Connections_Between_Tracklets_for_Unconstrained_Video-Based_Face_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_Uncertainty_Modeling_of_Contextual-Connections_Between_Tracklets_for_Unconstrained_Video-Based_Face_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008806/,"['Logic gates', 'Face', 'Face recognition', 'Uncertainty', 'Context modeling', 'Probes', 'Training']","['Face Recognition', 'Video-based Face Recognition', 'Graphical Model', 'Inference Time', 'Identical Distribution', 'Video Surveillance', 'Body Appearance', 'Training Data', 'Contextual Information', 'Greater Than Or Equal', 'Identity Information', 'Deep Convolutional Neural Network', 'Supplementary Materials For Details', 'Negative Information', 'Information Propagation', 'Still Images', 'Positive Information', 'Positive Connection', 'Graph Neural Networks', 'Challenging Dataset', 'Positive Gate', 'Negative Gate', 'Conditional Random Field', 'Negative Connectivity', 'Label Propagation', 'Linear Embedding', 'Embedding Learning', 'Deep Network', 'Zero-shot', 'Large Margin']",,8,"Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset."
Uncertainty-Aware Audiovisual Activity Recognition Using Deep Bayesian Variational Inference,"Mahesh Subedar, Ranganath Krishnan, Paulo Lopez Meyer, Omesh Tickoo, Jonathan Huang",Intel Labs,0.0,,100.0,USA,"Deep neural networks (DNNs) provide state-of-the-art results for a multitude of applications, but the approaches using DNNs for multimodal audiovisual applications do not consider predictive uncertainty associated with individual modalities. Bayesian deep learning methods provide principled confidence and quantify predictive uncertainty. Our contribution in this work is to propose an uncertainty aware multimodal Bayesian fusion framework for activity recognition. We demonstrate a novel approach that combines deterministic and variational layers to scale Bayesian DNNs to deeper architectures. Our experiments using in- and out-of-distribution samples selected from a subset of Moments-in-Time (MiT) dataset show a more reliable confidence measure as compared to the non-Bayesian baseline and the Monte Carlo dropout (MC dropout) approximate Bayesian inference. We also demonstrate the uncertainty estimates obtained from the proposed framework can identify out-of-distribution data on the UCF101 and MiT datasets. In the multimodal setting, the proposed framework improved precision-recall AUC by 10.2% on the subset of MiT dataset as compared to non-Bayesian baseline.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Subedar_Uncertainty-Aware_Audiovisual_Activity_Recognition_Using_Deep_Bayesian_Variational_Inference_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Subedar_Uncertainty-Aware_Audiovisual_Activity_Recognition_Using_Deep_Bayesian_Variational_Inference_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008242/,"['Bayes methods', 'Uncertainty', 'Activity recognition', 'Mathematical model', 'Monte Carlo methods', 'Task analysis', 'Neural networks']","['Bayesian Inference', 'Action Recognition', 'Variational Inference', 'Variational Bayesian Inference', 'Neural Network', 'Deep Learning', 'Deep Neural Network', 'Uncertainty Estimation', 'Contributions Of This Work', 'Individual Modules', 'Prediction Uncertainty', 'Measure Of Confidence', 'Area Under The Precision-recall Curve', 'Approximate Inference', 'Human Activities', 'Monte Carlo Simulation', 'Posterior Probability', 'Bayesian Model', 'Markov Chain Monte Carlo', 'Stochastic Model', 'Deep Neural Network Model', 'Human Activity Recognition', 'Bayesian Neural Network', 'Measurement Uncertainty', 'Audio Input', 'Distribution Of Categories', 'Deep Neural Network Architecture', 'Density Histogram', 'Predictive Distribution', 'Multimodal Methods']",,37,"Deep neural networks (DNNs) provide state-of-the-art results for a multitude of applications, but the approaches using DNNs for multimodal audiovisual applications do not consider predictive uncertainty associated with individual modalities. Bayesian deep learning methods provide principled confidence and quantify predictive uncertainty. Our contribution in this work is to propose an uncertainty aware multimodal Bayesian fusion framework for activity recognition. We demonstrate a novel approach that combines deterministic and variational layers to scale Bayesian DNNs to deeper architectures. Our experiments using in- and out-of-distribution samples selected from a subset of Moments-in-Time (MiT) dataset show a more reliable confidence measure as compared to the non-Bayesian baseline and the Monte Carlo dropout (MC dropout) approximate Bayesian inference. We also demonstrate the uncertainty estimates obtained from the proposed framework can identify out-of-distribution data on the UCF101 and MiT datasets. In the multimodal setting, the proposed framework improved precision-recall AUC by 10.2% on the subset of MiT dataset as compared to non-Bayesian baseline."
Unconstrained Foreground Object Search,"Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari",Adobe Research; University of Texas at Austin,50.0,usa,50.0,USA,"Many people search for foreground objects to use when editing images. While existing methods can retrieve candidates to aid in this, they are constrained to returning objects that belong to a pre-specified semantic class. We instead propose a novel problem of unconstrained foreground object (UFO) search and introduce a solution that supports efficient search by encoding the background image in the same latent space as the candidate foreground objects. A key contribution of our work is a cost-free, scalable approach for creating a large-scale training dataset with a variety of foreground objects of differing semantic categories per image location. Quantitative and human-perception experiments with two diverse datasets demonstrate the advantage of our UFO search solution over related baselines.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Unconstrained_Foreground_Object_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Unconstrained_Foreground_Object_Search_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010902/,"['Search problems', 'Training', 'Semantics', 'Task analysis', 'Training data', 'Databases', 'Image color analysis']","['Foreground Objects', 'Unconstrained Search', 'Training Dataset', 'Latent Space', 'Background Image', 'Training Data', 'Positive Samples', 'Percentage Points', 'Feature Space', 'Negative Samples', 'Object Detection', 'Search Method', 'Image Object', 'Color Difference', 'High-level Features', 'Image Retrieval', 'Nearest Neighbor Search', 'Original Objective', 'Score Map', 'Triplet Loss', 'Query Image', 'Generate Training Data', 'Search Problem', 'Ranking Problem', 'Background Scene', 'Usability Evaluation', 'Aspect Ratio', 'Surgical Margins']",,5,"Many people search for foreground objects to use when editing images. While existing methods can retrieve candidates to aid in this, they are constrained to returning objects that belong to a pre-specified semantic class. We instead propose a novel problem of unconstrained foreground object (UFO) search and introduce a solution that supports efficient search by encoding the background image in the same latent space as the candidate foreground objects. A key contribution of our work is a cost-free, scalable approach for creating a large-scale training dataset with a variety of foreground objects of differing semantic categories per image location. Quantitative and human-perception experiments with two diverse datasets demonstrate the advantage of our UFO search solution over related baselines."
Unconstrained Motion Deblurring for Dual-Lens Cameras,"M. R. Mahesh Mohan, Sharath Girish, A. N. Rajagopalan",Indian Institute of Technology Madras,100.0,"India, india",0.0,,"Recently, there has been a renewed interest in leveraging multiple cameras, but under unconstrained settings. They have been quite successfully deployed in smartphones, which have become de facto choice for many photographic applications. However, akin to normal cameras, the functionality of multi-camera systems can be marred by motion blur which is a ubiquitous phenomenon in hand-held cameras. Despite the far-reaching potential of unconstrained camera arrays, there is not a single deblurring method for such systems. In this paper, we propose a generalized blur model that elegantly explains the intrinsically coupled image formation model for dual-lens set-up, which are by far most predominant in smartphones. While image aesthetics is the main objective in normal camera deblurring, any method conceived for our problem is additionally tasked with ascertaining consistent scene-depth in the deblurred images. We reveal an intriguing challenge that stems from an inherent ambiguity unique to this problem which naturally disrupts this coherence. We address this issue by devising a judicious prior, and based on our model and prior propose a practical blind deblurring method for dual-lens cameras, that achieves state-of-the-art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Mohan_Unconstrained_Motion_Deblurring_for_Dual-Lens_Cameras_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Mohan_Unconstrained_Motion_Deblurring_for_Dual-Lens_Cameras_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010672/,"['Cameras', 'Image resolution', 'Smart phones', 'Estimation', 'Three-dimensional displays', 'Computer vision', 'Coherence']","['Motion Deblurring', 'Motion Blur', 'Camera Array', 'Deep Learning', 'Exposure Time', 'Focal Length', 'Clear Image', 'Point Spread Function', 'Light Field', 'Depth Estimation', 'Principal Plane', 'Camera Pose', 'Stereo Images', 'Blurred Images', 'Camera Motion', 'Scene Depth', 'Latent Image', 'Wide-angle Camera']",,1,"Recently, there has been a renewed interest in leveraging multiple cameras, but under unconstrained settings. They have been quite successfully deployed in smartphones, which have become de facto choice for many photographic applications. However, akin to normal cameras, the functionality of multi-camera systems can be marred by motion blur which is a ubiquitous phenomenon in hand-held cameras. Despite the far-reaching potential of unconstrained camera arrays, there is not a single deblurring method for such systems. In this paper, we propose a generalized blur model that elegantly explains the intrinsically coupled image formation model for dual-lens set-up, which are by far most predominant in smartphones. While image aesthetics is the main objective in normal camera deblurring, any method conceived for our problem is additionally tasked with ascertaining consistent scene-depth in the deblurred images. We reveal an intriguing challenge that stems from an inherent ambiguity unique to this problem which naturally disrupts this coherence. We address this issue by devising a judicious prior, and based on our model and prior propose a practical blind deblurring method for dual-lens cameras, that achieves state-of-the-art performance."
Understanding Deep Networks via Extremal Perturbations and Smooth Masks,"Ruth Fong, Mandela Patrick, Andrea Vedaldi",University of Oxford; Facebook AI Research,50.0,uk,50.0,USA,"Attribution is the problem of finding which parts of an image are the most responsible for the output of a deep neural network. An important family of attribution methods is based on measuring the effect of perturbations applied to the input image, either via exhaustive search or by finding representative perturbations via optimization. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute these extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable weighing factors from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the network under stimulation. We also extend perturbation analysis to the intermediate layers of a deep neural network. This application allows us to show how compactly an image can be represented (in terms of the number of channels it requires). We also demonstrate that the consistency with which images of a given class rely on the same intermediate channel correlates well with class accuracy.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010039/,"['Perturbation methods', 'Visualization', 'Biological neural networks', 'Computational modeling', 'Backpropagation', 'Optimization', 'Computer vision']","['Deep Network', 'Extreme Perturbations', 'Neural Network', 'Perturbation Theory', 'Intermediate Layer', 'Effects Of Perturbations', 'Part Of The Input', 'Attribution Methods', 'Area Constraints', 'Smoothing', 'Input Image', 'Implementation Details', 'Image Regions', 'Regularization Term', 'Important Channel', 'Saliency Map', 'Intermediate Activity', 'VGG-16 Network']",,172,"Attribution is the problem of finding which parts of an image are the most responsible for the output of a deep neural network. An important family of attribution methods is based on measuring the effect of perturbations applied to the input image, either via exhaustive search or by finding representative perturbations via optimization. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute these extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable weighing factors from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the network under stimulation. We also extend perturbation analysis to the intermediate layers of a deep neural network. This application allows us to show how compactly an image can be represented (in terms of the number of channels it requires). We also demonstrate that the consistency with which images of a given class rely on the same intermediate channel correlates well with class accuracy."
Understanding Generalized Whitening and Coloring Transform for Universal Style Transfer,Tai-Yin Chiu,University of Texas at Austin,100.0,usa,0.0,,"Style transfer is a task of rendering images in the styles of other images. In the past few years, neural style transfer has achieved a great success in this task, yet suffers from either the inability to generalize to unseen style images or fast style transfer. Recently, an universal style transfer technique that applies zero-phase component analysis (ZCA) for whitening and coloring image features realizes fast and arbitrary style transfer. However, using ZCA for style transfer is empirical and does not have any theoretical support. In addition, other whitening and coloring transforms (WCT) than ZCA have not been investigated. In this report, we generalize ZCA to the general form of WCT, provide an analytical performance analysis from the angle of neural style transfer, and show why ZCA is a good choice for style transfer among different WCTs and why some WCTs are not well applicable for style transfer.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010918/,"['Transforms', 'Feature extraction', 'Covariance matrices', 'Principal component analysis', 'Task analysis', 'Shape', 'Integrated circuits']","['Style Transfer', 'Universal Style Transfer', 'Image Features', 'Task Success', 'Style Image', 'Neural Network', 'Convolutional Neural Network', 'Covariance Matrix', 'Convolutional Layers', 'Feature Maps', 'Network Layer', 'Transformation Matrix', 'Random Vector', 'Orthogonal Matrix', 'Loss Of Content', 'Synthetic Images', 'Content Features', 'Image Texture', 'Cholesky Decomposition', 'General Appearance', 'Gram Matrix', 'Shape Matrix', 'VGG-19 Network', 'Style Features', 'Original Order', 'Higher Layers', 'Decoding', 'Tr Values', 'Linear Operator', 'Reverse Order']",,20,"Style transfer is a task of rendering images in the styles of other images. In the past few years, neural style transfer has achieved a great success in this task, yet suffers from either the inability to generalize to unseen style images or fast style transfer. Recently, an universal style transfer technique that applies zero-phase component analysis (ZCA) for whitening and coloring image features realizes fast and arbitrary style transfer. However, using ZCA for style transfer is empirical and does not have any theoretical support. In addition, other whitening and coloring transforms (WCT) than ZCA have not been investigated. In this report, we generalize ZCA to the general form of WCT, provide an analytical performance analysis from the angle of neural style transfer, and show why ZCA is a good choice for style transfer among different WCTs and why some WCTs are not well applicable for style transfer."
Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning,"Lifeng Fan, Wenguan Wang, Siyuan Huang, Xinyu Tang, Song-Chun Zhu","Inception Institute of Artiﬁcial Intelligence, UAE; University of Science and Technology of China, China; Center for Vision, Cognition, Learning and Autonomy, UCLA, USA",100.0,"china, uae, uk, usa",0.0,,"This paper addresses a new problem of understanding human gaze communication in social videos from both atomic-level and event-level, which is significant for studying human social interactions. To tackle this novel and challenging problem, we contribute a large-scale video dataset, VACATION, which covers diverse daily social scenes and gaze communication behaviors with complete annotations of objects and human faces, human attention, and communication structures and labels in both atomic-level and event-level. Together with VACATION, we propose a spatio-temporal graph neural network to explicitly represent the diverse gaze interactions in the social scenes and to infer atomic-level gaze communication by message passing. We further propose an event network with encoder-decoder structure to predict the event-level gaze communication. Our experiments demonstrate that the proposed model improves various baselines significantly in predicting the atomic-level and event-level gaze communications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.pdf,,https://github.com/LifengFan/Human-Gaze-Communication,,main,Poster,https://ieeexplore.ieee.org/document/9010935/,"['Videos', 'Cognition', 'Computer vision', 'Task analysis', 'Neural networks', 'Psychology']","['Graph Reasoning', 'Gaze Communication', 'Social Interaction', 'Human Faces', 'Graph Neural Networks', 'Video Dataset', 'Complete Annotation', 'Gaze Behavior', 'Encoder-decoder Structure', 'Human Attention', 'Social Scenes', 'Message Passing', 'Spatiotemporal Network', 'Human Behavior', 'Spatial Structure', 'Attention Mechanism', 'Bounding Box', 'Eye Contact', 'Spatial Domain', 'Graph Structure', 'Atom Labeling', 'Social Graph', 'Update Phase', 'Node Features', 'Mutual Gaze', 'Node Representations', 'Node Update', 'Joint Attention', 'Computer Vision Community', 'Gated Recurrent Unit']",,77,"This paper addresses a new problem of understanding human gaze communication in social videos from both atomic-level and event-level, which is significant for studying human social interactions. To tackle this novel and challenging problem, we contribute a large-scale video dataset, VACATION, which covers diverse daily social scenes and gaze communication behaviors with complete annotations of objects and human faces, human attention, and communication structures and labels in both atomic-level and event-level. Together with VACATION, we propose a spatio-temporal graph neural network to explicitly represent the diverse gaze interactions in the social scenes and to infer atomic-level gaze communication by message passing. We further propose an event network with encoder-decoder structure to predict the event-level gaze communication. Our experiments demonstrate that the proposed model improves various baselines significantly in predicting the atomic-level and event-level gaze communications."
Universal Adversarial Perturbation via Prior Driven Uncertainty Approximation,"Hong Liu, Rongrong Ji, Jie Li, Baochang Zhang, Yue Gao, Yongjian Wu, Feiyue Huang","Department of Artificial Intelligence, School of Informatics, Xiamen University; Tsinghua University; Peng Cheng Lab; Tencent Youtu Lab; Beihang University",80.0,"China, china",20.0,China,"Deep learning models have shown their vulnerabilities to universal adversarial perturbations (UAP), which are quasi-imperceptible. Compared to the conventional supervised UAPs that suffer from the knowledge of training data, the data-independent unsupervised UAPs are more applicable. Existing unsupervised methods fail to take advantage of the model uncertainty to produce robust perturbations. In this paper, we propose a new unsupervised universal adversarial perturbation method, termed as Prior Driven Uncertainty Approximation (PD-UA), to generate a robust UAP by fully exploiting the model uncertainty at each network layer. Specifically, a Monte Carlo sampling method is deployed to activate more neurons to increase the model uncertainty for a better adversarial perturbation. Thereafter, a textural bias prior to revealing a statistical uncertainty is proposed, which helps to improve the attacking performance. The UAP is crafted by the stochastic gradient descent algorithm with a boosted momentum optimizer, and a Laplacian pyramid frequency model is finally used to maintain the statistical uncertainty. Extensive experiments demonstrate that our method achieves well attacking performances on the ImageNet validation set, and significantly improves the fooling rate compared with the state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Universal_Adversarial_Perturbation_via_Prior_Driven_Uncertainty_Approximation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Universal_Adversarial_Perturbation_via_Prior_Driven_Uncertainty_Approximation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008259/,"['Uncertainty', 'Neurons', 'Perturbation methods', 'Computational modeling', 'Task analysis', 'Training data', 'Laplace equations']","['Adversarial Perturbations', 'Universal Adversarial Perturbations', 'Training Data', 'Model Uncertainty', 'Statistical Uncertainty', 'Monte Carlo Sampling Method', 'Attack Performance', 'Learning Rate', 'Convolutional Neural Network', 'Optimization Algorithm', 'Low-pass', 'Convolutional Layers', 'Deep Models', 'Transfer Learning', 'Deep Convolutional Neural Network', 'Convolutional Neural Network Model', 'Image Texture', 'Random Initialization', 'Low-frequency Signals', 'Bayesian Theory', 'Epistemic Uncertainty', 'Aleatoric Uncertainty', 'Adversarial Examples', 'Fast Gradient Sign Method', 'Random Gaussian', 'Low-frequency Part', 'Random Normal Distribution']",,51,"Deep learning models have shown their vulnerabilities to universal adversarial perturbations (UAP), which are quasi-imperceptible. Compared to the conventional supervised UAPs that suffer from the knowledge of training data, the data-independent unsupervised UAPs are more applicable. Existing unsupervised methods fail to take advantage of the model uncertainty to produce robust perturbations. In this paper, we propose a new unsupervised universal adversarial perturbation method, termed as Prior Driven Uncertainty Approximation (PD-UA), to generate a robust UAP by fully exploiting the model uncertainty at each network layer. Specifically, a Monte Carlo sampling method is deployed to activate more neurons to increase the model uncertainty for a better adversarial perturbation. Thereafter, a textural bias prior to revealing a statistical uncertainty is proposed, which helps to improve the attacking performance. The UAP is crafted by the stochastic gradient descent algorithm with a boosted momentum optimizer, and a Laplacian pyramid frequency model is finally used to maintain the statistical uncertainty. Extensive experiments demonstrate that our method achieves well attacking performances on the ImageNet validation set, and significantly improves the fooling rate compared with the state-of-the-art methods."
Universal Perturbation Attack Against Image Retrieval,"Jie Li, Rongrong Ji, Hong Liu, Xiaopeng Hong, Yue Gao, Qi Tian","Huawei Noah’s Ark Lab; Tsinghua University; Department of Artiﬁcial Intelligence, School of Informatics, Xiamen University, Peng Cheng Lab, Shenzhen, China; MOE Key Lab. for Intelligent Networks and Network Security/Faculty of Electronic and Information Engineering, Xi’an Jiaotong University, PRC, University of Oulu, Finland; Department of Artiﬁcial Intelligence, School of Informatics, Xiamen University",80.0,"China, china, finland",20.0,China,"Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classification models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the first attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a significant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Universal_Perturbation_Attack_Against_Image_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Universal_Perturbation_Attack_Against_Image_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010035/,"['Perturbation methods', 'Image retrieval', 'Computational modeling', 'Computer vision', 'Measurement', 'Search engines', 'Pipelines']","['Image Retrieval', 'Universal Perturbation', 'Perturbation Attacks', 'Corruption', 'Search Engine', 'Image Features', 'Image Classification', 'Input Size', 'Ranked List', 'Retrieval System', 'Neighborhood Relationship', 'Attack Methods', 'Adversarial Perturbations', 'Convolutional Neural Network', 'Computer Vision', 'Input Image', 'Image Size', 'Visual Features', 'Convolutional Neural Network Model', 'Adversarial Examples', 'Pairwise Relationships', 'Attack Performance', 'Deep Features', 'Drop Rate', 'Query Image', 'Real-world Systems', 'AlexNet', 'Triplet Loss', 'Reconstruction Of Datasets']",,60,"Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classification models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the first attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a significant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods."
Universal Semi-Supervised Semantic Segmentation,"Tarun Kalluri, Girish Varma, Manmohan Chandraker, C.V. Jawahar","University of California, San Diego; Center for Visual Information Technology, IIIT Hyderabad",100.0,"india, usa",0.0,,"In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kalluri_Universal_Semi-Supervised_Semantic_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kalluri_Universal_Semi-Supervised_Semantic_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010617/,"['Semantics', 'Entropy', 'Training', 'Task analysis', 'Data models', 'Image segmentation', 'Roads']","['Semantic Segmentation', 'Unlabeled Data', 'Domain Adaptation', 'Segmentation Dataset', 'Feature Alignment', 'Joint Training', 'Deployment Cost', 'Label Space', 'Semantic Segmentation Problem', 'Entropy Regularization', 'Transfer Learning', 'Intersection Over Union', 'Domain Shift', 'Segmentation Model', 'Latent Space', 'Target Domain', 'Entropy Loss', 'Source Model', 'Pixel Level', 'Joint Model', 'Universal Model', 'Unlabeled Examples', 'Street Scenes', 'Loss Term', 'Unlabeled Images', 'Source Domain', 'Semi-supervised Learning', 'Output Map', 'Limited Supervision', 'Decoder Module']",,69,"In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations."
Universally Slimmable Networks and Improved Training Techniques,"Jiahui Yu, Thomas S. Huang",University of Illinois at Urbana-Champaign,100.0,usa,0.0,,"Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Universally_Slimmable_Networks_and_Improved_Training_Techniques_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Universally_Slimmable_Networks_and_Improved_Training_Techniques_ICCV_2019_paper.pdf,,https://github.com/JiahuiYu/slimmable_networks,,main,Poster,https://ieeexplore.ieee.org/document/9009445/,"['Training', 'Neurons', 'Biological neural networks', 'Task analysis', 'Runtime', 'Testing']","['Neural Network', 'Deep Learning', 'Super-resolution', 'Batch Normalization', 'Batch Normalization Layer', 'Deep Reinforcement Learning', 'In-place', 'ImageNet Classification', 'Deep Neural Network', 'Single Layer', 'Residual Error', 'Single Network', 'Residual Network', 'Wider Network', 'Output Neurons', 'Training Iterations', 'Increase In Width', 'Self-driving', 'Feature Aggregation', 'Width Range', 'Group Of Channels', 'Single Neural Network', 'Entire Training Set', 'Input Neurons', 'Proximal Policy Optimization', 'Worse Accuracy', 'Memory Cost', 'Teacher Network']",,180,"Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: \url{https://github.com/JiahuiYu/slimmable_networks}."
Unpaired Image Captioning via Scene Graph Alignments,"Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, Gang Wang","Adobe Research, USA; Alibaba Group, China; Nanyang Technological University, Singapore",33.33333333333333,Singapore,66.66666666666667,USA,"Most of current image captioning models heavily rely on paired image-caption datasets. However, getting large scale image-caption paired data is labor-intensive and time-consuming. In this paper, we present a scene graph-based approach for unpaired image captioning. Our framework comprises an image scene graph generator, a sentence scene graph generator, a scene graph encoder, and a sentence decoder. Specifically, we first train the scene graph encoder and the sentence decoder on the text modality. To align the scene graphs between images and sentences, we propose an unsupervised feature alignment method that maps the scene graph features from the image to the sentence modality. Experimental results show that our proposed model can generate quite promising results without using any image-caption training pairs, outperforming existing methods by a wide margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010917/,"['Decoding', 'Generators', 'Training', 'Visualization', 'Detectors', 'Task analysis', 'Encoding']","['Image Captioning', 'Scene Graph', 'Paired Data', 'Scene Images', 'Paired Datasets', 'Feature Alignment', 'Text Modality', 'Convolutional Neural Network', 'Image Features', 'Imaging Modalities', 'Performance Metrics', 'Feature Space', 'Object Detection', 'Long Short-term Memory', 'Recurrent Neural Network', 'Cross-entropy Loss', 'Fully-connected Layer', 'Attention Module', 'Language Model', 'Word Embedding', 'Target Language', 'Image Descriptors', 'Adversarial Training', 'Previous Hidden State', 'Graph Convolution', 'Rich Semantic Information', 'Encoder-decoder Framework', 'Feature Maps', 'Attention Mechanism']",,96,"Most of current image captioning models heavily rely on paired image-caption datasets. However, getting large scale image-caption paired data is labor-intensive and time-consuming. In this paper, we present a scene graph-based approach for unpaired image captioning. Our framework comprises an image scene graph generator, a sentence scene graph generator, a scene graph encoder, and a sentence decoder. Specifically, we first train the scene graph encoder and the sentence decoder on the text modality. To align the scene graphs between images and sentences, we propose an unsupervised feature alignment method that maps the scene graph features from the image to the sentence modality. Experimental results show that our proposed model can generate quite promising results without using any image-caption training pairs, outperforming existing methods by a wide margin."
Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck,"Shuang Ma, Daniel McDuff, Yale Song","SUNY Buffalo, Buffalo, NY; Microsoft, Redmond, WA",0.0,,100.0,USA,"Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem ""skip-modal generation"" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Unpaired_Image-to-Speech_Synthesis_With_Multimodal_Information_Bottleneck_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Unpaired_Image-to-Speech_Synthesis_With_Multimodal_Information_Bottleneck_ICCV_2019_paper.pdf,,https://github.com/yunyikristy/skipNet,,main,Poster,https://ieeexplore.ieee.org/abstract/document/9009097/,"['Task analysis', 'Decoding', 'Data models', 'Training', 'Feeds', 'Computational modeling', 'Transforms']","['Information Bottleneck', 'Paired Data', 'Domain Gap', 'Direct Correspondence', 'Deep Generative Models', 'Multimodal Representation', 'Paired Samples', 'Berries', 'Attention Mechanism', 'Multiple Datasets', 'ImageNet', 'Latent Space', 'Human Faces', 'Memory Network', 'Reconstruction Loss', 'Unified Representation', 'Audio Data', 'Speech Samples', 'Unique Words', 'FC Layer', 'Word Error Rate', 'Text Modality', 'Multi-head Self-attention', 'Image Encoder', 'Cycle Consistency', 'External Memory']",,17,"Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem ``skip-modal generation'' because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks."
Unsupervised 3D Reconstruction Networks,"Geonho Cha, Minsik Lee, Songhwai Oh","Electrical and Computer Engineering, ASRI, Seoul National University, Korea; Division of Electrical Engineering, Hanyang University, Korea",100.0,south korea,0.0,,"In this paper, we propose 3D unsupervised reconstruction networks (3D-URN), which reconstruct the 3D structures of instances in a given object category from their 2D feature points under an orthographic camera model. 3D-URN consists of a 3D shape reconstructor and a rotation estimator, which are trained in a fully-unsupervised manner incorporating the proposed unsupervised loss functions. The role of the 3D shape reconstructor is to reconstruct the 3D shape of an instance from its 2D feature points, and the rotation estimator infers the camera pose. After training, 3D-URN can infer the 3D structure of an unseen instance in the same category, which is not possible in the conventional schemes of non-rigid structure from motion and structure from category. The experimental result shows the state-of-the-art performance, which demonstrates the effectiveness of the proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cha_Unsupervised_3D_Reconstruction_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cha_Unsupervised_3D_Reconstruction_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010628/,"['Three-dimensional displays', 'Shape', 'Two dimensional displays', 'Trajectory', 'Cameras', 'Training', 'Structure from motion']","['3D Reconstruction', 'Network Reconstruction', '3D Reconstruction Network', 'Unsupervised 3D Reconstruction', 'Loss Function', 'Feature Points', '3D Shape', 'Structure From Motion', 'Camera Pose', '2D Feature', 'Neural Network', 'Computer Vision', 'Rigid Body', 'Singular Value Decomposition', 'Fully-connected Layer', 'Projection Matrix', 'Human Faces', 'Frobenius Norm', 'Weight Estimation', 'Root Mean Square Error Of Cross-validation', 'Output Of Module', 'Orthogonality Constraint', 'Ground Truth 3D', 'Nuclear Norm', 'ReLU Activation Function', 'Noisy Input', 'Reconstruction Results', 'Object Trajectory', 'Estimation Network']",,9,"In this paper, we propose 3D unsupervised reconstruction networks (3D-URN), which reconstruct the 3D structures of instances in a given object category from their 2D feature points under an orthographic camera model. 3D-URN consists of a 3D shape reconstructor and a rotation estimator, which are trained in a fully-unsupervised manner incorporating the proposed unsupervised loss functions. The role of the 3D shape reconstructor is to reconstruct the 3D shape of an instance from its 2D feature points, and the rotation estimator infers the camera pose. After training, 3D-URN can infer the 3D structure of an unseen instance in the same category, which is not possible in the conventional schemes of non-rigid structure from motion and structure from category. The experimental result shows the state-of-the-art performance, which demonstrates the effectiveness of the proposed method."
Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM,"Lu Sheng, Dan Xu, Wanli Ouyang, Xiaogang Wang","CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong Kong; The University of Sydney, SenseTime Computer Vision Research Group, Australia; College of Software, Beihang University, China; University of Oxford, UK",100.0,"Hong Kong, australia, china, uk",0.0,,"In this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efficient camera relocalization and effective augmentation of visual odometry. To benefit from it, we first present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the first work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (i.e. KITTI raw dataset and its odometry split) clearly demonstrate the effectiveness of the proposed approach, and new state-of-the-art results are established on the unsupervised depth and pose estimation from monocular videos.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010691/,"['Visual odometry', 'Visualization', 'Simultaneous localization and mapping', 'Task analysis', 'Cameras', 'Machine learning', 'Optimization']","['Unsupervised Learning', 'Simultaneous Localization And Mapping', 'Visual Odometry', 'Pose Estimation', 'Joint Optimization', 'Depth Estimation', 'Joint Learning', 'Unsupervised Framework', 'Deep Learning', 'Deep Models', 'Target Image', 'Large Margin', 'Depth Map', 'Optical Flow', 'Global Average Pooling', 'Selection Task', 'Motion Estimation', 'Geometric Changes', 'Depth Prediction', 'Camera Motion', 'Cycle Consistency', 'Monocular Depth Estimation', 'Camera Pose', 'Warp Field', 'Geometric Cues', 'Target Frame', 'Online Update', 'Fully-connected Layer']",,18,"In this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efficient camera relocalization and effective augmentation of visual odometry. To benefit from it, we first present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the first work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (i.e. KITTI raw dataset and its odometry split [12]) clearly demonstrate the effectiveness of the proposed approach, and new state-ofthe-art results are established on the unsupervised depth and pose estimation from monocular video."
Unsupervised Deep Learning for Structured Shape Matching,"Jean-Michel Roufosse, Abhishek Sharma, Maks Ovsjanikov","LIX, Ecole Polytechnique",100.0,france,0.0,,"We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely unsupervised setting, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Roufosse_Unsupervised_Deep_Learning_for_Structured_Shape_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Roufosse_Unsupervised_Deep_Learning_for_Structured_Shape_Matching_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009544/,"['Shape', 'Optimization', 'Pipelines', 'Learning systems', 'Spectral analysis', 'Machine learning', 'Three-dimensional displays']","['Structural Properties', 'Supervised Learning', 'Unsupervised Learning', 'Unsupervised Methods', 'Nonlinear Transformation', '3D Shape', 'Isometry', 'Geodesic Distance', 'Neural Network', 'Training Set', 'Optimization Problem', 'Convolutional Neural Network', 'Structure Prediction', 'Learning-based Methods', 'Spectral Domain', 'Representation Of Function', 'Posterior Mode', 'Nearest Neighbor Search', 'Entire Map', 'Mapping Of Properties', 'Global Consistency', 'Ground Truth Map', 'Laplace-Beltrami Operator', 'Bilevel Optimization', 'Point-wise Multiplication', 'Heat Kernel', 'Preservation Of Products', 'Scalar Weights']",,72,"We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely \emph{unsupervised setting}, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing."
Unsupervised Domain Adaptation via Regularized Conditional Alignment,"Safa Cicek, Stefano Soatto","UCLA Vision Lab, University of California, Los Angeles, CA 90095",100.0,"uk, usa",0.0,,"We propose a method for unsupervised domain adaptation that trains a shared embedding to align the joint distributions of inputs (domain) and outputs (classes), making any classifier agnostic to the domain. Joint alignment ensures that not only the marginal distributions of the domains are aligned, but the labels as well. We propose a novel objective function that encourages the class-conditional distributions to have disjoint support in feature space. We further exploit adversarial regularization to improve the performance of the classifier on the domain for which no annotated data is available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cicek_Unsupervised_Domain_Adaptation_via_Regularized_Conditional_Alignment_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cicek_Unsupervised_Domain_Adaptation_via_Regularized_Conditional_Alignment_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009055/,"['Dogs', 'Training', 'Cats', 'Data models', 'Smoothing methods', 'Entropy', 'Neural networks']","['Domain Adaptation', 'Objective Function', 'Feature Space', 'Joint Distribution', 'Marginal Distribution', 'Input Distribution', 'Data Sources', 'Data Augmentation', 'Conditional Distribution', 'Class Labels', 'Class Prediction', 'Target Sample', 'Target Domain', 'Unlabeled Data', 'Target Data', 'Inference Time', 'Classification Loss', 'Source Characteristics', 'Semi-supervised Learning', 'Source Domain', 'Pseudo Labels', 'Samples Of The Same Class', 'Adversarial Domain Adaptation', 'Hypothesis Space', 'Standard Benchmark', 'Optimal Prediction', 'Alignment Loss', 'Finite Sample', 'Ground Truth Labels', 'Adversary Model']",,91,"We propose a method for unsupervised domain adaptation that trains a shared embedding to align the joint distributions of inputs (domain) and outputs (classes), making any classifier agnostic to the domain. Joint alignment ensures that not only the marginal distributions of the domains are aligned, but the labels as well. We propose a novel objective function that encourages the class-conditional distributions to have disjoint support in feature space. We further exploit adversarial regularization to improve the performance of the classifier on the domain for which no annotated data is available."
Unsupervised Graph Association for Person Re-Identification,"Jinlin Wu, Yang Yang, Hao Liu, Shengcai Liao, Zhen Lei, Stan Z. Li","Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE; CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China and University of Chinese Academy of Sciences, Beijing, China",100.0,"china, uae",0.0,,"In this paper, we propose an unsupervised graph association (UGA) framework to learn the underlying viewinvariant representations from the video pedestrian tracklets. The core points of UGA are mining the underlying cross-view associations and reducing the damage of noise associations. To this end, UGA is adopts a two-stage training strategy: (1) intra-camera learning stage and (2) intercamera learning stage. The former learns the intra-camera representation for each camera. While the latter builds a cross-view graph (CVG) to associate different cameras. By doing this, we can learn view-invariant representation for all person. Extensive experiments and ablation studies on seven re-id datasets demonstrate the superiority of the proposed UGA over most state-of-the-art unsupervised and domain adaptation re-id methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.pdf,,https://github.com/yichuan9527/Unsupervised-Graph-Association-for-Person-Re-identiﬁcation,,main,Poster,https://ieeexplore.ieee.org/document/9009485/,"['Cameras', 'Training', 'Noise measurement', 'Scalability', 'Graphics processing units', 'Micromechanical devices', 'Machine learning']","['Pedestrian', 'Unsupervised Methods', 'Learning Stage', 'Domain Adaptation', 'Positive Samples', 'Unsupervised Clustering', 'Stage Model', 'Target Domain', 'Unsupervised Algorithm', 'Recall Score', 'Batch Normalization Layer', 'Precision Score', 'Semi-supervised Learning', 'Source Domain', 'Pseudo Labels', 'Domain Adaptation Methods', 'Unsupervised Clustering Method', 'Unsupervised Clustering Algorithm', 'Large Batch Size', 'Mini Batch']",,79,"In this paper, we propose an unsupervised graph association (UGA) framework to learn the underlying viewinvariant representations from the video pedestrian tracklets. The core points of UGA are mining the underlying cross-view associations and reducing the damage of noise associations. To this end, UGA is adopts a two-stage training strategy: (1) intra-camera learning stage and (2) intercamera learning stage. The former learns the intra-camera representation for each camera. While the latter builds a cross-view graph (CVG) to associate different cameras. By doing this, we can learn view-invariant representation for all person. Extensive experiments and ablation studies on seven re-id datasets demonstrate the superiority of the proposed UGA over most state-of-the-art unsupervised and domain adaptation re-id methods."
Unsupervised High-Resolution Depth Learning From Videos With Dual Networks,"Junsheng Zhou, Yuwang Wang, Kaihuai Qin, Wenjun Zeng","Microsoft Research, Beijing, China; Tsinghua University, Beijing, China",50.0,China,50.0,USA,"Unsupervised depth learning takes the appearance difference between a target view and a view synthesized from its adjacent frame as supervisory signal. Since the supervisory signal only comes from images themselves, the resolution of training data significantly impacts the performance. High-resolution images contain more fine-grained details and provide more accurate supervisory signal. However, due to the limitation of memory and computation power, the original images are typically down-sampled during training, which suffers heavy loss of details and disparity accuracy. In order to fully explore the information contained in high-resolution data, we propose a simple yet effective dual networks architecture, which can directly take high-resolution images as input and generate high-resolution and high-accuracy depth map efficiently. We also propose a Self-assembled Attention (SA-Attention) module to handle low-texture region. The evaluation on the benchmark KITTI and Make3D datasets demonstrates that our method achieves state-of-the-art results in the monocular depth estimation task.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Unsupervised_High-Resolution_Depth_Learning_From_Videos_With_Dual_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Unsupervised_High-Resolution_Depth_Learning_From_Videos_With_Dual_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010979/,"['Image resolution', 'Signal resolution', 'Cameras', 'Training', 'Feature extraction', 'Training data', 'Network architecture']","['Unsupervised Learning', 'Dual Network', 'High-resolution Depth', 'Training Data', 'High-resolution Images', 'High-resolution Data', 'Depth Map', 'High-resolution Maps', 'Depth Estimation', 'Loss Of Details', 'KITTI Dataset', 'Fine-grained Details', 'Supervisory Signal', 'Target View', 'Similar Characteristics', 'Convolution', 'Single Image', 'Super-resolution', 'Global Features', 'RGB Images', 'Ground Truth Depth', 'Object Distance', 'Low-resolution Images', 'Homogeneous Coordinates', 'Markov Random Field', 'Smoothness Loss', 'Optical Flow', 'Nearby Objects', 'Long-range Dependencies', 'Random Cropping']",,44,"Unsupervised depth learning takes the appearance difference between a target view and a view synthesized from its adjacent frame as supervisory signal. Since the supervisory signal only comes from images themselves, the resolution of training data significantly impacts the performance. High-resolution images contain more fine-grained details and provide more accurate supervisory signal. However, due to the limitation of memory and computation power, the original images are typically down-sampled during training, which suffers heavy loss of details and disparity accuracy. In order to fully explore the information contained in high-resolution data, we propose a simple yet effective dual networks architecture, which can directly take high-resolution images as input and generate high-resolution and high-accuracy depth map efficiently. We also propose a Self-assembled Attention (SA-Attention) module to handle low-texture region. The evaluation on the benchmark KITTI and Make3D datasets demonstrates that our method achieves state-of-the-art results in the monocular depth estimation task."
Unsupervised Learning of Landmarks by Descriptor Vector Exchange,"James Thewlis, Samuel Albanie, Hakan Bilen, Andrea Vedaldi","University of Edinburgh; Unitary; VGG, University of Oxford",66.66666666666666,uk,33.33333333333334,USA,"Equivariance to random image transformations is an effective method to learn landmarks of object categories, such as the eyes and the nose in faces, without manual supervision. However, this method does not explicitly guarantee that the learned landmarks are consistent with changes between different instances of the same object, such as different facial identities. In this paper, we develop a new perspective on the equivariance approach by noting that dense landmark detectors can be interpreted as local image descriptors equipped with invariance to intra-category variations. We then propose a direct method to enforce such an invariance in the standard equivariant loss. We do so by exchanging descriptor vectors between images of different object instances prior to matching them geometrically. In this manner, the same vectors must work regardless of the specific object identity considered. We use this approach to learn vectors that can simultaneously be interpreted as local descriptors and dense landmarks, combining the advantages of both. Experiments on standard benchmarks show that this approach can match, and in some cases surpass state-of-the-art performance amongst existing methods that learn landmarks without supervision. Code is available at www.robots.ox.ac.uk/ vgg/research/DVE/.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.pdf,www.robots.ox.ac.uk/~vgg/research/DVE/,,,main,Poster,https://ieeexplore.ieee.org/document/9009523/,"['Detectors', 'Nose', 'Manuals', 'Computer vision', 'Unsupervised learning', 'Robustness', 'Feature extraction']","['Descriptor Vector', 'Equivalency', 'Local Descriptors', 'Image Descriptors', 'Image Transformation', 'Standard Benchmark', 'Object Instances', 'Landmark Detection', 'Local Features', 'Image Pixels', 'Image Registration', 'Image Object', 'Training Images', 'Image Pairs', 'Images In Set', 'Manual Annotation', 'Human Faces', 'Robotic Arm', 'Embedding Vectors', 'Target Dataset', 'Image X', 'Probabilistic Formulation', 'Intra-class Variance', 'Transit Use', 'Animal Faces', 'Geometric Consistency', 'Matching Probability', 'Geometric Transformation', 'Latent Space', 'Embedding Dimension']",,37,"Equivariance to random image transformations is an effective method to learn landmarks of object categories, such as the eyes and the nose in faces, without manual supervision. However, this method does not explicitly guarantee that the learned landmarks are consistent with changes between different instances of the same object, such as different facial identities. In this paper, we develop a new perspective on the equivariance approach by noting that dense landmark detectors can be interpreted as local image descriptors equipped with invariance to intra-category variations. We then propose a direct method to enforce such an invariance in the standard equivariant loss. We do so by exchanging descriptor vectors between images of different object instances prior to matching them geometrically. In this manner, the same vectors must work regardless of the specific object identity considered. We use this approach to learn vectors that can simultaneously be interpreted as local descriptors and dense landmarks, combining the advantages of both. Experiments on standard benchmarks show that this approach can match, and in some cases surpass state-of-the-art performance amongst existing methods that learn landmarks without supervision. Code is available at www.robots.ox.ac.uk/~vgg/research/DVE/."
Unsupervised Microvascular Image Segmentation Using an Active Contours Mimicking Neural Network,"Shir Gur,  Lior Wolf,  Lior Golgher,  Pablo Blinder","The School of Computer Science, Tel Aviv University; The School of Computer Science, Tel Aviv University; Facebook AI Research; School of Neurobiology, Biochemistry & Biophysics, Tel Aviv University; Sagol School of Neuroscience, Tel-Aviv University",80.0,"Israel, israel",20.0,USA,"The task of blood vessel segmentation in microscopy images is crucial for many diagnostic and research applications. However, vessels can look vastly different, depending on the transient imaging conditions, and collecting data for supervised training is laborious. We present a novel deep learning method for unsupervised segmentation of blood vessels. The method is inspired by the field of active contours and we introduce a new loss term, which is based on the morphological Active Contours Without Edges (ACWE) optimization method. The role of the morphological operators is played by novel pooling layers that are incorporated to the network's architecture. We demonstrate the challenges that are faced by previous supervised learning solutions, when the imaging conditions shift. Our unsupervised method is able to outperform such previous methods in both the labeled dataset, and when applied to similar but different datasets. Our code, as well as efficient pytorch reimplementations of the baseline methods VesselNN and DeepVess are attached as supplementary.",http://openaccess.thecvf.com/content_ICCV_2019/html/Gur_Unsupervised_Microvascular_Image_Segmentation_Using_an_Active_Contours_Mimicking_Neural_ICCV_2019_paper.html,,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gur_Unsupervised_Microvascular_Image_Segmentation_Using_an_Active_Contours_Mimicking_Neural_ICCV_2019_paper.pdf,,https://github.com/shirgur/UMIS,,,,https://ieeexplore.ieee.org/document/9008524/,"['Active contours', 'Image segmentation', 'Three-dimensional displays', 'Blood vessels', 'Biomedical imaging', 'Two dimensional displays']","['Neural Network', 'Active Contour', 'Blood Vessels', 'Unsupervised Methods', 'Imaging Conditions', 'Loss Term', 'Morphological Operations', 'Vessel Segmentation', 'Training Data', 'F1 Score', 'Unsupervised Learning', 'Intersection Over Union', 'Partial Differential Equations', 'Average Precision', 'Skip Connections', 'Types Of Layers', 'Reconstruction Loss', 'Morphological Methods', 'Retinal Blood Vessels', 'Evolution Curves', 'Active Contour Model', 'Common Segment']",,38,"The task of blood vessel segmentation in microscopy images is crucial for many diagnostic and research applications. However, vessels can look vastly different, depending on the transient imaging conditions, and collecting data for supervised training is laborious. We present a novel deep learning method for unsupervised segmentation of blood vessels. The method is inspired by the field of active contours and we introduce a new loss term, which is based on the morphological Active Contours Without Edges (ACWE) optimization method. The role of the morphological operators is played by novel pooling layers that are incorporated to the network's architecture. We demonstrate the challenges that are faced by previous supervised learning solutions, when the imaging conditions shift. Our unsupervised method is able to outperform such previous methods in both the labeled dataset, and when applied to similar but different datasets. Our code, as well as efficient pytorch reimplementations of the baseline methods VesselNN and DeepVess are attached as supplementary."
Unsupervised Multi-Task Feature Learning on Point Clouds,"Kaveh Hassani, Mike Haley","Autodesk AI Lab, San Francisco, USA; Autodesk AI Lab, Toronto, Canada",0.0,,100.0,USA,"We introduce an unsupervised multi-task model to jointly learn point and shape features on point clouds. We define three unsupervised tasks including clustering, reconstruction, and self-supervised classification to train a multi-scale graph-based encoder. We evaluate our model on shape classification and segmentation benchmarks. The results suggest that it outperforms prior state-of-the-art unsupervised models: In the ModelNet40 classification task, it achieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an mIoU of 68.2 and accuracy of 88.6%.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009542/,"['Task analysis', 'Three-dimensional displays', 'Shape', 'Decoding', 'Feature extraction', 'Aggregates', 'Convolution']","['Unsupervised Learning', 'Feature Learning', 'Point Cloud', 'Multi-task Learning', 'Unsupervised Feature', 'Unsupervised Feature Learning', 'Classification Task', 'Shape Features', 'Feature Points', 'Segmentation Task', 'Unsupervised Model', 'Learning Points', 'Shape Classification', 'Segmentation Benchmark', 'Training Set', 'Latent Variables', 'Learning Outcomes', 'Feature Space', 'Transfer Learning', 'Autoencoder', 'Self-supervised Learning', 'Graph Convolution', 'Cluster Assignment', 'Agglomerative Clustering', 'Variational Autoencoder', 'Input Point', 'Supervisory Signal', 'Part Segmentation', 'Ground Truth Labels', 'Reconstruction Task']",,120,"We introduce an unsupervised multi-task model to jointly learn point and shape features on point clouds. We define three unsupervised tasks including clustering, reconstruction, and self-supervised classification to train a multi-scale graph-based encoder. We evaluate our model on shape classification and segmentation benchmarks. The results suggest that it outperforms prior state-of-the-art unsupervised models: In the ModelNet40 classification task, it achieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an mIoU of 68.2 and accuracy of 88.6%."
Unsupervised Neural Quantization for Compressed-Domain Similarity Search,"Stanislav Morozov, Artem Babenko","Yandex, National Research University Higher School of Economics; Yandex, Lomonosov Moscow State University",100.0,"Russia, USA",0.0,,"We tackle the problem of unsupervised visual descriptors compression, which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines, the existing state-of-the-art compression methods employ shallow architectures, and we aim to close this gap by our paper. In more detail, we introduce a DNN architecture for the unsupervised compressed-domain retrieval, based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Morozov_Unsupervised_Neural_Quantization_for_Compressed-Domain_Similarity_Search_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Morozov_Unsupervised_Neural_Quantization_for_Compressed-Domain_Similarity_Search_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010943/,"['Quantization (signal)', 'Image coding', 'Computer architecture', 'Visualization', 'Computer vision', 'Encoding', 'Databases']","['Computer Vision', 'Lookup Table', 'Image Retrieval', 'Image Descriptors', 'Quantum', 'Discretion', 'Highest Accuracy', 'Quantification Method', 'Distance Function', 'Unsupervised Methods', 'Feed-forward Network', 'Discrete Variables', 'Vector Data', 'Learning Spaces', 'Nearest Neighbor Search', 'Variational Autoencoder', 'Codeword', 'Original Vector', 'Metric Learning', 'Triplet Loss', 'Original Data Space', 'Vector Quantization', 'Efficient Retrieval', 'Training Vectors', 'ReLU Activation Function', 'Retrieval Performance', 'Dot Product', 'Original Space', 'One-hot Vector', 'Hidden Variables']",,13,"We tackle the problem of unsupervised visual descriptors compression, which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines, the existing state-of-the-art compression methods employ shallow architectures, and we aim to close this gap by our paper. In more detail, we introduce a DNN architecture for the unsupervised compressed-domain retrieval, based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin."
Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy,"Qing Yu, Kiyoharu Aizawa","The University of Tokyo, Japan",100.0,Japan,0.0,,"Since deep learning models have been implemented in many commercial applications, it is important to detect out-of-distribution (OOD) inputs correctly to maintain the performance of the models, ensure the quality of the collected data, and prevent the applications from being used for other-than-intended purposes. In this work, we propose a two-head deep convolutional neural network (CNN) and maximize the discrepancy between the two classifiers to detect OOD inputs. We train a two-head CNN consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify in-distribution (ID) samples correctly. Unlike previous methods, we also utilize unlabeled data for unsupervised training and we use these unlabeled data to maximize the discrepancy between the decision boundaries of two classifiers to push OOD samples outside the manifold of the in-distribution (ID) samples, which enables us to detect OOD samples that are far from the support of the ID samples. Overall, our approach significantly outperforms other state-of-the-art methods on several OOD detection benchmarks and two cases of real-world simulation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Unsupervised_Out-of-Distribution_Detection_by_Maximum_Classifier_Discrepancy_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Unsupervised_Out-of-Distribution_Detection_by_Maximum_Classifier_Discrepancy_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009531,"['Training', 'Feature extraction', 'Neural networks', 'Data models', 'Entropy', 'Manifolds', 'Task analysis']","['Maximum Classifier Discrepancy', 'Benchmark', 'Neural Network', 'Convolutional Neural Network', 'Deep Neural Network', 'Deep Convolutional Neural Network', 'Commercial Applications', 'Unlabeled Data', 'Decision Boundary', 'Learning Rate', 'False Positive Rate', 'Training Procedure', 'Loss Of Diversity', 'Stochastic Gradient Descent', 'Neural Architecture', 'Confidence Score', 'False Negative Rate', 'Real-world Datasets', 'Fully-connected Layer', 'Sampling Step', 'Paired Datasets', 'Fine-tuning Step', 'Inference Time', 'Error Detection']",,83,"Since deep learning models have been implemented in many commercial applications, it is important to detect out-of-distribution (OOD) inputs correctly to maintain the performance of the models, ensure the quality of the collected data, and prevent the applications from being used for other-than-intended purposes. In this work, we propose a two-head deep convolutional neural network (CNN) and maximize the discrepancy between the two classifiers to detect OOD inputs. We train a two-head CNN consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify in-distribution (ID) samples correctly. Unlike previous methods, we also utilize unlabeled data for unsupervised training and we use these unlabeled data to maximize the discrepancy between the decision boundaries of two classifiers to push OOD samples outside the manifold of the in-distribution (ID) samples, which enables us to detect OOD samples that are far from the support of the ID samples. Overall, our approach significantly outperforms other state-of-the-art methods on several OOD detection benchmarks and two cases of real-world simulation."
Unsupervised Person Re-Identification by Camera-Aware Similarity Consistency Learning,"Ancong Wu, Wei-Shi Zheng, Jian-Huang Lai","School of Electronics and Information Technology, Sun Yat-sen University, China; Guangdong Province Key Laboratory of Information Security, China; School of Data and Computer Science, Sun Yat-sen University, China",66.66666666666666,"China, china",33.33333333333334,China,"For matching pedestrians across disjoint camera views in surveillance, person re-identification (Re-ID) has made great progress in supervised learning. However, it is infeasible to label data in a number of new scenes when extending a Re-ID system. Thus, studying unsupervised learning for Re-ID is important for saving labelling cost. Yet, cross-camera scene variation is a key challenge for unsupervised Re-ID, such as illumination, background and viewpoint variations, which cause domain shift in the feature space and result in inconsistent pairwise similarity distributions that degrade matching performance. To alleviate the effect of cross-camera scene variation, we propose a Camera-Aware Similarity Consistency Loss to learn consistent pairwise similarity distributions for intra-camera matching and cross-camera matching. To avoid learning ineffective knowledge in consistency learning, we preserve the prior common knowledge of intra-camera matching in the pretrained model as reliable guiding information, which does not suffer from cross-camera scene variation as cross-camera matching. To learn similarity consistency more effectively, we further develop a coarse-to-fine consistency learning scheme to learn consistency globally and locally in two steps. Experiments show that our method outperformed the state-of-the-art unsupervised Re-ID methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008563/,"['Cameras', 'Feature extraction', 'Reliability', 'Computational modeling', 'Labeling', 'Data models', 'Lighting']","['Unsupervised Person Re-identification', 'Supervised Learning', 'Feature Space', 'Social Cognition', 'Unsupervised Learning', 'Unsupervised Methods', 'Domain Shift', 'Pairwise Similarity', 'Variety Of Scenes', 'Pairwise Distribution', 'Number Of Scenes', 'Labeling Cost', 'Shift In Space', 'Viewpoint Variations', 'Paired Samples', 'Effect Of Parameters', 'Transfer Learning', 'Similarity Matrix', 'Model In Step', 'Reliable Knowledge', 'Unlabeled Data', 'Pair Of Cameras', 'Domain Adaptation', 'Unlabeled Target Data', 'Joint Learning', 'Positive Definite Matrix', 'Global Space', 'Feature Matrix', 'Trade-off Parameter']",,71,"For matching pedestrians across disjoint camera views in surveillance, person re-identification (Re-ID) has made great progress in supervised learning. However, it is infeasible to label data in a number of new scenes when extending a Re-ID system. Thus, studying unsupervised learning for Re-ID is important for saving labelling cost. Yet, cross-camera scene variation is a key challenge for unsupervised Re-ID, such as illumination, background and viewpoint variations, which cause domain shift in the feature space and result in inconsistent pairwise similarity distributions that degrade matching performance. To alleviate the effect of cross-camera scene variation, we propose a Camera-Aware Similarity Consistency Loss to learn consistent pairwise similarity distributions for intra-camera matching and cross-camera matching. To avoid learning ineffective knowledge in consistency learning, we preserve the prior common knowledge of intra-camera matching in the pretrained model as reliable guiding information, which does not suffer from cross-camera scene variation as cross-camera matching. To learn similarity consistency more effectively, we further develop a coarse-to-fine consistency learning scheme to learn consistency globally and locally in two steps. Experiments show that our method outperformed the state-of-the-art unsupervised Re-ID methods."
Unsupervised Pre-Training of Image Features on Non-Curated Data,"Mathilde Caron, Piotr Bojanowski, Julien Mairal, Armand Joulin","Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France; Facebook AI Research",50.0,France,50.0,USA,"Pre-training general-purpose visual features with convolutional neural networks without relying on annotations is a challenging and important task. Most recent efforts in unsupervised feature learning have focused on either small or highly curated datasets like ImageNet, whereas using uncurated raw datasets was found to decrease the feature quality when evaluated on a transfer task. Our goal is to bridge the performance gap between unsupervised methods trained on curated data, which are costly to obtain, and massive raw datasets that are easily available. To that effect, we propose a new unsupervised approach which leverages self-supervision and clustering to capture complementary statistics from large-scale data. We validate our approach on 96 million images from YFCC100M, achieving state-of-the-art results among unsupervised methods on standard benchmarks, which confirms the potential of unsupervised learning when only uncurated data are available. We also show that pre-training a supervised VGG-16 with our method achieves 74.9% top-1 classification accuracy on the validation set of ImageNet, which is an improvement of +0.8% over the same network trained from scratch. Our code is available at https://github.com/facebookresearch/DeeperCluster.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.pdf,,https://github.com/facebookresearch/DeeperCluster,,main,Oral,https://ieeexplore.ieee.org/document/9010376/,"['Task analysis', 'Training', 'Visualization', 'Data models', 'Optimization', 'Complexity theory', 'Standards']","['Unsupervised Pretraining', 'Unsupervised Learning', 'Visual Features', 'Large-scale Data', 'Unsupervised Methods', 'Standard Benchmark', 'Unsupervised Feature Learning', 'Training Set', 'Large Datasets', 'Convolutional Layers', 'Classification Task', 'Object Detection', 'Number Of Images', 'Multi-label', 'Stochastic Gradient Descent', 'Quality Characteristics', 'Clustering Approach', 'Cluster Assignment', 'Linear Classifier', 'Self-supervised Learning', 'Pretext Task', 'Image Clustering', 'Target Label', 'Self-supervised Task', 'VGG-16 Architecture', 'Self-supervised Approach', 'ImageNet Classification', 'Learning Rate', 'Visual Representation', 'Large Amount Of Data']",,135,"Pre-training general-purpose visual features with convolutional neural networks without relying on annotations is a challenging and important task. Most recent efforts in unsupervised feature learning have focused on either small or highly curated datasets like ImageNet, whereas using uncurated raw datasets was found to decrease the feature quality when evaluated on a transfer task. Our goal is to bridge the performance gap between unsupervised methods trained on curated data, which are costly to obtain, and massive raw datasets that are easily available. To that effect, we propose a new unsupervised approach which leverages self-supervision and clustering to capture complementary statistics from large-scale data. We validate our approach on 96 million images from YFCC100M, achieving state-of-the-art results among unsupervised methods on standard benchmarks, which confirms the potential of unsupervised learning when only uncurated data are available. We also show that pre-training a supervised VGG-16 with our method achieves 74.9% top-1 classification accuracy on the validation set of ImageNet, which is an improvement of +0.8% over the same network trained from scratch. Our code is available at https://github.com/facebookresearch/DeeperCluster."
Unsupervised Procedure Learning via Joint Dynamic Summarization,"Ehsan Elhamifar, Zwe Naing","Khoury College of Computer Sciences, Northeastern University",100.0,china,0.0,,"We address the problem of unsupervised procedure learning from unconstrained instructional videos. Our goal is to produce a summary of the procedure key-steps and their ordering needed to perform a given task, as well as localization of the key-steps in videos. We develop a collaborative sequential subset selection framework, where we build a dynamic model on videos by learning states and transitions between them, where states correspond to different subactivities, including background and procedure steps. To extract procedure key-steps, we develop an optimization framework that finds a sequence of a small number of states that well represents all videos and is compatible with the state transition model. Given that our proposed optimization is non-convex and NP-hard, we develop a fast greedy algorithm whose complexity is linear in the length of the videos and the number of states of the dynamic model, hence, scales to large datasets. Under appropriate conditions on the transition model, our proposed formulation is approximately submodular, hence, comes with performance guarantees. We also present ProceL, a new multimodal dataset of 47.3 hours of videos and their transcripts from diverse tasks, for procedure learning evaluation. By extensive experiments, we show that our framework significantly improves the state of the art performance.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Elhamifar_Unsupervised_Procedure_Learning_via_Joint_Dynamic_Summarization_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Elhamifar_Unsupervised_Procedure_Learning_via_Joint_Dynamic_Summarization_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009010/,"['Videos', 'Task analysis', 'Hidden Markov models', 'Visualization', 'Optimization', 'Feature extraction', 'Heuristic algorithms']","['Learning Procedure', 'Dynamic Model', 'Transition State', 'Unsupervised Learning', 'Video For Instructions', 'Fast Algorithm', 'Sequential Selection', 'Subset Selection', 'Video Length', 'Performance Guarantees', 'State Transition Model', 'Hours Of Video', 'Objective Function', 'Multiple Sequence Alignment', 'Data Visualization', 'State Of The Art', 'F1 Score', 'Representative Sequences', 'Hidden Markov Model', 'Subset Of States', 'State Transition Probability', 'Hidden State', 'Transition Probabilities', 'Sequence Of States', 'Assignment Of Sequences', 'Intersection Over Union', 'Dynamic Programming', 'YouTube', 'Alignment Results']",,31,"We address the problem of unsupervised procedure learning from unconstrained instructional videos. Our goal is to produce a summary of the procedure key-steps and their ordering needed to perform a given task, as well as localization of the key-steps in videos. We develop a collaborative sequential subset selection framework, where we build a dynamic model on videos by learning states and transitions between them, where states correspond to different subactivities, including background and procedure steps. To extract procedure key-steps, we develop an optimization framework that finds a sequence of a small number of states that well represents all videos and is compatible with the state transition model. Given that our proposed optimization is non-convex and NP-hard, we develop a fast greedy algorithm whose complexity is linear in the length of the videos and the number of states of the dynamic model, hence, scales to large datasets. Under appropriate conditions on the transition model, our proposed formulation is approximately submodular, hence, comes with performance guarantees. We also present ProceL, a new multimodal dataset of 47.3 hours of videos and their transcripts from diverse tasks, for procedure learning evaluation. By extensive experiments, we show that our framework significantly improves the state of the art performance."
Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis,"Patrick Esser, Johannes Haux, BjÃ¶rn Ommer","Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany",100.0,germany,0.0,,"Deep generative models come with the promise to learn an explainable representation for visual objects that allows image sampling, synthesis, and selective modification. The main challenge is to learn to properly model the independent latent characteristics of an object, especially its appearance and pose. We present a novel approach that learns disentangled representations of these characteristics and explains them individually. Training requires only pairs of images depicting the same object appearance, but no pose annotations. We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations together can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations. In experiments on diverse object categories, the approach successfully recombines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant improvements over state-of-the-art methods which utilize the same level of supervision, and reach performances comparable to those of pose-supervised approaches. However, we can handle the vast body of articulated object classes for which no pose models/annotations are available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Esser_Unsupervised_Robust_Disentangling_of_Latent_Characteristics_for_Image_Synthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Esser_Unsupervised_Robust_Disentangling_of_Latent_Characteristics_for_Image_Synthesis_ICCV_2019_paper.pdf,https://compvis.github.io/robust-disentangling,https://github.com/compvis/robust-disentangling,,main,Poster,https://ieeexplore.ieee.org/document/9009048/,"['Image reconstruction', 'Image generation', 'Training', 'Task analysis', 'Gallium nitride', 'Robustness', 'Computational modeling']","['Image Synthesis', 'Variable Approach', 'Deep Generative Models', 'Adversarial Approach', 'Disentangled Representation', 'Latent Variables', 'Probabilistic Model', 'Mutual Information', 'Lagrange Multiplier', 'Generative Adversarial Networks', 'Joint Distribution', 'Gameplay', 'Image Generation', 'Video Sequences', 'Latent Variable Model', 'Variational Autoencoder', 'Encoder Network', 'Unknown Distribution', 'Decoder Network', 'Variational Inference', 'Adversarial Attacks', 'Information Bottleneck', 'Mutual Information Estimation']",,17,"Deep generative models come with the promise to learn an explainable representation for visual objects that allows image sampling, synthesis, and selective modification. The main challenge is to learn to properly model the independent latent characteristics of an object, especially its appearance and pose. We present a novel approach that learns disentangled representations of these characteristics and explains them individually. Training requires only pairs of images depicting the same object appearance, but no pose annotations. We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations together can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations. In experiments on diverse object categories, the approach successfully recombines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant improvements over state-of-the-art methods which utilize the same level of supervision, and reach performances comparable to those of pose-supervised approaches. However, we can handle the vast body of articulated object classes for which no pose models/annotations are available."
Unsupervised Video Interpolation Using Cycle Consistency,"Fitsum A. Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, Guilin Liu, Kevin J. Shih, Andrew Tao, Jan Kautz, Bryan Catanzaro",Currently affiliated with Google; NVIDIA,0.0,,100.0,USA,"Learning to synthesize high frame rate videos via interpolation requires large quantities of high frame rate training videos, which, however, are scarce, especially at high resolutions. Here, we propose unsupervised techniques to synthesize high frame rate videos directly from low frame rate videos using cycle consistency. For a triplet of consecutive frames, we optimize models to minimize the discrepancy between the center frame and its cycle reconstruction, obtained by interpolating back from interpolated intermediate frames. This simple unsupervised constraint alone achieves results comparable with supervision using the ground truth intermediate frames. We further introduce a pseudo supervised loss term that enforces the interpolated frames to be consistent with predictions of a pre-trained interpolation model. The pseudo supervised loss term, used together with cycle consistency, can effectively adapt a pre-trained model to a new target domain. With no additional data and in a completely unsupervised fashion, our techniques significantly improve pre-trained models on new target domains, increasing PSNR values from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on the Sintel evaluation datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Reda_Unsupervised_Video_Interpolation_Using_Cycle_Consistency_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Reda_Unsupervised_Video_Interpolation_Using_Cycle_Consistency_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010052/,"['Interpolation', 'Streaming media', 'Mathematical model', 'Adaptation models', 'Predictive models', 'Training', 'Training data']","['Cycle Consistency', 'Frame Interpolation', 'High-resolution', 'Frame Rate', 'Target Domain', 'Consecutive Frames', 'Loss Term', 'Unsupervised Techniques', 'Training Videos', 'High Frame Rate', 'Interpolation Model', 'Simple Constraints', 'Intermediate Frames', 'Convolutional Neural Network', 'Generative Adversarial Networks', 'Video Frames', 'Optical Flow', 'Original Input', 'Visual Map', 'Consistent Time', 'Input Frames', 'Unsupervised Training', 'Domain Gap', 'Cycle Consistency Loss', 'Raw Video', 'Test Videos', 'Pair Of Frames', 'Gain In Accuracy', 'Temporal Consistency', 'Unsupervised Way']",,64,"Learning to synthesize high frame rate videos via interpolation requires large quantities of high frame rate training videos, which, however, are scarce, especially at high resolutions. Here, we propose unsupervised techniques to synthesize high frame rate videos directly from low frame rate videos using cycle consistency. For a triplet of consecutive frames, we optimize models to minimize the discrepancy between the center frame and its cycle reconstruction, obtained by interpolating back from interpolated intermediate frames. This simple unsupervised constraint alone achieves results comparable with supervision using the ground truth intermediate frames. We further introduce a pseudo supervised loss term that enforces the interpolated frames to be consistent with predictions of a pre-trained interpolation model. The pseudo supervised loss term, used together with cycle consistency, can effectively adapt a pre-trained model to a new target domain. With no additional data and in a completely unsupervised fashion, our techniques significantly improve pre-trained models on new target domains, increasing PSNR values from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on the Sintel evaluation datasets."
UprightNet: Geometry-Aware Camera Orientation Estimation From Single Images,"Wenqi Xian, Zhengqi Li, Matthew Fisher, Jonathan Eisenmann, Eli Shechtman, Noah Snavely","Adobe Research; Cornell Tech, Cornell University",50.0,usa,50.0,USA,"We introduce UprightNet, a learning-based approach for estimating 2DoF camera orientation from a single RGB image of an indoor scene. Unlike recent methods that leverage deep learning to perform black-box regression from image to orientation parameters, we propose an end-to-end framework that incorporates explicit geometric reasoning. In particular, we design a network that predicts two representations of scene geometry, in both the local camera and global reference coordinate systems, and solves for the camera orientation as the rotation that best aligns these two predictions via a differentiable least squares module. This network can be trained end-to-end, and can be supervised with both ground truth camera poses and intermediate representations of surface geometry. We evaluate UprightNet on the single-image camera orientation task on synthetic and real datasets, and show significant improvements over prior state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xian_UprightNet_Geometry-Aware_Camera_Orientation_Estimation_From_Single_Images_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xian_UprightNet_Geometry-Aware_Camera_Orientation_Estimation_From_Single_Images_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010973/,"['Cameras', 'Geometry', 'Three-dimensional displays', 'Estimation', 'Task analysis', 'Image segmentation', 'Machine learning']","['Single Image', 'Orientation Estimation', 'Camera Orientation', 'Camera Orientation Estimation', 'Coordinate System', 'Surface Geometry', 'Intermediate Representation', 'Camera Pose', 'Camera Coordinate System', 'Geometric Reasoning', 'Training Set', 'Unit Vector', 'Image Regions', 'Local Coordinate', 'Learning-based Methods', 'Ground Plane', 'Depth Camera', 'Global Surface', 'Alignment Score', 'Pitch Angle', 'Camera Coordinate', 'Surface Normals', 'Roll Angle', 'Least Squares Problem', 'Weight Map', 'Tangent Vector', 'Prior Methods', 'Alignment Problem', 'Global Coordinates', 'Error Metrics']",,30,"We introduce UprightNet, a learning-based approach for estimating 2DoF camera orientation from a single RGB image of an indoor scene. Unlike recent methods that leverage deep learning to perform black-box regression from image to orientation parameters, we propose an end-to-end framework that incorporates explicit geometric reasoning. In particular, we design a network that predicts two representations of scene geometry, in both the local camera and global reference coordinate systems, and solves for the camera orientation as the rotation that best aligns these two predictions via a differentiable least squares module. This network can be trained end-to-end, and can be supervised with both ground truth camera poses and intermediate representations of surface geometry. We evaluate UprightNet on the single-image camera orientation task on synthetic and real datasets, and show significant improvements over prior state-of-the-art approaches."
VTNFP: An Image-Based Virtual Try-On Network With Body and Clothing Feature Preservation,"Ruiyun Yu, Xiaoqi Wang, Xiaohui Xie","Department of computer science, University of California, Irvine, CA 92617; Software College, Northeastern University, China",100.0,"USA, china",0.0,,"Image-based virtual try-on systems with the goal of transferring a desired clothing item onto the corresponding region of a person have made great strides recently, but challenges remain in generating realistic looking images that preserve both body and clothing details. Here we present a new virtual try-on network, called VTNFP, to synthesize photo-realistic images given the images of a clothed person and a target clothing item. In order to better preserve clothing and body features, VTNFP follows a three-stage design strategy. First, it transforms the target clothing into a warped form compatible with the pose of the given person. Next, it predicts a body segmentation map of the person wearing the target clothing, delineating body parts as well as clothing regions. Finally, the warped clothing, body segmentation map and given person image are fused together for fine-scale image synthesis. A key innovation of VTNFP is the body segmentation map prediction module, which provides critical information to guide image synthesis in regions where body parts and clothing intersects, and is very beneficial for preventing blurry pictures and preserving clothing and body part details. Experiments on a fashion dataset demonstrate that VTNFP generates substantially better results than state-of-the-art methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008110/,"['Clothing', 'Image segmentation', 'Shape', 'Image generation', 'Three-dimensional displays', 'Semantics', 'Strain']","['Body Characteristics', 'Virtual Try-on', 'Virtual Try-on Network', 'Image-based Virtual Try-on', 'Body Parts', 'Segmentation Map', 'Image Synthesis', 'Realistic Images', 'Person Image', 'Clothing Items', 'Convolutional Neural Network', 'Convolutional Layers', 'Body Shape', 'Generative Adversarial Networks', 'Target Image', 'Image Generation', 'Final Image', 'Reference Image', 'Spatial Filter', 'Synthetic Images', 'Conditional Generative Adversarial Network', 'Final Synthesis', 'Representation Of A Person', 'Auxiliary Information', 'Number Of Filters', 'Lower Branch', 'Perceptual Loss', 'Decoder Layer', 'Semantic Map', 'Thin-plate Spline']",,109,"Image-based virtual try-on systems with the goal of transferring a desired clothing item onto the corresponding region of a person have made great strides recently, but challenges remain in generating realistic looking images that preserve both body and clothing details. Here we present a new virtual try-on network, called VTNFP, to synthesize photo-realistic images given the images of a clothed person and a target clothing item. In order to better preserve clothing and body features, VTNFP follows a three-stage design strategy. First, it transforms the target clothing into a warped form compatible with the pose of the given person. Next, it predicts a body segmentation map of the person wearing the target clothing, delineating body parts as well as clothing regions. Finally, the warped clothing, body segmentation map and given person image are fused together for fine-scale image synthesis. A key innovation of VTNFP is the body segmentation map prediction module, which provides critical information to guide image synthesis in regions where body parts and clothing intersects, and is very beneficial for preventing blurry pictures and preserving clothing and body part details. Experiments on a fashion dataset demonstrate that VTNFP generates substantially better results than state-of-the-art methods."
VV-Net: Voxel VAE Net With Group Convolutions for Point Cloud Segmentation,"Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, Dinesh Manocha","University of Maryland, College Park; School of Computer Science & Informatics, Cardiff University; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences",100.0,"china, uk, usa",0.0,,"We present a novel algorithm for point cloud segmentation.Our approach transforms unstructured point clouds into regular voxel grids, and further uses a kernel-based interpolated variational autoencoder (VAE) architecture to encode the local geometry within each voxel.Traditionally, the voxel representation only comprises Boolean occupancy information, which fails to capture the sparsely distributed points within voxels in a compact manner. In order to handle sparse distributions of points, we further employ radial basis functions (RBF) to compute a local, continuous representation within each voxel. Our approach results in a good volumetric representation that effectively tackles noisy point cloud datasets and is more robust for learning. Moreover, we further introduce group equivariant CNN to 3D, by defining the convolution operator on a symmetry group acting on  Z ^3 and its isomorphic sets. This improves the expressive capacity without increasing parameters, leading to more robust segmentation results.We highlight the performance on standard benchmarks and show that our approach outperforms state-of-the-art segmentation algorithms on the ShapeNet and S3DIS datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010292/,"['Three-dimensional displays', 'Feature extraction', 'Convolution', 'Robustness', 'Machine learning', 'Semantics', 'Task analysis']","['Point Cloud', 'Variational Autoencoder', 'Group Convolution', 'Point Cloud Segmentation', 'Radial Basis Function', 'Distribution Of Points', 'Symmetry Group', 'Voxel Grid', 'Point Cloud Dataset', 'Neural Network', 'Gaussian Kernel', 'Intersection Over Union', 'Latent Space', 'Semantic Segmentation', 'Euclidean Space', '3D Data', 'Inverse Function', 'Radial Basis Function Kernel', 'Part Segmentation', 'Latent Vector', 'Group Of Transformations', 'Mean Intersection Over Union', 'Label Probability', 'Point Cloud Processing', 'Spatial Distribution Of Points', 'Input Point Cloud', 'Center Of Rotation', 'Mirroring', 'Pre-defined Parameters', 'Graph Laplacian']",,158,"We present a novel algorithm for point cloud segmentation. Our approach transforms unstructured point clouds into regular voxel grids, and further uses a kernel-based interpolated variational autoencoder (VAE) architecture to encode the local geometry within each voxel. Traditionally, the voxel representation only comprises Boolean occupancy information which fails to capture the sparsely distributed points within voxels in a compact manner. In order to handle sparse distributions of points, we further employ radial basis functions (RBF) to compute a local, continuous representation within each voxel. Our approach results in a good volumetric representation that effectively tackles noisy point cloud datasets and is more robust for learning. Moreover, we further introduce group equivariant CNN to 3D, by defining the convolution operator on a symmetry group acting on Z3 and its isomorphic sets. This improves the expressive capacity without increasing parameters, leading to more robust segmentation results. We highlight the performance on standard benchmarks and show that our approach outperforms state-of-the-art segmentation algorithms on the ShapeNet and S3DIS datasets."
"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research","Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang","University of California, Santa Barbara, CA, USA; ByteDance AI Lab, Beijing, China",50.0,usa,50.0,China,"We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, \vatex is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on \vatex: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the \vatex dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using \vatex for other video-and-language research.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf,vatex.org,,,main,Oral,https://ieeexplore.ieee.org/document/9010676,"['Task analysis', 'Natural languages', 'Spatiotemporal phenomena', 'Motion pictures', 'Visualization', 'Bars', 'Social network services']","['Multilingual Dataset', 'Natural Language', 'Large-scale Datasets', 'Unified Model', 'Target Language', 'Machine Translation', 'Video Information', 'Description Language', 'Source Language', 'Spatiotemporal Context', 'Linguistic Complexity', 'Natural Language Descriptions', 'Video Captioning', 'Validation Set', 'Attention Mechanism', 'Video Clips', 'Multiple Languages', 'Word Embedding', 'Video Content', 'Video Encoding', 'Translation Of Information', 'Translation System', 'Video Features', 'Multilingual Learners', 'English Corpus', 'Temporal Attention', 'Target Sentence', 'Visual Question Answering', 'Decoding Step']",,221,"We present a new large-scale multilingual video description dataset, VATEX
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>
, which contains over 41,250 videos and 825, 000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSRVTT dataset [64], VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEXfor other video-and-language research."
Variable Rate Deep Image Compression With a Conditional Autoencoder,"Yoojin Choi, Mostafa El-Khamy, Jungwon Lee","SoC R&D, Samsung Semiconductor Inc., San Diego, CA 92121, USA",100.0,USA,0.0,,"In this paper, we propose a novel variable-rate learned image compression framework with a conditional autoencoder. Previous learning-based image compression methods mostly require training separate networks for different compression rates so they can yield compressed images of varying quality. In contrast, we train and deploy only one variable-rate image compression network implemented with a conditional autoencoder. We provide two rate control parameters, i.e., the Lagrange multiplier and the quantization bin size, which are given as conditioning variables to the network. Coarse rate adaptation to a target is performed by changing the Lagrange multiplier, while the rate can be further fine-tuned by adjusting the bin size used in quantizing the encoded representation. Our experimental results show that the proposed scheme provides a better rate-distortion trade-off than the traditional variable-rate image compression codecs such as JPEG2000 and BPG. Our model also shows comparable and sometimes better performance than the state-of-the-art learned image compression models that deploy multiple networks trained for varying rates.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Variable_Rate_Deep_Image_Compression_With_a_Conditional_Autoencoder_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Variable_Rate_Deep_Image_Compression_With_a_Conditional_Autoencoder_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008820/,"['Image coding', 'Quantization (signal)', 'Transform coding', 'Entropy', 'Training', 'Adaptation models', 'Codecs']","['Image Compression', 'Conditional Autoencoder', 'Histogram', 'Variety Of Conditions', 'Lagrange Multiplier', 'Adaptive Rate', 'Compression Rate', 'Neural Network', 'Deep Learning', 'Training Dataset', 'Recurrent Neural Network', 'Autoregressive Model', 'Peak Signal-to-noise Ratio', 'Target Rate', 'Approximate Entropy', 'Latent Representation', 'Lagrangian Method', 'Mean Square Error Loss', 'Discrete Cosine Transform', 'Uniform Noise', 'RNN-based Models', 'Arithmetic Coding', 'Entropy Coding']",,151,"In this paper, we propose a novel variable-rate learned image compression framework with a conditional autoencoder. Previous learning-based image compression methods mostly require training separate networks for different compression rates so they can yield compressed images of varying quality. In contrast, we train and deploy only one variable-rate image compression network implemented with a conditional autoencoder. We provide two rate control parameters, i.e., the Lagrange multiplier and the quantization bin size, which are given as conditioning variables to the network. Coarse rate adaptation to a target is performed by changing the Lagrange multiplier, while the rate can be further fine-tuned by adjusting the bin size used in quantizing the encoded representation. Our experimental results show that the proposed scheme provides a better rate-distortion trade-off than the traditional variable-rate image compression codecs such as JPEG2000 and BPG. Our model also shows comparable and sometimes better performance than the state-of-the-art learned image compression models that deploy multiple networks trained for varying rates."
Variational Adversarial Active Learning,"Samarth Sinha, Sayna Ebrahimi, Trevor Darrell",UC Berkeley; University of Toronto,100.0,"Canada, usa",0.0,,"Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on CIFAR10/100, Caltech-256, ImageNet, Cityscapes, and BDD100K. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.pdf,,https://github.com/sinhasam/vaal,,main,Oral,https://ieeexplore.ieee.org/document/9009538/,"['Uncertainty', 'Image segmentation', 'Semantics', 'Task analysis', 'Data models', 'Predictive models', 'Labeling']","['Active Learning', 'Generative Adversarial Networks', 'Dimensional Space', 'Image Classification', 'ImageNet', 'Adversarial Network', 'Latent Space', 'Semantic Segmentation', 'Low-dimensional Space', 'Variational Autoencoder', 'Minimax Game', 'Ablation', 'Training Set', 'High-dimensional', 'Objective Function', 'Random Sampling', 'Deep Neural Network', 'Classification Task', 'Feature Space', 'Image Segmentation', 'Unlabeled Data', 'Representation Learning', 'Image Classification Tasks', 'Noisy Labels', 'Active Learning Strategies', 'Conditional Entropy', 'Domain Adaptation', 'Ensemble Model', 'Segmentation Dataset', 'Decision Boundary']",,310,"Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on CIFAR10/100, Caltech-256, ImageNet, Cityscapes, and BDD100K. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at \url{https://github.com/sinhasam/vaal}."
Variational Few-Shot Learning,"Jian Zhang, Chenglong Zhao, Bingbing Ni, Minghao Xu, Xiaokang Yang","MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; Shanghai Jiao Tong University, Shanghai 200240, China",100.0,"China, china",0.0,,"We propose a variational Bayesian framework for enhancing few-shot learning performance. This idea is motivated by the fact that single point based metric learning approaches are inherently noise-vulnerable and easy-to-be-biased. In a nutshell, stochastic variational inference is invoked to approximate bias-eliminated class specific sample distributions. In the meantime, a classifier-free prediction is attained by leveraging the distribution statistics on novel samples. Extensive experimental results on several benchmarks well demonstrate the effectiveness of our distribution-driven few-shot learning framework over previous point estimates based methods, in terms of superior classification accuracy and robustness.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010263/,"['Measurement', 'Training', 'Task analysis', 'Feature extraction', 'Benchmark testing', 'Robustness', 'Prototypes']","['Few-shot Learning', 'Classification Accuracy', 'Point Estimates', 'Metric Learning', 'Variational Inference', 'Training Data', 'Variety Of Methods', 'Posterior Probability', 'Classification Task', 'Precise Estimates', 'Training Phase', 'Test Phase', 'Intersection Over Union', 'Kullback-Leibler', 'Class Distribution', 'Scarcity Of Data', 'Latent Space', 'Transformation Function', 'Microgrid', 'Support Set', 'Evidence Lower Bound', 'Segmentation Benchmark', 'Unseen Classes', 'Big Variation', 'Vector Of Variables', 'Segmentation Task', 'Few-shot Classification', 'Max-pooling Layer']",,84,"We propose a variational Bayesian framework for enhancing few-shot learning performance. This idea is motivated by the fact that single point based metric learning approaches are inherently noise-vulnerable and easy-to-be-biased. In a nutshell, stochastic variational inference is invoked to approximate bias-eliminated class specific sample distributions. In the meantime, a classifier-free prediction is attained by leveraging the distribution statistics on novel samples. Extensive experimental results on several benchmarks well demonstrate the effectiveness of our distribution-driven few-shot learning framework over previous point estimates based methods, in terms of superior classification accuracy and robustness."
Variational Uncalibrated Photometric Stereo Under General Lighting,"Bjoern Haefner, Zhenzhang Ye, Maolin Gao, Tao Wu, Yvain QuÃ©au, Daniel Cremers","Technical University of Munich; Artisense; GREYC, UMR CNRS 6072",33.33333333333333,germany,66.66666666666667,USA,"Photometric stereo (PS) techniques nowadays remain constrained to an ideal laboratory setup where modeling and calibration of lighting is amenable. To eliminate such restrictions, we propose an efficient principled variational approach to uncalibrated PS under general illumination. To this end, the Lambertian reflectance model is approximated through a spherical harmonic expansion, which preserves the spatial invariance of the lighting. The joint recovery of shape, reflectance and illumination is then formulated as a single variational problem. There the shape estimation is carried out directly in terms of the underlying perspective depth map, thus implicitly ensuring integrability and bypassing the need for a subsequent normal integration. To tackle the resulting nonconvex problem numerically, we undertake a two-phase procedure to initialize a balloon-like perspective depth map, followed by a ""lagged"" block coordinate descent scheme. The experiments validate efficiency and robustness of this approach. Across a variety of evaluations, we are able to reduce the mean angular error consistently by a factor of 2-3 compared to the state-of-the-art.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Haefner_Variational_Uncalibrated_Photometric_Stereo_Under_General_Lighting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Haefner_Variational_Uncalibrated_Photometric_Stereo_Under_General_Lighting_ICCV_2019_paper.pdf,,https://github.com/zhenzhangye/general_ups,,main,Poster,https://ieeexplore.ieee.org/document/9008998/,"['Lighting', 'Harmonic analysis', 'Shape', 'Estimation', 'Robustness', 'Calibration', 'Three-dimensional displays']","['Photometric Stereo', 'Uncalibrated Photometric Stereo', 'Non-convex', 'Depth Map', 'Variational Problem', 'Spherical Harmonics', 'Block Coordinate Descent', 'Spherical Harmonic Expansion', 'Inverse Problem', 'Image Formation', 'Viewing Angle', '3D Scanning', 'Visual Perspective', 'Real-world Experiments', 'Direct Light', 'Least Squares Problem', 'Armadillo', 'Joint Estimation', 'Robust Manner', 'Intrinsic Matrix', 'Surface Normals', 'Natural Illumination', 'Huber Loss', 'Spherical Approximation', 'Coordinate Descent Method']",,24,"Photometric stereo (PS) techniques nowadays remain constrained to an ideal laboratory setup where modeling and calibration of lighting is amenable. To eliminate such restrictions, we propose an efficient principled variational approach to uncalibrated PS under general illumination. To this end, the Lambertian reflectance model is approximated through a spherical harmonic expansion, which preserves the spatial invariance of the lighting. The joint recovery of shape, reflectance and illumination is then formulated as a single variational problem. There the shape estimation is carried out directly in terms of the underlying perspective depth map, thus implicitly ensuring integrability and bypassing the need for a subsequent normal integration. To tackle the resulting nonconvex problem numerically, we undertake a two-phase procedure to initialize a balloon-like perspective depth map, followed by a “lagged” block coordinate descent scheme. The experiments validate efficiency and robustness of this approach. Across a variety of evaluations, we are able to reduce the mean angular error consistently by a factor of 2-3 compared to the state-of-the-art."
Vehicle Re-Identification With Viewpoint-Aware Metric Learning,"Ruihang Chu, Yifan Sun, Yadong Li, Zheng Liu, Chi Zhang, Yichen Wei","School of Mechanical Engineering and Automation, Beihang University; Megvii Technology; Department of Electronic Engineering, Tsinghua University",66.66666666666666,"China, china",33.33333333333334,China,"This paper considers vehicle re-identification (re-ID) problem. The extreme viewpoint variation (up to 180 degrees) poses great challenges for existing approaches. Inspired by the behavior in human's recognition process, we propose a novel viewpoint-aware metric learning approach. It learns two metrics for similar viewpoints and different viewpoints in two feature spaces, respectively, giving rise to viewpoint-aware network (VANet). During training, two types of constraints are applied jointly. During inference, viewpoint is firstly estimated and the corresponding metric is used. Experimental results confirm that VANet significantly improves re-ID accuracy, especially when the pair is observed from different viewpoints. Our method establishes the new state-of-the-art on two benchmarks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Chu_Vehicle_Re-Identification_With_Viewpoint-Aware_Metric_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_Vehicle_Re-Identification_With_Viewpoint-Aware_Metric_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010988/,"['Training', 'Extraterrestrial measurements', 'Visualization', 'Surveillance', 'Cameras', 'Face recognition']","['Re-identification', 'Metric Learning', 'Vehicle Re-identification', 'Feature Space', 'Types Of Constraints', 'Similar Viewpoints', 'Viewpoint Variations', 'Red Cells', 'Deep Learning', 'Distance Matrix', 'Input Image', 'State Of The Art', 'Cross-entropy Loss', 'Face Recognition', 'Image Pairs', 'Intelligent Transportation', 'Matching Accuracy', 'Surveillance Cameras', 'Triplet Loss', 'Backbone Model', 'Deep Metric Learning', 'Distance In Feature Space', 'Gallery Images', 'Learning Baselines', 'Vehicle Images']",,133,"This paper considers vehicle re-identification (re-ID) problem. The extreme viewpoint variation (up to 180 degrees) poses great challenges for existing approaches. Inspired by the behavior in human's recognition process, we propose a novel viewpoint-aware metric learning approach. It learns two metrics for similar viewpoints and different viewpoints in two feature spaces, respectively, giving rise to viewpoint-aware network (VANet). During training, two types of constraints are applied jointly. During inference, viewpoint is firstly estimated and the corresponding metric is used. Experimental results confirm that VANet significantly improves re-ID accuracy, especially when the pair is observed from different viewpoints. Our method establishes the new state-of-the-art on two benchmarks."
Vehicle Re-Identification in Aerial Imagery: Dataset and Approach,"Peng Wang, Bingliang Jiao, Lu Yang, Yifei Yang, Shizhou Zhang, Wei Wei, Yanning Zhang","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, China",100.0,china,0.0,,"In this work, we construct a large-scale dataset for vehicle re-identification (ReID), which contains 137k images of 13k vehicle instances captured by UAV-mounted cameras. To our knowledge, it is the largest UAV-based vehicle ReID dataset. To increase intra-class variation, each vehicle is captured by at least two UAVs at different locations, with diverse view-angles and flight-altitudes. We manually label a variety of vehicle attributes, including vehicle type, color, skylight, bumper, spare tire and luggage rack. Furthermore, for each vehicle image, the annotator is also required to mark the discriminative parts that helps them to distinguish this particular vehicle from others. Besides the dataset, we also design a specific vehicle ReID algorithm to make full use of the rich annotation information. It is capable of explicitly detecting discriminative parts for each specific vehicle and significantly outperforming the evaluated baselines and state-of-the-art vehicle ReID approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Vehicle_Re-Identification_in_Aerial_Imagery_Dataset_and_Approach_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Vehicle_Re-Identification_in_Aerial_Imagery_Dataset_and_Approach_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010286/,"['Image color analysis', 'Cameras', 'Feature extraction', 'Task analysis', 'Image resolution', 'Tires', 'Visualization']","['Aerial Images', 'Vehicle Re-identification', 'Unmanned Aerial Vehicles', 'Vehicle Type', 'Specific Vehicle', 'Skylight', 'Vehicle Images', 'Discriminative Parts', 'Rich Annotations', 'Weight Matrix', 'Image Dataset', 'Object Detection', 'Bounding Box', 'Generative Adversarial Networks', 'Baseline Methods', 'Backbone Network', 'Classification Loss', 'Loss Of Identity', 'Feature Map Size', 'Query Set', 'Triplet Loss', 'Identity Classification', 'Categorical Attributes', 'Gallery Set', 'Multi-task Model', 'Unmanned Aerial Vehicle Flies', 'Individual Vehicles', 'Object Detection Task', 'Annotation Step', 'ImageNet']",,42,"In this work, we construct a large-scale dataset for vehicle re-identification (ReID), which contains 137k images of 13k vehicle instances captured by UAV-mounted cameras. To our knowledge, it is the largest UAV-based vehicle ReID dataset. To increase intra-class variation, each vehicle is captured by at least two UAVs at different locations, with diverse view-angles and flight-altitudes. We manually label a variety of vehicle attributes, including vehicle type, color, skylight, bumper, spare tire and luggage rack. Furthermore, for each vehicle image, the annotator is also required to mark the discriminative parts that helps them to distinguish this particular vehicle from others. Besides the dataset, we also design a specific vehicle ReID algorithm to make full use of the rich annotation information. It is capable of explicitly detecting discriminative parts for each specific vehicle and significantly outperforming the evaluated baselines and state-of-the-art vehicle ReID approaches."
Very Long Natural Scenery Image Prediction by Outpainting,"Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, Shuicheng Yan","SUSTech-UTS Joint Centre of CIS, Southern University of Science and Technology; Qihoo 360; ReLER, University of Technology Sydney; Yitu Technology",75.0,"australia, china",25.0,China,"Comparing to image inpainting, image outpainting receives less attention due to two challenges in it. The first challenge is how to keep the spatial and content consistency between generated images and original input. The second challenge is how to maintain high quality in generated results, especially for multi-step generations in which generated regions are spatially far away from the initial input. To solve the two problems, we devise some innovative modules, named Skip Horizontal Connection and Recurrent Content Transfer, and integrate them into our designed encoder-decoder structure. By this design, our network can generate highly realistic outpainting prediction effectively and efficiently. Other than that, our method can generate new images with very long sizes while keeping the same style and semantic content as the given input. To test the effectiveness of the proposed architecture, we collect a new scenery dataset with diverse, complicated natural scenes. The experimental results on this dataset have demonstrated the efficacy of our proposed network.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Very_Long_Natural_Scenery_Image_Prediction_by_Outpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Very_Long_Natural_Scenery_Image_Prediction_by_Outpainting_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010032/,"['Decoding', 'Semantics', 'Generators', 'Computer architecture', 'Gallium nitride', 'Generative adversarial networks', 'Painting']","['Original Input', 'Semantic Content', 'Image Inpainting', 'Convolutional Neural Network', 'Deep Network', 'Horizontal Plane', 'Convolutional Layers', 'Input Image', 'Feature Maps', 'Sudden Changes', 'Generative Adversarial Networks', 'Fully-connected Layer', 'Spatial Configuration', 'Region Of Origin', 'Local Loss', 'Latent Representation', 'Ground Truth Image', 'Reconstruction Loss', 'Horizontal Dimension', 'Input Regions', 'Multi-step Prediction', 'Transposed Convolution Layers', 'Contextual Attention', 'Residual Connection', 'Image Patches', 'Residual Block', 'Information Propagation', 'Riverbank', 'Receptive Field', 'Contralateral']",,58,"Comparing to image inpainting, image outpainting receives less attention due to two challenges in it. The first challenge is how to keep the spatial and content consistency between generated images and original input. The second challenge is how to maintain high quality in generated results, especially for multi-step generations in which generated regions are spatially far away from the initial input. To solve the two problems, we devise some innovative modules, named Skip Horizontal Connection and Recurrent Content Transfer, and integrate them into our designed encoder-decoder structure. By this design, our network can generate highly realistic outpainting prediction effectively and efficiently. Other than that, our method can generate new images with very long sizes while keeping the same style and semantic content as the given input. To test the effectiveness of the proposed architecture, we collect a new scenery dataset with diverse, complicated natural scenes. The experimental results on this dataset have demonstrated the efficacy of our proposed network."
ViCo: Word Embeddings From Visual Co-Occurrences,"Tanmay Gupta, Alexander Schwing, Derek Hoiem",University of Illinois at Urbana Champaign,100.0,usa,0.0,,"We propose to learn word embeddings from visual co-occurrences. Two words co-occur visually if both words apply to the same image or image region. Specifically, we extract four types of visual co-occurrences between object and attribute words from large-scale, textually-annotated visual databases like VisualGenome and ImageNet. We then train a multi-task log-bilinear model that compactly encodes word ""meanings"" represented by each co-occurrence type into a single visual word-vector. Through unsupervised clustering, supervised partitioning, and a zero-shot-like generalization analysis we show that our word embeddings complement text-only embeddings like GloVe by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone. We further evaluate our embeddings on five downstream applications, four of which are vision-language tasks. Augmenting GloVe with our embeddings yields gains on all tasks. We also find that random embeddings perform comparably to learned embeddings on all supervised vision-language tasks, contrary to conventional wisdom.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_ViCo_Word_Embeddings_From_Visual_Co-Occurrences_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_ViCo_Word_Embeddings_From_Visual_Co-Occurrences_ICCV_2019_paper.pdf,http://tanmaygupta.info/vico/,,,main,Poster,https://ieeexplore.ieee.org/document/9010420/,"['Visualization', 'Task analysis', 'Computational modeling', 'Context modeling', 'Transforms', 'Feature extraction', 'Vocabulary']","['Word Embedding', 'Image Regions', 'Text Data', 'Unsupervised Clustering', 'Downstream Applications', 'Embedding Learning', 'Visual Concepts', 'Source Of Information', 'Image Features', 'Linear Transformation', 'Latent Space', 'Random Vector', 'Embedding Dimension', 'Co-occurrence Matrix', 'Word Representations', 'Word Pairs', 'Compact Representation', 'Visual Classification', 'Naive Approach', 'WordNet', 'Concept Words', 'Continuous Bag-of-words', 'ImageNet Images', 'Co-occurrence Statistics', 'Context Vector', 'Vector Representations Of Words', 'Hypernym', 'Unseen Classes', 'Natural Scenes', 'AdaGrad']",,10,"We propose to learn word embeddings from visual co-occurrences. Two words co-occur visually if both words apply to the same image or image region. Specifically, we extract four types of visual co-occurrences between object and attribute words from large-scale, textually-annotated visual databases like VisualGenome and ImageNet. We then train a multi-task log-bilinear model that compactly encodes word “meanings” represented by each co-occurrence type into a single visual word-vector. Through unsupervised clustering, supervised partitioning, and a zero-shot-like generalization analysis we show that our word embeddings complement text-only embeddings like Glove by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone. We further evaluate our embeddings on five downstream applications, four of which are vision-language tasks. Augmenting Glove with our embeddings yields gains on all tasks. We also find that random embeddings perform comparably to learned embeddings on all supervised vision-language tasks, contrary to conventional wisdom."
ViSiL: Fine-Grained Spatio-Temporal Video Similarity Learning,"Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis Kompatsiaris","Information Technologies Institute, CERTH, Thessaloniki, Greece; Queen Mary University of London, Mile End road, E1 4NS London, UK",100.0,"Greece, uk",0.0,,"In this paper we introduce ViSiL, a Video Similarity Learning architecture that considers fine-grained Spatio-Temporal relations between pairs of videos -- such relations are typically lost in previous video retrieval approaches that embed the whole frame or even the whole video into a vector descriptor before the similarity estimation. By contrast, our Convolutional Neural Network (CNN)-based approach is trained to calculate video-to-video similarity from refined frame-to-frame similarity matrices, so as to consider both intra- and inter-frame relations. In the proposed method, pairwise frame similarity is estimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on regional CNN frame features - this avoids feature aggregation before the similarity calculation between frames. Subsequently, the similarity matrix between all video frames is fed to a four-layer CNN, and then summarized using Chamfer Similarity (CS) into a video-to-video similarity score -- this avoids feature aggregation before the similarity calculation between videos and captures the temporal similarity patterns between matching frame sequences. We train the proposed network using a triplet loss scheme and evaluate it on five public benchmark datasets on four different video retrieval problems where we demonstrate large improvements in comparison to the state of the art. The implementation of ViSiL is publicly available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kordopatis-Zilos_ViSiL_Fine-Grained_Spatio-Temporal_Video_Similarity_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kordopatis-Zilos_ViSiL_Fine-Grained_Spatio-Temporal_Video_Similarity_Learning_ICCV_2019_paper.pdf,,https://github.com/MKLab-ITI/visil,,main,Oral,https://ieeexplore.ieee.org/document/9008781/,"['Feature extraction', 'Tensile stress', 'Visualization', 'Training', 'Task analysis', 'Computer architecture', 'Kernel']","['Convolutional Neural Network', 'Similarity Score', 'Similarity Matrix', 'Video Frames', 'Pairwise Similarity', 'Similarity Calculation', 'Triplet Loss', 'Frame Features', 'Public Benchmark Datasets', 'Video Retrieval', 'Deep Learning', 'Training Dataset', 'Convolutional Layers', 'Feature Maps', 'Recurrent Neural Network', 'Attention Mechanism', 'Global Features', 'Network Output', 'Dynamic Programming', 'Dot Product', 'Spatiotemporal Representation', 'Deep Metric Learning', 'Pair Of Frames', 'Metric Learning', 'Items In Set', 'Convolutional Neural Network Architecture', 'Mean Average Precision', 'Irrelevant Ones', 'Temporal Alignment']",,45,"In this paper we introduce ViSiL, a Video Similarity Learning architecture that considers fine-grained Spatio-Temporal relations between pairs of videos -- such relations are typically lost in previous video retrieval approaches that embed the whole frame or even the whole video into a vector descriptor before the similarity estimation. By contrast, our Convolutional Neural Network (CNN)-based approach is trained to calculate video-to-video similarity from refined frame-to-frame similarity matrices, so as to consider both intra- and inter-frame relations. In the proposed method, pairwise frame similarity is estimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on regional CNN frame features - this avoids feature aggregation before the similarity calculation between frames. Subsequently, the similarity matrix between all video frames is fed to a four-layer CNN, and then summarized using Chamfer Similarity (CS) into a video-to-video similarity score - this avoids feature aggregation before the similarity calculation between videos and captures the temporal similarity patterns between matching frame sequences. We train the proposed network using a triplet loss scheme and evaluate it on five public benchmark datasets on four different video retrieval problems where we demonstrate large improvements in comparison to the state of the art. The implementation of ViSiL is publicly available."
Video Classification With Channel-Separated Convolutional Networks,"Du Tran, Heng Wang, Lorenzo Torresani, Matt Feiszli",Facebook AI,0.0,,100.0,USA,"Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks. This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture -- Channel-Separated Convolutional Network (CSN) -- which is simple, efficient, yet accurate. On Sports1M and Kinetics, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tran_Video_Classification_With_Channel-Separated_Convolutional_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tran_Video_Classification_With_Channel-Separated_Convolutional_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008828/,"['Convolution', 'Three-dimensional displays', 'Computer architecture', 'Two dimensional displays', 'Spatiotemporal phenomena', 'Standards', 'Computational efficiency']","['Convolutional Network', 'Video Analysis', 'Computational Cost', 'Factorization', 'Low Accuracy', 'Image Classification', '3D Network', '3D Convolution', '3D Convolutional Network', 'Computational Savings', 'Spatiotemporal Interactions', 'Role In The Accuracy', 'Group Convolution', 'Interaction Network', 'Experimental Section', 'Convolutional Layers', 'Local Interactions', 'Ablation Experiments', 'Input Channels', 'Output Channels', 'Depthwise Convolution', 'Floating-point Operations', 'Separate Channels', 'Separable Convolution', 'Conventional Convolution', 'Accuracy Drop', 'Convolutional Block', 'Simple Block', 'Temporal Jitter']",,352,"Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks. This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture -- Channel-Separated Convolutional Network (CSN) -- which is simple, efficient, yet accurate. On Sports1M and Kinetics, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient."
Video Compression With Rate-Distortion Autoencoders,"Amirhossein Habibian, Ties van Rozendaal, Jakub M. Tomczak, Taco S. Cohen","Qualcomm AI Research∗, Amsterdam, the Netherlands",100.0,Netherlands,0.0,,"In this paper we present a a deep generative model for lossy video compression. We employ a model that consists of a 3D autoencoder with a discrete latent space and an autoregressive prior used for entropy coding. Both autoencoder and prior are trained jointly to minimize a rate-distortion loss, which is closely related to the ELBO used in variational autoencoders. Despite its simplicity, we find that our method outperforms the state-of-the-art learned video compression networks based on motion compensation or interpolation. We systematically evaluate various design choices, such as the use of frame-based or spatio-temporal autoencoders, and the type of autoregressive prior. In addition, we present three extensions of the basic method that demonstrate the benefits over classical approaches to compression. First, we introduce semantic compression, where the model is trained to allocate more bits to objects of interest. Second, we study adaptive compression, where the model is adapted to a domain with limited variability, e.g. videos taken from an autonomous car, to achieve superior compression on that domain. Finally, we introduce multimodal compression, where we demonstrate the effectiveness of our model in joint compression of multiple modalities captured by non-standard imaging sensors, such as quad cameras. We believe that this opens up novel video compression applications, which have not been feasible with classical codecs.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Habibian_Video_Compression_With_Rate-Distortion_Autoencoders_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Habibian_Video_Compression_With_Rate-Distortion_Autoencoders_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008368/,"['Image coding', 'Video compression', 'Distortion', 'Rate distortion theory', 'Adaptation models', 'Rate-distortion', 'Three-dimensional displays']","['Multiple Modalities', 'Latent Space', 'Self-driving', 'Variational Autoencoder', 'Motion Compensation', 'Deep Generative Models', 'Video Compression', 'Video Learning', 'Training Set', 'Latent Variables', 'Recurrent Network', 'Autoregressive Model', 'Bitrate', 'Previous Frame', 'Least Significant Bit', 'Compression Method', 'Image Compression', '3D Convolution', 'Learning Image', 'Foreground Objects', 'Multimodal Model', 'Background Objects', 'Lossy Compression', 'Basis For Future Work', 'Latent Code', 'Arithmetic Coding', 'Lossless Compression', 'ReLU Nonlinearity', 'Restricted Set', 'Semantic Understanding']",,122,"In this paper we present a deep generative model for lossy video compression. We employ a model that consists of a 3D autoencoder with a discrete latent space and an autoregressive prior used for entropy coding. Both autoencoder and prior are trained jointly to minimize a ratedistortion loss, which is closely related to the ELBO used in variational autoencoders. Despite its simplicity, we find that our method outperforms the state-of-the-art learned video compression networks based on motion compensation or interpolation. We systematically evaluate various design choices, such as the use offrame-based or spatio-temporal autoencoders, and the type of autoregressive prior. In addition, we present three extensions of the basic method that demonstrate the benefits over classical approaches to compression. First, we introduce semantic compression, where the model is trained to allocate more bits to objects of interest. Second, we study adaptive compression, where the model is adapted to a domain with limited variability, e.g. videos taken from an autonomous car, to achieve superior compression on that domain. Finally, we introduce multimodal compression, where we demonstrate the effectiveness of our model in joint compression of multiple modalities captured by non-standard imaging sensors, such as quad cameras. We believe that this opens up novel video compression applications, which have not been feasible with classical codecs."
Video Face Clustering With Unknown Number of Clusters,"Makarand Tapaswi, Marc T. Law, Sanja Fidler","Inria, University of Toronto, Vector Institute, NVIDIA; University of Toronto, Vector Institute, NVIDIA",100.0,"Canada, France",0.0,,"Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded. To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tapaswi_Video_Face_Clustering_With_Unknown_Number_of_Clusters_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tapaswi_Video_Face_Clustering_With_Unknown_Number_of_Clusters_ICCV_2019_paper.pdf,,https://github.com/makarandtapaswi/BallClustering_ICCV2019,,main,Poster,https://ieeexplore.ieee.org/document/9010301/,"['Face', 'Streaming media', 'Clustering algorithms', 'Training', 'Euclidean distance', 'Target tracking']","['Face Clustering', 'Unknown Number Of Clusters', 'Latent Space', 'Stopping Criterion', 'Secondary Characteristics', 'Learning Literature', 'Ball Of Radius', 'Metric Learning', 'TV Series', 'Hierarchical Clustering', 'Performance Comparison', 'Computational Complexity', 'Validation Set', 'Protagonist', 'Clustering Algorithm', 'Clustering Method', 'Cross-entropy Loss', 'Single Cluster', 'Similar Samples', 'Contrastive Loss', 'Agglomerative Clustering Method', 'Triplet Loss', 'Agglomerative Clustering', 'Face Images', 'Validation Split', 'Secret Sharing', 'Embedding Dimension', 'Centroid']",,31,"Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded. To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task."
Video Instance Segmentation,"Linjie Yang, Yuchen Fan, Ning Xu",Adobe Research; ByteDance AI Lab; UIUC,33.33333333333333,usa,66.66666666666667,USA,"In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Video_Instance_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Video_Instance_Segmentation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008283/,"['Image segmentation', 'Task analysis', 'Motion segmentation', 'Semantics', 'Benchmark testing', 'Object detection', 'Object segmentation']","['Instance Segmentation', 'Video Instance', 'Benchmark', 'Computer Vision', 'Simultaneous Detection', 'Mask R-CNN', 'High-resolution Video', 'Validation Set', 'Object Detection', 'Large-scale Datasets', 'Bounding Box', 'Semantic Segmentation', 'Object Segmentation', 'Object Tracking', 'Category Labels', 'Region Proposal Network', 'Object Instances', 'Predicted Bounding Box', 'Bounding Box Regression', 'Object Bounding Boxes', 'Candidate Boxes', 'Instance Labels', 'External Memory', 'Intermediate Frames', 'Video Object', 'Detection Confidence', 'Largest Score', 'Binary Segmentation', 'Segmentation Dataset', 'Confidence Score']",,290,"In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding."
Video Object Segmentation Using Space-Time Memory Networks,"Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim",Adobe Research; Yonsei University,50.0,south korea,50.0,USA,"We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008790/,"['Object segmentation', 'Task analysis', 'Micromechanical devices', 'Feature extraction', 'Visualization', 'Tensile stress', 'Benchmark testing']","['Memory Network', 'Video Object Segmentation', 'Space-time Memory', 'Feature Space', 'Video Frames', 'Changes In Appearance', 'Information In Memory', 'Current Frame', 'External Memory', 'Past Frames', 'Training Data', 'Deep Network', 'Validation Set', 'Convolutional Layers', 'Feature Maps', 'Online Learning', 'Large Margin', 'Single Object', 'Target Object', 'Static Images', 'Previous Frame', 'Intermediate Frames', 'Memory Reading', 'Memory Encoding', 'Training Videos', 'Read Operation', 'Additional Training Data', '30th Percentile', 'Video Segments', 'Object Appearance']",,442,"We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set)."
VideoBERT: A Joint Model for Video and Language Representation Learning,"Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid",Google Research,0.0,,100.0,USA,"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009570/,"['Bit error rate', 'Visualization', 'Task analysis', 'Data models', 'Predictive models', 'Linguistics', 'Training']","['Representation Learning', 'Joint Model', 'Video Modeling', 'Language Representation Learning', 'Speech Recognition', 'High-level Features', 'Language Model', 'Self-supervised Learning', 'Vector Quantization', 'High-level Semantic Features', 'Sequence Of Tokens', 'BERT Model', 'High-level Semantics', 'Video Captioning', 'Contralateral', 'Random Variables', 'Model Formulation', 'Supervised Learning', 'Special Token', 'Tokenized', 'Image Captioning', 'Sequence Of Words', 'Linguistic Domains', 'Low-level Features', 'Video For Instructions', 'Postage', 'Adam Optimizer', 'Visual Domain']",,586,"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features."
"VideoMem: Constructing, Analyzing, Predicting Short-Term and Long-Term Video Memorability","Romain Cohendet, Claire-HÃ©lÃ¨ne Demarty, Ngoc Q. K. Duong, Martin Engilberge","Technicolor, Rennes, France; InterDigital, Rennes, France",0.0,,100.0,France,"Humans share a strong tendency to memorize/forget some of the visual information they encounter. This paper focuses on understanding the intrinsic memorability of visual content. To address this challenge, we introduce a large scale dataset (VideoMem) composed of 10,000 videos with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes and again 24-72 hours after memorization. Hence, the dataset comes with short-term and long-term memorability annotations. After an in-depth analysis of the dataset, we investigate various deep neural network-based models for the prediction of video memorability. Our best model using a ranking loss achieves a Spearman's rank correlation of 0.494 (respectively 0.256) for short-term (resp. long-term) memorability prediction, while our model with attention mechanism provides insights of what makes a content memorable. The VideoMem dataset with pre-extracted features is publicly available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Cohendet_VideoMem_Constructing_Analyzing_Predicting_Short-Term_and_Long-Term_Video_Memorability_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Cohendet_VideoMem_Constructing_Analyzing_Predicting_Short-Term_and_Long-Term_Video_Memorability_ICCV_2019_paper.pdf,https://www.technicolor.com/dream/research-innovation/video-memorability-dataset,,,main,Poster,https://ieeexplore.ieee.org/document/9008778/,"['Protocols', 'Predictive models', 'Atmospheric measurements', 'Particle measurements', 'Task analysis', 'Visualization', 'Time measurement']","['Spearman Correlation', 'Large Datasets', 'Rank Correlation', 'Memory Performance', 'Attention Mechanism', 'Large-scale Datasets', 'Ranking Loss', 'Image Memorability', 'Semantic', 'Response Time', 'Long-term Memory', 'Computer Vision', 'Short-term Memory', 'Adam Optimizer', 'Second Category', 'False Alarm Rate', 'Recognition Test', 'Measures Of Memory Function', 'Number Of Annotations', 'Main Object', 'Short-term Memory Performance', 'Fine-tuned Model', 'Image Captioning', 'Average Response Time', 'Video Annotation', 'Crowdsourcing', 'Choice Of Content']",,18,"Humans share a strong tendency to memorize/forget some of the visual information they encounter. This paper focuses on understanding the intrinsic memorability of visual content. To address this challenge, we introduce a large scale dataset (VideoMem) composed of 10,000 videos with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes and again 24-72 hours after memorization. Hence, the dataset comes with short-term and long-term memorability annotations. After an in-depth analysis of the dataset, we investigate various deep neural network-based models for the prediction of video memorability. Our best model using a ranking loss achieves a Spearman's rank correlation of 0.494 (respectively 0.256) for short-term (resp. long-term) memorability prediction, while our model with attention mechanism provides insights of what makes a content memorable. The VideoMem dataset with pre-extracted features is publicly available."
View Confusion Feature Learning for Person Re-Identification,"Fangyi Liu, Lei Zhang","School of Microelectronics and Communication Engineering, Chongqing University, Chongqing 400044, China",100.0,china,0.0,,"Person re-identification is an important task in video surveillance that aims to associate people across camera views at different locations and time. View variability is always a challenging problem seriously degrading person re-identification performance. Most of the existing methods either focus on how to learn view invariant feature or how to combine viewwise features. In this paper, we mainly focus on how to learn view-independent features by getting rid of view specific information through a view confusion learning mechanism. Specifically, we propose an end-to-end trainable framework, called View Confusion Feature Learning (VCFL), for person Re-ID across cameras. To the best of our knowledge, VCFL is originally proposed to learn view-independent identity-wise features, and it's a kind of combination of view-generic and view-specific methods. Furthermore, we extract sift-guided features by using bag-of-words model to help supervise the training of deep networks and enhance the view invariance of features. In experiments, our approach is validated on three benchmark datasets including CUHK01, CUHK03, and MARKET1501, which show the superiority of the proposed method over several state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_View_Confusion_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_View_Confusion_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010017/,"['Feature extraction', 'Cameras', 'Task analysis', 'Robustness', 'Training', 'Adaptation models', 'Measurement']","['Feature Learning', 'Specific Information', 'Network Training', 'Camera View', 'Video Surveillance', 'Deep Learning', 'Feature Maps', 'Bounding Box', 'Image Pairs', 'Generative Adversarial Networks', 'Discriminative Features', 'Deep Features', 'Handcrafted Features', 'Domain Adaptation', 'Common View', 'Scale-invariant Feature Transform', 'Central Loss', 'Triplet Loss', 'Specific View', 'Domain Adaptation Methods', 'SIFT Features', 'Cross-view', 'Ranking Problem', 'Discriminative Feature Learning', 'Re-identification Task', 'Feature Matching', 'Weight Of Parts', 'Similarity Measure', 'Transfer Learning']",,35,"Person re-identification is an important task in video surveillance that aims to associate people across camera views at different locations and time. View variability is always a challenging problem seriously degrading person re-identification performance. Most of the existing methods either focus on how to learn view invariant feature or how to combine viewwise features. In this paper, we mainly focus on how to learn view-independent features by getting rid of view specific information through a view confusion learning mechanism. Specifically, we propose an end-to-end trainable framework, called View Confusion Feature Learning (VCFL), for person Re-ID across cameras. To the best of our knowledge, VCFL is originally proposed to learn view-independent identity-wise features, and it’s a kind of combination of view-generic and view-specific methods. Furthermore, we extract sift-guided features by using bag-of-words model to help supervise the training of deep networks and enhance the view invariance of features. In experiments, our approach is validated on three benchmark datasets including CUHK01, CUHK03, and MARKET1501, which show the superiority of the proposed method over several state-of-the-art approaches."
View Independent Generative Adversarial Network for Novel View Synthesis,"Xiaogang Xu, Ying-Cong Chen, Jiaya Jia","The Chinese University of Hong Kong; The Chinese University of Hong Kong, Tencent YouTu Lab",100.0,"Hong Kong, china",0.0,,"Synthesizing novel views from a 2D image requires to infer 3D structure and project it back to 2D from a new viewpoint. In this paper, we propose an encoder-decoder based generative adversarial network VI-GAN to tackle this problem. Our method is to let the network, after seeing many images of objects belonging to the same category in different views, obtain essential knowledge of intrinsic properties of the objects. To this end, an encoder is designed to extract view-independent feature that characterizes intrinsic properties of the input image, which includes 3D structure, color, texture etc. We also make the decoder hallucinate the image of a novel view based on the extracted feature and an arbitrary user-specific camera pose. Extensive experiments demonstrate that our model can synthesize high-quality images in different views with continuous camera poses, and is general for various applications.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Xu_View_Independent_Generative_Adversarial_Network_for_Novel_View_Synthesis_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_View_Independent_Generative_Adversarial_Network_for_Novel_View_Synthesis_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9009096/,"['Three-dimensional displays', 'Cameras', 'Two dimensional displays', 'Feature extraction', 'Decoding', 'Solid modeling', 'Generative adversarial networks']","['Generative Adversarial Networks', 'View Synthesis', '3D Structure', 'Input Image', 'Hallucinations', '2D Images', 'Camera Pose', 'Convolutional Neural Network', 'Feature Learning', 'Real Samples', 'Extra Information', '3D Information', 'Loss Term', 'Latent Features', 'Single View', 'Object Rotation', 'Instance Normalization', 'Head Height', 'Structural Similarity Index Measure', 'Pose Information', 'Target View', 'Fr√©chet Inception Distance', 'Target Pose', '3D Landmarks', '3D Tasks', 'Accurate Pose', '3D Learning', '3D World', 'Generative Adversarial Networks Loss', 'Front Face']",,17,"Synthesizing novel views from a 2D image requires to infer 3D structure and project it back to 2D from a new viewpoint. In this paper, we propose an encoder-decoder based generative adversarial network VI-GAN to tackle this problem. Our method is to let the network, after seeing many images of objects belonging to the same category in different views, obtain essential knowledge of intrinsic properties of the objects. To this end, an encoder is designed to extract view-independent feature that characterizes intrinsic properties of the input image, which includes 3D structure, color, texture etc. We also make the decoder hallucinate the image of a novel view based on the extracted feature and an arbitrary user-specific camera pose. Extensive experiments demonstrate that our model can synthesize high-quality images in different views with continuous camera poses, and is general for various applications."
View N-Gram Network for 3D Object Retrieval,"Xinwei He, Tengteng Huang, Song Bai, Xiang Bai",Huazhong University of Science and Technology; University of Oxford,100.0,"china, uk",0.0,,"How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method.",,http://openaccess.thecvf.com/content_ICCV_2019/html/He_View_N-Gram_Network_for_3D_Object_Retrieval_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/He_View_N-Gram_Network_for_3D_Object_Retrieval_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009090/,"['Three-dimensional displays', 'Shape', 'Feature extraction', 'Visualization', 'Solid modeling', 'Aggregates', 'Computational modeling']","['3D Object Retrieval', 'Neural Network', 'Spatial Information', 'Recurrent Neural Network', '3D Shape', 'Convolutional Neural Network', 'Attention Mechanism', 'Point Cloud', 'Representation Learning', 'Language Model', 'Area Under Curve', 'Model-based Methods', 'Precision-recall Curve', '3D Representation', 'Compact Representation', 'Attention Scores', 'Metric Learning', 'Max-pooling Operation', 'Retrieval Performance', 'Discriminative Representations', 'Local Spatial Information', 'Metric Learning Methods', 'View Features', 'Consecutive Words', 'Recognition Stage', 'Rotation Invariance', 'Local Information', 'Shape Representation', 'Stochastic Gradient Descent', 'Dimensional Representation']",,41,"How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method."
View-Consistent 4D Light Field Superpixel Segmentation,"Numair Khan, Qian Zhang, Lucas Kasser, Henry Stone, Min H. Kim, James Tompkin",KAIST; Brown University,100.0,"south korea, usa",0.0,,"Many 4D light field processing applications rely on superpixel segmentations, for which occlusion-aware view consistency is important. Yet, existing methods often enforce consistency by propagating clusters from a central view only, which can lead to inconsistent superpixels for non-central views. Our proposed approach combines an occlusion-aware angular segmentation in horizontal and vertical EPI spaces with an occlusion-aware clustering and propagation step across all views. Qualitative video demonstrations show that this helps to remove flickering and inconsistent boundary shapes versus the state-of-the-art approach, and quantitative metrics reflect these findings with improved boundary accuracy and view consistency scores.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Khan_View-Consistent_4D_Light_Field_Superpixel_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Khan_View-Consistent_4D_Light_Field_Superpixel_Segmentation_ICCV_2019_paper.pdf,,https://github.com/brownvc/lightfieldsuperpixels,,main,Oral,https://ieeexplore.ieee.org/document/9008133/,"['Image edge detection', 'Image segmentation', 'Estimation', 'Shape', 'Two dimensional displays', 'Robustness']","['Light Field', 'Superpixel Segmentation', '4D Light Field', 'Horizontal Plane', 'Image Space', 'Self-similarity', 'Quantitative Metrics', 'Number Of Labels', 'Clustering Step', 'Central View', 'Small Region', 'Vertical Direction', 'Depth Estimation', 'Markov Random Field', 'Similar Error', 'Intersection Of Line', 'Central Rows', 'CIELAB Color Space', 'Set Of Views', 'Vertical Segments', 'Disparity Map', 'Bipartite Matching', 'Horizontal View', 'Disparity Estimation', 'Achievable Accuracy', 'Vertical View']",,14,"Many 4D light field processing applications rely on superpixel segmentations, for which occlusion-aware view consistency is important. Yet, existing methods often enforce consistency by propagating clusters from a central view only, which can lead to inconsistent superpixels for non-central views. Our proposed approach combines an occlusion-aware angular segmentation in horizontal and vertical EPI spaces with an occlusion-aware clustering and propagation step across all views. Qualitative video demonstrations show that this helps to remove flickering and inconsistent boundary shapes versus the state-of-the-art approach, and quantitative metrics reflect these findings with improved boundary accuracy and view consistency scores."
View-LSTM: Novel-View Video Synthesis Through View Decomposition,"Mohamed Ilyes Lakhal, Oswald Lanz, Andrea Cavallaro","TeV, Fondazione Bruno Kessler; CIS, Queen Mary University of London",100.0,"italy, uk",0.0,,"We tackle the problem of synthesizing a video of multiple moving people as seen from a novel view, given only an input video and depth information or human poses of the novel view as prior. This problem requires a model that learns to transform input features into target features while maintaining temporal consistency. To this end, we learn an invariant feature from the input video that is shared across all viewpoints of the same scene and a view-dependent feature obtained using the target priors. The proposed approach, View-LSTM, is a recurrent neural network structure that accounts for the temporal consistency and target feature approximation constraints. We validate View-LSTM by designing an end-to-end generator for novel-view video synthesis. Experiments on a large multi-view action recognition dataset validate the proposed model.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Lakhal_View-LSTM_Novel-View_Video_Synthesis_Through_View_Decomposition_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Lakhal_View-LSTM_Novel-View_Video_Synthesis_Through_View_Decomposition_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010330/,"['Three-dimensional displays', 'Spatiotemporal phenomena', 'Recurrent neural networks', 'Decoding', 'Computer architecture', 'Feature extraction', 'Task analysis']","['Video Synthesis', 'Recurrent Neural Network', 'Temporal Features', 'Target Features', 'Action Recognition', 'Invariant Features', 'Recurrent Structure', 'Recurrent Neural Network Structure', 'Feature Space', 'Feature Maps', 'Long Short-term Memory', 'Hidden State', 'Spatiotemporal Characteristics', 'Sequence Of Frames', 'Appearance Features', 'Longer Sequences', 'Perceptual Loss', 'Input Frames', 'Early Fusion', 'Maximum Mean Discrepancy', 'Target View', 'Invariant Representation', 'Perception Network', 'Spatiotemporal Representation', 'Temporal Learning', 'Late Fusion', 'Temporal Loss', 'ResNet Model', 'Input Features', 'Similarity Score']",,7,"We tackle the problem of synthesizing a video of multiple moving people as seen from a novel view, given only an input video and depth information or human poses of the novel view as prior. This problem requires a model that learns to transform input features into target features while maintaining temporal consistency. To this end, we learn an invariant feature from the input video that is shared across all viewpoints of the same scene and a view-dependent feature obtained using the target priors. The proposed approach, View-LSTM, is a recurrent neural network structure that accounts for the temporal consistency and target feature approximation constraints. We validate View-LSTM by designing an end-to-end generator for novel-view video synthesis. Experiments on a large multi-view action recognition dataset validate the proposed model."
Vision-Infused Deep Audio Inpainting,"Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, Xiaogang Wang","The University of Hong Kong; CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong",100.0,"Hong Kong, china",0.0,,"Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e., synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Vision-Infused_Deep_Audio_Inpainting_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Vision-Infused_Deep_Audio_Inpainting_ICCV_2019_paper.pdf,,https://github.com/Hangz-nju-cuhk/Vision-Infused-Audio-Inpainter-VIAI,,main,Poster,https://ieeexplore.ieee.org/document/9008233/,"['Spectrogram', 'Videos', 'Visualization', 'Image reconstruction', 'Decoding', 'Pipelines', 'Gallium nitride']","['Spectroscopic', 'Image Inpainting', 'Audio Segments', 'Convolutional Neural Network', 'Convolutional Layers', 'Visual Information', 'Autoregressive Model', 'Reasonable Results', 'Generative Adversarial Networks', 'Optical Flow', 'Skip Connections', 'Joint Space', 'Source Separation', 'Audio Data', 'Decay Parameter', 'Missing Parts', 'Musical Notation', 'Audio Input', 'Mean Opinion Score', 'Missing Areas']",,56,"Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e. synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset [51]. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://github.com/Hangz-nju-cuhk/ Vision-Infused-Audio-Inpainter-VIAI."
Visual Deprojection: Probabilistic Recovery of Collapsed Dimensions,"Guha Balakrishnan, Adrian V. Dalca, Amy Zhao, John V. Guttag, FrÃ©do Durand, William T. Freeman",MIT; MIT and MGH,100.0,usa,0.0,,"We introduce visual deprojection: the task of recovering an image or video that has been collapsed along a dimension. Projections arise in various contexts, such as long-exposure photography, where a dynamic scene is collapsed in time to produce a motion-blurred image, and corner cameras, where reflected light from a scene is collapsed along a spatial dimension because of an edge occluder to yield a 1D video. Deprojection is ill-posed-- often there are many plausible solutions for a given input. We first propose a probabilistic model capturing the ambiguity of the task. We then present a variational inference strategy using convolutional neural networks as functional approximators. Sampling from the inference network at test time yields plausible candidates from the distribution of original signals that are consistent with a given input projection. We evaluate the method on several datasets for both spatial and temporal deprojection tasks. We first demonstrate the method can recover human gait videos and face images from spatial projections, and then show that it can recover videos of moving digits from dramatically motion-blurred images obtained via temporal projection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Balakrishnan_Visual_Deprojection_Probabilistic_Recovery_of_Collapsed_Dimensions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Balakrishnan_Visual_Deprojection_Probabilistic_Recovery_of_Collapsed_Dimensions_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008562/,"['Task analysis', 'Convolution', 'Two dimensional displays', 'Probabilistic logic', 'Three-dimensional displays', 'Face', 'Computer vision']","['Convolutional Neural Network', 'Probabilistic Model', 'Spatial Dimensions', 'Signal Distribution', 'Face Images', 'Human Faces', 'Mapping Project', 'Network Inference', 'Variational Inference', 'Mean Square Error', 'General Method', 'Data Augmentation', 'Angular Position', 'Variational Autoencoder', 'Image Synthesis', 'Light Rays', 'Video Dataset', 'Motion Blur', 'Vertical Projection', 'Evidence Lower Bound', 'Conditional Variational Autoencoder']",,4,"We introduce visual deprojection: the task of recovering an image or video that has been collapsed along a dimension. Projections arise in various contexts, such as long-exposure photography, where a dynamic scene is collapsed in time to produce a motion-blurred image, and corner cameras, where reflected light from a scene is collapsed along a spatial dimension because of an edge occluder to yield a 1D video. Deprojection is ill-posed-- often there are many plausible solutions for a given input. We first propose a probabilistic model capturing the ambiguity of the task. We then present a variational inference strategy using convolutional neural networks as functional approximators. Sampling from the inference network at test time yields plausible candidates from the distribution of original signals that are consistent with a given input projection. We evaluate the method on several datasets for both spatial and temporal deprojection tasks. We first demonstrate the method can recover human gait videos and face images from spatial projections, and then show that it can recover videos of moving digits from dramatically motion-blurred images obtained via temporal projection."
Visual Semantic Reasoning for Image-Text Matching,"Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, Yun Fu","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA; Khoury College of Computer Science, Northeastern University, Boston, MA",100.0,china,0.0,,"Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose a simple and interpretable reasoning model to generate visual representation that captures key objects and semantic concepts of a scene. Specifically, we first build up connections between image regions and perform reasoning with Graph Convolutional Networks to generate features with semantic relationships. Then, we propose to use the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually generate the representation for the whole scene. Experiments validate that our method achieves a new state-of-the-art for the image-text matching on MS-COCO and Flickr30K datasets. It outperforms the current best method by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Recall@1).",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010696/,"['Semantics', 'Cognition', 'Visualization', 'Image representation', 'Feature extraction', 'Correlation', 'Image edge detection']","['Visual Reasoning', 'Semantic Reasoning', 'Image-text Matching', 'Visual Semantic Reasoning', 'Visual Representation', 'Image Regions', 'Semantic Similarity', 'Image Representation', 'Semantic Knowledge', 'Graph Convolutional Network', 'Image Retrieval', 'Caption Text', 'Language Areas', 'MS COCO Dataset', 'Convolutional Neural Network', 'Regional Characteristics', 'Attention Mechanism', 'Global Information', 'Latent Space', 'Textual Descriptions', 'Final Representation', 'Bottom-up Attention', 'Salient Regions', 'Query Image', 'Graph Topology', 'Text Query', 'Text Generation', 'Human Visual System', 'Element-wise Multiplication', 'Update Gate']",,347,"Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose a simple and interpretable reasoning model to generate visual representation that captures key objects and semantic concepts of a scene. Specifically, we first build up connections between image regions and perform reasoning with Graph Convolutional Networks to generate features with semantic relationships. Then, we propose to use the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually generate the representation for the whole scene. Experiments validate that our method achieves a new state-of-the-art for the image-text matching on MS-COCO and Flickr30K datasets. It outperforms the current best method by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Recall@1)."
Visualization of Convolutional Neural Networks for Monocular Depth Estimation,"Junjie Hu, Yan Zhang, Takayuki Okatani","Center for Advanced Intelligence Project, RIKEN, Japan; Graduate School of Information Sciences, Tohoku University, Japan",100.0,japan,0.0,,"Recently, convolutional neural networks (CNNs) have shown great success on the task of monocular depth estimation. A fundamental yet unanswered question is: how CNNs can infer depth from a single image. Toward answering this question, we consider visualization of inference of a CNN by identifying relevant pixels of an input image to depth estimation. We formulate it as an optimization problem of identifying the smallest number of image pixels from which the CNN can estimate a depth map with the minimum difference from the estimate from the entire image. To cope with a difficulty with optimization through a deep CNN, we propose to use another network to predict those relevant image pixels in a forward computation. In our experiments, we first show the effectiveness of this approach, and then apply it to different depth estimation networks on indoor and outdoor scene datasets. The results provide several findings that help exploration of the above question.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Visualization_of_Convolutional_Neural_Networks_for_Monocular_Depth_Estimation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Visualization_of_Convolutional_Neural_Networks_for_Monocular_Depth_Estimation_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009581/,"['Visualization', 'Estimation', 'Optimization', 'Task analysis', 'Training', 'Object recognition', 'Convolutional neural networks']","['Convolutional Neural Network', 'Depth Estimation', 'Monocular Depth Estimation', 'Input Image', 'Image Pixels', 'Single Image', 'Deep Convolutional Neural Network', 'Entire Image', 'Depth Map', 'Outdoor Scenes', 'Forward Calculation', 'Values In The Range', 'Object Recognition', 'Natural Images', 'Continuous Values', 'Image Edge', 'Human Vision', 'Class Activation Maps', 'Adversarial Examples', 'KITTI Dataset', 'Strong Edges', 'Non-zero Pixels', 'Monocular Images', 'Edge Strength', 'Scene Depth', 'Scene Point']",,54,"Recently, convolutional neural networks (CNNs) have shown great success on the task of monocular depth estimation. A fundamental yet unanswered question is: how CNNs can infer depth from a single image. Toward answering this question, we consider visualization of inference of a CNN by identifying relevant pixels of an input image to depth estimation. We formulate it as an optimization problem of identifying the smallest number of image pixels from which the CNN can estimate a depth map with the minimum difference from the estimate from the entire image. To cope with a difficulty with optimization through a deep CNN, we propose to use another network to predict those relevant image pixels in a forward computation. In our experiments, we first show the effectiveness of this approach, and then apply it to different depth estimation networks on indoor and outdoor scene datasets. The results provide several findings that help exploration of the above question."
Visualizing the Invisible: Occluded Vehicle Segmentation and Recovery,"Xiaosheng Yan, Feigege Wang, Wenxi Liu, Yuanlong Yu, Shengfeng He, Jia Pan","Department of Computer Science, The University of Hong Kong; School of Computer Science and Engineering, South China University of Technology; College of Mathematics and Computer Science, Fuzhou University",100.0,"China, Hong Kong, china",0.0,,"In this paper, we propose a novel iterative multi-task framework to complete the segmentation mask of an occluded vehicle and recover the appearance of its invisible parts. In particular, firstly, to improve the quality of the segmentation completion, we present two coupled discriminators that introduce an auxiliary 3D model pool for sampling authentic silhouettes as adversarial samples. In addition, we propose a two-path structure with a shared network to enhance the appearance recovery capability. By iteratively performing the segmentation completion and the appearance recovery, the results will be progressively refined. To evaluate our method, we present a dataset, Occluded Vehicle dataset, containing synthetic and real-world occluded vehicle images. Based on this dataset, we conduct comparison experiments and demonstrate that our model outperforms the state-of-the-arts in both tasks of recovering segmentation mask and appearance for occluded vehicles. Moreover, we also demonstrate that our appearance recovery approach can benefit the occluded vehicle tracking in real-world videos.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Visualizing_the_Invisible_Occluded_Vehicle_Segmentation_and_Recovery_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Visualizing_the_Invisible_Occluded_Vehicle_Segmentation_and_Recovery_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008247/,"['Image segmentation', 'Solid modeling', 'Three-dimensional displays', 'Task analysis', 'Gallium nitride', 'Videos', 'Training']","['Vehicle Segment', 'Occluded Vehicles', 'Synthetic Images', 'Recovery Model', 'Vehicle Track', 'Multi-task Framework', 'Iterative Framework', 'Auxiliary Model', 'Vehicle Images', 'Input Image', 'Generative Adversarial Networks', 'Segmentation Model', 'Testing Stage', 'Video Sequences', 'Human Visual System', 'Iterative Refinement', 'Visible Part', 'Real Ones', 'Perceptual Loss', 'Network Recovery', 'Real Vehicle', 'Generative Adversarial Networks Model', 'Object Discrimination', 'Mask R-CNN', 'Parking Garage']",,23,"In this paper, we propose a novel iterative multi-task framework to complete the segmentation mask of an occluded vehicle and recover the appearance of its invisible parts. In particular, firstly, to improve the quality of the segmentation completion, we present two coupled discriminators that introduce an auxiliary 3D model pool for sampling authentic silhouettes as adversarial samples. In addition, we propose a two-path structure with a shared network to enhance the appearance recovery capability. By iteratively performing the segmentation completion and the appearance recovery, the results will be progressively refined. To evaluate our method, we present a dataset, Occluded Vehicle dataset, containing synthetic and real-world occluded vehicle images. Based on this dataset, we conduct comparison experiments and demonstrate that our model outperforms the state-of-the-arts in both tasks of recovering segmentation mask and appearance for occluded vehicles. Moreover, we also demonstrate that our appearance recovery approach can benefit the occluded vehicle tracking in real-world videos."
VrR-VG: Refocusing Visually-Relevant Relationships,"Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei","2. JD AI Research, Beijing, China; 1. Xi’an Jiaotong University; 1. Xi’an Jiaotong University, 2. JD AI Research, Beijing, China",66.66666666666666,china,33.33333333333334,China,"Relationships encode the interactions among individual instances and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, relationship models tend to fit the statistical bias rather than ""learning"" to infer the relationships from images. To encourage further development in visual relationships, we propose a novel method to mine more valuable relationships by automatically pruning visually-irrelevant relationships. We construct a new scene graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved, which demonstrates the effectiveness of both our dataset and features embedding schema. Both our VrR-VG dataset and representation-aware features will be made publicly available soon.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.pdf,http://vrr-vg.com/,,,main,Poster,https://ieeexplore.ieee.org/document/9009080/,"['Visualization', 'Task analysis', 'Marine vehicles', 'Proposals', 'Genomics', 'Bioinformatics', 'Cognition']","['Statistical Methods', 'Feature Learning', 'Question Answering', 'Individual Instances', 'Image Captioning', 'Visual Question Answering', 'Visual Relationship', 'Scene Graph', 'Image Features', 'Visual Information', 'Cognitive Tasks', 'Visual Features', 'Visual Task', 'Visual Images', 'Bounding Box', 'Representation Learning', 'Current Dataset', 'Fully-connected Layer', 'Single Instance', 'Word Embedding', 'Representation Learning Methods', 'Subject And Object', 'Region Proposal', 'Previous Datasets', 'Bottom-up Attention', 'Data Split', 'Instances Of Categories', 'Visual Learning']",,38,"Relationships encode the interactions among individual instances and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, relationship models tend to fit the statistical bias rather than ``learning"" to infer the relationships from images. To encourage further development in visual relationships, we propose a novel method to mine more valuable relationships by automatically pruning visually-irrelevant relationships. We construct a new scene graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved, which demonstrates the effectiveness of both our dataset and features embedding schema. Both our VrR-VG dataset and representation-aware features will be made publicly available soon."
WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection,"Zhaoyang Zeng, Bei Liu, Jianlong Fu, Hongyang Chao, Lei Zhang","School of Data and Computer Science, Sun Yat-sen University; The Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University), Ministry of Education; Microsoft Research",66.66666666666666,China,33.33333333333334,USA,"We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., ""objectness""). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD2 can achieve state-of-the-art results.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_WSOD2_Learning_Bottom-Up_and_Top-Down_Objectness_Distillation_for_Weakly-Supervised_Object_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_WSOD2_Learning_Bottom-Up_and_Top-Down_Objectness_Distillation_for_Weakly-Supervised_Object_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009999/,"['Proposals', 'Object detection', 'Detectors', 'Training', 'Feature extraction', 'Transforms', 'Task analysis']","['Object Detection', 'Top-down And Bottom-up', 'Weakly Supervised Object Detection', 'Convolutional Neural Network', 'Bounding Box', 'Training Adaptations', 'Grand Challenge', 'Region Proposal', 'Bounding Box Regression', 'Top Down', 'Complete Object', 'Positive Samples', 'Impact Of Factors', 'Input Image', 'Attention In Recent Years', 'Detection Results', 'Image Object', 'Intersection Over Union', 'Fully-connected Layer', 'Object Detection Task', 'Multiple Instance Learning', 'Convolutional Neural Networks Backbone', 'Class Instances', 'Object Boundaries', 'Object Detection Results', 'Non-maximum Suppression', 'Classification Confidence', 'Edge Density', 'Convolutional Neural Network Features']",,87,"We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., “objectness”). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD2 can achieve state-of-the-art results."
Wasserstein GAN With Quadratic Transport Cost,"Huidong Liu, Xianfeng Gu, Dimitris Samaras","Stony Brook University, Stony Brook, NY 11794, USA",100.0,usa,0.0,,"Wasserstein GANs are increasingly used in Computer Vision applications as they are easier to train. Previous WGAN variants mainly use the l_1 transport cost to compute the Wasserstein distance between the real and synthetic data distributions. The l_1 transport cost restricts the discriminator to be 1-Lipschitz. However, WGANs with l_1 transport cost were recently shown to not always converge. In this paper, we propose WGAN-QC, a WGAN with quadratic transport cost. Based on the quadratic transport cost, we propose an Optimal Transport Regularizer (OTR) to stabilize the training process of WGAN-QC. We prove that the objective of the discriminator during each generator update computes the exact quadratic Wasserstein distance between real and synthetic data distributions. We also prove that WGAN-QC converges to a local equilibrium point with finite discriminator updates per generator update. We show experimentally on a Dirac distribution that WGAN-QC converges, when many of the l_1 cost WGANs fail to [22]. Qualitative and quantitative results on the CelebA, CelebA-HQ, LSUN and the ImageNet dog datasets show that WGAN-QC is better than state-of-art GAN methods. WGAN-QC has much faster runtime than other WGAN variants.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Wasserstein_GAN_With_Quadratic_Transport_Cost_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Wasserstein_GAN_With_Quadratic_Transport_Cost_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009084/,"['Gallium nitride', 'Generators', 'Generative adversarial networks', 'Training', 'Convergence', 'Jacobian matrices', 'Linear programming']","['Transportation Costs', 'Generative Adversarial Networks', 'Quadratic Cost', 'Data Distribution', 'Local Point', 'Equilibrium Point', 'Mahalanobis Distance', 'Dirac Delta', 'Real Distribution', 'Optimal Transport', 'Exact Distance', 'Real Data Distribution', 'Learning Rate', 'Gradient Descent', 'Second Derivative', 'Linear Programming', 'Regularization Term', 'Face Images', 'Generative Adversarial Networks Training', 'Optimal Discrimination', 'Generator Output', 'Red Box', 'Small Learning Rate', 'Discriminator Loss', 'Field Gradient', 'Positive Real']",,27,"Wasserstein GANs are increasingly used in Computer Vision applications as they are easier to train. Previous WGAN variants mainly use the lι transport cost to compute the Wasserstein distance between the real and synthetic data distributions. The lι transport cost restricts the discriminator to be 1-Lipschitz. However, WGANs with lι transport cost were recently shown to not always converge. In this paper, we propose WGAN-QC, a WGAN with quadratic transport cost. Based on the quadratic transport cost, we propose an Optimal Transport Regularizer (OTR) to stabilize the training process of WGAN-QC. We prove that the objective of the discriminator during each generator update computes the exact quadratic Wasserstein distance between real and synthetic data distributions. We also prove that WGAN-QC converges to a local equilibrium point with finite discriminator updates per generator update. We show experimentally on a Dirac distribution that WGAN-QC converges, when many of the lι cost WGANs fail to [22]. Qualitative and quantitative results on the CelebA, CelebA-HQ, LSUN and the ImageNet dog datasets show that WGAN-QC is better than state-of-art GAN methods. WGAN-QC has much faster runtime than other WGAN variants."
"Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning","Tanzila Rahman, Bicheng Xu, Leonid Sigal","University of British Columbia, Vector Institute for AI; University of British Columbia, Vector Institute for AI, Canada CIFAR AI Chair",100.0,canada,0.0,,"Multi-modal learning, particularly among imaging and linguistic modalities, has made amazing strides in many high-level fundamental visual understanding problems, ranging from language grounding to dense event captioning. However, much of the research has been limited to approaches that either do not take audio corresponding to video into account at all, or those that model the audio-visual correlations in service of sound or sound source localization. In this paper, we present the evidence, that audio signals can carry surprising amount of information when it comes to high-level visual-lingual tasks. Specifically, we focus on the problem of weakly-supervised dense event captioning in videos and show that audio on its own can nearly rival performance of a state-of-the-art visual model and, combined with video, can improve on the state-of-the-art performance. Extensive experiments on the ActivityNet Captions dataset show that our proposed multi-modal approach outperforms state-of-the-art unimodal methods, as well as validate specific feature representation and architecture design choices.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008125/,"['Visualization', 'Task analysis', 'Feature extraction', 'Proposals', 'Training', 'Generators', 'Mel frequency cepstral coefficient']","['Event Captioning', 'Feature Representation', 'Multimodal Approach', 'Sound Localization', 'Multimodal Learning', 'Final Model', 'Visual Features', 'Recurrent Neural Network', 'Multiple Modalities', 'Current Dataset', 'Training Loss', 'Visual Modality', 'Non-negative Matrix Factorization', 'Fusion Strategy', 'Localizer', 'Video Information', 'Fusion Techniques', 'Video Features', 'Multimodal Features', 'Computer Vision Community', 'Mel-frequency Cepstral Coefficients', 'Multimodal Model', 'Text Generation', 'Audio Information', 'Video Captioning', 'Video Analysis', 'Hyperbolic Tangent', 'Row Vector', 'Audio Data', 'Feature Fusion']",,52,"Multi-modal learning, particularly among imaging and linguistic modalities, has made amazing strides in many high-level fundamental visual understanding problems, ranging from language grounding to dense event captioning. However, much of the research has been limited to approaches that either do not take audio corresponding to video into account at all, or those that model the audio-visual correlations in service of sound or sound source localization. In this paper, we present the evidence, that audio signals can carry surprising amount of information when it comes to high-level visual-lingual tasks. Specifically, we focus on the problem of weakly-supervised dense event captioning in videos and show that audio on its own can nearly rival performance of a state-of-the-art visual model and, combined with video, can improve on the state-of-the-art performance. Extensive experiments on the ActivityNet Captions dataset show that our proposed multi-modal approach outperforms state-of-the-art unimodal methods, as well as validate specific feature representation and architecture design choices."
Wavelet Domain Style Transfer for an Effective Perception-Distortion Tradeoff in Single Image Super-Resolution,"Xin Deng, Ren Yang, Mai Xu, Pier Luigi Dragotti",ETH Zurich; Beihang University; Imperial College London,100.0,"china, switzerland, uk",0.0,,"In single image super-resolution (SISR), given a low-resolution (LR) image, one wishes to find a high-resolution (HR) version of it which is both accurate and photorealistic. Recently, it has been shown that there exists a fundamental tradeoff between low distortion and high perceptual quality, and the generative adversarial network (GAN) is demonstrated to approach the perception-distortion (PD) bound effectively. In this paper, we propose a novel method based on wavelet domain style transfer (WDST), which achieves a better PD tradeoff than the GAN based methods. Specifically, we propose to use 2D stationary wavelet transform (SWT) to decompose one image into low-frequency and high-frequency sub-bands. For the low-frequency sub-band, we improve its objective quality through an enhancement network. For the high-frequency sub-band, we propose to use WDST to effectively improve its perceptual quality. By feat of the perfect reconstruction property of wavelets, these sub-bands can be re-combined to obtain an image which has simultaneously high objective and perceptual quality. The numerical results on various datasets show that our method achieves the best trade-off between the distortion and perceptual quality among the existing state-of-the-art SISR methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Wavelet_Domain_Style_Transfer_for_an_Effective_Perception-Distortion_Tradeoff_in_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Wavelet_Domain_Style_Transfer_for_an_Effective_Perception-Distortion_Tradeoff_in_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9008402/,"['Wavelet transforms', 'Distortion', 'Image resolution', 'Wavelet domain', 'Image reconstruction', 'Histograms']","['Style Transfer', 'Wavelet Domain', 'Single Image Super-resolution', 'Perception-distortion Tradeoff', 'High-quality', 'Low Quality', 'Wavelet Transform', 'Generative Adversarial Networks', 'Perception Of Quality', 'Objective Quality', 'Stationary Wavelet Transform', 'Image Quality', 'Feature Maps', 'Stochastic Gradient Descent', 'Peak Signal-to-noise Ratio', 'Loss Of Content', 'Good Trade-off', 'Mean Square Error Loss', 'Wavelet Decomposition', 'Gram Matrix', 'Image Fusion Methods', 'Pixel Domain', 'Lower Mean Square Error']",,63,"In single image super-resolution (SISR), given a low-resolution (LR) image, one wishes to find a high-resolution (HR) version of it which is both accurate and photorealistic. Recently, it has been shown that there exists a fundamental tradeoff between low distortion and high perceptual quality, and the generative adversarial network (GAN) is demonstrated to approach the perception-distortion (PD) bound effectively. In this paper, we propose a novel method based on wavelet domain style transfer (WDST), which achieves a better PD tradeoff than the GAN based methods. Specifically, we propose to use 2D stationary wavelet transform (SWT) to decompose one image into low-frequency and high-frequency sub-bands. For the low-frequency sub-band, we improve its objective quality through an enhancement network. For the high-frequency sub-band, we propose to use WDST to effectively improve its perceptual quality. By feat of the perfect reconstruction property of wavelets, these sub-bands can be re-combined to obtain an image which has simultaneously high objective and perceptual quality. The numerical results on various datasets show that our method achieves the best trade-off between the distortion and perceptual quality among the existing state-of-the-art SISR methods."
Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection,"Lu Zhang, Xiangyu Zhu, Xiangyu Chen, Xu Yang, Zhen Lei, Zhiyong Liu","Renmin University of China; CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences; SKL-MCCS, Institute of Automation, Chinese Academy of Sciences; SKL-MCCS, Institute of Automation, Chinese Academy of Sciences; CEBSIT, Chinese Academy of Sciences",100.0,china,0.0,,"Multispectral pedestrian detection has shown great advantages under poor illumination conditions, since the thermal modality provides complementary information for the color image. However, real multispectral data suffers from the position shift problem, i.e. the color-thermal image pairs are not strictly aligned, making one object has different positions in different modalities. In deep learning based methods, this problem makes it difficult to fuse the feature maps from both modalities and puzzles the CNN training. In this paper, we propose a novel Aligned Region CNN (AR-CNN) to handle the weakly aligned multispectral data in an end-to-end way. Firstly, we design a Region Feature Alignment (RFA) module to capture the position shift and adaptively align the region features of the two modalities. Secondly, we present a new multimodal fusion method, which performs feature re-weighting to select more reliable features and suppress the useless ones. Besides, we propose a novel RoI jitter strategy to improve the robustness to unexpected shift patterns of different devices and system settings. Finally, since our method depends on a new kind of labelling: bounding boxes that match each modality, we manually relabel the KAIST dataset by locating bounding boxes in both modalities and building their relationships, providing a new KAIST-Paired Annotation. Extensive experimental validations on existing datasets are performed, demonstrating the effectiveness and robustness of the proposed method. Code and data are available at https://github.com/luzhang16/AR-CNN.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Weakly_Aligned_Cross-Modal_Learning_for_Multispectral_Pedestrian_Detection_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Weakly_Aligned_Cross-Modal_Learning_for_Multispectral_Pedestrian_Detection_ICCV_2019_paper.pdf,,https://github.com/luzhang16/AR-CNN,,main,Poster,https://ieeexplore.ieee.org/document/9009100/,"['Detectors', 'Image color analysis', 'Jitter', 'Robustness', 'Proposals', 'Lighting', 'Color']","['Pedestrian Detection', 'Multispectral Pedestrian Detection', 'Feature Maps', 'Regional Characteristics', 'Color Images', 'Positive Shift', 'Bounding Box', 'Image Pairs', 'Fusion Method', 'Reliable Characterization', 'Multispectral Data', 'Feature Alignment', 'Thermal Mode', 'Alignment Module', 'Infrared Imaging', 'Detection Performance', 'Object Detection', 'Intersection Over Union', 'Stochastic Gradient Descent', 'Face Recognition', 'Reference Mode', 'Alignment Process', 'Region Proposal Network', 'Shift Distance', 'Baseline Detection', 'Ground Truth Reference']",,109,"Multispectral pedestrian detection has shown great advantages under poor illumination conditions, since the thermal modality provides complementary information for the color image. However, real multispectral data suffers from the position shift problem, i.e. the color-thermal image pairs are not strictly aligned, making one object has different positions in different modalities. In deep learning based methods, this problem makes it difficult to fuse the feature maps from both modalities and puzzles the CNN training. In this paper, we propose a novel Aligned Region CNN (AR-CNN) to handle the weakly aligned multispectral data in an end-to-end way. Firstly, we design a Region Feature Alignment (RFA) module to capture the position shift and adaptively align the region features of the two modalities. Secondly, we present a new multimodal fusion method, which performs feature re-weighting to select more reliable features and suppress the useless ones. Besides, we propose a novel RoI jitter strategy to improve the robustness to unexpected shift patterns of different devices and system settings. Finally, since our method depends on a new kind of labelling: bounding boxes that match each modality, we manually relabel the KAIST dataset by locating bounding boxes in both modalities and building their relationships, providing a new KAIST-Paired Annotation. Extensive experimental validations on existing datasets are performed, demonstrating the effectiveness and robustness of the proposed method. Code and data are available at https://github.com/luzhang16/AR-CNN."
Weakly Supervised Energy-Based Learning for Action Segmentation,"Jun Li, Peng Lei, Sinisa Todorovic","Amazon.com Services, Inc.; Oregon State University",50.0,usa,50.0,USA,"This paper is about labeling video frames with action classes under weak supervision in training, where we have access to a temporal ordering of actions, but their start and end frames in training videos are unknown. Following prior work, we use an HMM grounded on a Gated Recurrent Unit (GRU) for frame labeling. Our key contribution is a new constrained discriminative forward loss (CDFL) that we use for training the HMM and GRU under weak supervision. While prior work typically estimates the loss on a single, inferred video segmentation, our CDFL discriminates between the energy of all valid and invalid frame labelings of a training video. A valid frame labeling satisfies the ground-truth temporal ordering of actions, whereas an invalid one violates the ground truth. We specify an efficient recursive algorithm for computing the CDFL in terms of the logadd function of the segmentation energy. Our evaluation on action segmentation and alignment gives superior results to those of the state of the art on the benchmark Breakfast Action, Hollywood Extended, and 50Salads datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Weakly_Supervised_Energy-Based_Learning_for_Action_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Weakly_Supervised_Energy-Based_Learning_for_Action_Segmentation_ICCV_2019_paper.pdf,,https://github.com/JunLi-Galios/CDFL,,main,Oral,https://ieeexplore.ieee.org/document/9010051/,"['Training', 'Hidden Markov models', 'Viterbi algorithm', 'Labeling', 'Color', 'Buildings', 'Logic gates']","['Action Segmentation', 'Energy-based Learning', 'State Of The Art', 'Hidden Markov Model', 'Video Frames', 'Temporal Order', 'Action Classes', 'Gated Recurrent Unit', 'Training Videos', 'Video Segments', 'Weak Supervision', 'End Of Frame', 'Supervision Training', 'Loss Function', 'Total Energy', 'Window Size', 'Pathfinding', 'Temporal Model', 'Iterative Refinement', 'Linear Graph', 'Valid Path', 'Test Videos', 'Optimal Segmentation', 'Hard Segments']",,64,"This paper is about labeling video frames with action classes under weak supervision in training, where we have access to a temporal ordering of actions, but their start and end frames in training videos are unknown. Following prior work, we use an HMM grounded on a Gated Recurrent Unit (GRU) for frame labeling. Our key contribution is a new constrained discriminative forward loss (CDFL) that we use for training the HMM and GRU under weak supervision. While prior work typically estimates the loss on a single, inferred video segmentation, our CDFL discriminates between the energy of all valid and invalid frame labelings of a training video. A valid frame labeling satisfies the ground-truth temporal ordering of actions, whereas an invalid one violates the ground truth. We specify an efficient recursive algorithm for computing the CDFL in terms of the logadd function of the segmentation energy. Our evaluation on action segmentation and alignment gives superior results to those of the state of the art on the benchmark Breakfast Action, Hollywood Extended, and 50Salads datasets."
Weakly Supervised Object Detection With Segmentation Collaboration,"Xiaoyan Li, Meina Kan, Shiguang Shan, Xilin Chen","2University of Chinese Academy of Sciences, Beijing 100049, China; 1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China; 3Peng Cheng Laboratory, Shenzhen, 518055, China; 1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China",100.0,china,0.0,,"Weakly supervised object detection aims at learning precise object detectors, given image category labels. In recent prevailing works, this problem is generally formulated as a multiple instance learning module guided by an image classification loss. The object bounding box is assumed to be the one contributing most to the classification among all proposals. However, the region contributing most is also likely to be a crucial part or the supporting context of an object. To obtain a more accurate detector, in this work we propose a novel end-to-end weakly supervised detection approach, where a newly introduced generative adversarial segmentation module interacts with the conventional detection module in a collaborative loop. The collaboration mechanism takes full advantages of the complementary interpretations of the weakly supervised localization task, namely detection and segmentation tasks, forming a more comprehensive solution. Consequently, our method obtains more precise object bounding boxes, rather than parts or irrelevant surroundings. Expectedly, the proposed method achieves an accuracy of 53.7% on the PASCAL VOC 2007 dataset, outperforming the state-of-the-arts and demonstrating its superiority for weakly supervised object detection.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Weakly_Supervised_Object_Detection_With_Segmentation_Collaboration_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Weakly_Supervised_Object_Detection_With_Segmentation_Collaboration_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010260/,"['Proposals', 'Object detection', 'Feature extraction', 'Collaboration', 'Task analysis', 'Detectors', 'Image segmentation']","['Object Detection', 'Bounding Box', 'Detection Task', 'Segmentation Task', 'Detection Module', 'Classification Loss', 'Category Labels', 'Object Bounding Boxes', 'Multiple Instance Learning', 'Precise Object', 'Input Image', 'Detection Results', 'General Strategy', 'Intersection Over Union', 'Training Strategy', 'Semantic Segmentation', 'Classification Score', 'Probability Matrix', 'Segmentation Map', 'Segmentation Branch', 'Object Proposals', 'Object Regions', 'Class Instances', 'Forward Pass', 'Discriminative Parts', 'Weak Supervision', 'Complete Object', 'Target Label', 'Pseudo Labels']",,72,"Weakly supervised object detection aims at learning precise object detectors, given image category labels. In recent prevailing works, this problem is generally formulated as a multiple instance learning module guided by an image classification loss. The object bounding box is assumed to be the one contributing most to the classification among all proposals. However, the region contributing most is also likely to be a crucial part or the supporting context of an object. To obtain a more accurate detector, in this work we propose a novel end-to-end weakly supervised detection approach, where a newly introduced generative adversarial segmentation module interacts with the conventional detection module in a collaborative loop. The collaboration mechanism takes full advantages of the complementary interpretations of the weakly supervised localization task, namely detection and segmentation tasks, forming a more comprehensive solution. Consequently, our method obtains more precise object bounding boxes, rather than parts or irrelevant surroundings. Expectedly, the proposed method achieves an accuracy of 53.7% on the PASCAL VOC 2007 dataset, outperforming the state-of-the-arts and demonstrating its superiority for weakly supervised object detection."
Weakly Supervised Temporal Action Localization Through Contrast Based Evaluation Networks,"Ziyi Liu, Le Wang, Qilin Zhang, Zhanning Gao, Zhenxing Niu, Nanning Zheng, Gang Hua","HERE Technologies; Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University; DAMO Academy, Alibaba Group; Machine Intelligence Israel Lab, Alibaba Group; Wormpex AI Research",40.0,china,60.0,Netherlands,"Weakly-supervised temporal action localization (WS-TAL) is a promising but challenging task with only video-level action categorical labels available during training. Without requiring temporal action boundary annotations in training data, WS-TAL could possibly exploit automatically retrieved video tags as video-level labels. However, such coarse video-level supervision inevitably incurs confusions, especially in untrimmed videos containing multiple action instances. To address this challenge, we propose the Contrast-based Localization EvaluAtioN Network (CleanNet) with our new action proposal evaluator, which provides pseudo-supervision by leveraging the temporal contrast in snippet-level action classification predictions. Essentially, the new action proposal evaluator enforces an additional temporal contrast constraint so that high-evaluation-score action proposals are more likely to coincide with true action instances. Moreover, the new action localization module is an integral part of CleanNet which enables end-to-end training. This is in contrast to many existing WS-TAL methods where action localization is merely a post-processing step. Experiments on THUMOS14 and ActivityNet datasets validate the efficacy of CleanNet against existing state-ofthe- art WS-TAL algorithms.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Weakly_Supervised_Temporal_Action_Localization_Through_Contrast_Based_Evaluation_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Weakly_Supervised_Temporal_Action_Localization_Through_Contrast_Based_Evaluation_Networks_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9428587/,"['Location awareness', 'Proposals', 'Videos', 'Task analysis', 'Feature extraction', 'Training', 'Three-dimensional displays']","['Temporal Localization', 'Action Localization', 'Temporal Action Localization', 'Weakly-supervised Temporal Action Localization', 'Sudden Changes', 'Local Module', 'Temporal Boundaries', 'True Activity', 'Action Instances', 'Weak Supervision', 'Temporal Contrast', 'Validation Set', 'Local Information', 'Receptive Field', 'Term In Eq', 'Action Recognition', 'Blue Box', 'Action Classes', 'Graph Convolutional Network', 'Joint Training', 'Intersection Over Union Threshold', 'Threshold-based Method', 'End Boundary', 'Accurate Boundary', 'Temporal Duration', 'Background Suppression', 'Second Term In Eq', 'First Term In Eq', 'Non-maximum Suppression', 'End Location']","['Action localization', 'weakly supervised learning', 'temporal contrast']",4,"Given only video-level action categorical labels during training, weakly-supervised temporal action localization (WS-TAL) learns to detect action instances and locates their temporal boundaries in untrimmed videos. Compared to its fully supervised counterpart, WS-TAL is more cost-effective in data labeling and thus favorable in practical applications. However, the coarse video-level supervision inevitably incurs ambiguities in action localization, especially in untrimmed videos containing multiple action instances. To overcome this challenge, we observe that significant temporal contrasts among video snippets, e.g., caused by temporal discontinuities and sudden changes, often occur around true action boundaries. This motivates us to introduce a Contrast-based Localization EvaluAtioN Network (CleanNet), whose core is a new temporal action proposal evaluator, which provides fine-grained pseudo supervision by leveraging the temporal contrasts among snippet-level classification predictions. As a result, the uncertainty in locating action instances can be resolved via evaluating their temporal contrast scores. Moreover, the new action localization module is an integral part of CleanNet which enables end-to-end training. This is in contrast to many existing WS-TAL methods where action localization is merely a post-processing step. Besides, we also explore the usage of temporal contrast on temporal action proposal (TAP) generation task, which we believe is the first attempt with the weak supervision setting. Experiments on the THUMOS14, ActivityNet v1.2 and v1.3 datasets validate the efficacy of our method against existing state-of-the-art WS-TAL algorithms."
Weakly-Supervised Action Localization With Background Modeling,"Phuc Xuan Nguyen, Deva Ramanan, Charless C. Fowlkes","University of California, Irvine; Carnegie Mellon University",100.0,"USA, usa",0.0,,"We describe a latent approach that learns to detect actions in long sequences given training videos with only whole-video class labels. Our approach makes use of two innovations to attention-modeling in weakly-supervised learning. First, and most notably, our framework uses an attention model to extract both foreground and background frames who's appearance is explicitly modeled. Most prior work ignores the background, but we show that modeling it allows our system to learn a richer notions of actions and their temporal extents. Second, we combine bottom-up, class-agnostic attention modules with top-down, class-specific activation maps, using the latter as form of self-supervision for the former. Doing so allows our model to learn a more accurate model of attention without explicit temporal supervision. These modifications lead to  10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed weakly-supervised system outperforms the recent state-of-the-art by at least 4.3% AP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used to aggressively scale-up learning to in-the-wild, uncurated Instagram videos (where relevant frames and videos are automatically selected through attentional processing). This allows our weakly supervised approach to even outperform fully-supervised methods for action detection at some overlap thresholds.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_Weakly-Supervised_Action_Localization_With_Background_Modeling_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_Weakly-Supervised_Action_Localization_With_Background_Modeling_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009568/,"['Videos', 'Training', 'Feature extraction', 'Proposals', 'Computational modeling', 'Technological innovation', 'Training data']","['Background Model', 'Action Localization', 'Weakly-supervised Action Localization', 'Explicit Model', 'Training Videos', 'Standard Protocol', 'Training Data', 'Total Loss', 'Social Media Platforms', 'Intersection Over Union', 'Average Precision', 'Optical Flow', 'Action Classes', 'Attention Map', 'Attention Weights', 'Test Videos', 'Class Activation Maps', 'Feature Pooling', 'Top-down Attention', 'Action Instances', 'Bottom-up Attention', 'Weak Supervision', 'Form Of Supervision', 'Clustering Loss']",,120,"We describe a latent approach that learns to detect actions in long sequences given training videos with only whole-video class labels. Our approach makes use of two innovations to attention-modeling in weakly-supervised learning. First, and most notably, our framework uses an attention model to extract both foreground and background frames whose appearance is explicitly modeled. Most prior works ignore the background, but we show that modeling it allows our system to learn a richer notion of actions and their temporal extents. Second, we combine bottom-up, class-agnostic attention modules with top-down, class-specific activation maps, using the latter as form of self-supervision for the former. Doing so allows our model to learn a more accurate model of attention without explicit temporal supervision. These modifications lead to 10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed weaklysupervised system outperforms recent state-of-the-arts by at least 4.3% AP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used to aggressively scale-up learning to in-the-wild, uncurated Instagram videos. The addition of these videos significantly improves localization performance of our weakly-supervised model."
What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance,"Mahmoud Afifi, Michael S. Brown","Samsung AI Center, Toronto; York University, Toronto",50.0,canada,50.0,South Korea,"There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into producing incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Afifi_What_Else_Can_Fool_Deep_Learning_Addressing_Color_Constancy_Errors_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Afifi_What_Else_Can_Fool_Deep_Learning_Addressing_Color_Constancy_Errors_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010271/,"['Image color analysis', 'Training', 'Cameras', 'Cats', 'Testing', 'Colored noise', 'Image segmentation']","['Deep Neural Network', 'Image Classification', 'Training Images', 'Augmentation Methods', 'Image Augmentation', 'White Balance', 'Performance Of Deep Neural Networks', 'CIFAR-100 Dataset', 'Training Set', 'Training Data', 'Validation Set', 'ImageNet', 'Batch Normalization', 'Semantic Segmentation', 'Deep Neural Network Model', 'AlexNet', 'Images In Order', 'Augmentation Techniques', 'Color Temperature', 'Adversarial Attacks', 'Histogram Features', 'Augmentation Process', 'Correction Matrix', 'Wrong Response', 'Inference Phase', 'Color Distribution', 'Use Of Color', 'Color Channels']",,69,"There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into producing incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets."
What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis,"Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, Hwalsuk Lee","Kyoto University; Clova AI Research, NA VER/LINE Corp.",50.0,japan,50.0,South Korea,"Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Baek_What_Is_Wrong_With_Scene_Text_Recognition_Model_Comparisons_Dataset_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Baek_What_Is_Wrong_With_Scene_Text_Recognition_Model_Comparisons_Dataset_ICCV_2019_paper.pdf,,https://github.com/clovaai/deep-text-recognition-benchmark,,main,Oral,https://ieeexplore.ieee.org/document/9010273/,"['Training', 'Data models', 'Benchmark testing', 'Text recognition', 'Analytical models', 'Task analysis', 'Recurrent neural networks']","['Recognition Model', 'Optical Character Recognition', 'Scene Text', 'Scene Text Recognition', 'Text Recognition Model', 'Scene Text Recognition Models', 'Training Dataset', 'Fair Comparison', 'Evaluation Dataset', 'Accurate Demand', 'Convolutional Neural Network', 'Combined Analysis', 'Feature Maps', 'Recurrent Neural Network', 'Receptive Field', 'Benchmark Datasets', 'Training Images', 'Imaging Evaluation', 'Failure Cases', 'Natural Scenes', 'Prediction Module', 'Transformation Module', 'Thin-plate Spline', 'Bidirectional Long Short-term Memory', 'Noisy Labels', 'Image Texture', 'Subset Of Dataset', 'Google Street View', 'Memory Consumption', 'Convolutional Recurrent Neural Network']",,347,"Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available."
What Synthesis Is Missing: Depth Adaptation Integrated With Weak Supervision for Indoor Scene Parsing,"Keng-Chi Liu, Yi-Ting Shen, Jan P. Klopp, Liang-Gee Chen",National Taiwan University,100.0,taiwan,0.0,,"Scene Parsing is a crucial step to enable autonomous systems to understand and interact with their surroundings. Supervised deep learning methods have made great progress in solving scene parsing problems, however, come at the cost of laborious manual pixel-level annotation. Synthetic data as well as weak supervision have been investigated to alleviate this effort. Nonetheless, synthetically generated data still suffers from severe domain shift while weak labels often lack precision. Moreover, most existing works for weakly supervised scene parsing are limited to salient foreground objects. The aim of this work is hence twofold: Exploit synthetic data where feasible and integrate weak supervision where necessary. More concretely, we address this goal by utilizing depth as transfer domain because its synthetic-to-real discrepancy is much lower than for color. At the same time, we perform weak localization from easily obtainable image level labels and integrate both using a novel contour-based scheme. Our approach is implemented as a teacher-student learning framework to solve the transfer learning problem by generating a pseudo ground truth. Using only depth-based adaptation, this approach already outperforms previous transfer learning approaches on the popular indoor scene parsing SUN RGB-D dataset. Our proposed two-stage integration more than halves the gap towards fully supervised methods when compared to previous state-of-the-art in transfer learning.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Liu_What_Synthesis_Is_Missing_Depth_Adaptation_Integrated_With_Weak_Supervision_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_What_Synthesis_Is_Missing_Depth_Adaptation_Integrated_With_Weak_Supervision_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9009548/,"['Training', 'Task analysis', 'Adaptation models', 'Semantics', 'Computer vision', 'Data models', 'Heating systems']","['Weak Supervision', 'Scene Parsing', 'Transfer Learning', 'Domain Shift', 'Weak Localization', 'Image-level Labels', 'Adaptive Method', 'Bounding Box', 'Generative Adversarial Networks', 'RGB Images', 'Depth Map', 'Target Domain', 'Depth Data', 'Domain Adaptation', 'Integration Step', 'Teacher Network', 'Depth Perception', 'Min-max Normalization', 'Student Network', 'Pseudo Labels', 'Real Domain', 'Domain Adaptation Methods', 'Multiple Instance Learning', 'Domain Discrepancy', 'Real Depth', 'Boundary Information', 'Sensor Noise', 'Heatmap', 'Small Objects']",,5,"Scene Parsing is a crucial step to enable autonomous systems to understand and interact with their surroundings. Supervised deep learning methods have made great progress in solving scene parsing problems, however, come at the cost of laborious manual pixel-level annotation. Synthetic data as well as weak supervision have been investigated to alleviate this effort. Nonetheless, synthetically generated data still suffers from severe domain shift while weak labels often lack precision. Moreover, most existing works for weakly supervised scene parsing are limited to salient foreground objects. The aim of this work is hence twofold: Exploit synthetic data where feasible and integrate weak supervision where necessary. More concretely, we address this goal by utilizing depth as transfer domain because its synthetic-to-real discrepancy is much lower than for color. At the same time, we perform weak localization from easily obtainable image level labels and integrate both using a novel contour-based scheme. Our approach is implemented as a teacher-student learning framework to solve the transfer learning problem by generating a pseudo ground truth. Using only depth-based adaptation, this approach already outperforms previous transfer learning approaches on the popular indoor scene parsing SUN RGB-D dataset. Our proposed two-stage integration more than halves the gap towards fully supervised methods when compared to previous state-of-the-art in transfer learning."
What Would You Expect? Anticipating Egocentric Actions With Rolling-Unrolling LSTMs and Modality Attention,"Antonino Furnari, Giovanni Maria Farinella",University of Catania - Department of Mathematics and Computer Science,100.0,Italy,0.0,,"Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see the project web page for code and additional details: http://iplab.dmi.unict.it/rulstm.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Furnari_What_Would_You_Expect_Anticipating_Egocentric_Actions_With_Rolling-Unrolling_LSTMs_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Furnari_What_Would_You_Expect_Anticipating_Egocentric_Actions_With_Rolling-Unrolling_LSTMs_ICCV_2019_paper.pdf,http://iplab.dmi.unict.it/rulstm,,,main,Oral,https://ieeexplore.ieee.org/document/9008264/,"['Task analysis', 'Computer architecture', 'Streaming media', 'Encoding', 'Containers', 'Microprocessors', 'Computational modeling']","['Attention Mechanism', 'Early Recognition', 'Action Recognition', 'Activity Prediction', 'Optical Flow', 'Prior Art', 'Multiple Temporal Scales', 'Action Recognition Task', 'Training Set', 'Time Step', 'Validation Set', 'Rate Measurements', 'Precision And Recall', 'Future Actions', 'Bounding Box', 'Past Work', 'Learnable Parameters', 'Hidden State', 'Single Branch', 'Personal Vision', 'Late Fusion', 'Early Fusion', 'Hidden Vector', 'Top-5 Accuracy', 'Fusion Weights', 'Partial Observation', 'Strong Baseline', 'Cell State', 'Aforementioned Works']",,98,"Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see the project web page for code and additional details: http://iplab.dmi.unict.it/rulstm."
Where Is My Mirror?,"Xin Yang, Haiyang Mei, Ke Xu, Xiaopeng Wei, Baocai Yin, Rynson W.H. Lau","Dalian University of Technology; Dalian University of Technology, City University of Hong Kong; Dalian University of Technology, Peng Cheng Laboratory; City University of Hong Kong",100.0,"Hong Kong, china",0.0,,"Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial. The key challenge is that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this paper, we present a novel method to segment mirrors from an input image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions. First, we construct a large-scale mirror dataset that contains mirror images with corresponding manually annotated masks. This dataset covers a variety of daily life scenes, and will be made publicly available for future research. Second, we propose a novel network, called MirrorNet, for mirror segmentation, by modeling both semantical and low-level color/texture discontinuities between the contents inside and outside of the mirrors. Third, we conduct extensive experiments to evaluate the proposed method, and show that it outperforms the carefully chosen baselines from the state-of-the-art detection and segmentation methods.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Where_Is_My_Mirror_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Where_Is_My_Mirror_ICCV_2019_paper.pdf,https://mhaiyang.github.io/ICCV2019_MirrorNet/index,,,main,Poster,https://ieeexplore.ieee.org/document/9008776/,"['Mirrors', 'Image segmentation', 'Semantics', 'Feature extraction', 'Image color analysis', 'Shape', 'Training']","['Detection Methods', 'Input Image', 'Large-scale Datasets', 'Segmentation Method', 'Mirror Image', 'Training Set', 'Input Features', 'Object Detection', 'Mean Absolute Error', 'Intersection Over Union', 'Stochastic Gradient Descent', 'Semantic Segmentation', 'Multi-scale Features', 'Segmentation Performance', 'Instance Segmentation', 'Feature Extraction Network', 'Multi-level Features', 'Mask R-CNN', 'Dilation Rate', 'Salient Object Detection', 'Nearby Objects', 'Pixel Accuracy', 'Global Contrast', 'Encoder Part', 'Instance Segmentation Methods', 'Contextual Information', 'Discriminative Feature Learning', 'Receptive Field', 'Low-level Features']",,54,"Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial. The key challenge is that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this paper, we present a novel method to segment mirrors from an input image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions. First, we construct a large-scale mirror dataset that contains mirror images with corresponding manually annotated masks. This dataset covers a variety of daily life scenes, and will be made publicly available for future research. Second, we propose a novel network, called MirrorNet, for mirror segmentation, by modeling both semantical and low-level color/texture discontinuities between the contents inside and outside of the mirrors. Third, we conduct extensive experiments to evaluate the proposed method, and show that it outperforms the carefully chosen baselines from the state-of-the-art detection and segmentation methods."
Why Does a Visual Question Have Different Answers?,"Nilavra Bhattacharya, Qing Li, Danna Gurari","University of California, Los Angeles; University of Texas at Austin",100.0,usa,0.0,,"Visual question answering is the task of returning the answer to a question about an image. A challenge is that different people often provide different answers to the same visual question. To our knowledge, this is the first work that aims to understand why. We propose a taxonomy of nine plausible reasons, and create two labelled datasets consisting of  45,000 visual questions indicating which reasons led to answer differences. We then propose a novel problem of predicting directly from a visual question which reasons will cause answer differences as well as a novel algorithm for this purpose. Experiments demonstrate the advantage of our approach over several related baselines on two diverse datasets. We publicly share the datasets and code at https://vizwiz.org.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bhattacharya_Why_Does_a_Visual_Question_Have_Different_Answers_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bhattacharya_Why_Does_a_Visual_Question_Have_Different_Answers_ICCV_2019_paper.pdf,https://vizwiz.org,https://github.com/vizwiz,,main,Poster,https://ieeexplore.ieee.org/document/9008245/,"['Visualization', 'Powders', 'Task analysis', 'Taxonomy', 'Knowledge discovery', 'Prediction algorithms', 'Computer vision']","['Visual Question', 'Plausible Reason', 'Visual Question Answering', 'Algorithms For Purposes', 'Ambiguity', 'Crowdsourcing', 'Search String', 'Average Precision', 'Random Guessing', 'Multiple Answers', 'Algorithmic Framework', 'Low-quality Images', 'Single Answer', 'Predictive Cues', 'Algorithms For Tasks', 'Soft Targets']",,18,"Visual question answering is the task of returning the answer to a question about an image. A challenge is that different people often provide different answers to the same visual question. To our knowledge, this is the first work that aims to understand why. We propose a taxonomy of nine plausible reasons, and create two labelled datasets consisting of ~45,000 visual questions indicating which reasons led to answer differences. We then propose a novel problem of predicting directly from a visual question which reasons will cause answer differences as well as a novel algorithm for this purpose. Experiments demonstrate the advantage of our approach over several related baselines on two diverse datasets. We publicly share the datasets and code at https://vizwiz.org."
"WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving","Senthil Yogamani, CiarÃ¡n Hughes, Jonathan Horgan, Ganesh Sistu, Padraig Varley, Derek O'Dea, Michal UÅiÄÃ¡Å, Stefan Milz, Martin Simon, Karl Amende, Christian Witt, Hazem Rashed, Sumanth Chennupati, Sanjaya Nayak, Saquib Mansoor, Xavier Perrotton, Patrick PÃ©rez",Valeo,0.0,,100.0,USA,"Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yogamani_WoodScape_A_Multi-Task_Multi-Camera_Fisheye_Dataset_for_Autonomous_Driving_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yogamani_WoodScape_A_Multi-Task_Multi-Camera_Fisheye_Dataset_for_Autonomous_Driving_ICCV_2019_paper.pdf,,https://github.com/valeoai/WoodScape,,main,Oral,https://ieeexplore.ieee.org/document/9008254/,"['Cameras', 'Task analysis', 'Adaptation models', 'Three-dimensional displays', 'Semantics', 'Nonlinear distortion']","['Fish-eye Datasets', 'Computer Vision', 'Bounding Box', 'Depth Estimation', 'Fisheye Lens', 'Semantic Annotation', '3D Bounding Box', 'Convolutional Neural Network', 'Standard Model', 'Object Detection', 'Image Regions', 'Pedestrian', 'Intersection Over Union', 'Point Cloud', 'Semantic Segmentation', 'General Data Protection Regulation', 'Segmentation Task', 'Model Projections', 'Biquadratic', 'Baseline Experiments', 'Monocular Depth Estimation', 'Multi-task Model', 'Visual Odometry', 'Visual Simultaneous Localization And Mapping', 'Instance Selection', 'Front Camera', 'Camera Images', 'Narrow Field Of View', 'Camera Model']",,160,"Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification."
X-Section: Cross-Section Prediction for Enhanced RGB-D Fusion,"Andrea Nicastro, Ronald Clark, Stefan Leutenegger","Dyson Robotics Lab, Imperial College London; Smart Robotics Lab, Imperial College London",100.0,uk,0.0,,"Detailed 3D reconstruction is an important challenge with application to robotics, augmented and virtual reality, which has seen impressive progress throughout the past years. Advancements were driven by the availability of depth cameras (RGB-D), as well as increased compute power, e.g. in the form of GPUs -- but also thanks to inclusion of machine learning in the process. Here, we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep learning to make object-level predictions about thicknesses that can be readily integrated into a volumetric multi-view fusion process, where we propose an extension to the popular KinectFusion approach. In essence, our method allows to complete shape in general indoor scenes behind what is sensed by the RGB-D camera, which may be crucial e.g. for robotic manipulation tasks or efficient scene exploration. Predicting object thicknesses rather than volumes allows us to work with comparably high spatial resolution without exploding memory and training data requirements on the employed Convolutional Neural Networks. In a series of qualitative and quantitative evaluations, we demonstrate how we accurately predict object thickness and reconstruct general 3D scenes containing multiple objects.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Nicastro_X-Section_Cross-Section_Prediction_for_Enhanced_RGB-D_Fusion_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Nicastro_X-Section_Cross-Section_Prediction_for_Enhanced_RGB-D_Fusion_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010822/,"['Three-dimensional displays', 'Image reconstruction', 'Shape', 'Task analysis', 'Robots', 'Surface reconstruction', 'Pipelines']","['Training Data', '3D Reconstruction', 'Fusion Process', 'Depth Camera', 'Robot Manipulator', 'Reconstruction Approach', 'Efficient Exploration', 'Shape Completion', 'Validation Set', 'Object Detection', 'Bounding Box', 'Reconstruction Algorithm', 'Single Frame', 'Depth Images', 'Object Shape', '3D Shape', 'Domain Adaptation', 'Valid Sequences', 'Advances In Deep Learning', 'Semantic Labels', 'Fusion Algorithm', '3D Grid', 'Voxel Grid', 'Cross-sectional Thickness', 'Single View', 'Object Instances', 'Short Values', 'Part Of The Scene', 'Individual Objects', 'ImageNet']",,9,"Detailed 3D reconstruction is an important challenge with application to robotics, augmented and virtual reality, which has seen impressive progress throughout the past years. Advancements were driven by the availability of depth cameras (RGB-D), as well as increased compute power, e.g.\ in the form of GPUs -- but also thanks to inclusion of machine learning in the process. Here, we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep learning to make object-level predictions about thicknesses that can be readily integrated into a volumetric multi-view fusion process, where we propose an extension to the popular KinectFusion approach. In essence, our method allows to complete shape in general indoor scenes behind what is sensed by the RGB-D camera, which may be crucial e.g.\ for robotic manipulation tasks or efficient scene exploration. Predicting object thicknesses rather than volumes allows us to work with comparably high spatial resolution without exploding memory and training data requirements on the employed Convolutional Neural Networks. In a series of qualitative and quantitative evaluations, we demonstrate how we accurately predict object thickness and reconstruct general 3D scenes containing multiple objects."
XRAI: Better Attributions Through Regions,"Andrei Kapishnikov, Tolga Bolukbasi, Fernanda ViÃ©gas, Michael Terry","Google Research, Cambridge, MA",0.0,,100.0,USA,"Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Kapishnikov_XRAI_Better_Attributions_Through_Regions_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kapishnikov_XRAI_Better_Attributions_Through_Regions_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008576/,"['Measurement', 'Predictive models', 'Perturbation methods', 'Image segmentation', 'Reliability', 'Neural networks', 'Birds']","['Neural Network', 'Evaluation Method', 'Deep Neural Network', 'Empirical Results', 'ImageNet Dataset', 'Saliency Map', 'Sanity Check', 'Region-based Methods', 'Attribution Methods', 'Receiver Operating Characteristic', 'Measurement Methods', 'Input Features', 'Network Layer', 'Image Regions', 'Object Of Interest', 'Changes In Output', 'Feature Subset', 'Neural Network Training', 'Random Networks', 'Approximate Entropy', 'Smallest Region', 'Smooth Regions', 'Dark Pixels', 'Local Metrics', 'Input Pixels', 'Blurred Images', 'Random Baseline', 'Individual Pixels']",,109,"Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset."
YOLACT: Real-Time Instance Segmentation,"Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee","University of California, Davis",100.0,usa,0.0,,"We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Bolya_YOLACT_Real-Time_Instance_Segmentation_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Bolya_YOLACT_Real-Time_Instance_Segmentation_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010373/,"['Prototypes', 'Real-time systems', 'Image segmentation', 'Object detection', 'Detectors', 'Computational modeling', 'Task analysis']","['Instance Segmentation', 'Real-time Instance Segmentation', 'Linear Combination', 'Temporal Stability', 'Emergent Behavior', 'Feature Space', 'Image Size', 'Object Detection', 'Bounding Box', 'Semantic Segmentation', 'Two-stage Method', 'Faster R-CNN', 'Local Failure', 'Conv Layer', 'Mask R-CNN', 'Box Regression', 'Predicted Bounding Box', 'Ground-truth Bounding Box', 'One-stage Methods', 'IoU Threshold', 'Instance Segmentation Methods', 'Prediction Head', 'One-stage Model', 'One-stage Object Detection', 'Real-time Object Detection', 'Assembly Step', 'Representation Of Distribution', 'Bounding Box Regression']",,1242,"We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty."
Zero-Shot Anticipation for Instructional Activities,"Fadime Sener, Angela Yao","National University of Singapore; University of Bonn, Germany",100.0,"germany, singapore",0.0,,"How can we teach a robot to predict what will happen next for an activity it has never seen before? We address the problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sener_Zero-Shot_Anticipation_for_Instructional_Activities_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sener_Zero-Shot_Anticipation_for_Instructional_Activities_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008304/,"['Visualization', 'Decoding', 'Robots', 'Predictive models', 'Encoding', 'Natural languages', 'Training']","['Natural Language', 'Activity Of Complex', 'Number Of Steps', 'Knowledge Transfer', 'User Study', 'Recurrent Neural Network', 'Hidden State', 'Action Recognition', 'Procedural Knowledge', 'Input Text', 'Video Segments', 'Joint Training', 'List Of Ingredients', 'Service Robots', 'Temporal Alignment', 'Context Vector', 'Video Encoding', 'Sentence Scores', 'Video Captioning', 'Hidden State Vector', 'Visual Evidence']",,36,"How can we teach a robot to predict what will happen next for an activity it has never seen before? We address the problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation."
Zero-Shot Emotion Recognition via Affective Structural Embedding,"Chi Zhan, Dongyu She, Sicheng Zhao, Ming-Ming Cheng, Jufeng Yang","Nankai University/University of California, Berkeley; University of California, Berkeley; Nankai University",100.0,"China, usa",0.0,,"Image emotion recognition attracts much attention in recent years due to its wide applications. It aims to classify the emotional response of humans, where candidate emotion categories are generally defined by specific psychological theories, such as Ekman's six basic emotions. However, with the development of psychological theories, emotion categories become increasingly diverse, fine-grained, and difficult to collect samples. In this paper, we investigate zero-shot learning (ZSL) problem in the emotion recognition task, which tries to recognize the new unseen emotions. Specifically, we propose a novel affective-structural embedding framework, utilizing mid-level semantic representation, i.e., adjective-noun pairs (ANP) features, to construct an affective embedding space. By doing this, the learned intermediate space can narrow the semantic gap between low-level visual and high-level semantic features. In addition, we introduce an affective adversarial constraint to retain the discriminative capacity of visual features and the affective structural information of semantic features during training process. Our method is evaluated on five widely used affective datasets and the perimental results show the proposed algorithm outperforms the state-of-the-art approaches.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Zhan_Zero-Shot_Emotion_Recognition_via_Affective_Structural_Embedding_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhan_Zero-Shot_Emotion_Recognition_via_Affective_Structural_Embedding_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9010694/,"['Semantics', 'Visualization', 'Emotion recognition', 'Training', 'Psychology', 'Task analysis', 'Bridges']","['Emotion Recognition', 'Zero-shot', 'Visual Features', 'Latent Space', 'Psychological Theory', 'Low-level Features', 'Structural Framework', 'Semantic Features', 'Learning Spaces', 'Semantic Representations', 'Emotion Categories', 'Emotion Recognition Task', 'Low-level Visual Features', 'Intermediate Space', 'Deep Learning', 'Negative Emotions', 'Convolutional Neural Network', 'Image Features', 'Disgust', 'Unseen Classes', 'Classifier Training', 'Deep Features', 'International Affective Picture System', 'Visual Space', 'Generative Adversarial Networks', 'Split Set', 'Word2vec', 'Image X', 'Semantic Space']",,39,"Image emotion recognition attracts much attention in recent years due to its wide applications. It aims to classify the emotional response of humans, where candidate emotion categories are generally defined by specific psychological theories, such as Ekman's six basic emotions. However, with the development of psychological theories, emotion categories become increasingly diverse, fine-grained, and difficult to collect samples. In this paper, we investigate zero-shot learning (ZSL) problem in the emotion recognition task, which tries to recognize the new unseen emotions. Specifically, we propose a novel affective-structural embedding framework, utilizing mid-level semantic representation, i.e., adjective-noun pairs (ANP) features, to construct an affective embedding space. By doing this, the learned intermediate space can narrow the semantic gap between low-level visual and high-level semantic features. In addition, we introduce an affective adversarial constraint to retain the discriminative capacity of visual features and the affective structural information of semantic features during training process. Our method is evaluated on five widely used affective datasets and the perimental results show the proposed algorithm outperforms the state-of-the-art approaches."
Zero-Shot Grounding of Objects From Natural Language Queries,"Arka Sadhu, Kan Chen, Ram Nevatia",Facebook Inc.; University of Southern California,50.0,usa,50.0,USA,"A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, ""unseen"" nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual ""seen"" settings and performs significantly better than baseline in the zero-shot setting.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Sadhu_Zero-Shot_Grounding_of_Objects_From_Natural_Language_Queries_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Sadhu_Zero-Shot_Grounding_of_Objects_From_Natural_Language_Queries_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010634/,"['Grounding', 'Training', 'Proposals', 'Automobiles', 'Detectors', 'Feature extraction', 'Visualization']","['Natural Language Query', 'Object Detection', 'Image Object', 'Regression Parameters', 'Detection Dataset', 'Category Learning', 'Grounding System', 'Training Set', 'Validation Set', 'Feature Maps', 'Visual Features', 'Bounding Box', 'Case Example', 'Word Embedding', 'Similar Objects', 'Focal Loss', 'Balanced Set', 'Noun Phrase', 'Fully Convolutional Network', 'Candidate Boxes', 'Scene Graph', 'Ground-truth Box', 'Multimodal Features', 'Language Faculty', 'Ground-truth Bounding Box', 'Unseen Classes', 'Training Time', 'Channel Dimension', 'Fourth Column']",,92,"A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, “unseen” nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual “seen” settings and performs significantly better than baseline in the zero-shot setting."
Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks,"Wenguan Wang, Xiankai Lu, Jianbing Shen, David J. Crandall, Ling Shao","Inception Institute of Artiﬁcial Intelligence, UAE; Indiana University, USA",100.0,"uae, usa",0.0,,"This work proposes a novel attentive graph neural network (AGNN) for zero-shot video object segmentation (ZVOS). The suggested AGNN recasts this task as a process of iterative information fusion over video graphs. Specifically, AGNN builds a fully connected graph to efficiently represent frames as nodes, and relations between arbitrary frame pairs as edges. The underlying pair-wise relations are described by a differentiable attention mechanism. Through parametric message passing, AGNN is able to efficiently capture and mine much richer and higher-order relations between video frames, thus enabling a more complete understanding of video content and more accurate foreground estimation. Experimental results on three video segmentation datasets show that AGNN sets a new state-of-the-art in each case. To further demonstrate the generalizability of our framework, we extend AGNN to an additional task: image object co-segmentation (IOCS). We perform experiments on two famous IOCS datasets and observe again the superiority of our AGNN model. The extensive experiments verify that AGNN is able to learn the underlying semantic/appearance relationships among video frames or related images, and discover the common objects.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Zero-Shot_Video_Object_Segmentation_via_Attentive_Graph_Neural_Networks_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Zero-Shot_Video_Object_Segmentation_via_Attentive_Graph_Neural_Networks_ICCV_2019_paper.pdf,,https://github.com/carrierlxk/AGNN,,main,Oral,https://ieeexplore.ieee.org/document/9010385/,"['Neural networks', 'Task analysis', 'Message passing', 'Machine learning', 'Object segmentation', 'Estimation', 'Image segmentation']","['Neural Network', 'Graph Neural Networks', 'Video Object Segmentation', 'Zero-shot Video Object Segmentation', 'Attention Mechanism', 'Video Frames', 'Common Objects', 'Additional Tasks', 'Video Content', 'Pairwise Relationships', 'Information Fusion', 'Message Passing', 'Convolutional Layers', 'Training Phase', 'Recurrent Neural Network', 'Node Status', 'Handcrafted Features', 'Optical Flow', 'Original Information', 'Graph Convolutional Network', 'Graph Neural Network Model', 'Fully Convolutional Network', 'Node Representations', 'Node Embeddings', 'Well-known Datasets', 'Successive Frames', 'Motion Information', 'Higher-order Relationships', 'Foreground Objects', 'Multiple Frames']",,200,"This work proposes a novel attentive graph neural network (AGNN) for zero-shot video object segmentation (ZVOS). The suggested AGNN recasts this task as a process of iterative information fusion over video graphs. Specifically, AGNN builds a fully connected graph to efficiently represent frames as nodes, and relations between arbitrary frame pairs as edges. The underlying pair-wise relations are described by a differentiable attention mechanism. Through parametric message passing, AGNN is able to efficiently capture and mine much richer and higher-order relations between video frames, thus enabling a more complete understanding of video content and more accurate foreground estimation. Experimental results on three video segmentation datasets show that AGNN sets a new state-of-the-art in each case. To further demonstrate the generalizability of our framework, we extend AGNN to an additional task: image object co-segmentation (IOCS). We perform experiments on two famous IOCS datasets and observe again the superiority of our AGNN model. The extensive experiments verify that AGNN is able to learn the underlying semantic/appearance relationships among video frames or related images, and discover the common objects."
advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns,"Zhibo Wang, Siyan Zheng, Mengkai Song, Qian Wang, Alireza Rahimpour, Hairong Qi","Dept. of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, P. R. China",100.0,"china, usa",0.0,,"Person re-identification (re-ID) is the task of matching person images across camera views, which plays an important role in surveillance and security applications. Inspired by great progress of deep learning, deep re-ID models began to be popular and gained state-of-the-art performance. However, recent works found that deep neural networks (DNNs) are vulnerable to adversarial examples, posing potential threats to DNNs based applications. This phenomenon throws a serious question about whether deep re-ID based systems are vulnerable to adversarial attacks. In this paper, we take the first attempt to implement robust physical-world attacks against deep re-ID. We propose a novel attack algorithm, called advPattern, for generating adversarial patterns on clothes, which learns the variations of image pairs across cameras to pull closer the image features from the same camera, while pushing features from different cameras farther. By wearing our crafted ""invisible cloak"", an adversary can evade person search, or impersonate a target person to fool deep re-ID models in physical world. We evaluate the effectiveness of our transformable patterns on adversaries' clothes with Market1501 and our established PRCS dataset. The experimental results show that the rank-1 accuracy of re-ID models for matching the adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore, the adversary can impersonate a target person with 47.1% rank-1 accuracy and 67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID systems are vulnerable to our physical attacks.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_advPattern_Physical-World_Attacks_on_Deep_Person_Re-Identification_via_Adversarially_Transformable_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_advPattern_Physical-World_Attacks_on_Deep_Person_Re-Identification_via_Adversarially_Transformable_ICCV_2019_paper.pdf,,,,main,Poster,https://ieeexplore.ieee.org/document/9008244/,"['Cameras', 'Task analysis', 'Pattern matching', 'Neural networks', 'Printing', 'Surveillance', 'Perturbation methods']","['Attacks In The Physical World', 'Deep Neural Network', 'Image Features', 'Deep Models', 'Image Pairs', 'Camera View', 'Person Image', 'Adversarial Attacks', 'Target Person', 'Adversarial Examples', 'Impersonation Attack', 'Invisibility Cloak', 'Important Role In Surveillance', 'Optimization Problem', 'High Success Rate', 'Test Points', 'Digital Environment', 'Robust Patterns', 'Threat Model', 'Siamese Network', 'Digital Pattern', 'Attack Methods', 'Gallery Images', 'Digital Domain', 'Fast Gradient Sign Method', 'Person Of Interest', 'Attack Performance', 'Ranking Task', 'Triplet Loss']",,42,"Person re-identification (re-ID) is the task of matching person images across camera views, which plays an important role in surveillance and security applications. Inspired by great progress of deep learning, deep re-ID models began to be popular and gained state-of-the-art performance. However, recent works found that deep neural networks (DNNs) are vulnerable to adversarial examples, posing potential threats to DNNs based applications. This phenomenon throws a serious question about whether deep re-ID based systems are vulnerable to adversarial attacks. In this paper, we take the first attempt to implement robust physical-world attacks against deep re-ID. We propose a novel attack algorithm, called advPattern, for generating adversarial patterns on clothes, which learns the variations of image pairs across cameras to pull closer the image features from the same camera, while pushing features from different cameras farther. By wearing our crafted “invisible cloak”, an adversary can evade person search, or impersonate a target person to fool deep re-ID models in physical world. We evaluate the effectiveness of our transformable patterns on adversaries' clothes with Market1501 and our established PRCS dataset. The experimental results show that the rank-1 accuracy of re-ID models for matching the adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore, the adversary can impersonate a target person with 47.1% rank-1 accuracy and 67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID systems are vulnerable to our physical attacks."
l-Net: Reconstruct Hyperspectral Images From a Snapshot Measurement,"Xin Miao, Xin Yuan, Yunchen Pu, Vassilis Athitsos","University of Texas at Arlington, TX, USA; Facebook, CA, USA; Nokia Bell Labs, NJ, USA",33.33333333333333,USA,66.66666666666667,USA,"We propose the l-net, which reconstructs hyperspectral images (e.g., with 24 spectral channels) from a single shot measurement. This task is usually termed snapshot compressive-spectral imaging (SCI), which enjoys low cost, low bandwidth and high-speed sensing rate via capturing the three-dimensional (3D) signal i.e., (x, y, l), using a 2D snapshot. Though proposed more than a decade ago, the poor quality and low-speed of reconstruction algorithms preclude wide applications of SCI. To address this challenge, in this paper, we develop a dual-stage generative model to reconstruct the desired 3D signal in SCI, dubbed l-net. Results on both simulation and real datasets demonstrate the significant advantages of l-net, which leads to >4dB improvement in PSNR for real-mask-in-the-loop simulation data compared to the current state-of-the-art. Furthermore, l-net can finish the reconstruction task within sub-seconds instead of hours taken by the most recently proposed DeSCI algorithm, thus speeding up the reconstruction >1000 times.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Miao_l-Net_Reconstruct_Hyperspectral_Images_From_a_Snapshot_Measurement_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_l-Net_Reconstruct_Hyperspectral_Images_From_a_Snapshot_Measurement_ICCV_2019_paper.pdf,,https://github.com/xinxinmiao/lambda-net,,main,Poster,https://ieeexplore.ieee.org/document/9010044/,"['Image reconstruction', 'Hyperspectral imaging', 'Imaging', 'Gallium nitride', 'Task analysis', 'Three-dimensional displays']","['Snapshot Measurement', 'Single Measurement', 'Reconstruction Algorithm', 'Spectral Channels', 'dB Improvement', 'Deep Learning', 'Feature Maps', 'Data Augmentation', 'Generative Adversarial Networks', 'High-quality Images', 'Spectral Imaging', 'Single Frame', 'Computer Image', 'Variational Autoencoder', 'Attention Map', 'GPU Memory', 'Long-range Dependencies', 'Spectral Curves', 'Residual Learning', 'Image Compression', 'Reconstruction Stage', 'I-frame', 'Hyperspectral Cube', 'Conditional Generative Adversarial Network', 'Input Measurements', 'Imaging System', 'CCD Camera', 'Feature Space', 'Final Results', 'Objective Function']",,135,"We propose the λ-net, which reconstructs hyperspectral images (e.g., with 24 spectral channels) from a single shot measurement. This task is usually termed snapshot compressive-spectral imaging (SCI), which enjoys low cost, low bandwidth and high-speed sensing rate via capturing the three-dimensional (3D) signal i.e., (x, y, λ), using a 2D snapshot. Though proposed more than a decade ago, the poor quality and low-speed of reconstruction algorithms preclude wide applications of SCI. To address this challenge, in this paper, we develop a dual-stage generative model to reconstruct the desired 3D signal in SCI, dubbed λ-net. Results on both simulation and real datasets demonstrate the significant advantages of λ-net, which leads to >4dB improvement in PSNR for real-mask-in-the-loop simulation data compared to the current state-of-the-art. Furthermore, λ-net can finish the reconstruction task within sub-seconds instead of hours taken by the most recently proposed DeSCI algorithm, thus speeding up the reconstruction >1000 times."
xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera,"Denis Tome, Patrick Peluse, Lourdes Agapito, Hernan Badino",Facebook Reality Lab; University College London,50.0,uk,50.0,USA,"We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just 2 cm. away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset -- xR-EgoPose -- offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.pdf,,,,main,Oral,https://ieeexplore.ieee.org/document/9010983/,"['Three-dimensional displays', 'Cameras', 'Pose estimation', 'Two dimensional displays', 'Training', 'Resists', 'Uncertainty']","['Head-mounted Display', 'Human Pose', '3D Human Pose', 'Egocentric 3D', 'Lower Body', 'State Of The Art', 'Body Shape', 'Upper Body', 'Real-world Datasets', 'Pose Estimation', 'Differences In Resolution', 'Drastic Differences', 'Fisheye Lens', '3D Pose', 'Training Corpus', 'Personal Point Of View', 'Training Set', 'Field Of View', 'Uncertainty Estimation', 'Human Pose Estimation', '3D Module', '2D Pose', 'External Camera', 'Inertial Measurement Unit', 'RGB Images', 'Latent Vector', 'Latent Space', '3D Joint', '2D Datasets']",,72,"We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just $2$ cm.~away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset -- xR-EgoPose -- offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint."
âSkimming-Perusalâ Tracking: A Framework for Real-Time and Robust Long-Term Tracking,"Bin Yan, Haojie Zhao, Dong Wang, Huchuan Lu, Xiaoyun Yang","China Science IntelliCloud Technology Co., Ltd.; School of Information and Communication Engineering, Dalian University of Technology, China",50.0,china,50.0,China,"Compared with traditional short-term tracking, long-term tracking poses more challenges and is much closer to realistic applications. However, few works have been done and their performance have also been limited. In this work, we present a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules. The perusal module consists of an effective bounding box regressor to generate a series of candidate proposals and a robust target verifier to infer the optimal candidate with its confidence score. Based on this score, our tracker determines whether the tracked object being present or absent, and then chooses the tracking strategies of local search or global search respectively in the next frame. To speed up the image-wide global search, a novel skimming module is designed to efficiently choose the most possible regions from a large number of sliding windows. Numerous experimental results on the VOT-2018 long-term and OxUvA long-term benchmarks demonstrate that the proposed method achieves the best performance and runs in real-time. The source codes are available at https://github.com/iiau-tracker/SPLT.",,http://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Skimming-Perusal_Tracking_A_Framework_for_Real-Time_and_Robust_Long-Term_Tracking_ICCV_2019_paper.html,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Skimming-Perusal_Tracking_A_Framework_for_Real-Time_and_Robust_Long-Term_Tracking_ICCV_2019_paper.pdf,,https://github.com/iiau-tracker/SPLT,,main,Poster,,,,,,
